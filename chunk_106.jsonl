{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck the current state of the experiment to see whether it has been completed.", "response": "def experiment_completed(self):\n        \"\"\"Checks the current state of the experiment to see whether it has\n        completed. This makes use of the experiment server `/summary` route,\n        which in turn uses :meth:`~Experiment.is_complete`.\n        \"\"\"\n        heroku_app = HerokuApp(self.app_id)\n        status_url = \"{}/summary\".format(heroku_app.url)\n        data = {}\n        try:\n            resp = requests.get(status_url)\n            data = resp.json()\n        except (ValueError, requests.exceptions.RequestException):\n            logger.exception(\"Error fetching experiment status.\")\n        logger.debug(\"Current application state: {}\".format(data))\n        return data.get(\"completed\", False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrieve_data(self):\n        local = False\n        if self.exp_config.get(\"mode\") == \"debug\":\n            local = True\n        filename = export(self.app_id, local=local)\n        logger.debug(\"Data exported to %s\" % filename)\n        return Data(filename)", "response": "Retrieves and saves data from a running experiment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef end_experiment(self):\n        if self.exp_config.get(\"mode\") != \"debug\":\n            HerokuApp(self.app_id).destroy()\n        return True", "response": "Terminates a running experiment"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef events_for_replay(self, session=None, target=None):\n        if session is None:\n            session = self.session\n        return session.query(Info).order_by(Info.creation_time)", "response": "Returns an ordered list of events for replaying."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndisplay Jupyter Notebook widget", "response": "def _ipython_display_(self):\n        \"\"\"Display Jupyter Notebook widget\"\"\"\n        from IPython.display import display\n\n        self.build_widget()\n        display(self.widget())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_config(config):\n    debug_mode = config.get(\"mode\") == \"debug\"\n    name = config.get(\"recruiter\", None)\n    recruiter = None\n\n    # Special case 1: Don't use a configured recruiter in replay mode\n    if config.get(\"replay\"):\n        return HotAirRecruiter()\n\n    if name is not None:\n        recruiter = by_name(name)\n\n        # Special case 2: may run BotRecruiter or MultiRecruiter in any mode\n        # (debug or not), so it trumps everything else:\n        if isinstance(recruiter, (BotRecruiter, MultiRecruiter)):\n            return recruiter\n\n    # Special case 3: if we're not using bots and we're in debug mode,\n    # ignore any configured recruiter:\n    if debug_mode:\n        return HotAirRecruiter()\n\n    # Configured recruiter:\n    if recruiter is not None:\n        return recruiter\n\n    if name and recruiter is None:\n        raise NotImplementedError(\"No such recruiter {}\".format(name))\n\n    # Default if we're not in debug mode:\n    return MTurkRecruiter()", "response": "Return a Recruiter instance based on the configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn initial experiment URL list plus instructions for finding subsequent recruitment events in experiemnt logs.", "response": "def open_recruitment(self, n=1):\n        \"\"\"Return initial experiment URL list, plus instructions\n        for finding subsequent recruitment events in experiemnt logs.\n        \"\"\"\n        logger.info(\"Opening CLI recruitment for {} participants\".format(n))\n        recruitments = self.recruit(n)\n        message = (\n            'Search for \"{}\" in the logs for subsequent recruitment URLs.\\n'\n            \"Open the logs for this experiment with \"\n            '\"dallinger logs --app {}\"'.format(\n                NEW_RECRUIT_LOG_PREFIX, self.config.get(\"id\")\n            )\n        )\n        return {\"items\": recruitments, \"message\": message}"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates experiemnt URLs and print them to the console.", "response": "def recruit(self, n=1):\n        \"\"\"Generate experiemnt URLs and print them to the console.\"\"\"\n        logger.info(\"Recruiting {} CLI participants\".format(n))\n        urls = []\n        template = \"{}/ad?recruiter={}&assignmentId={}&hitId={}&workerId={}&mode={}\"\n        for i in range(n):\n            ad_url = template.format(\n                get_base_url(),\n                self.nickname,\n                generate_random_id(),\n                generate_random_id(),\n                generate_random_id(),\n                self._get_mode(),\n            )\n            logger.info(\"{} {}\".format(NEW_RECRUIT_LOG_PREFIX, ad_url))\n            urls.append(ad_url)\n\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reward_bonus(self, assignment_id, amount, reason):\n        logger.info(\n            'Award ${} for assignment {}, with reason \"{}\"'.format(\n                amount, assignment_id, reason\n            )\n        )", "response": "Print out bonus info for the assignment"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns initial experiment URL list plus instructions for finding subsequent recruitment events in experiemnt logs.", "response": "def open_recruitment(self, n=1):\n        \"\"\"Return initial experiment URL list, plus instructions\n        for finding subsequent recruitment events in experiemnt logs.\n        \"\"\"\n        logger.info(\"Opening HotAir recruitment for {} participants\".format(n))\n        recruitments = self.recruit(n)\n        message = \"Recruitment requests will open browser windows automatically.\"\n\n        return {\"items\": recruitments, \"message\": message}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef open_recruitment(self, n=1):\n        logger.info(\"Opening Sim recruitment for {} participants\".format(n))\n        return {\"items\": self.recruit(n), \"message\": \"Simulated recruitment only\"}", "response": "Open recruitment for n participants."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open_recruitment(self, n=1):\n        logger.info(\"Opening MTurk recruitment for {} participants\".format(n))\n        if self.is_in_progress:\n            raise MTurkRecruiterException(\n                \"Tried to open_recruitment on already open recruiter.\"\n            )\n\n        if self.hit_domain is None:\n            raise MTurkRecruiterException(\"Can't run a HIT from localhost\")\n\n        self.mturkservice.check_credentials()\n\n        if self.config.get(\"assign_qualifications\"):\n            self._create_mturk_qualifications()\n\n        hit_request = {\n            \"max_assignments\": n,\n            \"title\": self.config.get(\"title\"),\n            \"description\": self.config.get(\"description\"),\n            \"keywords\": self._config_to_list(\"keywords\"),\n            \"reward\": self.config.get(\"base_payment\"),\n            \"duration_hours\": self.config.get(\"duration\"),\n            \"lifetime_days\": self.config.get(\"lifetime\"),\n            \"ad_url\": self.ad_url,\n            \"notification_url\": self.config.get(\"notification_url\"),\n            \"approve_requirement\": self.config.get(\"approve_requirement\"),\n            \"us_only\": self.config.get(\"us_only\"),\n            \"blacklist\": self._config_to_list(\"qualification_blacklist\"),\n            \"annotation\": self.config.get(\"id\"),\n        }\n        hit_info = self.mturkservice.create_hit(**hit_request)\n        if self.config.get(\"mode\") == \"sandbox\":\n            lookup_url = (\n                \"https://workersandbox.mturk.com/mturk/preview?groupId={type_id}\"\n            )\n        else:\n            lookup_url = \"https://worker.mturk.com/mturk/preview?groupId={type_id}\"\n\n        return {\n            \"items\": [lookup_url.format(**hit_info)],\n            \"message\": \"HIT now published to Amazon Mechanical Turk\",\n        }", "response": "Open a connection to AWS MTurk and create a HIT."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrecruits n new participants to an existing HIT", "response": "def recruit(self, n=1):\n        \"\"\"Recruit n new participants to an existing HIT\"\"\"\n        logger.info(\"Recruiting {} MTurk participants\".format(n))\n        if not self.config.get(\"auto_recruit\"):\n            logger.info(\"auto_recruit is False: recruitment suppressed\")\n            return\n\n        hit_id = self.current_hit_id()\n        if hit_id is None:\n            logger.info(\"no HIT in progress: recruitment aborted\")\n            return\n\n        try:\n            return self.mturkservice.extend_hit(\n                hit_id, number=n, duration_hours=self.config.get(\"duration\")\n            )\n        except MTurkServiceException as ex:\n            logger.exception(str(ex))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassigns a Qualification to the Participant for the experiment ID and for the configured group_name.", "response": "def notify_completed(self, participant):\n        \"\"\"Assign a Qualification to the Participant for the experiment ID,\n        and for the configured group_name, if it's been set.\n\n        Overrecruited participants don't receive qualifications, since they\n        haven't actually completed the experiment. This allows them to remain\n        eligible for future runs.\n        \"\"\"\n        if participant.status == \"overrecruited\" or not self.qualification_active:\n            return\n\n        worker_id = participant.worker_id\n\n        for name in self.qualifications:\n            try:\n                self.mturkservice.increment_qualification_score(name, worker_id)\n            except QualificationNotFoundException as ex:\n                logger.exception(ex)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reward_bonus(self, assignment_id, amount, reason):\n        try:\n            return self.mturkservice.grant_bonus(assignment_id, amount, reason)\n        except MTurkServiceException as ex:\n            logger.exception(str(ex))", "response": "Reward the Turker for a specified assignment with a bonus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating MTurk Qualification for experiment ID and for group_name .", "response": "def _create_mturk_qualifications(self):\n        \"\"\"Create MTurk Qualification for experiment ID, and for group_name\n        if it's been set. Qualifications with these names already exist, but\n        it's faster to try and fail than to check, then try.\n        \"\"\"\n        for name, desc in self.qualifications.items():\n            try:\n                self.mturkservice.create_qualification_type(name, desc)\n            except DuplicateQualificationNameError:\n                pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts recruiting right away.", "response": "def open_recruitment(self, n=1):\n        \"\"\"Start recruiting right away.\"\"\"\n        logger.info(\"Opening Bot recruitment for {} participants\".format(n))\n        factory = self._get_bot_factory()\n        bot_class_name = factory(\"\", \"\", \"\").__class__.__name__\n        return {\n            \"items\": self.recruit(n),\n            \"message\": \"Bot recruitment started using {}\".format(bot_class_name),\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrecruiting n new participant bots to the queue", "response": "def recruit(self, n=1):\n        \"\"\"Recruit n new participant bots to the queue\"\"\"\n        logger.info(\"Recruiting {} Bot participants\".format(n))\n        factory = self._get_bot_factory()\n        urls = []\n        q = _get_queue()\n        for _ in range(n):\n            base_url = get_base_url()\n            worker = generate_random_id()\n            hit = generate_random_id()\n            assignment = generate_random_id()\n            ad_parameters = (\n                \"recruiter={}&assignmentId={}&hitId={}&workerId={}&mode=sandbox\"\n            )\n            ad_parameters = ad_parameters.format(self.nickname, assignment, hit, worker)\n            url = \"{}/ad?{}\".format(base_url, ad_parameters)\n            urls.append(url)\n            bot = factory(url, assignment_id=assignment, worker_id=worker, hit_id=hit)\n            job = q.enqueue(bot.run_experiment, timeout=self._timeout)\n            logger.warning(\"Created job {} for url {}.\".format(job.id, url))\n\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notify_duration_exceeded(self, participants, reference_time):\n        for participant in participants:\n            participant.status = \"rejected\"\n            session.commit()", "response": "Notify the user that the duration of the participants has been exceeded."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the specification of how to recruit participants.", "response": "def parse_spec(self):\n        \"\"\"Parse the specification of how to recruit participants.\n\n        Example: recruiters = bots: 5, mturk: 1\n        \"\"\"\n        recruiters = []\n        spec = get_config().get(\"recruiters\")\n        for match in self.SPEC_RE.finditer(spec):\n            name = match.group(1)\n            count = int(match.group(2))\n            recruiters.append((name, count))\n        return recruiters"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recruiters(self, n=1):\n        recruit_count = 0\n        while recruit_count <= n:\n            counts = dict(\n                session.query(Recruitment.recruiter_id, func.count(Recruitment.id))\n                .group_by(Recruitment.recruiter_id)\n                .all()\n            )\n            for recruiter_id, target_count in self.spec:\n                remaining = 0\n                count = counts.get(recruiter_id, 0)\n                if count >= target_count:\n                    # This recruiter quota was reached;\n                    # move on to the next one.\n                    counts[recruiter_id] = count - target_count\n                    continue\n                else:\n                    # Quota is still available; let's use it.\n                    remaining = target_count - count\n                    break\n            else:\n                return\n\n            num_recruits = min(n - recruit_count, remaining)\n            # record the recruitments and commit\n            for i in range(num_recruits):\n                session.add(Recruitment(recruiter_id=recruiter_id))\n            session.commit()\n\n            recruit_count += num_recruits\n            yield by_name(recruiter_id), num_recruits", "response": "Iterator that provides the participant\n            count to be recruited for up to n participants."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef open_recruitment(self, n=1):\n        logger.info(\"Multi recruitment running for {} participants\".format(n))\n        recruitments = []\n        messages = {}\n        remaining = n\n        for recruiter, count in self.recruiters(n):\n            if not count:\n                break\n            if recruiter.nickname in messages:\n                result = recruiter.recruit(count)\n                recruitments.extend(result)\n            else:\n                result = recruiter.open_recruitment(count)\n                recruitments.extend(result[\"items\"])\n                messages[recruiter.nickname] = result[\"message\"]\n\n            remaining -= count\n            if remaining <= 0:\n                break\n\n        logger.info(\n            (\n                \"Multi-recruited {} out of {} participants, \" \"using {} recruiters.\"\n            ).format(n - remaining, n, len(messages))\n        )\n\n        return {\"items\": recruitments, \"message\": \"\\n\".join(messages.values())}", "response": "Return initial experiment URL list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_check(participants, config, reference_time):\n    recruiters_with_late_participants = defaultdict(list)\n    for p in participants:\n        timeline = ParticipationTime(p, reference_time, config)\n        if timeline.is_overdue:\n            print(\n                \"Error: participant {} with status {} has been playing for too \"\n                \"long - their recruiter will be notified.\".format(p.id, p.status)\n            )\n            recruiters_with_late_participants[p.recruiter_id].append(p)\n\n    for recruiter_id, participants in recruiters_with_late_participants.items():\n        recruiter = recruiters.by_name(recruiter_id)\n        recruiter.notify_duration_exceeded(participants, reference_time)", "response": "Check if the participants have been playing for too long."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_db_for_missing_notifications():\n    config = dallinger.config.get_config()\n    participants = Participant.query.filter_by(status=\"working\").all()\n    reference_time = datetime.now()\n\n    run_check(participants, config, reference_time)", "response": "Check the database for missing notifications."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _work(self, burst=False, logging_level=logging.INFO):\n        setup_loghandlers(logging_level)\n        self._install_signal_handlers()\n\n        self.did_perform_work = False\n        self.register_birth()\n        self.log.info(\n            \"RQ GEVENT worker (Greenlet pool size={0}) {1!r} started, version {2}\".format(\n                self.gevent_pool.size, self.key, VERSION\n            )\n        )\n        self.set_state(WorkerStatus.STARTED)\n\n        try:\n            while True:\n                try:\n                    self.check_for_suspension(burst)\n\n                    if self.should_run_maintenance_tasks:\n                        self.clean_registries()\n\n                    if self._stop_requested:\n                        self.log.info(\"Stopping on request.\")\n                        break\n\n                    timeout = None if burst else max(1, self.default_worker_ttl - 60)\n\n                    result = self.dequeue_job_and_maintain_ttl(timeout)\n                    if result is None and burst:\n                        self.log.info(\"RQ worker {0!r} done, quitting\".format(self.key))\n\n                        try:\n                            # Make sure dependented jobs are enqueued.\n                            gevent.wait(self.children)\n                        except LoopExit:\n                            pass\n                        result = self.dequeue_job_and_maintain_ttl(timeout)\n\n                    if result is None:\n                        break\n                except StopRequested:\n                    break\n\n                job, queue = result\n                self.execute_job(job, queue)\n\n        finally:\n            if not self.is_horse:\n                self.register_death()\n        return self.did_perform_work", "response": "Starts the work loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nspawns a greenlet to be able to kill it when it s blocked dequeueing the job.", "response": "def work(self, burst=False, logging_level=logging.INFO):\n        \"\"\"\n        Spawning a greenlet to be able to kill it when it's blocked dequeueing job\n        :param burst: if it's burst worker don't need to spawn a greenlet\n        \"\"\"\n        # If the is a burst worker it's not needed to spawn greenlet\n        if burst:\n            return self._work(burst, logging_level=logging_level)\n\n        self.gevent_worker = gevent.spawn(self._work, burst)\n        self.gevent_worker.join()\n        return self.gevent_worker.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(dlgr_id, snapshot=None):\n    try:\n        config.get(\"osf_access_token\")\n    except KeyError:\n        pass\n    else:\n        osf_id = _create_osf_project(dlgr_id)\n        _upload_assets_to_OSF(dlgr_id, osf_id)", "response": "Register the experiment using configured services."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a project on the OSF.", "response": "def _create_osf_project(dlgr_id, description=None):\n    \"\"\"Create a project on the OSF.\"\"\"\n\n    if not description:\n        description = \"Experiment {} registered by Dallinger.\".format(dlgr_id)\n\n    r = requests.post(\n        \"{}/nodes/\".format(root),\n        data={\n            \"type\": \"nodes\",\n            \"category\": \"project\",\n            \"title\": \"Experiment dlgr-{}\".format(dlgr_id[0:8]),\n            \"description\": description,\n        },\n        headers={\"Authorization\": \"Bearer {}\".format(config.get(\"osf_access_token\"))},\n    )\n    r.raise_for_status()\n    osf_id = r.json()[\"data\"][\"id\"]\n\n    logger.info(\"Project registered on OSF at http://osf.io/{}\".format(osf_id))\n\n    return osf_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _upload_assets_to_OSF(dlgr_id, osf_id, provider=\"osfstorage\"):\n    root = \"https://files.osf.io/v1\"\n    snapshot_filename = \"{}-code.zip\".format(dlgr_id)\n    snapshot_path = os.path.join(\"snapshots\", snapshot_filename)\n    r = requests.put(\n        \"{}/resources/{}/providers/{}/\".format(root, osf_id, provider),\n        params={\"kind\": \"file\", \"name\": snapshot_filename},\n        headers={\n            \"Authorization\": \"Bearer {}\".format(config.get(\"osf_access_token\")),\n            \"Content-Type\": \"text/plain\",\n        },\n        data=open(snapshot_path, \"rb\"),\n    )\n    r.raise_for_status()", "response": "Upload experimental assets to the OSF."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef outformat_is_json():\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.outformat_is_json()", "response": "Check if the current click context is json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the current click context is Unix.", "response": "def outformat_is_unix():\n    \"\"\"\n    Only safe to call within a click context.\n    \"\"\"\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.outformat_is_unix()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef outformat_is_text():\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.outformat_is_text()", "response": "Check if the outformat of the current click context is text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the JMESPATH expression for the current click context.", "response": "def get_jmespath_expression():\n    \"\"\"\n    Only safe to call within a click context.\n    \"\"\"\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.jmespath_expr"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef verbosity():\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.verbosity", "response": "Get the current verbosity of the current node."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_verbose():\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    return state.is_verbose()", "response": "Returns True if the current click context is verbose."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_task(successful_transfers, task_id):\n    client = get_client()\n\n    if successful_transfers:\n        print_successful_transfers(client, task_id)\n    else:\n        print_task_detail(client, task_id)", "response": "Executor for globus task show"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bookmark_create(endpoint_plus_path, bookmark_name):\n    endpoint_id, path = endpoint_plus_path\n    client = get_client()\n\n    submit_data = {\"endpoint_id\": str(endpoint_id), \"path\": path, \"name\": bookmark_name}\n\n    res = client.create_bookmark(submit_data)\n    formatted_print(res, simple_text=\"Bookmark ID: {}\".format(res[\"id\"]))", "response": "Executor for globus bookmark create"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef server_list(endpoint_id):\n    # raises usage error on shares for us\n    endpoint, server_list = get_endpoint_w_server_list(endpoint_id)\n\n    if server_list == \"S3\":  # not GCS -- this is an S3 endpoint\n        server_list = {\"s3_url\": endpoint[\"s3_url\"]}\n        fields = [(\"S3 URL\", \"s3_url\")]\n        text_format = FORMAT_TEXT_RECORD\n    else:  # regular GCS host endpoint\n        fields = (\n            (\"ID\", \"id\"),\n            (\"URI\", lambda s: (s[\"uri\"] or \"none (Globus Connect Personal)\")),\n        )\n        text_format = FORMAT_TEXT_TABLE\n    formatted_print(server_list, text_format=text_format, fields=fields)", "response": "Executor for globus endpoint server list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef task_list(\n    limit,\n    filter_task_id,\n    filter_status,\n    filter_type,\n    filter_label,\n    filter_not_label,\n    inexact,\n    filter_requested_after,\n    filter_requested_before,\n    filter_completed_after,\n    filter_completed_before,\n):\n    \"\"\"\n    Executor for `globus task-list`\n    \"\"\"\n\n    def _process_filterval(prefix, value, default=None):\n        if value:\n            if isinstance(value, six.string_types):\n                return \"{}:{}/\".format(prefix, value)\n            return \"{}:{}/\".format(prefix, \",\".join(str(x) for x in value))\n        else:\n            return default or \"\"\n\n    # make filter string\n    filter_string = \"\"\n    filter_string += _process_filterval(\"task_id\", filter_task_id)\n    filter_string += _process_filterval(\"status\", filter_status)\n    filter_string += _process_filterval(\n        \"type\", filter_type, default=\"type:TRANSFER,DELETE/\"\n    )\n\n    # combine data into one list for easier processing\n    if inexact:\n        label_data = [\"~\" + s for s in filter_label] + [\n            \"!~\" + s for s in filter_not_label\n        ]\n    else:\n        label_data = [\"=\" + s for s in filter_label] + [\n            \"!\" + s for s in filter_not_label\n        ]\n    filter_string += _process_filterval(\"label\", label_data)\n\n    filter_string += _process_filterval(\n        \"request_time\",\n        [(filter_requested_after or \"\"), (filter_requested_before or \"\")],\n    )\n    filter_string += _process_filterval(\n        \"completion_time\",\n        [(filter_completed_after or \"\"), (filter_completed_before or \"\")],\n    )\n\n    client = get_client()\n    task_iterator = client.task_list(\n        num_results=limit, filter=filter_string[:-1]\n    )  # ignore trailing /\n\n    fields = [\n        (\"Task ID\", \"task_id\"),\n        (\"Status\", \"status\"),\n        (\"Type\", \"type\"),\n        (\"Source Display Name\", \"source_endpoint_display_name\"),\n        (\"Dest Display Name\", \"destination_endpoint_display_name\"),\n        (\"Label\", \"label\"),\n    ]\n    formatted_print(\n        task_iterator, fields=fields, json_converter=iterable_response_to_dict\n    )", "response": "Executor for globus task - list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_command(\n    batch,\n    ignore_missing,\n    star_silent,\n    recursive,\n    enable_globs,\n    endpoint_plus_path,\n    label,\n    submission_id,\n    dry_run,\n    deadline,\n    skip_activation_check,\n    notify,\n):\n    \"\"\"\n    Executor for `globus delete`\n    \"\"\"\n    endpoint_id, path = endpoint_plus_path\n    if path is None and (not batch):\n        raise click.UsageError(\"delete requires either a PATH OR --batch\")\n\n    client = get_client()\n\n    # attempt to activate unless --skip-activation-check is given\n    if not skip_activation_check:\n        autoactivate(client, endpoint_id, if_expires_in=60)\n\n    delete_data = DeleteData(\n        client,\n        endpoint_id,\n        label=label,\n        recursive=recursive,\n        ignore_missing=ignore_missing,\n        submission_id=submission_id,\n        deadline=deadline,\n        skip_activation_check=skip_activation_check,\n        interpret_globs=enable_globs,\n        **notify\n    )\n\n    if batch:\n        # although this sophisticated structure (like that in transfer)\n        # isn't strictly necessary, it gives us the ability to add options in\n        # the future to these lines with trivial modifications\n        @click.command()\n        @click.argument(\"path\", type=TaskPath(base_dir=path))\n        def process_batch_line(path):\n            \"\"\"\n            Parse a line of batch input and add it to the delete submission\n            item.\n            \"\"\"\n            delete_data.add_item(str(path))\n\n        shlex_process_stdin(process_batch_line, \"Enter paths to delete, line by line.\")\n    else:\n        if not star_silent and enable_globs and path.endswith(\"*\"):\n            # not intuitive, but `click.confirm(abort=True)` prints to stdout\n            # unnecessarily, which we don't really want...\n            # only do this check if stderr is a pty\n            if (\n                err_is_terminal()\n                and term_is_interactive()\n                and not click.confirm(\n                    'Are you sure you want to delete all files matching \"{}\"?'.format(\n                        path\n                    ),\n                    err=True,\n                )\n            ):\n                safeprint(\"Aborted.\", write_to_stderr=True)\n                click.get_current_context().exit(1)\n        delete_data.add_item(path)\n\n    if dry_run:\n        formatted_print(delete_data, response_key=\"DATA\", fields=[(\"Path\", \"path\")])\n        # exit safely\n        return\n\n    res = client.submit_delete(delete_data)\n    formatted_print(\n        res,\n        text_format=FORMAT_TEXT_RECORD,\n        fields=((\"Message\", \"message\"), (\"Task ID\", \"task_id\")),\n    )", "response": "Executor for globus delete"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_command(value, parameter):\n    conf = get_config_obj()\n\n    section = \"cli\"\n    if \".\" in parameter:\n        section, parameter = parameter.split(\".\", 1)\n\n    # ensure that the section exists\n    if section not in conf:\n        conf[section] = {}\n    # set the value for the given parameter\n    conf[section][parameter] = value\n\n    # write to disk\n    safeprint(\"Writing updated config to {}\".format(conf.filename))\n    conf.write()", "response": "Executor for globus config set"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses the stdin line by line.", "response": "def shlex_process_stdin(process_command, helptext):\n    \"\"\"\n    Use shlex to process stdin line-by-line.\n    Also prints help text.\n\n    Requires that @process_command be a Click command object, used for\n    processing single lines of input. helptext is prepended to the standard\n    message printed to interactive sessions.\n    \"\"\"\n    # if input is interactive, print help to stderr\n    if sys.stdin.isatty():\n        safeprint(\n            (\n                \"{}\\n\".format(helptext) + \"Lines are split with shlex in POSIX mode: \"\n                \"https://docs.python.org/library/shlex.html#parsing-rules\\n\"\n                \"Terminate input with Ctrl+D or <EOF>\\n\"\n            ),\n            write_to_stderr=True,\n        )\n\n    # use readlines() rather than implicit file read line looping to force\n    # python to properly capture EOF (otherwise, EOF acts as a flush and\n    # things get weird)\n    for line in sys.stdin.readlines():\n        # get the argument vector:\n        # do a shlex split to handle quoted paths with spaces in them\n        # also lets us have comments with #\n        argv = shlex.split(line, comments=True)\n        if argv:\n            try:\n                process_command.main(args=argv)\n            except SystemExit as e:\n                if e.code != 0:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef globus_group(*args, **kwargs):\n\n    def inner_decorator(f):\n        f = click.group(*args, cls=GlobusCommandGroup, **kwargs)(f)\n        f = common_options(f)\n        return f\n\n    return inner_decorator", "response": "Decorator for globus group"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_package_data():\n    moddata = []\n    modlist = (\n        \"click\",\n        \"configobj\",\n        \"cryptography\",\n        \"globus_cli\",\n        \"globus_sdk\",\n        \"jmespath\",\n        \"requests\",\n        \"six\",\n    )\n    if verbosity() < 2:\n        modlist = (\"globus_cli\", \"globus_sdk\", \"requests\")\n\n    for mod in modlist:\n        cur = [mod]\n        try:\n            loaded_mod = __import__(mod)\n        except ImportError:\n            loaded_mod = None\n\n        for attr in (\"__version__\", \"__file__\", \"__path__\"):\n            # if loading failed, be sure to pad with error messages\n            if loaded_mod is None:\n                cur.append(\"[import failed]\")\n                continue\n\n            try:\n                attrval = getattr(loaded_mod, attr)\n            except AttributeError:\n                attrval = \"\"\n            cur.append(attrval)\n        moddata.append(cur)\n\n    return moddata", "response": "Imports a set of important packages and returns relevant data about them in a\n    dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_version():\n    latest, current = get_versions()\n    if latest is None:\n        safeprint(\n            (\"Installed Version: {0}\\n\" \"Failed to lookup latest version.\").format(\n                current\n            )\n        )\n    else:\n        safeprint(\n            (\"Installed Version: {0}\\n\" \"Latest Version:    {1}\\n\" \"\\n{2}\").format(\n                current,\n                latest,\n                \"You are running the latest version of the Globus CLI\"\n                if current == latest\n                else (\n                    \"You should update your version of the Globus CLI with\\n\"\n                    \"  globus update\"\n                )\n                if current < latest\n                else \"You are running a preview version of the Globus CLI\",\n            )\n        )\n\n    # verbose shows more platform and python info\n    # it also includes versions of some CLI dependencies\n    if is_verbose():\n        moddata = _get_package_data()\n\n        safeprint(\"\\nVerbose Data\\n---\")\n\n        safeprint(\"platform:\")\n        safeprint(\"  platform: {}\".format(platform.platform()))\n        safeprint(\"  py_implementation: {}\".format(platform.python_implementation()))\n        safeprint(\"  py_version: {}\".format(platform.python_version()))\n        safeprint(\"  sys.executable: {}\".format(sys.executable))\n        safeprint(\"  site.USER_BASE: {}\".format(site.USER_BASE))\n\n        safeprint(\"modules:\")\n        for mod, modversion, modfile, modpath in moddata:\n            safeprint(\"  {}:\".format(mod))\n            safeprint(\"    __version__: {}\".format(modversion))\n            safeprint(\"    __file__: {}\".format(modfile))\n            safeprint(\"    __path__: {}\".format(modpath))", "response": "Print out the current version of the Globus CLI."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninvoke pip and print the output.", "response": "def _call_pip(*args):\n    \"\"\"\n    Invoke pip *safely* and in the *supported* way:\n    https://pip.pypa.io/en/latest/user_guide/#using-pip-from-your-program\n    \"\"\"\n    all_args = [sys.executable, \"-m\", \"pip\"] + list(args)\n    print(\"> {}\".format(\" \".join(all_args)))\n    subprocess.check_call(all_args)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_pip_installed():\n    try:\n        subprocess.check_output(\n            [sys.executable, \"-m\", \"pip\", \"--version\"], stderr=subprocess.STDOUT\n        )\n        return True\n    except subprocess.CalledProcessError:\n        return False", "response": "Check if pip is installed and return True if it is installed False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_command(yes, development, development_version):\n    # enforce that pip MUST be installed\n    # Why not just include it in the setup.py requirements? Mostly weak\n    # reasons, but it shouldn't matter much.\n    # - if someone has installed the CLI without pip, then they haven't\n    #   followed our install instructions, so it's mostly a non-issue\n    # - we don't want to have `pip install -U globus-cli` upgrade pip -- that's\n    #   a little bit invasive and easy to do by accident on modern versions of\n    #   pip where `--upgrade-strategy` defaults to `eager`\n    # - we may want to do distributions in the future with dependencies baked\n    #   into a package, but we'd never want to do that with pip. More changes\n    #   would be needed to support that use-case, which we've discussed, but\n    #   not depending directly on pip gives us a better escape hatch\n    # - if we depend on pip, we need to start thinking about what versions we\n    #   support. In point of fact, that becomes an issue as soon as we add this\n    #   command, but not being explicit about it lets us punt for now (maybe\n    #   indefinitely) on figuring out version requirements. All of that is to\n    #   say: not including it is bad, and from that badness we reap the rewards\n    #   of procrastination and non-explicit requirements\n    # - Advanced usage, like `pip install -t` can produce an installed version\n    #   of the CLI which can't import its installing `pip`. If we depend on\n    #   pip, anyone doing this is forced to get two copies of pip, which seems\n    #   kind of nasty (even if \"they're asking for it\")\n    if not _check_pip_installed():\n        safeprint(\"`globus update` requires pip. \" \"Please install pip and try again\")\n        click.get_current_context().exit(1)\n\n    # --development-version implies --development\n    development = development or (development_version is not None)\n\n    # if we're running with `--development`, then the target version is a\n    # tarball from GitHub, and we can skip out on the safety checks\n    if development:\n        # default to master\n        development_version = development_version or \"master\"\n        target_version = (\n            \"https://github.com/globus/globus-cli/archive/{}\" \".tar.gz#egg=globus-cli\"\n        ).format(development_version)\n    else:\n        # lookup version from PyPi, abort if we can't get it\n        latest, current = get_versions()\n        if latest is None:\n            safeprint(\"Failed to lookup latest version. Aborting.\")\n            click.get_current_context().exit(1)\n\n        # in the case where we're already up to date, do nothing and exit\n        if current == latest:\n            safeprint(\"You are already running the latest version: {}\".format(current))\n            return\n\n        # if we're up to date (or ahead, meaning a dev version was installed)\n        # then prompt before continuing, respecting `--yes`\n        else:\n            safeprint(\n                (\n                    \"You are already running version {0}\\n\"\n                    \"The latest version is           {1}\"\n                ).format(current, latest)\n            )\n            if not yes and (\n                not click.confirm(\"Continue with the upgrade?\", default=True)\n            ):\n                click.get_current_context().exit(1)\n\n        # if we make it through to here, it means we didn't hit any safe (or\n        # unsafe) abort conditions, so set the target version for upgrade to\n        # the latest\n        target_version = \"globus-cli=={}\".format(latest)\n\n    # print verbose warning/help message, to guide less fortunate souls who hit\n    # Ctrl+C at a foolish time, lose connectivity, or don't invoke with `sudo`\n    # on a global install of the CLI\n    safeprint(\n        (\n            \"The Globus CLI will now update itself.\\n\"\n            \"In the event that an error occurs or the update is interrupted, we \"\n            \"recommend uninstalling and reinstalling the CLI.\\n\"\n            \"Update Target: {}\\n\"\n        ).format(target_version)\n    )\n\n    # register the upgrade activity as an atexit function\n    # this ensures that most library teardown (other than whatever libs might\n    # jam into atexit themselves...) has already run, and therefore protects us\n    # against most potential bugs resulting from upgrading click while a click\n    # command is running\n    #\n    # NOTE: there is a risk that we will see bugs on upgrade if the act of\n    # doing a pip upgrade install changes state on disk and we (or a lib we\n    # use) rely on that via pkg_resources, lazy/deferred imports, or good\n    # old-fashioned direct inspection of `__file__` and the like DURING an\n    # atexit method. Anything outside of atexit methods remains safe!\n    @atexit.register\n    def do_upgrade():\n        install_args = [\"install\", \"--upgrade\", target_version]\n        if IS_USER_INSTALL:\n            install_args.insert(1, \"--user\")\n        _call_pip(*install_args)", "response": "Executor for globus update"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef whoami_command(linked_identities):\n    client = get_auth_client()\n\n    # get userinfo from auth.\n    # if we get back an error the user likely needs to log in again\n    try:\n        res = client.oauth2_userinfo()\n    except AuthAPIError:\n        safeprint(\n            \"Unable to get user information. Please try \" \"logging in again.\",\n            write_to_stderr=True,\n        )\n        click.get_current_context().exit(1)\n\n    print_command_hint(\n        \"For information on which identities are in session see\\n\"\n        \"  globus session show\\n\"\n    )\n\n    # --linked-identities either displays all usernames or a table if verbose\n    if linked_identities:\n        try:\n            formatted_print(\n                res[\"identity_set\"],\n                fields=[\n                    (\"Username\", \"username\"),\n                    (\"Name\", \"name\"),\n                    (\"ID\", \"sub\"),\n                    (\"Email\", \"email\"),\n                ],\n                simple_text=(\n                    None\n                    if is_verbose()\n                    else \"\\n\".join([x[\"username\"] for x in res[\"identity_set\"]])\n                ),\n            )\n        except KeyError:\n            safeprint(\n                \"Your current login does not have the consents required \"\n                \"to view your full identity set. Please log in again \"\n                \"to agree to the required consents.\",\n                write_to_stderr=True,\n            )\n\n    # Default output is the top level data\n    else:\n        formatted_print(\n            res,\n            text_format=FORMAT_TEXT_RECORD,\n            fields=[\n                (\"Username\", \"preferred_username\"),\n                (\"Name\", \"name\"),\n                (\"ID\", \"sub\"),\n                (\"Email\", \"email\"),\n            ],\n            simple_text=(None if is_verbose() else res[\"preferred_username\"]),\n        )", "response": "Executor for globus whoami"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwalks the tree of commands to a terminal command or multicommand and returns a new context object that can be used to perform completion work on that command.", "response": "def get_completion_context(args):\n    \"\"\"\n    Walk the tree of commands to a terminal command or multicommand, using the\n    Click Context system.\n    Effectively, we'll be using the resilient_parsing mode of commands to stop\n    evaluation, then having them capture their options and arguments, passing\n    us on to the next subcommand. If we walk \"off the tree\" with a command that\n    we don't recognize, we have a hardstop condition, but otherwise, we walk as\n    far as we can go and that's the location from which we should do our\n    completion work.\n    \"\"\"\n    # get the \"globus\" command as a click.Command\n    root_command = click.get_current_context().find_root().command\n    # build a new context object off of it, with resilient_parsing set so that\n    # no callbacks are invoked\n    ctx = root_command.make_context(\"globus\", list(args), resilient_parsing=True)\n\n    # walk down multicommands until we've matched on everything and are at a\n    # terminal context that holds all of our completed args\n    while isinstance(ctx.command, click.MultiCommand) and args:\n        # trim out any params that are capturable at this level of the command\n        # tree by resetting the argument list\n        args = ctx.protected_args + ctx.args\n\n        # if there were no remaining args, stop walking the tree\n        if not args:\n            break\n\n        # check for a matching command, and if one isn't found stop the\n        # traversal and abort the whole process -- this would mean that a\n        # completed command was entered which doesn't match a known command\n        # there's nothing completion can do in this case unless it implements\n        # sophisticated fuzzy matching\n        command = ctx.command.get_command(ctx, args[0])\n        if not command:\n            return None\n\n        # otherwise, grab that command, and build a subcontext to continue the\n        # tree walk\n        else:\n            ctx = command.make_context(\n                args[0], args[1:], parent=ctx, resilient_parsing=True\n            )\n\n    # return the context we found\n    return ctx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all_choices(completed_args, cur, quoted):\n    ctx = get_completion_context(completed_args)\n    if not ctx:\n        return []\n\n    # matching rules, so we can toggle by type and such\n    def match_with_case(n, m):\n        \"\"\"case matters\"\"\"\n        return n.startswith(m)\n\n    def match_nocase(n, m):\n        \"\"\"case doesn't matter\"\"\"\n        return n.lower().startswith(m.lower())\n\n    match_func = match_with_case\n\n    ctx_options = [\n        p for p in ctx.command.get_params(ctx) if isinstance(p, click.Option)\n    ]\n\n    last_completed = None\n    if completed_args:\n        last_completed = completed_args[-1]\n\n    # if the last completed argument matches a Choice option, we're going to\n    # have to expand cur as a choice param\n    matching_choice_opt = None\n    for p in ctx_options:\n        if isinstance(p.type, click.Choice) and last_completed in p.opts:\n            matching_choice_opt = p\n\n    choices = []\n    # if we ended on a choice, complete with all of the available values\n    if matching_choice_opt:\n        # catch the case where it's case insensitive, and we need to change our\n        # comparisons / matching later on\n        if isinstance(matching_choice_opt.type, CaseInsensitiveChoice):\n            match_func = match_nocase\n        choices = [\n            (x, matching_choice_opt.help) for x in matching_choice_opt.type.choices\n        ]\n    # if cur looks like an option, just look for options\n    # but skip if it's quoted text\n    elif cur and cur.startswith(\"-\") and not quoted:\n        for param in ctx_options:\n            # skip hidden options\n            if isinstance(param, HiddenOption):\n                continue\n            for optset in (param.opts, param.secondary_opts):\n                for opt in optset:\n                    # only add long-opts, never short opts to completion,\n                    # unless the cur appears to be a short opt already\n                    if opt.startswith(\"--\") or (len(cur) > 1 and cur[1] != \"-\"):\n                        choices.append((opt, param.help))\n    # and if it's a multicommand we see, get the list of subcommands\n    elif isinstance(ctx.command, click.MultiCommand) and not quoted:\n        choices = [\n            (cmdname, ctx.command.get_command(ctx, cmdname).short_help)\n            for cmdname in ctx.command.list_commands(ctx)\n        ]\n    else:\n        pass\n\n    # now, do final filtering\n    if cur:\n        choices = [(n, h) for (n, h) in choices if match_func(n, cur)]\n\n    return choices", "response": "This function returns all available choices for the current word in progress."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef endpoint_is_activated(endpoint_id, until, absolute_time):\n    client = get_client()\n    res = client.endpoint_get_activation_requirements(endpoint_id)\n\n    def fail(deadline=None):\n        exp_string = \"\"\n        if deadline is not None:\n            exp_string = \" or will expire within {} seconds\".format(deadline)\n\n        message = \"The endpoint is not activated{}.\\n\\n\".format(\n            exp_string\n        ) + activation_requirements_help_text(res, endpoint_id)\n        formatted_print(res, simple_text=message)\n        click.get_current_context().exit(1)\n\n    def success(msg, *format_params):\n        formatted_print(res, simple_text=(msg.format(endpoint_id, *format_params)))\n        click.get_current_context().exit(0)\n\n    # eternally active endpoints have a special expires_in value\n    if res[\"expires_in\"] == -1:\n        success(\"{} does not require activation\")\n\n    # autoactivation is not supported and --until was not passed\n    if until is None:\n        # and we are active right now (0s in the future)...\n        if res.active_until(0):\n            success(\"{} is activated\")\n        # or we are not active\n        fail()\n\n    # autoactivation is not supported and --until was passed\n    if res.active_until(until, relative_time=not absolute_time):\n        success(\"{} will be active for at least {} seconds\", until)\n    else:\n        fail(deadline=until)", "response": "Executor for globus endpoint is - activated"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef endpoint_create(**kwargs):\n    client = get_client()\n\n    # get endpoint type, ensure unambiguous.\n    personal = kwargs.pop(\"personal\")\n    server = kwargs.pop(\"server\")\n    shared = kwargs.pop(\"shared\")\n\n    if personal and (not server) and (not shared):\n        endpoint_type = \"personal\"\n    elif server and (not personal) and (not shared):\n        endpoint_type = \"server\"\n    elif shared and (not personal) and (not server):\n        endpoint_type = \"shared\"\n    else:\n        raise click.UsageError(\n            \"Exactly one of --personal, --server, or --shared is required.\"\n        )\n\n    # validate options\n    kwargs[\"is_globus_connect\"] = personal or None\n    validate_endpoint_create_and_update_params(endpoint_type, False, kwargs)\n\n    # shared endpoint creation\n    if shared:\n        endpoint_id, host_path = shared\n        kwargs[\"host_endpoint\"] = endpoint_id\n        kwargs[\"host_path\"] = host_path\n\n        ep_doc = assemble_generic_doc(\"shared_endpoint\", **kwargs)\n        autoactivate(client, endpoint_id, if_expires_in=60)\n        res = client.create_shared_endpoint(ep_doc)\n\n    # non shared endpoint creation\n    else:\n        # omit `is_globus_connect` key if not GCP, otherwise include as `True`\n        ep_doc = assemble_generic_doc(\"endpoint\", **kwargs)\n        res = client.create_endpoint(ep_doc)\n\n    # output\n    formatted_print(\n        res,\n        fields=(COMMON_FIELDS + GCP_FIELDS if personal else COMMON_FIELDS),\n        text_format=FORMAT_TEXT_RECORD,\n    )", "response": "Executor for globus endpoint create"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef server_update(\n    endpoint_id,\n    server_id,\n    subject,\n    port,\n    scheme,\n    hostname,\n    incoming_data_ports,\n    outgoing_data_ports,\n):\n    \"\"\"\n    Executor for `globus endpoint server update`\n    \"\"\"\n    client = get_client()\n\n    server_doc = assemble_generic_doc(\n        \"server\", subject=subject, port=port, scheme=scheme, hostname=hostname\n    )\n\n    # n.b. must be done after assemble_generic_doc(), as that function filters\n    # out `None`s, which we need to be able to set for `'unspecified'`\n    if incoming_data_ports:\n        server_doc.update(\n            incoming_data_port_start=incoming_data_ports[0],\n            incoming_data_port_end=incoming_data_ports[1],\n        )\n    if outgoing_data_ports:\n        server_doc.update(\n            outgoing_data_port_start=outgoing_data_ports[0],\n            outgoing_data_port_end=outgoing_data_ports[1],\n        )\n\n    res = client.update_endpoint_server(endpoint_id, server_id, server_doc)\n    formatted_print(res, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "response": "Executor for globus endpoint server update"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry to decode a b32 - encoded username.", "response": "def _try_b32_decode(v):\n    \"\"\"\n    Attempt to decode a b32-encoded username which is sometimes generated by\n    internal Globus components.\n\n    The expectation is that the string is a valid ID, username, or b32-encoded\n    name. Therefore, we can do some simple checking on it.\n\n    If it does not appear to be formatted correctly, return None.\n    \"\"\"\n    # should start with \"u_\"\n    if not v.startswith(\"u_\"):\n        return None\n    # usernames have @ , we want to allow `u_foo@example.com`\n    # b32 names never have @\n    if \"@\" in v:\n        return None\n    # trim \"u_\"\n    v = v[2:]\n    # wrong length\n    if len(v) != 26:\n        return None\n\n    # append padding and uppercase so that b32decode will work\n    v = v.upper() + (6 * \"=\")\n\n    # try to decode\n    try:\n        return str(uuid.UUID(bytes=base64.b32decode(v)))\n    # if it fails, I guess it's a username? Not much left to do\n    except ValueError:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef supported_activation_methods(res):\n    supported = [\"web\"]  # web activation is always supported.\n\n    # oauth\n    if res[\"oauth_server\"]:\n        supported.append(\"oauth\")\n\n    for req in res[\"DATA\"]:\n        # myproxy\n        if (\n            req[\"type\"] == \"myproxy\"\n            and req[\"name\"] == \"hostname\"\n            and req[\"value\"] != \"myproxy.globusonline.org\"\n        ):\n            supported.append(\"myproxy\")\n\n        # delegate_proxy\n        if req[\"type\"] == \"delegate_proxy\" and req[\"name\"] == \"public_key\":\n            supported.append(\"delegate_proxy\")\n\n    return supported", "response": "Given an activation_requirements document returns a list of activation methods supported by this endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive an activation requirements document and an endpoint_id returns a string of help text for how to activate the endpoint", "response": "def activation_requirements_help_text(res, ep_id):\n    \"\"\"\n    Given an activation requirements document and an endpoint_id\n    returns a string of help text for how to activate the endpoint\n    \"\"\"\n    methods = supported_activation_methods(res)\n\n    lines = [\n        \"This endpoint supports the following activation methods: \",\n        \", \".join(methods).replace(\"_\", \" \"),\n        \"\\n\",\n        (\n            \"For web activation use:\\n\"\n            \"'globus endpoint activate --web {}'\\n\".format(ep_id)\n            if \"web\" in methods\n            else \"\"\n        ),\n        (\n            \"For myproxy activation use:\\n\"\n            \"'globus endpoint activate --myproxy {}'\\n\".format(ep_id)\n            if \"myproxy\" in methods\n            else \"\"\n        ),\n        (\n            \"For oauth activation use web activation:\\n\"\n            \"'globus endpoint activate --web {}'\\n\".format(ep_id)\n            if \"oauth\" in methods\n            else \"\"\n        ),\n        (\n            \"For delegate proxy activation use:\\n\"\n            \"'globus endpoint activate --delegate-proxy \"\n            \"X.509_PEM_FILE {}'\\n\".format(ep_id)\n            if \"delegate_proxy\" in methods\n            else \"\"\n        ),\n        (\n            \"Delegate proxy activation requires an additional dependency on \"\n            \"cryptography. See the docs for details:\\n\"\n            \"https://docs.globus.org/cli/reference/endpoint_activate/\\n\"\n            if \"delegate_proxy\" in methods\n            else \"\"\n        ),\n    ]\n\n    return \"\".join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting to auto - activate the given endpoint with the given client", "response": "def autoactivate(client, endpoint_id, if_expires_in=None):\n    \"\"\"\n    Attempts to auto-activate the given endpoint with the given client\n    If auto-activation fails, parses the returned activation requirements\n    to determine which methods of activation are supported, then tells\n    the user to use 'globus endpoint activate' with the correct options(s)\n    \"\"\"\n    kwargs = {}\n    if if_expires_in is not None:\n        kwargs[\"if_expires_in\"] = if_expires_in\n\n    res = client.endpoint_autoactivate(endpoint_id, **kwargs)\n    if res[\"code\"] == \"AutoActivationFailed\":\n\n        message = (\n            \"The endpoint could not be auto-activated and must be \"\n            \"activated before it can be used.\\n\\n\"\n            + activation_requirements_help_text(res, endpoint_id)\n        )\n\n        safeprint(message, write_to_stderr=True)\n        click.get_current_context().exit(1)\n\n    else:\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of server list for the given endpoint.", "response": "def get_endpoint_w_server_list(endpoint_id):\n    \"\"\"\n    A helper for handling endpoint server list lookups correctly accounting\n    for various endpoint types.\n\n    - Raises click.UsageError when used on Shares\n    - Returns (<get_endpoint_response>, \"S3\") for S3 endpoints\n    - Returns (<get_endpoint_response>, <server_list_response>) for all other\n      Endpoints\n    \"\"\"\n    client = get_client()\n\n    endpoint = client.get_endpoint(endpoint_id)\n\n    if endpoint[\"host_endpoint_id\"]:  # not GCS -- this is a share endpoint\n        raise click.UsageError(\n            dedent(\n                u\"\"\"\\\n            {id} ({0}) is a share and does not have servers.\n\n            To see details of the share, use\n                globus endpoint show {id}\n\n            To list the servers on the share's host endpoint, use\n                globus endpoint server list {host_endpoint_id}\n        \"\"\"\n            ).format(display_name_or_cname(endpoint), **endpoint.data)\n        )\n\n    if endpoint[\"s3_url\"]:  # not GCS -- legacy S3 endpoint type\n        return (endpoint, \"S3\")\n\n    else:\n        return (endpoint, client.endpoint_server_list(endpoint_id))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noptions are the core \"task wait\" options, including the `--meow` easter egg. This does the core \"task wait\" loop, including all of the IO. It *does exit* on behalf of the caller. (We can enhance with a `noabort=True` param or somesuch in the future if necessary.)", "response": "def task_wait_with_io(\n    meow, heartbeat, polling_interval, timeout, task_id, timeout_exit_code, client=None\n):\n    \"\"\"\n    Options are the core \"task wait\" options, including the `--meow` easter\n    egg.\n\n    This does the core \"task wait\" loop, including all of the IO.\n    It *does exit* on behalf of the caller. (We can enhance with a\n    `noabort=True` param or somesuch in the future if necessary.)\n    \"\"\"\n    client = client or get_client()\n\n    def timed_out(waited_time):\n        if timeout is None:\n            return False\n        else:\n            return waited_time >= timeout\n\n    def check_completed():\n        completed = client.task_wait(\n            task_id, timeout=polling_interval, polling_interval=polling_interval\n        )\n        if completed:\n            if heartbeat:\n                safeprint(\"\", write_to_stderr=True)\n            # meowing tasks wake up!\n            if meow:\n                safeprint(\n                    r\"\"\"\n                  _..\n  /}_{\\           /.-'\n ( a a )-.___...-'/\n ==._.==         ;\n      \\ i _..._ /,\n      {_;/   {_//\"\"\",\n                    write_to_stderr=True,\n                )\n\n            # TODO: possibly update TransferClient.task_wait so that we don't\n            # need to do an extra fetch to get the task status after completion\n            res = client.get_task(task_id)\n            formatted_print(res, text_format=FORMAT_SILENT)\n\n            status = res[\"status\"]\n            if status == \"SUCCEEDED\":\n                click.get_current_context().exit(0)\n            else:\n                click.get_current_context().exit(1)\n\n        return completed\n\n    # Tasks start out sleepy\n    if meow:\n        safeprint(\n            r\"\"\"\n   |\\      _,,,---,,_\n   /,`.-'`'    -.  ;-;;,_\n  |,4-  ) )-,_..;\\ (  `'-'\n '---''(_/--'  `-'\\_)\"\"\",\n            write_to_stderr=True,\n        )\n\n    waited_time = 0\n    while not timed_out(waited_time) and not check_completed():\n        if heartbeat:\n            safeprint(\".\", write_to_stderr=True, newline=False)\n            sys.stderr.flush()\n\n        waited_time += polling_interval\n\n    # add a trailing newline to heartbeats if we fail\n    if heartbeat:\n        safeprint(\"\", write_to_stderr=True)\n\n    exit_code = 1\n    if timed_out(waited_time):\n        safeprint(\n            \"Task has yet to complete after {} seconds\".format(timeout),\n            write_to_stderr=True,\n        )\n        exit_code = timeout_exit_code\n\n    # output json if requested, but nothing for text mode\n    res = client.get_task(task_id)\n    formatted_print(res, text_format=FORMAT_SILENT)\n\n    click.get_current_context().exit(exit_code)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry the given function f with the given arguments and kwargs.", "response": "def retry(self, f, *args, **kwargs):\n        \"\"\"\n        Retries the given function self.tries times on NetworkErros\n        \"\"\"\n        backoff = random.random() / 100  # 5ms on average\n        for _ in range(self.tries - 1):\n            try:\n                return f(*args, **kwargs)\n            except NetworkError:\n                time.sleep(backoff)\n                backoff *= 2\n        return f(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recursive_operation_ls(\n        self, endpoint_id, depth=3, filter_after_first=True, **params\n    ):\n        \"\"\"\n        Makes recursive calls to ``GET /operation/endpoint/<endpoint_id>/ls``\n        Does not preserve access to top level operation_ls fields, but\n        adds a \"path\" field for every item that represents the full\n        path to that item.\n        :rtype: iterable of :class:`GlobusResponse\n                <globus_sdk.response.GlobusResponse>`\n        **Parameters**\n            ``endpoint_id`` (*string*)\n              The endpoint being recursively ls'ed. If no \"path\" is given in\n              params, the start path is determined by this endpoint.\n            ``depth`` (*int*)\n              The maximum file depth the recursive ls will go to.\n            ``filter_after_first`` (*bool*)\n              If False, any \"filter\" in params will only be applied to the\n              first, top level ls, all results beyond that will be unfiltered.\n            ``params``\n              Parameters that will be passed through as query params.\n        **Examples**\n        >>> tc = globus_sdk.TransferClient(...)\n        >>> for entry in tc.recursive_operation_ls(ep_id, path=\"/~/project1/\"):\n        >>>     print(entry[\"path\"], entry[\"type\"])\n        **External Documentation**\n        See\n        `List Directory Contents \\\n        <https://docs.globus.org/api/transfer/file_operations/#list_directory_contents>`_\n        in the REST documentation for details, but note that top level data\n        fields are no longer available and an additional per item\n        \"path\" field is added.\n        \"\"\"\n        endpoint_id = safe_stringify(endpoint_id)\n        self.logger.info(\n            \"TransferClient.recursive_operation_ls({}, {}, {})\".format(\n                endpoint_id, depth, params\n            )\n        )\n        return RecursiveLsResponse(self, endpoint_id, depth, filter_after_first, params)", "response": "This method is used to recursively ls the directory contents of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert(self, value, param, ctx):\n        # passthrough conditions: None or already processed\n        if value is None or isinstance(value, tuple):\n            return value\n\n        # split the value on the first colon, leave the rest intact\n        splitval = value.split(\":\", 1)\n        # first element is the endpoint_id\n        endpoint_id = click.UUID(splitval[0])\n\n        # get the second element, defaulting to `None` if there was no colon in\n        # the original value\n        try:\n            path = splitval[1]\n        except IndexError:\n            path = None\n        # coerce path=\"\" to path=None\n        # means that we treat \"enpdoint_id\" and \"endpoint_id:\" equivalently\n        path = path or None\n\n        if path is None and self.path_required:\n            self.fail(\"The path component is required\", param=param)\n\n        return (endpoint_id, path)", "response": "Convert a value to a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef one_use_option(*args, **kwargs):\n    # cannot force a multiple or count option to be single use\n    if \"multiple\" in kwargs or \"count\" in kwargs:\n        raise ValueError(\n            \"Internal error, one_use_option cannot be  used \" \"with multiple or count.\"\n        )\n\n    # cannot force a non Option Paramater (argument) to be a OneUseOption\n    if kwargs.get(\"cls\"):\n        raise TypeError(\n            \"Internal error, one_use_option cannot overwrite \"\n            \"cls {}.\".format(kwargs.get(\"cls\"))\n        )\n\n    # use our OneUseOption class instead of a normal Option\n    kwargs[\"cls\"] = OneUseOption\n\n    # if dealing with a flag, switch to a counting option,\n    # and then assert if the count is not greater than 1 and cast to a bool\n    if kwargs.get(\"is_flag\"):\n        kwargs[\"is_flag\"] = False  # mutually exclusive with count\n        kwargs[\"count\"] = True\n\n    # if not a flag, this option takes an argument(s), switch to a multiple\n    # option, assert the len is 1, and treat the first element as the value\n    else:\n        kwargs[\"multiple\"] = True\n\n    # decorate with the click.option decorator, but with our custom kwargs\n    def decorator(f):\n        return click.option(*args, **kwargs)(f)\n\n    return decorator", "response": "Decorator that replaces any instances of the option class with the custom OneUseOption class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a server specification to a list of matching server docs.", "response": "def _spec_to_matches(server_list, server_spec, mode):\n    \"\"\"\n    mode is in {uri, hostname, hostname_port}\n\n    A list of matching server docs.\n    Should usually be 0 or 1 matches. Multiple matches are possible though.\n    \"\"\"\n    assert mode in (\"uri\", \"hostname\", \"hostname_port\")\n\n    def match(server_doc):\n        if mode == \"hostname\":\n            return server_spec == server_doc[\"hostname\"]\n        elif mode == \"hostname_port\":\n            return server_spec == \"{}:{}\".format(\n                server_doc[\"hostname\"], server_doc[\"port\"]\n            )\n        elif mode == \"uri\":\n            return server_spec == \"{}://{}:{}\".format(\n                server_doc[\"scheme\"], server_doc[\"hostname\"], server_doc[\"port\"]\n            )\n        else:\n            raise NotImplementedError(\"Unreachable error! Something is very wrong.\")\n\n    return [server_doc for server_doc in server_list if match(server_doc)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef server_delete(endpoint_id, server):\n    client = get_client()\n\n    mode = _detect_mode(server)\n\n    # list (even if not necessary) in order to make errors more consistent when\n    # mode='id'\n    endpoint, server_list = get_endpoint_w_server_list(endpoint_id)\n\n    if server_list == \"S3\":\n        raise click.UsageError(\"You cannot delete servers from S3 endpoints.\")\n\n    # we don't *have to* raise an error in the GCP case, since the API would\n    # deny it too, but doing so makes our errors a little bit more consistent\n    # with deletes against S3 endpoints and shares\n    if endpoint[\"is_globus_connect\"]:\n        raise click.UsageError(\n            \"You cannot delete servers from Globus Connect Personal endpoints\"\n        )\n\n    if mode != \"id\":\n        matches = _spec_to_matches(server_list, server, mode)\n        if not matches:\n            raise click.UsageError('No server was found matching \"{}\"'.format(server))\n        elif len(matches) > 1:\n            raise click.UsageError(\n                dedent(\n                    \"\"\"\\\n                Multiple servers matched \"{}\":\n                    {}\n            \"\"\"\n                ).format(server, [x[\"id\"] for x in matches])\n            )\n        else:\n            server = matches[0][\"id\"]\n\n    response = client.delete_endpoint_server(endpoint_id, server)\n\n    formatted_print(response, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "response": "Executor for globus endpoint server delete"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_command(endpoint_id, rule_id):\n    client = get_client()\n\n    rule = client.get_endpoint_acl_rule(endpoint_id, rule_id)\n    formatted_print(\n        rule,\n        text_format=FORMAT_TEXT_RECORD,\n        fields=(\n            (\"Rule ID\", \"id\"),\n            (\"Permissions\", \"permissions\"),\n            (\"Shared With\", _shared_with_keyfunc),\n            (\"Path\", \"path\"),\n        ),\n    )", "response": "Executor for globus endpoint permission show"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_command(parameter):\n    section = \"cli\"\n    if \".\" in parameter:\n        section, parameter = parameter.split(\".\", 1)\n\n    value = lookup_option(parameter, section=section)\n\n    if value is None:\n        safeprint(\"{} not set\".format(parameter))\n    else:\n        safeprint(\"{} = {}\".format(parameter, value))", "response": "Executor for globus config show"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rename_command(source, destination):\n    source_ep, source_path = source\n    dest_ep, dest_path = destination\n\n    if source_ep != dest_ep:\n        raise click.UsageError(\n            (\n                \"rename requires that the source and dest \"\n                \"endpoints are the same, {} != {}\"\n            ).format(source_ep, dest_ep)\n        )\n    endpoint_id = source_ep\n\n    client = get_client()\n    autoactivate(client, endpoint_id, if_expires_in=60)\n\n    res = client.operation_rename(endpoint_id, oldpath=source_path, newpath=dest_path)\n    formatted_print(res, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "response": "Executor for globus rename"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _format_value(self, val):\n        name = self.name + \":\"\n        if not self.multiline or \"\\n\" not in val:\n            val = u\"{0} {1}\".format(name.ljust(self._text_prefix_len), val)\n        else:\n            spacer = \"\\n\" + \" \" * (self._text_prefix_len + 1)\n            val = u\"{0}{1}{2}\".format(name, spacer, spacer.join(val.split(\"\\n\")))\n\n        return val", "response": "Formats a value to be good for textmode printing\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_command(endpoint_id, rule_id):\n    client = get_client()\n\n    res = client.delete_endpoint_acl_rule(endpoint_id, rule_id)\n    formatted_print(res, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "response": "Executor for globus endpoint permission delete"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transfer_command(\n    batch,\n    sync_level,\n    recursive,\n    destination,\n    source,\n    label,\n    preserve_mtime,\n    verify_checksum,\n    encrypt,\n    submission_id,\n    dry_run,\n    delete,\n    deadline,\n    skip_activation_check,\n    notify,\n    perf_cc,\n    perf_p,\n    perf_pp,\n    perf_udt,\n):\n    \"\"\"\n    Executor for `globus transfer`\n    \"\"\"\n    source_endpoint, cmd_source_path = source\n    dest_endpoint, cmd_dest_path = destination\n\n    if recursive and batch:\n        raise click.UsageError(\n            (\n                \"You cannot use --recursive in addition to --batch. \"\n                \"Instead, use --recursive on lines of --batch input \"\n                \"which need it\"\n            )\n        )\n\n    if (cmd_source_path is None or cmd_dest_path is None) and (not batch):\n        raise click.UsageError(\n            (\"transfer requires either SOURCE_PATH and DEST_PATH or \" \"--batch\")\n        )\n\n    # because python can't handle multiple **kwargs expansions in a single\n    # call, we need to get a little bit clever\n    # both the performance options (of which there are a few), and the\n    # notification options (also there are a few) have elements which should be\n    # omitted in some cases\n    # notify comes to us clean, perf opts need more care\n    # put them together into a dict before passing to TransferData\n    kwargs = {}\n    perf_opts = dict(\n        (k, v)\n        for (k, v) in dict(\n            perf_cc=perf_cc, perf_p=perf_p, perf_pp=perf_pp, perf_udt=perf_udt\n        ).items()\n        if v is not None\n    )\n    kwargs.update(perf_opts)\n    kwargs.update(notify)\n\n    client = get_client()\n    transfer_data = TransferData(\n        client,\n        source_endpoint,\n        dest_endpoint,\n        label=label,\n        sync_level=sync_level,\n        verify_checksum=verify_checksum,\n        preserve_timestamp=preserve_mtime,\n        encrypt_data=encrypt,\n        submission_id=submission_id,\n        delete_destination_extra=delete,\n        deadline=deadline,\n        skip_activation_check=skip_activation_check,\n        **kwargs\n    )\n\n    if batch:\n\n        @click.command()\n        @click.option(\"--recursive\", \"-r\", is_flag=True)\n        @click.argument(\"source_path\", type=TaskPath(base_dir=cmd_source_path))\n        @click.argument(\"dest_path\", type=TaskPath(base_dir=cmd_dest_path))\n        def process_batch_line(dest_path, source_path, recursive):\n            \"\"\"\n            Parse a line of batch input and turn it into a transfer submission\n            item.\n            \"\"\"\n            transfer_data.add_item(\n                str(source_path), str(dest_path), recursive=recursive\n            )\n\n        shlex_process_stdin(\n            process_batch_line,\n            (\n                \"Enter transfers, line by line, as\\n\\n\"\n                \"    [--recursive] SOURCE_PATH DEST_PATH\\n\"\n            ),\n        )\n    else:\n        transfer_data.add_item(cmd_source_path, cmd_dest_path, recursive=recursive)\n\n    if dry_run:\n        formatted_print(\n            transfer_data,\n            response_key=\"DATA\",\n            fields=(\n                (\"Source Path\", \"source_path\"),\n                (\"Dest Path\", \"destination_path\"),\n                (\"Recursive\", \"recursive\"),\n            ),\n        )\n        # exit safely\n        return\n\n    # autoactivate after parsing all args and putting things together\n    # skip this if skip-activation-check is given\n    if not skip_activation_check:\n        autoactivate(client, source_endpoint, if_expires_in=60)\n        autoactivate(client, dest_endpoint, if_expires_in=60)\n\n    res = client.submit_transfer(transfer_data)\n    formatted_print(\n        res,\n        text_format=FORMAT_TEXT_RECORD,\n        fields=((\"Message\", \"message\"), (\"Task ID\", \"task_id\")),\n    )", "response": "Executor for globus transfer"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef endpoint_delete(endpoint_id):\n    client = get_client()\n    res = client.delete_endpoint(endpoint_id)\n    formatted_print(res, text_format=FORMAT_TEXT_RAW, response_key=\"message\")", "response": "Executor for globus endpoint delete"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_command(default_output_format, default_myproxy_username):\n    # now handle the output format, requires a little bit more care\n    # first, prompt if it isn't given, but be clear that we have a sensible\n    # default if they don't set it\n    # then, make sure that if it is given, it's a valid format (discard\n    # otherwise)\n    # finally, set it only if given and valid\n    if not default_output_format:\n        safeprint(\n            textwrap.fill(\n                'This must be one of \"json\" or \"text\". Other values will be '\n                \"ignored. ENTER to skip.\"\n            )\n        )\n        default_output_format = (\n            click.prompt(\n                \"Default CLI output format (cli.output_format)\", default=\"text\"\n            )\n            .strip()\n            .lower()\n        )\n        if default_output_format not in (\"json\", \"text\"):\n            default_output_format = None\n\n    if not default_myproxy_username:\n        safeprint(textwrap.fill(\"ENTER to skip.\"))\n        default_myproxy_username = click.prompt(\n            \"Default myproxy username (cli.default_myproxy_username)\",\n            default=\"\",\n            show_default=False,\n        ).strip()\n\n    # write to disk\n    safeprint(\n        \"\\n\\nWriting updated config to {0}\".format(os.path.expanduser(\"~/.globus.cfg\"))\n    )\n    write_option(OUTPUT_FORMAT_OPTNAME, default_output_format)\n    write_option(MYPROXY_USERNAME_OPTNAME, default_myproxy_username)", "response": "Executor for globus config init"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef detect_and_decorate(decorator, args, kwargs):\n    # special behavior when invoked with only one non-keyword argument: act as\n    # a normal decorator, decorating and returning that argument with\n    # click.option\n    if len(args) == 1 and len(kwargs) == 0 and callable(args[0]):\n        return decorator(args[0])\n\n    # if we're not doing that, we should see no positional args\n    # the alternative behavior is to fall through and discard *args, but this\n    # will probably confuse someone in the future when their arguments are\n    # silently discarded\n    elif len(args) != 0:\n        raise ValueError(\"this decorator cannot take positional args\")\n\n    # final case: got 0 or more kwargs, no positionals\n    # do the function-which-returns-a-decorator dance to produce a\n    # new decorator based on the arguments given\n    else:\n\n        def inner_decorator(f):\n            return decorator(f, **kwargs)\n\n        return inner_decorator", "response": "Decorator for detecting and decorating a function based on the given arguments and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_command(endpoint_id):\n    client = get_client()\n\n    rules = client.endpoint_acl_list(endpoint_id)\n\n    resolved_ids = LazyIdentityMap(\n        x[\"principal\"] for x in rules if x[\"principal_type\"] == \"identity\"\n    )\n\n    def principal_str(rule):\n        principal = rule[\"principal\"]\n        if rule[\"principal_type\"] == \"identity\":\n            username = resolved_ids.get(principal)\n            return username or principal\n        elif rule[\"principal_type\"] == \"group\":\n            return (u\"https://app.globus.org/groups/{}\").format(principal)\n        else:\n            principal = rule[\"principal_type\"]\n\n        return principal\n\n    formatted_print(\n        rules,\n        fields=[\n            (\"Rule ID\", \"id\"),\n            (\"Permissions\", \"permissions\"),\n            (\"Shared With\", principal_str),\n            (\"Path\", \"path\"),\n        ],\n    )", "response": "Executor for globus endpoint permission list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bookmark_list():\n    client = get_client()\n\n    bookmark_iterator = client.bookmark_list()\n\n    def get_ep_name(item):\n        ep_id = item[\"endpoint_id\"]\n        try:\n            ep_doc = client.get_endpoint(ep_id)\n            return display_name_or_cname(ep_doc)\n        except TransferAPIError as err:\n            if err.code == \"EndpointDeleted\":\n                return \"[DELETED ENDPOINT]\"\n            else:\n                raise err\n\n    formatted_print(\n        bookmark_iterator,\n        fields=[\n            (\"Name\", \"name\"),\n            (\"Bookmark ID\", \"id\"),\n            (\"Endpoint ID\", \"endpoint_id\"),\n            (\"Endpoint Name\", get_ep_name),\n            (\"Path\", \"path\"),\n        ],\n        response_key=\"DATA\",\n        json_converter=iterable_response_to_dict,\n    )", "response": "Executor for globus bookmark list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rm_command(\n    ignore_missing,\n    star_silent,\n    recursive,\n    enable_globs,\n    endpoint_plus_path,\n    label,\n    submission_id,\n    dry_run,\n    deadline,\n    skip_activation_check,\n    notify,\n    meow,\n    heartbeat,\n    polling_interval,\n    timeout,\n    timeout_exit_code,\n):\n    \"\"\"\n    Executor for `globus rm`\n    \"\"\"\n    endpoint_id, path = endpoint_plus_path\n\n    client = get_client()\n\n    # attempt to activate unless --skip-activation-check is given\n    if not skip_activation_check:\n        autoactivate(client, endpoint_id, if_expires_in=60)\n\n    delete_data = DeleteData(\n        client,\n        endpoint_id,\n        label=label,\n        recursive=recursive,\n        ignore_missing=ignore_missing,\n        submission_id=submission_id,\n        deadline=deadline,\n        skip_activation_check=skip_activation_check,\n        interpret_globs=enable_globs,\n        **notify\n    )\n\n    if not star_silent and enable_globs and path.endswith(\"*\"):\n        # not intuitive, but `click.confirm(abort=True)` prints to stdout\n        # unnecessarily, which we don't really want...\n        # only do this check if stderr is a pty\n        if (\n            err_is_terminal()\n            and term_is_interactive()\n            and not click.confirm(\n                'Are you sure you want to delete all files matching \"{}\"?'.format(path),\n                err=True,\n            )\n        ):\n            safeprint(\"Aborted.\", write_to_stderr=True)\n            click.get_current_context().exit(1)\n    delete_data.add_item(path)\n\n    if dry_run:\n        formatted_print(delete_data, response_key=\"DATA\", fields=[(\"Path\", \"path\")])\n        # exit safely\n        return\n\n    # Print task submission to stderr so that `-Fjson` is still correctly\n    # respected, as it will be by `task wait`\n    res = client.submit_delete(delete_data)\n    task_id = res[\"task_id\"]\n    safeprint(\n        'Delete task submitted under ID \"{}\"'.format(task_id), write_to_stderr=True\n    )\n\n    # do a `task wait` equivalent, including printing and correct exit status\n    task_wait_with_io(\n        meow,\n        heartbeat,\n        polling_interval,\n        timeout,\n        task_id,\n        timeout_exit_code,\n        client=client,\n    )", "response": "Executor for globus rm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprompt the user with a link to authenticate with globus auth and authorize the CLI to act on their behalf.", "response": "def do_link_auth_flow(session_params=None, force_new_client=False):\n    \"\"\"\n    Prompts the user with a link to authenticate with globus auth\n    and authorize the CLI to act on their behalf.\n    \"\"\"\n    session_params = session_params or {}\n\n    # get the ConfidentialApp client object\n    auth_client = internal_auth_client(\n        requires_instance=True, force_new_client=force_new_client\n    )\n\n    # start the Confidential App Grant flow\n    auth_client.oauth2_start_flow(\n        redirect_uri=auth_client.base_url + \"v2/web/auth-code\",\n        refresh_tokens=True,\n        requested_scopes=SCOPES,\n    )\n\n    # prompt\n    additional_params = {\"prompt\": \"login\"}\n    additional_params.update(session_params)\n    linkprompt = \"Please authenticate with Globus here\"\n    safeprint(\n        \"{0}:\\n{1}\\n{2}\\n{1}\\n\".format(\n            linkprompt,\n            \"-\" * len(linkprompt),\n            auth_client.oauth2_get_authorize_url(additional_params=additional_params),\n        )\n    )\n\n    # come back with auth code\n    auth_code = click.prompt(\"Enter the resulting Authorization Code here\").strip()\n\n    # finish auth flow\n    exchange_code_and_store_config(auth_client, auth_code)\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_local_server_auth_flow(session_params=None, force_new_client=False):\n    session_params = session_params or {}\n\n    # start local server and create matching redirect_uri\n    with start_local_server(listen=(\"127.0.0.1\", 0)) as server:\n        _, port = server.socket.getsockname()\n        redirect_uri = \"http://localhost:{}\".format(port)\n\n        # get the ConfidentialApp client object and start a flow\n        auth_client = internal_auth_client(\n            requires_instance=True, force_new_client=force_new_client\n        )\n        auth_client.oauth2_start_flow(\n            refresh_tokens=True, redirect_uri=redirect_uri, requested_scopes=SCOPES\n        )\n        additional_params = {\"prompt\": \"login\"}\n        additional_params.update(session_params)\n        url = auth_client.oauth2_get_authorize_url(additional_params=additional_params)\n\n        # open web-browser for user to log in, get auth code\n        webbrowser.open(url, new=1)\n        auth_code = server.wait_for_code()\n\n    if isinstance(auth_code, LocalServerError):\n        safeprint(\"Authorization failed: {}\".format(auth_code), write_to_stderr=True)\n        click.get_current_context().exit(1)\n    elif isinstance(auth_code, Exception):\n        safeprint(\n            \"Authorization failed with unexpected error:\\n{}\".format(auth_code),\n            write_to_stderr=True,\n        )\n        click.get_current_context().exit(1)\n\n    # finish auth flow and return true\n    exchange_code_and_store_config(auth_client, auth_code)\n    return True", "response": "Starts a local server and opens a browser to authenticate the user and gets the code redirected to the server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexchanging code for tokens and gets user info from auth.", "response": "def exchange_code_and_store_config(auth_client, auth_code):\n    \"\"\"\n    Finishes auth flow after code is gotten from command line or local server.\n    Exchanges code for tokens and gets user info from auth.\n    Stores tokens and user info in config.\n    \"\"\"\n    # do a token exchange with the given code\n    tkn = auth_client.oauth2_exchange_code_for_tokens(auth_code)\n    tkn = tkn.by_resource_server\n\n    # extract access tokens from final response\n    transfer_at = tkn[\"transfer.api.globus.org\"][\"access_token\"]\n    transfer_at_expires = tkn[\"transfer.api.globus.org\"][\"expires_at_seconds\"]\n    transfer_rt = tkn[\"transfer.api.globus.org\"][\"refresh_token\"]\n    auth_at = tkn[\"auth.globus.org\"][\"access_token\"]\n    auth_at_expires = tkn[\"auth.globus.org\"][\"expires_at_seconds\"]\n    auth_rt = tkn[\"auth.globus.org\"][\"refresh_token\"]\n\n    # revoke any existing tokens\n    for token_opt in (\n        TRANSFER_RT_OPTNAME,\n        TRANSFER_AT_OPTNAME,\n        AUTH_RT_OPTNAME,\n        AUTH_AT_OPTNAME,\n    ):\n        token = lookup_option(token_opt)\n        if token:\n            auth_client.oauth2_revoke_token(token)\n\n    # write new tokens to config\n    write_option(TRANSFER_RT_OPTNAME, transfer_rt)\n    write_option(TRANSFER_AT_OPTNAME, transfer_at)\n    write_option(TRANSFER_AT_EXPIRES_OPTNAME, transfer_at_expires)\n    write_option(AUTH_RT_OPTNAME, auth_rt)\n    write_option(AUTH_AT_OPTNAME, auth_at)\n    write_option(AUTH_AT_EXPIRES_OPTNAME, auth_at_expires)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef endpoint_search(filter_fulltext, filter_owner_id, filter_scope):\n    if filter_scope == \"all\" and not filter_fulltext:\n        raise click.UsageError(\n            \"When searching all endpoints (--filter-scope=all, the default), \"\n            \"a full-text search filter is required. Other scopes (e.g. \"\n            \"--filter-scope=recently-used) may be used without specifying \"\n            \"an additional filter.\"\n        )\n\n    client = get_client()\n\n    owner_id = filter_owner_id\n    if owner_id:\n        owner_id = maybe_lookup_identity_id(owner_id)\n\n    search_iterator = client.endpoint_search(\n        filter_fulltext=filter_fulltext,\n        filter_scope=filter_scope,\n        filter_owner_id=owner_id,\n    )\n\n    formatted_print(\n        search_iterator,\n        fields=ENDPOINT_LIST_FIELDS,\n        json_converter=iterable_response_to_dict,\n    )", "response": "Executor for globus endpoint search"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping around click. echo that does not raise IOError.", "response": "def safeprint(message, write_to_stderr=False, newline=True):\n    \"\"\"\n    Wrapper around click.echo used to encapsulate its functionality.\n    Also protects against EPIPE during click.echo calls, as this can happen\n    normally in piped commands when the consumer closes before the producer.\n    \"\"\"\n    try:\n        click.echo(message, nl=newline, err=write_to_stderr)\n    except IOError as err:\n        if err.errno is errno.EPIPE:\n            pass\n        else:\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _key_to_keyfunc(k):\n    # if the key is a string, then the \"keyfunc\" is just a basic lookup\n    # operation -- return that\n    if isinstance(k, six.string_types):\n\n        def lookup(x):\n            return x[k]\n\n        return lookup\n    # otherwise, the key must be a function which is executed on the item\n    # to produce a value -- return it verbatim\n    return k", "response": "Converts a key to a function that returns a value for the item\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef endpoint_id_arg(*args, **kwargs):\n\n    def decorate(f, **kwargs):\n        \"\"\"\n        Work of actually decorating a function -- wrapped in here because we\n        want to dispatch depending on how this is invoked\n        \"\"\"\n        metavar = kwargs.get(\"metavar\", \"ENDPOINT_ID\")\n        f = click.argument(\"endpoint_id\", metavar=metavar, type=click.UUID)(f)\n        return f\n\n    return detect_and_decorate(decorate, args, kwargs)", "response": "Decorator for handling the endpoint_id argument."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the parameters for an endpoint create and update.", "response": "def validate_endpoint_create_and_update_params(endpoint_type, managed, params):\n    \"\"\"\n    Given an endpoint type of \"shared\" \"server\" or \"personal\" and option values\n    Confirms the option values are valid for the given endpoint\n    \"\"\"\n    # options only allowed for GCS endpoints\n    if endpoint_type != \"server\":\n        # catch params with two option flags\n        if params[\"public\"] is False:\n            raise click.UsageError(\n                \"Option --private only allowed \" \"for Globus Connect Server endpoints\"\n            )\n        # catch any params only usable with GCS\n        for option in [\n            \"public\",\n            \"myproxy_dn\",\n            \"myproxy_server\",\n            \"oauth_server\",\n            \"location\",\n            \"network_use\",\n            \"max_concurrency\",\n            \"preferred_concurrency\",\n            \"max_parallelism\",\n            \"preferred_parallelism\",\n        ]:\n            if params[option] is not None:\n                raise click.UsageError(\n                    (\n                        \"Option --{} can only be used with Globus Connect Server \"\n                        \"endpoints\".format(option.replace(\"_\", \"-\"))\n                    )\n                )\n\n    # if the endpoint was not previously managed, and is not being passed\n    # a subscription id, it cannot use managed endpoint only fields\n    if (not managed) and not (params[\"subscription_id\"] or params[\"managed\"]):\n        for option in [\n            \"network_use\",\n            \"max_concurrency\",\n            \"preferred_concurrency\",\n            \"max_parallelism\",\n            \"preferred_parallelism\",\n        ]:\n            if params[option] is not None:\n                raise click.UsageError(\n                    (\n                        \"Option --{} can only be used with managed \"\n                        \"endpoints\".format(option.replace(\"_\", \"-\"))\n                    )\n                )\n\n    # because the Transfer service doesn't do network use level updates in a\n    # patchy way, *both* endpoint `POST`s *and* `PUT`s must either use\n    # - `network_use='custom'` with *every* other parameter specified (which\n    #   is validated by the service), or\n    # - a preset/absent `network_use` with *no* other parameter specified\n    #   (which is *not* validated by the service; in this case, Transfer will\n    #   accept but ignore the others parameters if given, leading to user\n    #   confusion if we don't do this validation check)\n    custom_network_use_params = (\n        \"max_concurrency\",\n        \"preferred_concurrency\",\n        \"max_parallelism\",\n        \"preferred_parallelism\",\n    )\n    if params[\"network_use\"] != \"custom\":\n        for option in custom_network_use_params:\n            if params[option] is not None:\n                raise click.UsageError(\n                    \"The {} options require you use --network-use=custom.\".format(\n                        \"/\".join(\n                            \"--\" + option.replace(\"_\", \"-\")\n                            for option in custom_network_use_params\n                        )\n                    )\n                )\n\n    # make sure --(no-)managed and --subscription-id are mutually exclusive\n    # if --managed given pass DEFAULT as the subscription_id\n    # if --no-managed given, pass None\n    managed_flag = params.get(\"managed\")\n    if managed_flag is not None:\n        params.pop(\"managed\")\n        if managed_flag:\n            params[\"subscription_id\"] = params.get(\"subscription_id\") or \"DEFAULT\"\n        else:\n            if params.get(\"subscription_id\"):\n                raise click.UsageError(\n                    \"Cannot specify --subscription-id and \"\n                    \"use the --no-managed option.\"\n                )\n            params[\"subscription_id\"] = EXPLICIT_NULL\n\n    # make sure --no-default-directory are mutually exclusive\n    # if --no-managed given, pass an EXPLICIT_NULL as the default directory\n    if params.get(\"no_default_directory\"):\n\n        if params.get(\"default_directory\"):\n            raise click.UsageError(\n                \"--no-default-directory and --default-directory are mutually \"\n                \"exclusive.\"\n            )\n        else:\n            params[\"default_directory\"] = EXPLICIT_NULL\n            params.pop(\"no_default_directory\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef task_id_arg(*args, **kwargs):\n\n    def inner_decorator(f, required=True):\n        f = click.argument(\"TASK_ID\", required=required)(f)\n        return f\n\n    return detect_and_decorate(inner_decorator, args, kwargs)", "response": "Decorator to specify the TASK_ID argument for a Transfer Task."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef task_submission_options(f):\n\n    def notify_opt_callback(ctx, param, value):\n        \"\"\"\n        Parse --notify\n        - \"\" is the same as \"off\"\n        - parse by lowercase, comma-split, strip spaces\n        - \"off,x\" is invalid for any x\n        - \"on,x\" is valid for any valid x (other than \"off\")\n        - \"failed\", \"succeeded\", \"inactive\" are normal vals\n\n        In code, produces True, False, or a set\n        \"\"\"\n        # if no value was set, don't set any explicit options\n        # the API default is \"everything on\"\n        if value is None:\n            return {}\n\n        value = value.lower()\n        value = [x.strip() for x in value.split(\",\")]\n        # [\"\"] is what you'll get if value is \"\" to start with\n        # special-case it into \"off\", which helps avoid surprising scripts\n        # which take a notification settings as inputs and build --notify\n        if value == [\"\"]:\n            value = [\"off\"]\n\n        off = \"off\" in value\n        on = \"on\" in value\n        # set-ize it -- duplicates are fine\n        vals = set([x for x in value if x not in (\"off\", \"on\")])\n\n        if (vals or on) and off:\n            raise click.UsageError('--notify cannot accept \"off\" and another value')\n\n        allowed_vals = set((\"on\", \"succeeded\", \"failed\", \"inactive\"))\n        if not vals <= allowed_vals:\n            raise click.UsageError(\n                \"--notify received at least one invalid value among {}\".format(\n                    list(vals)\n                )\n            )\n\n        # return the notification options to send!\n        # on means don't set anything (default)\n        if on:\n            return {}\n        # off means turn off everything\n        if off:\n            return {\n                \"notify_on_succeeded\": False,\n                \"notify_on_failed\": False,\n                \"notify_on_inactive\": False,\n            }\n        # otherwise, return the exact set of values seen\n        else:\n            return {\n                \"notify_on_succeeded\": \"succeeded\" in vals,\n                \"notify_on_failed\": \"failed\" in vals,\n                \"notify_on_inactive\": \"inactive\" in vals,\n            }\n\n    f = click.option(\n        \"--dry-run\",\n        is_flag=True,\n        help=(\"Don't actually submit the task, print submission \" \"data instead\"),\n    )(f)\n    f = click.option(\n        \"--notify\",\n        callback=notify_opt_callback,\n        help=(\n            \"Comma separated list of task events which notify by email. \"\n            \"'on' and 'off' may be used to enable or disable notifications \"\n            \"for all event types. Otherwise, use 'succeeded', 'failed', or \"\n            \"'inactive'\"\n        ),\n    )(f)\n    f = click.option(\n        \"--submission-id\",\n        help=(\n            \"Task submission ID, as generated by `globus task \"\n            \"generate-submission-id`. Used for safe resubmission in the \"\n            \"presence of network failures.\"\n        ),\n    )(f)\n    f = click.option(\"--label\", default=None, help=\"Set a label for this task.\")(f)\n    f = click.option(\n        \"--deadline\",\n        default=None,\n        type=ISOTimeType(),\n        help=\"Set a deadline for this to be canceled if not completed by.\",\n    )(f)\n    f = click.option(\n        \"--skip-activation-check\",\n        is_flag=True,\n        help=(\"Submit the task even if the endpoint(s) \" \"aren't currently activated.\"),\n    )(f)\n\n    return f", "response": "This function returns a list of options shared by both transfer and delete tasks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_and_rm_options(*args, **kwargs):\n\n    def inner_decorator(f, supports_batch=True, default_enable_globs=False):\n        f = click.option(\n            \"--recursive\", \"-r\", is_flag=True, help=\"Recursively delete dirs\"\n        )(f)\n        f = click.option(\n            \"--ignore-missing\",\n            \"-f\",\n            is_flag=True,\n            help=\"Don't throw errors if the file or dir is absent\",\n        )(f)\n        f = click.option(\n            \"--star-silent\",\n            \"--unsafe\",\n            \"star_silent\",\n            is_flag=True,\n            help=(\n                'Don\\'t prompt when the trailing character is a \"*\".'\n                + (\" Implicit in --batch\" if supports_batch else \"\")\n            ),\n        )(f)\n        f = click.option(\n            \"--enable-globs/--no-enable-globs\",\n            is_flag=True,\n            default=default_enable_globs,\n            show_default=True,\n            help=(\n                \"Enable expansion of *, ?, and [ ] characters in the last \"\n                \"component of file paths, unless they are escaped with \"\n                \"a preceeding backslash, \\\\\"\n            ),\n        )(f)\n        if supports_batch:\n            f = click.option(\n                \"--batch\",\n                is_flag=True,\n                help=(\n                    \"Accept a batch of paths on stdin (i.e. run in \"\n                    \"batchmode). Uses ENDPOINT_ID as passed on the \"\n                    \"commandline. Any commandline PATH given will be used \"\n                    \"as a prefix to all paths given\"\n                ),\n            )(f)\n        return f\n\n    return detect_and_decorate(inner_decorator, args, kwargs)", "response": "Decorator to add globus delete and globus rm options to a single globus archive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsharing collection of options for `globus transfer endpoint server add` and `globus transfer endpoint server update`. Accepts a toggle to know if it's being used as `add` or `update`. usage: >>> @server_add_and_update_opts >>> def command_func(subject, port, scheme, hostname): >>> ... or >>> @server_add_and_update_opts(add=True) >>> def command_func(subject, port, scheme, hostname): >>> ...", "response": "def server_add_and_update_opts(*args, **kwargs):\n    \"\"\"\n    shared collection of options for `globus transfer endpoint server add` and\n    `globus transfer endpoint server update`.\n    Accepts a toggle to know if it's being used as `add` or `update`.\n\n    usage:\n\n    >>> @server_add_and_update_opts\n    >>> def command_func(subject, port, scheme, hostname):\n    >>>     ...\n\n    or\n\n    >>> @server_add_and_update_opts(add=True)\n    >>> def command_func(subject, port, scheme, hostname):\n    >>>     ...\n    \"\"\"\n\n    def port_range_callback(ctx, param, value):\n        if not value:\n            return None\n\n        value = value.lower().strip()\n        if value == \"unspecified\":\n            return None, None\n        if value == \"unrestricted\":\n            return 1024, 65535\n\n        try:\n            lower, upper = map(int, value.split(\"-\"))\n        except ValueError:  # too many/few values from split or non-integer(s)\n            raise click.BadParameter(\n                \"must specify as 'unspecified', \"\n                \"'unrestricted', or as range separated \"\n                \"by a hyphen (e.g. '50000-51000')\"\n            )\n        if not 1024 <= lower <= 65535 or not 1024 <= upper <= 65535:\n            raise click.BadParameter(\"must be within the 1024-65535 range\")\n\n        return (lower, upper) if lower <= upper else (upper, lower)\n\n    def inner_decorator(f, add=False):\n        f = click.option(\"--hostname\", required=add, help=\"Server Hostname.\")(f)\n\n        default_scheme = \"gsiftp\" if add else None\n        f = click.option(\n            \"--scheme\",\n            help=\"Scheme for the Server.\",\n            type=CaseInsensitiveChoice((\"gsiftp\", \"ftp\")),\n            default=default_scheme,\n            show_default=add,\n        )(f)\n\n        default_port = 2811 if add else None\n        f = click.option(\n            \"--port\",\n            help=\"Port for Globus control channel connections.\",\n            type=int,\n            default=default_port,\n            show_default=add,\n        )(f)\n\n        f = click.option(\n            \"--subject\",\n            help=(\n                \"Subject of the X509 Certificate of the server. When \"\n                \"unspecified, the CN must match the server hostname.\"\n            ),\n        )(f)\n\n        for adjective, our_preposition, their_preposition in [\n            (\"incoming\", \"to\", \"from\"),\n            (\"outgoing\", \"from\", \"to\"),\n        ]:\n            f = click.option(\n                \"--{}-data-ports\".format(adjective),\n                callback=port_range_callback,\n                help=\"Indicate to firewall administrators at other sites how to \"\n                \"allow {} traffic {} this server {} their own. Specify as \"\n                \"either 'unspecified', 'unrestricted', or as range of \"\n                \"ports separated by a hyphen (e.g. '50000-51000') within \"\n                \"the 1024-65535 range.\".format(\n                    adjective, our_preposition, their_preposition\n                ),\n            )(f)\n\n        return f\n\n    return detect_and_decorate(inner_decorator, args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_versions():\n    # import in the func (rather than top-level scope) so that at setup time,\n    # `requests` isn't required -- otherwise, setuptools will fail to run\n    # because requests isn't installed yet.\n    import requests\n\n    try:\n        version_data = requests.get(\n            \"https://pypi.python.org/pypi/globus-cli/json\"\n        ).json()\n        latest = max(LooseVersion(v) for v in version_data[\"releases\"])\n        return latest, LooseVersion(__version__)\n    # if the fetch from pypi fails\n    except requests.RequestException:\n        return None, LooseVersion(__version__)", "response": "Get the latest version of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _lookup_identity_names(self):\n        id_batch_size = 100\n\n        # fetch in batches of 100, store in a dict\n        ac = get_auth_client()\n        self._resolved_map = {}\n        for i in range(0, len(self.identity_ids), id_batch_size):\n            chunk = self.identity_ids[i : i + id_batch_size]\n            resolved_result = ac.get_identities(ids=chunk)\n            for x in resolved_result[\"identities\"]:\n                self._resolved_map[x[\"id\"]] = x[\"username\"]", "response": "Resolve identities to usernames."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_option(option, value, section=\"cli\", system=False):\n    # deny rwx to Group and World -- don't bother storing the returned old mask\n    # value, since we'll never restore it in the CLI anyway\n    # do this on every call to ensure that we're always consistent about it\n    os.umask(0o077)\n\n    # FIXME: DRY violation with config_commands.helpers\n    conf = get_config_obj(system=system)\n\n    # add the section if absent\n    if section not in conf:\n        conf[section] = {}\n\n    conf[section][option] = value\n    conf.write()", "response": "Write an option to disk"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef internal_auth_client(requires_instance=False, force_new_client=False):\n    client_id = lookup_option(CLIENT_ID_OPTNAME)\n    client_secret = lookup_option(CLIENT_SECRET_OPTNAME)\n    template_id = lookup_option(TEMPLATE_ID_OPTNAME) or DEFAULT_TEMPLATE_ID\n    template_client = internal_native_client()\n    existing = client_id and client_secret\n\n    # if we are forcing a new client, delete any existing client\n    if force_new_client and existing:\n        existing_client = globus_sdk.ConfidentialAppAuthClient(client_id, client_secret)\n        try:\n            existing_client.delete(\"/v2/api/clients/{}\".format(client_id))\n\n        # if the client secret has been invalidated or the client has\n        # already been removed, we continue on\n        except globus_sdk.exc.AuthAPIError:\n            pass\n\n    # if we require a new client to be made\n    if force_new_client or (requires_instance and not existing):\n        # register a new instance client with auth\n        body = {\"client\": {\"template_id\": template_id, \"name\": \"Globus CLI\"}}\n        res = template_client.post(\"/v2/api/clients\", json_body=body)\n\n        # get values and write to config\n        credential_data = res[\"included\"][\"client_credential\"]\n        client_id = credential_data[\"client\"]\n        client_secret = credential_data[\"secret\"]\n        write_option(CLIENT_ID_OPTNAME, client_id)\n        write_option(CLIENT_SECRET_OPTNAME, client_secret)\n\n        return globus_sdk.ConfidentialAppAuthClient(\n            client_id, client_secret, app_name=\"Globus CLI\"\n        )\n\n    # if we already have a client, just return it\n    elif existing:\n        return globus_sdk.ConfidentialAppAuthClient(\n            client_id, client_secret, app_name=\"Globus CLI\"\n        )\n\n    # fall-back to a native client to not break old logins\n    # TOOD: eventually remove this behavior\n    else:\n        return template_client", "response": "Returns a ConfidentialAppAuthClient object for the current CLI s Instance Client."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexits with the status mapped by what we were given.", "response": "def exit_with_mapped_status(http_status):\n    \"\"\"\n    Given an HTTP Status, exit with either an error status of 1 or the\n    status mapped by what we were given.\n    \"\"\"\n    # get the mapping by looking up the state and getting the mapping attr\n    mapping = click.get_current_context().ensure_object(CommandState).http_status_map\n\n    # if there is a mapped exit code, exit with that. Otherwise, exit 1\n    if http_status in mapping:\n        sys.exit(mapping[http_status])\n    else:\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef session_hook(exception):\n    safeprint(\n        \"The resource you are trying to access requires you to \"\n        \"re-authenticate with specific identities.\"\n    )\n\n    params = exception.raw_json[\"authorization_parameters\"]\n    message = params.get(\"session_message\")\n    if message:\n        safeprint(\"message: {}\".format(message))\n\n    identities = params.get(\"session_required_identities\")\n    if identities:\n        id_str = \" \".join(identities)\n        safeprint(\n            \"Please run\\n\\n\"\n            \"    globus session update {}\\n\\n\"\n            \"to re-authenticate with the required identities\".format(id_str)\n        )\n    else:\n        safeprint(\n            'Please use \"globus session update\" to re-authenticate '\n            \"with specific identities\".format(id_str)\n        )\n\n    exit_with_mapped_status(exception.http_status)", "response": "Handles globus session update"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef custom_except_hook(exc_info):\n    exception_type, exception, traceback = exc_info\n\n    # check if we're in debug mode, and run the real excepthook if we are\n    ctx = click.get_current_context()\n    state = ctx.ensure_object(CommandState)\n    if state.debug:\n        sys.excepthook(exception_type, exception, traceback)\n\n    # we're not in debug mode, do custom handling\n    else:\n\n        # if it's a click exception, re-raise as original -- Click's main\n        # execution context will handle pretty-printing\n        if isinstance(exception, click.ClickException):\n            reraise(exception_type, exception, traceback)\n\n        # catch any session errors to give helpful instructions\n        # on how to use globus session update\n        elif (\n            isinstance(exception, exc.GlobusAPIError)\n            and exception.raw_json\n            and \"authorization_parameters\" in exception.raw_json\n        ):\n            session_hook(exception)\n\n        # handle the Globus-raised errors with our special hooks\n        # these will present the output (on stderr) as JSON\n        elif isinstance(exception, exc.TransferAPIError):\n            if exception.code == \"ClientError.AuthenticationFailed\":\n                authentication_hook(exception)\n            else:\n                transferapi_hook(exception)\n\n        elif isinstance(exception, exc.AuthAPIError):\n            if exception.code == \"UNAUTHORIZED\":\n                authentication_hook(exception)\n            # invalid_grant occurs when the users refresh tokens are not valid\n            elif exception.message == \"invalid_grant\":\n                invalidrefresh_hook(exception)\n            else:\n                authapi_hook(exception)\n\n        elif isinstance(exception, exc.GlobusAPIError):\n            globusapi_hook(exception)\n\n        # specific checks fell through -- now check if it's any kind of\n        # GlobusError\n        elif isinstance(exception, exc.GlobusError):\n            globus_generic_hook(exception)\n\n        # not a GlobusError, not a ClickException -- something like ValueError\n        # or NotImplementedError bubbled all the way up here: just print it\n        # out, basically\n        else:\n            safeprint(u\"{}: {}\".format(exception_type.__name__, exception))\n            sys.exit(1)", "response": "Custom excepthook to present python errors produced by the CLI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ls_command(\n    endpoint_plus_path,\n    recursive_depth_limit,\n    recursive,\n    long_output,\n    show_hidden,\n    filter_val,\n):\n    \"\"\"\n    Executor for `globus ls`\n    \"\"\"\n    endpoint_id, path = endpoint_plus_path\n\n    # do autoactivation before the `ls` call so that recursive invocations\n    # won't do this repeatedly, and won't have to instantiate new clients\n    client = get_client()\n    autoactivate(client, endpoint_id, if_expires_in=60)\n\n    # create the query paramaters to send to operation_ls\n    ls_params = {\"show_hidden\": int(show_hidden)}\n    if path:\n        ls_params[\"path\"] = path\n    if filter_val:\n        # this char has special meaning in the LS API's filter clause\n        # can't be part of the pattern (but we don't support globbing across\n        # dir structures anyway)\n        if \"/\" in filter_val:\n            raise click.UsageError('--filter cannot contain \"/\"')\n        # format into a simple filter clause which operates on filenames\n        ls_params[\"filter\"] = \"name:{}\".format(filter_val)\n\n    # get the `ls` result\n    if recursive:\n        # NOTE:\n        # --recursive and --filter have an interplay that some users may find\n        # surprising\n        # if we're asked to change or \"improve\" the behavior in the future, we\n        # could do so with \"type:dir\" or \"type:file\" filters added in, and\n        # potentially work out some viable behavior based on what people want\n        res = client.recursive_operation_ls(\n            endpoint_id, depth=recursive_depth_limit, **ls_params\n        )\n    else:\n        res = client.operation_ls(endpoint_id, **ls_params)\n\n    def cleaned_item_name(item):\n        return item[\"name\"] + (\"/\" if item[\"type\"] == \"dir\" else \"\")\n\n    # and then print it, per formatting rules\n    formatted_print(\n        res,\n        fields=[\n            (\"Permissions\", \"permissions\"),\n            (\"User\", \"user\"),\n            (\"Group\", \"group\"),\n            (\"Size\", \"size\"),\n            (\"Last Modified\", \"last_modified\"),\n            (\"File Type\", \"type\"),\n            (\"Filename\", cleaned_item_name),\n        ],\n        simple_text=(\n            None\n            if long_output or is_verbose() or not outformat_is_text()\n            else \"\\n\".join(cleaned_item_name(x) for x in res)\n        ),\n        json_converter=iterable_response_to_dict,\n    )", "response": "Executor for globus ls"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef iterable_func(self):\n        # BFS is not done until the queue is empty\n        while self.queue:\n            logger.debug(\n                (\n                    \"recursive_operation_ls BFS queue not empty, \"\n                    \"getting next path now.\"\n                )\n            )\n\n            # rate limit based on number of ls calls we have made\n            self.ls_count += 1\n            if self.ls_count % SLEEP_FREQUENCY == 0:\n                logger.debug(\n                    (\n                        \"recursive_operation_ls sleeping {} seconds to \"\n                        \"rate limit itself.\".format(SLEEP_LEN)\n                    )\n                )\n                time.sleep(SLEEP_LEN)\n\n            # get path and current depth from the queue\n            abs_path, rel_path, depth = self.queue.pop()\n\n            # set the target path to the popped absolute path if it exists\n            if abs_path:\n                self.ls_params[\"path\"] = abs_path\n\n            # if filter_after_first is False, stop filtering after the first\n            # ls call has been made\n            if not self.filter_after_first:\n                if self.filtering:\n                    self.filtering = False\n                else:\n                    try:\n                        self.ls_params.pop(\"filter\")\n                    except KeyError:\n                        pass\n\n            # do the operation_ls with the updated params\n            res = self.client.operation_ls(self.endpoint_id, **self.ls_params)\n            res_data = res[\"DATA\"]\n\n            # if we aren't at the depth limit, add dir entries to the queue.\n            # including the dir's name in the absolute and relative paths\n            # and increase the depth by one.\n            # data is reversed to maintain any \"orderby\" ordering\n            if depth < self.max_depth:\n                self.queue.extend(\n                    [\n                        (\n                            res[\"path\"] + item[\"name\"],\n                            (rel_path + \"/\" if rel_path else \"\") + item[\"name\"],\n                            depth + 1,\n                        )\n                        for item in reversed(res_data)\n                        if item[\"type\"] == \"dir\"\n                    ]\n                )\n\n            # for each item in the response data update the item's name with\n            # the relative path popped from the queue, and yield the item\n            for item in res_data:\n                item[\"name\"] = (rel_path + \"/\" if rel_path else \"\") + item[\"name\"]\n                yield GlobusResponse(item)", "response": "This is the internal function which is used by the recursive_operation_ls method. It is called by the recursive_operation_ls method in order to get the next item from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive the activation requirements for an endpoint and a filename for a file for a Delegate Proxy activation requirements returns the activation requirements data with the proxy chain filled in.", "response": "def fill_delegate_proxy_activation_requirements(\n    requirements_data, cred_file, lifetime_hours=12\n):\n    \"\"\"\n    Given the activation requirements for an endpoint and a filename for\n    X.509 credentials, extracts the public key from the activation\n    requirements, uses the key and the credentials to make a proxy credential,\n    and returns the requirements data with the proxy chain filled in.\n    \"\"\"\n    # get the public key from the activation requirements\n    for data in requirements_data[\"DATA\"]:\n        if data[\"type\"] == \"delegate_proxy\" and data[\"name\"] == \"public_key\":\n            public_key = data[\"value\"]\n            break\n    else:\n        raise ValueError(\n            (\n                \"No public_key found in activation requirements, this endpoint \"\n                \"does not support Delegate Proxy activation.\"\n            )\n        )\n\n    # get user credentials from user credential file\"\n    with open(cred_file) as f:\n        issuer_cred = f.read()\n\n    # create the proxy credentials\n    proxy = create_proxy_credentials(issuer_cred, public_key, lifetime_hours)\n\n    # return the activation requirements document with the proxy_chain filled\n    for data in requirements_data[\"DATA\"]:\n        if data[\"type\"] == \"delegate_proxy\" and data[\"name\"] == \"proxy_chain\":\n            data[\"value\"] = proxy\n            return requirements_data\n    else:\n        raise ValueError(\n            (\n                \"No proxy_chain found in activation requirements, this endpoint \"\n                \"does not support Delegate Proxy activation.\"\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_proxy_credentials(issuer_cred, public_key, lifetime_hours):\n    # parse the issuer credential\n    loaded_cert, loaded_private_key, issuer_chain = parse_issuer_cred(issuer_cred)\n\n    # load the public_key into a cryptography object\n    loaded_public_key = serialization.load_pem_public_key(\n        public_key.encode(\"ascii\"), backend=default_backend()\n    )\n\n    # check that the issuer certificate is not an old proxy\n    # and is using the keyUsage section as required\n    confirm_not_old_proxy(loaded_cert)\n    validate_key_usage(loaded_cert)\n\n    # create the proxy cert cryptography object\n    new_cert = create_proxy_cert(\n        loaded_cert, loaded_private_key, loaded_public_key, lifetime_hours\n    )\n\n    # extend the proxy chain as a unicode string\n    extended_chain = loaded_cert.public_bytes(serialization.Encoding.PEM).decode(\n        \"ascii\"\n    ) + six.u(issuer_chain)\n\n    # return in PEM format as a unicode string\n    return (\n        new_cert.public_bytes(serialization.Encoding.PEM).decode(\"ascii\")\n        + extended_chain\n    )", "response": "Create a proxy credentials file from an activation requirements document and a public key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a PEM file containing a list of issuer credentials and returns the list of cryptography objects and the proxy chain of the certificate and private key.", "response": "def parse_issuer_cred(issuer_cred):\n    \"\"\"\n    Given an X509 PEM file in the form of a string, parses it into sections\n    by the PEM delimiters of: -----BEGIN <label>----- and -----END <label>----\n    Confirms the sections can be decoded in the proxy credential order of:\n    issuer cert, issuer private key, proxy chain of 0 or more certs .\n    Returns the issuer cert and private key as loaded cryptography objects\n    and the proxy chain as a potentially empty string.\n    \"\"\"\n    # get each section of the PEM file\n    sections = re.findall(\n        \"-----BEGIN.*?-----.*?-----END.*?-----\", issuer_cred, flags=re.DOTALL\n    )\n    try:\n        issuer_cert = sections[0]\n        issuer_private_key = sections[1]\n        issuer_chain_certs = sections[2:]\n    except IndexError:\n        raise ValueError(\n            \"Unable to parse PEM data in credentials, \"\n            \"make sure the X.509 file is in PEM format and \"\n            \"consists of the issuer cert, issuer private key, \"\n            \"and proxy chain (if any) in that order.\"\n        )\n\n    # then validate that each section of data can be decoded as expected\n    try:\n        loaded_cert = x509.load_pem_x509_certificate(\n            six.b(issuer_cert), default_backend()\n        )\n        loaded_private_key = serialization.load_pem_private_key(\n            six.b(issuer_private_key), password=None, backend=default_backend()\n        )\n        for chain_cert in issuer_chain_certs:\n            x509.load_pem_x509_certificate(six.b(chain_cert), default_backend())\n        issuer_chain = \"\".join(issuer_chain_certs)\n    except ValueError:\n        raise ValueError(\n            \"Failed to decode PEM data in credentials. Make sure \"\n            \"the X.509 file consists of the issuer cert, \"\n            \"issuer private key, and proxy chain (if any) \"\n            \"in that order.\"\n        )\n\n    # return loaded cryptography objects and the issuer chain\n    return loaded_cert, loaded_private_key, issuer_chain"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_proxy_cert(\n    loaded_cert, loaded_private_key, loaded_public_key, lifetime_hours\n):\n    \"\"\"\n    Given cryptography objects for an issuing certificate, a public_key,\n    a private_key, and an int for lifetime in hours, creates a proxy\n    cert from the issuer and public key signed by the private key.\n    \"\"\"\n    builder = x509.CertificateBuilder()\n\n    # create a serial number for the new proxy\n    # Under RFC 3820 there are many ways to generate the serial number. However\n    # making the number unpredictable has security benefits, e.g. it can make\n    # this style of attack more difficult:\n    # http://www.win.tue.nl/hashclash/rogue-ca\n    serial = struct.unpack(\"<Q\", os.urandom(8))[0]\n    builder = builder.serial_number(serial)\n\n    # set the new proxy as valid from now until lifetime_hours have passed\n    builder = builder.not_valid_before(datetime.datetime.utcnow())\n    builder = builder.not_valid_after(\n        datetime.datetime.utcnow() + datetime.timedelta(hours=lifetime_hours)\n    )\n\n    # set the public key of the new proxy to the given public key\n    builder = builder.public_key(loaded_public_key)\n\n    # set the issuer of the new cert to the subject of the issuing cert\n    builder = builder.issuer_name(loaded_cert.subject)\n\n    # set the new proxy's subject\n    # append a CommonName to the new proxy's subject\n    # with the serial as the value of the CN\n    new_atribute = x509.NameAttribute(x509.oid.NameOID.COMMON_NAME, six.u(str(serial)))\n    subject_attributes = list(loaded_cert.subject)\n    subject_attributes.append(new_atribute)\n    builder = builder.subject_name(x509.Name(subject_attributes))\n\n    # add proxyCertInfo extension to the new proxy (We opt not to add keyUsage)\n    # For RFC proxies the effective usage is defined as the intersection\n    # of the usage of each cert in the chain. See section 4.2 of RFC 3820.\n\n    # the constants 'oid' and 'value' are gotten from\n    # examining output from a call to the open ssl function:\n    # X509V3_EXT_conf(NULL, ctx, name, value)\n    # ctx set by X509V3_set_nconf(&ctx, NCONF_new(NULL))\n    # name = \"proxyCertInfo\"\n    # value = \"critical,language:Inherit all\"\n    oid = x509.ObjectIdentifier(\"1.3.6.1.5.5.7.1.14\")\n    value = b\"0\\x0c0\\n\\x06\\x08+\\x06\\x01\\x05\\x05\\x07\\x15\\x01\"\n    extension = x509.extensions.UnrecognizedExtension(oid, value)\n    builder = builder.add_extension(extension, critical=True)\n\n    # sign the new proxy with the issuer's private key\n    new_certificate = builder.sign(\n        private_key=loaded_private_key,\n        algorithm=hashes.SHA256(),\n        backend=default_backend(),\n    )\n\n    # return the new proxy as a cryptography object\n    return new_certificate", "response": "Creates a proxyCert from the given cryptography objects for an issuing certificate a public key a private key and an int for lifetime in hours."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef confirm_not_old_proxy(loaded_cert):\n    # Examine the last CommonName to see if it looks like an old proxy.\n    last_cn = loaded_cert.subject.get_attributes_for_oid(x509.oid.NameOID.COMMON_NAME)[\n        -1\n    ]\n    # if the last CN is 'proxy' or 'limited proxy' we are in an old proxy\n    if last_cn.value in (\"proxy\", \"limited proxy\"):\n        raise ValueError(\n            \"Proxy certificate is in an outdated format \" \"that is no longer supported\"\n        )", "response": "Checks if the certificate is an old proxy and raise an error if so."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the keyUsage extension is being used and that the digital signature bit has been asserted.", "response": "def validate_key_usage(loaded_cert):\n    \"\"\"\n    Given a cryptography object for the issuer cert, checks that if\n    the keyUsage extension is being used that the digital signature\n    bit has been asserted. (As specified in RFC 3820 section 3.1.)\n    \"\"\"\n    try:\n        key_usage = loaded_cert.extensions.get_extension_for_oid(\n            x509.oid.ExtensionOID.KEY_USAGE\n        )\n        if not key_usage.value.digital_signature:\n            raise ValueError(\n                \"Certificate is using the keyUsage extension, but has \"\n                \"not asserted the Digital Signature bit.\"\n            )\n    except x509.ExtensionNotFound:  # keyUsage extension not used\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self):\n        self.timer = t.Thread(target=self.report_spans)\n        self.timer.daemon = True\n        self.timer.name = \"Instana Span Reporting\"\n        self.timer.start()", "response": "Start the thread that reports queued spans"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef report_spans(self):\n        logger.debug(\"Span reporting thread is now alive\")\n\n        def span_work():\n            queue_size = self.queue.qsize()\n            if queue_size > 0 and instana.singletons.agent.can_send():\n                response = instana.singletons.agent.report_traces(self.queued_spans())\n                if response:\n                    logger.debug(\"reported %d spans\" % queue_size)\n            return True\n\n        every(2, span_work, \"Span Reporting\")", "response": "Periodically report the queued spans"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget all of the spans in the queue", "response": "def queued_spans(self):\n        \"\"\" Get all of the spans in the queue \"\"\"\n        spans = []\n        while True:\n            try:\n                s = self.queue.get(False)\n            except queue.Empty:\n                break\n            else:\n                spans.append(s)\n        return spans"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef record_span(self, span):\n        if instana.singletons.agent.can_send() or \"INSTANA_TEST\" in os.environ:\n            json_span = None\n\n            if span.operation_name in self.registered_spans:\n                json_span = self.build_registered_span(span)\n            else:\n                json_span = self.build_sdk_span(span)\n\n            self.queue.put(json_span)", "response": "Convert the passed BasicSpan into a JsonSpan and add it to the span queue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking a BasicSpan and converts it into a registered JsonSpan.", "response": "def build_registered_span(self, span):\n        \"\"\" Takes a BasicSpan and converts it into a registered JsonSpan \"\"\"\n        data = Data(baggage=span.context.baggage)\n\n        kind = 1 # entry\n        if span.operation_name in self.exit_spans:\n            kind = 2 # exit\n        # log is a special case as it is not entry nor exit\n        if span.operation_name == \"log\":\n            kind = 3 # intermediate span\n\n        logs = self.collect_logs(span)\n        if len(logs) > 0:\n            if data.custom is None:\n                data.custom = CustomData()\n            data.custom.logs = logs\n\n        if span.operation_name in self.http_spans:\n            data.http = HttpData(host=span.tags.pop(\"http.host\", None),\n                                 url=span.tags.pop(ext.HTTP_URL, None),\n                                 params=span.tags.pop('http.params', None),\n                                 method=span.tags.pop(ext.HTTP_METHOD, None),\n                                 status=span.tags.pop(ext.HTTP_STATUS_CODE, None),\n                                 path_tpl=span.tags.pop(\"http.path_tpl\", None),\n                                 error=span.tags.pop('http.error', None))\n\n        if span.operation_name == \"rabbitmq\":\n            data.rabbitmq = RabbitmqData(exchange=span.tags.pop('exchange', None),\n                                         queue=span.tags.pop('queue', None),\n                                         sort=span.tags.pop('sort', None),\n                                         address=span.tags.pop('address', None),\n                                         key=span.tags.pop('key', None))\n            if data.rabbitmq.sort == 'consume':\n                kind = 1 # entry\n\n        if span.operation_name == \"redis\":\n            data.redis = RedisData(connection=span.tags.pop('connection', None),\n                                   driver=span.tags.pop('driver', None),\n                                   command=span.tags.pop('command', None),\n                                   error=span.tags.pop('redis.error', None),\n                                   subCommands=span.tags.pop('subCommands', None))\n\n        if span.operation_name == \"rpc-client\" or span.operation_name == \"rpc-server\":\n            data.rpc = RPCData(flavor=span.tags.pop('rpc.flavor', None),\n                               host=span.tags.pop('rpc.host', None),\n                               port=span.tags.pop('rpc.port', None),\n                               call=span.tags.pop('rpc.call', None),\n                               call_type=span.tags.pop('rpc.call_type', None),\n                               params=span.tags.pop('rpc.params', None),\n                               baggage=span.tags.pop('rpc.baggage', None),\n                               error=span.tags.pop('rpc.error', None))\n\n        if span.operation_name == \"sqlalchemy\":\n            data.sqlalchemy = SQLAlchemyData(sql=span.tags.pop('sqlalchemy.sql', None),\n                                             eng=span.tags.pop('sqlalchemy.eng', None),\n                                             url=span.tags.pop('sqlalchemy.url', None),\n                                             err=span.tags.pop('sqlalchemy.err', None))\n\n        if span.operation_name == \"soap\":\n            data.soap = SoapData(action=span.tags.pop('soap.action', None))\n\n        if span.operation_name == \"mysql\":\n            data.mysql = MySQLData(host=span.tags.pop('host', None),\n                                   db=span.tags.pop(ext.DATABASE_INSTANCE, None),\n                                   user=span.tags.pop(ext.DATABASE_USER, None),\n                                   stmt=span.tags.pop(ext.DATABASE_STATEMENT, None))\n            if (data.custom is not None) and (data.custom.logs is not None) and len(data.custom.logs):\n                tskey = list(data.custom.logs.keys())[0]\n                data.mysql.error = data.custom.logs[tskey]['message']\n\n        if span.operation_name == \"log\":\n            data.log = {}\n            # use last special key values\n            # TODO - logic might need a tweak here\n            for l in span.logs:\n                if \"message\" in l.key_values:\n                    data.log[\"message\"] = l.key_values.pop(\"message\", None)\n                if \"parameters\" in l.key_values:\n                    data.log[\"parameters\"] = l.key_values.pop(\"parameters\", None)\n\n        entity_from = {'e': instana.singletons.agent.from_.pid,\n                      'h': instana.singletons.agent.from_.agentUuid}\n\n        json_span = JsonSpan(n=span.operation_name,\n                             k=kind,\n                             t=span.context.trace_id,\n                             p=span.parent_id,\n                             s=span.context.span_id,\n                             ts=int(round(span.start_time * 1000)),\n                             d=int(round(span.duration * 1000)),\n                             f=entity_from,\n                             data=data)\n\n        if span.stack:\n            json_span.stack = span.stack\n\n        error = span.tags.pop(\"error\", False)\n        ec = span.tags.pop(\"ec\", None)\n\n        if error and ec:\n            json_span.error = error\n            json_span.ec = ec\n\n        if len(span.tags) > 0:\n            if data.custom is None:\n                data.custom = CustomData()\n            data.custom.tags = span.tags\n\n        return json_span"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_sdk_span(self, span):\n\n        custom_data = CustomData(tags=span.tags,\n                                 logs=self.collect_logs(span))\n\n        sdk_data = SDKData(name=span.operation_name,\n                           custom=custom_data,\n                           Type=self.get_span_kind_as_string(span))\n\n        if \"arguments\" in span.tags:\n            sdk_data.arguments = span.tags[\"arguments\"]\n\n        if \"return\" in span.tags:\n            sdk_data.Return = span.tags[\"return\"]\n\n        data = Data(service=instana.singletons.agent.sensor.options.service_name, sdk=sdk_data)\n        entity_from = {'e': instana.singletons.agent.from_.pid,\n                      'h': instana.singletons.agent.from_.agentUuid}\n\n        json_span = JsonSpan(\n                             t=span.context.trace_id,\n                             p=span.parent_id,\n                             s=span.context.span_id,\n                             ts=int(round(span.start_time * 1000)),\n                             d=int(round(span.duration * 1000)),\n                             k=self.get_span_kind_as_int(span),\n                             n=\"sdk\",\n                             f=entity_from,\n                             data=data)\n\n        error = span.tags.pop(\"error\", False)\n        ec = span.tags.pop(\"ec\", None)\n\n        if error and ec:\n            json_span.error = error\n            json_span.ec = ec\n\n        return json_span", "response": "Takes a BasicSpan and converts into an SDK type JsonSpan"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the string value for the Instana backend or None if the span. kind tag is not set.", "response": "def get_span_kind_as_string(self, span):\n        \"\"\"\n            Will retrieve the `span.kind` tag and return the appropriate string value for the Instana backend or\n            None if the tag is set to something we don't recognize.\n\n        :param span: The span to search for the `span.kind` tag\n        :return: String\n        \"\"\"\n        kind = None\n        if \"span.kind\" in span.tags:\n            if span.tags[\"span.kind\"] in self.entry_kind:\n                kind = \"entry\"\n            elif span.tags[\"span.kind\"] in self.exit_kind:\n                kind = \"exit\"\n            else:\n                kind = \"intermediate\"\n        return kind"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_span_kind_as_int(self, span):\n        kind = None\n        if \"span.kind\" in span.tags:\n            if span.tags[\"span.kind\"] in self.entry_kind:\n                kind = 1\n            elif span.tags[\"span.kind\"] in self.exit_kind:\n                kind = 2\n            else:\n                kind = 3\n        return kind", "response": "Returns the span. kind tag and the appropriate integer value for Instana backend or None if the tag is not set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef __get_real_pid(self):\n        pid = None\n\n        if os.path.exists(\"/proc/\"):\n            sched_file = \"/proc/%d/sched\" % os.getpid()\n\n            if os.path.isfile(sched_file):\n                try:\n                    file = open(sched_file)\n                    line = file.readline()\n                    g = re.search(r'\\((\\d+),', line)\n                    if len(g.groups()) == 1:\n                        pid = int(g.groups()[0])\n                except Exception:\n                    logger.debug(\"parsing sched file failed\", exc_info=True)\n                    pass\n\n        if pid is None:\n            pid = os.getpid()\n\n        return pid", "response": "Attempts to determine the real process ID by querying the sched file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the host and port of the Instana agent.", "response": "def __get_agent_host_port(self):\n        \"\"\"\n        Iterates the the various ways the host and port of the Instana host\n        agent may be configured: default, env vars, sensor options...\n        \"\"\"\n        host = AGENT_DEFAULT_HOST\n        port = AGENT_DEFAULT_PORT\n\n        if \"INSTANA_AGENT_HOST\" in os.environ:\n            host = os.environ[\"INSTANA_AGENT_HOST\"]\n            if \"INSTANA_AGENT_PORT\" in os.environ:\n                port = int(os.environ[\"INSTANA_AGENT_PORT\"])\n\n        elif \"INSTANA_AGENT_IP\" in os.environ:\n            # Deprecated: INSTANA_AGENT_IP environment variable\n            # To be removed in a future version\n            host = os.environ[\"INSTANA_AGENT_IP\"]\n            if \"INSTANA_AGENT_PORT\" in os.environ:\n                port = int(os.environ[\"INSTANA_AGENT_PORT\"])\n\n        elif self.agent.sensor.options.agent_host != \"\":\n            host = self.agent.sensor.options.agent_host\n            if self.agent.sensor.options.agent_port != 0:\n                port = self.agent.sensor.options.agent_port\n\n        return host, port"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self):\n        self.thr = threading.Thread(target=self.collect_and_report)\n        self.thr.daemon = True\n        self.thr.name = \"Instana Metric Collection\"\n        self.thr.start()", "response": "Spawns the metric reporting thread"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the state of the current process to new", "response": "def reset(self):\n        \"\"\"\" Reset the state as new \"\"\"\n        self.last_usage = None\n        self.last_collect = None\n        self.last_metrics = None\n        self.snapshot_countdown = 0\n        self.run()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_and_report(self):\n        logger.debug(\"Metric reporting thread is now alive\")\n\n        def metric_work():\n            self.process()\n            if self.agent.is_timed_out():\n                logger.warn(\"Host agent offline for >1 min.  Going to sit in a corner...\")\n                self.agent.reset()\n                return False\n            return True\n\n        every(1, metric_work, \"Metrics Collection\")", "response": "This function is a simple loop to collect entity data and report entity data every 1 second."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self):\n        if self.agent.machine.fsm.current is \"wait4init\":\n            # Test the host agent if we're ready to send data\n            if self.agent.is_agent_ready():\n                self.agent.machine.fsm.ready()\n            else:\n                return\n\n        if self.agent.can_send():\n            self.snapshot_countdown = self.snapshot_countdown - 1\n            ss = None\n            cm = self.collect_metrics()\n\n            if self.snapshot_countdown < 1:\n                logger.debug(\"Sending process snapshot data\")\n                self.snapshot_countdown = self.SNAPSHOT_PERIOD\n                ss = self.collect_snapshot()\n                md = copy.deepcopy(cm).delta_data(None)\n            else:\n                md = copy.deepcopy(cm).delta_data(self.last_metrics)\n\n            ed = EntityData(pid=self.agent.from_.pid, snapshot=ss, metrics=md)\n            response = self.agent.report_data(ed)\n\n            if response:\n                if response.status_code is 200 and len(response.content) > 2:\n                    # The host agent returned something indicating that is has a request for us that we\n                    # need to process.\n                    self.handle_agent_tasks(json.loads(response.content)[0])\n\n                self.last_metrics = cm.__dict__", "response": "Collects processes and reports metrics"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef collect_snapshot(self):\n        try:\n            if \"INSTANA_SERVICE_NAME\" in os.environ:\n                appname = os.environ[\"INSTANA_SERVICE_NAME\"]\n            elif \"FLASK_APP\" in os.environ:\n                appname = os.environ[\"FLASK_APP\"]\n            elif \"DJANGO_SETTINGS_MODULE\" in os.environ:\n                appname = os.environ[\"DJANGO_SETTINGS_MODULE\"].split('.')[0]\n            elif os.path.basename(sys.argv[0]) == '' and sys.stdout.isatty():\n                appname = \"Interactive Console\"\n            else:\n                if os.path.basename(sys.argv[0]) == '':\n                    appname = os.path.basename(sys.executable)\n                else:\n                    appname = os.path.basename(sys.argv[0])\n\n            s = Snapshot(name=appname, version=platform.version(),\n                         f=platform.python_implementation(),\n                         a=platform.architecture()[0],\n                         djmw=self.djmw)\n            s.version = sys.version\n            s.versions = self.collect_modules()\n        except Exception as e:\n            logger.debug(e.message)\n        else:\n            return s", "response": "Collects snapshot related information to this process and environment"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncollects up the list of modules in use", "response": "def collect_modules(self):\n        \"\"\" Collect up the list of modules in use \"\"\"\n        try:\n            res = {}\n            m = sys.modules\n            for k in m:\n                # Don't report submodules (e.g. django.x, django.y, django.z)\n                # Skip modules that begin with underscore\n                if ('.' in k) or k[0] == '_':\n                    continue\n                if m[k]:\n                    try:\n                        d = m[k].__dict__\n                        if \"version\" in d and d[\"version\"]:\n                            res[k] = self.jsonable(d[\"version\"])\n                        elif \"__version__\" in d and d[\"__version__\"]:\n                            res[k] = self.jsonable(d[\"__version__\"])\n                        else:\n                            res[k] = get_distribution(k).version\n                    except DistributionNotFound:\n                        pass\n                    except Exception:\n                        logger.debug(\"collect_modules: could not process module: %s\" % k)\n\n        except Exception:\n            logger.debug(\"collect_modules\", exc_info=True)\n        else:\n            return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncollect up and return various metrics", "response": "def collect_metrics(self):\n        \"\"\" Collect up and return various metrics \"\"\"\n        try:\n            g = None\n            u = resource.getrusage(resource.RUSAGE_SELF)\n            if gc_.isenabled():\n                c = list(gc_.get_count())\n                th = list(gc_.get_threshold())\n                g = GC(collect0=c[0] if not self.last_collect else c[0] - self.last_collect[0],\n                       collect1=c[1] if not self.last_collect else c[\n                           1] - self.last_collect[1],\n                       collect2=c[2] if not self.last_collect else c[\n                           2] - self.last_collect[2],\n                       threshold0=th[0],\n                       threshold1=th[1],\n                       threshold2=th[2])\n\n            thr = threading.enumerate()\n            daemon_threads = [tr.daemon is True for tr in thr].count(True)\n            alive_threads = [tr.daemon is False for tr in thr].count(True)\n            dummy_threads = [type(tr) is threading._DummyThread for tr in thr].count(True)\n\n            m = Metrics(ru_utime=u[0] if not self.last_usage else u[0] - self.last_usage[0],\n                        ru_stime=u[1] if not self.last_usage else u[1] - self.last_usage[1],\n                        ru_maxrss=u[2],\n                        ru_ixrss=u[3],\n                        ru_idrss=u[4],\n                        ru_isrss=u[5],\n                        ru_minflt=u[6] if not self.last_usage else u[6] - self.last_usage[6],\n                        ru_majflt=u[7] if not self.last_usage else u[7] - self.last_usage[7],\n                        ru_nswap=u[8] if not self.last_usage else u[8] - self.last_usage[8],\n                        ru_inblock=u[9] if not self.last_usage else u[9] - self.last_usage[9],\n                        ru_oublock=u[10] if not self.last_usage else u[10] - self.last_usage[10],\n                        ru_msgsnd=u[11] if not self.last_usage else u[11] - self.last_usage[11],\n                        ru_msgrcv=u[12] if not self.last_usage else u[12] - self.last_usage[12],\n                        ru_nsignals=u[13] if not self.last_usage else u[13] - self.last_usage[13],\n                        ru_nvcs=u[14] if not self.last_usage else u[14] - self.last_usage[14],\n                        ru_nivcsw=u[15] if not self.last_usage else u[15] - self.last_usage[15],\n                        alive_threads=alive_threads,\n                        dummy_threads=dummy_threads,\n                        daemon_threads=daemon_threads,\n                        gc=g)\n\n            self.last_usage = u\n            if gc_.isenabled():\n                self.last_collect = c\n\n            return m\n        except:\n            logger.debug(\"collect_metrics\", exc_info=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self, e):\n        logger.debug(\"Spawning metric & trace reporting threads\")\n        self.sensor.meter.run()\n        instana.singletons.tracer.recorder.run()", "response": "Starts the agent and required threads"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_fork(self):\n        self.reset()\n        self.sensor.handle_fork()\n        instana.singletons.tracer.handle_fork()", "response": "This method is called when a fork is happening."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_agent_listening(self, host, port):\n        try:\n            rv = False\n            url = \"http://%s:%s/\" % (host, port)\n            response = self.client.get(url, timeout=0.8)\n\n            server_header = response.headers[\"Server\"]\n            if server_header == AGENT_HEADER:\n                logger.debug(\"Host agent found on %s:%d\" % (host, port))\n                rv = True\n            else:\n                logger.debug(\"...something is listening on %s:%d but it's not the Instana Agent: %s\"\n                             % (host, port, server_header))\n        except (requests.ConnectTimeout, requests.ConnectionError):\n            logger.debug(\"No host agent listening on %s:%d\" % (host, port))\n            rv = False\n        finally:\n            return rv", "response": "Check if the Instana Agent is listening on the given host and port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef announce(self, discovery):\n        try:\n            url = self.__discovery_url()\n            logger.debug(\"making announce request to %s\" % (url))\n            response = None\n            response = self.client.put(url,\n                                       data=self.to_json(discovery),\n                                       headers={\"Content-Type\": \"application/json\"},\n                                       timeout=0.8)\n\n            if response.status_code is 200:\n                self.last_seen = datetime.now()\n        except (requests.ConnectTimeout, requests.ConnectionError):\n            logger.debug(\"announce\", exc_info=True)\n        finally:\n            return response", "response": "Attempt to announce to the host agent."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the agent is ready to accept data.", "response": "def is_agent_ready(self):\n        \"\"\"\n        Used after making a successful announce to test when the agent is ready to accept data.\n        \"\"\"\n        try:\n            response = self.client.head(self.__data_url(), timeout=0.8)\n\n            if response.status_code is 200:\n                return True\n            return False\n        except (requests.ConnectTimeout, requests.ConnectionError):\n            logger.debug(\"is_agent_ready: host agent connection error\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef report_data(self, entity_data):\n        try:\n            response = None\n            response = self.client.post(self.__data_url(),\n                                        data=self.to_json(entity_data),\n                                        headers={\"Content-Type\": \"application/json\"},\n                                        timeout=0.8)\n\n            # logger.warn(\"report_data: response.status_code is %s\" % response.status_code)\n\n            if response.status_code is 200:\n                self.last_seen = datetime.now()\n        except (requests.ConnectTimeout, requests.ConnectionError):\n            logger.debug(\"report_data: host agent connection error\")\n        finally:\n            return response", "response": "Report entity data to the host agent."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreporting the given spans to the host agent.", "response": "def report_traces(self, spans):\n        \"\"\"\n        Used to report entity data (metrics & snapshot) to the host agent.\n        \"\"\"\n        try:\n            response = None\n            response = self.client.post(self.__traces_url(),\n                                        data=self.to_json(spans),\n                                        headers={\"Content-Type\": \"application/json\"},\n                                        timeout=0.8)\n\n            # logger.warn(\"report_traces: response.status_code is %s\" % response.status_code)\n\n            if response.status_code is 200:\n                self.last_seen = datetime.now()\n        except (requests.ConnectTimeout, requests.ConnectionError):\n            logger.debug(\"report_traces: host agent connection error\")\n        finally:\n            return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the discovery URL for the agent.", "response": "def __discovery_url(self):\n        \"\"\"\n        URL for announcing to the host agent\n        \"\"\"\n        port = self.sensor.options.agent_port\n        if port == 0:\n            port = AGENT_DEFAULT_PORT\n\n        return \"http://%s:%s/%s\" % (self.host, port, AGENT_DISCOVERY_PATH)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __data_url(self):\n        path = AGENT_DATA_PATH % self.from_.pid\n        return \"http://%s:%s/%s\" % (self.host, self.port, path)", "response": "URL for posting metrics to the host agent. Only valid when announced."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __traces_url(self):\n        path = AGENT_TRACES_PATH % self.from_.pid\n        return \"http://%s:%s/%s\" % (self.host, self.port, path)", "response": "URL for posting traces to the host agent. Only valid when announced."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __response_url(self, message_id):\n        if self.from_.pid != 0:\n            path = AGENT_RESPONSE_PATH % (self.from_.pid, message_id)\n\n        return \"http://%s:%s/%s\" % (self.host, self.port, path)", "response": "Return the URL for responding to the given message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_id():\n    global _current_pid\n\n    pid = os.getpid()\n    if _current_pid != pid:\n        _current_pid = pid\n        _rnd.seed(int(1000000 * time.time()) ^ pid)\n    id = format(_rnd.randint(0, 18446744073709551615), '02x')\n\n    if len(id) < 16:\n        id = id.zfill(16)\n\n    return id", "response": "Generate a 64bit base 16 ID for use as a Span or Trace ID"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef header_to_id(header):\n    if not isinstance(header, string_types):\n        return BAD_ID\n\n    try:\n        # Test that header is truly a hexadecimal value before we try to convert\n        int(header, 16)\n\n        length = len(header)\n        if length < 16:\n            # Left pad ID with zeros\n            header = header.zfill(16)\n        elif length > 16:\n            # Phase 0: Discard everything but the last 16byte\n            header = header[-16:]\n\n        return header\n    except ValueError:\n        return BAD_ID", "response": "Convert a header to a valid ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert obj to json. Used mostly to convert the classes in json_span. py until we switch to nested dicts.", "response": "def to_json(obj):\n    \"\"\"\n    Convert obj to json.  Used mostly to convert the classes in json_span.py until we switch to nested\n    dicts (or something better)\n\n    :param obj: the object to serialize to json\n    :return:  json string\n    \"\"\"\n    try:\n        return json.dumps(obj, default=lambda obj: {k.lower(): v for k, v in obj.__dict__.items()},\n                          sort_keys=False, separators=(',', ':')).encode()\n    except Exception as e:\n        logger.info(\"to_json: \", e, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef package_version():\n    version = \"\"\n    try:\n        version = pkg_resources.get_distribution('instana').version\n    except pkg_resources.DistributionNotFound:\n        version = 'unknown'\n    finally:\n        return version", "response": "Determine the version of this package."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_default_gateway():\n    try:\n        # The first line is the header line\n        # We look for the line where the Destination is 00000000 - that is the default route\n        # The Gateway IP is encoded backwards in hex.\n        with open(\"/proc/self/net/route\") as routes:\n            for line in routes:\n                parts = line.split('\\t')\n                if '00000000' == parts[1]:\n                    hip = parts[2]\n\n        if hip is not None and len(hip) is 8:\n            # Reverse order, convert hex to int\n            return \"%i.%i.%i.%i\" % (int(hip[6:8], 16), int(hip[4:6], 16), int(hip[2:4], 16), int(hip[0:2], 16))\n\n    except:\n        logger.warn(\"get_default_gateway: \", exc_info=True)", "response": "Reads the default gateway in use."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_py_source(file):\n    try:\n        response = None\n        pysource = \"\"\n\n        if regexp_py.search(file) is None:\n            response = {\"error\": \"Only Python source files are allowed. (*.py)\"}\n        else:\n            with open(file, 'r') as pyfile:\n                pysource = pyfile.read()\n\n            response = {\"data\": pysource}\n\n    except Exception as e:\n        response = {\"error\": str(e)}\n    finally:\n        return response", "response": "Retrieves and returns the source code for any Python file requested by the UI via the host agent."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef every(delay, task, name):\n    next_time = time.time() + delay\n\n    while True:\n        time.sleep(max(0, next_time - time.time()))\n        try:\n            if task() is False:\n                break\n        except Exception:\n            logger.debug(\"Problem while executing repetitive task: %s\" % name, exc_info=True)\n\n        # skip tasks if we are behind schedule:\n        next_time += (time.time() - next_time) // delay * delay + delay", "response": "Executes a task every delay seconds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive an app return that app wrapped in iWSGIMiddleware", "response": "def make_middleware(app=None, *args, **kw):\n    \"\"\" Given an app, return that app wrapped in iWSGIMiddleware \"\"\"\n    app = iWSGIMiddleware(app, *args, **kw)\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an EUM snippet for use in views templates and layouts that reports the client side metrics to Instana that will automagically be linked to the current trace.", "response": "def eum_snippet(trace_id=None, eum_api_key=None, meta={}):\n    \"\"\"\n    Return an EUM snippet for use in views, templates and layouts that reports\n    client side metrics to Instana that will automagically be linked to the\n    current trace.\n\n    @param trace_id [optional] the trace ID to insert into the EUM string\n    @param eum_api_key [optional] the EUM API key from your Instana dashboard\n    @param meta [optional] optional additional KVs you want reported with the\n                EUM metrics\n\n    @return string\n    \"\"\"\n    try:\n        eum_file = open(os.path.dirname(__file__) + '/eum.js')\n        eum_src = Template(eum_file.read())\n\n        # Prepare the standard required IDs\n        ids = {}\n        ids['meta_kvs'] = ''\n\n        parent_span = tracer.active_span\n\n        if trace_id or parent_span:\n            ids['trace_id'] = trace_id or parent_span.trace_id\n        else:\n            # No trace_id passed in and tracer doesn't show an active span so\n            # return nothing, nada & zip.\n            return ''\n\n        if eum_api_key:\n            ids['eum_api_key'] = eum_api_key\n        else:\n            ids['eum_api_key'] = global_eum_api_key\n\n        # Process passed in EUM 'meta' key/values\n        for key, value in meta.items():\n            ids['meta_kvs'] += (\"'ineum('meta', '%s', '%s');'\" % (key, value))\n\n        return eum_src.substitute(ids)\n    except Exception as e:\n        logger.debug(e)\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking from BasicTracer so we can override generate_id calls to ours", "response": "def start_span(self,\n                   operation_name=None,\n                   child_of=None,\n                   references=None,\n                   tags=None,\n                   start_time=None,\n                   ignore_active_span=False):\n        \"Taken from BasicTracer so we can override generate_id calls to ours\"\n\n        start_time = time.time() if start_time is None else start_time\n\n        # See if we have a parent_ctx in `references`\n        parent_ctx = None\n        if child_of is not None:\n            parent_ctx = (\n                child_of if isinstance(child_of, ot.SpanContext)\n                else child_of.context)\n        elif references is not None and len(references) > 0:\n            # TODO only the first reference is currently used\n            parent_ctx = references[0].referenced_context\n\n        # retrieve the active SpanContext\n        if not ignore_active_span and parent_ctx is None:\n            scope = self.scope_manager.active\n            if scope is not None:\n                parent_ctx = scope.span.context\n\n        # Assemble the child ctx\n        gid = generate_id()\n        ctx = SpanContext(span_id=gid)\n        if parent_ctx is not None:\n            if parent_ctx._baggage is not None:\n                ctx._baggage = parent_ctx._baggage.copy()\n            ctx.trace_id = parent_ctx.trace_id\n            ctx.sampled = parent_ctx.sampled\n        else:\n            ctx.trace_id = gid\n            ctx.sampled = self.sampler.sampled(ctx.trace_id)\n\n        # Tie it all together\n        span = InstanaSpan(self,\n                           operation_name=operation_name,\n                           context=ctx,\n                           parent_id=(None if parent_ctx is None else parent_ctx.span_id),\n                           tags=tags,\n                           start_time=start_time)\n\n        if operation_name in self.recorder.exit_spans:\n            self.__add_stack(span)\n\n        elif operation_name in self.recorder.entry_spans:\n            # For entry spans, add only a backtrace fingerprint\n            self.__add_stack(span, limit=2)\n\n        return span"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding a backtrace to this span.", "response": "def __add_stack(self, span, limit=None):\n        \"\"\" Adds a backtrace to this span \"\"\"\n        span.stack = []\n        frame_count = 0\n\n        tb = traceback.extract_stack()\n        tb.reverse()\n        for frame in tb:\n            if limit is not None and frame_count >= limit:\n                break\n\n            # Exclude Instana frames unless we're in dev mode\n            if \"INSTANA_DEV\" not in os.environ:\n                if re_tracer_frame.search(frame[0]) is not None:\n                    continue\n\n                if re_with_stan_frame.search(frame[2]) is not None:\n                    continue\n\n            span.stack.append({\n                \"c\": frame[0],\n                \"n\": frame[1],\n                \"m\": frame[2]\n            })\n\n            if limit is not None:\n                frame_count += 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hook(module):\n    if \"INSTANA_DEV\" in os.environ:\n        print(\"==============================================================\")\n        print(\"Instana: Running flask hook\")\n        print(\"==============================================================\")\n    wrapt.wrap_function_wrapper('flask', 'Flask.__init__', wrapper)", "response": "This method is called by the Instana module when it is loaded."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the appropriate Error subclass for the given HTTP status code.", "response": "def error_class_for_http_status(status):\n    \"\"\"Return the appropriate `ResponseError` subclass for the given\n    HTTP status code.\"\"\"\n    try:\n        return error_classes[status]\n    except KeyError:\n        def new_status_error(xml_response):\n            if (status > 400 and status < 500):\n                return UnexpectedClientError(status, xml_response)\n            if (status > 500 and status < 600):\n                return UnexpectedServerError(status, xml_response)\n            return UnexpectedStatusError(status, xml_response)\n        return new_status_error"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef response_doc(self):\n        try:\n            return self.__dict__['response_doc']\n        except KeyError:\n            self.__dict__['response_doc'] = ElementTree.fromstring(\n                self.response_xml\n            )\n            return self.__dict__['response_doc']", "response": "The XML document received from the service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a push notification with the given body XML.", "response": "def objects_for_push_notification(notification):\n    \"\"\"Decode a push notification with the given body XML.\n\n    Returns a dictionary containing the constituent objects of the push\n    notification. The kind of push notification is given in the ``\"type\"``\n    member of the returned dictionary.\n\n    \"\"\"\n    notification_el = ElementTree.fromstring(notification)\n    objects = {'type': notification_el.tag}\n    for child_el in notification_el:\n        tag = child_el.tag\n        res = Resource.value_for_element(child_el)\n        objects[tag] = res\n    return objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef charge(self, charge):\n        url = urljoin(self._url, '/adjustments')\n        return charge.post(url)", "response": "Charge this account with the given Adjustment."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an invoice for any outstanding adjustments this account has.", "response": "def invoice(self, **kwargs):\n        \"\"\"Create an invoice for any outstanding adjustments this account has.\"\"\"\n        url = urljoin(self._url, '/invoices')\n\n        if kwargs:\n            response = self.http_request(url, 'POST', Invoice(**kwargs), {'Content-Type':\n                'application/xml; charset=utf-8'})\n        else:\n            response = self.http_request(url, 'POST')\n\n        if response.status != 201:\n            self.raise_http_error(response)\n\n        response_xml = response.read()\n        logging.getLogger('recurly.http.response').debug(response_xml)\n        elem = ElementTree.fromstring(response_xml)\n\n        invoice_collection = InvoiceCollection.from_element(elem)\n        return invoice_collection"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the given Subscription for this existing account.", "response": "def subscribe(self, subscription):\n        \"\"\"Create the given `Subscription` for this existing account.\"\"\"\n        url = urljoin(self._url, '/subscriptions')\n        return subscription.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges this account s billing information to the given BillingInfo object.", "response": "def update_billing_info(self, billing_info):\n        \"\"\"Change this account's billing information to the given `BillingInfo`.\"\"\"\n        url = urljoin(self._url, '/billing_info')\n        response = billing_info.http_request(url, 'PUT', billing_info,\n            {'Content-Type': 'application/xml; charset=utf-8'})\n        if response.status == 200:\n            pass\n        elif response.status == 201:\n            billing_info._url = response.getheader('Location')\n        else:\n            billing_info.raise_http_error(response)\n\n        response_xml = response.read()\n        logging.getLogger('recurly.http.response').debug(response_xml)\n        billing_info.update_from_element(ElementTree.fromstring(response_xml))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a shipping address on an existing account.", "response": "def create_shipping_address(self, shipping_address):\n        \"\"\"Creates a shipping address on an existing account. If you are\n        creating an account, you can embed the shipping addresses with the\n        request\"\"\"\n        url = urljoin(self._url, '/shipping_addresses')\n        return shipping_address.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npreview the purchase of this gift card", "response": "def preview(self):\n        \"\"\"Preview the purchase of this gift card\"\"\"\n\n        if hasattr(self, '_url'):\n            url = self._url + '/preview'\n            return self.post(url)\n        else:\n            url = urljoin(recurly.base_uri(), self.collection_path + '/preview')\n            return self.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef redeem(self, account_code):\n\n        redemption_path = '%s/redeem' % (self.redemption_code)\n\n        if hasattr(self, '_url'):\n            url = urljoin(self._url, '/redeem')\n        else:\n            url = urljoin(recurly.base_uri(), self.collection_path + '/' + redemption_path)\n\n        recipient_account = _RecipientAccount(account_code=account_code)\n        return self.post(url, recipient_account)", "response": "Redeem this gift card on the specified account code"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a PDF of the invoice identified by the UUID", "response": "def pdf(cls, uuid):\n        \"\"\"Return a PDF of the invoice identified by the UUID\n\n        This is a raw string, which can be written to a file with:\n        `\n            with open('invoice.pdf', 'w') as invoice_file:\n                invoice_file.write(recurly.Invoice.pdf(uuid))\n        `\n\n        \"\"\"\n        url = urljoin(base_uri(), cls.member_path % (uuid,))\n        pdf_response = cls.http_request(url, headers={'Accept': 'application/pdf'})\n        return pdf_response.read()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenter an offline payment on the invoice.", "response": "def enter_offline_payment(self, transaction):\n        \"\"\"\n        Records an offline (external) payment on the invoice.\n        Pass in a Transaction object to set the details of the created\n        transaction. The attributes available to set are\n        (payment_method, collected_at, amount_in_cents, description)\n\n        Returns:\n            Transaction: The created transaction\n        \"\"\"\n        url = urljoin(self._url, '/transactions')\n        transaction.post(url)\n        return transaction"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the notes on the subscription without generating a change", "response": "def update_notes(self, **kwargs):\n        \"\"\"\n        Updates the notes on the subscription without generating a change\n        This endpoint also allows you to update custom fields:\n\n            `\n                sub.custom_fields[0].value = 'A new value'\n                sub.update_notes()\n            `\n        \"\"\"\n        for key, val in iteritems(kwargs):\n            setattr(self, key, val)\n        url = urljoin(self._url, '/notes')\n        self.put(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_usage(self, sub_add_on, usage):\n        url = urljoin(self._url, '/add_ons/%s/usage' % (sub_add_on.add_on_code,))\n        return usage.post(url)", "response": "Record the usage on the given subscription add on and update the usage object with returned xml"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the refund transaction for this transaction immediately after refunding.", "response": "def get_refund_transaction(self):\n        \"\"\"Retrieve the refund transaction for this transaction, immediately\n        after refunding.\n\n        After calling `refund()` to refund a transaction, call this method to\n        retrieve the new transaction representing the refund.\n\n        \"\"\"\n        try:\n            url = self._refund_transaction_url\n        except AttributeError:\n            raise ValueError(\"No refund transaction is available for this transaction\")\n\n        resp, elem = self.element_for_url(url)\n        value = self.value_for_element(elem)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrefunds this transaction. Calling this method returns the refunded transaction (that is, ``self``) if the refund was successful, or raises a `ResponseError` if an error occurred requesting the refund. After a successful call to `refund()`, to retrieve the new transaction representing the refund, use the `get_refund_transaction()` method.", "response": "def refund(self, **kwargs):\n        \"\"\"Refund this transaction.\n\n        Calling this method returns the refunded transaction (that is,\n        ``self``) if the refund was successful, or raises a `ResponseError` if\n        an error occurred requesting the refund. After a successful call to\n        `refund()`, to retrieve the new transaction representing the refund,\n        use the `get_refund_transaction()` method.\n\n        \"\"\"\n        # Find the URL and method to refund the transaction.\n        try:\n            selfnode = self._elem\n        except AttributeError:\n            raise AttributeError('refund')\n        url, method = None, None\n        for anchor_elem in selfnode.findall('a'):\n            if anchor_elem.attrib.get('name') == 'refund':\n                url = anchor_elem.attrib['href']\n                method = anchor_elem.attrib['method'].upper()\n        if url is None or method is None:\n            raise AttributeError(\"refund\")  # should do something more specific probably\n\n        actionator = self._make_actionator(url, method, extra_handler=self._handle_refund_accepted)\n        return actionator(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_add_on(self, add_on_code):\n        url = urljoin(self._url, '/add_ons/%s' % (add_on_code,))\n        resp, elem = AddOn.element_for_url(url)\n        return AddOn.from_element(elem)", "response": "Return the AddOn for this plan with the given add - on code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_add_on(self, add_on):\n        url = urljoin(self._url, '/add_ons')\n        return add_on.post(url)", "response": "Make the given AddOn available to subscribers on this plan."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading estimates from disk instead of processing", "response": "def _load_track_estimates(track, estimates_dir, output_dir):\n    \"\"\"load estimates from disk instead of processing\"\"\"\n    user_results = {}\n    track_estimate_dir = os.path.join(\n        estimates_dir,\n        track.subset,\n        track.name\n    )\n    for target in glob.glob(\n        track_estimate_dir + '/*.wav'\n    ):\n\n        target_name = op.splitext(\n            os.path.basename(target)\n        )[0]\n        try:\n            target_audio, _ = sf.read(\n                target,\n                always_2d=True\n            )\n            user_results[target_name] = target_audio\n        except RuntimeError:\n            pass\n\n    if user_results:\n        eval_mus_track(\n            track,\n            user_results,\n            output_dir=output_dir\n        )\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute bss_eval metrics for two given directories assuming file names are identical for both reference sources and estimates.", "response": "def eval_dir(\n    reference_dir,\n    estimates_dir,\n    output_dir=None,\n    mode='v4',\n    win=1.0,\n    hop=1.0,\n):\n    \"\"\"Compute bss_eval metrics for two given directories assuming file\n    names are identical for both, reference source and estimates.\n\n    Parameters\n    ----------\n    reference_dir : str\n        path to reference sources directory.\n    estimates_dir : str\n        path to estimates directory.\n    output_dir : str\n        path to output directory used to save evaluation results. Defaults to\n        `None`, meaning no evaluation files will be saved.\n    mode : str\n        bsseval version number. Defaults to 'v4'.\n    win : int\n        window size in\n\n    Returns\n    -------\n    scores : EvalStore\n        scores object that holds the framewise and global evaluation scores.\n    \"\"\"\n\n    reference = []\n    estimates = []\n\n    data = EvalStore(win=win, hop=hop)\n\n    global_rate = None\n    reference_glob = os.path.join(reference_dir, '*.wav')\n    # Load in each reference file in the supplied dir\n    for reference_file in glob.glob(reference_glob):\n        ref_audio, rate = sf.read(\n            reference_file,\n            always_2d=True\n        )\n        # Make sure fs is the same for all files\n        assert (global_rate is None or rate == global_rate)\n        global_rate = rate\n        reference.append(ref_audio)\n\n    if not reference:\n        raise ValueError('`reference_dir` contains no wav files')\n\n    estimated_glob = os.path.join(estimates_dir, '*.wav')\n    targets = []\n    for estimated_file in glob.glob(estimated_glob):\n        targets.append(os.path.basename(estimated_file))\n        ref_audio, rate = sf.read(\n            estimated_file,\n            always_2d=True\n        )\n        assert (global_rate is None or rate == global_rate)\n        global_rate = rate\n        estimates.append(ref_audio)\n\n    SDR, ISR, SIR, SAR = evaluate(\n        reference,\n        estimates,\n        win=int(win*global_rate),\n        hop=int(hop*global_rate),\n        mode=mode\n    )\n    for i, target in enumerate(targets):\n        values = {\n            \"SDR\": SDR[i].tolist(),\n            \"SIR\": SIR[i].tolist(),\n            \"ISR\": ISR[i].tolist(),\n            \"SAR\": SAR[i].tolist()\n        }\n\n        data.add_target(\n            target_name=target,\n            values=values\n        )\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning musdb. run for the purpose of evaluation of musdb estimate dir dataset", "response": "def eval_mus_dir(\n    dataset,\n    estimates_dir,\n    output_dir=None,\n    *args, **kwargs\n):\n    \"\"\"Run musdb.run for the purpose of evaluation of musdb estimate dir\n\n    Parameters\n    ----------\n    dataset : DB(object)\n        Musdb Database object.\n    estimates_dir : str\n        Path to estimates folder.\n    output_dir : str\n        Output folder where evaluation json files are stored.\n    *args\n        Variable length argument list for `musdb.run()`.\n    **kwargs\n        Arbitrary keyword arguments for `musdb.run()`.\n    \"\"\"\n    # create a new musdb instance for estimates with the same file structure\n    est = musdb.DB(root_dir=estimates_dir, is_wav=True)\n    # load all estimates track_names\n    est_tracks = est.load_mus_tracks()\n    # get a list of track names\n    tracknames = [t.name for t in est_tracks]\n    # load only musdb tracks where we have estimate tracks\n    tracks = dataset.load_mus_tracks(tracknames=tracknames)\n    # wrap the estimate loader\n    run_fun = functools.partial(\n        _load_track_estimates,\n        estimates_dir=estimates_dir,\n        output_dir=output_dir\n    )\n    # evaluate tracks\n    dataset.run(run_fun, estimates_dir=None, tracks=tracks, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing all bss_eval metrics for a given musdb track and estimated signals and save the result in output_dir.", "response": "def eval_mus_track(\n    track,\n    user_estimates,\n    output_dir=None,\n    mode='v4',\n    win=1.0,\n    hop=1.0\n):\n    \"\"\"Compute all bss_eval metrics for the musdb track and estimated signals,\n    given by a `user_estimates` dict.\n\n    Parameters\n    ----------\n    track : Track\n        musdb track object loaded using musdb\n    estimated_sources : Dict\n        dictionary, containing the user estimates as np.arrays.\n    output_dir : str\n        path to output directory used to save evaluation results. Defaults to\n        `None`, meaning no evaluation files will be saved.\n    mode : str\n        bsseval version number. Defaults to 'v4'.\n    win : int\n        window size in\n\n    Returns\n    -------\n    scores : EvalStore\n        scores object that holds the framewise and global evaluation scores.\n    \"\"\"\n\n    audio_estimates = []\n    audio_reference = []\n\n    # make sure to always build the list in the same order\n    # therefore track.targets is an OrderedDict\n    eval_targets = []  # save the list of target names to be evaluated\n    for key, target in list(track.targets.items()):\n        try:\n            # try to fetch the audio from the user_results of a given key\n            user_estimates[key]\n        except KeyError:\n            # ignore wrong key and continue\n            continue\n\n        # append this target name to the list of target to evaluate\n        eval_targets.append(key)\n\n    data = EvalStore(win=win, hop=hop)\n\n    # check if vocals and accompaniment is among the targets\n    has_acc = all(x in eval_targets for x in ['vocals', 'accompaniment'])\n    if has_acc:\n        # remove accompaniment from list of targets, because\n        # the voc/acc scenario will be evaluated separately\n        eval_targets.remove('accompaniment')\n\n    if len(eval_targets) >= 2:\n        # compute evaluation of remaining targets\n        for target in eval_targets:\n            audio_estimates.append(user_estimates[target])\n            audio_reference.append(track.targets[target].audio)\n\n        SDR, ISR, SIR, SAR = evaluate(\n            audio_reference,\n            audio_estimates,\n            win=int(win*track.rate),\n            hop=int(hop*track.rate),\n            mode=mode\n        )\n\n        # iterate over all evaluation results except for vocals\n        for i, target in enumerate(eval_targets):\n            if target == 'vocals' and has_acc:\n                continue\n\n            values = {\n                \"SDR\": SDR[i].tolist(),\n                \"SIR\": SIR[i].tolist(),\n                \"ISR\": ISR[i].tolist(),\n                \"SAR\": SAR[i].tolist()\n            }\n\n            data.add_target(\n                target_name=target,\n                values=values\n            )\n\n    # add vocal accompaniment targets later\n    if has_acc:\n        # add vocals and accompaniments as a separate scenario\n        eval_targets = ['vocals', 'accompaniment']\n\n        audio_estimates = []\n        audio_reference = []\n\n        for target in eval_targets:\n            audio_estimates.append(user_estimates[target])\n            audio_reference.append(track.targets[target].audio)\n\n        SDR, ISR, SIR, SAR = evaluate(\n            audio_reference,\n            audio_estimates,\n            win=int(win*track.rate),\n            hop=int(hop*track.rate),\n            mode=mode\n        )\n\n        # iterate over all targets\n        for i, target in enumerate(eval_targets):\n            values = {\n                \"SDR\": SDR[i].tolist(),\n                \"SIR\": SIR[i].tolist(),\n                \"ISR\": ISR[i].tolist(),\n                \"SAR\": SAR[i].tolist()\n            }\n\n            data.add_target(\n                target_name=target,\n                values=values\n            )\n\n    if output_dir:\n        # validate against the schema\n        data.validate()\n\n        try:\n            subset_path = op.join(\n                output_dir,\n                track.subset\n            )\n\n            if not op.exists(subset_path):\n                os.makedirs(subset_path)\n\n            with open(\n                op.join(subset_path, track.name) + '.json', 'w+'\n            ) as f:\n                f.write(data.json)\n\n        except (IOError):\n            pass\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npads or truncate estimates by duration of references", "response": "def pad_or_truncate(\n    audio_reference,\n    audio_estimates\n):\n    \"\"\"Pad or truncate estimates by duration of references:\n    - If reference > estimates: add zeros at the and of the estimated signal\n    - If estimates > references: truncate estimates to duration of references\n\n    Parameters\n    ----------\n    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing true reference sources\n    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing estimated sources\n    Returns\n    -------\n    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing true reference sources\n    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing estimated sources\n    \"\"\"\n    est_shape = audio_estimates.shape\n    ref_shape = audio_reference.shape\n    if est_shape[1] != ref_shape[1]:\n        if est_shape[1] >= ref_shape[1]:\n            audio_estimates = audio_estimates[:, :ref_shape[1], :]\n        else:\n            # pad end with zeros\n            audio_estimates = np.pad(\n                audio_estimates,\n                [\n                    (0, 0),\n                    (0, ref_shape[1] - est_shape[1]),\n                    (0, 0)\n                ],\n                mode='constant'\n            )\n\n    return audio_reference, audio_estimates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef evaluate(\n    references,\n    estimates,\n    win=1*44100,\n    hop=1*44100,\n    mode='v4',\n    padding=True\n):\n    \"\"\"BSS_EVAL images evaluation using metrics module\n\n    Parameters\n    ----------\n    references : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing true reference sources\n    estimates : np.ndarray, shape=(nsrc, nsampl, nchan)\n        array containing estimated sources\n    window : int, defaults to 44100\n        window size in samples\n    hop : int\n        hop size in samples, defaults to 44100 (no overlap)\n    mode : str\n        BSSEval version, default to `v4`\n    Returns\n    -------\n    SDR : np.ndarray, shape=(nsrc,)\n        vector of Signal to Distortion Ratios (SDR)\n    ISR : np.ndarray, shape=(nsrc,)\n        vector of Source to Spatial Distortion Image (ISR)\n    SIR : np.ndarray, shape=(nsrc,)\n        vector of Source to Interference Ratios (SIR)\n    SAR : np.ndarray, shape=(nsrc,)\n        vector of Sources to Artifacts Ratios (SAR)\n    \"\"\"\n\n    estimates = np.array(estimates)\n    references = np.array(references)\n\n    if padding:\n        references, estimates = pad_or_truncate(references, estimates)\n\n    SDR, ISR, SIR, SAR, _ = metrics.bss_eval(\n        references,\n        estimates,\n        compute_permutation=False,\n        window=win,\n        hop=hop,\n        framewise_filters=(mode == \"v3\"),\n        bsseval_sources_version=False\n    )\n\n    return SDR, ISR, SIR, SAR", "response": "Evaluate BSS_EVAL images using metrics module"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_target(self, target_name, values):\n        target_data = {\n            'name': target_name,\n            'frames': []\n        }\n        for i, v in enumerate(values['SDR']):\n            frame_data = {\n                'time': i * self.hop,\n                'duration': self.win,\n                'metrics': {\n                    \"SDR\": self._q(values['SDR'][i]),\n                    \"SIR\": self._q(values['SIR'][i]),\n                    \"SAR\": self._q(values['SAR'][i]),\n                    \"ISR\": self._q(values['ISR'][i])\n                }\n            }\n            target_data['frames'].append(frame_data)\n\n        self.scores['targets'].append(target_data)", "response": "add target to scores Dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a json string representation of the scores dictionary", "response": "def json(self):\n        \"\"\"add target to scores Dictionary\n\n        Returns\n        ----------\n        json_string : str\n            json dump of the scores dictionary\n        \"\"\"\n        json_string = json.dumps(\n            self.scores,\n            indent=2,\n            allow_nan=True\n        )\n        return json_string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _q(self, number, precision='.00001'):\n        if np.isinf(number):\n            return np.nan\n        else:\n            return D(D(number).quantize(D(precision)))", "response": "quantiztion of BSSEval values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bsseval(inargs=None):\n    parser = argparse.ArgumentParser()\n\n    parser.add_argument(\n        'reference_dir',\n        type=str\n    )\n\n    parser.add_argument(\n        'estimates_dir',\n        type=str\n    )\n\n    parser.add_argument('-o', help='output_dir')\n\n    parser.add_argument(\n        '--win', type=float, help='Window size in seconds', default=1.0\n    )\n\n    parser.add_argument(\n        '--hop', type=float, help='Hop size in seconds', default=1.0\n    )\n\n    parser.add_argument(\n        '-m', type=str, help='bss_eval version [`v3`, `v4`]', default='v4'\n    )\n\n    parser.add_argument(\n        '--version', '-v',\n        action='version',\n        version='%%(prog)s %s' % util.__version__\n    )\n\n    args = parser.parse_args(inargs)\n\n    if not args.o:\n        output_dir = args.estimates_dir\n    else:\n        output_dir = args.o\n\n    # evaluate an existing estimate folder with wav files\n    data = eval_dir(\n        args.reference_dir,\n        args.estimates_dir,\n        output_dir=output_dir,\n        mode=args.m,\n        win=args.win,\n        hop=args.hop\n    )\n\n    print(data)", "response": "Command line interface for bsseval."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the next page after this one in the result sequence it s from.", "response": "def next_page(self):\n        \"\"\"Return the next `Page` after this one in the result sequence\n        it's from.\n\n        If the current page is the last page in the sequence, calling\n        this method raises a `ValueError`.\n\n        \"\"\"\n        try:\n            next_url = self.next_url\n        except AttributeError:\n            raise PageError(\"Page %r has no next page\" % self)\n        return self.page_for_url(next_url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef first_page(self):\n        try:\n            start_url = self.start_url\n        except AttributeError:\n            raise PageError(\"Page %r is already the first page\" % self)\n        return self.page_for_url(start_url)", "response": "Return the first page in the result sequence this Page instance is from."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef page_for_url(cls, url):\n        resp, elem = Resource.element_for_url(url)\n\n        value = Resource.value_for_element(elem)\n\n        return cls.page_for_value(resp, value)", "response": "Return a new Page containing the items at the given url."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef page_for_value(cls, resp, value):\n        page = cls(value)\n        links = parse_link_value(resp.getheader('Link'))\n        for url, data in six.iteritems(links):\n            if data.get('rel') == 'start':\n                page.start_url = url\n            if data.get('rel') == 'next':\n                page.next_url = url\n\n        return page", "response": "Return a new Page representing the given resource value retrieved using the HTTP response resp."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the list of attributes to be serialized in a POST or PUT request.", "response": "def serializable_attributes(self):\n        \"\"\" Attributes to be serialized in a ``POST`` or ``PUT`` request.\n        Returns all attributes unless a blacklist is specified\n        \"\"\"\n\n        if hasattr(self, 'blacklist_attributes'):\n            return [attr for attr in self.attributes if attr not in\n                    self.blacklist_attributes]\n        else:\n            return self.attributes"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef http_request(cls, url, method='GET', body=None, headers=None):\n\n        if recurly.API_KEY is None:\n            raise recurly.UnauthorizedError('recurly.API_KEY not set')\n\n        url_parts = urlparse(url)\n        if not any(url_parts.netloc.endswith(d) for d in recurly.VALID_DOMAINS):\n            # TODO Exception class used for clean backport, change to\n            # ConfigurationError\n            raise Exception('Only a recurly domain may be called')\n\n        is_non_ascii = lambda s: any(ord(c) >= 128 for c in s)\n\n        if is_non_ascii(recurly.API_KEY) or is_non_ascii(recurly.SUBDOMAIN):\n            raise recurly.ConfigurationError(\"\"\"Setting API_KEY or SUBDOMAIN to\n                    unicode strings may cause problems. Please use strings.\n                    Issue described here:\n                    https://gist.github.com/maximehardy/d3a0a6427d2b6791b3dc\"\"\")\n\n        urlparts = urlsplit(url)\n        connection_options = {}\n        if recurly.SOCKET_TIMEOUT_SECONDS:\n            connection_options['timeout'] = recurly.SOCKET_TIMEOUT_SECONDS\n        if urlparts.scheme != 'https':\n            connection = http_client.HTTPConnection(urlparts.netloc, **connection_options)\n        elif recurly.CA_CERTS_FILE is None:\n            connection = http_client.HTTPSConnection(urlparts.netloc, **connection_options)\n        else:\n            connection_options['context'] = ssl.create_default_context(cafile=recurly.CA_CERTS_FILE)\n            connection = http_client.HTTPSConnection(urlparts.netloc, **connection_options)\n\n        headers = {} if headers is None else dict(headers)\n        headers.setdefault('Accept', 'application/xml')\n        headers.update({\n            'User-Agent': recurly.USER_AGENT\n        })\n        headers['X-Api-Version'] = recurly.api_version()\n        headers['Authorization'] = 'Basic %s' % base64.b64encode(six.b('%s:' % recurly.API_KEY)).decode()\n\n        log = logging.getLogger('recurly.http.request')\n        if log.isEnabledFor(logging.DEBUG):\n            log.debug(\"%s %s HTTP/1.1\", method, url)\n            for header, value in six.iteritems(headers):\n                if header == 'Authorization':\n                    value = '<redacted>'\n                log.debug(\"%s: %s\", header, value)\n            log.debug('')\n            if method in ('POST', 'PUT') and body is not None:\n                if isinstance(body, Resource):\n                    log.debug(body.as_log_output())\n                else:\n                    log.debug(body)\n\n        if isinstance(body, Resource):\n            body = ElementTreeBuilder.tostring(body.to_element(), encoding='UTF-8')\n            headers['Content-Type'] = 'application/xml; charset=utf-8'\n        if method in ('POST', 'PUT') and body is None:\n            headers['Content-Length'] = '0'\n        connection.request(method, url, body, headers)\n        resp = connection.getresponse()\n\n        resp_headers = cls.headers_as_dict(resp)\n\n        log = logging.getLogger('recurly.http.response')\n        if log.isEnabledFor(logging.DEBUG):\n            log.debug(\"HTTP/1.1 %d %s\", resp.status, resp.reason)\n            log.debug(resp_headers)\n            log.debug('')\n\n        recurly.cache_rate_limit_headers(resp_headers)\n\n        return resp", "response": "Make an HTTP request to the given URL and return the HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nturn an array of response headers into a dictionary", "response": "def headers_as_dict(cls, resp):\n        \"\"\"Turns an array of response headers into a dictionary\"\"\"\n        if six.PY2:\n            pairs = [header.split(':', 1) for header in resp.msg.headers]\n            return dict([(k, v.strip()) for k, v in pairs])\n        else:\n            return dict([(k, v.strip()) for k, v in resp.msg._headers])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an XML string containing a serialization of thisCOOKIE instance suitable for logging.", "response": "def as_log_output(self):\n        \"\"\"Returns an XML string containing a serialization of this\n        instance suitable for logging.\n\n        Attributes named in the instance's `sensitive_attributes` are\n        redacted.\n\n        \"\"\"\n        elem = self.to_element()\n        for attrname in self.sensitive_attributes:\n            for sensitive_el in elem.iter(attrname):\n                sensitive_el.text = 'XXXXXXXXXXXXXXXX'\n        return ElementTreeBuilder.tostring(elem, encoding='UTF-8')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Resource instance identified by the given code or UUID.", "response": "def get(cls, uuid):\n        \"\"\"Return a `Resource` instance of this class identified by\n        the given code or UUID.\n\n        Only `Resource` classes with specified `member_path` attributes\n        can be directly requested with this method.\n\n        \"\"\"\n        if not uuid:\n            raise ValueError(\"get must have a value passed as an argument\")\n        uuid = quote(str(uuid))\n        url = recurly.base_uri() + (cls.member_path % (uuid,))\n        _resp, elem = cls.element_for_url(url)\n        return cls.from_element(elem)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the headers only for the given URL as a dict", "response": "def headers_for_url(cls, url):\n        \"\"\"Return the headers only for the given URL as a dict\"\"\"\n        response = cls.http_request(url, method='HEAD')\n        if response.status != 200:\n            cls.raise_http_error(response)\n\n        return Resource.headers_as_dict(response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef element_for_url(cls, url):\n        response = cls.http_request(url)\n        if response.status != 200:\n            cls.raise_http_error(response)\n\n        assert response.getheader('Content-Type').startswith('application/xml')\n\n        response_xml = response.read()\n        logging.getLogger('recurly.http.response').debug(response_xml)\n        response_doc = ElementTree.fromstring(response_xml)\n\n        return response, response_doc", "response": "Return the resource at the given URL as a tuple\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef value_for_element(cls, elem):\n        log = logging.getLogger('recurly.resource')\n        if elem is None:\n            log.debug(\"Converting %r element into None value\", elem)\n            return\n\n        if elem.attrib.get('nil') is not None:\n            log.debug(\"Converting %r element with nil attribute into None value\", elem.tag)\n            return\n\n        if elem.tag.endswith('_in_cents') and 'currency' not in cls.attributes and not cls.inherits_currency:\n            log.debug(\"Converting %r element in class with no matching 'currency' into a Money value\", elem.tag)\n            return Money.from_element(elem)\n\n        attr_type = elem.attrib.get('type')\n        log.debug(\"Converting %r element with type %r\", elem.tag, attr_type)\n\n        if attr_type == 'integer':\n            return int(elem.text.strip())\n        if attr_type == 'float':\n            return float(elem.text.strip())\n        if attr_type == 'boolean':\n            return elem.text.strip() == 'true'\n        if attr_type == 'datetime':\n            return iso8601.parse_date(elem.text.strip())\n        if attr_type == 'array':\n            return [cls._subclass_for_nodename(sub_elem.tag).from_element(sub_elem) for sub_elem in elem]\n\n        # Unknown types may be the names of resource classes.\n        if attr_type is not None:\n            try:\n                value_class = cls._subclass_for_nodename(attr_type)\n            except ValueError:\n                log.debug(\"Not converting %r element with type %r to a resource as that matches no known nodename\",\n                    elem.tag, attr_type)\n            else:\n                return value_class.from_element(elem)\n\n        # Untyped complex elements should still be resource instances. Guess from the nodename.\n        if len(elem):  # has children\n            value_class = cls._subclass_for_nodename(elem.tag)\n            log.debug(\"Converting %r tag into a %s\", elem.tag, value_class.__name__)\n            return value_class.from_element(elem)\n\n        value = elem.text or ''\n        return value.strip()", "response": "Deserialize the given XML element into its representative\n        value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef element_for_value(cls, attrname, value):\n        if isinstance(value, Resource):\n            if attrname in cls._classes_for_nodename:\n                # override the child's node name with this attribute name\n                return value.to_element(attrname)\n\n            return value.to_element()\n\n        el = ElementTreeBuilder.Element(attrname)\n\n        if value is None:\n            el.attrib['nil'] = 'nil'\n        elif isinstance(value, bool):\n            el.attrib['type'] = 'boolean'\n            el.text = 'true' if value else 'false'\n        elif isinstance(value, int):\n            el.attrib['type'] = 'integer'\n            el.text = str(value)\n        elif isinstance(value, datetime):\n            el.attrib['type'] = 'datetime'\n            el.text = value.strftime('%Y-%m-%dT%H:%M:%SZ')\n        elif isinstance(value, list) or isinstance(value, tuple):\n            for sub_resource in value:\n                if hasattr(sub_resource, 'to_element'):\n                  el.append(sub_resource.to_element())\n                else:\n                  el.append(cls.element_for_value(re.sub(r\"s$\", \"\", attrname), sub_resource))\n        elif isinstance(value, Money):\n            value.add_to_element(el)\n        else:\n            el.text = six.text_type(value)\n\n        return el", "response": "Serialize the given value into an XML Element with the given tag name returning it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_from_element(self, elem):\n        self._elem = elem\n\n        for attrname in self.attributes:\n            try:\n                delattr(self, attrname)\n            except AttributeError:\n                pass\n\n        document_url = elem.attrib.get('href')\n        if document_url is not None:\n            self._url = document_url\n\n        return self", "response": "Reset this Resource instance to represent the values in\n        the given XML element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Page of instances of this Resource class from its general collection endpoint.", "response": "def all(cls, **kwargs):\n        \"\"\"Return a `Page` of instances of this `Resource` class from\n        its general collection endpoint.\n\n        Only `Resource` classes with specified `collection_path`\n        endpoints can be requested with this method. Any provided\n        keyword arguments are passed to the API endpoint as query\n        parameters.\n\n        \"\"\"\n        url = recurly.base_uri() + cls.collection_path\n        if kwargs:\n            url = '%s?%s' % (url, urlencode_params(kwargs))\n        return Page.page_for_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a count of server side resources given a set of keyword arguments in kwargs.", "response": "def count(cls, **kwargs):\n        \"\"\"Return a count of server side resources given\n        filtering arguments in kwargs.\n        \"\"\"\n        url = recurly.base_uri() + cls.collection_path\n        if kwargs:\n            url = '%s?%s' % (url, urlencode_params(kwargs))\n        return Page.count_for_url(url)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef put(self, url):\n        response = self.http_request(url, 'PUT', self, {'Content-Type': 'application/xml; charset=utf-8'})\n        if response.status != 200:\n            self.raise_http_error(response)\n\n        response_xml = response.read()\n        logging.getLogger('recurly.http.response').debug(response_xml)\n        self.update_from_element(ElementTree.fromstring(response_xml))", "response": "Sends this Resource instance to the service with a\n        PUT request to the given URL."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(self, url, body=None):\n        response = self.http_request(url, 'POST', body or self, {'Content-Type': 'application/xml; charset=utf-8'})\n        if response.status not in (200, 201, 204):\n            self.raise_http_error(response)\n\n        self._url = response.getheader('Location')\n\n        if response.status in (200, 201):\n            response_xml = response.read()\n            logging.getLogger('recurly.http.response').debug(response_xml)\n            self.update_from_element(ElementTree.fromstring(response_xml))", "response": "Sends this Resource instance to the service with a\n        POST request to the given URL. Takes an optional body."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n        response = self.http_request(self._url, 'DELETE')\n        if response.status != 204:\n            self.raise_http_error(response)", "response": "Submits a DELETE request to the resource instance s URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises a HTTPError of the appropriate subclass of InfrastructureException.", "response": "def raise_http_error(cls, response):\n        \"\"\"Raise a `ResponseError` of the appropriate subclass in\n        reaction to the given `http_client.HTTPResponse`.\"\"\"\n        response_xml = response.read()\n        logging.getLogger('recurly.http.response').debug(response_xml)\n        exc_class = recurly.errors.error_class_for_http_status(response.status)\n        raise exc_class(response_xml)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nserialize this Resource instance to an XML element.", "response": "def to_element(self, root_name=None):\n        \"\"\"Serialize this `Resource` instance to an XML element.\"\"\"\n        if not root_name:\n            root_name = self.nodename\n        elem = ElementTreeBuilder.Element(root_name)\n        for attrname in self.serializable_attributes():\n            # Only use values that have been loaded into the internal\n            # __dict__. For retrieved objects we look into the XML response at\n            # access time, so the internal __dict__ contains only the elements\n            # that have been set on the client side.\n            try:\n                value = self.__dict__[attrname]\n            except KeyError:\n                continue\n\n            if attrname in self.xml_attribute_attributes:\n                elem.attrib[attrname] = six.text_type(value)\n            else:\n                sub_elem = self.element_for_value(attrname, value)\n                elem.append(sub_elem)\n\n        return elem"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bss_eval_sources(reference_sources, estimated_sources,\n                     compute_permutation=True):\n    \"\"\"\n    BSS Eval v3 bss_eval_sources\n\n    Wrapper to ``bss_eval`` with the right parameters.\n    The call to this function is not recommended. See the description for the\n    ``bsseval_sources`` parameter of ``bss_eval``.\n\n    \"\"\"\n    (sdr, isr, sir, sar, perm) = \\\n        bss_eval(\n            reference_sources, estimated_sources,\n            window=np.inf, hop=np.inf,\n            compute_permutation=compute_permutation, filters_len=512,\n            framewise_filters=True,\n            bsseval_sources_version=True\n    )\n    return (sdr, sir, sar, perm)", "response": "Wrapper to bss_eval with the right parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bss_eval_sources_framewise(reference_sources, estimated_sources,\n                               window=30 * 44100, hop=15 * 44100,\n                               compute_permutation=False):\n    \"\"\"\n    BSS Eval v3 bss_eval_sources_framewise\n\n    Wrapper to ``bss_eval`` with the right parameters.\n    The call to this function is not recommended. See the description for the\n    ``bsseval_sources`` parameter of ``bss_eval``.\n\n    \"\"\"\n    (sdr, isr, sir, sar, perm) = \\\n        bss_eval(\n            reference_sources, estimated_sources,\n            window=window, hop=hop,\n            compute_permutation=compute_permutation, filters_len=512,\n            framewise_filters=True,\n            bsseval_sources_version=True)\n    return (sdr, sir, sar, perm)", "response": "Wrapper for bss_eval that returns the framewise version of the BSS EVAL sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps for bss_eval that returns a list of images for the given set of reference sources and estimated sources.", "response": "def bss_eval_images(reference_sources, estimated_sources,\n                    compute_permutation=True):\n    \"\"\"\n    BSS Eval v3 bss_eval_images\n\n    Wrapper to ``bss_eval`` with the right parameters.\n\n    \"\"\"\n    return bss_eval(\n        reference_sources, estimated_sources,\n        window=np.inf, hop=np.inf,\n        compute_permutation=compute_permutation, filters_len=512,\n        framewise_filters=True,\n        bsseval_sources_version=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps of bss_eval with the right parameters.", "response": "def bss_eval_images_framewise(reference_sources, estimated_sources,\n                              window=30 * 44100, hop=15 * 44100,\n                              compute_permutation=False):\n    \"\"\"\n    BSS Eval v3 bss_eval_images_framewise\n\n    Framewise computation of bss_eval_images.\n    Wrapper to ``bss_eval`` with the right parameters.\n\n    \"\"\"\n    return bss_eval(\n        reference_sources, estimated_sources,\n        window=window, hop=hop,\n        compute_permutation=compute_permutation, filters_len=512,\n        framewise_filters=True,\n        bsseval_sources_version=False\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _zeropad(sig, N, axis=0):\n    # ensures concatenation dimension is the first\n    sig = np.moveaxis(sig, axis, 0)\n    # zero pad\n    out = np.zeros((sig.shape[0] + N,) + sig.shape[1:])\n    out[:sig.shape[0], ...] = sig\n    # put back axis in place\n    out = np.moveaxis(out, 0, axis)\n    return out", "response": "pads with N zeros at the end of the signal along given axis"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reshape_G(G):\n    G = np.moveaxis(G, (1, 3), (3, 4))\n    (nsrc, nchan, filters_len) = G.shape[0:3]\n    G = np.reshape(\n        G, (nsrc * nchan * filters_len, nsrc * nchan * filters_len)\n    )\n    return G", "response": "From a correlation matrix of sizensrc nsrc nchan X nchan X filters_len X filters_len creates a new one of sizensrc nchan X nchan X filters_len X nsrc X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X nchan X filters_len X X"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the inner products between delayed versions of reference_sources reference is nsrc X nsamp X nchan X filters_len X filters_len", "response": "def _compute_reference_correlations(reference_sources, filters_len):\n    \"\"\"Compute the inner products between delayed versions of reference_sources\n    reference is nsrc X nsamp X nchan.\n    Returns\n    * G, matrix : nsrc X nsrc X nchan X nchan X filters_len X filters_len\n    * sf, reference spectra: nsrc X nchan X filters_len\"\"\"\n\n    # reshape references as nsrc X nchan X nsampl\n    (nsrc, nsampl, nchan) = reference_sources.shape\n    reference_sources = np.moveaxis(reference_sources, (1), (2))\n\n    # zero padding and FFT of references\n    reference_sources = _zeropad(reference_sources, filters_len - 1, axis=2)\n    n_fft = int(2**np.ceil(np.log2(nsampl + filters_len - 1.)))\n    sf = scipy.fftpack.fft(reference_sources, n=n_fft, axis=2)\n\n    # compute intercorrelation between sources\n    G = np.zeros((nsrc, nsrc, nchan, nchan, filters_len, filters_len))\n    for ((i, c1), (j, c2)) in itertools.combinations_with_replacement(\n        itertools.product(\n            list(range(nsrc)), list(range(nchan))\n        ),\n        2\n    ):\n\n        ssf = sf[j, c2] * np.conj(sf[i, c1])\n        ssf = np.real(scipy.fftpack.ifft(ssf))\n        ss = toeplitz(\n            np.hstack((ssf[0], ssf[-1:-filters_len:-1])),\n            r=ssf[:filters_len]\n        )\n        G[j, i, c2, c1] = ss\n        G[i, j, c1, c2] = ss.T\n    return G, sf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the least - squares projection of the estimated source on the subspace spanned by delayed versions of 0 and filters_len - 1", "response": "def _compute_projection_filters(G, sf, estimated_source):\n    \"\"\"Least-squares projection of estimated source on the subspace spanned by\n    delayed versions of reference sources, with delays between 0 and\n    filters_len-1\n    \"\"\"\n    # epsilon\n    eps = np.finfo(np.float).eps\n\n    # shapes\n    (nsampl, nchan) = estimated_source.shape\n    # handles the case where we are calling this with only one source\n    # G should be nsrc X nsrc X nchan X nchan X filters_len X filters_len\n    # and sf should be nsrc X nchan X filters_len\n    if len(G.shape) == 4:\n        G = G[None, None, ...]\n        sf = sf[None, ...]\n    nsrc = G.shape[0]\n    filters_len = G.shape[-1]\n\n    # zero pad estimates and put chan in first dimension\n    estimated_source = _zeropad(estimated_source.T, filters_len - 1, axis=1)\n\n    # compute its FFT\n    n_fft = int(2**np.ceil(np.log2(nsampl + filters_len - 1.)))\n    sef = scipy.fftpack.fft(estimated_source, n=n_fft)\n\n    # compute the cross-correlations between sources and estimates\n    D = np.zeros((nsrc, nchan, filters_len, nchan))\n    for (j, cj, c) in itertools.product(\n        list(range(nsrc)), list(range(nchan)), list(range(nchan))\n    ):\n        ssef = sf[j, cj] * np.conj(sef[c])\n        ssef = np.real(scipy.fftpack.ifft(ssef))\n        D[j, cj, :, c] = np.hstack((ssef[0], ssef[-1:-filters_len:-1]))\n\n    # reshape matrices to build the filters\n    D = D.reshape(nsrc * nchan * filters_len, nchan)\n    G = _reshape_G(G)\n\n    # Distortion filters\n    try:\n        C = np.linalg.solve(G + eps*np.eye(G.shape[0]), D).reshape(\n            nsrc, nchan, filters_len, nchan\n        )\n    except np.linalg.linalg.LinAlgError:\n        C = np.linalg.lstsq(G, D)[0].reshape(\n            nsrc, nchan, filters_len, nchan\n        )\n\n    # if we asked for one single reference source,\n    # return just a nchan X filters_len matrix\n    if nsrc == 1:\n        C = C[0]\n    return C"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _project(reference_sources, C):\n    # shapes: ensure that input is 3d (comprising the source index)\n    if len(reference_sources.shape) == 2:\n        reference_sources = reference_sources[None, ...]\n        C = C[None, ...]\n\n    (nsrc, nsampl, nchan) = reference_sources.shape\n    filters_len = C.shape[-2]\n\n    # zero pad\n    reference_sources = _zeropad(reference_sources, filters_len - 1, axis=1)\n    sproj = np.zeros((nchan, nsampl + filters_len - 1))\n\n    for (j, cj, c) in itertools.product(\n        list(range(nsrc)), list(range(nchan)), list(range(nchan))\n    ):\n        sproj[c] += fftconvolve(\n            C[j, cj, :, c],\n            reference_sources[j, :, cj]\n        )[:nsampl + filters_len - 1]\n    return sproj.T", "response": "Project images using pre - computed filters C\n reference_sources are nsrc nsampl nchan C is nsrc nsampl nchan C is nsrc nchan C is nsrc nchan C is nsrc nchan nchan"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _bss_crit(s_true, e_spat, e_interf, e_artif, bsseval_sources_version):\n    # energy ratios\n    if bsseval_sources_version:\n        s_filt = s_true + e_spat\n        energy_s_filt = np.sum(s_filt**2)\n        sdr = _safe_db(energy_s_filt,\n                       np.sum((e_interf + e_artif)**2))\n        isr = np.empty(sdr.shape) * np.nan\n        sir = _safe_db(energy_s_filt, np.sum(e_interf**2))\n        sar = _safe_db(np.sum((s_filt + e_interf)**2),\n                       np.sum(e_artif**2))\n    else:\n        energy_s_true = np.sum((s_true)**2)\n        sdr = _safe_db(energy_s_true,\n                       np.sum((e_spat + e_interf + e_artif)**2))\n        isr = _safe_db(energy_s_true, np.sum(e_spat**2))\n        sir = _safe_db(np.sum((s_true + e_spat)**2), np.sum(e_interf**2))\n        sar = _safe_db(np.sum((s_true + e_spat + e_interf)**2),\n                       np.sum(e_artif**2))\n\n    return (sdr, isr, sir, sar)", "response": "Measurement of the separation quality for a given source in terms of\n    filtered true source interference and artifacts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _safe_db(num, den):\n    if den == 0:\n        return np.inf\n    return 10 * np.log10(num / den)", "response": "Properly handle the potential + Inf db SIR instead of raising RuntimeWarning. a\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a link - value return a dictionary whose keys are link URLs and values are dictionaries whose keys are parameters and values are dictionaries whose keys are parameters and values are lists of link URLs and their associated parameters.", "response": "def parse_link_value(instr):\n    \"\"\"\n    Given a link-value (i.e., after separating the header-value on commas), \n    return a dictionary whose keys are link URLs and values are dictionaries\n    of the parameters for their associated links.\n    \n    Note that internationalised parameters (e.g., title*) are \n    NOT percent-decoded.\n    \n    Also, only the last instance of a given parameter will be included.\n    \n    For example, \n    \n    >>> parse_link_value('</foo>; rel=\"self\"; title*=utf-8\\'de\\'letztes%20Kapitel')\n    {'/foo': {'title*': \"utf-8'de'letztes%20Kapitel\", 'rel': 'self'}}\n    \n    \"\"\"\n    out = {}\n    if not instr:\n        return out\n    for link in [h.strip() for h in link_splitter.findall(instr)]:\n        url, params = link.split(\">\", 1)\n        url = url[1:]\n        param_dict = {}\n        for param in _splitstring(params, PARAMETER, \"\\s*;\\s*\"):\n            try:\n                a, v = param.split(\"=\", 1)\n                param_dict[a.lower()] = _unquotestring(v)\n            except ValueError:\n                param_dict[param.lower()] = None\n        out[url] = param_dict\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the URL for the desired API endpoint.", "response": "def construct_api_url(input, representation, resolvers=None, get3d=False, tautomers=False, xml=True, **kwargs):\n    \"\"\"Return the URL for the desired API endpoint.\n\n    :param string input: Chemical identifier to resolve\n    :param string representation: Desired output representation\n    :param list(str) resolvers: (Optional) Ordered list of resolvers to use\n    :param bool get3d: (Optional) Whether to return 3D coordinates (where applicable)\n    :param bool tautomers: (Optional) Whether to return all tautomers\n    :param bool xml: (Optional) Whether to return full XML response\n    :returns: CIR API URL\n    :rtype: str\n    \"\"\"\n    # File formats require representation=file and the format in the querystring\n    if representation in FILE_FORMATS:\n        kwargs['format'] = representation\n        representation = 'file'\n    # Prepend input with 'tautomers:' to return all tautomers\n    if tautomers:\n        input = 'tautomers:%s' % input\n    url = '%s/%s/%s' % (API_BASE, quote(input), representation)\n    if xml:\n        url += '/xml'\n    if resolvers:\n        kwargs['resolver'] = ','.join(resolvers)\n    if get3d:\n        kwargs['get3d'] = True\n    if kwargs:\n        url += '?%s' % urlencode(kwargs)\n    return url"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(input, representation, resolvers=None, get3d=False, tautomers=False, **kwargs):\n    url = construct_api_url(input, representation, resolvers, get3d, tautomers, **kwargs)\n    log.debug('Making request: %s', url)\n    response = urlopen(url)\n    return etree.parse(response).getroot()", "response": "Make a request to CIR and return the XML response."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query(input, representation, resolvers=None, get3d=False, tautomers=False, **kwargs):\n    tree = request(input, representation, resolvers, get3d, tautomers, **kwargs)\n    results = []\n    for data in tree.findall('.//data'):\n        value = [item.text for item in data.findall('item')]\n        result = Result(\n            input=tree.attrib['string'],\n            representation=tree.attrib['representation'],\n            resolver=data.attrib['resolver'],\n            input_format=data.attrib['string_class'],\n            notation=data.attrib['notation'],\n            value=value[0] if len(value) == 1 else value\n        )\n        results.append(result)\n    log.debug('Received %s query results', len(results))\n    return results", "response": "Query the CIR for all results for resolving input to the specified output representation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve(input, representation, resolvers=None, get3d=False, **kwargs):\n    # Take first result from XML query\n    results = query(input, representation, resolvers, False, get3d, **kwargs)\n    result = results[0].value if results else None\n    return result", "response": "Resolve input to the specified output representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves input to a 2D image depiction.", "response": "def resolve_image(input, resolvers=None, fmt='png', width=300, height=300, frame=False, crop=None, bgcolor=None,\n                  atomcolor=None, hcolor=None, bondcolor=None, framecolor=None, symbolfontsize=11, linewidth=2,\n                  hsymbol='special', csymbol='special', stereolabels=False, stereowedges=True, header=None, footer=None,\n                  **kwargs):\n    \"\"\"Resolve input to a 2D image depiction.\n\n    :param string input: Chemical identifier to resolve\n    :param list(string) resolvers: (Optional) Ordered list of resolvers to use\n    :param string fmt: (Optional) gif or png image format (default png)\n\n    :param int width: (Optional) Image width in pixels (default 300)\n    :param int height: (Optional) Image height in pixels (default 300)\n    :param bool frame: (Optional) Whether to show border frame (default False)\n    :param int crop: (Optional) Crop image with specified padding\n\n    :param int symbolfontsize: (Optional) Atom label font size (default 11)\n    :param int linewidth: (Optional) Bond line width (default 2)\n\n    :param string bgcolor: (Optional) Background color\n    :param string atomcolor: (Optional) Atom label color\n    :param string hcolor: (Optional) Hydrogen atom label color\n    :param string bondcolor: (Optional) Bond color\n    :param string framecolor: (Optional) Border frame color\n\n    :param bool hsymbol: (Optional) Hydrogens: all, special or none (default special)\n    :param bool csymbol: (Optional) Carbons: all, special or none (default special)\n    :param bool stereolabels: (Optional) Whether to show stereochemistry labels (default False)\n    :param bool stereowedges: (Optional) Whether to show wedge/dash bonds (default True)\n    :param string header: (Optional) Header text above structure\n    :param string footer: (Optional) Footer text below structure\n\n    \"\"\"\n    # Aggregate all arguments into kwargs\n    args, _, _, values = inspect.getargvalues(inspect.currentframe())\n    for arg in args:\n        if values[arg] is not None:\n            kwargs[arg] = values[arg]\n    # Turn off anti-aliasing for transparent background\n    if kwargs.get('bgcolor') == 'transparent':\n        kwargs['antialiasing'] = False\n    # Renamed parameters\n    if 'stereolabels' in kwargs:\n        kwargs['showstereo'] = kwargs.pop('stereolabels')\n    if 'fmt' in kwargs:\n        kwargs['format'] = kwargs.pop('fmt')\n    # Toggle stereo wedges\n    if 'stereowedges' in kwargs:\n        status = kwargs.pop('stereowedges')\n        kwargs.update({'wedges': status, 'dashes': status})\n    # Constant values\n    kwargs.update({'representation': 'image', 'xml': False})\n    url = construct_api_url(**kwargs)\n    log.debug('Making image request: %s', url)\n    response = urlopen(url)\n    return response.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download(input, filename, representation, overwrite=False, resolvers=None, get3d=False, **kwargs):\n    result = resolve(input, representation, resolvers, get3d, **kwargs)\n    # Just log and return if nothing resolved\n    if not result:\n        log.debug('No file to download.')\n        return\n    # Only overwrite an existing file if explicitly instructed to.\n    if not overwrite and os.path.isfile(filename):\n        raise IOError(\"%s already exists. Use 'overwrite=True' to overwrite it.\" % filename)\n    # Ensure file ends with a newline\n    if not result.endswith('\\n'):\n        result += '\\n'\n    with open(filename, 'w') as f:\n        f.write(result)", "response": "Convenience function to save a CIR response as a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef twirl_url(self):\n        return construct_api_url(self.input, 'twirl', self.resolvers, False, self.get3d, False, **self.kwargs)", "response": "Url of a TwirlyMol 3D viewer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(self, filename, representation, overwrite=False):\n        download(self.input, filename, representation, overwrite, self.resolvers, self.get3d, **self.kwargs)", "response": "Download the resolved structure as a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_token(refresh_url, exceptions=(), callback=None,\n                   access_key='access_token', refresh_key='refresh_token'):\n    ''' a decorator used to validate the access_token for oauth based\n    data sources.\n    This decorator should be used on every method in the data source that\n    fetches data from the oauth controlled resource, and that relies on a\n    valid access_token in order to operate properly.\n    If the token is valid, the normal flow continues without any change.\n    Otherwise, if any of `exceptions` tuple is raised, the normal flow\n    will be preceded by the following steps:\n\n    1. `refresh_url` will be called in order to refresh the token\n    2. the newly refreshed token will be saved in the source\n    3. the `callback` function will be called\n\n    If the refresh fails for any reason, the user would have to re-grant\n    permission for the application\n\n    Parameters\n    ----------\n    refresh_url : str\n        The URL to be called in order to refresh the access token.\n    callback : str or callable\n        A callback function to be called whenever the access_token is\n        validated. The callback function would be called with the refreshed\n        token as an argument.\n        If the `callback` is not `callable`, but an `str` it will be called\n        on `self` (i.e. call a method on your Data Source)\n        Defaults to None\n    exceptions : tuple\n        A list of exceptions that should cause token revalidation\n        Defaults to Exception, meaning that all errors will cause token\n        refresh\n    access_key : str\n        The access token key as defined in the source and in the response from\n        the refresh URL.\n        Defaults to `access_token`\n    refresh_key : str\n        The refresh token key as defined in the source and in the request to\n        the refresh URL.\n        Defaults to `refresh_token`\n    '''\n    def _validate_token(f):\n        def wrapper(*args):\n            self = args[0]\n            try:\n                return f(*args)\n            except exceptions:\n                try:\n                    self.log('Revalidating the access token...')\n                    self.source[access_key] = None\n\n                    # get a new token from refresh_url\n                    token = self.source.get(refresh_key)\n                    data = dict(self.options['refresh'],\n                                **{refresh_key: token})\n                    r = requests.post(refresh_url, data=data)\n                    self.source[access_key] = r.json()[access_key]\n\n                    # save the new token in the database\n                    changes = {access_key: self.source[access_key]}\n                    self.fire('source-change', changes)\n\n                    # notify the callback that a new token was issued\n                    if callback:\n                        if callable(callback):\n                            _callback = callback\n                        else:\n                            _callback = getattr(self, callback)\n                        _callback(self.source.get(access_key))\n\n                    return f(*args)\n                except Exception, e:\n                    self.log('Error: Access token can\\'t be revalidated. '\n                             'The user would have to re-authenticate',\n                             traceback.format_exc())\n                    # raise a non-retryable exception\n                    raise PanoplyException(\n                        'access token could not be refreshed ({})'\n                        .format(str(e)), retryable=False)\n        return wrapper\n    return _validate_token", "response": "Decorator that validates the access_token for OAuth based data sources."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef progress(self, loaded, total, msg=''):\n\n        self.fire('progress', {\n            'loaded': loaded,\n            'total': total,\n            'msg': msg\n        })", "response": "Notify about a progress change"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a raw response object", "response": "def raw(self, tag, raw, metadata):\n        \"\"\" Create a raw response object \"\"\"\n        raw = base64.b64encode(raw)\n        return {\n            'type': 'raw',\n            'tag': tag,\n            'raw': raw,\n            'metadata': metadata\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef assert_looks_like(first, second, msg=None):\n    first = _re.sub(\"\\s+\", \" \", first.strip())\n    second = _re.sub(\"\\s+\", \" \", second.strip())\n    if first != second:\n        raise AssertionError(msg or \"%r does not look like %r\" % (first, second))", "response": "Assert that two strings look like first."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload and parse a feature file.", "response": "def load_feature(fname, language):\n    \"\"\" Load and parse a feature file. \"\"\"\n\n    fname = os.path.abspath(fname)\n    feat = parse_file(fname, language)\n    return feat"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the given steps in the given language.", "response": "def run_steps(spec, language=\"en\"):\n    \"\"\" Can be called by the user from within a step definition to execute other steps. \"\"\"\n\n    # The way this works is a little exotic, but I couldn't think of a better way to work around\n    # the fact that this has to be a global function and therefore cannot know about which step\n    # runner to use (other than making step runner global)\n    \n    # Find the step runner that is currently running and use it to run the given steps\n    fr = inspect.currentframe()\n    while fr:\n        if \"self\" in fr.f_locals:\n            f_self = fr.f_locals['self']\n            if isinstance(f_self, StepsRunner):\n                return f_self.run_steps_from_string(spec, language)\n        fr = fr.f_back"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalling from within step definitions to run other steps.", "response": "def run_steps_from_string(self, spec, language_name='en'):\n        \"\"\" Called from within step definitions to run other steps. \"\"\"\n        \n        caller = inspect.currentframe().f_back\n        line = caller.f_lineno - 1\n        fname = caller.f_code.co_filename\n        \n        steps = parse_steps(spec, fname, line, load_language(language_name))\n        for s in steps:\n            self.run_step(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns all the synonymns of a word in the requested language .", "response": "def words(self, key):\n        \"\"\"\n        Give all the synonymns of a word in the requested language \n        (or the default language if no word is available).\n        \"\"\"\n        if self.default_mappings is not None and key not in self.mappings:\n            return self.default_mappings[key].encode('utf').split(\"|\")\n        else:\n            return self.mappings[key].encode('utf').split(\"|\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef simulate_async_event():\n    scc.state = 'executing'\n    def async_event(result):\n        \"\"\"All other asynchronous events or function calls\n        returned from later steps will wait until this\n        callback fires.\"\"\"\n        scc.state = result\n        return 'some event result'\n    deferred = Deferred()\n    reactor.callLater(1, deferred.callback, 'done') # pylint: disable=E1101\n    deferred.addCallback(async_event)\n    return deferred", "response": "Simulate an asynchronous event."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hook_decorator(cb_type):\n    def decorator_wrapper(*tags_or_func):\n        if len(tags_or_func) == 1 and callable(tags_or_func[0]):\n            # No tags were passed to this decorator\n            func = tags_or_func[0]\n            return HookImpl(cb_type, func)\n        else:\n            # We got some tags, so we need to produce the real decorator\n            tags = tags_or_func\n            def d(func):\n                return HookImpl(cb_type, func, tags)\n            return d\n    return decorator_wrapper", "response": "Decorator to wrap hook definitions in. Registers hook."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_steps_impl(self, registry, path, module_names=None):\n\n        if not module_names:\n            module_names = ['steps']\n\n        path = os.path.abspath(path)\n\n        for module_name in module_names:\n            mod = self.modules.get((path, module_name))\n\n            if mod is None:\n                #log.debug(\"Looking for step def module '%s' in %s\" % (module_name, path))\n                cwd = os.getcwd()\n                if cwd not in sys.path:\n                    sys.path.append(cwd)\n\n                try:\n                    actual_module_name = os.path.basename(module_name)\n                    complete_path = os.path.join(path, os.path.dirname(module_name))\n                    info = imp.find_module(actual_module_name, [complete_path])\n                except ImportError:\n                    #log.debug(\"Did not find step defs module '%s' in %s\" % (module_name, path))\n                    return\n                \n                try:\n                    # Modules have to be loaded with unique names or else problems arise\n                    mod = imp.load_module(\"stepdefs_\" + str(self.module_counter), *info)\n                except:\n                    exc = sys.exc_info()\n                    raise StepImplLoadException(exc)\n\n                self.module_counter += 1\n                self.modules[(path, module_name)] = mod\n\n            for item_name in dir(mod):\n                item = getattr(mod, item_name)\n                if isinstance(item, StepImpl):\n                    registry.add_step(item.step_type, item)\n                elif isinstance(item, HookImpl):\n                    registry.add_hook(item.cb_type, item)\n                elif isinstance(item, NamedTransformImpl):\n                    registry.add_named_transform(item)\n                elif isinstance(item, TransformImpl):\n                    registry.add_transform(item)", "response": "Load the step implementations at the given path and add them to the given registry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_step_impl(self, step):\n        result = None\n        for si in self.steps[step.step_type]:\n            matches = si.match(step.match)\n            if matches:\n                if result:\n                    raise AmbiguousStepImpl(step, result[0], si)\n\n                args = [self._apply_transforms(arg, si) for arg in matches.groups()]\n                result = si, args\n\n        if not result:\n            raise UndefinedStepImpl(step)\n        return result", "response": "Find the implementation of the given step."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens a child process and return its exit status and stdout.", "response": "def run_command(cmd):\n    \"\"\"\n    Open a child process, and return its exit status and stdout.\n\n    \"\"\"\n    child = subprocess.Popen(cmd, shell=True, stderr=subprocess.PIPE,\n                             stdin=subprocess.PIPE, stdout=subprocess.PIPE)\n    out = [s.decode(\"utf-8\").strip() for s in child.stdout]\n    err = [s.decode(\"utf-8\").strip() for s in child.stderr]\n    w = child.wait()\n    return os.WEXITSTATUS(w), out, err"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransform filters into list of table rows.", "response": "def make_filter_list(filters):\n    \"\"\"Transform filters into list of table rows.\"\"\"\n    filter_list = []\n    filter_ids = []\n    for f in filters:\n        filter_ids.append(f.index)\n        fullname = URL_P.sub(r'`<\\1>`_', f.fullname)\n        filter_list.append((str(f.index + 1),\n                            f.name,\n                            \"{0:.2f}\".format(f.msun_vega),\n                            \"{0:.2f}\".format(f.msun_ab),\n                            \"{0:.1f}\".format(f.lambda_eff),\n                            fullname))\n    sortf = lambda item: int(item[0])\n    filter_list.sort(key=sortf)\n    return filter_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncodes for this RST-formatted table generator comes from http://stackoverflow.com/a/11350643", "response": "def make_table(data, col_names):\n    \"\"\"Code for this RST-formatted table generator comes from\n    http://stackoverflow.com/a/11350643\n    \"\"\"\n    n_cols = len(data[0])\n    assert n_cols == len(col_names)\n    col_sizes = [max(len(r[i]) for r in data) for i in range(n_cols)]\n    for i, cname in enumerate(col_names):\n        if col_sizes[i] < len(cname):\n            col_sizes[i] = len(cname)\n    formatter = ' '.join('{:<%d}' % c for c in col_sizes)\n    rows = '\\n'.join([formatter.format(*row) for row in data])\n    header = formatter.format(*col_names)\n    divider = formatter.format(*['=' * c for c in col_sizes])\n    output = '\\n'.join((divider, header, divider, rows, divider))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge multiple PDF input files in one output file.", "response": "def pdf_merge(inputs: [str], output: str, delete: bool = False):\n    \"\"\"\n    Merge multiple Pdf input files in one output file.\n    :param inputs: input files\n    :param output: output file\n    :param delete: delete input files after completion if true\n\n    \"\"\"\n    writer = PdfFileWriter()\n    if os.path.isfile(output):\n        ans = input(\n            \"The file '%s' already exists. \"\n            \"Overwrite? Yes/Abort [Y/a]: \" % output\n        ).lower()\n        if ans == \"a\":\n            return\n\n    outputfile = open(output, \"wb\")\n    try:\n        infiles = []\n        for filename in inputs:\n            f = open(filename, \"rb\")\n            reader = PdfFileReader(f)\n            for page in reader.pages:\n                writer.addPage(page)\n            infiles.append(f)\n        writer.write(outputfile)\n    except FileNotFoundError as e:\n        print(e.strerror + \": \" + e.filename)\n    finally:\n        outputfile.close()\n        for f in infiles:\n            f.close()\n    if delete:\n        for filename in inputs:\n            os.remove(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrotates the given pdf files clockwise or counter clockwise.", "response": "def pdf_rotate(\n    input: str,\n    counter_clockwise: bool = False,\n    pages: [str] = None,\n    output: str = None,\n):\n    \"\"\"\n    Rotate the given Pdf files clockwise or counter clockwise.\n    :param inputs: pdf files\n    :param counter_clockwise: rotate counter clockwise if true else clockwise\n    :param pages: list of page numbers to rotate, if None all pages will be\n        rotated\n    \"\"\"\n    infile = open(input, \"rb\")\n    reader = PdfFileReader(infile)\n    writer = PdfFileWriter()\n\n    # get pages from source depending on pages parameter\n    if pages is None:\n        source_pages = reader.pages\n    else:\n        pages = parse_rangearg(pages, len(reader.pages))\n        source_pages = [reader.getPage(i) for i in pages]\n\n    # rotate pages and add to writer\n    for i, page in enumerate(source_pages):\n        if pages is None or i in pages:\n            if counter_clockwise:\n                writer.addPage(page.rotateCounterClockwise(90))\n            else:\n                writer.addPage(page.rotateClockwise(90))\n        else:\n            writer.addPage(page)\n\n    # Open output file or temporary file for writing\n    if output is None:\n        outfile = NamedTemporaryFile(delete=False)\n    else:\n        if not os.path.isfile(output) or overwrite_dlg(output):\n            outfile = open(output, \"wb\")\n        else:\n            return\n\n    # Write to file\n    writer.write(outfile)\n    infile.close()\n    outfile.close()\n\n    # If no output defined move temporary file to input\n    if output is None:\n        if overwrite_dlg(input):\n            os.remove(input)\n            move(outfile.name, input)\n        else:\n            os.remove(outfile.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying pages from the input file in a new output file.", "response": "def pdf_copy(input: str, output: str, pages: [int], yes_to_all=False):\n    \"\"\"\n    Copy pages from the input file in a new output file.\n    :param input: name of the input pdf file\n    :param output: name of the output pdf file\n    :param pages: list containing the page numbers to copy in the new file\n\n    \"\"\"\n    if not os.path.isfile(input):\n        print(\"Error. The file '%s' does not exist.\" % input)\n        return\n\n    if os.path.isfile(output) and not yes_to_all and not overwrite_dlg(output):\n        return\n\n    with open(input, \"rb\") as inputfile:\n        reader = PdfFileReader(inputfile)\n        outputfile = open(output, \"wb\")\n        writer = PdfFileWriter()\n        if pages is None:\n            pages = range(len(reader.pages))\n        else:\n            pages = parse_rangearg(pages, len(reader.pages))\n        for pagenr in sorted(pages):\n            page = reader.getPage(pagenr)\n            writer.addPage(page)\n            writer.write(outputfile)\n        outputfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit the input file in multiple output files.", "response": "def pdf_split(\n    input: str, output: str, stepsize: int = 1, sequence: [int] = None\n):\n    \"\"\"\n    Split the input file in multiple output files\n    :param input: name of the input file\n    :param output: name of the output files\n    :param stepsize: how many pages per file, only if sequence is None\n    :param sequence: list with number of pages per file\n\n    \"\"\"\n    output = output or os.path.splitext(input)[0]\n    if not os.path.isfile(input):\n        print(\"Error. The file '%s' does not exist.\" % input)\n        return\n\n    with open(input, \"rb\") as inputfile:\n        reader = PdfFileReader(inputfile)\n        pagenr = 0\n        outputfile = None\n        if sequence is None:\n            for i, page in enumerate(reader.pages):\n                if not i % stepsize:\n                    pagenr += 1\n                    outputfile = open(output + \"_%i.pdf\" % pagenr, \"wb\")\n                    writer = PdfFileWriter()\n                writer.addPage(page)\n                if not (i + 1) % stepsize:\n                    writer.write(outputfile)\n                    outputfile.close()\n        else:\n            sequence = map(int, sequence)\n            iter_pages = iter(reader.pages)\n            for filenr, pagecount in enumerate(sequence):\n                with open(\n                    output + \"_%i.pdf\" % (filenr + 1), \"wb\"\n                ) as outputfile:\n                    writer = PdfFileWriter()\n                    for i in range(pagecount):\n                        try:\n                            page = next(iter_pages)\n                            writer.addPage(page)\n                        except StopIteration:\n                            writer.write(outputfile)\n                            return\n\n                    writer.write(outputfile)\n\n        if not outputfile.closed:\n            writer.write(outputfile)\n            outputfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nzips two PDF files.", "response": "def pdf_zip(\n    input1: str,\n    input2: str,\n    output: str,\n    delete: bool = False,\n    revert: bool = False,\n):\n    \"\"\"\n    Zip pages of input1 and input2 in one output file. Useful for putting\n    even and odd pages together in one document.\n    :param input1: first input file\n    :param input2: second input file\n    :param output: output file\n    :param delete: if true the input files will be deleted after zipping\n\n    \"\"\"\n    if os.path.isfile(output):\n        ans = input(\n            \"The file '%s' already exists. \"\n            \"Overwrite? Yes/Abort [Y/a]: \" % output\n        ).lower()\n        if ans not in [\"y\", \"\"]:\n            return\n\n    outputfile = open(output, \"wb\")\n    try:\n        f1, f2 = open(input1, \"rb\"), open(input2, \"rb\")\n        r1, r2 = PdfFileReader(f1), PdfFileReader(f2)\n        writer = PdfFileWriter()\n        pages1 = [page for page in r1.pages]\n        pages2 = [page for page in r2.pages]\n        if not revert:\n            for p1, p2 in zip(pages1, pages2):\n                writer.addPage(p1)\n                writer.addPage(p2)\n        else:\n            for p1, p2 in zip(pages1, reversed(pages2)):\n                writer.addPage(p1)\n                writer.addPage(p2)\n        writer.write(outputfile)\n        f1.close()\n        f2.close()\n    except FileNotFoundError as e:\n        print(e.strerror + \": \" + e.filename)\n    finally:\n        outputfile.close()\n\n    if delete:\n        os.remove(input1)\n        os.remove(input2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninserts pages from one file into another.", "response": "def pdf_insert(\n    dest: str,\n    source: str,\n    pages: [str] = None,\n    index: int = None,\n    output: str = None,\n):\n    \"\"\"\n    Insert pages from one file into another.\n    :param dest: Destination file\n    :param source: Source file\n    :param pages: list of page numbers to insert\n    :param index: index in destination file where to insert the pages\n    :param output: output file\n\n    \"\"\"\n    if output is not None and os.path.isfile(output):\n        ans = input(\n            \"The file '%s' already exists. \"\n            \"Overwrite? Yes/Abort [Y/a]: \" % output\n        ).lower()\n        if ans not in [\"y\", \"\"]:\n            return\n\n    writer = PdfFileWriter()\n    # read pages from file1\n    destfile = open(dest, \"rb\")\n    destreader = PdfFileReader(destfile)\n    for page in destreader.pages:\n        writer.addPage(page)\n\n    # read pages from file2\n    srcfile = open(source, \"rb\")\n    srcreader = PdfFileReader(srcfile)\n\n    # if no page numbers are given insert all pages\n    index = limit(index - 1, 0, len(destreader.pages))\n    if pages is None:\n        for i, page in enumerate(srcreader.pages):\n            if index is None:\n                writer.addPage(page)\n            else:\n                writer.insertPage(page, index + i)\n    else:\n        pages = parse_rangearg(pages, len(srcreader.pages))\n        for i, pagenr in enumerate(pages):\n            page = srcreader.getPage(pagenr)\n            if index is None:\n                writer.addPage(page)\n            else:\n                writer.insertPage(page, index + i)\n\n    if output is None:\n        # Write into Temporary File first and then overwrite dest file\n        ans = input(\n            \"Overwrite the file '%s'? Yes/Abort [Y/a]: \" % dest\n        ).lower()\n        if ans in [\"y\", \"\"]:\n            tempfile = NamedTemporaryFile(delete=False)\n            writer.write(tempfile)\n            tempfile.close()\n            move(tempfile.name, dest)\n    else:\n        with open(output, \"wb\") as outfile:\n            writer.write(outfile)\n    destfile.close()\n    srcfile.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pdf_remove(source: str, pages: [str], output: str = None):\n    if output is not None and os.path.isfile(output):\n        if overwrite_dlg(output) is False:\n            return\n\n    writer = PdfFileWriter()\n    srcfile = open(source, \"rb\")\n    srcreader = PdfFileReader(srcfile)\n\n    # Add pages, leave out removed pages\n    pages = parse_rangearg(pages, len(srcreader.pages))\n    for pagenr, page in enumerate(srcreader.pages):\n        if pagenr not in pages:\n            writer.addPage(page)\n\n    # Open output file or temporary file for writing\n    if output is None:\n        outfile = NamedTemporaryFile(delete=False)\n    else:\n        outfile = open(output, \"wb\")\n\n    # Write file and close\n    writer.write(outfile)\n    srcfile.close()\n    outfile.close()\n\n    # Move temporary file to source\n    if output is None:\n        if overwrite_dlg(source):\n            os.remove(source)\n            move(outfile.name, source)\n        else:\n            os.remove(outfile)", "response": "Remove pages from a PDF source file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pdf_add(dest: str, source: str, pages: [str], output: str):\n    if output is not None and os.path.isfile(output):\n        if not overwrite_dlg(output):\n            return\n\n    writer = PdfFileWriter()\n\n    # read pages from destination file\n    destfile = open(dest, \"rb\")\n    destreader = PdfFileReader(destfile)\n    for page in destreader.pages:\n        writer.addPage(page)\n\n    # read pages from source file\n    srcfile = open(source, \"rb\")\n    srcreader = PdfFileReader(srcfile)\n\n    # if no page numbers are given add all pages from source\n    if pages is None:\n        for i, page in enumerate(srcreader.pages):\n            writer.addPage(page)\n    else:\n        pages = parse_rangearg(pages, len(srcreader.pages))\n        for pagenr in pages:\n            page = srcreader.getPage(pagenr)\n            writer.addPage(page)\n\n    if output is None:\n        # Write into Temporary File first and then overwrite dest file\n        if overwrite_dlg(dest):\n            tempfile = NamedTemporaryFile(delete=False)\n            writer.write(tempfile)\n            tempfile.close()\n            destfile.close()\n            srcfile.close()\n            os.remove(dest)\n            move(tempfile.name, dest)\n    else:\n        with open(output, \"wb\") as outfile:\n            writer.write(outfile)\n            destfile.close()\n            srcfile.close()", "response": "Add pages from a source pdf file to an output pdf file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_version():\n    with open('pdftools/__init__.py', 'r') as f:\n        content = f.read()\n\n    version_match = _version_re.search(content)\n    version = str(ast.literal_eval(version_match.group(1)))\n    return version", "response": "Extract the version from the package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_pypiper_args(parser, groups=(\"pypiper\", ), args=None,\n                     required=None, all_args=False):\n    \"\"\"\n    Use this to add standardized pypiper arguments to your python pipeline.\n\n    There are two ways to use `add_pypiper_args`: by specifying argument groups,\n    or by specifying individual arguments. Specifying argument groups will add\n    multiple arguments to your parser; these convenient argument groupings\n    make it easy to add arguments to certain types of pipeline. For example,\n    to make a looper-compatible pipeline, use `groups = [\"pypiper\", \"looper\"]`.\n\n    :param argparse.ArgumentParser parser: ArgumentParser object from a pipeline\n    :param str | Iterable[str] groups: Adds arguments belong to specified group\n        of args. Options: pypiper, config, looper, resources, common, ngs, all.\n    :param str | Iterable[str] args: You may specify a list of specific arguments one by one.\n    :param Iterable[str] required: Arguments to be flagged as 'required' by argparse.\n    :param bool all_args: Whether to include all of pypiper's arguments defined here.\n    :return argparse.ArgumentParser: A new ArgumentParser object, with selected\n        pypiper arguments added\n    \"\"\"\n    args_to_add = _determine_args(\n        argument_groups=groups, arguments=args, use_all_args=all_args)\n    parser = _add_args(parser, args_to_add, required)\n    return parser", "response": "Adds pypiper arguments to a pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_command(chunks):\n\n    if not chunks:\n        raise ValueError(\n            \"No command parts: {} ({})\".format(chunks, type(chunks)))\n\n    if isinstance(chunks, str):\n        return chunks\n\n    parsed_pieces = []\n\n    for cmd_part in chunks:\n        if cmd_part is None:\n            continue\n        try:\n            # Trim just space, not all whitespace.\n            # This prevents damage to an option that specifies,\n            # say, tab as a delimiter.\n            parsed_pieces.append(cmd_part.strip(\" \"))\n        except AttributeError:\n            option, argument = cmd_part\n            if argument is None or argument == \"\":\n                continue\n            option, argument = option.strip(\" \"), str(argument).strip(\" \")\n            parsed_pieces.append(\"{} {}\".format(option, argument))\n\n    return \" \".join(parsed_pieces)", "response": "Builds a single meaningful command from a collection of command parts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef build_sample_paths(sample):\n    for path_name, path in sample.paths.items():\n        print(\"{}: '{}'\".format(path_name, path))\n        base, ext = os.path.splitext(path)\n        if ext:\n            print(\"Skipping file-like: '[}'\".format(path))\n        elif not os.path.isdir(base):\n            os.makedirs(base)", "response": "Build the base folder paths for a Sample."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef checkpoint_filename(checkpoint, pipeline_name=None):\n    # Allow Stage as type for checkpoint parameter's argument without\n    # needing to import here the Stage type from stage.py module.\n    try:\n        base = checkpoint.checkpoint_name\n    except AttributeError:\n        base = translate_stage_name(checkpoint)\n    if pipeline_name:\n        base = \"{}{}{}\".format(\n            pipeline_name, PIPELINE_CHECKPOINT_DELIMITER, base)\n    return base + CHECKPOINT_EXTENSION", "response": "Translate a checkpoint to a filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate filepath for indicated checkpoint.", "response": "def checkpoint_filepath(checkpoint, pm):\n    \"\"\"\n    Create filepath for indicated checkpoint.\n\n    :param str | pypiper.Stage checkpoint: Pipeline phase/stage or one's name\n    :param pypiper.PipelineManager | pypiper.Pipeline pm: manager of a pipeline\n        instance, relevant for output folder path.\n    :return str: standardized checkpoint name for file, plus extension\n    :raise ValueError: if the checkpoint is given as absolute path that does\n        not point within pipeline output folder\n    \"\"\"\n\n    # Handle case in which checkpoint is given not just as a string, but\n    # as a checkpoint-like filename. Don't worry about absolute path status\n    # of a potential filename input, or whether it's in the pipeline's\n    # output folder. That's handled upstream. While this isn't a protected\n    # function, there's no real reason to call this from outside the package.\n    if isinstance(checkpoint, str):\n        if os.path.isabs(checkpoint):\n            if is_in_file_tree(checkpoint, pm.outfolder):\n                return checkpoint\n            else:\n                raise ValueError(\n                    \"Absolute checkpoint path '{}' is not in pipeline output \"\n                    \"folder '{}'\".format(checkpoint, pm.outfolder))\n        _, ext = os.path.splitext(checkpoint)\n        if ext == CHECKPOINT_EXTENSION:\n            return pipeline_filepath(pm, filename=checkpoint)\n\n    # Allow Pipeline as pm type without importing Pipeline.\n    try:\n        pm = pm.manager\n    except AttributeError:\n        pass\n\n    # We want the checkpoint filename itself to become a suffix, with a\n    # delimiter intervening between the pipeline name and the checkpoint\n    # name + extension. This is to handle the case in which a single, e.g.,\n    # sample's output folder is the destination for output from multiple\n    # pipelines, and we thus want to be able to distinguish between\n    # checkpoint files from different pipelines for that sample that may\n    # well define one or more stages with the same name (e.g., trim_reads,\n    # align_reads, etc.)\n    chkpt_name = checkpoint_filename(checkpoint, pipeline_name=pm.name)\n    return pipeline_filepath(pm, filename=chkpt_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_shell(cmd, shell=None):\n    if isinstance(shell, bool):\n        return shell\n    return \"|\" in cmd or \">\" in cmd or r\"*\" in cmd", "response": "Checks whether a command appears to involve a shell process."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether a command appears to contain shell redirection symbol outside of curly brackets.", "response": "def check_shell_redirection(cmd):\n    \"\"\"\n    Determine whether a command appears to contain shell redirection symbol outside of curly brackets\n\n    :param str cmd: Command to investigate.\n    :return bool: Whether the command appears to contain shell redirection.\n    \"\"\"\n    curly_brackets = True\n    while curly_brackets:\n        SRE_match_obj = re.search(r'\\{(.*?)}',cmd)\n        if SRE_match_obj is not None:\n            cmd = cmd[:SRE_match_obj.start()] + cmd[(SRE_match_obj.end()+1):]\n            if re.search(r'\\{(.*?)}',cmd) is None:\n                curly_brackets = False\n        else:\n            curly_brackets = False\n    return \">\" in cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_proc_name(cmd):\n    if isinstance(cmd, Iterable) and not isinstance(cmd, str):\n        cmd = \" \".join(cmd)\n\n    return cmd.split()[0].replace('(', '').replace(')', '')", "response": "Get the representative process name from complex command sequence"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the first value for a particular parameter from the first pool of parameter pools.", "response": "def get_first_value(param, param_pools, on_missing=None, error=True):\n    \"\"\"\n    Get the value for a particular parameter from the first pool in the provided\n    priority list of parameter pools.\n\n    :param str param: Name of parameter for which to determine/fetch value.\n    :param Sequence[Mapping[str, object]] param_pools: Ordered (priority)\n        collection of mapping from parameter name to value; this should be\n        ordered according to descending priority.\n    :param object | function(str) -> object on_missing: default value or\n        action to take if the requested parameter is missing from all of the\n        pools. If a callable, it should return a value when passed the\n        requested parameter as the one and only argument.\n    :param bool error: Whether to raise an error if the requested parameter\n        is not mapped to a value AND there's no value or strategy provided\n        with 'on_missing' with which to handle the case of a request for an\n        unmapped parameter.\n    :return object: Value to which the requested parameter first mapped in\n        the (descending) priority collection of parameter 'pools,' or\n        a value explicitly defined or derived with 'on_missing.'\n    :raise KeyError: If the requested parameter is unmapped in all of the\n        provided pools, and the argument to the 'error' parameter evaluates\n        to True.\n    \"\"\"\n\n    # Search for the requested parameter.\n    for pool in param_pools:\n        if param in pool:\n            return pool[param]\n\n    # Raise error if unfound and no strategy or value is provided or handling\n    # unmapped parameter requests.\n    if error and on_missing is None:\n        raise KeyError(\"Unmapped parameter: '{}'\".format(param))\n\n    # Use the value or strategy for handling unmapped parameter case.\n    try:\n        return on_missing(param)\n    except TypeError:\n        if hasattr(on_missing, \"__call__\"):\n            raise TypeError(\n                \"Any callable passed as the action to take when a requested \"\n                \"parameter is missing should accept that parameter and return \"\n                \"a value.\")\n        return on_missing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_in_file_tree(fpath, folder):\n    file_folder, _ = os.path.split(fpath)\n    other_folder = os.path.join(folder, \"\")\n    return other_folder.startswith(file_folder)", "response": "Determines whether a file is in a folder."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining whether indicated file appears to be gzipped FASTQ.", "response": "def is_gzipped_fastq(file_name):\n    \"\"\"\n    Determine whether indicated file appears to be a gzipped FASTQ.\n\n    :param str file_name: Name/path of file to check as gzipped FASTQ.\n    :return bool: Whether indicated file appears to be in gzipped FASTQ format.\n    \"\"\"\n    _, ext = os.path.splitext(file_name)\n    return file_name.endswith(\".fastq.gz\") or file_name.endswith(\".fq.gz\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_lock_name(original_path, path_base_folder):\n    def make_name(p):\n        return p.replace(path_base_folder, \"\").replace(os.sep, \"__\")\n    if isinstance(original_path, str):\n        return make_name(original_path)\n    elif isinstance(original_path, Sequence):\n        return [make_name(p) for p in original_path]\n    raise TypeError(\"Neither string nor other sequence type: {} ({})\".\n                    format(original_path, type(original_path)))", "response": "Create a name for a lock file from an absolute path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_multi_target(target):\n    if target is None or isinstance(target, str):\n        return False\n    elif isinstance(target, Sequence):\n        return len(target) > 1\n    else:\n        raise TypeError(\"Could not interpret argument as a target: {} ({})\".\n                        format(target, type(target)))", "response": "Determines if the pipeline manager s run target is multiple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_stage_name(stage):\n    if isinstance(stage, str):\n        return stage\n    try:\n        return stage.name\n    except AttributeError:\n        try:\n            return stage.__name__\n        except AttributeError:\n            raise TypeError(\"Unsupported stage type: {}\".format(type(stage)))", "response": "Determines the name of a stage."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pipeline_filepath(pm, filename=None, suffix=None):\n\n    if filename is None and suffix is None:\n        raise TypeError(\"Provide filename and/or suffix to create \"\n                        \"path to a pipeline file.\")\n\n    filename = (filename or pm.name) + (suffix or \"\")\n\n    # Note that Pipeline and PipelineManager define the same outfolder.\n    # In fact, a Pipeline just references its manager's outfolder.\n    # So we can handle argument of either type to pm parameter.\n    return filename if os.path.isabs(filename) \\\n        else os.path.join(pm.outfolder, filename)", "response": "Derive path to a file within a managed pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntranslating a stage name into a standardized pipeline phase name.", "response": "def translate_stage_name(stage):\n    \"\"\"\n    Account for potential variability in stage/phase name definition.\n\n    Since a pipeline author is free to name his/her processing phases/stages\n    as desired, but these choices influence file names, enforce some\n    standardization. Specifically, prohibit potentially problematic spaces.\n\n    :param str | pypiper.Stage | function stage: Pipeline stage, its name, or a\n        representative function.\n    :return str: Standardized pipeline phase/stage name.\n    \"\"\"\n    # First ensure that we have text.\n    name = parse_stage_name(stage)\n    # Cast to string to ensure that indexed stages (ints are handled).\n    return str(name).lower().replace(\" \", STAGE_NAME_SPACE_REPLACEMENT)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the arguments to add to a parser (for a pipeline). :param Iterable[str] | str argument_groups: Collection of names of groups of arguments to add to an argument parser. :param Iterable[str] | str arguments: Collection of specific arguments to add to the parser. :param bool use_all_args: Whether to use all arguments defined here. :return set[str]: Collection of (unique) argument names to add to a parser.", "response": "def _determine_args(argument_groups, arguments, use_all_args=False):\n    \"\"\"\n    Determine the arguments to add to a parser (for a pipeline).\n\n    :param Iterable[str] | str argument_groups: Collection of names of groups\n        of arguments to add to an argument parser.\n    :param Iterable[str] | str arguments: Collection of specific arguments to\n        add to the parser.\n    :param bool use_all_args: Whether to use all arguments defined here.\n    :return set[str]: Collection of (unique) argument names to add to a parser.\n    \"\"\"\n\n    if sys.version_info < (3, 3):\n        from collections import Iterable\n    else:\n        from collections.abc import Iterable\n\n    # Define the argument groups.\n    args_by_group = {\n        \"pypiper\": [\"recover\", \"new-start\", \"dirty\", \"force-follow\"],\n        \"config\": [\"config\"],\n        \"checkpoint\": [\"stop-before\", \"stop-after\"],\n        \"resource\": [\"mem\", \"cores\"],\n        \"looper\": [\"config\", \"output-parent\", \"mem\", \"cores\"],\n        \"common\": [\"input\", \"sample-name\"],\n        \"ngs\": [\"sample-name\", \"input\", \"input2\", \"genome\", \"single-or-paired\"]\n    }\n\n    # Handle various types of group specifications.\n    groups = None\n    if use_all_args:\n        groups = args_by_group.keys()\n    elif isinstance(argument_groups, str):\n        groups = [argument_groups]\n    elif isinstance(argument_groups, Iterable):\n        groups = argument_groups\n    elif argument_groups:\n        raise TypeError(\"arguments must be a str or a list.\")\n\n    # Collect the groups of arguments.\n    final_args = list()\n    if groups:\n        for g in groups:\n            try:\n                this_group_args = args_by_group[g]\n            except KeyError:\n                print(\"Skipping undefined pypiper argument group '{}'\".format(g))\n            else:\n                final_args.extend(this_group_args)\n                # final_args |= {this_group_args} if \\\n                #     isinstance(this_group_args, str) else set(this_group_args)\n\n    # Handle various types of specific, individual argument specifications.\n    if isinstance(arguments, str):\n        final_args.append(arguments)\n    elif isinstance(arguments, Iterable):\n        final_args.extend(arguments)\n    elif arguments:\n        raise TypeError(\"arguments must be a str or a list.\")\n\n    return uniqify(final_args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _add_args(parser, args, required):\n\n    import copy\n\n    required = required or []\n\n    # Determine the default pipeline config file.\n    pipeline_script = os.path.basename(sys.argv[0])\n    default_config, _ = os.path.splitext(pipeline_script)\n    default_config += \".yaml\"\n\n    # Define the arguments.\n    argument_data = {\n        \"recover\":\n            (\"-R\", {\"action\": \"store_true\",\n                    \"help\": \"Overwrite locks to recover from previous failed run\"}),\n        \"new-start\":\n            (\"-N\", {\"action\": \"store_true\",\n                    \"help\": \"Overwrite all results to start a fresh run\"}),\n        \"dirty\":\n            (\"-D\", {\"action\": \"store_true\",\n                    \"help\": \"Don't auto-delete intermediate files\"}),\n        \"force-follow\":\n            (\"-F\", {\"action\": \"store_true\",\n                    \"help\": \"Always run 'follow' commands\"}),\n        \"start-point\":\n            {\"help\": \"Name of pipeline stage at which to begin\"},\n        \"stop-before\":\n            {\"help\": \"Name of pipeline stage at which to stop \"\n                     \"(exclusive, i.e. not run)\"},\n        \"stop-after\":\n            {\"help\": \"Name of pipeline stage at which to stop \"\n                     \"(inclusive, i.e. run)\"},\n        \"config\":\n            (\"-C\", {\"dest\": \"config_file\", \"metavar\": \"CONFIG_FILE\",\n                    \"default\": default_config,\n                    \"help\": \"Pipeline configuration file (YAML). \"\n                            \"Relative paths are with respect to the \"\n                            \"pipeline script.\"}),\n        \"sample-name\":\n            (\"-S\", {\"metavar\": \"SAMPLE_NAME\",\n                    \"help\": \"Name for sample to run\"}),\n        \"output-parent\":\n            (\"-O\", {\"metavar\": \"PARENT_OUTPUT_FOLDER\",\n                    \"help\": \"Parent output directory of project\"}),\n        \"cores\":\n            (\"-P\", {\"type\": int, \"default\": 1, \"metavar\": \"NUMBER_OF_CORES\",\n                    \"help\": \"Number of cores for parallelized processes\"}),\n        \"mem\":\n            (\"-M\", {\"default\": \"4000\", \"metavar\": \"MEMORY_LIMIT\",\n                    \"help\": \"Memory limit for processes accepting such. \"\n                    \"Default units are megabytes unless specified \"\n                    \"using the suffix [K|M|G|T].\"}),\n        \"input\":\n            (\"-I\", {\"nargs\": \"+\", \"metavar\": \"INPUT_FILES\",\n                    \"help\": \"One or more primary input files\"}),\n        \"input2\":\n            (\"-I2\", {\"nargs\": \"*\", \"metavar\": \"INPUT_FILES2\",\n                     \"help\": \"Secondary input files, such as read2\"}),\n        \"genome\":\n            (\"-G\", {\"dest\": \"genome_assembly\",\n                    \"help\": \"Identifier for genome assembly\"}),\n        \"single-or-paired\":\n            (\"-Q\", {\"default\": \"single\",\n                    \"help\": \"Single- or paired-end sequencing protocol\"})\n    }\n\n    if len(required) > 0:\n        required_named = parser.add_argument_group('required named arguments')\n\n    # Configure the parser for each argument.\n    for arg in args:\n        try:\n            argdata = copy.deepcopy(argument_data[arg])\n        except KeyError:\n            print(\"Skipping undefined pypiper argument: '{}'\".format(arg))\n            continue\n        if isinstance(argdata, dict):\n            short_opt = None\n        else:\n            try:\n                short_opt, argdata = argdata\n            except ValueError:\n                raise TypeError(\n                    \"Option name must map to dict or two-tuple (short \"\n                    \"name and dict) of argument command-line argument \"\n                    \"specification data.\")\n\n        argdata[\"required\"] = arg in required\n\n        long_opt = \"--{}\".format(arg)\n        opts = (short_opt, long_opt) if short_opt else (long_opt, )\n        if arg in required:\n            required_named.add_argument(*opts, **argdata)\n        else:\n            parser.add_argument(*opts, **argdata)\n\n    return parser", "response": "Add new arguments to an ArgumentParser."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nensures that paths to folders exist.", "response": "def _ensure_folders(self, *paths):\n        \"\"\"\n        Ensure that paths to folder(s) exist.\n\n        Some command-line tools will not attempt to create folder(s) needed\n        for output path to exist. They instead assume that they already are\n        present and will fail if that assumption does not hold.\n\n        :param Iterable[str] paths: Collection of path for which\n        \"\"\"\n        for p in paths:\n            # Only provide assurance for absolute paths.\n            if not p or not os.path.isabs(p):\n                continue\n            # See if what we're assuring is file- or folder-like.\n            fpath, fname = os.path.split(p)\n            base, ext = os.path.splitext(fname)\n            # If there's no extension, ensure that we have the whole path.\n            # Otherwise, just ensure that we have path to file's folder.\n            self.make_dir(fpath if ext else p)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if command can be called.", "response": "def check_command(self, command):\n        \"\"\"\n        Check if command can be called.\n        \"\"\"\n\n        # Use `command` to see if command is callable, store exit code\n        code = os.system(\"command -v {0} >/dev/null 2>&1 || {{ exit 1; }}\".format(command))\n\n        # If exit code is not 0, report which command failed and return False, else return True\n        if code != 0:\n            print(\"Command is not callable: {0}\".format(command))\n            return False\n        else:\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets size of all files in string ( space - separated ) in megabytes ( Mb.", "response": "def get_file_size(self, filenames):\n        \"\"\"\n        Get size of all files in string (space-separated) in megabytes (Mb).\n\n        :param str filenames: a space-separated string of filenames\n        \"\"\"\n        # use (1024 ** 3) for gigabytes\n        # equivalent to: stat -Lc '%s' filename\n\n        # If given a list, recurse through it.\n        if type(filenames) is list:\n            return sum([self.get_file_size(filename) for filename in filenames])\n\n        return round(sum([float(os.stat(f).st_size) for f in filenames.split(\" \")]) / (1024 ** 2), 4)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert BAM file to FASTQ.", "response": "def bam2fastq(self, input_bam, output_fastq,\n                  output_fastq2=None, unpaired_fastq=None):\n        \"\"\"\n        Create command to convert BAM(s) to FASTQ(s).\n\n        :param str input_bam: Path to sequencing reads file to convert\n        :param output_fastq: Path to FASTQ to write\n        :param output_fastq2: Path to (R2) FASTQ to write\n        :param unpaired_fastq: Path to unpaired FASTQ to write\n        :return str: Command to convert BAM(s) to FASTQ(s)\n        \"\"\"\n        self._ensure_folders(output_fastq, output_fastq2, unpaired_fastq)\n        cmd = self.tools.java + \" -Xmx\" + self.pm.javamem\n        cmd += \" -jar \" + self.tools.picard + \" SamToFastq\"\n        cmd += \" INPUT={0}\".format(input_bam)\n        cmd += \" FASTQ={0}\".format(output_fastq)\n        if output_fastq2 is not None and unpaired_fastq is not None:\n            cmd += \" SECOND_END_FASTQ={0}\".format(output_fastq2)\n            cmd += \" UNPAIRED_FASTQ={0}\".format(unpaired_fastq)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bam_to_fastq(self, bam_file, out_fastq_pre, paired_end):\n        self.make_sure_path_exists(os.path.dirname(out_fastq_pre))\n        cmd = self.tools.java + \" -Xmx\" + self.pm.javamem\n        cmd += \" -jar \" + self.tools.picard + \" SamToFastq\"\n        cmd += \" I=\" + bam_file\n        cmd += \" F=\" + out_fastq_pre + \"_R1.fastq\"\n        if paired_end:\n            cmd += \" F2=\" + out_fastq_pre + \"_R2.fastq\"\n        cmd += \" INCLUDE_NON_PF_READS=true\"\n        cmd += \" QUIET=true\"\n        cmd += \" VERBOSITY=ERROR\"\n        cmd += \" VALIDATION_STRINGENCY=SILENT\"\n        return cmd", "response": "Build command to convert BAM file to FASTQ file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bam_to_fastq_awk(self, bam_file, out_fastq_pre, paired_end):\n        self.make_sure_path_exists(os.path.dirname(out_fastq_pre))\n        fq1 = out_fastq_pre + \"_R1.fastq\"\n        if paired_end:\n            fq2 = out_fastq_pre + \"_R2.fastq\"\n            cmd = self.tools.samtools + \" view \" + bam_file + \" | awk '\"\n            cmd += r'{ if (NR%2==1) print \"@\"$1\"/1\\n\"$10\"\\n+\\n\"$11 > \"' + fq1 + '\";'\n            cmd += r' else print \"@\"$1\"/2\\n\"$10\"\\n+\\n\"$11 > \"' + fq2 + '\"; }'\n            cmd += \"'\"  # end the awk command\n        else:\n            fq2 = None\n            cmd = self.tools.samtools + \" view \" + bam_file + \" | awk '\"\n            cmd += r'{ print \"@\"$1\"\\n\"$10\"\\n+\\n\"$11 > \"' + fq1 + '\"; }'\n            cmd += \"'\"\n        return cmd, fq1, fq2", "response": "This function converts a BAM file to fastq files using awk."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts bam to fastq using bedtools", "response": "def bam_to_fastq_bedtools(self, bam_file, out_fastq_pre, paired_end):\n        \"\"\"\n        Converts bam to fastq; A version using bedtools\n        \"\"\"\n        self.make_sure_path_exists(os.path.dirname(out_fastq_pre))\n        fq1 = out_fastq_pre + \"_R1.fastq\"\n        fq2 = None\n        cmd = self.tools.bedtools + \" bamtofastq -i \" + bam_file + \" -fq \" + fq1 + \".fastq\"\n        if paired_end:\n            fq2 = out_fastq_pre + \"_R2.fastq\"\n            cmd += \" -fq2 \" + fq2\n\n        return cmd, fq1, fq2"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the extension of the input file. Assumes you re using either. bam or. fastq. gz. fq or. fq. gz. gz.", "response": "def get_input_ext(self, input_file):\n        \"\"\"\n        Get the extension of the input_file. Assumes you're using either\n        .bam or .fastq/.fq or .fastq.gz/.fq.gz.\n        \"\"\"\n        if input_file.endswith(\".bam\"):\n            input_ext = \".bam\"\n        elif input_file.endswith(\".fastq.gz\") or input_file.endswith(\".fq.gz\"):\n            input_ext = \".fastq.gz\"\n        elif input_file.endswith(\".fastq\") or input_file.endswith(\".fq\"):\n            input_ext = \".fastq\"\n        else:\n            errmsg = \"'{}'; this pipeline can only deal with .bam, .fastq, \" \\\n                     \"or .fastq.gz files\".format(input_file)\n            raise UnsupportedFiletypeException(errmsg)\n        return input_ext"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge_or_link(self, input_args, raw_folder, local_base=\"sample\"):\n        self.make_sure_path_exists(raw_folder)\n\n        if not isinstance(input_args, list):\n            raise Exception(\"Input must be a list\")\n\n        if any(isinstance(i, list) for i in input_args):\n            # We have a list of lists. Process each individually.\n            local_input_files = list()\n            n_input_files = len(filter(bool, input_args))\n            print(\"Number of input file sets:\\t\\t\" + str(n_input_files))\n\n            for input_i, input_arg in enumerate(input_args):\n                # Count how many non-null items there are in the list;\n                # we only append _R1 (etc.) if there are multiple input files.\n                if n_input_files > 1:\n                    local_base_extended = local_base + \"_R\" + str(input_i + 1)\n                else:\n                    local_base_extended = local_base\n                if input_arg:\n                    out = self.merge_or_link(\n                            input_arg, raw_folder, local_base_extended)\n\n                    print(\"Local input file: '{}'\".format(out))\n                    # Make sure file exists:\n                    if not os.path.isfile(out):\n                        print(\"Not a file: '{}'\".format(out))\n\n                    local_input_files.append(out)\n\n            return local_input_files\n\n        else:\n            # We have a list of individual arguments. Merge them.\n\n            if len(input_args) == 1:\n                # Only one argument in this list. A single input file; we just link\n                # it, regardless of file type:\n                # Pull the value out of the list\n                input_arg = input_args[0]\n                input_ext = self.get_input_ext(input_arg)\n\n                # Convert to absolute path\n                if not os.path.isabs(input_arg):\n                    input_arg = os.path.abspath(input_arg)\n\n                # Link it to into the raw folder\n                local_input_abs = os.path.join(raw_folder, local_base + input_ext)\n                self.pm.run(\n                    \"ln -sf \" + input_arg + \" \" + local_input_abs,\n                    target=local_input_abs,\n                    shell=True)\n                # return the local (linked) filename absolute path\n                return local_input_abs\n\n            else:\n                # Otherwise, there are multiple inputs.\n                # If more than 1 input file is given, then these are to be merged\n                # if they are in bam format.\n                if all([self.get_input_ext(x) == \".bam\" for x in input_args]):\n                    sample_merged = local_base + \".merged.bam\"\n                    output_merge = os.path.join(raw_folder, sample_merged)\n                    cmd = self.merge_bams(input_args, output_merge)\n                    self.pm.run(cmd, output_merge)\n                    cmd2 = self.validate_bam(output_merge)\n                    self.pm.run(cmd, output_merge, nofail=True)\n                    return output_merge\n\n                # if multiple fastq\n                if all([self.get_input_ext(x) == \".fastq.gz\" for x in input_args]):\n                    sample_merged_gz = local_base + \".merged.fastq.gz\"\n                    output_merge_gz = os.path.join(raw_folder, sample_merged_gz)\n                    #cmd1 = self.ziptool + \"-d -c \" + \" \".join(input_args) + \" > \" + output_merge\n                    #cmd2 = self.ziptool + \" \" + output_merge\n                    #self.pm.run([cmd1, cmd2], output_merge_gz)\n                    # you can save yourself the decompression/recompression:\n                    cmd = \"cat \" + \" \".join(input_args) + \" > \" + output_merge_gz \n                    self.pm.run(cmd, output_merge_gz)\n                    return output_merge_gz\n\n                if all([self.get_input_ext(x) == \".fastq\" for x in input_args]):\n                    sample_merged = local_base + \".merged.fastq\"\n                    output_merge = os.path.join(raw_folder, sample_merged)\n                    cmd = \"cat \" + \" \".join(input_args) + \" > \" + output_merge\n                    self.pm.run(cmd, output_merge)\n                    return output_merge\n\n                # At this point, we don't recognize the input file types or they\n                # do not match.\n                raise NotImplementedError(\n                        \"Input files must be of the same type, and can only \"\n                        \"merge bam or fastq.\")", "response": "This function merges the input files into a local file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef input_to_fastq(\n        self, input_file, sample_name,\n        paired_end, fastq_folder, output_file=None, multiclass=False):\n        \"\"\"\n        Builds a command to convert input file to fastq, for various inputs.\n\n        Takes either .bam, .fastq.gz, or .fastq input and returns\n        commands that will create the .fastq file, regardless of input type.\n        This is useful to made your pipeline easily accept any of these input\n        types seamlessly, standardizing you to the fastq which is still the\n        most common format for adapter trimmers, etc.\n\n        It will place the output fastq file in given `fastq_folder`.\n\n        :param str input_file: filename of input you want to convert to fastq\n        :return str: A command (to be run with PipelineManager) that will ensure\n            your fastq file exists.\n        \"\"\"\n\n        fastq_prefix = os.path.join(fastq_folder, sample_name)\n        self.make_sure_path_exists(fastq_folder)\n\n        # this expects a list; if it gets a string, convert it to a list.\n        if type(input_file) != list:\n            input_file = [input_file]\n\n        if len(input_file) > 1:\n            cmd = []\n            output_file = []\n            for in_i, in_arg in enumerate(input_file):\n                output = fastq_prefix + \"_R\" + str(in_i + 1) + \".fastq\"\n                result_cmd, uf, result_file = \\\n                    self.input_to_fastq(in_arg, sample_name, paired_end,\n                                        fastq_folder, output, multiclass=True)\n                cmd.append(result_cmd)\n                output_file.append(result_file)\n\n        else:\n            # There was only 1 input class.\n            # Convert back into a string\n            input_file = input_file[0]\n            if not output_file:\n                output_file = fastq_prefix + \"_R1.fastq\"\n            input_ext = self.get_input_ext(input_file)\n\n            if input_ext == \".bam\":\n                print(\"Found .bam file\")\n                #cmd = self.bam_to_fastq(input_file, fastq_prefix, paired_end)\n                cmd, fq1, fq2 = self.bam_to_fastq_awk(input_file, fastq_prefix, paired_end)\n                # pm.run(cmd, output_file, follow=check_fastq)\n            elif input_ext == \".fastq.gz\":\n                print(\"Found .fastq.gz file\")\n                if paired_end and not multiclass:\n                    # For paired-end reads in one fastq file, we must split the file into 2.\n                    script_path = os.path.join(\n                            self.tools.scripts_dir, \"fastq_split.py\")\n                    cmd = self.tools.python + \" -u \" + script_path\n                    cmd += \" -i \" + input_file\n                    cmd += \" -o \" + fastq_prefix\n                    # Must also return the set of output files\n                    output_file = [fastq_prefix + \"_R1.fastq\", fastq_prefix + \"_R2.fastq\"]\n                else:\n                    # For single-end reads, we just unzip the fastq.gz file.\n                    # or, paired-end reads that were already split.\n                    cmd = self.ziptool + \" -d -c \" + input_file + \" > \" + output_file\n                    # a non-shell version\n                    # cmd1 = \"gunzip --force \" + input_file\n                    # cmd2 = \"mv \" + os.path.splitext(input_file)[0] + \" \" + output_file\n                    # cmd = [cmd1, cmd2]\n            elif input_ext == \".fastq\":\n                cmd = \"ln -sf \" + input_file + \" \" + output_file\n                print(\"Found .fastq file; no conversion necessary\")\n\n        return [cmd, fastq_prefix, output_file]", "response": "This function converts the input file to fastq."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a follow sanity-check function to be run after a fastq conversion. Run following a command that will produce the fastq files. This function will make sure any input files have the same number of reads as the output files.", "response": "def check_fastq(self, input_files, output_files, paired_end):\n        \"\"\"\n        Returns a follow sanity-check function to be run after a fastq conversion.\n        Run following a command that will produce the fastq files.\n\n        This function will make sure any input files have the same number of reads as the\n        output files.\n        \"\"\"\n\n        # Define a temporary function which we will return, to be called by the\n        # pipeline.\n        # Must define default parameters here based on the parameters passed in. This locks\n        # these values in place, so that the variables will be defined when this function\n        # is called without parameters as a follow function by pm.run.\n\n        # This is AFTER merge, so if there are multiple files it means the\n        # files were split into read1/read2; therefore I must divide by number\n        # of files for final reads.\n        def temp_func(input_files=input_files, output_files=output_files,\n                      paired_end=paired_end):\n\n            if type(input_files) != list:\n                input_files = [input_files]\n            if type(output_files) != list:\n                output_files = [output_files]\n\n            print(input_files)\n            print(output_files)\n\n            n_input_files = len(filter(bool, input_files))\n\n            total_reads = sum([int(self.count_reads(input_file, paired_end))\n                               for input_file in input_files])\n            raw_reads = total_reads / n_input_files\n            self.pm.report_result(\"Raw_reads\", str(raw_reads))\n\n            total_fastq_reads = sum(\n                [int(self.count_reads(output_file, paired_end))\n                 for output_file in output_files])\n            fastq_reads = total_fastq_reads / n_input_files\n\n            self.pm.report_result(\"Fastq_reads\", fastq_reads)\n            input_ext = self.get_input_ext(input_files[0])\n            # We can only assess pass filter reads in bam files with flags.\n            if input_ext == \".bam\":\n                num_failed_filter = sum(\n                    [int(self.count_fail_reads(f, paired_end))\n                     for f in input_files])\n                pf_reads = int(raw_reads) - num_failed_filter\n                self.pm.report_result(\"PF_reads\", str(pf_reads))\n            if fastq_reads != int(raw_reads):\n                raise Exception(\"Fastq conversion error? Number of reads \"\n                                \"doesn't match unaligned bam\")\n\n            return fastq_reads\n\n        return temp_func"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate read trimming and optionally run fastqc if necessary.", "response": "def check_trim(self, trimmed_fastq, paired_end, trimmed_fastq_R2=None, fastqc_folder=None):\n        \"\"\"\n        Build function to evaluate read trimming, and optionally run fastqc.\n\n        This is useful to construct an argument for the 'follow' parameter of\n        a PipelineManager's 'run' method.\n\n        :param str trimmed_fastq: Path to trimmed reads file.\n        :param bool paired_end: Whether the processing is being done with\n            paired-end sequencing data.\n        :param str trimmed_fastq_R2: Path to read 2 file for the paired-end case.\n        :param str fastqc_folder: Path to folder within which to place fastqc\n            output files; if unspecified, fastqc will not be run.\n        :return callable: Function to evaluate read trimming and possibly run\n            fastqc.\n        \"\"\"\n\n        def temp_func():\n\n            print(\"Evaluating read trimming\")\n\n            if paired_end and not trimmed_fastq_R2:\n                print(\"WARNING: specified paired-end but no R2 file\")\n\n            n_trim = float(self.count_reads(trimmed_fastq, paired_end))\n            self.pm.report_result(\"Trimmed_reads\", int(n_trim))\n            try:\n                rr = float(self.pm.get_stat(\"Raw_reads\"))\n            except:\n                print(\"Can't calculate trim loss rate without raw read result.\")\n            else:\n                self.pm.report_result(\n                    \"Trim_loss_rate\", round((rr - n_trim) * 100 / rr, 2))\n\n            # Also run a fastqc (if installed/requested)\n            if fastqc_folder:\n                if fastqc_folder and os.path.isabs(fastqc_folder):\n                    self.make_sure_path_exists(fastqc_folder)\n                cmd = self.fastqc(trimmed_fastq, fastqc_folder)\n                self.pm.run(cmd, lock_name=\"trimmed_fastqc\", nofail=True)\n                fname, ext = os.path.splitext(os.path.basename(trimmed_fastq))\n                fastqc_html = os.path.join(fastqc_folder, fname + \"_fastqc.html\")\n                self.pm.report_object(\"FastQC report r1\", fastqc_html)\n\n                if paired_end and trimmed_fastq_R2:\n                    cmd = self.fastqc(trimmed_fastq_R2, fastqc_folder)\n                    self.pm.run(cmd, lock_name=\"trimmed_fastqc_R2\", nofail=True)\n                    fname, ext = os.path.splitext(os.path.basename(trimmed_fastq_R2))\n                    fastqc_html = os.path.join(fastqc_folder, fname + \"_fastqc.html\")\n                    self.pm.report_object(\"FastQC report r2\", fastqc_html)\n\n        return temp_func"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_bam(self, input_bam):\n        cmd = self.tools.java + \" -Xmx\" + self.pm.javamem\n        cmd += \" -jar \" + self.tools.picard + \" ValidateSamFile\"\n        cmd += \" INPUT=\" + input_bam\n        return cmd", "response": "Wrapper for Picard s ValidateSamFile. validate_bam wrapper for Picard s ValidateSamFile. validate_bam"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_bams(self, input_bams, merged_bam, in_sorted=\"TRUE\", tmp_dir=None):\n        if not len(input_bams) > 1:\n            print(\"No merge required\")\n            return 0\n\n        outdir, _ = os.path.split(merged_bam)\n        if outdir and not os.path.exists(outdir):\n            print(\"Creating path to merge file's folder: '{}'\".format(outdir))\n            os.makedirs(outdir)\n\n        # Handle more intuitive boolean argument.\n        if in_sorted in [False, True]:\n            in_sorted = \"TRUE\" if in_sorted else \"FALSE\"\n\n        input_string = \" INPUT=\" + \" INPUT=\".join(input_bams)\n        cmd = self.tools.java + \" -Xmx\" + self.pm.javamem\n        cmd += \" -jar \" + self.tools.picard + \" MergeSamFiles\"\n        cmd += input_string\n        cmd += \" OUTPUT=\" + merged_bam\n        cmd += \" ASSUME_SORTED=\" + str(in_sorted)\n        cmd += \" CREATE_INDEX=TRUE\"\n        cmd += \" VALIDATION_STRINGENCY=SILENT\"\n        if tmp_dir:\n            cmd += \" TMP_DIR=\" + tmp_dir\n        return cmd", "response": "Combine multiple bams into one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_fastq(self, inputs, output, run=False, remove_inputs=False):\n        if remove_inputs and not run:\n            raise ValueError(\"Can't delete files if command isn't run\")\n        cmd = \"cat {} > {}\".format(\" \".join(inputs), output)\n        if run:\n            subprocess.check_call(cmd.split(), shell=True)\n            if remove_inputs:\n                cmd = \"rm {}\".format(\" \".join(inputs))\n                subprocess.check_call(cmd.split(), shell=True)\n        else:\n            return cmd", "response": "Merge FASTQ files into one."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_lines(self, file_name):\n        x = subprocess.check_output(\"wc -l \" + file_name + \" | sed -E 's/^[[:space:]]+//' | cut -f1 -d' '\", shell=True)\n        return x.strip()", "response": "Uses the command - line utility wc to count the number of lines in a file. For MacOS this is not a good idea."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_chrs_from_bam(self, file_name):\n        x = subprocess.check_output(self.tools.samtools + \" view -H \" + file_name + \" | grep '^@SQ' | cut -f2| sed s'/SN://'\", shell=True)\n        # Chromosomes will be separated by newlines; split into list to return\n        return x.split()", "response": "Uses samtools to grab the chromosomes from the header that are contained\n        in this bam file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef count_unique_reads(self, file_name, paired_end):\n        if file_name.endswith(\"sam\"):\n            param = \"-S\"\n        if file_name.endswith(\"bam\"):\n            param = \"\"\n        if paired_end:\n            r1 = self.samtools_view(file_name, param=param + \" -f64\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n            r2 = self.samtools_view(file_name, param=param + \" -f128\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n        else:\n            r1 = self.samtools_view(file_name, param=param + \"\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n            r2 = 0\n        return int(r1) + int(r2)", "response": "Counts the number of unique reads in a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_unique_mapped_reads(self, file_name, paired_end):\n\n        _, ext = os.path.splitext(file_name)\n        ext = ext.lower()\n\n        if ext == \".sam\":\n            param = \"-S -F4\"\n        elif ext == \"bam\":\n            param = \"-F4\"\n        else:\n            raise ValueError(\"Not a SAM or BAM: '{}'\".format(file_name))\n\n        if paired_end: \n            r1 = self.samtools_view(file_name, param=param + \" -f64\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n            r2 = self.samtools_view(file_name, param=param + \" -f128\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n        else:\n            r1 = self.samtools_view(file_name, param=param + \"\", postpend=\" | cut -f1 | sort -k1,1 -u | wc -l | sed -E 's/^[[:space:]]+//'\")\n            r2 = 0\n\n        return int(r1) + int(r2)", "response": "Returns the number of unique mapped reads for a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncount the number of reads with the specified flag.", "response": "def count_flag_reads(self, file_name, flag, paired_end):\n        \"\"\"\n        Counts the number of reads with the specified flag.\n\n        :param str file_name: name of reads file\n        :param str flag: sam flag value to be read\n        :param bool paired_end: This parameter is ignored; samtools automatically correctly responds depending\n            on the data in the bamfile. We leave the option here just for consistency, since all the other\n            counting functions require the parameter. This makes it easier to swap counting functions during\n            pipeline development.\n        \"\"\"\n\n        param = \" -c -f\" + str(flag)\n        if file_name.endswith(\"sam\"):\n            param += \" -S\"\n        return self.samtools_view(file_name, param=param)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef count_uniquelymapping_reads(self, file_name, paired_end):\n        param = \" -c -F256\"\n        if file_name.endswith(\"sam\"):\n            param += \" -S\"\n        return self.samtools_view(file_name, param=param)", "response": "Counts the number of reads that mapped to a unique position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun samtools view on the specified file", "response": "def samtools_view(self, file_name, param, postpend=\"\"):\n        \"\"\"\n        Run samtools view, with flexible parameters and post-processing.\n\n        This is used internally to implement the various count_reads functions.\n\n        :param str file_name: file_name\n        :param str param: String of parameters to pass to samtools view\n        :param str postpend: String to append to the samtools command;\n            useful to add cut, sort, wc operations to the samtools view output.\n        \"\"\"\n        cmd = \"{} view {} {} {}\".format(\n                self.tools.samtools, param, file_name, postpend)\n        return subprocess.check_output(cmd, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncounts the number of reads in a file.", "response": "def count_reads(self, file_name, paired_end):\n        \"\"\"\n        Count reads in a file.\n\n        Paired-end reads count as 2 in this function.\n        For paired-end reads, this function assumes that the reads are split\n        into 2 files, so it divides line count by 2 instead of 4.\n        This will thus give an incorrect result if your paired-end fastq files\n        are in only a single file (you must divide by 2 again).\n\n        :param str file_name: Name/path of file whose reads are to be counted.\n        :param bool paired_end: Whether the file contains paired-end reads.\n        \"\"\"\n\n        _, ext = os.path.splitext(file_name)\n        if not (is_sam_or_bam(file_name) or is_fastq(file_name)):\n            # TODO: make this an exception and force caller to handle that\n            # rather than relying on knowledge of possibility of negative value.\n            return -1\n\n        if is_sam_or_bam(file_name):\n            param_text = \"-c\" if ext == \".bam\" else \"-c -S\"\n            return self.samtools_view(file_name, param=param_text)\n        else:\n            num_lines = self.count_lines_zip(file_name) \\\n                    if is_gzipped_fastq(file_name) \\\n                    else self.count_lines(file_name)\n            divisor = 2 if paired_end else 4\n            return int(num_lines) / divisor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef count_concordant(self, aligned_bam):\n        cmd = self.tools.samtools + \" view \" + aligned_bam + \" | \"\n        cmd += \"grep 'YT:Z:CP'\" + \" | uniq -u | wc -l | sed -E 's/^[[:space:]]+//'\"\n        \n        return subprocess.check_output(cmd, shell=True)", "response": "Count only reads that aligned concordantly exactly 1 time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount the number of mapped reads for a specific file.", "response": "def count_mapped_reads(self, file_name, paired_end):\n        \"\"\"\n        Mapped_reads are not in fastq format, so this one doesn't need to accommodate fastq,\n        and therefore, doesn't require a paired-end parameter because it only uses samtools view.\n        Therefore, it's ok that it has a default parameter, since this is discarded.\n\n        :param str file_name: File for which to count mapped reads.\n        :param bool paired_end: This parameter is ignored; samtools automatically correctly responds depending\n            on the data in the bamfile. We leave the option here just for consistency, since all the other\n            counting functions require the parameter. This makes it easier to swap counting functions during\n            pipeline development.\n        :return int: Either return code from samtools view command, or -1 to indicate an error state.\n        \"\"\"\n        if file_name.endswith(\"bam\"):\n            return self.samtools_view(file_name, param=\"-c -F4\")\n        if file_name.endswith(\"sam\"):\n            return self.samtools_view(file_name, param=\"-c -F4 -S\")\n        return -1"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sam_conversions(self, sam_file, depth=True):\n        cmd = self.tools.samtools + \" view -bS \" + sam_file + \" > \" + sam_file.replace(\".sam\", \".bam\") + \"\\n\"\n        cmd += self.tools.samtools + \" sort \" + sam_file.replace(\".sam\", \".bam\") + \" -o \" + sam_file.replace(\".sam\", \"_sorted.bam\") + \"\\n\"\n        cmd += self.tools.samtools + \" index \" + sam_file.replace(\".sam\", \"_sorted.bam\") + \"\\n\"\n        if depth:\n            cmd += self.tools.samtools + \" depth \" + sam_file.replace(\".sam\", \"_sorted.bam\") + \" > \" + sam_file.replace(\".sam\", \"_sorted.depth\") + \"\\n\"\n        return cmd", "response": "Convert sam files to bam files then sort and index them for later use."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsort and index bam files for later use.", "response": "def bam_conversions(self, bam_file, depth=True):\n        \"\"\"\n        Sort and index bam files for later use.\n\n        :param bool depth: also calculate coverage over each position\n        \"\"\"\n        cmd = self.tools.samtools + \" view -h \" + bam_file + \" > \" + bam_file.replace(\".bam\", \".sam\") + \"\\n\"\n        cmd += self.tools.samtools + \" sort \" + bam_file + \" -o \" + bam_file.replace(\".bam\", \"_sorted.bam\") + \"\\n\"\n        cmd += self.tools.samtools + \" index \" + bam_file.replace(\".bam\", \"_sorted.bam\") + \"\\n\"\n        if depth:\n            cmd += self.tools.samtools + \" depth \" + bam_file.replace(\".bam\", \"_sorted.bam\") + \" > \" + bam_file.replace(\".bam\", \"_sorted.depth\") + \"\\n\"\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fastqc(self, file, output_dir):\n        # You can find the fastqc help with fastqc --help\n        try:\n            pm = self.pm\n        except AttributeError:\n            # Do nothing, this is just for path construction.\n            pass\n        else:\n            if not os.path.isabs(output_dir) and pm is not None:\n                output_dir = os.path.join(pm.outfolder, output_dir)\n        self.make_sure_path_exists(output_dir)\n        return \"{} --noextract --outdir {} {}\".\\\n                format(self.tools.fastqc, output_dir, file)", "response": "Create command to run fastqc on a FASTQ file with sequencing reads."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating pair of commands to run fastqc and organize files. The first command returned is the one that actually runs fastqc when it's executed; the second moves the output files to the output folder for the sample indicated. :param str input_bam: Path to file for which to run fastqc. :param str output_dir: Path to folder in which fastqc output will be written, and within which the sample's output folder lives. :param str sample_name: Sample name, which determines subfolder within output_dir for the fastqc files. :return list[str]: Pair of commands, to run fastqc and then move the files to their intended destination based on sample name.", "response": "def fastqc_rename(self, input_bam, output_dir, sample_name):\n        \"\"\"\n        Create pair of commands to run fastqc and organize files.\n\n        The first command returned is the one that actually runs fastqc when\n        it's executed; the second moves the output files to the output\n        folder for the sample indicated.\n\n        :param str input_bam: Path to file for which to run fastqc.\n        :param str output_dir: Path to folder in which fastqc output will be\n            written, and within which the sample's output folder lives.\n        :param str sample_name: Sample name, which determines subfolder within\n            output_dir for the fastqc files.\n        :return list[str]: Pair of commands, to run fastqc and then move the files to\n            their intended destination based on sample name.\n        \"\"\"\n        cmds = list()\n        initial = os.path.splitext(os.path.basename(input_bam))[0]\n        cmd1 = self.fastqc(input_bam, output_dir)\n        cmds.append(cmd1)\n        cmd2 = \"if [[ ! -s {1}_fastqc.html ]]; then mv {0}_fastqc.html {1}_fastqc.html; mv {0}_fastqc.zip {1}_fastqc.zip; fi\".format(\n            os.path.join(output_dir, initial), os.path.join(output_dir, sample_name))\n        cmds.append(cmd2)\n        return cmds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef samtools_index(self, bam_file):\n        cmd = self.tools.samtools + \" index {0}\".format(bam_file)\n        return cmd", "response": "Index a bam file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef skewer(\n            self, input_fastq1, output_prefix, output_fastq1,\n            log, cpus, adapters, input_fastq2=None, output_fastq2=None):\n        \"\"\"\n        Create commands with which to run skewer.\n\n        :param str input_fastq1: Path to input (read 1) FASTQ file\n        :param str output_prefix: Prefix for output FASTQ file names\n        :param str output_fastq1: Path to (read 1) output FASTQ file\n        :param str log: Path to file to which to write logging information\n        :param int | str cpus: Number of processing cores to allow\n        :param str adapters: Path to file with sequencing adapters\n        :param str input_fastq2: Path to read 2 input FASTQ file\n        :param str output_fastq2: Path to read 2 output FASTQ file\n        :return list[str]: Sequence of commands to run to trim reads with\n            skewer and rename files as desired.\n        \"\"\"\n\n        pe = input_fastq2 is not None\n        mode = \"pe\" if pe else \"any\"\n        cmds = list()\n        cmd1 = self.tools.skewer + \" --quiet\"\n        cmd1 += \" -f sanger\"\n        cmd1 += \" -t {0}\".format(cpus)\n        cmd1 += \" -m {0}\".format(mode)\n        cmd1 += \" -x {0}\".format(adapters)\n        cmd1 += \" -o {0}\".format(output_prefix)\n        cmd1 += \" {0}\".format(input_fastq1)\n        if input_fastq2 is None:\n            cmds.append(cmd1)\n        else:\n            cmd1 += \" {0}\".format(input_fastq2)\n            cmds.append(cmd1)\n        if input_fastq2 is None:\n            cmd2 = \"mv {0} {1}\".format(output_prefix + \"-trimmed.fastq\", output_fastq1)\n            cmds.append(cmd2)\n        else:\n            cmd2 = \"mv {0} {1}\".format(output_prefix + \"-trimmed-pair1.fastq\", output_fastq1)\n            cmds.append(cmd2)\n            cmd3 = \"mv {0} {1}\".format(output_prefix + \"-trimmed-pair2.fastq\", output_fastq2)\n            cmds.append(cmd3)\n        cmd4 = \"mv {0} {1}\".format(output_prefix + \"-trimmed.log\", log)\n        cmds.append(cmd4)\n        return cmds", "response": "This function will run the skewer command that will trim reads with the same name and write the new file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_reads(self, input_bam, output_bam, metrics_file, paired=False, cpus=16, Q=30):\n        nodups = re.sub(\"\\.bam$\", \"\", output_bam) + \".nodups.nofilter.bam\"\n        cmd1 = self.tools.sambamba + \" markdup -t {0} -r --compression-level=0 {1} {2} 2> {3}\".format(cpus, input_bam, nodups, metrics_file)\n        cmd2 = self.tools.sambamba + ' view -t {0} -f bam --valid'.format(cpus)\n        if paired:\n            cmd2 += ' -F \"not (unmapped or mate_is_unmapped) and proper_pair'\n        else:\n            cmd2 += ' -F \"not unmapped'\n        cmd2 += ' and not (secondary_alignment or supplementary) and mapping_quality >= {0}\"'.format(Q)\n        cmd2 += ' {0} |'.format(nodups)\n        cmd2 += self.tools.sambamba + \" sort -t {0} /dev/stdin -o {1}\".format(cpus, output_bam)\n        cmd3 = \"if [[ -s {0} ]]; then rm {0}; fi\".format(nodups)\n        cmd4 = \"if [[ -s {0} ]]; then rm {0}; fi\".format(nodups + \".bai\")\n        return [cmd1, cmd2, cmd3, cmd4]", "response": "Filter the input bam to output bam."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_spp(self, input_bam, output, plot, cpus):\n        base = \"{} {} -rf -savp\".format(self.tools.Rscript, self.tools.spp)\n        cmd = base + \" -savp={} -s=0:5:500 -c={} -out={} -p={}\".format(\n            plot, input_bam, output, cpus)\n        return cmd", "response": "Run the SPP read peak analysis tool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_atacseq_insert_sizes(self, bam, plot, output_csv, max_insert=1500, smallest_insert=30):\n        try:\n            import pysam\n            import numpy as np\n            import matplotlib.mlab as mlab\n            from scipy.optimize import curve_fit\n            from scipy.integrate import simps\n            import matplotlib\n            matplotlib.use('Agg')\n            import matplotlib.pyplot as plt\n        except:\n            print(\"Necessary Python modules couldn't be loaded.\")\n            return\n\n        try:\n            import seaborn as sns\n            sns.set_style(\"whitegrid\")\n        except:\n            pass\n\n        def get_fragment_sizes(bam, max_insert=1500):\n            frag_sizes = list()\n\n            bam = pysam.Samfile(bam, 'rb')\n\n            for i, read in enumerate(bam):\n                if read.tlen < max_insert:\n                    frag_sizes.append(read.tlen)\n            bam.close()\n\n            return np.array(frag_sizes)\n\n        def mixture_function(x, *p):\n            \"\"\"\n            Mixture function to model four gaussian (nucleosomal)\n            and one exponential (nucleosome-free) distributions.\n            \"\"\"\n            m1, s1, w1, m2, s2, w2, m3, s3, w3, m4, s4, w4, q, r = p\n            nfr = expo(x, 2.9e-02, 2.8e-02)\n            nfr[:smallest_insert] = 0\n\n            return (mlab.normpdf(x, m1, s1) * w1 +\n                    mlab.normpdf(x, m2, s2) * w2 +\n                    mlab.normpdf(x, m3, s3) * w3 +\n                    mlab.normpdf(x, m4, s4) * w4 +\n                    nfr)\n\n        def expo(x, q, r):\n            \"\"\"\n            Exponential function.\n            \"\"\"\n            return q * np.exp(-r * x)\n\n        # get fragment sizes\n        frag_sizes = get_fragment_sizes(bam)\n\n        # bin\n        numBins = np.linspace(0, max_insert, max_insert + 1)\n        y, scatter_x = np.histogram(frag_sizes, numBins, density=1)\n        # get the mid-point of each bin\n        x = (scatter_x[:-1] + scatter_x[1:]) / 2\n\n        # Parameters are empirical, need to check\n        paramGuess = [\n            200, 50, 0.7,  # gaussians\n            400, 50, 0.15,\n            600, 50, 0.1,\n            800, 55, 0.045,\n            2.9e-02, 2.8e-02  # exponential\n        ]\n\n        try:\n            popt3, pcov3 = curve_fit(\n                mixture_function, x[smallest_insert:], y[smallest_insert:],\n                p0=paramGuess, maxfev=100000)\n        except:\n            print(\"Nucleosomal fit could not be found.\")\n            return\n\n        m1, s1, w1, m2, s2, w2, m3, s3, w3, m4, s4, w4, q, r = popt3\n\n        # Plot\n        plt.figure(figsize=(12, 12))\n\n        # Plot distribution\n        plt.hist(frag_sizes, numBins, histtype=\"step\", ec=\"k\", normed=1, alpha=0.5)\n\n        # Plot nucleosomal fits\n        plt.plot(x, mlab.normpdf(x, m1, s1) * w1, 'r-', lw=1.5, label=\"1st nucleosome\")\n        plt.plot(x, mlab.normpdf(x, m2, s2) * w2, 'g-', lw=1.5, label=\"2nd nucleosome\")\n        plt.plot(x, mlab.normpdf(x, m3, s3) * w3, 'b-', lw=1.5, label=\"3rd nucleosome\")\n        plt.plot(x, mlab.normpdf(x, m4, s4) * w4, 'c-', lw=1.5, label=\"4th nucleosome\")\n\n        # Plot nucleosome-free fit\n        nfr = expo(x, 2.9e-02, 2.8e-02)\n        nfr[:smallest_insert] = 0\n        plt.plot(x, nfr, 'k-', lw=1.5, label=\"nucleosome-free\")\n\n        # Plot sum of fits\n        ys = mixture_function(x, *popt3)\n        plt.plot(x, ys, 'k--', lw=3.5, label=\"fit sum\")\n\n        plt.legend()\n        plt.xlabel(\"Fragment size (bp)\")\n        plt.ylabel(\"Density\")\n        plt.savefig(plot, bbox_inches=\"tight\")\n\n        # Integrate curves and get areas under curve\n        areas = [\n            [\"fraction\", \"area under curve\", \"max density\"],\n            [\"Nucleosome-free fragments\", simps(nfr), max(nfr)],\n            [\"1st nucleosome\", simps(mlab.normpdf(x, m1, s1) * w1), max(mlab.normpdf(x, m1, s1) * w1)],\n            [\"2nd nucleosome\", simps(mlab.normpdf(x, m2, s2) * w1), max(mlab.normpdf(x, m2, s2) * w2)],\n            [\"3rd nucleosome\", simps(mlab.normpdf(x, m3, s3) * w1), max(mlab.normpdf(x, m3, s3) * w3)],\n            [\"4th nucleosome\", simps(mlab.normpdf(x, m4, s4) * w1), max(mlab.normpdf(x, m4, s4) * w4)]\n        ]\n\n        try:\n            import csv\n\n            with open(output_csv, \"w\") as f:\n                writer = csv.writer(f)\n                writer.writerows(areas)\n        except:\n            pass", "response": "Plot the length of the fragmented sequence in a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bam_to_bigwig(\n            self, input_bam, output_bigwig, genome_sizes, genome,\n            tagmented=False, normalize=False, norm_factor=1000):\n        \"\"\"\n        Convert a BAM file to a bigWig file.\n\n        :param str input_bam: path to BAM file to convert\n        :param str output_bigwig: path to which to write file in bigwig format\n        :param str genome_sizes: path to file with chromosome size information\n        :param str genome: name of genomic assembly\n        :param bool tagmented: flag related to read-generating protocol\n        :param bool normalize: whether to normalize coverage\n        :param int norm_factor: number of bases to use for normalization\n        :return list[str]: sequence of commands to execute\n        \"\"\"\n        # TODO:\n        # addjust fragment length dependent on read size and real fragment size\n        # (right now it asssumes 50bp reads with 180bp fragments)\n        cmds = list()\n        transient_file = os.path.abspath(re.sub(\"\\.bigWig\", \"\", output_bigwig))\n        cmd1 = self.tools.bedtools + \" bamtobed -i {0} |\".format(input_bam)\n        if not tagmented:\n            cmd1 += \" \" + self.tools.bedtools + \" slop -i stdin -g {0} -s -l 0 -r 130 |\".format(genome_sizes)\n            cmd1 += \" fix_bedfile_genome_boundaries.py {0} |\".format(genome)\n        cmd1 += \" \" + self.tools.genomeCoverageBed + \" {0}-bg -g {1} -i stdin > {2}.cov\".format(\n            \"-5 \" if tagmented else \"\",\n            genome_sizes,\n            transient_file\n        )\n        cmds.append(cmd1)\n        if normalize:\n            cmds.append(\"\"\"awk 'NR==FNR{{sum+= $4; next}}{{ $4 = ($4 / sum) * {1}; print}}' {0}.cov {0}.cov | sort -k1,1 -k2,2n > {0}.normalized.cov\"\"\".format(transient_file, norm_factor))\n        cmds.append(self.tools.bedGraphToBigWig + \" {0}{1}.cov {2} {3}\".format(transient_file, \".normalized\" if normalize else \"\", genome_sizes, output_bigwig))\n        # remove tmp files\n        cmds.append(\"if [[ -s {0}.cov ]]; then rm {0}.cov; fi\".format(transient_file))\n        if normalize:\n            cmds.append(\"if [[ -s {0}.normalized.cov ]]; then rm {0}.normalized.cov; fi\".format(transient_file))\n        cmds.append(\"chmod 755 {0}\".format(output_bigwig))\n        return cmds", "response": "Convert a BAM file to a bigWig file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the fraction of reads in peaks defined in input_bam and input_bed.", "response": "def calc_frip(self, input_bam, input_bed, threads=4):\n        \"\"\"\n        Calculate fraction of reads in peaks.\n\n        A file of with a pool of sequencing reads and a file with peak call\n        regions define the operation that will be performed. Thread count\n        for samtools can be specified as well.\n\n        :param str input_bam: sequencing reads file\n        :param str input_bed: file with called peak regions\n        :param int threads: number of threads samtools may use\n        :return float: fraction of reads in peaks defined in given peaks file\n        \"\"\"\n        cmd = self.simple_frip(input_bam, input_bed, threads)\n        return subprocess.check_output(cmd.split(\" \"), shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses MACS2 to call peaks in the specified sample.", "response": "def macs2_call_peaks(\n                self, treatment_bams, output_dir, sample_name, genome,\n                control_bams=None, broad=False, paired=False,\n                pvalue=None, qvalue=None, include_significance=None):\n        \"\"\"\n        Use MACS2 to call peaks.\n\n        :param str | Iterable[str] treatment_bams: Paths to files with data to\n            regard as treatment.\n        :param str output_dir: Path to output folder.\n        :param str sample_name: Name for the sample involved.\n        :param str genome: Name of the genome assembly to use.\n        :param str | Iterable[str] control_bams: Paths to files with data to\n            regard as control\n        :param bool broad: Whether to do broad peak calling.\n        :param bool paired: Whether reads are paired-end\n        :param float | NoneType pvalue: Statistical significance measure to\n            pass as --pvalue to peak calling with MACS\n        :param float | NoneType qvalue: Statistical significance measure to\n            pass as --qvalue to peak calling with MACS\n        :param bool | NoneType include_significance: Whether to pass a\n            statistical significance argument to peak calling with MACS; if\n            omitted, this will be True if the peak calling is broad or if\n            either p-value or q-value is specified; default significance\n            specification is a p-value of 0.001 if a significance is to be\n            specified but no value is provided for p-value or q-value.\n        :return str: Command to run.\n        \"\"\"\n        sizes = {\"hg38\": 2.7e9, \"hg19\": 2.7e9, \"mm10\": 1.87e9, \"dr7\": 1.412e9, \"mm9\": 1.87e9}\n\n        # Whether to specify to MACS2 a value for statistical significance\n        # can be either directly indicated, but if not, it's determined by\n        # whether the mark is associated with broad peaks. By default, we\n        # specify a significance value to MACS2 for a mark associated with a\n        # broad peak.\n        if include_significance is None:\n            include_significance = broad\n\n        cmd = self.tools.macs2 + \" callpeak -t {0}\".format(treatment_bams if type(treatment_bams) is str else \" \".join(treatment_bams))\n\n        if control_bams is not None:\n            cmd += \" -c {0}\".format(control_bams if type(control_bams) is str else \" \".join(control_bams))\n\n        if paired:\n            cmd += \" -f BAMPE \"\n\n        # Additional settings based on whether the marks is associated with\n        # broad peaks\n        if broad:\n            cmd += \" --broad --nomodel --extsize 73\"\n        else:\n            cmd += \" --fix-bimodal --extsize 180 --bw 200\"\n\n        if include_significance:\n            # Allow significance specification via either p- or q-value,\n            # giving preference to q-value if both are provided but falling\n            # back on a default p-value if neither is provided but inclusion\n            # of statistical significance measure is desired.\n            if qvalue is not None:\n                cmd += \" --qvalue {}\".format(qvalue)\n            else:\n                cmd += \" --pvalue {}\".format(pvalue or 0.00001)\n        cmd += \" -g {0} -n {1} --outdir {2}\".format(sizes[genome], sample_name, output_dir)\n\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spp_call_peaks(\n                self, treatment_bam, control_bam, treatment_name, control_name,\n                output_dir, broad, cpus, qvalue=None):\n        \"\"\"\n        Build command for R script to call peaks with SPP.\n\n        :param str treatment_bam: Path to file with data for treatment sample.\n        :param str control_bam: Path to file with data for control sample.\n        :param str treatment_name: Name for the treatment sample.\n        :param str control_name: Name for the control sample.\n        :param str output_dir: Path to folder for output.\n        :param str | bool broad: Whether to specify broad peak calling mode.\n        :param int cpus: Number of cores the script may use.\n        :param float qvalue: FDR, as decimal value\n        :return str: Command to run.\n        \"\"\"\n        broad = \"TRUE\" if broad else \"FALSE\"\n        cmd = self.tools.Rscript + \" `which spp_peak_calling.R` {0} {1} {2} {3} {4} {5} {6}\".format(\n            treatment_bam, control_bam, treatment_name, control_name, broad, cpus, output_dir\n        )\n        if qvalue is not None:\n            cmd += \" {}\".format(qvalue)\n        return cmd", "response": "Build command to call peaks with SPP."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_read_type(self, bam_file, n=10):\n        from collections import Counter\n        try:\n            p = subprocess.Popen([self.tools.samtools, 'view', bam_file],\n                                 stdout=subprocess.PIPE)\n            # Count paired alignments\n            paired = 0\n            read_length = Counter()\n            while n > 0:\n                line = p.stdout.next().split(\"\\t\")\n                flag = int(line[1])\n                read_length[len(line[9])] += 1\n                if 1 & flag:  # check decimal flag contains 1 (paired)\n                    paired += 1\n                n -= 1\n            p.kill()\n        except IOError(\"Cannot read provided bam file.\") as e:\n            raise e\n        # Get most abundant read read_length\n        read_length = sorted(read_length)[-1]\n        # If at least half is paired, return True\n        if paired > (n / 2.):\n            return \"PE\", read_length\n        else:\n            return \"SE\", read_length", "response": "Get the read type and length of a specific read from a BAM file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_bowtie_stats(self, stats_file):\n        import pandas as pd\n        stats = pd.Series(index=[\"readCount\", \"unpaired\", \"unaligned\", \"unique\", \"multiple\", \"alignmentRate\"])\n        try:\n            with open(stats_file) as handle:\n                content = handle.readlines()  # list of strings per line\n        except:\n            return stats\n        # total reads\n        try:\n            line = [i for i in range(len(content)) if \" reads; of these:\" in content[i]][0]\n            stats[\"readCount\"] = re.sub(\"\\D.*\", \"\", content[line])\n            if 7 > len(content) > 2:\n                line = [i for i in range(len(content)) if \"were unpaired; of these:\" in content[i]][0]\n                stats[\"unpaired\"] = re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line]))\n            else:\n                line = [i for i in range(len(content)) if \"were paired; of these:\" in content[i]][0]\n                stats[\"unpaired\"] = stats[\"readCount\"] - int(re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line])))\n            line = [i for i in range(len(content)) if \"aligned 0 times\" in content[i]][0]\n            stats[\"unaligned\"] = re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line]))\n            line = [i for i in range(len(content)) if \"aligned exactly 1 time\" in content[i]][0]\n            stats[\"unique\"] = re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line]))\n            line = [i for i in range(len(content)) if \"aligned >1 times\" in content[i]][0]\n            stats[\"multiple\"] = re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line]))\n            line = [i for i in range(len(content)) if \"overall alignment rate\" in content[i]][0]\n            stats[\"alignmentRate\"] = re.sub(\"\\%.*\", \"\", content[line]).strip()\n        except IndexError:\n            pass\n        return stats", "response": "Parses Bowtie2 stats file and returns pandas Series with values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_duplicate_stats(self, stats_file):\n        import pandas as pd\n        series = pd.Series()\n        try:\n            with open(stats_file) as handle:\n                content = handle.readlines()  # list of strings per line\n        except:\n            return series\n        try:\n            line = [i for i in range(len(content)) if \"single ends (among them \" in content[i]][0]\n            series[\"single-ends\"] = re.sub(\"\\D\", \"\", re.sub(\"\\(.*\", \"\", content[line]))\n            line = [i for i in range(len(content)) if \" end pairs...   done in \" in content[i]][0]\n            series[\"paired-ends\"] = re.sub(\"\\D\", \"\", re.sub(\"\\.\\.\\..*\", \"\", content[line]))\n            line = [i for i in range(len(content)) if \" duplicates, sorting the list...   done in \" in content[i]][0]\n            series[\"duplicates\"] = re.sub(\"\\D\", \"\", re.sub(\"\\.\\.\\..*\", \"\", content[line]))\n        except IndexError:\n            pass\n        return series", "response": "Parses sambamba markdup output returns series with values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_qc(self, qc_file):\n        import pandas as pd\n        series = pd.Series()\n        try:\n            with open(qc_file) as handle:\n                line = handle.readlines()[0].strip().split(\"\\t\")  # list of strings per line\n            series[\"NSC\"] = line[-3]\n            series[\"RSC\"] = line[-2]\n            series[\"qualityTag\"] = line[-1]\n        except:\n            pass\n        return series", "response": "Parse the phantompeakqualtools QC table and return a pandas Series."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount the number of peaks from a sample s peak file.", "response": "def get_peak_number(self, sample):\n        \"\"\"\n        Counts number of peaks from a sample's peak file.\n\n        :param pipelines.Sample sample: Sample object with \"peaks\" attribute.\n        \"\"\"\n        proc = subprocess.Popen([\"wc\", \"-l\", sample.peaks], stdout=subprocess.PIPE)\n        out, err = proc.communicate()\n        sample[\"peakNumber\"] = re.sub(\"\\D.*\", \"\", out)\n        return sample"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_frip(self, sample):\n        import pandas as pd\n        with open(sample.frip, \"r\") as handle:\n            content = handle.readlines()\n        reads_in_peaks = int(re.sub(\"\\D\", \"\", content[0]))\n        mapped_reads = sample[\"readCount\"] - sample[\"unaligned\"]\n        return pd.Series(reads_in_peaks / mapped_reads, index=\"FRiP\")", "response": "Calculates the fraction of reads in peaks for a given sample."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nignores interrupt and termination signals.", "response": "def _ignore_interrupts(self):\n        \"\"\"\n        Ignore interrupt and termination signals. Used as a pre-execution\n        function (preexec_fn) for subprocess.Popen calls that pypiper will\n        control over (i.e., manually clean up).\n        \"\"\"\n        signal.signal(signal.SIGINT, signal.SIG_IGN)\n        signal.signal(signal.SIGTERM, signal.SIG_IGN)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart a new pipeline with the given arguments.", "response": "def start_pipeline(self, args=None, multi=False):\n        \"\"\"\n        Initialize setup. Do some setup, like tee output, print some diagnostics, create temp files.\n        You provide only the output directory (used for pipeline stats, log, and status flag files).\n        \"\"\"\n        # Perhaps this could all just be put into __init__, but I just kind of like the idea of a start function\n        self.make_sure_path_exists(self.outfolder)\n\n        # By default, Pypiper will mirror every operation so it is displayed both\n        # on sys.stdout **and** to a log file. Unfortunately, interactive python sessions\n        # ruin this by interfering with stdout. So, for interactive mode, we do not enable \n        # the tee subprocess, sending all output to screen only.\n        # Starting multiple PipelineManagers in the same script has the same problem, and\n        # must therefore be run in interactive_mode.\n\n        interactive_mode = multi or not hasattr(__main__, \"__file__\")\n        if interactive_mode:\n            print(\"Warning: You're running an interactive python session. \"\n                  \"This works, but pypiper cannot tee the output, so results \"\n                  \"are only logged to screen.\")\n        else:\n            sys.stdout = Unbuffered(sys.stdout)\n            # sys.stdout = os.fdopen(sys.stdout.fileno(), 'w', 0)  # Unbuffer output\n\n            # The tee subprocess must be instructed to ignore TERM and INT signals;\n            # Instead, I will clean up this process in the signal handler functions.\n            # This is required because otherwise, if pypiper receives a TERM or INT,\n            # the tee will be automatically terminated by python before I have a chance to\n            # print some final output (for example, about when the process stopped),\n            # and so those things don't end up in the log files because the tee\n            # subprocess is dead. Instead, I will handle the killing of the tee process\n            # manually (in the exit handler).\n\n            # a for append to file\n            \n            tee = subprocess.Popen(\n                [\"tee\", \"-a\", self.pipeline_log_file], stdin=subprocess.PIPE,\n                preexec_fn=self._ignore_interrupts)\n\n            # If the pipeline is terminated with SIGTERM/SIGINT,\n            # make sure we kill this spawned tee subprocess as well.\n            # atexit.register(self._kill_child_process, tee.pid, proc_name=\"tee\")\n            os.dup2(tee.stdin.fileno(), sys.stdout.fileno())\n            os.dup2(tee.stdin.fileno(), sys.stderr.fileno())\n\n            self.tee = tee\n\n        # For some reason, this exit handler function MUST be registered after\n        # the one that kills the tee process.\n        atexit.register(self._exit_handler)\n\n        # A future possibility to avoid this tee, is to use a Tee class; this works for anything printed here\n        # by pypiper, but can't tee the subprocess output. For this, it would require using threading to\n        # simultaneously capture and display subprocess output. I shelve this for now and stick with the tee option.\n        # sys.stdout = Tee(self.pipeline_log_file)\n\n        # Record the git version of the pipeline and pypiper used. This gets (if it is in a git repo):\n        # dir: the directory where the code is stored\n        # hash: the commit id of the last commit in this repo\n        # date: the date of the last commit in this repo\n        # diff: a summary of any differences in the current (run) version vs. the committed version\n\n        # Wrapped in try blocks so that the code will not fail if the pipeline or pypiper are not git repositories\n        gitvars = {}\n        try:\n            # pypiper dir\n            ppd = os.path.dirname(os.path.realpath(__file__))\n            gitvars['pypiper_dir'] = ppd\n            gitvars['pypiper_hash'] = subprocess.check_output(\"cd \" + ppd + \"; git rev-parse --verify HEAD 2>/dev/null\", shell=True)\n            gitvars['pypiper_date'] = subprocess.check_output(\"cd \" + ppd + \"; git show -s --format=%ai HEAD 2>/dev/null\", shell=True)\n            gitvars['pypiper_diff'] = subprocess.check_output(\"cd \" + ppd + \"; git diff --shortstat HEAD 2>/dev/null\", shell=True)\n            gitvars['pypiper_branch'] = subprocess.check_output(\"cd \" + ppd + \"; git branch | grep '*' 2>/dev/null\", shell=True)\n        except Exception:\n            pass\n        try:\n            # pipeline dir\n            pld = os.path.dirname(os.path.realpath(sys.argv[0]))\n            gitvars['pipe_dir'] = pld\n            gitvars['pipe_hash'] = subprocess.check_output(\"cd \" + pld + \"; git rev-parse --verify HEAD 2>/dev/null\", shell=True)\n            gitvars['pipe_date'] = subprocess.check_output(\"cd \" + pld + \"; git show -s --format=%ai HEAD 2>/dev/null\", shell=True)\n            gitvars['pipe_diff'] = subprocess.check_output(\"cd \" + pld + \"; git diff --shortstat HEAD 2>/dev/null\", shell=True)\n            gitvars['pipe_branch'] = subprocess.check_output(\"cd \" + pld + \"; git branch | grep '*' 2>/dev/null\", shell=True)\n        except Exception:\n            pass\n        \n        # Print out a header section in the pipeline log:\n        # Wrap things in backticks to prevent markdown from interpreting underscores as emphasis.\n        # print(\"----------------------------------------\")\n        print(\"### [Pipeline run code and environment:]\\n\")\n        print(\"* \" + \"Command\".rjust(20) + \":  \" + \"`\" + str(\" \".join(sys.argv)) + \"`\")\n        print(\"* \" + \"Compute host\".rjust(20) + \":  \" + platform.node())\n        print(\"* \" + \"Working dir\".rjust(20) + \":  \" + os.getcwd())\n        print(\"* \" + \"Outfolder\".rjust(20) + \":  \" + self.outfolder)\n\n        self.timestamp(\"* \" + \"Pipeline started at\".rjust(20) + \":  \")\n\n        print(\"\\n### [Version log:]\\n\")\n        print(\"* \" + \"Python version\".rjust(20) + \":  \" + platform.python_version())\n        try:\n            print(\"* \" + \"Pypiper dir\".rjust(20) + \":  \" + \"`\" + gitvars['pypiper_dir'].strip() + \"`\")\n            print(\"* \" + \"Pypiper version\".rjust(20) + \":  \" + __version__)\n            print(\"* \" + \"Pypiper hash\".rjust(20) + \":  \" + str(gitvars['pypiper_hash']).strip())\n            print(\"* \" + \"Pypiper branch\".rjust(20) + \":  \" + str(gitvars['pypiper_branch']).strip())\n            print(\"* \" + \"Pypiper date\".rjust(20) + \":  \" + str(gitvars['pypiper_date']).strip())\n            if \"\" != str(gitvars['pypiper_diff']):\n                print(\"* \" + \"Pypiper diff\".rjust(20) + \":  \" + str(gitvars['pypiper_diff']).strip())\n        except KeyError:\n            # It is ok if keys aren't set, it means pypiper isn't in a  git repo.\n            pass\n\n        try:\n            print(\"* \" + \"Pipeline dir\".rjust(20) + \":  \" + \"`\" + gitvars['pipe_dir'].strip() + \"`\")\n            print(\"* \" + \"Pipeline version\".rjust(20) + \":  \" + str(self.pl_version))\n            print(\"* \" + \"Pipeline hash\".rjust(20) + \":  \" + str(gitvars['pipe_hash']).strip())\n            print(\"* \" + \"Pipeline branch\".rjust(20) + \":  \" + str(gitvars['pipe_branch']).strip())\n            print(\"* \" + \"Pipeline date\".rjust(20) + \":  \" + str(gitvars['pipe_date']).strip())\n            if (gitvars['pipe_diff'] != \"\"):\n                print(\"* \" + \"Pipeline diff\".rjust(20) + \":  \" + str(gitvars['pipe_diff']).strip())\n        except KeyError:\n            # It is ok if keys aren't set, it means the pipeline isn't a git repo.\n            pass\n\n        # Print all arguments (if any)\n        print(\"\\n### [Arguments passed to pipeline:]\\n\")\n        for arg, val in (vars(args) if args else dict()).items():\n            argtext = \"`{}`\".format(arg)\n            valtext = \"`{}`\".format(val)\n            print(\"* {}:  {}\".format(argtext.rjust(20), valtext))\n        print(\"\\n----------------------------------------\\n\")\n        self._set_status_flag(RUN_FLAG)\n\n        # Record the start in PIPE_profile and PIPE_commands output files so we\n        # can trace which run they belong to\n\n        with open(self.pipeline_commands_file, \"a\") as myfile:\n            myfile.write(\"# Pipeline started at \" + time.strftime(\"%m-%d %H:%M:%S\", time.localtime(self.starttime)) + \"\\n\\n\")\n\n        with open(self.pipeline_profile_file, \"a\") as myfile:\n            myfile.write(\"# Pipeline started at \" + time.strftime(\"%m-%d %H:%M:%S\", time.localtime(self.starttime)) + \"\\n\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconfigure state and files on disk to match current processing status.", "response": "def _set_status_flag(self, status):\n        \"\"\"\n        Configure state and files on disk to match current processing status.\n\n        :param str status: Name of new status designation for pipeline.\n        \"\"\"\n\n        # Remove previous status flag file.\n        flag_file_path = self._flag_file_path()\n        try:\n            os.remove(flag_file_path)\n        except:\n            # Print message only if the failure to remove the status flag\n            # is unexpected; there's no flag for initialization, so we\n            # can't remove the file.\n            if self.status != \"initializing\":\n                print(\"Could not remove flag file: '{}'\".format(flag_file_path))\n            pass\n\n        # Set new status.\n        prev_status = self.status\n        self.status = status\n        self._create_file(self._flag_file_path())\n        print(\"\\nChanged status from {} to {}.\".format(\n                prev_status, self.status))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _flag_file_path(self, status=None):\n        flag_file_name = \"{}_{}\".format(\n                self.name, flag_name(status or self.status))\n        return pipeline_filepath(self, filename=flag_file_name)", "response": "Create path to flag file based on indicated or current status."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, cmd, target=None, lock_name=None, shell=None, nofail=False, clean=False, follow=None, container=None):\n\n        # If the pipeline's not been started, skip ahead.\n        if not self._active:\n            cmds = [cmd] if isinstance(cmd, str) else cmd\n            cmds_text = [c if isinstance(c, str) else \" \".join(c) for c in cmds]\n            print(\"Pipeline is inactive; skipping {} command(s):\\n{}\".\n                  format(len(cmds), \"\\n\".join(cmds_text)))\n            return 0\n\n        # Short-circuit if the checkpoint file exists and the manager's not\n        # been configured to overwrite such files.\n        if self.curr_checkpoint is not None:\n            check_fpath = checkpoint_filepath(self.curr_checkpoint, self)\n            if os.path.isfile(check_fpath) and not self.overwrite_checkpoints:\n                print(\"Checkpoint file exists for '{}' ('{}'), and the {} has \"\n                      \"been configured to not overwrite checkpoints; \"\n                      \"skipping command '{}'\".format(\n                    self.curr_checkpoint, check_fpath,\n                    self.__class__.__name__, cmd))\n                return 0\n\n        # TODO: consider making the logic such that locking isn't implied, or\n        # TODO (cont.): that we can make it otherwise such that it's not\n        # TODO (cont.): strictly necessary to provide target or lock_name.\n        # The default lock name is based on the target name.\n        # Therefore, a targetless command that you want\n        # to lock must specify a lock_name manually.\n        if target is None and lock_name is None:\n            self.fail_pipeline(Exception(\n                \"You must provide either a target or a lock_name.\"))\n\n        # Downstream code requires target to be a list, so convert if only\n        # a single item was given\n        if not is_multi_target(target) and target is not None:\n            target = [target]\n\n        # Downstream code requires a list of locks; convert \n        if isinstance(lock_name, str):\n            lock_name = [lock_name]\n        \n        # Default lock_name (if not provided) is based on the target file name,\n        # but placed in the parent pipeline outfolder\n        lock_name = lock_name or make_lock_name(target, self.outfolder)\n        lock_files = [self._make_lock_path(ln) for ln in lock_name]\n\n        process_return_code = 0\n        local_maxmem = 0\n\n        # Decide how to do follow-up.\n        if not follow:\n            call_follow = lambda: None\n        elif not hasattr(follow, \"__call__\"):\n            # Warn about non-callable argument to follow-up function.\n            print(\"Follow-up function is not callable and won't be used: {}\".\n                  format(type(follow)))\n            call_follow = lambda: None\n        else:\n            # Wrap the follow-up function so that the log shows what's going on.\n            def call_follow():\n                print(\"Follow:\")\n                follow()\n\n\n        # The while=True loop here is unlikely to be triggered, and is just a\n        # wrapper to prevent race conditions; the lock_file must be created by\n        # the current loop. If not, we loop again and then re-do the tests.\n        # The recover and newstart options inform the pipeline to run a command\n        # in a scenario where it normally would not. We use these \"local\" flags\n        # to allow us to report on the state of the pipeline in the first round\n        # as normal, but then proceed on the next iteration through the outer\n        # loop. The proceed_through_locks is a flag that is set if any lockfile\n        # is found that needs to be recovered or overwritten. It instructs us to\n        # ignore lock files on the next iteration.\n        local_recover = False\n        local_newstart = False \n        proceed_through_locks = False\n\n        while True:\n            ##### Tests block\n            # Base case: All targets exists and not set to overwrite targets break loop, don't run process.\n            # os.path.exists returns True for either a file or directory; .isfile is file-only\n            if target is not None and all([os.path.exists(t) for t in target]) \\\n                    and not any([os.path.isfile(l) for l in lock_files]) \\\n                    and not local_newstart:\n                for tgt in target:\n                    if os.path.exists(tgt): print(\"Target exists: `\" + tgt + \"`\")\n                if self.new_start:\n                    print(\"New start mode; run anyway.\")\n                    # Set the local_newstart flag so the command will run anyway.\n                    # Doing this in here instead of outside the loop allows us\n                    # to still report the target existence.\n                    local_newstart = True\n                    continue\n                # Normally we don't run the follow, but if you want to force. . .\n                if self.force_follow:\n                    call_follow()\n                break  # Do not run command\n\n            # Scenario 1: Lock file exists, but we're supposed to overwrite target; Run process.\n            if not proceed_through_locks:\n                for lock_file in lock_files:\n                    recover_file = self._recoverfile_from_lockfile(lock_file)\n                    if os.path.isfile(lock_file):\n                        print(\"Found lock file: {}\".format(lock_file))\n                        if self.overwrite_locks:\n                            print(\"Overwriting target. . .\")\n                            proceed_through_locks = True\n                        elif os.path.isfile(recover_file):\n                            print(\"Found dynamic recovery file ({}); \"\n                                  \"overwriting target. . .\".format(recover_file))\n                            # remove the lock file which will then be promptly re-created for the current run.\n                            local_recover = True\n                            proceed_through_locks = True\n                            # the recovery flag is now spent; remove so we don't accidentally re-recover a failed job\n                            os.remove(recover_file)\n                        else:  # don't overwrite locks\n                            self._wait_for_lock(lock_file)\n                            # when it's done loop through again to try one more\n                            # time (to see if the target exists now)\n                            continue\n\n\n            # If you get to this point, the target doesn't exist, and the lock_file doesn't exist \n            # (or we should overwrite). create the lock (if you can)\n            # Initialize lock in master lock list\n            for lock_file in lock_files:\n                self.locks.append(lock_file)\n                if self.overwrite_locks or local_recover:\n                    self._create_file(lock_file)\n                else:\n                    try:\n                        self._create_file_racefree(lock_file)  # Create lock\n                    except OSError as e:\n                        if e.errno == errno.EEXIST:  # File already exists\n                            print (\"Lock file created after test! Looping again: {}\".format(\n                                lock_file))\n\n                            # Since a lock file was created by a different source, \n                            # we need to reset this flag to re-check the locks.\n                            proceed_through_locks = False\n                            continue  # Go back to start\n\n            ##### End tests block\n            # If you make it past these tests, we should proceed to run the process.\n\n            if target is not None:\n                print(\"Target to produce: {}\\n\".format(\",\".join(['`'+x+'`' for x in target])))\n            else:\n                print(\"Targetless command, running...\\n\")\n\n            if isinstance(cmd, list):  # Handle command lists\n                for cmd_i in cmd:\n                    list_ret, maxmem = \\\n                        self.callprint(cmd_i, shell, lock_file, nofail, container)\n                    maxmem = max(maxmem) if isinstance(maxmem, Iterable) else maxmem\n                    local_maxmem = max(local_maxmem,  maxmem)\n                    list_ret = max(list_ret) if isinstance(list_ret, Iterable) else list_ret\n                    process_return_code = max(process_return_code, list_ret)\n\n            else:  # Single command (most common)\n                process_return_code, local_maxmem = \\\n                    self.callprint(cmd, shell, lock_file, nofail, container)  # Run command\n                if isinstance(process_return_code, list):\n                    process_return_code = max(process_return_code)\n\n            # For temporary files, you can specify a clean option to automatically\n            # add them to the clean list, saving you a manual call to clean_add\n            if target is not None and clean:\n                for tgt in target:\n                    self.clean_add(tgt)\n\n            call_follow()\n            for lock_file in lock_files:\n                os.remove(lock_file)  # Remove lock file\n                self.locks.remove(lock_file)\n\n            # If you make it to the end of the while loop, you're done\n            break\n\n        return process_return_code", "response": "This is the primary workhorse function of PipelineManager. It runs a command in a Docker container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef checkprint(self, cmd, shell=None, nofail=False):\n\n        self._report_command(cmd)\n\n        likely_shell = check_shell(cmd, shell)\n\n        if shell is None:\n            shell = likely_shell\n\n        if not shell:\n            if likely_shell:\n                print(\"Should this command run in a shell instead of directly in a subprocess?\")\n            cmd = shlex.split(cmd)\n            \n        try:\n            return subprocess.check_output(cmd, shell=shell)\n        except Exception as e:\n            self._triage_error(e, nofail)", "response": "This function is used to run a command in a specific shell and return the output of the command."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints the given command and then executes it.", "response": "def callprint(self, cmd, shell=None, lock_file=None, nofail=False, container=None):\n        \"\"\"\n        Prints the command, and then executes it, then prints the memory use and\n        return code of the command.\n\n        Uses python's subprocess.Popen() to execute the given command. The shell argument is simply\n        passed along to Popen(). You should use shell=False (default) where possible, because this enables memory\n        profiling. You should use shell=True if you require shell functions like redirects (>) or stars (*), but this\n        will prevent the script from monitoring memory use. The pipes (|) will be used to split the command into\n        subprocesses run within python, so the memory profiling is possible.\n\n        cmd can also be a series (a dict object) of multiple commands, which will be run in succession.\n\n        :param str | Iterable[str] cmd: Bash command(s) to be run.\n        :param str lock_file: a lock file name\n        :param bool nofail: Should the pipeline bail on a nonzero return from a process? Default: False\n            Nofail can be used to implement non-essential parts of the pipeline; if these processes fail,\n            they will not cause the pipeline to bail out.\n        :param bool shell: if the command should be run it its own shell, default: None (will try\n            to determine based on the command)\n        :param container: Named Docker container in which to execute.\n        :param container: str\n        \"\"\"\n        # The Popen shell argument works like this:\n        # if shell=False, then we format the command (with split()) to be a list of command and its arguments.\n        # Split the command to use shell=False;\n        # leave it together to use shell=True;\n\n        def get_mem_child_sum(proc):\n            try:\n                # get children processes\n                children = proc.children(recursive=True)\n                # get RSS memory of each child proc and sum all\n                mem_sum = sum([x.memory_info().rss for x in children])\n                # return in gigs\n                return mem_sum/1e9\n            except (psutil.NoSuchProcess, psutil.ZombieProcess) as e:\n                print(e)\n                print(\"Warning: couldn't add memory use for process: {}\".format(proc.pid))\n                return 0\n\n\n        def display_memory(memval):\n            return None if memval < 0 else \"{}GB\".format(round(memval, 3))\n\n        def make_dict(command):\n            a, s = (command, True) if check_shell(command, shell) else (shlex.split(command), False)\n            return dict(args=a, stdout=subprocess.PIPE, shell=s)\n\n        if container:\n            cmd = \"docker exec \" + container + \" \" + cmd\n\n        param_list = [make_dict(c) for c in split_by_pipes(cmd)] \\\n            if check_shell_pipes(cmd) else [dict(args=cmd, stdout=None, shell=True)]\n\n        proc_name = get_proc_name(cmd)\n\n        # stop_times = []\n        processes = []\n        running_processes = []\n        start_time = time.time()\n        for i in range(len(param_list)):\n            running_processes.append(i)\n            if i == 0:\n                processes.append(psutil.Popen(preexec_fn=os.setpgrp, **param_list[i]))\n            else:\n                param_list[i][\"stdin\"] = processes[i - 1].stdout\n                processes.append(psutil.Popen(preexec_fn=os.setpgrp, **param_list[i]))\n\n            self.procs[processes[-1].pid] = {\n                \"proc_name\": proc_name,\n                \"subproc_name\" : get_proc_name(param_list[i][\"args\"]),\n                \"start_time\": start_time,\n                \"container\": container,\n                \"p\": processes[-1]\n            }\n\n        self._report_command(cmd, [x.pid for x in processes])\n            # Capture the subprocess output in <pre> tags to make it format nicely\n            # if the markdown log file is displayed as HTML.\n        print(\"<pre>\")\n\n        local_maxmems = [-1] * len(running_processes)\n        returncodes = [None] * len(running_processes)\n\n        if not self.wait:\n            print(\"</pre>\")\n            ids = [x.pid for x in processes]\n            print (\"Not waiting for subprocess(es): \" + str(ids))\n            return [0, -1]\n\n        def proc_wrapup(i):\n            \"\"\"\n            :param i: internal ID number of the subprocess\n            \"\"\"\n            returncode = processes[i].returncode\n            current_pid = processes[i].pid\n\n            info = \"Process {pid} returned {ret}; memory: {mem}. \".format(\n                pid=current_pid, \n                ret=processes[i].returncode,\n                mem=display_memory(local_maxmems[i]))\n            \n            # report process profile\n            self._report_profile(self.procs[current_pid][\"proc_name\"], lock_file, time.time() - self.procs[current_pid][\"start_time\"], local_maxmems[i])\n\n            # Remove this as a running subprocess\n            del self.procs[current_pid]\n            running_processes.remove(i)\n\n            returncodes[i] = returncode\n            return info\n\n\n        sleeptime = .0001\n        info = \"\"\n        while running_processes:\n            for i in running_processes:\n                local_maxmems[i] = max(local_maxmems[i], (get_mem_child_sum(processes[i])))\n                self.peak_memory = max(self.peak_memory, local_maxmems[i])\n                if not self._attend_process(processes[i], sleeptime):\n                    info += proc_wrapup(i)\n\n            # the sleeptime is extremely short at the beginning and gets longer exponentially \n            # (+ constant to prevent copious checks at the very beginning)\n            # = more precise mem tracing for short processes\n            sleeptime = min((sleeptime + 0.25) * 3 , 60/len(processes))\n\n        # All jobs are done, print a final closing and job info\n        stop_time = time.time()\n        info += \" Elapsed: \" + str(datetime.timedelta(seconds=self.time_elapsed(start_time))) + \".\"\n        info += \" Peak memory: {pipe}.\".format(pipe=display_memory(self.peak_memory))\n\n        print(\"</pre>\")\n        print(info)\n\n        for rc in returncodes:\n            if rc != 0:\n                msg = \"Subprocess returned nonzero result. Check above output for details\"\n                self._triage_error(SubprocessError(msg), nofail)\n\n\n\n        return [returncodes, local_maxmems]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndebug function used in unit tests. :param p: A subprocess.Popen process. :param bool shell: If command requires should be run in its own shell. Optional. Default: False.", "response": "def _wait_for_process(self, p, shell=False):\n        \"\"\"\n        Debug function used in unit tests.\n\n        :param p: A subprocess.Popen process.\n        :param bool shell: If command requires should be run in its own shell. Optional. Default: False.\n        \"\"\"\n        local_maxmem = -1\n        sleeptime = .5\n        while p.poll() is None:\n            if not shell:\n                local_maxmem = max(local_maxmem, self._memory_usage(p.pid) / 1e6)\n                # print(\"int.maxmem (pid:\" + str(p.pid) + \") \" + str(local_maxmem))\n            time.sleep(sleeptime)\n            sleeptime = min(sleeptime + 5, 60)\n\n        self.peak_memory = max(self.peak_memory, local_maxmem)\n        \n        del self.procs[p.pid]\n\n        info = \"Process \" + str(p.pid) + \" returned: (\" + str(p.returncode) + \").\"\n        if not shell:\n            info += \" Peak memory: (Process: \" + str(round(local_maxmem, 3)) + \"GB;\"\n            info += \" Pipeline: \" + str(round(self.peak_memory, 3)) + \"GB)\\n\"\n\n        print(info + \"\\n\")\n        if p.returncode != 0:\n            raise Exception(\"Process returned nonzero result.\")\n        return [p.returncode, local_maxmem]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwaiting until the lock file exists and a dynamic recovery flag is spotted.", "response": "def _wait_for_lock(self, lock_file):\n        \"\"\"\n        Just sleep until the lock_file does not exist or a lock_file-related dynamic recovery flag is spotted\n\n        :param str lock_file: Lock file to wait upon.\n        \"\"\"\n        sleeptime = .5\n        first_message_flag = False\n        dot_count = 0\n        recover_file = self._recoverfile_from_lockfile(lock_file)\n        while os.path.isfile(lock_file):\n            if first_message_flag is False:\n                self.timestamp(\"Waiting for file lock: \" + lock_file)\n                self._set_status_flag(WAIT_FLAG)\n                first_message_flag = True\n            else:\n                sys.stdout.write(\".\")\n                dot_count = dot_count + 1\n                if dot_count % 60 == 0:\n                    print(\"\")  # linefeed\n            # prevents the issue of pypier waiting for the lock file to be gone infinitely\n            # in case the recovery flag is sticked by other pipeline when it's interrupted\n            if os.path.isfile(recover_file):\n                sys.stdout.write(\" Dynamic recovery flag found\")\n                break\n            time.sleep(sleeptime)\n            sleeptime = min(sleeptime + 2.5, 60)\n\n        if first_message_flag:\n            self.timestamp(\"File unlocked.\")\n            self._set_status_flag(RUN_FLAG)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint a message time and time elapsed for the current checkpoint.", "response": "def timestamp(self, message=\"\", checkpoint=None,\n                  finished=False, raise_error=True):\n        \"\"\"\n        Print message, time, and time elapsed, perhaps creating checkpoint.\n\n        This prints your given message, along with the current time, and time\n        elapsed since the previous timestamp() call.  If you specify a\n        HEADING by beginning the message with \"###\", it surrounds the message\n        with newlines for easier readability in the log file. If a checkpoint\n        is designated, an empty file is created corresponding to the name\n        given. Depending on how this manager's been configured, the value of\n        the checkpoint, and whether this timestamp indicates initiation or\n        completion of a group of pipeline steps, this call may stop the\n        pipeline's execution.\n\n        :param str message: Message to timestamp.\n        :param str checkpoint: Name of checkpoint; this tends to be something\n            that reflects the processing logic about to be or having just been\n            completed. Provision of an argument to this parameter means that\n            a checkpoint file will be created, facilitating arbitrary starting\n            and stopping point for the pipeline as desired.\n        :param bool finished: Whether this call represents the completion of a\n            conceptual unit of a pipeline's processing\n        :param raise_error: Whether to raise exception if\n            checkpoint or current state indicates that a halt should occur.\n        \"\"\"\n\n        # Halt if the manager's state has been set such that this call\n        # should halt the pipeline.\n        if self.halt_on_next:\n            self.halt(checkpoint, finished, raise_error=raise_error)\n\n        # Determine action to take with respect to halting if needed.\n        if checkpoint:\n            if finished:\n                # Write the file.\n                self._checkpoint(checkpoint)\n                self.prev_checkpoint = checkpoint\n                self.curr_checkpoint = None\n            else:\n                self.prev_checkpoint = self.curr_checkpoint\n                self.curr_checkpoint = checkpoint\n                self._checkpoint(self.prev_checkpoint)\n            # Handle the two halting conditions.\n            if (finished and checkpoint == self.stop_after) or (not finished and checkpoint == self.stop_before):\n                self.halt(checkpoint, finished, raise_error=raise_error)\n            # Determine if we've started executing.\n            elif checkpoint == self.start_point:\n                self._active = True\n            # If this is a prospective checkpoint, set the current checkpoint\n            # accordingly and whether we should halt the pipeline on the\n            # next timestamp call.\n            if not finished and checkpoint == self.stop_after:\n                self.halt_on_next = True\n\n        elapsed = self.time_elapsed(self.last_timestamp)\n        t = time.strftime(\"%m-%d %H:%M:%S\")\n        if checkpoint is None:\n            msg = \"{m} ({t}) elapsed: {delta_t} _TIME_\".\\\n                    format(m=message, t=t, delta_t=elapsed)\n        else:\n            msg = \"{m} ({t}) ({status} {stage}) elapsed: {delta_t} _TIME_\".\\\n                format(m=message, t=t,\n                       status=\"finished\" if finished else \"starting\",\n                       stage=checkpoint, delta_t=elapsed)\n        if re.match(\"^###\", message):\n            msg = \"\\n{}\\n\".format(msg)\n        print(msg)\n        self.last_timestamp = time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites a string to self. pipeline_profile_file.", "response": "def _report_profile(self, command, lock_name, elapsed_time, memory):\n        \"\"\"\n        Writes a string to self.pipeline_profile_file.\n        \"\"\"\n        message_raw = str(command) + \"\\t \" + \\\n            str(lock_name) + \"\\t\" + \\\n            str(datetime.timedelta(seconds = round(elapsed_time, 2))) + \"\\t \" + \\\n            str(memory)\n\n        with open(self.pipeline_profile_file, \"a\") as myfile:\n            myfile.write(message_raw + \"\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef report_result(self, key, value, annotation=None):\n        # Default annotation is current pipeline name.\n        annotation = str(annotation or self.name)\n\n        # In case the value is passed with trailing whitespace.\n        value = str(value).strip()\n\n        # keep the value in memory:\n        self.stats_dict[key] = value\n        message_raw = \"{key}\\t{value}\\t{annotation}\".format(\n            key=key, value=value, annotation=annotation)\n\n        message_markdown = \"\\n> `{key}`\\t{value}\\t{annotation}\\t_RES_\".format(\n            key=key, value=value, annotation=annotation)\n\n        print(message_markdown)\n\n        # Just to be extra careful, let's lock the file while we we write\n        # in case multiple pipelines write to the same file.\n        self._safe_write_to_file(self.pipeline_stats_file, message_raw)", "response": "Writes a string to self. pipeline_stats_file. A message is printed to self. stats_dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a string to self. pipeline_objects_file. Used to report figures and others.", "response": "def report_object(self, key, filename, anchor_text=None, anchor_image=None,\n       annotation=None):\n        \"\"\"\n        Writes a string to self.pipeline_objects_file. Used to report figures and others.\n\n        :param str key: name (key) of the object\n        :param str filename: relative path to the file (relative to parent output dir)\n        :param str anchor_text: text used as the link anchor test or caption to\n            refer to the object. If not provided, defaults to the key.\n        :param str anchor_image: a path to an HTML-displayable image thumbnail (so,\n            .png or .jpg, for example). If a path, the path should be relative\n            to the parent output dir.\n        :param str annotation: By default, the figures will be annotated with the\n            pipeline name, so you can tell which pipeline records which figures.\n            If you want, you can change this.\n        \"\"\"\n\n        # Default annotation is current pipeline name.\n        annotation = str(annotation or self.name)\n\n        # In case the value is passed with trailing whitespace.\n        filename = str(filename).strip()\n        if anchor_text:\n            anchor_text = str(anchor_text).strip()\n        else:\n            anchor_text = str(key).strip()\n\n        # better to use a relative path in this file\n        # convert any absolute paths into relative paths\n        relative_filename = os.path.relpath(filename, self.outfolder) \\\n                if os.path.isabs(filename) else filename\n\n        if anchor_image:\n            relative_anchor_image = os.path.relpath(anchor_image, self.outfolder) \\\n                if os.path.isabs(anchor_image) else anchor_image\n        else:\n            relative_anchor_image = \"None\"\n\n        message_raw = \"{key}\\t{filename}\\t{anchor_text}\\t{anchor_image}\\t{annotation}\".format(\n            key=key, filename=relative_filename, anchor_text=anchor_text, \n            anchor_image=relative_anchor_image, annotation=annotation)\n\n        message_markdown = \"> `{key}`\\t{filename}\\t{anchor_text}\\t{anchor_image}\\t{annotation}\\t_OBJ_\".format(\n            key=key, filename=relative_filename, anchor_text=anchor_text, \n            anchor_image=relative_anchor_image,annotation=annotation)\n\n        print(message_markdown)\n\n        self._safe_write_to_file(self.pipeline_objects_file, message_raw)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _safe_write_to_file(self, file, message):\n        target = file\n        lock_name = make_lock_name(target, self.outfolder)\n        lock_file = self._make_lock_path(lock_name)\n\n        while True:\n            if os.path.isfile(lock_file):\n                self._wait_for_lock(lock_file)\n            else:\n                try:\n                    self.locks.append(lock_file)\n                    self._create_file_racefree(lock_file)\n                except OSError as e:\n                    if e.errno == errno.EEXIST:\n                        print (\"Lock file created after test! Looping again.\")\n                        continue  # Go back to start\n\n                # Proceed with file writing\n                with open(file, \"a\") as myfile:\n                    myfile.write(message + \"\\n\")\n\n                os.remove(lock_file)\n                self.locks.remove(lock_file)\n                \n                # If you make it to the end of the while loop, you're done\n                break", "response": "Writes a string to a file safely."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a command to both stdout and to the commands log file", "response": "def _report_command(self, cmd, procs=None):\n        \"\"\"\n        Writes a command to both stdout and to the commands log file \n        (self.pipeline_commands_file).\n\n        :param str cmd: command to report\n        :param str | list[str] procs: process numbers for processes in the command\n        \"\"\"\n        if isinstance(procs, list):\n            procs = \",\".join(map(str,procs))\n        if procs:\n            line = \"\\n> `{cmd}` ({procs})\\n\".format(cmd=str(cmd), procs=procs)\n        else:\n            line = \"\\n> `{cmd}`\\n\".format(cmd=str(cmd))\n        print(line)\n\n        with open(self.pipeline_commands_file, \"a\") as myfile:\n            myfile.write(line + \"\\n\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_file_racefree(self, file):\n        write_lock_flags = os.O_CREAT | os.O_EXCL | os.O_WRONLY\n        os.open(file, write_lock_flags)", "response": "Creates a file and throws an OSError if it already exists."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates path to the lock file with given name as base.", "response": "def _make_lock_path(self, lock_name_base):\n        \"\"\"\n        Create path to lock file with given name as base.\n        \n        :param str lock_name_base: Lock file name, designed to not be prefixed \n            with the lock file designation, but that's permitted.\n        :return str: Path to the lock file.\n        \"\"\"\n\n        # For lock prefix validation, separate file name from other path\n        # components, as we care about the name prefix not path prefix.\n        base, name = os.path.split(lock_name_base)\n\n        lock_name = self._ensure_lock_prefix(name)\n        if base:\n            lock_name = os.path.join(base, lock_name)\n        return pipeline_filepath(self, filename=lock_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _recoverfile_from_lockfile(self, lockfile):\n        # Require that the lock file path be absolute, or at least relative\n        # and starting with the pipeline output folder.\n        if not (os.path.isabs(lockfile) or lockfile.startswith(self.outfolder)):\n            lockfile = self._make_lock_path(lockfile)\n        return lockfile.replace(LOCK_PREFIX, \"recover.\" + LOCK_PREFIX)", "response": "Create path to recovery file with given name as base."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_sure_path_exists(self, path):\n        try:\n            os.makedirs(path)\n        except OSError as exception:\n            if exception.errno != errno.EEXIST:\n                raise", "response": "Creates all directories in a path if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _refresh_stats(self):\n\n        # regex identifies all possible stats files.\n        #regex = self.outfolder +  \"*_stats.tsv\"       \n        #stats_files = glob.glob(regex)\n        #stats_files.insert(self.pipeline_stats_file) # last one is the current pipeline\n        #for stats_file in stats_files:\n\n        stats_file = self.pipeline_stats_file\n        if os.path.isfile(self.pipeline_stats_file):\n            with open(stats_file, 'r') as stat_file:\n                for line in stat_file:\n                    try:\n                        # Someone may have put something that's not 3 columns in the stats file\n                        # if so, shame on him, but we can just ignore it.\n                        key, value, annotation  = line.split('\\t')\n                    except ValueError:\n                        print(\"WARNING: Each row in a stats file is expected to have 3 columns\")\n\n                    if annotation.rstrip() == self.name or annotation.rstrip() == \"shared\":\n                        self.stats_dict[key] = value.strip()", "response": "Load up the stats sheet created for this pipeline and reads those stats into memory\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_stat(self, key):\n\n        try:\n            return self.stats_dict[key]\n        except KeyError:\n            self._refresh_stats()\n            try:\n                return self.stats_dict[key]\n            except KeyError:\n                print(\"Missing stat '{}'\".format(key))\n                return None", "response": "Retrieves a stat that was previously reported by the current pipeline run."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _checkpoint(self, stage):\n\n        # For null stage, short-circuit and indicate no file write.\n        # This handles case in which we're timestamping prospectively and\n        # previously weren't in a stage.\n        if stage is None:\n            return False\n\n        try:\n            is_checkpoint = stage.checkpoint\n        except AttributeError:\n            # Maybe we have a raw function, not a stage.\n            if hasattr(stage, \"__call__\"):\n                stage = stage.__name__\n            else:\n                # Maybe we have a stage name not a Stage.\n                # In that case, we can proceed as-is, with downstream\n                # processing handling Stage vs. stage name disambiguation.\n                # Here, though, warn about inputs that appear filename/path-like.\n                # We can't rely on raw text being a filepath or filename,\n                # because that would ruin the ability to pass stage name rather\n                # than actual stage. We can issue a warning message based on the\n                # improbability of a stage name containing the '.' that would\n                # be expected to characterize the extension of a file name/path.\n                base, ext = os.path.splitext(stage)\n                if ext and \".\" not in base:\n                    print(\"WARNING: '{}' looks like it may be the name or path of \"\n                          \"a file; for such a checkpoint, use touch_checkpoint.\".\n                          format(stage))\n        else:\n            if not is_checkpoint:\n                print(\"Not a checkpoint: {}\".format(stage))\n                return False\n            stage = stage.name\n\n        print(\"Checkpointing: '{}'\".format(stage))\n        if os.path.isabs(stage):\n            check_fpath = stage\n        else:\n            check_fpath = checkpoint_filepath(stage, pm=self)\n        return self._touch_checkpoint(check_fpath)", "response": "This is the hook that is used to stop processing of a logical processing phase. This is the hook that is used to stop processing of a logical processing phase."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fail_pipeline(self, e, dynamic_recover=False):\n        # Take care of any active running subprocess\n        sys.stdout.flush()\n        self._terminate_running_subprocesses()\n\n        if dynamic_recover:\n            # job was terminated, not failed due to a bad process.\n            # flag this run as recoverable.\n            if len(self.locks) < 1:\n                # If there is no process locked, then recovery will be automatic.\n                print(\"No locked process. Dynamic recovery will be automatic.\")\n            # make a copy of self.locks to iterate over since we'll be clearing them as we go\n            # set a recovery flag for each lock.\n            for lock_file in self.locks[:]:\n                recover_file = self._recoverfile_from_lockfile(lock_file)\n                print(\"Setting dynamic recover file: {}\".format(recover_file))\n                self._create_file(recover_file)\n                self.locks.remove(lock_file)\n\n        # Produce cleanup script\n        self._cleanup(dry_run=True)\n\n        # Finally, set the status to failed and close out with a timestamp\n        if not self._failed:  # and not self._completed:\n            self.timestamp(\"### Pipeline failed at: \")\n            total_time = datetime.timedelta(seconds=self.time_elapsed(self.starttime))\n            print(\"Total time: \" + str(total_time))\n            self._set_status_flag(FAIL_FLAG)\n\n        raise e", "response": "This function will stop the pipeline and set the status flag to failed and skips normal success completion procedure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef halt(self, checkpoint=None, finished=False, raise_error=True):\n        self.stop_pipeline(PAUSE_FLAG)\n        self._active = False\n        if raise_error:\n            raise PipelineHalt(checkpoint, finished)", "response": "Stop the pipeline before completion point.\n\n        :param str checkpoint: Name of stage just reached or just completed.\n        :param bool finished: Whether the indicated stage was just finished\n            (True), or just reached (False)\n        :param bool raise_error: Whether to raise an exception to truly\n            halt execution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop_pipeline(self, status=COMPLETE_FLAG):\n        self._set_status_flag(status)\n        self._cleanup()\n        self.report_result(\"Time\", str(datetime.timedelta(seconds=self.time_elapsed(self.starttime))))\n        self.report_result(\"Success\", time.strftime(\"%m-%d-%H:%M:%S\"))\n        print(\"\\n##### [Epilogue:]\")\n        print(\"* \" + \"Total elapsed time\".rjust(20) + \":  \" + str(datetime.timedelta(seconds=self.time_elapsed(self.starttime))))\n        # print(\"Peak memory used: \" + str(memory_usage()[\"peak\"]) + \"kb\")\n        print(\"* \" + \"Peak memory used\".rjust(20) + \":  \" + str(round(self.peak_memory, 2)) + \" GB\")\n        if self.halted:\n            return\n        self.timestamp(\"* Pipeline completed at: \".rjust(20))", "response": "Terminate the healthy pipeline and report the status of the pipeline."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions for handling both SIGTERM and SIGINT", "response": "def _generic_signal_handler(self, signal_type):\n        \"\"\"\n        Function for handling both SIGTERM and SIGINT\n        \"\"\"\n        print(\"</pre>\")\n        message = \"Got \" + signal_type + \". Failing gracefully...\"\n        self.timestamp(message)\n        self.fail_pipeline(KeyboardInterrupt(signal_type), dynamic_recover=True)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _exit_handler(self):\n\n        # TODO: consider handling sys.stderr/sys.stdout exceptions related to\n        # TODO (cont.): order of interpreter vs. subprocess shutdown signal receipt.\n        # TODO (cont.): see https://bugs.python.org/issue11380\n\n\n        # Make the cleanup file executable if it exists\n        if os.path.isfile(self.cleanup_file):\n            # Make the cleanup file self destruct.\n            with open(self.cleanup_file, \"a\") as myfile:\n                myfile.write(\"rm \" + self.cleanup_file + \"\\n\")\n            os.chmod(self.cleanup_file, 0o755)\n\n        # If the pipeline hasn't completed successfully, or already been marked\n        # as failed, then mark it as failed now.\n\n        if not self._has_exit_status:\n            print(\"Pipeline status: {}\".format(self.status))\n            self.fail_pipeline(Exception(\"Pipeline failure. See details above.\"))\n\n        if self.tee:\n            self.tee.kill()", "response": "This function is called when the script is completing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _kill_child_process(self, child_pid, proc_name=None):\n\n        # When we kill process, it turns into a zombie, and we have to reap it.\n        # So we can't just kill it and then let it go; we call wait\n\n        def pskill(proc_pid, sig=signal.SIGINT):\n            parent_process = psutil.Process(proc_pid)\n            for child_proc in parent_process.children(recursive=True):\n                child_proc.send_signal(sig)\n            parent_process.send_signal(sig)\n\n\n        if child_pid is None:\n            return\n\n        if proc_name:\n            proc_string = \" ({proc_name})\".format(proc_name=proc_name)\n\n        # First a gentle kill            \n        sys.stdout.flush()\n        still_running = self._attend_process(psutil.Process(child_pid), 0)\n        sleeptime = .25\n        time_waiting = 0\n\n        while still_running and time_waiting < 3:\n            try:\n                if time_waiting > 2:\n                    pskill(child_pid, signal.SIGKILL)\n                    # print(\"pskill(\"+str(child_pid)+\", signal.SIGKILL)\")\n                elif time_waiting > 1:\n                    pskill(child_pid, signal.SIGTERM)\n                    # print(\"pskill(\"+str(child_pid)+\", signal.SIGTERM)\")\n                else:\n                    pskill(child_pid, signal.SIGINT)\n                    # print(\"pskill(\"+str(child_pid)+\", signal.SIGINT)\")\n\n            except OSError:\n                # This would happen if the child process ended between the check\n                # and the next kill step\n                still_running = False\n                time_waiting = time_waiting + sleeptime\n\n            # Now see if it's still running\n            time_waiting = time_waiting + sleeptime\n            if not self._attend_process(psutil.Process(child_pid), sleeptime):\n                still_running = False\n\n\n        if still_running:\n            # still running!?\n            print(\"Child process {child_pid}{proc_string} never responded\"\n                \"I just can't take it anymore. I don't know what to do...\".format(child_pid=child_pid,\n                    proc_string=proc_string))\n        else:\n            if time_waiting > 0:\n                note = \"terminated after {time} sec\".format(time=int(time_waiting))\n            else:\n                note = \"was already terminated\"\n\n            msg = \"Child process {child_pid}{proc_string} {note}.\".format(\n                child_pid=child_pid, proc_string=proc_string, note=note)\n            print(msg)", "response": "Kills a child process."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding files (or regexs) to a cleanup list, to delete when this pipeline completes successfully. When making a call with run that produces intermediate files that should be deleted after the pipeline completes, you flag these files for deletion with this command. Files added with clean_add will only be deleted upon success of the pipeline. :param str regex: A unix-style regular expression that matches files to delete (can also be a file name). :param bool conditional: True means the files will only be deleted if no other pipelines are currently running; otherwise they are added to a manual cleanup script called {pipeline_name}_cleanup.sh :param bool manual: True means the files will just be added to a manual cleanup script.", "response": "def clean_add(self, regex, conditional=False, manual=False):\n        \"\"\"\n        Add files (or regexs) to a cleanup list, to delete when this pipeline completes successfully.\n        When making a call with run that produces intermediate files that should be\n        deleted after the pipeline completes, you flag these files for deletion with this command.\n        Files added with clean_add will only be deleted upon success of the pipeline.\n\n        :param str regex:  A unix-style regular expression that matches files to delete\n            (can also be a file name).\n        :param bool conditional: True means the files will only be deleted if no other\n            pipelines are currently running; otherwise they are added to a manual cleanup script\n            called {pipeline_name}_cleanup.sh\n        :param bool manual: True means the files will just be added to a manual cleanup script.\n        \"\"\"\n        if self.dirty:\n            # Override the user-provided option and force manual cleanup.\n            manual = True\n\n        if not self.clean_initialized:\n            # Make cleanup files relative to the cleanup script in case the result folder moves.\n            with open(self.cleanup_file, \"a\") as myfile:\n                clean_init = 'DIR=\"$(cd -P -- \"$(dirname -- \"$0\")\" && pwd -P)\"'\n                myfile.write(clean_init + \"\\n\")\n                myfile.write(\"cd ${DIR}\\n\")\n                self.clean_initialized = True\n\n        if manual:\n            try:\n                filenames = glob.glob(regex)\n                for filename in filenames:\n                    with open(self.cleanup_file, \"a\") as myfile:\n                        relative_filename = os.path.relpath(filename, self.outfolder) \\\n                            if os.path.isabs(filename) else filename\n                        if os.path.isfile(relative_filename):\n                            myfile.write(\"rm \" + relative_filename + \"\\n\")\n                        elif os.path.isdir(relative_filename):\n                            # first, add all filenames in the directory\n                            myfile.write(\"rm \" + relative_filename + \"/*\\n\")\n                            # and the directory itself\n                            myfile.write(\"rmdir \" + relative_filename + \"\\n\")\n            except:\n                pass\n        elif conditional:\n            self.cleanup_list_conditional.append(regex)\n        else:\n            self.cleanup_list.append(regex)\n            # TODO: what's the \"absolute\" list?\n            # Remove it from the conditional list if added to the absolute list\n            while regex in self.cleanup_list_conditional:\n                self.cleanup_list_conditional.remove(regex)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cleanup(self, dry_run=False):\n\n        if dry_run:\n            # Move all unconditional cleans into the conditional list\n            if len(self.cleanup_list) > 0:\n                combined_list = self.cleanup_list_conditional + self.cleanup_list\n                self.cleanup_list_conditional = combined_list\n                self.cleanup_list = []\n\n        if len(self.cleanup_list) > 0:\n            print(\"\\nCleaning up flagged intermediate files. . .\")\n            for expr in self.cleanup_list:\n                print(\"\\nRemoving glob: \" + expr)\n                try:\n                    # Expand regular expression\n                    files = glob.glob(expr)\n                    # Remove entry from cleanup list\n                    while files in self.cleanup_list:\n                        self.cleanup_list.remove(files)\n                    # and delete the files\n                    for file in files:\n                        if os.path.isfile(file):\n                            print(\"`rm \" + file + \"`\")\n                            os.remove(os.path.join(file))\n                        elif os.path.isdir(file):\n                            print(\"`rmdir \" + file + \"`\")\n                            os.rmdir(os.path.join(file))\n                except:\n                    pass\n\n        if len(self.cleanup_list_conditional) > 0:\n            run_flag = flag_name(RUN_FLAG)\n            flag_files = [fn for fn in glob.glob(self.outfolder + flag_name(\"*\"))\n                          if COMPLETE_FLAG not in os.path.basename(fn)\n                          and not \"{}_{}\".format(self.name, run_flag) == os.path.basename(fn)]\n            if len(flag_files) == 0 and not dry_run:\n                print(\"\\nCleaning up conditional list. . .\")\n                for expr in self.cleanup_list_conditional:\n                    print(\"\\nRemoving glob: \" + expr)\n                    try:\n                        files = glob.glob(expr)\n                        while files in self.cleanup_list_conditional:\n                            self.cleanup_list_conditional.remove(files)\n                        for file in files:\n                            if os.path.isfile(file):\n                                print(\"`rm \" + file + \"`\")\n                                os.remove(os.path.join(file))\n                            elif os.path.isdir(file):\n                                print(\"`rmdir \" + file + \"`\")\n                                os.rmdir(os.path.join(file))\n                    except:\n                        pass\n            else:\n                print(\"\\nConditional flag found: \" + str([os.path.basename(i) for i in flag_files]))\n                print(\"\\nThese conditional files were left in place:\\n\\n- \" + \"\\n- \".join(self.cleanup_list_conditional))\n                # Produce a cleanup script.\n                no_cleanup_script = []\n                for cleandir in self.cleanup_list_conditional:\n                    try:\n                        items_to_clean = glob.glob(cleandir)\n                        for clean_item in items_to_clean:\n                            with open(self.cleanup_file, \"a\") as clean_script:\n                                if os.path.isfile(clean_item):\n                                    clean_script.write(\"rm \" + clean_item + \"\\n\")\n                                elif os.path.isdir(clean_item):\n                                    clean_script.write(\"rmdir \" + clean_item + \"\\n\")\n                    except Exception as e:\n                        no_cleanup_script.append(cleandir)\n                if no_cleanup_script: \n                    print('\\n\\nCould not produce cleanup script for item(s):\\n\\n- ' + '\\n- '.join(no_cleanup_script))", "response": "Cleans up the intermediate files and deletes the files that are not in the output folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _memory_usage(self, pid='self', category=\"hwm\", container=None):\n        if container:\n            # TODO: Put some debug output here with switch to Logger\n            # since this is relatively untested.\n            cmd = \"docker stats \" + container + \" --format '{{.MemUsage}}' --no-stream\"\n            mem_use_str = subprocess.check_output(cmd, shell=True)\n            mem_use = mem_use_str.split(\"/\")[0].split()\n            \n            mem_num = re.findall('[\\d\\.]+', mem_use_str.split(\"/\")[0])[0]\n            mem_scale = re.findall('[A-Za-z]+', mem_use_str.split(\"/\")[0])[0]\n\n            #print(mem_use_str, mem_num, mem_scale)\n            mem_num = float(mem_num)\n            if mem_scale == \"GiB\":\n                return mem_num * 1e6\n            elif mem_scale == \"MiB\":\n                return mem_num * 1e3\n            elif mem_scale == \"KiB\":\n                return mem_num\n            else:\n                # What type is this?\n                return 0\n\n        # Thanks Martin Geisler:\n        status = None\n        result = {'peak': 0, 'rss': 0, 'hwm': 0}\n        \n        try:\n            # This will only work on systems with a /proc file system\n            # (like Linux).\n            # status = open('/proc/self/status')\n            proc_spot = '/proc/%s/status' % pid\n            status = open(proc_spot)\n            for line in status:\n                parts = line.split()\n                key = parts[0][2:-1].lower()\n                if key in result:\n                    result[key] = int(parts[1])\n        except:\n            return 0\n\n        finally:\n            if status is not None:\n                status.close()\n        # print(result[category])\n        return result[category]", "response": "Return the memory usage of the process in kilobytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _triage_error(self, e, nofail):\n        if not nofail:\n            self.fail_pipeline(e)\n        elif self._failed:\n            print(\"This is a nofail process, but the pipeline was terminated for other reasons, so we fail.\")\n            raise e\n        else:\n            print(e)\n            print(\"ERROR: Subprocess returned nonzero result, but pipeline is continuing because nofail=True\")", "response": "Print a message and decide what to do with the error."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the requirements file for given requirements group.", "response": "def read_reqs_file(reqs_name):\n    \"\"\" Read requirements file for given requirements group. \"\"\"\n    path_reqs_file = os.path.join(\n            \"requirements\", \"reqs-{}.txt\".format(reqs_name))\n    with open(path_reqs_file, 'r') as reqs_file:\n        return [pkg.rstrip() for pkg in reqs_file.readlines()\n                if not pkg.startswith(\"#\")]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_unordered(collection):\n    if not isinstance(collection, Iterable):\n        raise TypeError(\"Non-iterable alleged collection: {}\".\n                        format(type(collection)))\n    return isinstance(collection, set) or \\\n           (isinstance(collection, dict) and\n            not isinstance(collection, OrderedDict))", "response": "Checks whether the given object is an unordered sequence of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_stage_spec(stage_spec):\n\n    # The logic used here, a message to a user about how to specify Stage.\n    req_msg = \"Stage specification must be either a {0} itself, a \" \\\n              \"(<name>, {0}) pair, or a callable with a __name__ attribute \" \\\n              \"(e.g., a non-anonymous function)\".format(Stage.__name__)\n\n    # Simplest case is stage itself.\n    if isinstance(stage_spec, Stage):\n        return stage_spec.name, stage_spec\n\n    # Handle alternate forms of specification.\n    try:\n        # Unpack pair of name and stage, requiring name first.\n        name, stage = stage_spec\n    except (TypeError, ValueError):\n        # Normally, this sort of unpacking issue create a ValueError. Here,\n        # though, we also need to catch TypeError since that's what arises\n        # if an attempt is made to unpack a single function.\n        # Attempt to parse stage_spec as a single named callable.\n        try:\n            name = stage_spec.__name__\n        except AttributeError:\n            raise TypeError(req_msg)\n        else:\n            # Control flow here indicates an anonymous function that was not\n            # paired with a name. Prohibit that.\n            if name == (lambda: None).__name__:\n                raise TypeError(req_msg)\n        stage = stage_spec\n\n    # Ensure that the stage is callable.\n    if not hasattr(stage, \"__call__\"):\n        raise TypeError(req_msg)\n\n    return name, Stage(stage, name=name)", "response": "Parses a stage specification and returns a name and Stage instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntouching checkpoint file for given stage and provide timestamp message.", "response": "def checkpoint(self, stage, msg=\"\"):\n        \"\"\"\n        Touch checkpoint file for given stage and provide timestamp message.\n\n        :param pypiper.Stage stage: Stage for which to mark checkpoint\n        :param str msg: Message to embed in timestamp.\n        :return bool: Whether a checkpoint file was written.\n        \"\"\"\n        # Canonical usage model for Pipeline checkpointing through\n        # implementations of this class is by automatically creating a\n        # checkpoint when a conceptual unit or group of operations of a\n        # pipeline completes, so fix the 'finished' parameter to the manager's\n        # timestamp method to be True.\n        return self.manager.timestamp(\n                message=msg, checkpoint=stage.checkpoint_name, finished=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef completed_stage(self, stage):\n        check_path = checkpoint_filepath(stage, self.manager)\n        return os.path.exists(check_path)", "response": "Determines whether this pipeline has completed the given stage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_flags(self, only_name=False):\n        paths = glob.glob(os.path.join(self.outfolder, flag_name(\"*\")))\n        if only_name:\n            return [os.path.split(p)[1] for p in paths]\n        else:\n            return paths", "response": "List the flag files associated with this pipeline."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(self, start_point=None, stop_before=None, stop_after=None):\n\n        # Start the run with a clean slate of Stage status/label tracking.\n        self._reset()\n\n        # TODO: validate starting point against checkpoint flags for\n        # TODO (cont.): earlier stages if the pipeline defines its stages as a\n        # TODO (cont.): sequence (i.e., probably prohibit start point with\n        # TODO (cont): nonexistent earlier checkpoint flag(s).)\n\n        if stop_before and stop_after:\n            raise IllegalPipelineExecutionError(\n                    \"Cannot specify both inclusive and exclusive stops.\")\n\n        if stop_before:\n            stop = stop_before\n            inclusive_stop = False\n        elif stop_after:\n            stop = stop_after\n            inclusive_stop = True\n        else:\n            stop = None\n            inclusive_stop = None\n\n        # Ensure that a stage name--if specified--is supported.\n        for s in [start_point, stop]:\n            if s is None:\n                continue\n            name = parse_stage_name(s)\n            if name not in self.stage_names:\n                raise UnknownPipelineStageError(name, self)\n\n        # Permit order-agnostic pipelines, but warn.\n        if self._unordered and (start_point or stop_before or stop_after):\n            print(\"WARNING: Starting and stopping points are nonsense for \"\n                  \"pipeline with unordered stages.\")\n\n        # TODO: consider context manager based on start/stop points.\n\n        # Determine where to start (but perhaps skip further based on\n        # checkpoint completions.)\n        start_index = self._start_index(start_point)\n        stop_index = self._stop_index(stop, inclusive=inclusive_stop)\n        assert stop_index <= len(self._stages)\n        if start_index >= stop_index:\n            raise IllegalPipelineExecutionError(\n                    \"Cannot start pipeline at or after stopping point\")\n\n        # TODO: consider storing just stage name rather than entire stage.\n        # TODO (cont.): the bad case for whole-Stage is if associated data\n        # TODO (cont.): (i.e., one or more args) are large.\n        self.skipped.extend(self._stages[:start_index])\n\n        # TODO: support both courses of action for non-continuous checkpoints.\n        # TODO (cont.): That is, what if there's a stage with a checkpoint\n        # TODO (cont.): file downstream of one without it? Naively, we'll\n        # TODO (cont.): skip it, but we may want to re-run.\n        skip_mode = True\n\n        for stage in self._stages[start_index:stop_index]:\n\n            # TODO: Note that there's no way to tell whether a non-checkpointed\n            # TODO (cont.) Stage has been completed, and thus this seek\n            # TODO (cont.) operation will find the first Stage, starting\n            # TODO (cont.) the specified start point, either uncheckpointed or\n            # TODO (cont.) for which the checkpoint file does not exist.\n            # Look for checkpoint file.\n            if skip_mode and self.completed_stage(stage):\n                print(\"Skipping completed checkpoint stage: {}\".format(stage))\n                self.skipped.append(stage)\n                continue\n\n            # Once we've found where to being execution, ignore checkpoint\n            # flags downstream if they exist since there may be dependence\n            # between results from different stages.\n            skip_mode = False\n\n            print(\"Running stage: {}\".format(stage))\n\n            stage.run()\n            self.executed.append(stage)\n            self.checkpoint(stage)\n\n        # Add any unused stages to the collection of skips.\n        self.skipped.extend(self._stages[stop_index:])\n\n        # Where we stopped determines the shutdown mode.\n        if stop_index == len(self._stages):\n            self.wrapup()\n        else:\n            self.halt(raise_error=False)", "response": "Run the pipeline, optionally specifying start and/or stop points.\n\n        :param str start_point: Name of stage at which to begin execution.\n        :param str stop_before: Name of stage at which to cease execution;\n            exclusive, i.e. this stage is not run\n        :param str stop_after: Name of stage at which to cease execution;\n            inclusive, i.e. this stage is the last one run\n        :raise IllegalPipelineExecutionError: If both inclusive (stop_after)\n            and exclusive (stop_before) halting points are provided, or if that\n            start stage is the same as or after the stop stage, raise an\n            IllegalPipelineExecutionError."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_index(self, start=None):\n        if start is None:\n            return 0\n        start_stage = translate_stage_name(start)\n        internal_names = [translate_stage_name(s.name) for s in self._stages]\n        try:\n            return internal_names.index(start_stage)\n        except ValueError:\n            raise UnknownPipelineStageError(start, self)", "response": "Return the index of the first stage to run."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _stop_index(self, stop_point, inclusive):\n        if not stop_point:\n            # Null case, no stopping point\n            return len(self._stages)\n        stop_name = parse_stage_name(stop_point)\n        try:\n            stop_index = self.stage_names.index(stop_name)\n        except ValueError:\n            raise UnknownPipelineStageError(stop_name, self)\n        return stop_index + 1 if inclusive else stop_index", "response": "Determines the index of the next stop point in the sequence of Pipeline s stages that indicates that the stop point is the one that is not in the final stage."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_ufo_font_attributes(self, family_name):\n\n    font = self.font\n\n    # \"date\" can be missing; Glyphs.app removes it on saving if it's empty:\n    # https://github.com/googlei18n/glyphsLib/issues/134\n    date_created = getattr(font, \"date\", None)\n    if date_created is not None:\n        date_created = to_ufo_time(date_created)\n    units_per_em = font.upm\n    version_major = font.versionMajor\n    version_minor = font.versionMinor\n    copyright = font.copyright\n    designer = font.designer\n    designer_url = font.designerURL\n    manufacturer = font.manufacturer\n    manufacturer_url = font.manufacturerURL\n    # XXX note is unused?\n    # note = font.note\n    glyph_order = list(glyph.name for glyph in font.glyphs)\n\n    for index, master in enumerate(font.masters):\n        source = self._designspace.newSourceDescriptor()\n        ufo = self.ufo_module.Font()\n        source.font = ufo\n\n        ufo.lib[APP_VERSION_LIB_KEY] = font.appVersion\n        ufo.lib[KEYBOARD_INCREMENT_KEY] = font.keyboardIncrement\n\n        if date_created is not None:\n            ufo.info.openTypeHeadCreated = date_created\n        ufo.info.unitsPerEm = units_per_em\n        ufo.info.versionMajor = version_major\n        ufo.info.versionMinor = version_minor\n\n        if copyright:\n            ufo.info.copyright = copyright\n        if designer:\n            ufo.info.openTypeNameDesigner = designer\n        if designer_url:\n            ufo.info.openTypeNameDesignerURL = designer_url\n        if manufacturer:\n            ufo.info.openTypeNameManufacturer = manufacturer\n        if manufacturer_url:\n            ufo.info.openTypeNameManufacturerURL = manufacturer_url\n\n        ufo.glyphOrder = glyph_order\n\n        self.to_ufo_names(ufo, master, family_name)\n        self.to_ufo_family_user_data(ufo)\n        self.to_ufo_custom_params(ufo, font)\n\n        self.to_ufo_master_attributes(source, master)\n\n        ufo.lib[MASTER_ORDER_LIB_KEY] = index\n        # FIXME: (jany) in the future, yield this UFO (for memory, lazy iter)\n        self._designspace.addSource(source)\n        self._sources[master.id] = source", "response": "Generate a list of UFOs with metadata loaded from. glyphs data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies font attributes from source UFO to self. font master.", "response": "def to_glyphs_font_attributes(self, source, master, is_initial):\n    \"\"\"\n    Copy font attributes from `ufo` either to `self.font` or to `master`.\n\n    Arguments:\n    self -- The UFOBuilder\n    ufo -- The current UFO being read\n    master -- The current master being written\n    is_initial -- True iff this the first UFO that we process\n    \"\"\"\n    if is_initial:\n        _set_glyphs_font_attributes(self, source)\n    else:\n        _compare_and_merge_glyphs_font_attributes(self, source)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_ufo_glyph(self, ufo_glyph, layer, glyph):\n    ufo_glyph.unicodes = [int(uval, 16) for uval in glyph.unicodes]\n\n    note = glyph.note\n    if note is not None:\n        ufo_glyph.note = note\n\n    last_change = glyph.lastChange\n    if last_change is not None:\n        ufo_glyph.lib[GLYPHLIB_PREFIX + \"lastChange\"] = to_ufo_time(last_change)\n\n    color_index = glyph.color\n    if color_index is not None:\n        # .3f is enough precision to round-trip uint8 to float losslessly.\n        # https://github.com/unified-font-object/ufo-spec/issues/61\n        # #issuecomment-389759127\n        if (\n            isinstance(color_index, list)\n            and len(color_index) == 4\n            and all(0 <= v < 256 for v in color_index)\n        ):\n            ufo_glyph.markColor = \",\".join(\n                \"{:.3f}\".format(v / 255) for v in color_index\n            )\n        elif isinstance(color_index, int) and color_index in range(len(GLYPHS_COLORS)):\n            ufo_glyph.markColor = GLYPHS_COLORS[color_index]\n        else:\n            logger.warning(\n                \"Glyph {}, layer {}: Invalid color index/tuple {}\".format(\n                    glyph.name, layer.name, color_index\n                )\n            )\n\n    export = glyph.export\n    if not export:\n        if \"public.skipExportGlyphs\" not in self._designspace.lib:\n            self._designspace.lib[\"public.skipExportGlyphs\"] = []\n        self._designspace.lib[\"public.skipExportGlyphs\"].append(glyph.name)\n\n    # FIXME: (jany) next line should be an API of GSGlyph?\n    glyphinfo = glyphsLib.glyphdata.get_glyph(ufo_glyph.name)\n    production_name = glyph.production or glyphinfo.production_name\n    if production_name != ufo_glyph.name:\n        postscriptNamesKey = PUBLIC_PREFIX + \"postscriptNames\"\n        if postscriptNamesKey not in ufo_glyph.font.lib:\n            ufo_glyph.font.lib[postscriptNamesKey] = dict()\n        ufo_glyph.font.lib[postscriptNamesKey][ufo_glyph.name] = production_name\n\n    for key in [\"leftMetricsKey\", \"rightMetricsKey\", \"widthMetricsKey\"]:\n        value = getattr(layer, key, None)\n        if value:\n            ufo_glyph.lib[GLYPHLIB_PREFIX + \"layer.\" + key] = value\n        value = getattr(glyph, key, None)\n        if value:\n            ufo_glyph.lib[GLYPHLIB_PREFIX + \"glyph.\" + key] = value\n\n    if glyph.script is not None:\n        ufo_glyph.lib[SCRIPT_LIB_KEY] = glyph.script\n\n    # if glyph contains custom 'category' and 'subCategory' overrides, store\n    # them in the UFO glyph's lib\n    category = glyph.category\n    if category is None:\n        category = glyphinfo.category\n    else:\n        ufo_glyph.lib[GLYPHLIB_PREFIX + \"category\"] = category\n    subCategory = glyph.subCategory\n    if subCategory is None:\n        subCategory = glyphinfo.subCategory\n    else:\n        ufo_glyph.lib[GLYPHLIB_PREFIX + \"subCategory\"] = subCategory\n\n    # load width before background, which is loaded with lib data\n    width = layer.width\n    if width is None:\n        pass\n    elif category == \"Mark\" and subCategory == \"Nonspacing\" and width > 0:\n        # zero the width of Nonspacing Marks like Glyphs.app does on export\n        # TODO: (jany) check for customParameter DisableAllAutomaticBehaviour\n        # FIXME: (jany) also don't do that when rt UFO -> glyphs -> UFO\n        ufo_glyph.lib[ORIGINAL_WIDTH_KEY] = width\n        ufo_glyph.width = 0\n    else:\n        ufo_glyph.width = width\n\n    self.to_ufo_background_image(ufo_glyph, layer)\n    self.to_ufo_guidelines(ufo_glyph, layer)\n    self.to_ufo_glyph_background(ufo_glyph, layer)\n    self.to_ufo_annotations(ufo_glyph, layer)\n    self.to_ufo_hints(ufo_glyph, layer)\n    self.to_ufo_glyph_user_data(ufo_glyph.font, glyph)\n    self.to_ufo_layer_user_data(ufo_glyph, layer)\n    self.to_ufo_smart_component_axes(ufo_glyph, glyph)\n\n    self.to_ufo_paths(ufo_glyph, layer)\n    self.to_ufo_components(ufo_glyph, layer)\n    self.to_ufo_glyph_anchors(ufo_glyph, layer.anchors)", "response": "Add. glyphs metadata paths components and anchors to a glyph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_glyphs_glyph(self, ufo_glyph, ufo_layer, master):\n\n    # FIXME: (jany) split between glyph and layer attributes\n    #        have a write the first time, compare the next times for glyph\n    #        always write for the layer\n\n    if ufo_glyph.name in self.font.glyphs:\n        glyph = self.font.glyphs[ufo_glyph.name]\n    else:\n        glyph = self.glyphs_module.GSGlyph(name=ufo_glyph.name)\n        # FIXME: (jany) ordering?\n        self.font.glyphs.append(glyph)\n\n    if ufo_glyph.unicodes:\n        glyph.unicodes = [\"{:04X}\".format(c) for c in ufo_glyph.unicodes]\n    glyph.note = ufo_glyph.note or \"\"\n    if GLYPHLIB_PREFIX + \"lastChange\" in ufo_glyph.lib:\n        last_change = ufo_glyph.lib[GLYPHLIB_PREFIX + \"lastChange\"]\n        # We cannot be strict about the dateformat because it's not an official\n        # UFO field mentioned in the spec so it could happen to have a timezone\n        glyph.lastChange = from_loose_ufo_time(last_change)\n    if ufo_glyph.markColor:\n        glyph.color = _to_glyphs_color(ufo_glyph.markColor)\n\n    # The export flag can be stored in the glyph's lib key (for upgrading legacy\n    # sources) or the Designspace-level public.skipExportGlyphs lib key (canonical\n    # place to store the information). The UFO level lib key is ignored.\n    if GLYPHLIB_PREFIX + \"Export\" in ufo_glyph.lib:\n        glyph.export = ufo_glyph.lib[GLYPHLIB_PREFIX + \"Export\"]\n    if ufo_glyph.name in self.designspace.lib.get(\"public.skipExportGlyphs\", []):\n        glyph.export = False\n\n    ps_names_key = PUBLIC_PREFIX + \"postscriptNames\"\n    if (\n        ps_names_key in ufo_glyph.font.lib\n        and ufo_glyph.name in ufo_glyph.font.lib[ps_names_key]\n    ):\n        glyph.production = ufo_glyph.font.lib[ps_names_key][ufo_glyph.name]\n        # FIXME: (jany) maybe put something in glyphinfo? No, it's readonly\n        #        maybe don't write in glyph.production if glyphinfo already\n        #        has something\n        # glyphinfo = glyphsLib.glyphdata.get_glyph(ufo_glyph.name)\n        # production_name = glyph.production or glyphinfo.production_name\n\n    glyphinfo = glyphsLib.glyphdata.get_glyph(ufo_glyph.name)\n\n    layer = self.to_glyphs_layer(ufo_layer, glyph, master)\n\n    for key in [\"leftMetricsKey\", \"rightMetricsKey\", \"widthMetricsKey\"]:\n        # Also read the old version of the key that didn't have a prefix and\n        # store it on the layer (because without the \"glyph\"/\"layer\" prefix we\n        # didn't know whether it originally came from the layer of the glyph,\n        # so it's easier to put it back on the most specific level, i.e. the\n        # layer)\n        for prefix, glyphs_object in (\n            (\"glyph.\", glyph),\n            (\"\", layer),\n            (\"layer.\", layer),\n        ):\n            full_key = GLYPHLIB_PREFIX + prefix + key\n            if full_key in ufo_glyph.lib:\n                value = ufo_glyph.lib[full_key]\n                setattr(glyphs_object, key, value)\n\n    if SCRIPT_LIB_KEY in ufo_glyph.lib:\n        glyph.script = ufo_glyph.lib[SCRIPT_LIB_KEY]\n\n    if GLYPHLIB_PREFIX + \"category\" in ufo_glyph.lib:\n        # TODO: (jany) store category only if different from glyphinfo?\n        category = ufo_glyph.lib[GLYPHLIB_PREFIX + \"category\"]\n        glyph.category = category\n    else:\n        category = glyphinfo.category\n    if GLYPHLIB_PREFIX + \"subCategory\" in ufo_glyph.lib:\n        sub_category = ufo_glyph.lib[GLYPHLIB_PREFIX + \"subCategory\"]\n        glyph.subCategory = sub_category\n    else:\n        sub_category = glyphinfo.subCategory\n\n    # load width before background, which is loaded with lib data\n    if hasattr(layer, \"foreground\"):\n        if ufo_glyph.width:\n            # Don't store \"0\", it's the default in UFO.\n            # Store in userData because the background's width is not relevant\n            # in Glyphs.\n            layer.userData[BACKGROUND_WIDTH_KEY] = ufo_glyph.width\n    else:\n        layer.width = ufo_glyph.width\n    if category == \"Mark\" and sub_category == \"Nonspacing\" and layer.width == 0:\n        # Restore originalWidth\n        if ORIGINAL_WIDTH_KEY in ufo_glyph.lib:\n            layer.width = ufo_glyph.lib[ORIGINAL_WIDTH_KEY]\n            # TODO: (jany) check for customParam DisableAllAutomaticBehaviour?\n\n    self.to_glyphs_background_image(ufo_glyph, layer)\n    self.to_glyphs_guidelines(ufo_glyph, layer)\n    self.to_glyphs_annotations(ufo_glyph, layer)\n    self.to_glyphs_hints(ufo_glyph, layer)\n    self.to_glyphs_glyph_user_data(ufo_glyph.font, glyph)\n    self.to_glyphs_layer_user_data(ufo_glyph, layer)\n    self.to_glyphs_smart_component_axes(ufo_glyph, glyph)\n\n    self.to_glyphs_paths(ufo_glyph, layer)\n    self.to_glyphs_components(ufo_glyph, layer)\n    self.to_glyphs_glyph_anchors(ufo_glyph, layer)", "response": "Add a UFO glyph to the GSLayer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_designspace_instances(self):\n    for instance in self.font.instances:\n        if self.minimize_glyphs_diffs or (\n            is_instance_active(instance)\n            and _is_instance_included_in_family(self, instance)\n        ):\n            _to_designspace_instance(self, instance)", "response": "Write instance data from self. font to self. designspace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_instance_data(designspace, include_filenames=None, Font=defcon.Font):\n    from fontTools.designspaceLib import DesignSpaceDocument\n    from os.path import normcase, normpath\n\n    if hasattr(designspace, \"__fspath__\"):\n        designspace = designspace.__fspath__()\n    if isinstance(designspace, basestring):\n        designspace = DesignSpaceDocument.fromfile(designspace)\n\n    basedir = os.path.dirname(designspace.path)\n    instance_ufos = []\n    if include_filenames is not None:\n        include_filenames = {normcase(normpath(p)) for p in include_filenames}\n\n    for designspace_instance in designspace.instances:\n        fname = designspace_instance.filename\n        assert fname is not None, \"instance %r missing required filename\" % getattr(\n            designspace_instance, \"name\", designspace_instance\n        )\n        if include_filenames is not None:\n            fname = normcase(normpath(fname))\n            if fname not in include_filenames:\n                continue\n\n        logger.debug(\"Applying instance data to %s\", fname)\n        # fontmake <= 1.4.0 compares the ufo paths returned from this function\n        # to the keys of a dict of designspace locations that have been passed\n        # through normpath (but not normcase). We do the same.\n        ufo = Font(normpath(os.path.join(basedir, fname)))\n\n        set_weight_class(ufo, designspace, designspace_instance)\n        set_width_class(ufo, designspace, designspace_instance)\n\n        glyphs_instance = InstanceDescriptorAsGSInstance(designspace_instance)\n        to_ufo_custom_params(None, ufo, glyphs_instance)\n        ufo.save()\n        instance_ufos.append(ufo)\n    return instance_ufos", "response": "Open UFO instances referenced by designspace and apply Glyphs instancedata to the UFOs and return updated UFOs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _to_ufo_features(self, master, ufo):\n\n    # Recover the original feature code if it was stored in the user data\n    original = master.userData[ORIGINAL_FEATURE_CODE_KEY]\n    if original is not None:\n        ufo.features.text = original\n        return\n\n    prefixes = []\n    for prefix in self.font.featurePrefixes:\n        strings = []\n        if prefix.name != ANONYMOUS_FEATURE_PREFIX_NAME:\n            strings.append(\"# Prefix: %s\\n\" % prefix.name)\n        strings.append(autostr(prefix.automatic))\n        strings.append(prefix.code)\n        prefixes.append(\"\".join(strings))\n\n    prefix_str = \"\\n\\n\".join(prefixes)\n\n    class_defs = []\n    for class_ in self.font.classes:\n        prefix = \"@\" if not class_.name.startswith(\"@\") else \"\"\n        name = prefix + class_.name\n        class_defs.append(\n            \"{}{} = [ {} ];\".format(autostr(class_.automatic), name, class_.code)\n        )\n    class_str = \"\\n\\n\".join(class_defs)\n\n    feature_defs = []\n    for feature in self.font.features:\n        code = feature.code\n        lines = [\"feature %s {\" % feature.name]\n        if feature.notes:\n            lines.append(\"# notes:\")\n            lines.extend(\"# \" + line for line in feature.notes.splitlines())\n        if feature.automatic:\n            lines.append(\"# automatic\")\n        if feature.disabled:\n            lines.append(\"# disabled\")\n            lines.extend(\"#\" + line for line in code.splitlines())\n        else:\n            lines.append(code)\n        lines.append(\"} %s;\" % feature.name)\n        feature_defs.append(\"\\n\".join(lines))\n    fea_str = \"\\n\\n\".join(feature_defs)\n\n    # Don't add a GDEF table when planning to round-trip. To get Glyphs.app-like\n    # results, we would need anchor propagation or user intervention. Glyphs.app\n    # only generates it on generating binaries.\n    gdef_str = None\n    if self.generate_GDEF:\n        if re.search(r\"^\\s*table\\s+GDEF\\s+{\", prefix_str, flags=re.MULTILINE):\n            raise ValueError(\n                \"The features already contain a `table GDEF {...}` statement. \"\n                \"Either delete it or set generate_GDEF to False.\"\n            )\n        gdef_str = _build_gdef(\n            ufo, self._designspace.lib.get(\"public.skipExportGlyphs\")\n        )\n\n    # make sure feature text is a unicode string, for defcon\n    full_text = (\n        \"\\n\\n\".join(filter(None, [class_str, prefix_str, fea_str, gdef_str])) + \"\\n\"\n    )\n    ufo.features.text = full_text if full_text.strip() else \"\"", "response": "Write an UFO s OpenType feature file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_gdef(ufo, skipExportGlyphs=None):\n    from glyphsLib import glyphdata\n\n    bases, ligatures, marks, carets = set(), set(), set(), {}\n    category_key = GLYPHLIB_PREFIX + \"category\"\n    subCategory_key = GLYPHLIB_PREFIX + \"subCategory\"\n\n    for glyph in ufo:\n        # Do not generate any entries for non-export glyphs, as looking them up on\n        # compilation will fail.\n        if skipExportGlyphs is not None:\n            if glyph.name in skipExportGlyphs:\n                continue\n\n        has_attaching_anchor = False\n        for anchor in glyph.anchors:\n            name = anchor.name\n            if name and not name.startswith(\"_\"):\n                has_attaching_anchor = True\n            if name and name.startswith(\"caret_\") and \"x\" in anchor:\n                carets.setdefault(glyph.name, []).append(round(anchor[\"x\"]))\n\n        # First check glyph.lib for category/subCategory overrides. Otherwise,\n        # use global values from GlyphData.\n        glyphinfo = glyphdata.get_glyph(glyph.name)\n        category = glyph.lib.get(category_key) or glyphinfo.category\n        subCategory = glyph.lib.get(subCategory_key) or glyphinfo.subCategory\n\n        if subCategory == \"Ligature\" and has_attaching_anchor:\n            ligatures.add(glyph.name)\n        elif category == \"Mark\" and (\n            subCategory == \"Nonspacing\" or subCategory == \"Spacing Combining\"\n        ):\n            marks.add(glyph.name)\n        elif has_attaching_anchor:\n            bases.add(glyph.name)\n\n    if not any((bases, ligatures, marks, carets)):\n        return None\n\n    def fmt(g):\n        return (\"[%s]\" % \" \".join(sorted(g, key=ufo.glyphOrder.index))) if g else \"\"\n\n    lines = [\n        \"table GDEF {\",\n        \"  # automatic\",\n        \"  GlyphClassDef\",\n        \"    %s, # Base\" % fmt(bases),\n        \"    %s, # Liga\" % fmt(ligatures),\n        \"    %s, # Mark\" % fmt(marks),\n        \"    ;\",\n    ]\n    for glyph, caretPos in sorted(carets.items()):\n        lines.append(\n            \"  LigatureCaretByPos %s %s;\"\n            % (glyph, \" \".join(unicode(p) for p in sorted(caretPos)))\n        )\n    lines.append(\"} GDEF;\")\n\n    return \"\\n\".join(lines)", "response": "Build a GDEF table statement for a UFO."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks for the comment that matches the given regex. If it does not match return the regex match object and list of statements without the special one.", "response": "def _pop_comment(self, statements, comment_re):\n        \"\"\"Look for the comment that matches the given regex.\n        If it matches, return the regex match object and list of statements\n        without the special one.\n        \"\"\"\n        res = []\n        match = None\n        for st in statements:\n            if match or not isinstance(st, ast.Comment):\n                res.append(st)\n                continue\n            match = comment_re.match(st.text)\n            if not match:\n                res.append(st)\n        return match, res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pop_comment_block(self, statements, header_re):\n        res = []\n        comments = []\n        match = None\n        st_iter = iter(statements)\n        # Look for the header\n        for st in st_iter:\n            if isinstance(st, ast.Comment):\n                match = header_re.match(st.text)\n                if match:\n                    # Drop this comment an move on to consuming the block\n                    break\n                else:\n                    res.append(st)\n            else:\n                res.append(st)\n        # Consume consecutive comments\n        for st in st_iter:\n            if isinstance(st, ast.Comment):\n                comments.append(st)\n            else:\n                # The block is over, keep the rest of the statements\n                res.append(st)\n                break\n        # Keep the rest of the statements\n        res.extend(list(st_iter))\n        # Inside the comment block, drop the pound sign and any common indent\n        return match, dedent(\"\".join(c.text[1:] + \"\\n\" for c in comments)), res", "response": "Look for a series of comments that start with one that matches the\n        regex and return the match and the dedented version of the comments."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the schema name.", "response": "def get_marshmallow_schema_name(self, plugin, schema):\n        \"\"\"Get the schema name.\n\n        If the schema doesn't exist, create it.\n        \"\"\"\n        try:\n            return plugin.openapi.refs[schema]\n        except KeyError:\n            plugin.spec.definition(schema.__name__, schema=schema)\n            return schema.__name__"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_ufo_components(self, ufo_glyph, layer):\n    pen = ufo_glyph.getPointPen()\n\n    for index, component in enumerate(layer.components):\n        pen.addComponent(component.name, component.transform)\n\n        if component.anchor:\n            if COMPONENT_INFO_KEY not in ufo_glyph.lib:\n                ufo_glyph.lib[COMPONENT_INFO_KEY] = []\n            ufo_glyph.lib[COMPONENT_INFO_KEY].append(\n                {\"name\": component.name, \"index\": index, \"anchor\": component.anchor}\n            )\n\n    # data related to components stored in lists of booleans\n    # each list's elements correspond to the components in order\n    for key in [\"alignment\", \"locked\", \"smartComponentValues\"]:\n        values = [getattr(c, key) for c in layer.components]\n        if any(values):\n            ufo_glyph.lib[_lib_key(key)] = values", "response": "Draw. glyphs components onto a pen adding them to the parent glyph."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nuses args_schema to parse request query arguments.", "response": "def request_args(self):\n        \"\"\"Use args_schema to parse request query arguments.\"\"\"\n        args = flask.request.args\n        data_raw = {}\n\n        for field_name, field in self.args_schema.fields.items():\n            alternate_field_name = field.load_from if MA2 else field.data_key\n\n            if alternate_field_name and alternate_field_name in args:\n                field_name = alternate_field_name\n            elif field_name not in args:\n                # getlist will return an empty list instead of raising a\n                # KeyError for args that aren't present.\n                continue\n\n            value = args.getlist(field_name)\n            if not self.is_list_field(field) and len(value) == 1:\n                value = value[0]\n\n            data_raw[field_name] = value\n\n        return self.deserialize_args(data_raw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the options to apply to the query for the view.", "response": "def query_options(self):\n        \"\"\"Options to apply to the query for the view.\n\n        Set this to configure relationship and column loading.\n\n        By default, this calls the ``get_query_options`` method on the\n        serializer with a `Load` object bound to the model, if that serializer\n        method exists.\n        \"\"\"\n        if not hasattr(self.serializer, 'get_query_options'):\n            return ()\n\n        return self.serializer.get_query_options(Load(self.model))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndrawing glyphs paths onto a pen.", "response": "def to_ufo_paths(self, ufo_glyph, layer):\n    \"\"\"Draw .glyphs paths onto a pen.\"\"\"\n    pen = ufo_glyph.getPointPen()\n\n    for path in layer.paths:\n        # the list is changed below, otherwise you can't draw more than once\n        # per session.\n        nodes = list(path.nodes)\n        for node in nodes:\n            self.to_ufo_node_user_data(ufo_glyph, node)\n\n        pen.beginPath()\n        if not nodes:\n            pen.endPath()\n            continue\n        if not path.closed:\n            node = nodes.pop(0)\n            assert node.type == \"line\", \"Open path starts with off-curve points\"\n            pen.addPoint(tuple(node.position), segmentType=\"move\")\n        else:\n            # In Glyphs.app, the starting node of a closed contour is always\n            # stored at the end of the nodes list.\n            nodes.insert(0, nodes.pop())\n        for node in nodes:\n            node_type = _to_ufo_node_type(node.type)\n            pen.addPoint(\n                tuple(node.position), segmentType=node_type, smooth=node.smooth\n            )\n        pen.endPath()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_cached_property(func):\n    @property\n    @functools.wraps(func)\n    def wrapped(self):\n        cached_value = context.get_for_view(self, func.__name__, UNDEFINED)\n        if cached_value is not UNDEFINED:\n            return cached_value\n\n        value = func(self)\n        context.set_for_view(self, func.__name__, value)\n\n        return value\n\n    return wrapped", "response": "Make the given method a per - request cached property."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a string that can identify this UFO in logs.", "response": "def _ufo_logging_ref(ufo):\n    \"\"\"Return a string that can identify this UFO in logs.\"\"\"\n    if ufo.path:\n        return os.path.basename(ufo.path)\n    return ufo.info.styleName"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_datetime(src=None):\n    if src is None:\n        return None\n    string = src.replace('\"', \"\")\n    # parse timezone ourselves, since %z is not always supported\n    # see: http://bugs.python.org/issue6641\n    m = UTC_OFFSET_RE.match(string)\n    if m:\n        sign = 1 if m.group(\"sign\") == \"+\" else -1\n        tz_hours = sign * int(m.group(\"hours\"))\n        tz_minutes = sign * int(m.group(\"minutes\"))\n        offset = datetime.timedelta(hours=tz_hours, minutes=tz_minutes)\n        string = string[:-6]\n    else:\n        # no explicit timezone\n        offset = datetime.timedelta(0)\n    if \"AM\" in string or \"PM\" in string:\n        datetime_obj = datetime.datetime.strptime(string, \"%Y-%m-%d %I:%M:%S %p\")\n    else:\n        datetime_obj = datetime.datetime.strptime(string, \"%Y-%m-%d %H:%M:%S\")\n    return datetime_obj + offset", "response": "Parse a datetime object from a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_color(src=None):\n    # type: (Optional[str]) -> Optional[Union[Tuple[int, ...], int]]\n    \"\"\"Parse a string representing a color value.\n\n    Color is either a fixed color (when coloring something from the UI, see\n    the GLYPHS_COLORS constant) or a list of the format [u8, u8, u8, u8],\n\n    Glyphs does not support an alpha channel as of 2.5.1 (confirmed by Georg\n    Seifert), and always writes a 1 to it. This was brought up and is probably\n    corrected in the next versions.\n    https://github.com/googlei18n/glyphsLib/pull/363#issuecomment-390418497\n    \"\"\"\n    if src is None:\n        return None\n\n    # Tuple.\n    if src[0] == \"(\":\n        rgba = tuple(int(v) for v in src[1:-1].split(\",\") if v)\n\n        if not (len(rgba) == 4 and all(0 <= v < 256 for v in rgba)):\n            raise ValueError(\n                \"Broken color tuple: {}. Must have four values from 0 to 255.\".format(\n                    src\n                )\n            )\n\n        return rgba\n\n    # Constant.\n    return int(src)", "response": "Parses a string representing a color value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a GSFont object to a. glyphs file.", "response": "def dump(obj, fp):\n    \"\"\"Write a GSFont object to a .glyphs file.\n    'fp' should be a (writable) file object.\n    \"\"\"\n    writer = Writer(fp)\n    logger.info(\"Writing .glyphs file\")\n    writer.write(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef loads(s):\n    p = Parser(current_type=glyphsLib.classes.GSFont)\n    logger.info(\"Parsing .glyphs file\")\n    data = p.parse(s)\n    return data", "response": "Read a. glyphs file from a ( unicode ) str object or from\n    a UTF - 8 encoded bytes object. Return a GSFont object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses data into an existing GSFont instance.", "response": "def parse_into_object(self, res, text):\n        \"\"\"Parse data into an existing GSFont instance.\"\"\"\n\n        text = tounicode(text, encoding=\"utf-8\")\n\n        m = self.start_dict_re.match(text, 0)\n        if m:\n            i = self._parse_dict_into_object(res, text, 1)\n        else:\n            self._fail(\"not correct file format\", text, 0)\n        if text[i:].strip():\n            self._fail(\"Unexpected trailing content\", text, i)\n        return i"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse(self, text, i):\n\n        m = self.start_dict_re.match(text, i)\n        if m:\n            parsed = m.group(0)\n            i += len(parsed)\n            return self._parse_dict(text, i)\n\n        m = self.start_list_re.match(text, i)\n        if m:\n            parsed = m.group(0)\n            i += len(parsed)\n            return self._parse_list(text, i)\n\n        m = self.value_re.match(text, i)\n        if m:\n            parsed = m.group(0)\n            i += len(parsed)\n            if hasattr(self.current_type, \"read\"):\n                reader = self.current_type()\n                # Give the escaped value to `read` to be symetrical with\n                # `plistValue` which handles the escaping itself.\n                value = reader.read(m.group(1))\n                return value, i\n\n            value = self._trim_value(m.group(1))\n\n            if self.current_type in (None, dict, OrderedDict):\n                self.current_type = self._guess_current_type(parsed, value)\n\n            if self.current_type == bool:\n                value = bool(int(value))  # bool(u'0') returns True\n                return value, i\n\n            value = self.current_type(value)\n\n            return value, i\n\n        m = self.hex_re.match(text, i)\n        if m:\n            from glyphsLib.types import BinaryData\n\n            parsed, value = m.group(0), m.group(1)\n            decoded = BinaryData.fromHex(value)\n            i += len(parsed)\n            return decoded, i\n        else:\n            self._fail(\"Unexpected content\", text, i)", "response": "Recursive function to parse a single dictionary list or value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_dict(self, text, i):\n        old_current_type = self.current_type\n        new_type = self.current_type\n        if new_type is None:\n            # customparameter.value needs to be set from the found value\n            new_type = dict\n        elif type(new_type) == list:\n            new_type = new_type[0]\n        res = new_type()\n        i = self._parse_dict_into_object(res, text, i)\n        self.current_type = old_current_type\n        return res, i", "response": "Parse a dictionary from source text starting at i."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a list from source text starting at i.", "response": "def _parse_list(self, text, i):\n        \"\"\"Parse a list from source text starting at i.\"\"\"\n\n        res = []\n        end_match = self.end_list_re.match(text, i)\n        old_current_type = self.current_type\n        while not end_match:\n            list_item, i = self._parse(text, i)\n            res.append(list_item)\n\n            end_match = self.end_list_re.match(text, i)\n\n            if not end_match:\n                m = self.list_delim_re.match(text, i)\n                if not m:\n                    self._fail(\"Missing delimiter in list before content\", text, i)\n                parsed = m.group(0)\n                i += len(parsed)\n            self.current_type = old_current_type\n\n        parsed = end_match.group(0)\n        i += len(parsed)\n        return res, i"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntrim double quotes off the ends of a value and convert escapes to unicode.", "response": "def _trim_value(self, value):\n        \"\"\"Trim double quotes off the ends of a value, un-escaping inner\n        double quotes and literal backslashes. Also convert escapes to unicode.\n        If the string is not quoted, return it unmodified.\n        \"\"\"\n\n        if value[0] == '\"':\n            assert value[-1] == '\"'\n            value = value[1:-1].replace('\\\\\"', '\"').replace(\"\\\\\\\\\", \"\\\\\")\n            return Parser._unescape_re.sub(Parser._unescape_fn, value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fail(self, message, text, i):\n\n        raise ValueError(\"{}:\\n{}\".format(message, text[i : i + 79]))", "response": "Raise an exception with given message and text at i."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_stylemap_names(\n    family_name, style_name, is_bold=False, is_italic=False, linked_style=None\n):\n    \"\"\"Build UFO `styleMapFamilyName` and `styleMapStyleName` based on the\n    family and style names, and the entries in the \"Style Linking\" section\n    of the \"Instances\" tab in the \"Font Info\".\n\n    The value of `styleMapStyleName` can be either \"regular\", \"bold\", \"italic\"\n    or \"bold italic\", depending on the values of `is_bold` and `is_italic`.\n\n    The `styleMapFamilyName` is a combination of the `family_name` and the\n    `linked_style`.\n\n    If `linked_style` is unset or set to 'Regular', the linked style is equal\n    to the style_name with the last occurrences of the strings 'Regular',\n    'Bold' and 'Italic' stripped from it.\n    \"\"\"\n\n    styleMapStyleName = (\n        \" \".join(\n            s for s in (\"bold\" if is_bold else \"\", \"italic\" if is_italic else \"\") if s\n        )\n        or \"regular\"\n    )\n    if not linked_style or linked_style == \"Regular\":\n        linked_style = _get_linked_style(style_name, is_bold, is_italic)\n    if linked_style:\n        styleMapFamilyName = (family_name or \"\") + \" \" + linked_style\n    else:\n        styleMapFamilyName = family_name\n    return styleMapFamilyName, styleMapStyleName", "response": "Build the UFO styleMapFamilyName and styleMapStyleName from family_name and style_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets postscript blue values from Glyphs alignment zones.", "response": "def to_ufo_blue_values(self, ufo, master):\n    \"\"\"Set postscript blue values from Glyphs alignment zones.\"\"\"\n\n    alignment_zones = master.alignmentZones\n    blue_values = []\n    other_blues = []\n    for zone in sorted(alignment_zones):\n        pos = zone.position\n        size = zone.size\n        val_list = blue_values if pos == 0 or size >= 0 else other_blues\n        val_list.extend(sorted((pos, pos + size)))\n\n    ufo.info.postscriptBlueValues = blue_values\n    ufo.info.postscriptOtherBlues = other_blues"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the GSFontMaster alignmentZones from the postscript blue values.", "response": "def to_glyphs_blue_values(self, ufo, master):\n    \"\"\"Sets the GSFontMaster alignmentZones from the postscript blue values.\"\"\"\n\n    zones = []\n    blue_values = _pairs(ufo.info.postscriptBlueValues)\n    other_blues = _pairs(ufo.info.postscriptOtherBlues)\n    for y1, y2 in blue_values:\n        size = y2 - y1\n        if y2 == 0:\n            pos = 0\n            size = -size\n        else:\n            pos = y1\n        zones.append(self.glyphs_module.GSAlignmentZone(pos, size))\n    for y1, y2 in other_blues:\n        size = y1 - y2\n        pos = y2\n        zones.append(self.glyphs_module.GSAlignmentZone(pos, size))\n\n    master.alignmentZones = sorted(zones, key=lambda zone: -zone.position)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_glyphs_filter(filter_str, is_pre=False):\n    elements = filter_str.split(\";\")\n\n    if elements[0] == \"\":\n        logger.error(\n            \"Failed to parse glyphs filter, expecting a filter name: \\\n             %s\",\n            filter_str,\n        )\n        return None\n\n    result = {\"name\": elements[0]}\n    for idx, elem in enumerate(elements[1:]):\n        if not elem:\n            # skip empty arguments\n            continue\n        if \":\" in elem:\n            # Key value pair\n            key, value = elem.split(\":\", 1)\n            if key.lower() in [\"include\", \"exclude\"]:\n                if idx != len(elements[1:]) - 1:\n                    logger.error(\n                        \"{} can only present as the last argument in the filter. \"\n                        \"{} is ignored.\".format(key, elem)\n                    )\n                    continue\n                result[key.lower()] = re.split(\"[ ,]+\", value)\n            else:\n                if \"kwargs\" not in result:\n                    result[\"kwargs\"] = {}\n                result[\"kwargs\"][key] = cast_to_number_or_bool(value)\n        else:\n            if \"args\" not in result:\n                result[\"args\"] = []\n            result[\"args\"].append(cast_to_number_or_bool(elem))\n    if is_pre:\n        result[\"pre\"] = True\n    return result", "response": "Parses a glyphs custom filter string into a dictionary that can be used to create a new glyph app."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a UFO path from family_name and style_name.", "response": "def build_ufo_path(out_dir, family_name, style_name):\n    \"\"\"Build string to use as a UFO path.\"\"\"\n\n    return os.path.join(\n        out_dir,\n        \"%s-%s.ufo\"\n        % ((family_name or \"\").replace(\" \", \"\"), (style_name or \"\").replace(\" \", \"\")),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking sure old UFO data is removed.", "response": "def clean_ufo(path):\n    \"\"\"Make sure old UFO data is removed, as it may contain deleted glyphs.\"\"\"\n\n    if path.endswith(\".ufo\") and os.path.exists(path):\n        shutil.rmtree(path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ufo_create_background_layer_for_all_glyphs(ufo_font):\n    # type: (defcon.Font) -> None\n    \"\"\"Create a background layer for all glyphs in ufo_font if not present to\n    reduce roundtrip differences.\"\"\"\n\n    if \"public.background\" in ufo_font.layers:\n        background = ufo_font.layers[\"public.background\"]\n    else:\n        background = ufo_font.newLayer(\"public.background\")\n\n    for glyph in ufo_font:\n        if glyph.name not in background:\n            background.newGlyph(glyph.name)", "response": "Create a background layer for all glyphs in ufo_font."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cast_to_number_or_bool(inputstr):\n    if inputstr.strip().lower() == \"true\":\n        return True\n    elif inputstr.strip().lower() == \"false\":\n        return False\n    try:\n        return int(inputstr)\n    except ValueError:\n        try:\n            return float(inputstr)\n        except ValueError:\n            return inputstr", "response": "Cast a string to int float or bool. Return original string if it can t be\n    converted."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_ufo_background_image(self, ufo_glyph, layer):\n    image = layer.backgroundImage\n    if image is None:\n        return\n    ufo_image = ufo_glyph.image\n    ufo_image.fileName = image.path\n    ufo_image.transformation = image.transform\n    ufo_glyph.lib[CROP_KEY] = list(image.crop)\n    ufo_glyph.lib[LOCKED_KEY] = image.locked\n    ufo_glyph.lib[ALPHA_KEY] = image.alpha", "response": "Copy the backgound image from the GSLayer to the UFO Glyph."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_glyphs_background_image(self, ufo_glyph, layer):\n    ufo_image = ufo_glyph.image\n    if ufo_image.fileName is None:\n        return\n    image = self.glyphs_module.GSBackgroundImage()\n    image.path = ufo_image.fileName\n    image.transform = Transform(*ufo_image.transformation)\n    if CROP_KEY in ufo_glyph.lib:\n        x, y, w, h = ufo_glyph.lib[CROP_KEY]\n        image.crop = Rect(Point(x, y), Size(w, h))\n    if LOCKED_KEY in ufo_glyph.lib:\n        image.locked = ufo_glyph.lib[LOCKED_KEY]\n    if ALPHA_KEY in ufo_glyph.lib:\n        image.alpha = ufo_glyph.lib[ALPHA_KEY]\n    layer.backgroundImage = image", "response": "Copy the background image from the UFO Glyph to the GSLayer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_resource(\n        self,\n        base_rule,\n        base_view,\n        alternate_view=None,\n        alternate_rule=None,\n        id_rule=None,\n        app=None,\n    ):\n        \"\"\"Add route or routes for a resource.\n\n        :param str base_rule: The URL rule for the resource. This will be\n            prefixed by the API prefix.\n        :param base_view: Class-based view for the resource.\n        :param alternate_view: If specified, an alternate class-based view for\n            the resource. Usually, this will be a detail view, when the base\n            view is a list view.\n        :param alternate_rule: If specified, the URL rule for the alternate\n            view. This will be prefixed by the API prefix. This is mutually\n            exclusive with id_rule, and must not be specified if alternate_view\n            is not specified.\n        :type alternate_rule: str or None\n        :param id_rule: If specified, a suffix to append to base_rule to get\n            the alternate view URL rule. If alternate_view is specified, and\n            alternate_rule is not, then this defaults to '<id>'. This is\n            mutually exclusive with alternate_rule, and must not be specified\n            if alternate_view is not specified.\n        :type id_rule: str or None\n        :param app: If specified, the application to which to add the route(s).\n            Otherwise, this will be the bound application, if present.\n        \"\"\"\n        if alternate_view:\n            if not alternate_rule:\n                id_rule = id_rule or DEFAULT_ID_RULE\n                alternate_rule = posixpath.join(base_rule, id_rule)\n            else:\n                assert id_rule is None\n        else:\n            assert alternate_rule is None\n            assert id_rule is None\n\n        app = self._get_app(app)\n        endpoint = self._get_endpoint(base_view, alternate_view)\n\n        # Store the view rules for reference. Doesn't support multiple routes\n        # mapped to same view.\n        views = app.extensions['resty'].views\n\n        base_rule_full = '{}{}'.format(self.prefix, base_rule)\n        base_view_func = base_view.as_view(endpoint)\n\n        if not alternate_view:\n            app.add_url_rule(base_rule_full, view_func=base_view_func)\n            views[base_view] = Resource(base_view, base_rule_full)\n            return\n\n        alternate_rule_full = '{}{}'.format(self.prefix, alternate_rule)\n        alternate_view_func = alternate_view.as_view(endpoint)\n\n        @functools.wraps(base_view_func)\n        def view_func(*args, **kwargs):\n            if flask.request.url_rule.rule == base_rule_full:\n                return base_view_func(*args, **kwargs)\n            else:\n                return alternate_view_func(*args, **kwargs)\n\n        app.add_url_rule(\n            base_rule_full, view_func=view_func, endpoint=endpoint,\n            methods=base_view.methods,\n        )\n        app.add_url_rule(\n            alternate_rule_full, view_func=view_func, endpoint=endpoint,\n            methods=alternate_view.methods,\n        )\n\n        views[base_view] = Resource(base_view, base_rule_full)\n        views[alternate_view] = Resource(alternate_view, alternate_rule_full)", "response": "Add a resource to the resource cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_ping(self, rule, status_code=200, app=None):\n        app = self._get_app(app)\n\n        @app.route(rule)\n        def ping():\n            return '', status_code", "response": "Add a ping route."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef masters(self):\n        if self._sources:\n            for source in self._sources.values():\n                yield source.font\n            return\n\n        # Store set of actually existing master (layer) ids. This helps with\n        # catching dangling layer data that Glyphs may ignore, e.g. when\n        # copying glyphs from other fonts with, naturally, different master\n        # ids. Note: Masters have unique ids according to the Glyphs\n        # documentation and can therefore be stored in a set.\n        master_layer_ids = {m.id for m in self.font.masters}\n\n        # stores background data from \"associated layers\"\n        supplementary_layer_data = []\n\n        # TODO(jamesgk) maybe create one font at a time to reduce memory usage\n        # TODO: (jany) in the future, return a lazy iterator that builds UFOs\n        #     on demand.\n        self.to_ufo_font_attributes(self.family_name)\n\n        # Generate the main (master) layers first.\n        for glyph in self.font.glyphs:\n            for layer in glyph.layers.values():\n                if layer.associatedMasterId != layer.layerId:\n                    # The layer is not the main layer of a master\n                    # Store all layers, even the invalid ones, and just skip\n                    # them and print a warning below.\n                    supplementary_layer_data.append((glyph, layer))\n                    continue\n\n                ufo_layer = self.to_ufo_layer(glyph, layer)\n                ufo_glyph = ufo_layer.newGlyph(glyph.name)\n                self.to_ufo_glyph(ufo_glyph, layer, glyph)\n\n        # And sublayers (brace, bracket, ...) second.\n        for glyph, layer in supplementary_layer_data:\n            if (\n                layer.layerId not in master_layer_ids\n                and layer.associatedMasterId not in master_layer_ids\n            ):\n                if self.minimize_glyphs_diffs:\n                    self.logger.warning(\n                        '{}, glyph \"{}\": Layer \"{}\" is dangling and will be '\n                        \"skipped. Did you copy a glyph from a different font?\"\n                        \" If so, you should clean up any phantom layers not \"\n                        \"associated with an actual master.\".format(\n                            self.font.familyName, glyph.name, layer.layerId\n                        )\n                    )\n                continue\n\n            if not layer.name:\n                # Empty layer names are invalid according to the UFO spec.\n                if self.minimize_glyphs_diffs:\n                    self.logger.warning(\n                        '{}, glyph \"{}\": Contains layer without a name which '\n                        \"will be skipped.\".format(self.font.familyName, glyph.name)\n                    )\n                continue\n\n            # Save processing bracket layers for when designspace() is called, as we\n            # have to extract them to free-standing glyphs.\n            if (\n                \"[\" in layer.name\n                and \"]\" in layer.name\n                and \".background\" not in layer.name\n            ):\n                self.bracket_layers.append(layer)\n            else:\n                ufo_layer = self.to_ufo_layer(glyph, layer)\n                ufo_glyph = ufo_layer.newGlyph(glyph.name)\n                self.to_ufo_glyph(ufo_glyph, layer, layer.parent)\n\n        for source in self._sources.values():\n            ufo = source.font\n            if self.propagate_anchors:\n                self.to_ufo_propagate_font_anchors(ufo)\n            for layer in ufo.layers:\n                self.to_ufo_layer_lib(layer)\n\n        # Sanitize skip list and write it to both Designspace- and UFO-level lib keys.\n        # The latter is unnecessary when using e.g. the `ufo2ft.compile*FromDS`\n        # functions, but the data may take a different path. Writing it everywhere can\n        # save on surprises/logic in other software.\n        skip_export_glyphs = self._designspace.lib.get(\"public.skipExportGlyphs\")\n        if skip_export_glyphs is not None:\n            skip_export_glyphs = sorted(set(skip_export_glyphs))\n            self._designspace.lib[\"public.skipExportGlyphs\"] = skip_export_glyphs\n            for source in self._sources.values():\n                source.font.lib[\"public.skipExportGlyphs\"] = skip_export_glyphs\n\n        self.to_ufo_features()  # This depends on the glyphOrder key\n        self.to_ufo_groups()\n        self.to_ufo_kerning()\n\n        for source in self._sources.values():\n            yield source.font", "response": "Get an iterator over master UFOs that match the family_name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef designspace(self):\n        if self._designspace_is_complete:\n            return self._designspace\n\n        self._designspace_is_complete = True\n        list(self.masters)  # Make sure that the UFOs are built\n        self.to_designspace_axes()\n        self.to_designspace_sources()\n        self.to_designspace_instances()\n        self.to_designspace_family_user_data()\n\n        if self.bracket_layers:\n            self._apply_bracket_layers()\n\n        # append base style shared by all masters to designspace file name\n        base_family = self.family_name or \"Unnamed\"\n        base_style = find_base_style(self.font.masters)\n        if base_style:\n            base_style = \"-\" + base_style\n        name = (base_family + base_style).replace(\" \", \"\") + \".designspace\"\n        self.designspace.filename = name\n\n        return self._designspace", "response": "Get a designspace Document instance that links the masters together\n        and holds instance data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying bracket layers to a free -standing UFO glyph.", "response": "def _apply_bracket_layers(self):\n        \"\"\"Extract bracket layers in a GSGlyph into free-standing UFO glyphs with\n        Designspace substitution rules.\n\n        As of Glyphs.app 2.6, only single axis bracket layers are supported, we\n        assume the axis to be the first axis in the Designspace. Bracket layer\n        backgrounds are not round-tripped.\n\n        A glyph can have more than one bracket layer but Designspace\n        rule/OpenType variation condition sets apply all substitutions in a rule\n        in a range, so we have to potentially sort bracket layers into rule\n        buckets. Example: if a glyph \"x\" has two bracket layers [300] and [600]\n        and glyph \"a\" has bracket layer [300] and the bracket axis tops out at\n        1000, we need the following Designspace rules:\n\n        - BRACKET.300.600  # min 300, max 600 on the bracket axis.\n          - x -> x.BRACKET.300\n        - BRACKET.600.1000\n          - x -> x.BRACKET.600\n        - BRACKET.300.1000\n          - a -> a.BRACKET.300\n        \"\"\"\n        if not self._designspace.axes:\n            raise ValueError(\n                \"Cannot apply bracket layers unless at least one axis is defined.\"\n            )\n        bracket_axis = self._designspace.axes[0]\n\n        # Determine the axis scale in design space because crossovers/locations are\n        # in design space (axis.default/minimum/maximum may be user space).\n        if bracket_axis.map:\n            axis_scale = [design_location for _, design_location in bracket_axis.map]\n            bracket_axis_min = min(axis_scale)\n            bracket_axis_max = max(axis_scale)\n        else:  # No mapping means user and design space are the same.\n            bracket_axis_min = bracket_axis.minimum\n            bracket_axis_max = bracket_axis.maximum\n\n        # 1. bracket_layer_map: Organize all bracket layers by crossover value, so\n        #    we can go through the layers by location and copy them to free-standing\n        #    glyphs.\n        # 2. glyph_crossovers: Keep track of the crossover values of a single glyph, so\n        #    we can easily sort them into rule buckets.\n        # 3. glyph_sanity_counter: Count the number of master layers providing\n        #    bracket layers per glyph and crossover value. We currently only support\n        #    the situation where there is a bracket layer for _all_ masters, what the\n        #    Glyphs.app tutorial calls 'Changing All Masters'.\n        bracket_layer_map = defaultdict(list)  # type: Dict[int, List[classes.GSLayer]]\n        glyph_crossovers = defaultdict(set)  # type: Dict[str, Set[int]]\n        glyph_sanity_counter = defaultdict(\n            list\n        )  # type: Dict[Tuple[str, int], List[str]]\n        for layer in self.bracket_layers:\n            glyph_name = layer.parent.name\n            n = layer.name\n            try:\n                bracket_crossover = int(n[n.index(\"[\") + 1 : n.index(\"]\")])\n            except ValueError:\n                raise ValueError(\n                    \"Only bracket layers with one numerical (design space) location \"\n                    \"(meaning the first axis in the designspace file) are currently \"\n                    \"supported.\"\n                )\n            if not bracket_axis_min <= bracket_crossover <= bracket_axis_max:\n                raise ValueError(\n                    \"Glyph {glyph_name}: Bracket layer {layer_name} must be within the \"\n                    \"design space bounds of the {bracket_axis_name} axis: minimum \"\n                    \"{bracket_axis_minimum}, maximum {bracket_axis_maximum}.\".format(\n                        glyph_name=glyph_name,\n                        layer_name=n,\n                        bracket_axis_name=bracket_axis.name,\n                        bracket_axis_minimum=bracket_axis_min,\n                        bracket_axis_maximum=bracket_axis_max,\n                    )\n                )\n            bracket_layer_map[bracket_crossover].append(layer)\n            glyph_crossovers[glyph_name].add(bracket_crossover)\n            glyph_sanity_counter[(glyph_name, bracket_crossover)].append(\n                layer.associatedMasterId\n            )\n\n        # Check that each bracket layer is present in all master layers.\n        unbalanced_bracket_layers = []\n        n_masters = len(list(self.masters))\n        for ((glyph_name, _), master_layer_ids) in glyph_sanity_counter.items():\n            if not len(master_layer_ids) == n_masters:\n                unbalanced_bracket_layers.append(glyph_name)\n        if unbalanced_bracket_layers:\n            raise ValueError(\n                \"Currently, we only support bracket layers that are present on all \"\n                \"masters, i.e. what the Glyphs.app tutorial calls 'Changing All \"\n                \"Masters'. There is a/are bracket layer(s) missing \"\n                \"for glyph(s) {unbalanced_glyphs}.\".format(\n                    unbalanced_glyphs=unbalanced_bracket_layers\n                )\n            )\n\n        # Sort crossovers into buckets.\n        rule_bucket = defaultdict(list)  # type: Dict[Tuple[int, int], List[int]]\n        for glyph_name, crossovers in sorted(glyph_crossovers.items()):\n            for crossover_min, crossover_max in util.pairwise(\n                sorted(crossovers) + [bracket_axis_max]\n            ):\n                rule_bucket[(int(crossover_min), int(crossover_max))].append(glyph_name)\n\n        # Generate rules for the bracket layers.\n        for (axis_range_min, axis_range_max), glyph_names in sorted(\n            rule_bucket.items()\n        ):\n            rule_name = \"BRACKET.{}.{}\".format(axis_range_min, axis_range_max)\n            glyph_sub_suffix = \".BRACKET.{}\".format(axis_range_min)\n            rule = designspaceLib.RuleDescriptor()\n            rule.name = rule_name\n            rule.conditionSets.append(\n                [\n                    {\n                        \"name\": bracket_axis.name,\n                        \"minimum\": axis_range_min,\n                        \"maximum\": axis_range_max,\n                    }\n                ]\n            )\n            rule.subs.extend(\n                [\n                    (glyph_name, glyph_name + glyph_sub_suffix)\n                    for glyph_name in glyph_names\n                ]\n            )\n            self._designspace.addRule(rule)\n\n        # Finally, copy bracket layers to their own glyphs.\n        for location, layers in bracket_layer_map.items():\n            for layer in layers:\n                ufo_font = self._sources[\n                    layer.associatedMasterId or layer.layerId\n                ].font.layers.defaultLayer\n                ufo_glyph_name = \"{glyph_name}.BRACKET.{location}\".format(\n                    glyph_name=layer.parent.name, location=location\n                )\n                ufo_glyph = ufo_font.newGlyph(ufo_glyph_name)\n                self.to_ufo_glyph(ufo_glyph, layer, layer.parent)\n                ufo_glyph.unicodes = []  # Avoid cmap interference\n                ufo_glyph.lib[GLYPHLIB_PREFIX + \"_originalLayerName\"] = layer.name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the GSFont object for this Glyphs file.", "response": "def font(self):\n        \"\"\"Get the GSFont built from the UFOs + designspace.\"\"\"\n        if self._font is not None:\n            return self._font\n\n        # Sort UFOS in the original order from the Glyphs file\n        sorted_sources = self.to_glyphs_ordered_masters()\n\n        # Convert all full source UFOs to Glyphs masters. Sources with layer names\n        # are assumed to be sparse or \"brace\" layers and are ignored because Glyphs\n        # considers them to be special layers and will handle them itself.\n        self._font = self.glyphs_module.GSFont()\n        self._sources = OrderedDict()  # Same as in UFOBuilder\n        for index, source in enumerate(s for s in sorted_sources if not s.layerName):\n            master = self.glyphs_module.GSFontMaster()\n\n            # Filter bracket glyphs out of public.glyphOrder.\n            if GLYPH_ORDER_KEY in source.font.lib:\n                source.font.lib[GLYPH_ORDER_KEY] = [\n                    glyph_name\n                    for glyph_name in source.font.lib[GLYPH_ORDER_KEY]\n                    if \".BRACKET.\" not in glyph_name\n                ]\n\n            self.to_glyphs_font_attributes(source, master, is_initial=(index == 0))\n            self.to_glyphs_master_attributes(source, master)\n            self._font.masters.insert(len(self._font.masters), master)\n            self._sources[master.id] = source\n\n            # First, move free-standing bracket glyphs back to layers to avoid dealing\n            # with GSLayer transplantation.\n            for bracket_glyph in [g for g in source.font if \".BRACKET.\" in g.name]:\n                base_glyph, threshold = bracket_glyph.name.split(\".BRACKET.\")\n                try:\n                    int(threshold)\n                except ValueError:\n                    raise ValueError(\n                        \"Glyph '{}' has malformed bracket layer name. Must be '<glyph \"\n                        \"name>.BRACKET.<crossover value>'.\".format(bracket_glyph)\n                    )\n                layer_name = bracket_glyph.lib.get(\n                    GLYPHLIB_PREFIX + \"_originalLayerName\", \"[{}]\".format(threshold)\n                )\n                if layer_name not in source.font.layers:\n                    ufo_layer = source.font.newLayer(layer_name)\n                else:\n                    ufo_layer = source.font.layers[layer_name]\n                bracket_glyph_new = ufo_layer.newGlyph(base_glyph)\n                bracket_glyph_new.copyDataFromGlyph(bracket_glyph)\n\n                # Remove all freestanding bracket layer glyphs from all layers.\n                for layer in source.font.layers:\n                    if bracket_glyph.name in layer:\n                        del layer[bracket_glyph.name]\n\n            for layer in _sorted_backgrounds_last(source.font.layers):\n                self.to_glyphs_layer_lib(layer)\n                for glyph in layer:\n                    self.to_glyphs_glyph(glyph, layer, master)\n\n        self.to_glyphs_features()\n        self.to_glyphs_groups()\n        self.to_glyphs_kerning()\n\n        # Now that all GSGlyph are built, restore the glyph order\n        if self.designspace.sources:\n            first_ufo = self.designspace.sources[0].font\n            if GLYPH_ORDER_KEY in first_ufo.lib:\n                glyph_order = first_ufo.lib[GLYPH_ORDER_KEY]\n                lookup = {name: i for i, name in enumerate(glyph_order)}\n                self.font.glyphs = sorted(\n                    self.font.glyphs, key=lambda glyph: lookup.get(glyph.name, 1 << 63)\n                )\n            # FIXME: (jany) We only do that on the first one. Maybe we should\n            # merge the various `public.glyphorder` values?\n\n            # Restore the layer ordering in each glyph\n            for glyph in self._font.glyphs:\n                self.to_glyphs_layer_order(glyph)\n\n        self.to_glyphs_family_user_data_from_designspace()\n        self.to_glyphs_axes()\n        self.to_glyphs_sources()\n        self.to_glyphs_instances()\n\n        return self._font"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake sure that the user - provided designspace has loaded fonts and that the UFOs and UFOs are not already loaded.", "response": "def _valid_designspace(self, designspace):\n        \"\"\"Make sure that the user-provided designspace has loaded fonts and\n        that names are the same as those from the UFOs.\n        \"\"\"\n        # TODO: (jany) really make a copy to avoid modifying the original object\n        copy = designspace\n        # Load only full UFO masters, sparse or \"brace\" layer sources are assumed\n        # to point to existing layers within one of the full masters.\n        for source in (s for s in copy.sources if not s.layerName):\n            if not hasattr(source, \"font\") or source.font is None:\n                if source.path:\n                    # FIXME: (jany) consider not changing the caller's objects\n                    source.font = defcon.Font(source.path)\n                else:\n                    dirname = os.path.dirname(designspace.path)\n                    ufo_path = os.path.join(dirname, source.filename)\n                    source.font = defcon.Font(ufo_path)\n            if source.location is None:\n                source.location = {}\n            for name in (\"familyName\", \"styleName\"):\n                if getattr(source, name) != getattr(source.font.info, name):\n                    self.logger.warning(\n                        dedent(\n                            \"\"\"\\\n                    The {name} is different between the UFO and the designspace source:\n                        source filename: {filename}\n                        source {name}: {source_name}\n                        ufo {name}: {ufo_name}\n\n                    The UFO name will be used.\n                    \"\"\"\n                        ).format(\n                            name=name,\n                            filename=source.filename,\n                            source_name=getattr(source, name),\n                            ufo_name=getattr(source.font.info, name),\n                        )\n                    )\n                    setattr(source, name, getattr(source.font.info, name))\n        return copy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fake_designspace(self, ufos):\n        designspace = designspaceLib.DesignSpaceDocument()\n\n        ufo_to_location = defaultdict(dict)\n\n        # Make weight and width axis if relevant\n        for info_key, axis_def in zip(\n            (\"openTypeOS2WeightClass\", \"openTypeOS2WidthClass\"),\n            (WEIGHT_AXIS_DEF, WIDTH_AXIS_DEF),\n        ):\n            axis = designspace.newAxisDescriptor()\n            axis.tag = axis_def.tag\n            axis.name = axis_def.name\n            mapping = []\n            for ufo in ufos:\n                user_loc = getattr(ufo.info, info_key)\n                if user_loc is not None:\n                    design_loc = class_to_value(axis_def.tag, user_loc)\n                    mapping.append((user_loc, design_loc))\n                    ufo_to_location[ufo][axis_def.name] = design_loc\n\n            mapping = sorted(set(mapping))\n            if len(mapping) > 1:\n                axis.map = mapping\n                axis.minimum = min([user_loc for user_loc, _ in mapping])\n                axis.maximum = max([user_loc for user_loc, _ in mapping])\n                axis.default = min(\n                    axis.maximum, max(axis.minimum, axis_def.default_user_loc)\n                )\n                designspace.addAxis(axis)\n\n        for ufo in ufos:\n            source = designspace.newSourceDescriptor()\n            source.font = ufo\n            source.familyName = ufo.info.familyName\n            source.styleName = ufo.info.styleName\n            # source.name = '%s %s' % (source.familyName, source.styleName)\n            source.path = ufo.path\n            source.location = ufo_to_location[ufo]\n            designspace.addSource(source)\n\n        # UFO-level skip list lib keys are usually ignored, except when we don't have a\n        # Designspace file to start from. If they exist in the UFOs, promote them to a\n        # Designspace-level lib key. However, to avoid accidents, expect the list to\n        # exist in none or be the same in all UFOs.\n        if any(\"public.skipExportGlyphs\" in ufo.lib for ufo in ufos):\n            skip_export_glyphs = {\n                frozenset(ufo.lib.get(\"public.skipExportGlyphs\", [])) for ufo in ufos\n            }\n            if len(skip_export_glyphs) == 1:\n                designspace.lib[\"public.skipExportGlyphs\"] = sorted(\n                    next(iter(skip_export_glyphs))\n                )\n            else:\n                raise ValueError(\n                    \"The `public.skipExportGlyphs` list of all UFOs must either not \"\n                    \"exist or be the same in every UFO.\"\n                )\n\n        return designspace", "response": "Build a fake designspace with the given UFOs as sources."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_ufo_kerning(self, ufo, kerning_data):\n\n    warning_msg = \"Non-existent glyph class %s found in kerning rules.\"\n\n    for left, pairs in kerning_data.items():\n        match = re.match(r\"@MMK_L_(.+)\", left)\n        left_is_class = bool(match)\n        if left_is_class:\n            left = \"public.kern1.%s\" % match.group(1)\n            if left not in ufo.groups:\n                self.logger.warning(warning_msg % left)\n        for right, kerning_val in pairs.items():\n            match = re.match(r\"@MMK_R_(.+)\", right)\n            right_is_class = bool(match)\n            if right_is_class:\n                right = \"public.kern2.%s\" % match.group(1)\n                if right not in ufo.groups:\n                    self.logger.warning(warning_msg % right)\n            ufo.kerning[left, right] = kerning_val", "response": "Add. glyphs kerning to an UFO."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_glyphs_kerning(self):\n    for master_id, source in self._sources.items():\n        for (left, right), value in source.font.kerning.items():\n            left_match = UFO_KERN_GROUP_PATTERN.match(left)\n            right_match = UFO_KERN_GROUP_PATTERN.match(right)\n            if left_match:\n                left = \"@MMK_L_{}\".format(left_match.group(2))\n            if right_match:\n                right = \"@MMK_R_{}\".format(right_match.group(2))\n            self.font.setKerningForPair(master_id, left, right, value)", "response": "Add UFO kerning to GSFont."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes a custom parameter name.", "response": "def _normalize_custom_param_name(name):\n    \"\"\"Replace curved quotes with straight quotes in a custom parameter name.\n    These should be the only keys with problematic (non-ascii) characters,\n    since they can be user-generated.\n    \"\"\"\n\n    replacements = ((\"\\u2018\", \"'\"), (\"\\u2019\", \"'\"), (\"\\u201C\", '\"'), (\"\\u201D\", '\"'))\n    for orig, replacement in replacements:\n        name = name.replace(orig, replacement)\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_default_params(ufo):\n    for _, ufo_name, default_value in DEFAULT_PARAMETERS:\n        if getattr(ufo.info, ufo_name) is None:\n            if isinstance(default_value, list):\n                # Prevent problem if the same default value list is put in\n                # several unrelated objects.\n                default_value = default_value[:]\n            setattr(ufo.info, ufo_name, default_value)", "response": "Set Glyphs. app s default parameters when different from ufo2ft ones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the first and only custom parameter matching the given name.", "response": "def get_custom_value(self, key):\n        \"\"\"Return the first and only custom parameter matching the given name.\"\"\"\n        self._handled.add(key)\n        values = self._lookup[key]\n        if len(values) > 1:\n            raise RuntimeError(\n                \"More than one value for this customParameter: {}\".format(key)\n            )\n        if values:\n            return values[0]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_custom_values(self, key):\n        self._handled.add(key)\n        return self._lookup[key]", "response": "Return a set of values for the given customParameter name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets one custom parameter with the given value.", "response": "def set_custom_value(self, key, value):\n        \"\"\"Set one custom parameter with the given value.\n        We assume that the list of custom parameters does not already contain\n        the given parameter so we only append.\n        \"\"\"\n        self._owner.customParameters.append(\n            self._glyphs_module.GSCustomParameter(name=key, value=value)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset several values for the given key.", "response": "def set_custom_values(self, key, values):\n        \"\"\"Set several values for the customParameter with the given key.\n        We append one GSCustomParameter per value.\n        \"\"\"\n        for value in values:\n            self.set_custom_value(key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncast some known data in custom parameters.", "response": "def setValue(self, value):\n        \"\"\"Cast some known data in custom parameters.\"\"\"\n        if self.name in self._CUSTOM_INT_PARAMS:\n            value = int(value)\n        elif self.name in self._CUSTOM_FLOAT_PARAMS:\n            value = float(value)\n        elif self.name in self._CUSTOM_BOOL_PARAMS:\n            value = bool(value)\n        elif self.name in self._CUSTOM_INTLIST_PARAMS:\n            value = readIntlist(value)\n        elif self.name in self._CUSTOM_DICT_PARAMS:\n            parser = Parser()\n            value = parser.parse(value)\n        elif self.name == \"note\":\n            value = unicode(value)\n        self._value = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef name(self, name):\n        weight, width, custom_name = self._splitName(name)\n        self.set_all_name_components(name, weight, width, custom_name)", "response": "This function will take the given name and split it into weight width customName and possibly the full name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_all_name_components(self, name, weight, width, custom_name):\n        self.weight = weight or \"Regular\"\n        self.width = width or \"Regular\"\n        self.customName = custom_name or \"\"\n        # Only store the requested name if we can't build it from the parts\n        if self._joinName() == name:\n            self._name = None\n            del self.customParameters[\"Master Name\"]\n        else:\n            self._name = name\n            self.customParameters[\"Master Name\"] = name", "response": "This function sets the master. name weight and width and custom name of the master."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntaking the PLIST string of a dict and returns the same string representation", "response": "def _encode_dict_as_string(value):\n        \"\"\"Takes the PLIST string of a dict, and returns the same string\n        encoded such that it can be included in the string representation\n        of a GSNode.\"\"\"\n        # Strip the first and last newlines\n        if value.startswith(\"{\\n\"):\n            value = \"{\" + value[2:]\n        if value.endswith(\"\\n}\"):\n            value = value[:-2] + \"}\"\n        # escape double quotes and newlines\n        return value.replace('\"', '\\\\\"').replace(\"\\\\n\", \"\\\\\\\\n\").replace(\"\\n\", \"\\\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _indices(self):\n        path = self.parent\n        layer = path.parent\n        for path_index in range(len(layer.paths)):\n            if path == layer.paths[path_index]:\n                for node_index in range(len(path.nodes)):\n                    if self == path.nodes[node_index]:\n                        return Point(path_index, node_index)\n        return None", "response": "Find the path_index and node_index that identify the given node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the GSNode that is refered to by the given indices.", "response": "def _find_node_by_indices(self, point):\n        \"\"\"\"Find the GSNode that is refered to by the given indices.\n\n        See GSNode::_indices()\n        \"\"\"\n        path_index, node_index = point\n        path = self.paths[int(path_index)]\n        node = path.nodes[int(node_index)]\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npropagate anchors from parent glyphs components to the parent glyphs.", "response": "def to_ufo_propagate_font_anchors(self, ufo):\n    \"\"\"Copy anchors from parent glyphs' components to the parent.\"\"\"\n\n    processed = set()\n    for glyph in ufo:\n        _propagate_glyph_anchors(self, ufo, glyph, processed)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npropagating anchors for a single parent glyph.", "response": "def _propagate_glyph_anchors(self, ufo, parent, processed):\n    \"\"\"Propagate anchors for a single parent glyph.\"\"\"\n\n    if parent.name in processed:\n        return\n    processed.add(parent.name)\n\n    base_components = []\n    mark_components = []\n    anchor_names = set()\n    to_add = {}\n    for component in parent.components:\n        try:\n            glyph = ufo[component.baseGlyph]\n        except KeyError:\n            self.logger.warning(\n                \"Anchors not propagated for inexistent component {} in glyph {}\".format(\n                    component.baseGlyph, parent.name\n                )\n            )\n        else:\n            _propagate_glyph_anchors(self, ufo, glyph, processed)\n            if any(a.name.startswith(\"_\") for a in glyph.anchors):\n                mark_components.append(component)\n            else:\n                base_components.append(component)\n                anchor_names |= {a.name for a in glyph.anchors}\n\n    for anchor_name in anchor_names:\n        # don't add if parent already contains this anchor OR any associated\n        # ligature anchors (e.g. \"top_1, top_2\" for \"top\")\n        if not any(a.name.startswith(anchor_name) for a in parent.anchors):\n            _get_anchor_data(to_add, ufo, base_components, anchor_name)\n\n    for component in mark_components:\n        _adjust_anchors(to_add, ufo, component)\n\n    # we sort propagated anchors to append in a deterministic order\n    for name, (x, y) in sorted(to_add.items()):\n        anchor_dict = {\"name\": name, \"x\": x, \"y\": y}\n        parent.appendAnchor(glyph.anchorClass(anchorDict=anchor_dict))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _adjust_anchors(anchor_data, ufo, component):\n\n    glyph = ufo[component.baseGlyph]\n    t = Transform(*component.transformation)\n    for anchor in glyph.anchors:\n        # only adjust if this anchor has data and the component also contains\n        # the associated mark anchor (e.g. \"_top\" for \"top\")\n        if anchor.name in anchor_data and any(\n            a.name == \"_\" + anchor.name for a in glyph.anchors\n        ):\n            anchor_data[anchor.name] = t.transformPoint((anchor.x, anchor.y))", "response": "Adjust anchor data to which a mark component may have been attached."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_ufo_glyph_anchors(self, glyph, anchors):\n\n    for anchor in anchors:\n        x, y = anchor.position\n        anchor_dict = {\"name\": anchor.name, \"x\": x, \"y\": y}\n        glyph.appendAnchor(anchor_dict)", "response": "Add. glyphs anchors to a glyph."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_glyphs_glyph_anchors(self, ufo_glyph, layer):\n    for ufo_anchor in ufo_glyph.anchors:\n        anchor = self.glyphs_module.GSAnchor()\n        anchor.name = ufo_anchor.name\n        anchor.position = Point(ufo_anchor.x, ufo_anchor.y)\n        layer.anchors.append(anchor)", "response": "Add UFO glif anchors to a GSLayer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cached_property(func):\n    name = func.__name__\n    doc = func.__doc__\n\n    def getter(self, name=name):\n        try:\n            return self.__dict__[name]\n        except KeyError:\n            self.__dict__[name] = value = func(self)\n            return value\n\n    getter.func_name = name\n    return property(getter, doc=doc)", "response": "A property decorator that caches the computed\n    property value in the object s instance dict the first\nalues time it is accessed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the cosine and sin for the given angle in degrees.", "response": "def cos_sin_deg(deg):\n    \"\"\"Return the cosine and sin for the given angle\n    in degrees, with special-case handling of multiples\n    of 90 for perfect right angles\n    \"\"\"\n    deg = deg % 360.0\n    if deg == 90.0:\n        return 0.0, 1.0\n    elif deg == 180.0:\n        return -1.0, 0\n    elif deg == 270.0:\n        return 0, -1.0\n    rad = math.radians(deg)\n    return math.cos(rad), math.sin(rad)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scale(cls, *scaling):\n        if len(scaling) == 1:\n            sx = sy = float(scaling[0])\n        else:\n            sx, sy = scaling\n        return tuple.__new__(cls, (sx, 0.0, 0.0, 0.0, sy, 0.0, 0.0, 0.0, 1.0))", "response": "Create a scaling transform from a scalar or vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef shear(cls, x_angle=0, y_angle=0):\n        sx = math.tan(math.radians(x_angle))\n        sy = math.tan(math.radians(y_angle))\n        return tuple.__new__(cls, (1.0, sy, 0.0, sx, 1.0, 0.0, 0.0, 0.0, 1.0))", "response": "Create a shear transform along one or both axes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotation(cls, angle, pivot=None):\n        ca, sa = cos_sin_deg(angle)\n        if pivot is None:\n            return tuple.__new__(cls, (ca, sa, 0.0, -sa, ca, 0.0, 0.0, 0.0, 1.0))\n        else:\n            px, py = pivot\n            return tuple.__new__(\n                cls,\n                (\n                    ca,\n                    sa,\n                    px - px * ca + py * sa,\n                    -sa,\n                    ca,\n                    py - px * sa - py * ca,\n                    0.0,\n                    0.0,\n                    1.0,\n                ),\n            )", "response": "Create a rotation transform at the specified angle about the specified pivot point."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef determinant(self):\n        a, b, c, d, e, f, g, h, i = self\n        return a * e - b * d", "response": "The determinant of the transform matrix. This is equal to the area scaling factor when the transform matrix is applied to a shape."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntruing if the transform is rectilinear.", "response": "def is_rectilinear(self):\n        \"\"\"True if the transform is rectilinear, i.e., whether a shape would\n        remain axis-aligned, within rounding limits, after applying the\n        transform.\n        \"\"\"\n        a, b, c, d, e, f, g, h, i = self\n        return (abs(a) < EPSILON and abs(e) < EPSILON) or (\n            abs(d) < EPSILON and abs(b) < EPSILON\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntrues if the transform is conformal i. e. angles between points are preserved after applying the transform.", "response": "def is_conformal(self):\n        \"\"\"True if the transform is conformal, i.e., if angles between points\n        are preserved after applying the transform, within rounding limits.\n        This implies that the transform has no effective shear.\n        \"\"\"\n        a, b, c, d, e, f, g, h, i = self\n        return abs(a * b + d * e) < EPSILON"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_orthonormal(self):\n        a, b, c, d, e, f, g, h, i = self\n        return (\n            self.is_conformal\n            and abs(1.0 - (a * a + d * d)) < EPSILON\n            and abs(1.0 - (b * b + e * e)) < EPSILON\n        )", "response": "True if the transform is orthonormal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomparing transforms for approximate equality.", "response": "def almost_equals(self, other):\n        \"\"\"Compare transforms for approximate equality.\n\n        :param other: Transform being compared.\n        :type other: Affine\n        :return: True if absolute difference between each element\n            of each respective tranform matrix < ``EPSILON``.\n        \"\"\"\n        for i in (0, 1, 2, 3, 4, 5):\n            if abs(self[i] - other[i]) >= EPSILON:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform a sequence of points or vectors in place.", "response": "def itransform(self, seq):\n        \"\"\"Transform a sequence of points or vectors in place.\n\n        :param seq: Mutable sequence of :class:`~planar.Vec2` to be\n            transformed.\n        :returns: None, the input sequence is mutated in place.\n        \"\"\"\n        if self is not identity and self != identity:\n            sa, sb, sc, sd, se, sf, _, _, _ = self\n            for i, (x, y) in enumerate(seq):\n                seq[i] = (x * sa + y * sd + sc, x * sb + y * se + sf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_glyph(glyph_name, data=None):\n\n    # Read data on first use.\n    if data is None:\n        global GLYPHDATA\n        if GLYPHDATA is None:\n            GLYPHDATA = GlyphData.from_files(\n                os.path.join(\n                    os.path.dirname(glyphsLib.__file__), \"data\", \"GlyphData.xml\"\n                ),\n                os.path.join(\n                    os.path.dirname(glyphsLib.__file__),\n                    \"data\",\n                    \"GlyphData_Ideographs.xml\",\n                ),\n            )\n        data = GLYPHDATA\n\n    # Look up data by full glyph name first.\n    attributes = _lookup_attributes(glyph_name, data)\n\n    production_name = attributes.get(\"production\")\n    if production_name is None:\n        production_name = _construct_production_name(glyph_name, data=data)\n\n    unicode_value = attributes.get(\"unicode\")\n\n    category = attributes.get(\"category\")\n    sub_category = attributes.get(\"subCategory\")\n    if category is None:\n        category, sub_category = _construct_category(glyph_name, data)\n\n    # TODO: Determine script in ligatures.\n    script = attributes.get(\"script\")\n    description = attributes.get(\"description\")\n\n    return Glyph(\n        glyph_name,\n        production_name,\n        unicode_value,\n        category,\n        sub_category,\n        script,\n        description,\n    )", "response": "Return a named tuple containing information derived from a glyph name akin to GSGlyphInfo."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlooking up glyph attributes in data by glyph name alternative name or production name in order or return empty dictionary.", "response": "def _lookup_attributes(glyph_name, data):\n    \"\"\"Look up glyph attributes in data by glyph name, alternative name or\n    production name in order or return empty dictionary.\n\n    Look up by alternative and production names for legacy projects and\n    because of issue #232.\n    \"\"\"\n    attributes = (\n        data.names.get(glyph_name)\n        or data.alternative_names.get(glyph_name)\n        or data.production_names.get(glyph_name)\n        or {}\n    )\n    return attributes"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns an AGL - compliant name string or None if we can t make one.", "response": "def _agl_compliant_name(glyph_name):\n    \"\"\"Return an AGL-compliant name string or None if we can't make one.\"\"\"\n    MAX_GLYPH_NAME_LENGTH = 63\n    clean_name = re.sub(\"[^0-9a-zA-Z_.]\", \"\", glyph_name)\n    if len(clean_name) > MAX_GLYPH_NAME_LENGTH:\n        return None\n    return clean_name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nderiving ( sub ) category of a glyph name.", "response": "def _construct_category(glyph_name, data):\n    \"\"\"Derive (sub)category of a glyph name.\"\"\"\n    # Glyphs creates glyphs that start with an underscore as \"non-exportable\" glyphs or\n    # construction helpers without a category.\n    if glyph_name.startswith(\"_\"):\n        return None, None\n\n    # Glyph variants (e.g. \"fi.alt\") don't have their own entry, so we strip e.g. the\n    # \".alt\" and try a second lookup with just the base name. A variant is hopefully in\n    # the same category as its base glyph.\n    base_name = glyph_name.split(\".\", 1)[0]\n    base_attribute = data.names.get(base_name) or {}\n    if base_attribute:\n        category = base_attribute.get(\"category\")\n        sub_category = base_attribute.get(\"subCategory\")\n        return category, sub_category\n\n    # Detect ligatures.\n    if \"_\" in base_name:\n        base_names = base_name.split(\"_\")\n        base_names_attributes = [_lookup_attributes(name, data) for name in base_names]\n        first_attribute = base_names_attributes[0]\n\n        # If the first part is a Mark, Glyphs 2.6 declares the entire glyph a Mark\n        if first_attribute.get(\"category\") == \"Mark\":\n            category = first_attribute.get(\"category\")\n            sub_category = first_attribute.get(\"subCategory\")\n            return category, sub_category\n\n        # If the first part is a Letter...\n        if first_attribute.get(\"category\") == \"Letter\":\n            # ... and the rest are only marks or separators or don't exist, the\n            # sub_category is that of the first part ...\n            if all(\n                a.get(\"category\") in (None, \"Mark\", \"Separator\")\n                for a in base_names_attributes[1:]\n            ):\n                category = first_attribute.get(\"category\")\n                sub_category = first_attribute.get(\"subCategory\")\n                return category, sub_category\n            # ... otherwise, a ligature.\n            category = first_attribute.get(\"category\")\n            sub_category = \"Ligature\"\n            return category, sub_category\n\n        # TODO: Cover more cases. E.g. \"one_one\" -> (\"Number\", \"Ligature\") but\n        # \"one_onee\" -> (\"Number\", \"Composition\").\n\n    # Still nothing? Maybe we're looking at something like \"uni1234.alt\", try\n    # using fontTools' AGL module to convert the base name to something meaningful.\n    # Corner case: when looking at ligatures, names that don't exist in the AGLFN\n    # are skipped, so len(\"acutecomb_o\") == 2 but len(\"dotaccentcomb_o\") == 1.\n    character = fontTools.agl.toUnicode(base_name)\n    if character:\n        category, sub_category = _translate_category(\n            glyph_name, unicodedata.category(character[0])\n        )\n        return category, sub_category\n\n    return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a translation from Unicode category letters to Glyphs categories.", "response": "def _translate_category(glyph_name, unicode_category):\n    \"\"\"Return a translation from Unicode category letters to Glyphs\n    categories.\"\"\"\n    DEFAULT_CATEGORIES = {\n        None: (\"Letter\", None),\n        \"Cc\": (\"Separator\", None),\n        \"Cf\": (\"Separator\", \"Format\"),\n        \"Cn\": (\"Symbol\", None),\n        \"Co\": (\"Letter\", \"Compatibility\"),\n        \"Ll\": (\"Letter\", \"Lowercase\"),\n        \"Lm\": (\"Letter\", \"Modifier\"),\n        \"Lo\": (\"Letter\", None),\n        \"Lt\": (\"Letter\", \"Uppercase\"),\n        \"Lu\": (\"Letter\", \"Uppercase\"),\n        \"Mc\": (\"Mark\", \"Spacing Combining\"),\n        \"Me\": (\"Mark\", \"Enclosing\"),\n        \"Mn\": (\"Mark\", \"Nonspacing\"),\n        \"Nd\": (\"Number\", \"Decimal Digit\"),\n        \"Nl\": (\"Number\", None),\n        \"No\": (\"Number\", \"Decimal Digit\"),\n        \"Pc\": (\"Punctuation\", None),\n        \"Pd\": (\"Punctuation\", \"Dash\"),\n        \"Pe\": (\"Punctuation\", \"Parenthesis\"),\n        \"Pf\": (\"Punctuation\", \"Quote\"),\n        \"Pi\": (\"Punctuation\", \"Quote\"),\n        \"Po\": (\"Punctuation\", None),\n        \"Ps\": (\"Punctuation\", \"Parenthesis\"),\n        \"Sc\": (\"Symbol\", \"Currency\"),\n        \"Sk\": (\"Mark\", \"Spacing\"),\n        \"Sm\": (\"Symbol\", \"Math\"),\n        \"So\": (\"Symbol\", None),\n        \"Zl\": (\"Separator\", None),\n        \"Zp\": (\"Separator\", None),\n        \"Zs\": (\"Separator\", \"Space\"),\n    }\n\n    glyphs_category = DEFAULT_CATEGORIES.get(unicode_category, (\"Letter\", None))\n\n    # Exception: Something like \"one_two\" should be a (_, Ligature),\n    # \"acutecomb_brevecomb\" should however stay (Mark, Nonspacing).\n    if \"_\" in glyph_name and glyphs_category[0] != \"Mark\":\n        return glyphs_category[0], \"Ligature\"\n\n    return glyphs_category"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the production name for a glyph name from the GlyphData. xml database according to the AGL specification.", "response": "def _construct_production_name(glyph_name, data=None):\n    \"\"\"Return the production name for a glyph name from the GlyphData.xml\n    database according to the AGL specification.\n\n    This should be run only if there is no official entry with a production\n    name in it.\n\n    Handles single glyphs (e.g. \"brevecomb\") and ligatures (e.g.\n    \"brevecomb_acutecomb\"). Returns None when a valid and semantically\n    meaningful production name can't be constructed or when the AGL\n    specification would be violated, get_glyph() will use the bare glyph\n    name then.\n\n    Note:\n    - Glyph name is the full name, e.g. \"brevecomb_acutecomb.case\".\n    - Base name is the base part, e.g. \"brevecomb_acutecomb\"\n    - Suffix is e.g. \"case\".\n    \"\"\"\n\n    # At this point, we have already checked the data for the full glyph name, so\n    # directly go to the base name here (e.g. when looking at \"fi.alt\").\n    base_name, dot, suffix = glyph_name.partition(\".\")\n    glyphinfo = _lookup_attributes(base_name, data)\n    if glyphinfo and glyphinfo.get(\"production\"):\n        # Found the base glyph.\n        return glyphinfo[\"production\"] + dot + suffix\n\n    if glyph_name in fontTools.agl.AGL2UV or base_name in fontTools.agl.AGL2UV:\n        # Glyph name is actually an AGLFN name.\n        return glyph_name\n\n    if \"_\" not in base_name:\n        # Nothing found so far and the glyph name isn't a ligature (\"_\"\n        # somewhere in it). The name does not carry any discernable Unicode\n        # semantics, so just return something sanitized.\n        return _agl_compliant_name(glyph_name)\n\n    # So we have a ligature that is not mapped in the data. Split it up and\n    # look up the individual parts.\n    base_name_parts = base_name.split(\"_\")\n\n    # If all parts are in the AGLFN list, the glyph name is our production\n    # name already.\n    if all(part in fontTools.agl.AGL2UV for part in base_name_parts):\n        return _agl_compliant_name(glyph_name)\n\n    # Turn all parts of the ligature into production names.\n    _character_outside_BMP = False\n    production_names = []\n    for part in base_name_parts:\n        if part in fontTools.agl.AGL2UV:\n            # A name present in the AGLFN is a production name already.\n            production_names.append(part)\n        else:\n            part_entry = data.names.get(part) or {}\n            part_production_name = part_entry.get(\"production\")\n            if part_production_name:\n                production_names.append(part_production_name)\n\n                # Take note if there are any characters outside the Unicode\n                # BMP, e.g. \"u10FFF\" or \"u10FFFF\". Do not catch e.g. \"u013B\"\n                # though.\n                if len(part_production_name) > 5 and _is_unicode_u_value(\n                    part_production_name\n                ):\n                    _character_outside_BMP = True\n            else:\n                # We hit a part that does not seem to be a valid glyph name known to us,\n                # so the entire glyph name can't carry Unicode meaning. Return it\n                # sanitized.\n                return _agl_compliant_name(glyph_name)\n\n    # Some names Glyphs uses resolve to other names that are not uniXXXX names and may\n    # contain dots (e.g. idotaccent -> i.loclTRK). If there is any name with a \".\" in\n    # it before the last element, punt. We'd have to introduce a \".\" into the ligature\n    # midway, which is invalid according to the AGL. Example: \"a_i.loclTRK\" is valid,\n    # but \"a_i.loclTRK_a\" isn't.\n    if any(\".\" in part for part in production_names[:-1]):\n        return _agl_compliant_name(glyph_name)\n\n    # If any production name starts with a \"uni\" and there are none of the\n    # \"uXXXXX\" format, try to turn all parts into \"uni\" names and concatenate\n    # them.\n    if not _character_outside_BMP and any(\n        part.startswith(\"uni\") for part in production_names\n    ):\n        uni_names = []\n\n        for part in production_names:\n            if part.startswith(\"uni\"):\n                uni_names.append(part[3:])\n            elif len(part) == 5 and _is_unicode_u_value(part):\n                uni_names.append(part[1:])\n            elif part in fontTools.agl.AGL2UV:\n                uni_names.append(\"{:04X}\".format(fontTools.agl.AGL2UV[part]))\n            else:\n                return None\n\n        final_production_name = \"uni\" + \"\".join(uni_names) + dot + suffix\n    else:\n        final_production_name = \"_\".join(production_names) + dot + suffix\n\n    return _agl_compliant_name(final_production_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn GlyphData holding data from a list of XML file paths.", "response": "def from_files(cls, *glyphdata_files):\n        \"\"\"Return GlyphData holding data from a list of XML file paths.\"\"\"\n        name_mapping = {}\n        alt_name_mapping = {}\n        production_name_mapping = {}\n\n        for glyphdata_file in glyphdata_files:\n            glyph_data = xml.etree.ElementTree.parse(glyphdata_file).getroot()\n            for glyph in glyph_data:\n                glyph_name = glyph.attrib[\"name\"]\n                glyph_name_alternatives = glyph.attrib.get(\"altNames\")\n                glyph_name_production = glyph.attrib.get(\"production\")\n\n                name_mapping[glyph_name] = glyph.attrib\n                if glyph_name_alternatives:\n                    alternatives = glyph_name_alternatives.replace(\" \", \"\").split(\",\")\n                    for glyph_name_alternative in alternatives:\n                        alt_name_mapping[glyph_name_alternative] = glyph.attrib\n                if glyph_name_production:\n                    production_name_mapping[glyph_name_production] = glyph.attrib\n\n        return cls(name_mapping, alt_name_mapping, production_name_mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_to_ufos(\n    file_or_path, include_instances=False, family_name=None, propagate_anchors=True\n):\n    \"\"\"Load an unpacked .glyphs object to UFO objects.\"\"\"\n\n    if hasattr(file_or_path, \"read\"):\n        font = load(file_or_path)\n    else:\n        with open(file_or_path, \"r\", encoding=\"utf-8\") as ifile:\n            font = load(ifile)\n    logger.info(\"Loading to UFOs\")\n    return to_ufos(\n        font,\n        include_instances=include_instances,\n        family_name=family_name,\n        propagate_anchors=propagate_anchors,\n    )", "response": "Load an unpacked. glyphs object to UFOs objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite and return UFOs from the masters and designspace defined in a. glyphs file.", "response": "def build_masters(\n    filename,\n    master_dir,\n    designspace_instance_dir=None,\n    designspace_path=None,\n    family_name=None,\n    propagate_anchors=True,\n    minimize_glyphs_diffs=False,\n    normalize_ufos=False,\n    create_background_layers=False,\n    generate_GDEF=True,\n    store_editor_state=True,\n):\n    \"\"\"Write and return UFOs from the masters and the designspace defined in a\n    .glyphs file.\n\n    Args:\n        master_dir: Directory where masters are written.\n        designspace_instance_dir: If provided, a designspace document will be\n            written alongside the master UFOs though no instances will be built.\n        family_name: If provided, the master UFOs will be given this name and\n            only instances with this name will be included in the designspace.\n\n    Returns:\n        A named tuple of master UFOs (`ufos`) and the path to the designspace\n        file (`designspace_path`).\n    \"\"\"\n\n    font = GSFont(filename)\n\n    if not os.path.isdir(master_dir):\n        os.mkdir(master_dir)\n\n    if designspace_instance_dir is None:\n        instance_dir = None\n    else:\n        instance_dir = os.path.relpath(designspace_instance_dir, master_dir)\n\n    designspace = to_designspace(\n        font,\n        family_name=family_name,\n        propagate_anchors=propagate_anchors,\n        instance_dir=instance_dir,\n        minimize_glyphs_diffs=minimize_glyphs_diffs,\n        generate_GDEF=generate_GDEF,\n        store_editor_state=store_editor_state,\n    )\n\n    # Only write full masters to disk. This assumes that layer sources are always part\n    # of another full master source, which must always be the case in a .glyphs file.\n    ufos = {}\n    for source in designspace.sources:\n        if source.filename in ufos:\n            assert source.font is ufos[source.filename]\n            continue\n\n        if create_background_layers:\n            ufo_create_background_layer_for_all_glyphs(source.font)\n\n        ufo_path = os.path.join(master_dir, source.filename)\n        clean_ufo(ufo_path)\n        source.font.save(ufo_path)\n\n        if normalize_ufos:\n            import ufonormalizer\n\n            ufonormalizer.normalizeUFO(ufo_path, writeModTimes=False)\n\n        ufos[source.filename] = source.font\n\n    if not designspace_path:\n        designspace_path = os.path.join(master_dir, designspace.filename)\n    designspace.write(designspace_path)\n\n    return Masters(ufos, designspace_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef glyphs2ufo(options):\n    if options.output_dir is None:\n        options.output_dir = os.path.dirname(options.glyphs_file) or \".\"\n\n    if options.designspace_path is None:\n        options.designspace_path = os.path.join(\n            options.output_dir,\n            os.path.basename(os.path.splitext(options.glyphs_file)[0]) + \".designspace\",\n        )\n\n    # If options.instance_dir is None, instance UFO paths in the designspace\n    # file will either use the value in customParameter's FULL_FILENAME_KEY or be\n    # made relative to \"instance_ufos/\".\n    glyphsLib.build_masters(\n        options.glyphs_file,\n        options.output_dir,\n        options.instance_dir,\n        designspace_path=options.designspace_path,\n        minimize_glyphs_diffs=options.no_preserve_glyphsapp_metadata,\n        propagate_anchors=options.propagate_anchors,\n        normalize_ufos=options.normalize_ufos,\n        create_background_layers=options.create_background_layers,\n        generate_GDEF=options.generate_GDEF,\n        store_editor_state=not options.no_store_editor_state,\n    )", "response": "Converts a Glyphs. app source file into UFO masters and a designspace file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ufo2glyphs(options):\n    import fontTools.designspaceLib\n    import defcon\n\n    sources = options.designspace_file_or_UFOs\n    designspace_file = None\n    if (\n        len(sources) == 1\n        and sources[0].endswith(\".designspace\")\n        and os.path.isfile(sources[0])\n    ):\n        designspace_file = sources[0]\n        designspace = fontTools.designspaceLib.DesignSpaceDocument()\n        designspace.read(designspace_file)\n        object_to_read = designspace\n    elif all(source.endswith(\".ufo\") and os.path.isdir(source) for source in sources):\n        ufos = [defcon.Font(source) for source in sources]\n        ufos.sort(\n            key=lambda ufo: [  # Order the masters by weight and width\n                ufo.info.openTypeOS2WeightClass or 400,\n                ufo.info.openTypeOS2WidthClass or 5,\n            ]\n        )\n        object_to_read = ufos\n    else:\n        print(\n            \"Please specify just one designspace file *or* one or more \"\n            \"UFOs. They must end in '.designspace' or '.ufo', respectively.\",\n            file=sys.stderr,\n        )\n        return 1\n\n    font = glyphsLib.to_glyphs(\n        object_to_read, minimize_ufo_diffs=options.no_preserve_glyphsapp_metadata\n    )\n\n    # Make the Glyphs file more suitable for roundtrip:\n    font.customParameters[\"Disable Last Change\"] = options.enable_last_change\n    font.disablesAutomaticAlignment = options.enable_automatic_alignment\n\n    if options.output_path:\n        font.save(options.output_path)\n    else:\n        if designspace_file:\n            filename_to_write = os.path.splitext(designspace_file)[0] + \".glyphs\"\n        else:\n            filename_to_write = os.path.join(\n                os.path.dirname(sources[0]),\n                font.familyName.replace(\" \", \"\") + \".glyphs\",\n            )\n        font.save(filename_to_write)", "response": "Convert one designspace file or one or more UFOs to Glyphs. app source file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _has_manual_kern_feature(font):\n    return any(f for f in font.features if f.name == \"kern\" and not f.automatic)", "response": "Return true if the GSFont contains a manually written kern feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_ufo_family_user_data(self, ufo):\n    if not self.use_designspace:\n        ufo.lib[FONT_USER_DATA_KEY] = dict(self.font.userData)", "response": "Set family - wide user data as Glyphs does."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting master - specific user data as Glyphs does.", "response": "def to_ufo_master_user_data(self, ufo, master):\n    \"\"\"Set master-specific user data as Glyphs does.\"\"\"\n    for key in master.userData.keys():\n        if _user_data_has_no_special_meaning(key):\n            ufo.lib[key] = master.userData[key]\n\n    # Restore UFO data files. This code assumes that all paths are POSIX paths.\n    if UFO_DATA_KEY in master.userData:\n        for filename, data in master.userData[UFO_DATA_KEY].items():\n            ufo.data[filename] = bytes(data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the GSFont userData from the designspace family - wide lib data.", "response": "def to_glyphs_family_user_data_from_designspace(self):\n    \"\"\"Set the GSFont userData from the designspace family-wide lib data.\"\"\"\n    target_user_data = self.font.userData\n    for key, value in self.designspace.lib.items():\n        if key == UFO2FT_FEATURE_WRITERS_KEY and value == DEFAULT_FEATURE_WRITERS:\n            # if the designspace contains featureWriters settings that are the\n            # same as glyphsLib default settings, there's no need to store them\n            continue\n        if _user_data_has_no_special_meaning(key):\n            target_user_data[key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the GSFont userData from the UFO family - wide lib data.", "response": "def to_glyphs_family_user_data_from_ufo(self, ufo):\n    \"\"\"Set the GSFont userData from the UFO family-wide lib data.\"\"\"\n    target_user_data = self.font.userData\n    try:\n        for key, value in ufo.lib[FONT_USER_DATA_KEY].items():\n            # Existing values taken from the designspace lib take precedence\n            if key not in target_user_data.keys():\n                target_user_data[key] = value\n    except KeyError:\n        # No FONT_USER_DATA in ufo.lib\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_glyphs_master_user_data(self, ufo, master):\n    target_user_data = master.userData\n    for key, value in ufo.lib.items():\n        if _user_data_has_no_special_meaning(key):\n            target_user_data[key] = value\n\n    # Save UFO data files\n    if ufo.data.fileNames:\n        from glyphsLib.types import BinaryData\n\n        ufo_data = {}\n        for os_filename in ufo.data.fileNames:\n            filename = posixpath.join(*os_filename.split(os.path.sep))\n            ufo_data[filename] = BinaryData(ufo.data[os_filename])\n        master.userData[UFO_DATA_KEY] = ufo_data", "response": "Set the GSFontMaster userData from the UFO master - specific lib data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_ufos(\n    font,\n    include_instances=False,\n    family_name=None,\n    propagate_anchors=True,\n    ufo_module=defcon,\n    minimize_glyphs_diffs=False,\n    generate_GDEF=True,\n    store_editor_state=True,\n):\n    \"\"\"Take a GSFont object and convert it into one UFO per master.\n\n    Takes in data as Glyphs.app-compatible classes, as documented at\n    https://docu.glyphsapp.com/\n\n    If include_instances is True, also returns the parsed instance data.\n\n    If family_name is provided, the master UFOs will be given this name and\n    only instances with this name will be returned.\n\n    If generate_GDEF is True, write a `table GDEF {...}` statement in the\n    UFO's features.fea, containing GlyphClassDef and LigatureCaretByPos.\n    \"\"\"\n    builder = UFOBuilder(\n        font,\n        ufo_module=ufo_module,\n        family_name=family_name,\n        propagate_anchors=propagate_anchors,\n        minimize_glyphs_diffs=minimize_glyphs_diffs,\n        generate_GDEF=generate_GDEF,\n        store_editor_state=store_editor_state,\n    )\n\n    result = list(builder.masters)\n\n    if include_instances:\n        return result, builder.instance_data\n    return result", "response": "Take a GSFont object and convert it into one UFO per master."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake a GSFont object and convert it into a Designspace Document + UFOs.", "response": "def to_designspace(\n    font,\n    family_name=None,\n    instance_dir=None,\n    propagate_anchors=True,\n    ufo_module=defcon,\n    minimize_glyphs_diffs=False,\n    generate_GDEF=True,\n    store_editor_state=True,\n):\n    \"\"\"Take a GSFont object and convert it into a Designspace Document + UFOS.\n    The UFOs are available as the attribute `font` of each SourceDescriptor of\n    the DesignspaceDocument:\n\n        ufos = [source.font for source in designspace.sources]\n\n    The designspace and the UFOs are not written anywhere by default, they\n    are all in-memory. If you want to write them to the disk, consider using\n    the `filename` attribute of the DesignspaceDocument and of its\n    SourceDescriptor as possible file names.\n\n    Takes in data as Glyphs.app-compatible classes, as documented at\n    https://docu.glyphsapp.com/\n\n    If include_instances is True, also returns the parsed instance data.\n\n    If family_name is provided, the master UFOs will be given this name and\n    only instances with this name will be returned.\n\n    If generate_GDEF is True, write a `table GDEF {...}` statement in the\n    UFO's features.fea, containing GlyphClassDef and LigatureCaretByPos.\n    \"\"\"\n    builder = UFOBuilder(\n        font,\n        ufo_module=ufo_module,\n        family_name=family_name,\n        instance_dir=instance_dir,\n        propagate_anchors=propagate_anchors,\n        use_designspace=True,\n        minimize_glyphs_diffs=minimize_glyphs_diffs,\n        generate_GDEF=generate_GDEF,\n        store_editor_state=store_editor_state,\n    )\n    return builder.designspace"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a UFOs or DesignspaceDocument into a GSFont object.", "response": "def to_glyphs(ufos_or_designspace, glyphs_module=classes, minimize_ufo_diffs=False):\n    \"\"\"\n    Take a list of UFOs or a single DesignspaceDocument with attached UFOs\n    and converts it into a GSFont object.\n\n    The GSFont object is in-memory, it's up to the user to write it to the disk\n    if needed.\n\n    This should be the inverse function of `to_ufos` and `to_designspace`,\n    so we should have to_glyphs(to_ufos(font)) == font\n    and also to_glyphs(to_designspace(font)) == font\n    \"\"\"\n    if hasattr(ufos_or_designspace, \"sources\"):\n        builder = GlyphsBuilder(\n            designspace=ufos_or_designspace,\n            glyphs_module=glyphs_module,\n            minimize_ufo_diffs=minimize_ufo_diffs,\n        )\n    else:\n        builder = GlyphsBuilder(\n            ufos=ufos_or_designspace,\n            glyphs_module=glyphs_module,\n            minimize_ufo_diffs=minimize_ufo_diffs,\n        )\n    return builder.font"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef path_helper(self, path, view, **kwargs):\n        super(FlaskRestyPlugin, self).path_helper(\n            path=path,\n            view=view,\n            **kwargs\n        )\n\n        resource = self.get_state().views[view]\n        rule = self._rules[resource.rule]\n\n        operations = defaultdict(Operation)\n        view_instance = view()\n        view_instance.spec_declaration(view, operations, self)\n\n        # add path arguments\n        parameters = []\n        for arg in rule.arguments:\n            parameters.append({\n                'name': arg,\n                'in': 'path',\n                'required': True,\n                'type': 'string',\n            })\n        if parameters:\n            operations['parameters'] = parameters\n\n        path.path = FlaskPlugin.flaskpath2openapi(resource.rule)\n        path.operations = dict(**operations)", "response": "Path helper for Flask - RESTy views."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert UFO class to value.", "response": "def class_to_value(axis, ufo_class):\n    \"\"\"\n    >>> class_to_value('wdth', 7)\n    125\n    \"\"\"\n    if axis == \"wght\":\n        # 600.0 => 600, 250 => 250\n        return int(ufo_class)\n    elif axis == \"wdth\":\n        return WIDTH_CLASS_TO_VALUE[int(ufo_class)]\n\n    raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef user_loc_string_to_value(axis_tag, user_loc):\n    if axis_tag == \"wght\":\n        try:\n            value = _nospace_lookup(WEIGHT_CODES, user_loc)\n        except KeyError:\n            return None\n        return class_to_value(\"wght\", value)\n    elif axis_tag == \"wdth\":\n        try:\n            value = _nospace_lookup(WIDTH_CODES, user_loc)\n        except KeyError:\n            return None\n        return class_to_value(\"wdth\", value)\n\n    # Currently this function should only be called with a width or weight\n    raise NotImplementedError", "response": "Go from Glyphs UI strings to user space location."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the OS class that is closest to the provided user location.", "response": "def user_loc_value_to_class(axis_tag, user_loc):\n    \"\"\"Return the OS/2 weight or width class that is closest to the provided\n    user location. For weight the user location is between 0 and 1000 and for\n    width it is a percentage.\n\n    >>> user_loc_value_to_class('wght', 310)\n    310\n    >>> user_loc_value_to_class('wdth', 62)\n    2\n    \"\"\"\n    if axis_tag == \"wght\":\n        return int(user_loc)\n    elif axis_tag == \"wdth\":\n        return min(\n            sorted(WIDTH_CLASS_TO_VALUE.items()),\n            key=lambda item: abs(item[1] - user_loc),\n        )[0]\n\n    raise NotImplementedError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_loc_value_to_instance_string(axis_tag, user_loc):\n    codes = {}\n    if axis_tag == \"wght\":\n        codes = WEIGHT_CODES\n    elif axis_tag == \"wdth\":\n        codes = WIDTH_CODES\n    else:\n        raise NotImplementedError\n    class_ = user_loc_value_to_class(axis_tag, user_loc)\n    return min(\n        sorted((code, class_) for code, class_ in codes.items() if code is not None),\n        key=lambda item: abs(item[1] - class_),\n    )[0]", "response": "Return the Glyphs UI string that is\n    closest to the provided user location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the regular master among the GSFontMasters.", "response": "def get_regular_master(font):\n    \"\"\"Find the \"regular\" master among the GSFontMasters.\n\n    Tries to find the master with the passed 'regularName'.\n    If there is no such master or if regularName is None,\n    tries to find a base style shared between all masters\n    (defaulting to \"Regular\"), and then tries to find a master\n    with that style name. If there is no master with that name,\n    returns the first master in the list.\n    \"\"\"\n    if not font.masters:\n        return None\n    regular_name = font.customParameters[\"Variation Font Origin\"]\n    if regular_name is not None:\n        for master in font.masters:\n            if master.name == regular_name:\n                return master\n    base_style = find_base_style(font.masters)\n    if not base_style:\n        base_style = \"Regular\"\n    for master in font.masters:\n        if master.name == base_style:\n            return master\n    # Second try: maybe the base style has regular in it as well\n    for master in font.masters:\n        name_without_regular = \" \".join(\n            n for n in master.name.split(\" \") if n != \"Regular\"\n        )\n        if name_without_regular == base_style:\n            return master\n    return font.masters[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_base_style(masters):\n    if not masters:\n        return \"\"\n    base_style = (masters[0].name or \"\").split()\n    for master in masters:\n        style = master.name.split()\n        base_style = [s for s in style if s in base_style]\n    base_style = \" \".join(base_style)\n    return base_style", "response": "Find a base style shared between all masters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef interp(mapping, x):\n    mapping = sorted(mapping)\n    if len(mapping) == 1:\n        xa, ya = mapping[0]\n        if xa == x:\n            return ya\n        return x\n    for (xa, ya), (xb, yb) in zip(mapping[:-1], mapping[1:]):\n        if xa <= x <= xb:\n            return ya + float(x - xa) / (xb - xa) * (yb - ya)\n    return x", "response": "Compute the piecewise linear interpolation given by mapping for input x."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_user_loc(self, master_or_instance):\n        user_loc = self.default_user_loc\n\n        if self.tag != \"wght\":\n            # The user location is by default the same as the design location.\n            user_loc = self.get_design_loc(master_or_instance)\n\n        # Try to guess the user location by looking at the OS/2 weightClass\n        # and widthClass. If a weightClass is found, it translates directly\n        # to a user location in 0..1000. If a widthClass is found, it\n        # translate to a percentage of extension according to the spec, see\n        # the mapping named `WIDTH_CLASS_TO_VALUE` at the top.\n        if self.user_loc_key is not None and hasattr(\n            master_or_instance, self.user_loc_key\n        ):\n            # Instances have special ways to specify a user location.\n            # Only weight and with have a custom user location via a key.\n            # The `user_loc_key` gives a \"location code\" = Glyphs UI string\n            user_loc_str = getattr(master_or_instance, self.user_loc_key)\n            new_user_loc = user_loc_string_to_value(self.tag, user_loc_str)\n            if new_user_loc is not None:\n                user_loc = new_user_loc\n\n        # The custom param takes over the key if it exists\n        # e.g. for weight:\n        #       key = \"weight\" -> \"Bold\" -> 700\n        # but param = \"weightClass\" -> 600       => 600 wins\n        if self.user_loc_param is not None:\n            class_ = master_or_instance.customParameters[self.user_loc_param]\n            if class_ is not None:\n                user_loc = class_to_value(self.tag, class_)\n\n        # Masters have a customParameter that specifies a user location\n        # along custom axes. If this is present it takes precedence over\n        # everything else.\n        loc_param = master_or_instance.customParameters[\"Axis Location\"]\n        try:\n            for location in loc_param:\n                if location.get(\"Axis\") == self.name:\n                    user_loc = int(location[\"Location\"])\n        except (TypeError, KeyError):\n            pass\n\n        return user_loc", "response": "Get the user location of a Glyphs master or instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the user location of a Glyphs master or instance.", "response": "def set_user_loc(self, master_or_instance, value):\n        \"\"\"Set the user location of a Glyphs master or instance.\"\"\"\n        if hasattr(master_or_instance, \"instanceInterpolations\"):\n            # The following code is only valid for instances.\n            # Masters also the keys `weight` and `width` but they should not be\n            # used, they are deprecated and should only be used to store\n            # (parts of) the master's name, but not its location.\n\n            # Try to set the key if possible, i.e. if there is a key, and\n            # if there exists a code that can represent the given value, e.g.\n            # for \"weight\": 600 can be represented by SemiBold so we use that,\n            # but for 550 there is no code so we will have to set the custom\n            # parameter as well.\n            if self.user_loc_key is not None and hasattr(\n                master_or_instance, self.user_loc_key\n            ):\n                code = user_loc_value_to_instance_string(self.tag, value)\n                value_for_code = user_loc_string_to_value(self.tag, code)\n                setattr(master_or_instance, self.user_loc_key, code)\n                if self.user_loc_param is not None and value != value_for_code:\n                    try:\n                        class_ = user_loc_value_to_class(self.tag, value)\n                        master_or_instance.customParameters[\n                            self.user_loc_param\n                        ] = class_\n                    except NotImplementedError:\n                        # user_loc_value_to_class only works for weight & width\n                        pass\n            return\n\n        # For masters, set directly the custom parameter (old way)\n        # and also the Axis Location (new way).\n        # Only masters can have an 'Axis Location' parameter.\n        if self.user_loc_param is not None:\n            try:\n                class_ = user_loc_value_to_class(self.tag, value)\n                master_or_instance.customParameters[self.user_loc_param] = class_\n            except NotImplementedError:\n                pass\n\n        loc_param = master_or_instance.customParameters[\"Axis Location\"]\n        if loc_param is None:\n            loc_param = []\n            master_or_instance.customParameters[\"Axis Location\"] = loc_param\n        location = None\n        for loc in loc_param:\n            if loc.get(\"Axis\") == self.name:\n                location = loc\n        if location is None:\n            loc_param.append({\"Axis\": self.name, \"Location\": value})\n        else:\n            location[\"Location\"] = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_parameter(self, location='query', **kwargs):\n        kwargs.setdefault('in', location)\n        if kwargs['in'] != 'body':\n            kwargs.setdefault('type', 'string')\n        self['parameters'].append(kwargs)", "response": "Adds a new parameter to the request\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a property to the response identified by code", "response": "def add_property_to_response(self, code='200', prop_name='data', **kwargs):\n        \"\"\"Add a property (http://json-schema.org/latest/json-schema-validation.html#anchor64)  # noqa: E501\n        to the schema of the response identified by the code\"\"\"\n        self['responses'] \\\n            .setdefault(str(code), self._new_operation()) \\\n            .setdefault('schema', {'type': 'object'}) \\\n            .setdefault('properties', {}) \\\n            .setdefault(prop_name, {}) \\\n            .update(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndeclare a response for the specified code", "response": "def declare_response(self, code='200', **kwargs):\n        \"\"\"Declare a response for the specified code\n        https://github.com/swagger-api/swagger-spec/blob/master/versions/2.0.md#responseObject\"\"\"  # noqa: E501\n        self['responses'][str(code)] = self._new_operation(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _deserialize(self, value, attr, data):\n        value = super(SanitizedHTML, self)._deserialize(value, attr, data)\n        return bleach.clean(\n            value,\n            tags=self.tags,\n            attributes=self.attrs,\n            strip=True,\n        ).strip()", "response": "Deserialize string by sanitizing HTML."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_default_endpoint_prefixes(records_rest_endpoints):\n    pid_types = set()\n    guessed = set()\n    endpoint_prefixes = {}\n\n    for key, endpoint in records_rest_endpoints.items():\n        pid_type = endpoint['pid_type']\n        pid_types.add(pid_type)\n        is_guessed = key == pid_type\n        is_default = endpoint.get('default_endpoint_prefix', False)\n\n        if is_default:\n            if pid_type in endpoint_prefixes and pid_type not in guessed:\n                raise ValueError('More than one \"{0}\" defined.'.format(\n                    pid_type\n                ))\n            endpoint_prefixes[pid_type] = key\n            guessed -= {pid_type}\n        elif is_guessed and pid_type not in endpoint_prefixes:\n            endpoint_prefixes[pid_type] = key\n            guessed |= {pid_type}\n\n    not_found = pid_types - set(endpoint_prefixes.keys())\n    if not_found:\n        raise ValueError('No endpoint-prefix for {0}.'.format(\n            ', '.join(not_found)\n        ))\n\n    return endpoint_prefixes", "response": "Build the default_endpoint_prefixes map."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef obj_or_import_string(value, default=None):\n    if isinstance(value, six.string_types):\n        return import_string(value)\n    elif value:\n        return value\n    return default", "response": "Import string or return object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_or_import_from_config(key, app=None, default=None):\n    app = app or current_app\n    imp = app.config.get(key)\n    return obj_or_import_string(imp, default=default)", "response": "Load or import value from config.\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_elasticsearch(record, *args, **kwargs):\n    def can(self):\n        \"\"\"Try to search for given record.\"\"\"\n        search = request._methodview.search_class()\n        search = search.get_record(str(record.id))\n        return search.count() == 1\n\n    return type('CheckES', (), {'can': can})()", "response": "Returns a method that checks if the record exists in ES index."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves PID from a value and return a tuple with the PID and the record resolved.", "response": "def data(self):\n        \"\"\"Resolve PID from a value and return a tuple with PID and the record.\n\n        :returns: A tuple with the PID and the record resolved.\n        \"\"\"\n        try:\n            return self.resolver.resolve(self.value)\n        except PIDDoesNotExistError as pid_error:\n            raise PIDDoesNotExistRESTError(pid_error=pid_error)\n        except PIDUnregistered as pid_error:\n            raise PIDUnregisteredRESTError(pid_error=pid_error)\n        except PIDDeletedError as pid_error:\n            raise PIDDeletedRESTError(pid_error=pid_error)\n        except PIDMissingObjectError as pid_error:\n            current_app.logger.exception(\n                'No object assigned to {0}.'.format(pid_error.pid),\n                extra={'pid': pid_error.pid})\n            raise PIDMissingObjectRESTError(pid_error.pid, pid_error=pid_error)\n        except PIDRedirectedError as pid_error:\n            try:\n                location = url_for(\n                    '.{0}_item'.format(\n                        current_records_rest.default_endpoint_prefixes[\n                            pid_error.destination_pid.pid_type]),\n                    pid_value=pid_error.destination_pid.pid_value)\n                data = dict(\n                    status=301,\n                    message='Moved Permanently',\n                    location=location,\n                )\n                response = make_response(jsonify(data), data['status'])\n                response.headers['Location'] = location\n                abort(response)\n            except (BuildError, KeyError):\n                current_app.logger.exception(\n                    'Invalid redirect - pid_type \"{0}\" '\n                    'endpoint missing.'.format(\n                        pid_error.destination_pid.pid_type),\n                    extra={\n                        'pid': pid_error.pid,\n                        'destination_pid': pid_error.destination_pid,\n                    })\n                raise PIDRedirectedRESTError(\n                    pid_error.destination_pid.pid_type, pid_error=pid_error)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting JSON dump indentation and separates.", "response": "def _format_args():\n        \"\"\"Get JSON dump indentation and separates.\"\"\"\n        if request and request.args.get('prettyprint'):\n            return dict(\n                indent=2,\n                separators=(', ', ': '),\n            )\n        else:\n            return dict(\n                indent=None,\n                separators=(',', ':'),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize a single record and persistent identifier.", "response": "def serialize(self, pid, record, links_factory=None, **kwargs):\n        \"\"\"Serialize a single record and persistent identifier.\n\n        :param pid: Persistent identifier instance.\n        :param record: Record instance.\n        :param links_factory: Factory function for record links.\n        \"\"\"\n        return json.dumps(\n            self.transform_record(pid, record, links_factory, **kwargs),\n            **self._format_args())"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize_search(self, pid_fetcher, search_result, links=None,\n                         item_links_factory=None, **kwargs):\n        \"\"\"Serialize a search result.\n\n        :param pid_fetcher: Persistent identifier fetcher.\n        :param search_result: Elasticsearch search result.\n        :param links: Dictionary of links to add to response.\n        \"\"\"\n        return json.dumps(dict(\n            hits=dict(\n                hits=[self.transform_search_hit(\n                    pid_fetcher(hit['_id'], hit['_source']),\n                    hit,\n                    links_factory=item_links_factory,\n                    **kwargs\n                ) for hit in search_result['hits']['hits']],\n                total=search_result['hits']['total'],\n            ),\n            links=links or {},\n            aggregations=search_result.get('aggregations', dict()),\n        ), **self._format_args())", "response": "Serialize a search result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_error_handlers(blueprint, error_handlers_registry=None):\n    error_handlers_registry = error_handlers_registry or {}\n\n    # Catch record validation errors\n    @blueprint.errorhandler(ValidationError)\n    def validation_error(error):\n        \"\"\"Catch validation errors.\"\"\"\n        return JSONSchemaValidationError(error=error).get_response()\n\n    @blueprint.errorhandler(RequestError)\n    def elasticsearch_badrequest_error(error):\n        \"\"\"Catch errors of ElasticSearch.\"\"\"\n        handlers = current_app.config[\n            'RECORDS_REST_ELASTICSEARCH_ERROR_HANDLERS']\n        cause_types = {c['type'] for c in error.info['error']['root_cause']}\n\n        for cause_type, handler in handlers.items():\n            if cause_type in cause_types:\n                return handler(error)\n\n        # Default exception for unhandled errors\n        exception = UnhandledElasticsearchError()\n        current_app.logger.exception(error)  # Log the original stacktrace\n        return exception.get_response()\n\n    for exc_or_code, handlers in error_handlers_registry.items():\n        # Build full endpoint names and resolve handlers\n        handlers = {\n            '.'.join([blueprint.name, view_name]): obj_or_import_string(func)\n            for view_name, func in handlers.items()\n        }\n\n        def dispatch_handler(error):\n            def default_handler(e):\n                raise e\n            return handlers.get(request.endpoint, default_handler)(error)\n        blueprint.register_error_handler(exc_or_code, dispatch_handler)\n\n    return blueprint", "response": "Create error handlers on Records API blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_blueprint(endpoints):\n    endpoints = endpoints or {}\n\n    blueprint = Blueprint(\n        'invenio_records_rest',\n        __name__,\n        url_prefix='',\n    )\n\n    error_handlers_registry = defaultdict(dict)\n    for endpoint, options in endpoints.items():\n        error_handlers = options.pop('error_handlers', {})\n        for rule in create_url_rules(endpoint, **options):\n            for exc_or_code, handler in error_handlers.items():\n                view_name = rule['view_func'].__name__\n                error_handlers_registry[exc_or_code][view_name] = handler\n            blueprint.add_url_rule(**rule)\n\n    return create_error_handlers(blueprint, error_handlers_registry)", "response": "Create Invenio - Records - REST blueprint."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_url_rules(endpoint, list_route=None, item_route=None,\n                     pid_type=None, pid_minter=None, pid_fetcher=None,\n                     read_permission_factory_imp=None,\n                     create_permission_factory_imp=None,\n                     update_permission_factory_imp=None,\n                     delete_permission_factory_imp=None,\n                     list_permission_factory_imp=None,\n                     record_class=None,\n                     record_serializers=None,\n                     record_serializers_aliases=None,\n                     record_loaders=None,\n                     search_class=None,\n                     indexer_class=RecordIndexer,\n                     search_serializers=None,\n                     search_index=None, search_type=None,\n                     default_media_type=None,\n                     max_result_window=None, use_options_view=True,\n                     search_factory_imp=None, links_factory_imp=None,\n                     suggesters=None, default_endpoint_prefix=None):\n    \"\"\"Create Werkzeug URL rules.\n\n    :param endpoint: Name of endpoint.\n    :param list_route: Record listing URL route. Required.\n    :param item_route: Record URL route (must include ``<pid_value>`` pattern).\n        Required.\n    :param pid_type: Persistent identifier type for endpoint. Required.\n    :param pid_minter: It identifies the registered minter name.\n    :param pid_fetcher: It identifies the registered fetcher name.\n    :param read_permission_factory_imp: Import path to factory that creates a\n        read permission object for a given record.\n    :param create_permission_factory_imp: Import path to factory that creates a\n        create permission object for a given record.\n    :param update_permission_factory_imp: Import path to factory that creates a\n        update permission object for a given record.\n    :param delete_permission_factory_imp: Import path to factory that creates a\n        delete permission object for a given record.\n    :param list_permission_factory_imp: Import path to factory that\n        creates a list permission object for a given index/list.\n    :param default_endpoint_prefix: ignored.\n    :param record_class: A record API class or importable string used when\n        creating new records.\n    :param record_serializers: Serializers used for records.\n    :param record_serializers_aliases: A mapping of query arg `format` values\n        to valid mimetypes: dict(alias -> mimetype).\n    :param record_loaders: It contains the list of record deserializers for\n        supperted formats.\n    :param search_class: Import path or class object for the object in charge\n        of execute the search queries. The default search class is\n        :class:`invenio_search.api.RecordsSearch`.\n        For more information about resource loading, see the Search of\n        ElasticSearch DSL library.\n    :param indexer_class: Import path or class object for the object in charge\n        of indexing records. The default indexer is\n        :class:`invenio_indexer.api.RecordIndexer`.\n    :param search_serializers: Serializers used for search results.\n    :param search_index: Name of the search index used when searching records.\n    :param search_type: Name of the search type used when searching records.\n    :param default_media_type: Default media type for both records and search.\n    :param max_result_window: Maximum number of results that Elasticsearch can\n        provide for the given search index without use of scroll. This value\n        should correspond to Elasticsearch ``index.max_result_window`` value\n        for the index.\n    :param use_options_view: Determines if a special option view should be\n        installed.\n    :param search_factory_imp: Factory to parse quieries.\n    :param links_factory_imp: Factory for record links generation.\n    :param suggesters: Suggester fields configuration.\n\n    :returns: a list of dictionaries with can each be passed as keywords\n        arguments to ``Blueprint.add_url_rule``.\n    \"\"\"\n    assert list_route\n    assert item_route\n    assert pid_type\n    assert search_serializers\n    assert record_serializers\n\n    read_permission_factory = obj_or_import_string(\n        read_permission_factory_imp\n    )\n    create_permission_factory = obj_or_import_string(\n        create_permission_factory_imp\n    )\n    update_permission_factory = obj_or_import_string(\n        update_permission_factory_imp\n    )\n    delete_permission_factory = obj_or_import_string(\n        delete_permission_factory_imp\n    )\n    list_permission_factory = obj_or_import_string(\n        list_permission_factory_imp\n    )\n    links_factory = obj_or_import_string(\n        links_factory_imp, default=default_links_factory\n    )\n    # For backward compatibility. Previous signature was links_factory(pid).\n    if wrap_links_factory(links_factory):\n        orig_links_factory = links_factory\n\n        def links_factory(pid, record=None, **kwargs):\n            return orig_links_factory(pid)\n\n    record_class = obj_or_import_string(\n        record_class, default=Record\n    )\n    search_class = obj_or_import_string(\n        search_class, default=RecordsSearch\n    )\n\n    indexer_class = obj_or_import_string(\n        indexer_class, default=None\n    )\n\n    search_class_kwargs = {}\n    if search_index:\n        search_class_kwargs['index'] = search_index\n    else:\n        search_index = search_class.Meta.index\n\n    if search_type:\n        search_class_kwargs['doc_type'] = search_type\n    else:\n        search_type = search_class.Meta.doc_types\n\n    if search_class_kwargs:\n        search_class = partial(search_class, **search_class_kwargs)\n\n    if record_loaders:\n        record_loaders = {mime: obj_or_import_string(func)\n                          for mime, func in record_loaders.items()}\n    record_serializers = {mime: obj_or_import_string(func)\n                          for mime, func in record_serializers.items()}\n    search_serializers = {mime: obj_or_import_string(func)\n                          for mime, func in search_serializers.items()}\n\n    list_view = RecordsListResource.as_view(\n        RecordsListResource.view_name.format(endpoint),\n        minter_name=pid_minter,\n        pid_type=pid_type,\n        pid_fetcher=pid_fetcher,\n        read_permission_factory=read_permission_factory,\n        create_permission_factory=create_permission_factory,\n        list_permission_factory=list_permission_factory,\n        record_serializers=record_serializers,\n        record_loaders=record_loaders,\n        search_serializers=search_serializers,\n        search_class=search_class,\n        indexer_class=indexer_class,\n        default_media_type=default_media_type,\n        max_result_window=max_result_window,\n        search_factory=(obj_or_import_string(\n            search_factory_imp, default=es_search_factory\n        )),\n        item_links_factory=links_factory,\n        record_class=record_class,\n    )\n    item_view = RecordResource.as_view(\n        RecordResource.view_name.format(endpoint),\n        read_permission_factory=read_permission_factory,\n        update_permission_factory=update_permission_factory,\n        delete_permission_factory=delete_permission_factory,\n        serializers=record_serializers,\n        serializers_query_aliases=record_serializers_aliases,\n        loaders=record_loaders,\n        search_class=search_class,\n        indexer_class=indexer_class,\n        links_factory=links_factory,\n        default_media_type=default_media_type)\n\n    views = [\n        dict(rule=list_route, view_func=list_view),\n        dict(rule=item_route, view_func=item_view),\n    ]\n    if suggesters:\n        suggest_view = SuggestResource.as_view(\n            SuggestResource.view_name.format(endpoint),\n            suggesters=suggesters,\n            search_class=search_class,\n        )\n\n        views.append(dict(\n            rule=list_route + '_suggest',\n            view_func=suggest_view\n        ))\n\n    if use_options_view:\n        options_view = RecordsListOptionsResource.as_view(\n            RecordsListOptionsResource.view_name.format(endpoint),\n            search_index=search_index,\n            max_result_window=max_result_window,\n            default_media_type=default_media_type,\n            search_media_types=search_serializers.keys(),\n            item_media_types=record_serializers.keys(),\n        )\n        return [\n                   dict(rule=\"{0}_options\".format(list_route),\n                        view_func=options_view)\n               ] + views\n    return views", "response": "Creates Werkzeug URL rules."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef verify_record_permission(permission_factory, record):\n    # Note, cannot be done in one line due overloading of boolean\n    # operations permission object.\n    if not permission_factory(record=record).can():\n        from flask_login import current_user\n        if not current_user.is_authenticated:\n            abort(401)\n        abort(403)", "response": "Verify that the current user has the required permissions on record."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef need_record_permission(factory_name):\n    def need_record_permission_builder(f):\n        @wraps(f)\n        def need_record_permission_decorator(self, record=None, *args,\n                                             **kwargs):\n            permission_factory = (\n                getattr(self, factory_name) or\n                getattr(current_records_rest, factory_name)\n            )\n\n            # FIXME use context instead\n            request._methodview = self\n\n            if permission_factory:\n                verify_record_permission(permission_factory, record)\n            return f(self, record=record, *args, **kwargs)\n        return need_record_permission_decorator\n    return need_record_permission_builder", "response": "Decorator checking that the user has the required permissions on record."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Search result containing the records and aggregations as .", "response": "def get(self, **kwargs):\n        \"\"\"Search records.\n\n        Permissions: the `list_permission_factory` permissions are\n            checked.\n\n        :returns: Search result containing hits and aggregations as\n                  returned by invenio-search.\n        \"\"\"\n        default_results_size = current_app.config.get(\n            'RECORDS_REST_DEFAULT_RESULTS_SIZE', 10)\n        page = request.values.get('page', 1, type=int)\n        size = request.values.get('size', default_results_size, type=int)\n        if page * size >= self.max_result_window:\n            raise MaxResultWindowRESTError()\n\n        # Arguments that must be added in prev/next links\n        urlkwargs = dict()\n        search_obj = self.search_class()\n        search = search_obj.with_preference_param().params(version=True)\n        search = search[(page - 1) * size:page * size]\n\n        search, qs_kwargs = self.search_factory(search)\n        urlkwargs.update(qs_kwargs)\n\n        # Execute search\n        search_result = search.execute()\n\n        # Generate links for prev/next\n        urlkwargs.update(\n            size=size,\n            _external=True,\n        )\n        endpoint = '.{0}_list'.format(\n            current_records_rest.default_endpoint_prefixes[self.pid_type])\n        links = dict(self=url_for(endpoint, page=page, **urlkwargs))\n        if page > 1:\n            links['prev'] = url_for(endpoint, page=page - 1, **urlkwargs)\n        if size * page < search_result.hits.total and \\\n                size * page < self.max_result_window:\n            links['next'] = url_for(endpoint, page=page + 1, **urlkwargs)\n\n        return self.make_response(\n            pid_fetcher=self.pid_fetcher,\n            search_result=search_result.to_dict(),\n            links=links,\n            item_links_factory=self.item_links_factory,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a record. Permissions: ``create_permission_factory`` Procedure description: #. First of all, the `create_permission_factory` permissions are checked. #. Then, the record is deserialized by the proper loader. #. A second call to the `create_permission_factory` factory is done: it differs from the previous call because this time the record is passed as parameter. #. A `uuid` is generated for the record and the minter is called. #. The record class is called to create the record. #. The HTTP response is built with the help of the item link factory. :returns: The created record.", "response": "def post(self, **kwargs):\n        \"\"\"Create a record.\n\n        Permissions: ``create_permission_factory``\n\n        Procedure description:\n\n        #. First of all, the `create_permission_factory` permissions are\n            checked.\n\n        #. Then, the record is deserialized by the proper loader.\n\n        #. A second call to the `create_permission_factory` factory is done:\n            it differs from the previous call because this time the record is\n            passed as parameter.\n\n        #. A `uuid` is generated for the record and the minter is called.\n\n        #. The record class is called to create the record.\n\n        #. The HTTP response is built with the help of the item link factory.\n\n        :returns: The created record.\n        \"\"\"\n        if request.mimetype not in self.loaders:\n            raise UnsupportedMediaRESTError(request.mimetype)\n\n        data = self.loaders[request.mimetype]()\n        if data is None:\n            raise InvalidDataRESTError()\n\n        # Check permissions\n        permission_factory = self.create_permission_factory\n        if permission_factory:\n            verify_record_permission(permission_factory, data)\n\n        # Create uuid for record\n        record_uuid = uuid.uuid4()\n        # Create persistent identifier\n        pid = self.minter(record_uuid, data=data)\n        # Create record\n        record = self.record_class.create(data, id_=record_uuid)\n\n        db.session.commit()\n\n        # Index the record\n        if self.indexer_class:\n            self.indexer_class().index(record)\n\n        response = self.make_response(\n            pid, record, 201, links_factory=self.item_links_factory)\n\n        # Add location headers\n        endpoint = '.{0}_item'.format(\n            current_records_rest.default_endpoint_prefixes[pid.pid_type])\n        location = url_for(endpoint, pid_value=pid.pid_value, _external=True)\n        response.headers.extend(dict(location=location))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, pid, record, **kwargs):\n        self.check_etag(str(record.model.version_id))\n\n        record.delete()\n        # mark all PIDs as DELETED\n        all_pids = PersistentIdentifier.query.filter(\n            PersistentIdentifier.object_type == pid.object_type,\n            PersistentIdentifier.object_uuid == pid.object_uuid,\n        ).all()\n        for rec_pid in all_pids:\n            if not rec_pid.is_deleted():\n                rec_pid.delete()\n        db.session.commit()\n        if self.indexer_class:\n            self.indexer_class().delete(record)\n\n        return '', 204", "response": "Delete a record.\n\n        Permissions: ``delete_permission_factory``\n\n        Procedure description:\n\n        #. The record is resolved reading the pid value from the url.\n\n        #. The ETag is checked.\n\n        #. The record is deleted.\n\n        #. All PIDs are marked as DELETED.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a record. Permissions: ``read_permission_factory`` Procedure description: #. The record is resolved reading the pid value from the url. #. The ETag and If-Modifed-Since is checked. #. The HTTP response is built with the help of the link factory. :param pid: Persistent identifier for record. :param record: Record object. :returns: The requested record.", "response": "def get(self, pid, record, **kwargs):\n        \"\"\"Get a record.\n\n        Permissions: ``read_permission_factory``\n\n        Procedure description:\n\n        #. The record is resolved reading the pid value from the url.\n\n        #. The ETag and If-Modifed-Since is checked.\n\n        #. The HTTP response is built with the help of the link factory.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object.\n        :returns: The requested record.\n        \"\"\"\n        etag = str(record.revision_id)\n        self.check_etag(str(record.revision_id))\n        self.check_if_modified_since(record.updated, etag=etag)\n\n        return self.make_response(\n            pid, record, links_factory=self.links_factory\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmodify a record. Permissions: ``update_permission_factory`` The data should be a JSON-patch, which will be applied to the record. Requires header ``Content-Type: application/json-patch+json``. Procedure description: #. The record is deserialized using the proper loader. #. The ETag is checked. #. The record is patched. #. The HTTP response is built with the help of the link factory. :param pid: Persistent identifier for record. :param record: Record object. :returns: The modified record.", "response": "def patch(self, pid, record, **kwargs):\n        \"\"\"Modify a record.\n\n        Permissions: ``update_permission_factory``\n\n        The data should be a JSON-patch, which will be applied to the record.\n        Requires header ``Content-Type: application/json-patch+json``.\n\n        Procedure description:\n\n        #. The record is deserialized using the proper loader.\n\n        #. The ETag is checked.\n\n        #. The record is patched.\n\n        #. The HTTP response is built with the help of the link factory.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object.\n        :returns: The modified record.\n        \"\"\"\n        data = self.loaders[request.mimetype]()\n        if data is None:\n            raise InvalidDataRESTError()\n\n        self.check_etag(str(record.revision_id))\n        try:\n            record = record.patch(data)\n        except (JsonPatchException, JsonPointerException):\n            raise PatchJSONFailureRESTError()\n\n        record.commit()\n        db.session.commit()\n        if self.indexer_class:\n            self.indexer_class().index(record)\n\n        return self.make_response(\n            pid, record, links_factory=self.links_factory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, pid, record, **kwargs):\n        if request.mimetype not in self.loaders:\n            raise UnsupportedMediaRESTError(request.mimetype)\n\n        data = self.loaders[request.mimetype]()\n        if data is None:\n            raise InvalidDataRESTError()\n\n        self.check_etag(str(record.revision_id))\n\n        record.clear()\n        record.update(data)\n        record.commit()\n        db.session.commit()\n        if self.indexer_class:\n            self.indexer_class().index(record)\n        return self.make_response(\n            pid, record, links_factory=self.links_factory)", "response": "Replace a record.\n\n        Permissions: ``update_permission_factory``\n\n        The body should be a JSON object, which will fully replace the current\n        record metadata.\n\n        Procedure description:\n\n        #. The ETag is checked.\n\n        #. The record is updated by calling the record API `clear()`,\n           `update()` and then `commit()`.\n\n        #. The HTTP response is built with the help of the link factory.\n\n        :param pid: Persistent identifier for record.\n        :param record: Record object.\n        :returns: The modified record."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _serialize(self, value, attr, obj):\n        try:\n            return super(DateString, self)._serialize(\n                arrow.get(value).date(), attr, obj)\n        except ParserError:\n            return missing", "response": "Serialize an ISO8601 - formatted date."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading test data fixture.", "response": "def records():\n    \"\"\"Load test data fixture.\"\"\"\n    import uuid\n    from invenio_records.api import Record\n    from invenio_pidstore.models import PersistentIdentifier, PIDStatus\n\n    indexer = RecordIndexer()\n    index_queue = []\n\n    # Record 1 - Live record\n    with db.session.begin_nested():\n        rec_uuid = uuid.uuid4()\n        pid1 = PersistentIdentifier.create(\n            'recid', '1', object_type='rec', object_uuid=rec_uuid,\n            status=PIDStatus.REGISTERED)\n        Record.create({\n            'title': 'Registered',\n            'description': 'This is an awesome description',\n            # \"mint\" the record as recid minter does\n            'control_number': '1',\n        }, id_=rec_uuid)\n        index_queue.append(pid1.object_uuid)\n\n        # Record 2 - Deleted PID with record\n        rec_uuid = uuid.uuid4()\n        pid = PersistentIdentifier.create(\n            'recid', '2', object_type='rec', object_uuid=rec_uuid,\n            status=PIDStatus.REGISTERED)\n        Record.create({\n            'title': 'Live ',\n            'control_number': '2',\n        }, id_=rec_uuid)\n        pid.delete()\n\n        # Record 3 - Deleted PID without a record\n        PersistentIdentifier.create(\n            'recid', '3', status=PIDStatus.DELETED)\n\n        # Record 4 - Registered PID without a record\n        PersistentIdentifier.create(\n            'recid', '4', status=PIDStatus.REGISTERED)\n\n        # Record 5 - Redirected PID\n        pid = PersistentIdentifier.create(\n            'recid', '5', status=PIDStatus.REGISTERED)\n        pid.redirect(pid1)\n\n        # Record 6 - Redirected non existing endpoint\n        doi = PersistentIdentifier.create(\n            'doi', '10.1234/foo', status=PIDStatus.REGISTERED)\n        pid = PersistentIdentifier.create(\n            'recid', '6', status=PIDStatus.REGISTERED)\n        pid.redirect(doi)\n\n        # Record 7 - Unregistered PID\n        PersistentIdentifier.create(\n            'recid', '7', status=PIDStatus.RESERVED)\n\n        for rec_idx in range(len(record_examples)):\n            rec_uuid = uuid.uuid4()\n            rec_pid = 8 + rec_idx\n            pid1 = PersistentIdentifier.create(\n                'recid', str(rec_pid), object_type='rec', object_uuid=rec_uuid,\n                status=PIDStatus.REGISTERED)\n            # \"mint\" the record as recid minter does\n            record = dict(record_examples[rec_idx])\n            record['control_number'] = str(rec_pid)\n            # create the record\n            Record.create(record, id_=rec_uuid)\n            index_queue.append(rec_uuid)\n    db.session.commit()\n\n    for i in index_queue:\n        indexer.index_by_id(i)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preprocess_record(self, pid, record, links_factory=None, **kwargs):\n        links_factory = links_factory or (lambda x, record=None, **k: dict())\n        metadata = copy.deepcopy(record.replace_refs()) if self.replace_refs \\\n            else record.dumps()\n        return dict(\n            pid=pid,\n            metadata=metadata,\n            links=links_factory(pid, record=record, **kwargs),\n            revision=record.revision_id,\n            created=(pytz.utc.localize(record.created).isoformat()\n                     if record.created else None),\n            updated=(pytz.utc.localize(record.updated).isoformat()\n                     if record.updated else None),\n        )", "response": "Prepare a record and persistent identifier for serialization."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing a record hit from Elasticsearch for serialization.", "response": "def preprocess_search_hit(pid, record_hit, links_factory=None, **kwargs):\n        \"\"\"Prepare a record hit from Elasticsearch for serialization.\"\"\"\n        links_factory = links_factory or (lambda x, **k: dict())\n        record = dict(\n            pid=pid,\n            metadata=record_hit['_source'],\n            links=links_factory(pid, record_hit=record_hit, **kwargs),\n            revision=record_hit['_version'],\n            created=None,\n            updated=None,\n        )\n        # Move created/updated attrs from source to object.\n        for key in ['_created', '_updated']:\n            if key in record['metadata']:\n                record[key[1:]] = record['metadata'][key]\n                del record['metadata'][key]\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the request body.", "response": "def get_body(self, environ=None):\n        \"\"\"Get the request body.\"\"\"\n        body = dict(\n            status=self.code,\n            message=self.get_description(environ),\n        )\n\n        if self.errors:\n            body['errors'] = self.errors\n\n        return json.dumps(body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_permission_factories(self):\n        for key in ('read', 'create', 'update', 'delete'):\n            full_key = '{0}_permission_factory'.format(key)\n            if full_key in self.__dict__:\n                del self.__dict__[full_key]", "response": "Remove cached permission factories."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the Flask application.", "response": "def init_app(self, app):\n        \"\"\"Flask application initialization.\"\"\"\n        self.init_config(app)\n        app.extensions['invenio-records-rest'] = _RecordRESTState(app)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a range filter.", "response": "def range_filter(field, start_date_math=None, end_date_math=None, **kwargs):\n    \"\"\"Create a range filter.\n\n    :param field: Field name.\n    :param start_date_math: Starting date.\n    :param end_date_math: Ending date.\n    :param kwargs: Addition arguments passed to the Range query.\n    :returns: Function that returns the Range query.\n    \"\"\"\n    def inner(values):\n        if len(values) != 1 or values[0].count('--') != 1 or values[0] == '--':\n            raise RESTValidationError(\n                errors=[FieldError(field, 'Invalid range format.')])\n\n        range_ends = values[0].split('--')\n        range_args = dict()\n\n        ineq_opers = [{'strict': 'gt', 'nonstrict': 'gte'},\n                      {'strict': 'lt', 'nonstrict': 'lte'}]\n        date_maths = [start_date_math, end_date_math]\n\n        # Add the proper values to the dict\n        for (range_end, strict, opers,\n             date_math) in zip(range_ends, ['>', '<'], ineq_opers, date_maths):\n\n            if range_end != '':\n                # If first char is '>' for start or '<' for end\n                if range_end[0] == strict:\n                    dict_key = opers['strict']\n                    range_end = range_end[1:]\n                else:\n                    dict_key = opers['nonstrict']\n\n                if date_math:\n                    range_end = '{0}||{1}'.format(range_end, date_math)\n\n                range_args[dict_key] = range_end\n\n        args = kwargs.copy()\n        args.update(range_args)\n\n        return Range(**{field: args})\n\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_filter_dsl(urlkwargs, definitions):\n    filters = []\n    for name, filter_factory in definitions.items():\n        values = request.values.getlist(name, type=text_type)\n        if values:\n            filters.append(filter_factory(values))\n            for v in values:\n                urlkwargs.add(name, v)\n\n    return (filters, urlkwargs)", "response": "Create a filter DSL expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _post_filter(search, urlkwargs, definitions):\n    filters, urlkwargs = _create_filter_dsl(urlkwargs, definitions)\n\n    for filter_ in filters:\n        search = search.post_filter(filter_)\n\n    return (search, urlkwargs)", "response": "Ingest post filter in query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _query_filter(search, urlkwargs, definitions):\n    filters, urlkwargs = _create_filter_dsl(urlkwargs, definitions)\n\n    for filter_ in filters:\n        search = search.filter(filter_)\n\n    return (search, urlkwargs)", "response": "Ingest query filter in query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _aggregations(search, definitions):\n    if definitions:\n        for name, agg in definitions.items():\n            search.aggs[name] = agg if not callable(agg) else agg()\n    return search", "response": "Add aggregations to query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_facets_factory(search, index):\n    urlkwargs = MultiDict()\n\n    facets = current_app.config['RECORDS_REST_FACETS'].get(index)\n\n    if facets is not None:\n        # Aggregations.\n        search = _aggregations(search, facets.get(\"aggs\", {}))\n\n        # Query filter\n        search, urlkwargs = _query_filter(\n            search, urlkwargs, facets.get(\"filters\", {}))\n\n        # Post filter\n        search, urlkwargs = _post_filter(\n            search, urlkwargs, facets.get(\"post_filters\", {}))\n\n    return (search, urlkwargs)", "response": "Add a default facets to query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dump(self, obj, context=None):\n        return self.schema_class(context=context).dump(obj).data", "response": "Serialize object with schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms record into an intermediate representation.", "response": "def transform_record(self, pid, record, links_factory=None, **kwargs):\n        \"\"\"Transform record into an intermediate representation.\"\"\"\n        context = kwargs.get('marshmallow_context', {})\n        context.setdefault('pid', pid)\n        return self.dump(self.preprocess_record(pid, record,\n                         links_factory=links_factory, **kwargs), context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_search_hit(self, pid, record_hit, links_factory=None,\n                             **kwargs):\n        \"\"\"Transform search result hit into an intermediate representation.\"\"\"\n        context = kwargs.get('marshmallow_context', {})\n        context.setdefault('pid', pid)\n        return self.dump(self.preprocess_search_hit(pid, record_hit,\n                         links_factory=links_factory, **kwargs), context)", "response": "Transform search result hit into an intermediate representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pid_from_context(_, context):\n    pid = (context or {}).get('pid')\n    return pid.pid_value if pid else missing", "response": "Get PID from marshmallow context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef record_responsify(serializer, mimetype):\n    def view(pid, record, code=200, headers=None, links_factory=None):\n        response = current_app.response_class(\n            serializer.serialize(pid, record, links_factory=links_factory),\n            mimetype=mimetype)\n        response.status_code = code\n        response.set_etag(str(record.revision_id))\n        response.last_modified = record.updated\n        if headers is not None:\n            response.headers.extend(headers)\n\n        if links_factory is not None:\n            add_link_header(response, links_factory(pid))\n\n        return response\n\n    return view", "response": "Create a Records - REST response serializer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search_responsify(serializer, mimetype):\n    def view(pid_fetcher, search_result, code=200, headers=None, links=None,\n             item_links_factory=None):\n        response = current_app.response_class(\n            serializer.serialize_search(pid_fetcher, search_result,\n                                        links=links,\n                                        item_links_factory=item_links_factory),\n            mimetype=mimetype)\n        response.status_code = code\n        if headers is not None:\n            response.headers.extend(headers)\n\n        if links is not None:\n            add_link_header(response, links)\n\n        return response\n\n    return view", "response": "Create a Records - REST search result response serializer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_link_header(response, links):\n    if links is not None:\n        response.headers.extend({\n            'Link': ', '.join([\n                '<{0}>; rel=\"{1}\"'.format(l, r) for r, l in links.items()])\n        })", "response": "Add a Link HTTP header to a REST response instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses style and locale.", "response": "def _get_args(cls, **kwargs):\n        \"\"\"Parse style and locale.\n\n        Argument location precedence: kwargs > view_args > query\n        \"\"\"\n        csl_args = {\n            'style': cls._default_style,\n            'locale': cls._default_locale\n        }\n\n        if has_request_context():\n            parser = FlaskParser(locations=('view_args', 'query'))\n            csl_args.update(parser.parse(cls._user_args, request))\n\n        csl_args.update({k: kwargs[k]\n                         for k in ('style', 'locale') if k in kwargs})\n\n        try:\n            csl_args['style'] = get_style_filepath(csl_args['style'].lower())\n        except StyleNotFoundError:\n            if has_request_context():\n                raise StyleNotFoundRESTError(csl_args['style'])\n            raise\n        return csl_args"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets source data object for citeproc - py.", "response": "def _get_source(self, data):\n        \"\"\"Get source data object for citeproc-py.\"\"\"\n        if self.record_format == 'csl':\n            return CiteProcJSON([json.loads(data)])\n        elif self.record_format == 'bibtex':\n            return BibTeX(data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _clean_result(self, text):\n        text = re.sub('\\s\\s+', ' ', text)\n        text = re.sub('\\.\\.+', '.', text)\n        text = text.replace(\"'\", \"\\\\'\")\n        return text", "response": "Remove double spaces punctuation and escapes apostrophes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a single record.", "response": "def serialize(self, pid, record, links_factory=None, **kwargs):\n        \"\"\"Serialize a single record.\n\n        :param pid: Persistent identifier instance.\n        :param record: Record instance.\n        :param links_factory: Factory function for record links.\n        \"\"\"\n        data = self.serializer.serialize(pid, record, links_factory)\n        source = self._get_source(data)\n        style = CitationStylesStyle(validate=False, **self._get_args(**kwargs))\n        bib = CitationStylesBibliography(style, source, formatter.plain)\n        citation = Citation([CitationItem(pid.pid_value)])\n        bib.register(citation)\n\n        return self._clean_result(''.join(bib.bibliography()[0]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a character is valid based on the XML specification.", "response": "def is_valid_xml_char(self, char):\n        \"\"\"Check if a character is valid based on the XML specification.\"\"\"\n        codepoint = ord(char)\n        return (0x20 <= codepoint <= 0xD7FF or\n                codepoint in (0x9, 0xA, 0xD) or\n                0xE000 <= codepoint <= 0xFFFD or\n                0x10000 <= codepoint <= 0x10FFFF)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a single record and persistent identifier.", "response": "def serialize(self, pid, record, links_factory=None):\n        \"\"\"Serialize a single record and persistent identifier.\n\n        :param pid: Persistent identifier instance.\n        :param record: Record instance.\n        :param links_factory: Factory function for record links.\n        \"\"\"\n        return self.schema.tostring(\n            self.transform_record(pid, record, links_factory))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing a search result.", "response": "def serialize_search(self, pid_fetcher, search_result, links=None,\n                         item_links_factory=None):\n        \"\"\"Serialize a search result.\n\n        :param pid_fetcher: Persistent identifier fetcher.\n        :param search_result: Elasticsearch search result.\n        :param links: Dictionary of links to add to response.\n        \"\"\"\n        records = []\n        for hit in search_result['hits']['hits']:\n            records.append(self.schema.tostring(self.transform_search_hit(\n                pid_fetcher(hit['_id'], hit['_source']),\n                hit,\n                links_factory=item_links_factory,\n            )))\n\n        return \"\\n\".join(records)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing a single record for OAI - PMH.", "response": "def serialize_oaipmh(self, pid, record):\n        \"\"\"Serialize a single record for OAI-PMH.\"\"\"\n        obj = self.transform_record(pid, record['_source']) \\\n            if isinstance(record['_source'], Record) \\\n            else self.transform_search_hit(pid, record)\n\n        return self.schema.dump_etree(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef serialize_oaipmh(self, pid, record):\n        root = etree.Element(\n            'oai_datacite',\n            nsmap={\n                None: 'http://schema.datacite.org/oai/oai-1.0/',\n                'xsi': 'http://www.w3.org/2001/XMLSchema-instance',\n                'xml': 'xml',\n            },\n            attrib={\n                '{http://www.w3.org/2001/XMLSchema-instance}schemaLocation':\n                'http://schema.datacite.org/oai/oai-1.0/ oai_datacite.xsd',\n            }\n        )\n\n        root.append(E.isReferenceQuality(self.is_reference_quality))\n        root.append(E.schemaVersion(self.serializer.version))\n        root.append(E.datacentreSymbol(self.datacentre))\n        root.append(E.payload(\n            self.serializer.serialize_oaipmh(pid, record)\n        ))\n\n        return root", "response": "Serialize a single record for OAI - PMH."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef default_search_factory(self, search, query_parser=None):\n    def _default_parser(qstr=None):\n        \"\"\"Default parser that uses the Q() from elasticsearch_dsl.\"\"\"\n        if qstr:\n            return Q('query_string', query=qstr)\n        return Q()\n\n    from .facets import default_facets_factory\n    from .sorter import default_sorter_factory\n\n    query_string = request.values.get('q')\n    query_parser = query_parser or _default_parser\n\n    try:\n        search = search.query(query_parser(query_string))\n    except SyntaxError:\n        current_app.logger.debug(\n            \"Failed parsing query: {0}\".format(\n                request.values.get('q', '')),\n            exc_info=True)\n        raise InvalidQueryRESTError()\n\n    search_index = search._index[0]\n    search, urlkwargs = default_facets_factory(search, search_index)\n    search, sortkwargs = default_sorter_factory(search, search_index)\n    for key, value in sortkwargs.items():\n        urlkwargs.add(key, value)\n\n    urlkwargs.add('q', query_string)\n    return search, urlkwargs", "response": "Parse query using elasticsearch DSL query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a list of the arguments a function or method has.", "response": "def _get_func_args(func):\n    \"\"\"Get a list of the arguments a function or method has.\"\"\"\n    if isinstance(func, functools.partial):\n        return _get_func_args(func.func)\n    if inspect.isfunction(func) or inspect.ismethod(func):\n        return list(inspect.getargspec(func).args)\n    if callable(func):\n        return list(inspect.getargspec(func.__call__).args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self, pid, record, links_factory=None):\n        return simpledc.tostring(\n            self.transform_record(pid, record, links_factory))", "response": "Serialize a single record and persistent identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting JSON - LD expanded state.", "response": "def expanded(self):\n        \"\"\"Get JSON-LD expanded state.\"\"\"\n        # Ensure we can run outside a application/request context.\n        if request:\n            if 'expanded' in request.args:\n                return True\n            elif 'compacted' in request.args:\n                return False\n        return self._expanded"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncompacts JSON according to context.", "response": "def transform_jsonld(self, obj):\n        \"\"\"Compact JSON according to context.\"\"\"\n        rec = copy.deepcopy(obj)\n        rec.update(self.context)\n        compacted = jsonld.compact(rec, self.context)\n        if not self.expanded:\n            return compacted\n        else:\n            return jsonld.expand(compacted)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransforming record into an intermediate representation.", "response": "def transform_record(self, pid, record, links_factory=None, **kwargs):\n        \"\"\"Transform record into an intermediate representation.\"\"\"\n        result = super(JSONLDTransformerMixin, self).transform_record(\n            pid, record, links_factory, **kwargs\n        )\n        return self.transform_jsonld(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform search result hit into an intermediate representation.", "response": "def transform_search_hit(self, pid, record_hit, links_factory=None,\n                             **kwargs):\n        \"\"\"Transform search result hit into an intermediate representation.\"\"\"\n        result = super(JSONLDTransformerMixin, self).transform_search_hit(\n            pid, record_hit, links_factory, **kwargs\n        )\n        return self.transform_jsonld(result)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_links_factory_with_additional(additional_links):\n    def factory(pid, **kwargs):\n        links = default_links_factory(pid)\n        for link in additional_links:\n            links[link] = additional_links[link].format(pid=pid,\n                                                        scheme=request.scheme,\n                                                        host=request.host)\n        return links\n\n    return factory", "response": "Generate a links generation factory with the specified additional links."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef geolocation_sort(field_name, argument, unit, mode=None,\n                     distance_type=None):\n    \"\"\"Sort field factory for geo-location based sorting.\n\n    :param argument: Name of URL query string field to parse pin location from.\n        Multiple locations can be provided. Each location can be either a\n        string \"latitude,longitude\" or a geohash.\n    :param unit: Distance unit (e.g. km).\n    :param mode: Sort mode (avg, min, max).\n    :param distance_type: Distance calculation mode.\n    :returns: Function that returns geolocation sort field.\n    \"\"\"\n    def inner(asc):\n        locations = request.values.getlist(argument, type=str)\n        field = {\n            '_geo_distance': {\n                field_name: locations,\n                'order': 'asc' if asc else 'desc',\n                'unit': unit,\n            }\n        }\n        if mode:\n            field['_geo_distance']['mode'] = mode\n        if distance_type:\n            field['_geo_distance']['distance_type'] = distance_type\n        return field\n    return inner", "response": "Returns a function that returns a geo - location based sorting field."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef eval_field(field, asc):\n    if isinstance(field, dict):\n        if asc:\n            return field\n        else:\n            # Field should only have one key and must have an order subkey.\n            field = copy.deepcopy(field)\n            key = list(field.keys())[0]\n            field[key]['order'] = reverse_order(field[key]['order'])\n            return field\n    elif callable(field):\n        return field(asc)\n    else:\n        key, key_asc = parse_sort_field(field)\n        if not asc:\n            key_asc = not key_asc\n        return {key: {'order': 'asc' if key_asc else 'desc'}}", "response": "Evaluate a field for sorting purpose."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndefaults sort query factory.", "response": "def default_sorter_factory(search, index):\n    \"\"\"Default sort query factory.\n\n    :param query: Search query.\n    :param index: Index to search in.\n    :returns: Tuple of (query, URL arguments).\n    \"\"\"\n    sort_arg_name = 'sort'\n    urlfield = request.values.get(sort_arg_name, '', type=str)\n\n    # Get default sorting if sort is not specified.\n    if not urlfield:\n        # cast to six.text_type to handle unicodes in Python 2\n        has_query = request.values.get('q', type=six.text_type)\n        urlfield = current_app.config['RECORDS_REST_DEFAULT_SORT'].get(\n            index, {}).get('query' if has_query else 'noquery', '')\n\n    # Parse sort argument\n    key, asc = parse_sort_field(urlfield)\n\n    # Get sort options\n    sort_options = current_app.config['RECORDS_REST_SORT_OPTIONS'].get(\n        index, {}).get(key)\n    if sort_options is None:\n        return (search, {})\n\n    # Get fields to sort query by\n    search = search.sort(\n        *[eval_field(f, asc) for f in sort_options['fields']]\n    )\n    return (search, {sort_arg_name: urlfield})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_unknown_fields(self, data, original_data):\n        if isinstance(original_data, list):\n            for elem in original_data:\n                self.check_unknown_fields(data, elem)\n        else:\n            for key in original_data:\n                if key not in [\n                        self.fields[field].attribute or field\n                        for field in self.fields\n                ]:\n                    raise ValidationError(\n                        'Unknown field name {}'.format(key), field_names=[key])", "response": "Check for unknown keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_unknown_fields(self, data, original_data):\n        if isinstance(original_data, list):\n            for elem in original_data:\n                self.load_unknown_fields(data, elem)\n        else:\n            for key, value in original_data.items():\n                if key not in data:\n                    data[key] = value\n        return data", "response": "Check for unknown keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inject_pid(self, data):\n        # Remove already deserialized \"pid\" field\n        pid_value = data.pop('pid', None)\n        if pid_value:\n            pid_field = current_app.config['PIDSTORE_RECID_FIELD']\n            data.setdefault(pid_field, pid_value)\n        return data", "response": "Inject the context PID in the RECID field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_uni_form(form):\n    if isinstance(form, BaseFormSet):\n        if settings.DEBUG:\n            template = get_template('uni_form/uni_formset.html')\n        else:\n            template = uni_formset_template\n        c = Context({'formset': form})\n    else:\n        if settings.DEBUG:\n            template = get_template('uni_form/uni_form.html')\n        else:\n            template = uni_form_template\n        c = Context({'form': form})\n    return template.render(c)", "response": "Returns a string that can be used to generate a uni - form for a given formset."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a string that can be used to display the form errors in the Uni - Forms page.", "response": "def as_uni_errors(form):\n    \"\"\"\n    Renders only form errors like django-uni-form::\n\n        {% load uni_form_tags %}\n        {{ form|as_uni_errors }}\n    \"\"\"\n    if isinstance(form, BaseFormSet):\n        template = get_template('uni_form/errors_formset.html')\n        c = Context({'formset': form})\n    else:\n        template = get_template('uni_form/errors.html')\n        c = Context({'form':form})\n    return template.render(c)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef as_uni_field(field):\n    template = get_template('uni_form/field.html')\n    c = Context({'field':field})\n    return template.render(c)", "response": "Returns a Django - uni - form field as a string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render(self, form, form_style, context):\n        return render_to_string(self.template, Context({'input': self}))", "response": "Renders the input for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering a field using the given form and context.", "response": "def render_field(field, form, form_style, context, template=None, labelclass=None, layout_object=None):\n    \"\"\"\n    Renders a django-uni-form field\n    \n    :param field: Can be a string or a Layout object like `Row`. If it's a layout\n        object, we call its render method, otherwise we instantiate a BoundField\n        and render it using default template 'uni_form/field.html'\n        The field is added to a list that the form holds called `rendered_fields`\n        to avoid double rendering fields.\n\n    :param form: The form/formset to which that field belongs to.\n    \n    :param form_style: We need this to render uni-form divs using helper's chosen\n        style.\n\n    :template: Template used for rendering the field.\n\n    :layout_object: If passed, it points to the Layout object that is being rendered.\n        We use it to store its bound fields in a list called `layout_object.bound_fields`\n    \"\"\"\n    FAIL_SILENTLY = getattr(settings, 'UNIFORM_FAIL_SILENTLY', True)\n\n    if hasattr(field, 'render'):\n        return field.render(form, form_style, context)\n    else:\n        # This allows fields to be unicode strings, always they don't use non ASCII\n        try:\n            if isinstance(field, unicode):\n                field = str(field)\n            # If `field` is not unicode then we turn it into a unicode string, otherwise doing\n            # str(field) would give no error and the field would not be resolved, causing confusion \n            else:\n                field = str(unicode(field))\n                \n        except (UnicodeEncodeError, UnicodeDecodeError):\n            raise Exception(\"Field '%s' is using forbidden unicode characters\" % field)\n\n    try:\n        field_instance = form.fields[field]\n    except KeyError:\n        if not FAIL_SILENTLY:\n            raise Exception(\"Could not resolve form field '%s'.\" % field)\n        else:\n            field_instance = None\n            logging.warning(\"Could not resolve form field '%s'.\" % field, exc_info=sys.exc_info())\n            \n    if not field in form.rendered_fields:\n        form.rendered_fields.append(field)\n    else:\n        if not FAIL_SILENTLY:\n            raise Exception(\"A field should only be rendered once: %s\" % field)\n        else:\n            logging.warning(\"A field should only be rendered once: %s\" % field, exc_info=sys.exc_info())\n\n    if field_instance is None:\n        html = ''\n    else:\n        bound_field = BoundField(form, field_instance, field)\n\n        if template is None:\n            template = default_field_template\n        else:\n            template = get_template(template)\n\n        # We save the Layout object's bound fields in the layout object's `bound_fields` list\n        if layout_object is not None:\n            layout_object.bound_fields.append(bound_field) \n        \n        html = template.render(Context({'field': bound_field, 'labelclass': labelclass}))\n\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_uni_form(parser, token):\n    token = token.split_contents()\n    form = token.pop(1)\n\n    try:\n        helper = token.pop(1)\n    except IndexError:\n        helper = None\n\n    return UniFormNode(form, helper)", "response": "Handles uni - form tagging."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Context object with all the necesarry stuff for rendering the form.", "response": "def get_render(self, context):\n        \"\"\" \n        Returns a `Context` object with all the necesarry stuff for rendering the form\n\n        :param context: `django.template.Context` variable holding the context for the node\n\n        `self.form` and `self.helper` are resolved into real Python objects resolving them\n        from the `context`. The `actual_form` can be a form or a formset. If it's a formset \n        `is_formset` is set to True. If the helper has a layout we use it, for rendering the\n        form or the formset's forms.\n        \"\"\"\n        actual_form = self.form.resolve(context)\n        attrs = {}\n        if self.helper is not None:\n            helper = self.helper.resolve(context)\n\n            if not isinstance(helper, FormHelper):\n                raise TypeError('helper object provided to uni_form tag must be a uni_form.helpers.FormHelper object.')\n            attrs = helper.get_attributes()\n        else:\n            helper = None\n\n        # We get the response dictionary \n        is_formset = isinstance(actual_form, BaseFormSet)\n        response_dict = self.get_response_dict(attrs, context, is_formset)\n\n        # If we have a helper's layout we use it, for the form or the formset's forms\n        if helper and helper.layout:\n            if not is_formset:\n                actual_form.form_html = helper.render_layout(actual_form, context)\n            else:\n                forloop = ForLoopSimulator(actual_form)\n                for form in actual_form.forms:\n                    context.update({'forloop': forloop})\n                    form.form_html = helper.render_layout(form, context)\n                    forloop.iterate()\n\n        if is_formset:\n            response_dict.update({'formset': actual_form})\n        else:\n            response_dict.update({'form': actual_form})\n\n        return Context(response_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_response_dict(self, attrs, context, is_formset):\n        form_type = \"form\"\n        if is_formset:\n            form_type = \"formset\"\n\n        # We take form/formset parameters from attrs if they are set, otherwise we use defaults\n        response_dict = {\n            '%s_action' % form_type: attrs.get(\"form_action\", ''),\n            '%s_method' % form_type: attrs.get(\"form_method\", 'post'),\n            '%s_tag' % form_type: attrs.get(\"form_tag\", True),\n            '%s_class' % form_type: attrs.get(\"class\", ''),\n            '%s_id' % form_type: attrs.get(\"id\", \"\"),\n            '%s_style' % form_type: attrs.get(\"form_style\", None),\n            'form_error_title': attrs.get(\"form_error_title\", None),\n            'formset_error_title': attrs.get(\"formset_error_title\", None),\n            'inputs': attrs.get('inputs', []),\n            'is_formset': is_formset,\n        }\n\n        if context.has_key('csrf_token'):\n            response_dict['csrf_token'] = context['csrf_token']\n\n        return response_dict", "response": "Returns a dictionary with all the parameters necessary to render the formset in a template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_layout(self, form, context):\n        form.rendered_fields = []\n        \n        html = self.layout.render(form, self.form_style, context)\n\n        for field in form.fields.keys():\n            if not field in form.rendered_fields:\n                html += render_field(field, form, self.form_style, context)\n\n        return mark_safe(html)", "response": "Returns safe html of the rendering of the layout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_attributes(self):\n        items = {}\n        items['form_method'] = self.form_method.strip()\n        items['form_tag'] = self.form_tag\n        items['form_style'] = self.form_style.strip()\n        \n        if self.form_action:\n            items['form_action'] = self.form_action.strip()\n        if self.form_id:\n            items['id'] = self.form_id.strip()\n        if self.form_class:\n            items['class'] = self.form_class.strip()\n        if self.inputs:\n            items['inputs'] = self.inputs\n        if self.form_error_title:\n            items['form_error_title'] = self.form_error_title.strip()\n        if self.formset_error_title:\n            items['formset_error_title'] = self.formset_error_title.strip()\n        return items", "response": "Returns a dictionary of all the attributes that are set in the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to add the counts for each sample in the group", "response": "def add_exp(self,gr,exp):\n        \"\"\"Function to add the counts for each sample\n\n        :param gr: name of the sample\n        :param exp: counts of sample **gr**\n\n        :returns: dict with key,values equally to name,counts.\n        \"\"\"\n        self.group[gr] = exp\n        self.total = sum(self.group.values())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a joint distribution of values from pmf1 and pmf2.", "response": "def MakeJoint(pmf1, pmf2):\n    \"\"\"Joint distribution of values from pmf1 and pmf2.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        Joint pmf of value pairs\n    \"\"\"\n    joint = Joint()\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            joint.Set((v1, v2), p1 * p2)\n    return joint"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MakeHistFromList(t, name=''):\n    hist = Hist(name=name)\n    [hist.Incr(x) for x in t]\n    return hist", "response": "Makes a histogram from an unsorted sequence of numbers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef MakePmfFromList(t, name=''):\n    hist = MakeHistFromList(t)\n    d = hist.GetDict()\n    pmf = Pmf(d, name)\n    pmf.Normalize()\n    return pmf", "response": "Makes a PMF from an unsorted sequence of numbers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef MakePmfFromDict(d, name=''):\n    pmf = Pmf(d, name)\n    pmf.Normalize()\n    return pmf", "response": "Makes a PMF from a dictionary that maps values to probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MakePmfFromItems(t, name=''):\n    pmf = Pmf(dict(t), name)\n    pmf.Normalize()\n    return pmf", "response": "Makes a PMF from a sequence of value - probability pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a normalized PMF from a Hist object.", "response": "def MakePmfFromHist(hist, name=None):\n    \"\"\"Makes a normalized PMF from a Hist object.\n\n    Args:\n        hist: Hist object\n        name: string name\n\n    Returns:\n        Pmf object\n    \"\"\"\n    if name is None:\n        name = hist.name\n\n    # make a copy of the dictionary\n    d = dict(hist.GetDict())\n    pmf = Pmf(d, name)\n    pmf.Normalize()\n    return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a normalized Pmf from a Cdf object.", "response": "def MakePmfFromCdf(cdf, name=None):\n    \"\"\"Makes a normalized Pmf from a Cdf object.\n\n    Args:\n        cdf: Cdf object\n        name: string name for the new Pmf\n\n    Returns:\n        Pmf object\n    \"\"\"\n    if name is None:\n        name = cdf.name\n\n    pmf = Pmf(name=name)\n\n    prev = 0.0\n    for val, prob in cdf.Items():\n        pmf.Incr(val, prob - prev)\n        prev = prob\n\n    return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a mixture distribution.", "response": "def MakeMixture(metapmf, name='mix'):\n    \"\"\"Make a mixture distribution.\n\n    Args:\n      metapmf: Pmf that maps from Pmfs to probs.\n      name: string name for the new Pmf.\n\n    Returns: Pmf object.\n    \"\"\"\n    mix = Pmf(name=name)\n    for pmf, p1 in metapmf.Items():\n        for x, p2 in pmf.Items():\n            mix.Incr(x, p1 * p2)\n    return mix"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking a uniform Pmf.", "response": "def MakeUniformPmf(low, high, n):\n    \"\"\"Make a uniform Pmf.\n\n    low: lowest value (inclusive)\n    high: highest value (inclusize)\n    n: number of values\n    \"\"\"\n    pmf = Pmf()\n    for x in numpy.linspace(low, high, n):\n        pmf.Set(x, 1)\n    pmf.Normalize()\n    return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a cdf from an unsorted sequence of ( value frequency ) pairs.", "response": "def MakeCdfFromItems(items, name=''):\n    \"\"\"Makes a cdf from an unsorted sequence of (value, frequency) pairs.\n\n    Args:\n        items: unsorted sequence of (value, frequency) pairs\n        name: string name for this CDF\n\n    Returns:\n        cdf: list of (value, fraction) pairs\n    \"\"\"\n    runsum = 0\n    xs = []\n    cs = []\n\n    for value, count in sorted(items):\n        runsum += count\n        xs.append(value)\n        cs.append(runsum)\n\n    total = float(runsum)\n    ps = [c / total for c in cs]\n\n    cdf = Cdf(xs, ps, name)\n    return cdf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MakeCdfFromPmf(pmf, name=None):\n    if name == None:\n        name = pmf.name\n    return MakeCdfFromItems(pmf.Items(), name)", "response": "Makes a CDF from a Pmf object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MakeSuiteFromList(t, name=''):\n    hist = MakeHistFromList(t)\n    d = hist.GetDict()\n    return MakeSuiteFromDict(d)", "response": "Makes a suite from an unsorted sequence of values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmake a normalized suite from a Hist object.", "response": "def MakeSuiteFromHist(hist, name=None):\n    \"\"\"Makes a normalized suite from a Hist object.\n\n    Args:\n        hist: Hist object\n        name: string name\n\n    Returns:\n        Suite object\n    \"\"\"\n    if name is None:\n        name = hist.name\n\n    # make a copy of the dictionary\n    d = dict(hist.GetDict())\n    return MakeSuiteFromDict(d, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef MakeSuiteFromDict(d, name=''):\n    suite = Suite(name=name)\n    suite.SetDict(d)\n    suite.Normalize()\n    return suite", "response": "Makes a suite from a dictionary that maps values to probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a normalized Suite from a Cdf object.", "response": "def MakeSuiteFromCdf(cdf, name=None):\n    \"\"\"Makes a normalized Suite from a Cdf object.\n\n    Args:\n        cdf: Cdf object\n        name: string name for the new Suite\n\n    Returns:\n        Suite object\n    \"\"\"\n    if name is None:\n        name = cdf.name\n\n    suite = Suite(name=name)\n\n    prev = 0.0\n    for val, prob in cdf.Items():\n        suite.Incr(val, prob - prev)\n        prev = prob\n\n    return suite"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes a percentile of a given Pmf.", "response": "def Percentile(pmf, percentage):\n    \"\"\"Computes a percentile of a given Pmf.\n\n    percentage: float 0-100\n    \"\"\"\n    p = percentage / 100.0\n    total = 0\n    for val, prob in pmf.Items():\n        total += prob\n        if total >= p:\n            return val"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CredibleInterval(pmf, percentage=90):\n    cdf = pmf.MakeCdf()\n    prob = (1 - percentage / 100.0) / 2\n    interval = cdf.Value(prob), cdf.Value(1 - prob)\n    return interval", "response": "Computes a credible interval for a given posterior distribution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the probability that a value from pmf1 is less than a value from pmf2.", "response": "def PmfProbLess(pmf1, pmf2):\n    \"\"\"Probability that a value from pmf1 is less than a value from pmf2.\n\n    Args:\n        pmf1: Pmf object\n        pmf2: Pmf object\n\n    Returns:\n        float probability\n    \"\"\"\n    total = 0.0\n    for v1, p1 in pmf1.Items():\n        for v2, p2 in pmf2.Items():\n            if v1 < v2:\n                total += p1 * p2\n    return total"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef SampleSum(dists, n):\n    pmf = MakePmfFromList(RandomSum(dists) for i in xrange(n))\n    return pmf", "response": "Draws a sample of sums from a list of distributions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef EvalGaussianPdf(x, mu, sigma):\n    return scipy.stats.norm.pdf(x, mu, sigma)", "response": "Computes the unnormalized PDF of the normal distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a PMF discrete approx to a Gaussian distribution.", "response": "def MakeGaussianPmf(mu, sigma, num_sigmas, n=201):\n    \"\"\"Makes a PMF discrete approx to a Gaussian distribution.\n    \n    mu: float mean\n    sigma: float standard deviation\n    num_sigmas: how many sigmas to extend in each direction\n    n: number of values in the Pmf\n\n    returns: normalized Pmf\n    \"\"\"\n    pmf = Pmf()\n    low = mu - num_sigmas * sigma\n    high = mu + num_sigmas * sigma\n\n    for x in numpy.linspace(low, high, n):\n        p = EvalGaussianPdf(x, mu, sigma)\n        pmf.Set(x, p)\n    pmf.Normalize()\n    return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef EvalBinomialPmf(k, n, p):\n    return scipy.stats.binom.pmf(k, n, p)", "response": "Evaluates the binomial pmf."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef EvalPoissonPmf(k, lam):\n    # don't use the scipy function (yet).  for lam=0 it returns NaN;\n    # should be 0.0\n    # return scipy.stats.poisson.pmf(k, lam)\n\n    return lam ** k * math.exp(-lam) / math.factorial(k)", "response": "Computes the Poisson PMF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MakePoissonPmf(lam, high, step=1):\n    pmf = Pmf()\n    for k in xrange(0, high + 1, step):\n        p = EvalPoissonPmf(k, lam)\n        pmf.Set(k, p)\n    pmf.Normalize()\n    return pmf", "response": "Makes a PMF discrete approx to a Poisson distribution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef MakeExponentialPmf(lam, high, n=200):\n    pmf = Pmf()\n    for x in numpy.linspace(0, high, n):\n        p = EvalExponentialPdf(x, lam)\n        pmf.Set(x, p)\n    pmf.Normalize()\n    return pmf", "response": "Makes a PMF discrete approx to an exponential distribution."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GaussianCdfInverse(p, mu=0, sigma=1):\n    x = ROOT2 * erfinv(2 * p - 1)\n    return mu + x * sigma", "response": "Evaluates the inverse CDF of the gaussian distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the log of the binomial coefficient.", "response": "def LogBinomialCoef(n, k):\n    \"\"\"Computes the log of the binomial coefficient.\n\n    http://math.stackexchange.com/questions/64716/\n    approximating-the-logarithm-of-the-binomial-coefficient\n\n    n: number of trials\n    k: number of successes\n\n    Returns: float\n    \"\"\"\n    return n * log(n) - k * log(k) - (n - k) * log(n - k)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Lookup(self, x):\n        return self._Bisect(x, self.xs, self.ys)", "response": "Looks up x and returns the corresponding value of y."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Reverse(self, y):\n        return self._Bisect(y, self.ys, self.xs)", "response": "Looks up y and returns the corresponding value of x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing with a map from value to probability.", "response": "def InitMapping(self, values):\n        \"\"\"Initializes with a map from value to probability.\n\n        values: map from value to probability\n        \"\"\"\n        for value, prob in values.iteritems():\n            self.Set(value, prob)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing with a Pmf.", "response": "def InitPmf(self, values):\n        \"\"\"Initializes with a Pmf.\n\n        values: Pmf object\n        \"\"\"\n        for value, prob in values.Items():\n            self.Set(value, prob)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Copy(self, name=None):\n        new = copy.copy(self)\n        new.d = copy.copy(self.d)\n        new.name = name if name is not None else self.name\n        return new", "response": "Returns a shallow copy of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Scale(self, factor):\n        new = self.Copy()\n        new.d.clear()\n\n        for val, prob in self.Items():\n            new.Set(val * factor, prob)\n        return new", "response": "Multiplies the values by a factor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Log(self, m=None):\n        if self.log:\n            raise ValueError(\"Pmf/Hist already under a log transform\")\n        self.log = True\n\n        if m is None:\n            m = self.MaxLike()\n\n        for x, p in self.d.iteritems():\n            if p:\n                self.Set(x, math.log(p / m))\n            else:\n                self.Remove(x)", "response": "Log transforms the probabilities."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the values and freqs and probabilities in ascending order.", "response": "def Print(self):\n        \"\"\"Prints the values and freqs/probs in ascending order.\"\"\"\n        for val, prob in sorted(self.d.iteritems()):\n            print(val, prob)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nincrementing the freq / prob associated with the value x.", "response": "def Incr(self, x, term=1):\n        \"\"\"Increments the freq/prob associated with the value x.\n\n        Args:\n            x: number value\n            term: how much to increment by\n        \"\"\"\n        self.d[x] = self.d.get(x, 0) + term"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmultiply the freq / prob associated with the value x.", "response": "def Mult(self, x, factor):\n        \"\"\"Scales the freq/prob associated with the value x.\n\n        Args:\n            x: number value\n            factor: how much to multiply by\n        \"\"\"\n        self.d[x] = self.d.get(x, 0) * factor"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef IsSubset(self, other):\n        for val, freq in self.Items():\n            if freq > other.Freq(val):\n                return False\n        return True", "response": "Checks whether the values in this histogram are a subset of the values in the given histogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Subtract(self, other):\n        for val, freq in other.Items():\n            self.Incr(val, -freq)", "response": "Subtracts the values in the given histogram from this histogram."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the probability that a sample from this Pmf exceeds x.", "response": "def ProbGreater(self, x):\n        \"\"\"Probability that a sample from this Pmf exceeds x.\n\n        x: number\n\n        returns: float probability\n        \"\"\"\n        t = [prob for (val, prob) in self.d.iteritems() if val > x]\n        return sum(t)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nnormalizes this PMF so the sum of all probs is fraction.", "response": "def Normalize(self, fraction=1.0):\n        \"\"\"Normalizes this PMF so the sum of all probs is fraction.\n\n        Args:\n            fraction: what the total should be after normalization\n\n        Returns: the total probability before normalizing\n        \"\"\"\n        if self.log:\n            raise ValueError(\"Pmf is under a log transform\")\n\n        total = self.Total()\n        if total == 0.0:\n            raise ValueError('total probability is zero.')\n            logging.warning('Normalize: total probability is zero.')\n            return total\n\n        factor = float(fraction) / total\n        for x in self.d:\n            self.d[x] *= factor\n\n        return total"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Random(self):\n        if len(self.d) == 0:\n            raise ValueError('Pmf contains no values.')\n\n        target = random.random()\n        total = 0.0\n        for x, p in self.d.iteritems():\n            total += p\n            if total >= target:\n                return x\n\n        # we shouldn't get here\n        assert False", "response": "Chooses a random element from this PMF."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Mean(self):\n        mu = 0.0\n        for x, p in self.d.iteritems():\n            mu += p * x\n        return mu", "response": "Computes the mean of a PMF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the variance of a PMF.", "response": "def Var(self, mu=None):\n        \"\"\"Computes the variance of a PMF.\n\n        Args:\n            mu: the point around which the variance is computed;\n                if omitted, computes the mean\n\n        Returns:\n            float variance\n        \"\"\"\n        if mu is None:\n            mu = self.Mean()\n\n        var = 0.0\n        for x, p in self.d.iteritems():\n            var += p * (x - mu) ** 2\n        return var"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value with the highest probability.", "response": "def MaximumLikelihood(self):\n        \"\"\"Returns the value with the highest probability.\n\n        Returns: float probability\n        \"\"\"\n        prob, val = max((prob, val) for val, prob in self.Items())\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the Pmf of the sum of values drawn from self and other.", "response": "def AddPmf(self, other):\n        \"\"\"Computes the Pmf of the sum of values drawn from self and other.\n\n        other: another Pmf\n\n        returns: new Pmf\n        \"\"\"\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            for v2, p2 in other.Items():\n                pmf.Incr(v1 + v2, p1 * p2)\n        return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the Pmf of the sum a constant and values from self and other.", "response": "def AddConstant(self, other):\n        \"\"\"Computes the Pmf of the sum a constant and  values from self.\n\n        other: a number\n\n        returns: new Pmf\n        \"\"\"\n        pmf = Pmf()\n        for v1, p1 in self.Items():\n            pmf.Set(v1 + other, p1)\n        return pmf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the CDF of the maximum of k selections from this distribution.", "response": "def Max(self, k):\n        \"\"\"Computes the CDF of the maximum of k selections from this dist.\n\n        k: int\n\n        returns: new Cdf\n        \"\"\"\n        cdf = self.MakeCdf()\n        cdf.ps = [p ** k for p in cdf.ps]\n        return cdf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the marginal distribution of the indicated variable.", "response": "def Marginal(self, i, name=''):\n        \"\"\"Gets the marginal distribution of the indicated variable.\n\n        i: index of the variable we want\n\n        Returns: Pmf\n        \"\"\"\n        pmf = Pmf(name=name)\n        for vs, prob in self.Items():\n            pmf.Incr(vs[i], prob)\n        return pmf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Conditional(self, i, j, val, name=''):\n        pmf = Pmf(name=name)\n        for vs, prob in self.Items():\n            if vs[j] != val: continue\n            pmf.Incr(vs[i], prob)\n\n        pmf.Normalize()\n        return pmf", "response": "Gets the conditional distribution of the indicated variable."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MaxLikeInterval(self, percentage=90):\n        interval = []\n        total = 0\n\n        t = [(prob, val) for val, prob in self.Items()]\n        t.sort(reverse=True)\n\n        for prob, val in t:\n            interval.append(val)\n            total += prob\n            if total >= percentage / 100.0:\n                break\n\n        return interval", "response": "Returns the maximum - likelihood credible interval."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a copy of this Cdf.", "response": "def Copy(self, name=None):\n        \"\"\"Returns a copy of this Cdf.\n\n        Args:\n            name: string name for the new Cdf\n        \"\"\"\n        if name is None:\n            name = self.name\n        return Cdf(list(self.xs), list(self.ps), name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Append(self, x, p):\n        self.xs.append(x)\n        self.ps.append(p)", "response": "Add an x and p pair to the end of this CDF."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Shift(self, term):\n        new = self.Copy()\n        new.xs = [x + term for x in self.xs]\n        return new", "response": "Adds a term to the xs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Scale(self, factor):\n        new = self.Copy()\n        new.xs = [x * factor for x in self.xs]\n        return new", "response": "Multiplies the xs by a factor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn CDF ( x ) the probability that corresponds to value x.", "response": "def Prob(self, x):\n        \"\"\"Returns CDF(x), the probability that corresponds to value x.\n\n        Args:\n            x: number\n\n        Returns:\n            float probability\n        \"\"\"\n        if x < self.xs[0]: return 0.0\n        index = bisect.bisect(self.xs, x)\n        p = self.ps[index - 1]\n        return p"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Value(self, p):\n        if p < 0 or p > 1:\n            raise ValueError('Probability p must be in range [0, 1]')\n\n        if p == 0: return self.xs[0]\n        if p == 1: return self.xs[-1]\n        index = bisect.bisect(self.ps, p)\n        if p == self.ps[index - 1]:\n            return self.xs[index - 1]\n        else:\n            return self.xs[index]", "response": "Returns InverseCDF ( p ) the value that corresponds to probability p."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Mean(self):\n        old_p = 0\n        total = 0.0\n        for x, new_p in zip(self.xs, self.ps):\n            p = new_p - old_p\n            total += p * x\n            old_p = new_p\n        return total", "response": "Computes the mean of a CDF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the central credible interval.", "response": "def CredibleInterval(self, percentage=90):\n        \"\"\"Computes the central credible interval.\n\n        If percentage=90, computes the 90% CI.\n\n        Args:\n            percentage: float between 0 and 100\n\n        Returns:\n            sequence of two floats, low and high\n        \"\"\"\n        prob = (1 - percentage / 100.0) / 2\n        interval = self.Value(prob), self.Value(1 - prob)\n        return interval"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef Render(self):\n        xs = [self.xs[0]]\n        ps = [0.0]\n        for i, p in enumerate(self.ps):\n            xs.append(self.xs[i])\n            ps.append(p)\n\n            try:\n                xs.append(self.xs[i + 1])\n                ps.append(p)\n            except IndexError:\n                pass\n        return xs, ps", "response": "Generates a sequence of points suitable for plotting."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the CDF of the maximum of k selections from this distribution.", "response": "def Max(self, k):\n        \"\"\"Computes the CDF of the maximum of k selections from this dist.\n\n        k: int\n\n        returns: new Cdf\n        \"\"\"\n        cdf = self.Copy()\n        cdf.ps = [p ** k for p in cdf.ps]\n        return cdf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef LogUpdate(self, data):\n        for hypo in self.Values():\n            like = self.LogLikelihood(data, hypo)\n            self.Incr(hypo, like)", "response": "Updates a suite of hypotheses based on new data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating each hypothesis in the set based on the dataset.", "response": "def UpdateSet(self, dataset):\n        \"\"\"Updates each hypothesis based on the dataset.\n\n        This is more efficient than calling Update repeatedly because\n        it waits until the end to Normalize.\n\n        Modifies the suite directly; if you want to keep the original, make\n        a copy.\n\n        dataset: a sequence of data\n\n        returns: the normalizing constant\n        \"\"\"\n        for data in dataset:\n            for hypo in self.Values():\n                like = self.Likelihood(data, hypo)\n                self.Mult(hypo, like)\n        return self.Normalize()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint the hypotheses and their probabilities.", "response": "def Print(self):\n        \"\"\"Prints the hypotheses and their probabilities.\"\"\"\n        for hypo, prob in sorted(self.Items()):\n            print(hypo, prob)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef MakeOdds(self):\n        for hypo, prob in self.Items():\n            if prob:\n                self.Set(hypo, Odds(prob))\n            else:\n                self.Remove(hypo)", "response": "Transforms from probabilities to odds."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransforming from odds to probabilities.", "response": "def MakeProbs(self):\n        \"\"\"Transforms from odds to probabilities.\"\"\"\n        for hypo, odds in self.Items():\n            self.Set(hypo, Probability(odds))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MakePmf(self, xs, name=''):\n        pmf = Pmf(name=name)\n        for x in xs:\n            pmf.Set(x, self.Density(x))\n        pmf.Normalize()\n        return pmf", "response": "Makes a discrete version of this Pdf evaluated at xs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Update(self, data):\n        heads, tails = data\n        self.alpha += heads\n        self.beta += tails", "response": "Updates a Beta distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a random sample from this distribution.", "response": "def Sample(self, n):\n        \"\"\"Generates a random sample from this distribution.\n\n        n: int sample size\n        \"\"\"\n        size = n,\n        return numpy.random.beta(self.alpha, self.beta, size)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef MakePmf(self, steps=101, name=''):\n        if self.alpha < 1 or self.beta < 1:\n            cdf = self.MakeCdf()\n            pmf = cdf.MakePmf()\n            return pmf\n\n        xs = [i / (steps - 1.0) for i in xrange(steps)]\n        probs = [self.EvalPdf(x) for x in xs]\n        pmf = MakePmfFromDict(dict(zip(xs, probs)), name)\n        return pmf", "response": "Returns a Pmf of this distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the CDF of this distribution.", "response": "def MakeCdf(self, steps=101):\n        \"\"\"Returns the CDF of this distribution.\"\"\"\n        xs = [i / (steps - 1.0) for i in xrange(steps)]\n        ps = [scipy.special.betainc(self.alpha, self.beta, x) for x in xs]\n        cdf = Cdf(xs, ps)\n        return cdf"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Update(self, data):\n        m = len(data)\n        self.params[:m] += data", "response": "Updates a Dirichlet distribution.\n        data : sequence of observations in order corresponding to params\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a random variate from this distribution.", "response": "def Random(self):\n        \"\"\"Generates a random variate from this distribution.\n\n        Returns: normalized vector of fractions\n        \"\"\"\n        p = numpy.random.gamma(self.params)\n        return p / p.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the likelihood of the data.", "response": "def Likelihood(self, data):\n        \"\"\"Computes the likelihood of the data.\n\n        Selects a random vector of probabilities from this distribution.\n\n        Returns: float probability\n        \"\"\"\n        m = len(data)\n        if self.n < m:\n            return 0\n\n        x = data\n        p = self.Random()\n        q = p[:m] ** x\n        return q.prod()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the log likelihood of the data.", "response": "def LogLikelihood(self, data):\n        \"\"\"Computes the log likelihood of the data.\n\n        Selects a random vector of probabilities from this distribution.\n\n        Returns: float log probability\n        \"\"\"\n        m = len(data)\n        if self.n < m:\n            return float('-inf')\n\n        x = self.Random()\n        y = numpy.log(x[:m]) * data\n        return y.sum()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef MarginalBeta(self, i):\n        alpha0 = self.params.sum()\n        alpha = self.params[i]\n        return Beta(alpha, alpha0 - alpha)", "response": "Computes the marginal distribution of the ith element."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef PredictivePmf(self, xs, name=''):\n        alpha0 = self.params.sum()\n        ps = self.params / alpha0\n        return MakePmfFromItems(zip(xs, ps), name=name)", "response": "Makes a predictive distribution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the annotation for the given databases and features.", "response": "def _get_ann(dbs, features):\n    \"\"\"\n    Gives format to annotation for html table output\n    \"\"\"\n    value = \"\"\n    for db, feature in zip(dbs, features):\n        value += db + \":\" + feature\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef make_profile(data, out_dir, args):\n    safe_dirs(out_dir)\n    main_table = []\n    header = ['id', 'ann']\n    n = len(data[0])\n    bar = ProgressBar(maxval=n)\n    bar.start()\n    bar.update(0)\n    for itern, c in enumerate(data[0]):\n        bar.update(itern)\n        logger.debug(\"creating cluser: {}\".format(c))\n        safe_dirs(os.path.join(out_dir, c))\n        valid, ann, pos_structure = _single_cluster(c, data, os.path.join(out_dir, c, \"maps.tsv\"), args)\n        data[0][c].update({'profile': pos_structure})\n        loci = data[0][c]['loci']\n        data[0][c]['precursor'] = {\"seq\": precursor_sequence(loci[0][0:5], args.ref)}\n        data[0][c]['precursor'][\"colors\"] = _parse(data[0][c]['profile'], data[0][c]['precursor'][\"seq\"])\n        data[0][c]['precursor'].update(run_rnafold(data[0][c]['precursor']['seq']))\n\n    return data", "response": "Make data report for each cluster"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand(dat, counts, start, end):\n    for pos in range(start, end):\n        for s in counts:\n            dat[s][pos] += counts[s]\n    return dat", "response": "expand the same counts from start to end"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting data frame into pandas DataFrame", "response": "def _convert_to_df(in_file, freq, raw_file):\n    \"\"\"\n    convert data frame into table with pandas\n    \"\"\"\n    dat = defaultdict(Counter)\n    if isinstance(in_file, (str, unicode)):\n        with open(in_file) as in_handle:\n            for line in in_handle:\n                cols = line.strip().split(\"\\t\")\n                counts = freq[cols[3]]\n                dat = _expand(dat, counts, int(cols[1]), int(cols[2]))\n    else:\n        if raw_file:\n            out_handle = open(raw_file, \"w\")\n        for name in in_file:\n            counts = freq[name]\n            if raw_file:\n                print(\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\" % (\"chr\", in_file[name][0], in_file[name][1], name, sum(counts.values()), \"+\"), file=out_handle, end=\"\")\n            dat = _expand(dat, counts, in_file[name][0], in_file[name][1])\n\n    for s in dat:\n        for p in dat[s]:\n            dat[s][p] = mlog2(dat[s][p] + 1)\n    return dat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make(c):\n    ann = defaultdict(list)\n\n    for pos in c['ann']:\n        for db in pos:\n            ann[db] += list(pos[db])\n    logger.debug(ann)\n\n    valid = [l for l in c['valid']]\n    ann_list = [\", \".join(list(set(ann[feature]))) for feature in ann if feature in valid]\n\n    return valid, ann_list", "response": "create html from template adding figure and sequence counts counts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _single_cluster(c, data, out_file, args):\n    valid, ann = 0, 0\n    raw_file = None\n    freq = defaultdict()\n    [freq.update({s.keys()[0]: s.values()[0]}) for s in data[0][c]['freq']]\n    names = [s.keys()[0] for s in data[0][c]['seqs']]\n    seqs = [s.values()[0] for s in data[0][c]['seqs']]\n    loci = data[0][c]['loci']\n\n    if loci[0][3] - loci[0][2] > 500:\n        logger.info(\"locus bigger > 500 nt, skipping: %s\" % loci)\n        return valid, ann, {}\n    if not file_exists(out_file):\n        if args.razer:\n            logger.debug(\"map with razer all sequences to all loci %s \" % loci)\n            map_to_precursors(seqs, names, {loci[0][0]: [loci[0][0:5]]}, out_file, args)\n        else:\n            logger.debug(\"map with biopython fn all sequences to all loci %s \" % loci)\n            if args.debug:\n                raw_file = out_file\n            out_file = map_to_precursor_biopython(seqs, names, loci[0][0:5], args)\n\n    logger.debug(\"plot sequences on loci\")\n    df = _convert_to_df(out_file, freq, raw_file)\n    if df:\n        valid, ann = _make(data[0][c])\n\n    return valid, ann, df", "response": "Map sequences on precursors and create\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreads json cluster and populate as cluster class", "response": "def read_cluster(data, id=1):\n    \"\"\"Read json cluster and populate as cluster class\"\"\"\n    cl = cluster(1)\n\n    # seqs = [s.values()[0] for s in data['seqs']]\n    names = [s.keys()[0] for s in data['seqs']]\n    cl.add_id_member(names, 1)\n    freq = defaultdict()\n    [freq.update({s.keys()[0]: s.values()[0]}) for s in data['freq']]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites json file from seqcluster cluster", "response": "def write_data(data, out_file):\n    \"\"\"write json file from seqcluster cluster\"\"\"\n    with open(out_file, 'w') as handle_out:\n        handle_out.write(json.dumps([data], skipkeys=True, indent=2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_sequences_from_cluster(c1, c2, data):\n    seqs1 = data[c1]['seqs']\n    seqs2 = data[c2]['seqs']\n    seqs = list(set(seqs1 + seqs2))\n    names = []\n    for s in seqs:\n        if s in seqs1 and s in seqs2:\n            names.append(\"both\")\n        elif s in seqs1:\n            names.append(c1)\n        else:\n            names.append(c2)\n    return seqs, names", "response": "get all sequences from on cluster"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmaps sequences to precursors with razers3", "response": "def map_to_precursors(seqs, names, loci, out_file, args):\n    \"\"\"map sequences to precursors with razers3\"\"\"\n    with make_temp_directory() as temp:\n        pre_fasta = os.path.join(temp, \"pre.fa\")\n        seqs_fasta = os.path.join(temp, \"seqs.fa\")\n        out_sam = os.path.join(temp, \"out.sam\")\n        pre_fasta = get_loci_fasta(loci, pre_fasta, args.ref)\n        out_precursor_file = out_file.replace(\"tsv\", \"fa\")\n        seqs_fasta = get_seqs_fasta(seqs, names, seqs_fasta)\n\n        # print(open(pre_fasta).read().split(\"\\n\")[1])\n        if find_cmd(\"razers3\"):\n            cmd = \"razers3 -dr 2 -i 80 -rr 90 -f -o {out_sam} {temp}/pre.fa  {seqs_fasta}\"\n            run(cmd.format(**locals()))\n            out_file = read_alignment(out_sam, loci, seqs, out_file)\n            shutil.copy(pre_fasta, out_precursor_file)\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget sequence from genome", "response": "def precursor_sequence(loci, reference):\n    \"\"\"Get sequence from genome\"\"\"\n    region = \"%s\\t%s\\t%s\\t.\\t.\\t%s\" % (loci[1], loci[2], loci[3], loci[4])\n    precursor = pybedtools.BedTool(str(region), from_string=True).sequence(fi=reference, s=True)\n    return open(precursor.seqfn).read().split(\"\\n\")[1]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap sequences to precursors with franpr algorithm to avoid writting on disk", "response": "def map_to_precursors_on_fly(seqs, names, loci, args):\n    \"\"\"map sequences to precursors with franpr algorithm to avoid writting on disk\"\"\"\n    precursor = precursor_sequence(loci, args.ref).upper()\n    dat = dict()\n    for s, n in itertools.izip(seqs, names):\n        res = pyMatch.Match(precursor, str(s), 1, 3)\n        if res > -1:\n            dat[n] = [res, res + len(s)]\n    logger.debug(\"mapped in %s: %s out of %s\" % (loci, len(dat), len(seqs)))\n    return dat"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _align(x, y, local = False):\n    if local:\n        aligned_x = pairwise2.align.localxx(x, y)\n    else:\n        aligned_x =  pairwise2.align.globalms(x, y, 1, -1, -1, -0.5)\n    \n    if aligned_x:\n        sorted_alignments = sorted(aligned_x, key=operator.itemgetter(2))\n        e = enumerate(sorted_alignments[0][0])\n        nts = [i for i,c in e if c != \"-\"]\n        return [min(nts), max(nts)]", "response": "aligns x y with local or global"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map_to_precursor_biopython(seqs, names, loci, args):\n    precursor = precursor_sequence(loci, args.ref).upper()\n    dat = dict()\n    for s, n in itertools.izip(seqs, names):\n        res = _align(str(s), precursor)\n        if res:\n            dat[n] = res\n    logger.debug(\"mapped in %s: %s out of %s\" % (loci, len(dat), len(seqs)))\n    return dat", "response": "map the sequences to the precursor sequence"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_seqs_fasta(seqs, names, out_fa):\n    with open(out_fa, 'w') as fa_handle:\n        for s, n in itertools.izip(seqs, names):\n            print(\">cx{1}-{0}\\n{0}\".format(s, n), file=fa_handle)\n    return out_fa", "response": "get fasta from sequences"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_loci_fasta(loci, out_fa, ref):\n    if not find_cmd(\"bedtools\"):\n        raise ValueError(\"Not bedtools installed\")\n    with make_temp_directory() as temp:\n        bed_file = os.path.join(temp, \"file.bed\")\n        for nc, loci in loci.iteritems():\n            for l in loci:\n                with open(bed_file, 'w') as bed_handle:\n                    logger.debug(\"get_fasta: loci %s\" % l)\n                    nc, c, s, e, st = l\n                    print(\"{0}\\t{1}\\t{2}\\t{3}\\t{3}\\t{4}\".format(c, s, e, nc, st), file=bed_handle)\n                get_fasta(bed_file, ref, out_fa)\n    return out_fa", "response": "get fasta from precursor"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_alignment(out_sam, loci, seqs, out_file):\n    hits = defaultdict(list)\n    with open(out_file, \"w\") as out_handle:\n        samfile = pysam.Samfile(out_sam, \"r\")\n        for a in samfile.fetch():\n            if not a.is_unmapped:\n                nm = int([t[1] for t in a.tags if t[0] == \"NM\"][0])\n                a = makeBED(a)\n                if not a:\n                    continue\n                ref, locus = get_loci(samfile.getrname(int(a.chr)), loci)\n                hits[a.name].append((nm, \"%s %s %s %s %s %s\" % (a.name, a.name.split(\"-\")[0], locus, ref, a.start, a.end)))\n        for hit in hits.values():\n            nm = hit[0][0]\n            for l in hit:\n                if nm == l[0]:\n                    print(l[1], file=out_handle)\n    return out_file", "response": "read which seqs map to which loci and\n    return a tab separated file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _download_mirbase(args, version=\"CURRENT\"):\n    if not args.hairpin or not args.mirna:\n        logger.info(\"Working with version %s\" % version)\n        hairpin_fn = op.join(op.abspath(args.out), \"hairpin.fa.gz\")\n        mirna_fn = op.join(op.abspath(args.out), \"miRNA.str.gz\")\n        if not file_exists(hairpin_fn):\n            cmd_h = \"wget ftp://mirbase.org/pub/mirbase/%s/hairpin.fa.gz -O %s &&  gunzip -f !$\" % (version, hairpin_fn)\n            do.run(cmd_h, \"download hairpin\")\n        if not file_exists(mirna_fn):\n            cmd_m = \"wget ftp://mirbase.org/pub/mirbase/%s/miRNA.str.gz -O %s && gunzip -f !$\" % (version, mirna_fn)\n            do.run(cmd_m, \"download mirna\")\n    else:\n        return args.hairpin, args.mirna", "response": "Download files from mirbase if not present"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _make_unique(name, idx):\n    p = re.compile(\".[aA-zZ]+_x[0-9]+\")\n    if p.match(name):\n        tags = name[1:].split(\"_x\")\n        return \">%s_%s_x%s\" % (tags[0], idx, tags[1])\n    return name.replace(\"@\", \">\")", "response": "Make name unique in case only counts there"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert names of sequences to unique ids", "response": "def _filter_seqs(fn):\n    \"\"\"Convert names of sequences to unique ids\"\"\"\n    out_file = op.splitext(fn)[0] + \"_unique.fa\"\n    idx = 0\n    if not file_exists(out_file):\n        with open(out_file, 'w') as out_handle:\n            with open(fn) as in_handle:\n                for line in in_handle:\n                    if line.startswith(\"@\") or line.startswith(\">\"):\n                        fixed_name = _make_unique(line.strip(), idx)\n                        seq = in_handle.next().strip()\n                        counts = _get_freq(fixed_name)\n                        if len(seq) < 26 and (counts > 1 or counts == 0):\n                            idx += 1\n                            print(fixed_name, file=out_handle, end=\"\\n\")\n                            print(seq, file=out_handle, end=\"\\n\")\n                        if line.startswith(\"@\"):\n                            in_handle.next()\n                            in_handle.next()\n    return out_file"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the precursor file for that species and return a dictionary of hairpin names to species.", "response": "def _read_precursor(precursor, sps):\n    \"\"\"\n    Load precursor file for that species\n    \"\"\"\n    hairpin = defaultdict(str)\n    name = None\n    with open(precursor) as in_handle:\n        for line in in_handle:\n            if line.startswith(\">\"):\n                if hairpin[name]:\n                    hairpin[name] = hairpin[name] + \"NNNNNNNNNNNN\"\n                name = line.strip().replace(\">\", \" \").split()[0]\n            else:\n                hairpin[name] += line.strip()\n        hairpin[name] = hairpin[name] + \"NNNNNNNNNNNN\"\n    return hairpin"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the GTF file and return a dictionary of precursor positions on the genome.", "response": "def _read_gtf(gtf):\n    \"\"\"\n    Load GTF file with precursor positions on genome\n    \"\"\"\n    if not gtf:\n        return gtf\n    db = defaultdict(list)\n    with open(gtf) as in_handle:\n        for line in in_handle:\n            if line.startswith(\"#\"):\n                continue\n            cols = line.strip().split(\"\\t\")\n            name = [n.split(\"=\")[1] for n in cols[-1].split(\";\") if n.startswith(\"Name\")]\n            chrom, start, end, strand = cols[0], cols[3], cols[4], cols[6]\n            if cols[2] == \"miRNA_primary_transcript\":\n                db[name[0]].append([chrom, int(start), int(end), strand])\n    return db"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _coord(sequence, start, mirna, precursor, iso):\n    dif = abs(mirna[0] - start)\n    if start < mirna[0]:\n        iso.t5 = sequence[:dif].upper()\n    elif start > mirna[0]:\n        iso.t5 = precursor[mirna[0] - 1:mirna[0] - 1 + dif].lower()\n    elif start == mirna[0]:\n        iso.t5 = \"NA\"\n    if dif > 4:\n        logger.debug(\"start > 3 %s %s %s %s %s\" % (start, len(sequence), dif, mirna, iso.format()))\n        return None\n\n    end = start + (len(sequence) - len(iso.add)) - 1\n    dif = abs(mirna[1] - end)\n    if iso.add:\n        sequence = sequence[:-len(iso.add)]\n    # if dif > 3:\n    #    return None\n    if end > mirna[1]:\n        iso.t3 = sequence[-dif:].upper()\n    elif end < mirna[1]:\n        iso.t3 = precursor[mirna[1] - dif:mirna[1]].lower()\n    elif end == mirna[1]:\n        iso.t3 = \"NA\"\n    if dif > 4:\n        logger.debug(\"end > 3 %s %s %s %s %s\" % (len(sequence), end, dif, mirna, iso.format()))\n        return None\n    logger.debug(\"%s %s %s %s %s %s\" % (start, len(sequence), end, dif, mirna, iso.format()))\n    return True", "response": "Return a new ISO record for the given mirna."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nannotate the sequence with the MIRNA coordinates.", "response": "def _annotate(reads, mirbase_ref, precursors):\n    \"\"\"\n    Using SAM/BAM coordinates, mismatches and realign to annotate isomiRs\n    \"\"\"\n    for r in reads:\n        for p in reads[r].precursors:\n            start = reads[r].precursors[p].start + 1  # convert to 1base\n            end = start + len(reads[r].sequence)\n            for mature in mirbase_ref[p]:\n                mi = mirbase_ref[p][mature]\n                is_iso = _coord(reads[r].sequence, start, mi, precursors[p], reads[r].precursors[p])\n                logger.debug((\"{r} {p} {start} {is_iso} {mature} {mi} {mature_s}\").format(s=reads[r].sequence, mature_s=precursors[p][mi[0]-1:mi[1]], **locals()))\n                if is_iso:\n                    reads[r].precursors[p].mirna = mature\n                    break\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove all hits that are not in the best match.", "response": "def _clean_hits(reads):\n    \"\"\"\n    Select only best matches\n    \"\"\"\n    new_reads = defaultdict(realign)\n    for r in reads:\n        world = {}\n        sc = 0\n        for p in reads[r].precursors:\n            world[p] = reads[r].precursors[p].get_score(len(reads[r].sequence))\n            if sc < world[p]:\n                sc = world[p]\n        new_reads[r] = reads[r]\n        for p in world:\n            logger.debug(\"score %s %s %s\" % (r, p, world[p]))\n            if sc != world[p]:\n                logger.debug(\"remove %s %s %s\" % (r, p, world[p]))\n                new_reads[r].remove_precursor(p)\n\n    return new_reads"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread bam file and perform realignment of hits", "response": "def _read_bam(bam_fn, precursors):\n    \"\"\"\n    read bam file and perform realignment of hits\n    \"\"\"\n    mode = \"r\" if bam_fn.endswith(\"sam\") else \"rb\"\n    handle = pysam.Samfile(bam_fn, mode)\n    reads = defaultdict(realign)\n    for line in handle:\n        chrom = handle.getrname(line.reference_id)\n        # print(\"%s %s %s %s\" % (line.query_name, line.reference_start, line.query_sequence, chrom))\n        query_name = line.query_name\n        if query_name not in reads:\n            reads[query_name].sequence = line.query_sequence\n        iso = isomir()\n        iso.align = line\n        iso.start = line.reference_start\n        iso.subs, iso.add = _realign(reads[query_name].sequence, precursors[chrom], line.reference_start)\n        reads[query_name].set_precursor(chrom, iso)\n\n    reads = _clean_hits(reads)\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _collapse_fastq(in_fn):\n    args = argparse.Namespace()\n    args.fastq = in_fn\n    args.minimum = 1\n    args.out = op.dirname(in_fn)\n    return collapse_fastq(args)", "response": "collapse reads into unique sequences"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading pyMatch file and perform realignment of hits", "response": "def _read_pyMatch(fn, precursors):\n    \"\"\"\n    read pyMatch file and perform realignment of hits\n    \"\"\"\n    with open(fn) as handle:\n        reads = defaultdict(realign)\n        for line in handle:\n            query_name, seq, chrom, reference_start, end, mism, add = line.split()\n            reference_start = int(reference_start)\n            # chrom = handle.getrname(cols[1])\n            # print(\"%s %s %s %s\" % (line.query_name, line.reference_start, line.query_sequence, chrom))\n            if query_name not in reads:\n                reads[query_name].sequence = seq\n            iso = isomir()\n            iso.align = line\n            iso.start = reference_start\n            iso.subs, iso.add = _realign(reads[query_name].sequence, precursors[chrom], reference_start)\n            logger.debug(\"%s %s %s %s %s\" % (query_name, reference_start, chrom, iso.subs, iso.add))\n            if len(iso.subs) > 1:\n                continue\n            reads[query_name].set_precursor(chrom, iso)\n\n        reads = _clean_hits(reads)\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_mut(subs):\n    if subs!=\"0\":\n        subs = [[subs.replace(subs[-2:], \"\"),subs[-2], subs[-1]]]\n    return subs", "response": "Parse mutation tag from miraligner output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread ouput of miraligner and create compatible output.", "response": "def _read_miraligner(fn):\n    \"\"\"Read ouput of miraligner and create compatible output.\"\"\"\n    reads = defaultdict(realign)\n    with open(fn) as in_handle:\n        in_handle.next()\n        for line in in_handle:\n            cols = line.strip().split(\"\\t\")\n            iso = isomir()\n            query_name, seq = cols[1], cols[0]\n            chrom, reference_start = cols[-2], cols[3]\n            iso.mirna = cols[3]\n            subs, add, iso.t5, iso.t3 = cols[6:10]\n            if query_name not in reads:\n                reads[query_name].sequence = seq\n            iso.align = line\n            iso.start = reference_start\n            iso.subs, iso.add = _parse_mut(subs), add\n            logger.debug(\"%s %s %s %s %s\" % (query_name, reference_start, chrom, iso.subs, iso.add))\n            reads[query_name].set_precursor(chrom, iso)\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cmd_miraligner(fn, out_file, species, hairpin, out):\n    tool = _get_miraligner()\n    path_db = op.dirname(op.abspath(hairpin))\n    cmd = \"{tool} -freq -i {fn} -o {out_file} -s {species} -db {path_db} -sub 1 -trim 3 -add 3\"\n    if not file_exists(out_file):\n        logger.info(\"Running miraligner with %s\" % fn)\n        do.run(cmd.format(**locals()), \"miraligner with %s\" % fn)\n        shutil.move(out_file + \".mirna\", out_file)\n    return out_file", "response": "Run miraligner for miRNA annotation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _mirtop(out_files, hairpin, gff3, species, out):\n    args = argparse.Namespace()\n    args.hairpin = hairpin\n    args.sps = species\n    args.gtf = gff3\n    args.add_extra = True\n    args.files = out_files\n    args.format = \"seqbuster\"\n    args.out_format = \"gff\"\n    args.out = out\n    reader(args)", "response": "Convert miraligner to mirtop format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge(dts):\n    df = pd.concat(dts)\n\n    ma = df.pivot(index='isomir', columns='sample', values='counts')\n    ma_mirna = ma\n    ma = ma.fillna(0)\n    ma_mirna['mirna'] = [m.split(\":\")[0] for m in ma.index.values]\n    ma_mirna = ma_mirna.groupby(['mirna']).sum()\n    ma_mirna = ma_mirna.fillna(0)\n    return ma, ma_mirna", "response": "merge multiple samples in one matrix\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsummarizing results into single files.", "response": "def _create_counts(out_dts, out_dir):\n    \"\"\"Summarize results into single files.\"\"\"\n    ma, ma_mirna = _merge(out_dts)\n    out_ma = op.join(out_dir, \"counts.tsv\")\n    out_ma_mirna = op.join(out_dir, \"counts_mirna.tsv\")\n    ma.to_csv(out_ma, sep=\"\\t\")\n    ma_mirna.to_csv(out_ma_mirna, sep=\"\\t\")\n    return out_ma_mirna, out_ma"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the list of files and return a list of dicts.", "response": "def miraligner(args):\n    \"\"\"\n    Realign BAM hits to miRBAse to get better accuracy and annotation\n    \"\"\"\n    hairpin, mirna = _download_mirbase(args)\n    precursors = _read_precursor(args.hairpin, args.sps)\n    matures = _read_mature(args.mirna, args.sps)\n    gtf = _read_gtf(args.gtf)\n    out_dts = []\n    out_files = []\n    for bam_fn in args.files:\n        sample = op.splitext(op.basename(bam_fn))[0]\n        logger.info(\"Reading %s\" % bam_fn)\n        if bam_fn.endswith(\"bam\") or bam_fn.endswith(\"sam\"):\n            bam_fn = _sam_to_bam(bam_fn)\n            bam_sort_by_n = op.splitext(bam_fn)[0] + \"_sort\"\n            pysam.sort(\"-n\", bam_fn, bam_sort_by_n)\n            reads = _read_bam(bam_sort_by_n + \".bam\", precursors)\n        elif bam_fn.endswith(\"fasta\") or bam_fn.endswith(\"fa\") or \\\n                bam_fn.endswith(\"fastq\"):\n            if args.collapse:\n                bam_fn = _collapse_fastq(bam_fn)\n            out_file = op.join(args.out, sample + \".premirna\")\n            bam_fn = _filter_seqs(bam_fn)\n            if args.miraligner:\n                _cmd_miraligner(bam_fn, out_file, args.sps, args.hairpin, args.out)\n                reads = _read_miraligner(out_file)\n                out_files.append(out_file)\n        else:\n            raise ValueError(\"Format not recognized.\")\n\n        if args.miraligner:\n            _mirtop(out_files, args.hairpin, args.gtf, args.sps, args.out)\n\n        if not args.miraligner:\n            reads = _annotate(reads, matures, precursors)\n\n        out_file = op.join(args.out, sample + \".mirna\")\n        out_file, dt, dt_pre = _tab_output(reads, out_file, sample)\n        try:\n            vcf_file = op.join(args.out, sample + \".vcf\")\n            if not file_exists(vcf_file):\n                # if True:\n                create_vcf(dt_pre, matures, gtf, vcf_file)\n            try:\n                import vcf\n                vcf.Reader(filename=vcf_file)\n            except Exception as e:\n                logger.warning(e.__doc__)\n                logger.warning(e.message)\n        except Exception as e:\n            # traceback.print_exc()\n            logger.warning(e.__doc__)\n            logger.warning(e.message)\n        if isinstance(dt, pd.DataFrame):\n            out_dts.append(dt)\n\n    if out_dts:\n        _create_counts(out_dts, args.out)\n    else:\n        print(\"No files analyzed!\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chdir(new_dir):\n    cur_dir = os.getcwd()\n    _mkdir(new_dir)\n    os.chdir(new_dir)\n    try:\n        yield\n    finally:\n        os.chdir(cur_dir)", "response": "Context manager to temporarily change to a new directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_flavor():\n    target = op.join(\"seqcluster\", \"flavor\")\n    url = \"https://github.com/lpantano/seqcluster.git\"\n    if not os.path.exists(target):\n    #   shutil.rmtree(\"seqcluster\")\n        subprocess.check_call([\"git\", \"clone\",\"-b\", \"flavor\", \"--single-branch\", url])\n    return op.abspath(target)", "response": "Download flavor from github\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall bcbio to the local_install folder", "response": "def _install(path, args):\n    \"\"\"\n    small helper for installation in case outside bcbio\n    \"\"\"\n    try:\n       from bcbio import install as bcb\n    except:\n        raise ImportError(\"It needs bcbio to do the quick installation.\")\n\n    path_flavor = _get_flavor()\n    s = {\"fabricrc_overrides\": {\"system_install\": path,\n                                \"local_install\": os.path.join(path, \"local_install\"),\n                                \"use_sudo\": \"false\",\n                                \"edition\": \"minimal\"}}\n    s = {\"flavor\": path_flavor,\n         # \"target\": \"[brew, conda]\",\n         \"vm_provider\": \"novm\",\n         \"hostname\": \"localhost\",\n         \"fabricrc_overrides\": {\"edition\": \"minimal\",\n                                \"use_sudo\": \"false\",\n                                \"keep_isolated\": \"true\",\n                                \"conda_cmd\": bcb._get_conda_bin(),\n                                \"distribution\": \"__auto__\",\n                                \"dist_name\": \"__auto__\"}}\n\n\n    s[\"actions\"] = [\"install_biolinux\"]\n    s[\"fabricrc_overrides\"][\"system_install\"] = path\n    s[\"fabricrc_overrides\"][\"local_install\"] = os.path.join(path, \"local_install\")\n    cbl = bcb.get_cloudbiolinux(bcb.REMOTES)\n    sys.path.insert(0, cbl[\"dir\"])\n    cbl_deploy = __import__(\"cloudbio.deploy\", fromlist=[\"deploy\"])\n    cbl_deploy.deploy(s)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninstalling required genome data files in place.", "response": "def _install_data(data_dir, path_flavor, args):\n    \"\"\"Upgrade required genome data files in place.\n    \"\"\"\n    try:\n       from bcbio import install as bcb\n    except:\n        raise ImportError(\"It needs bcbio to do the quick installation.\")\n\n    bio_data = op.join(path_flavor, \"../biodata.yaml\")\n    s = {\"flavor\": path_flavor,\n         # \"target\": \"[brew, conda]\",\n         \"vm_provider\": \"novm\",\n         \"hostname\": \"localhost\",\n         \"fabricrc_overrides\": {\"edition\": \"minimal\",\n                                \"use_sudo\": \"false\",\n                                \"keep_isolated\": \"true\",\n                                \"conda_cmd\": bcb._get_conda_bin(),\n                                \"distribution\": \"__auto__\",\n                                \"dist_name\": \"__auto__\"}}\n    s[\"actions\"] = [\"setup_biodata\"]\n    s[\"fabricrc_overrides\"][\"data_files\"] = data_dir\n    s[\"fabricrc_overrides\"][\"galaxy_home\"] = os.path.join(data_dir, \"galaxy\")\n    cbl = bcb.get_cloudbiolinux(bcb.REMOTES)\n    s[\"genomes\"] = _get_biodata(bio_data, args)\n    sys.path.insert(0, cbl[\"dir\"])\n    cbl_deploy = __import__(\"cloudbio.deploy\", fromlist=[\"deploy\"])\n    cbl_deploy.deploy(s)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predictions(args):\n\n    logger.info(args)\n    logger.info(\"reading sequeces\")\n    out_file = os.path.abspath(os.path.splitext(args.json)[0] + \"_prediction.json\")\n    data = load_data(args.json)\n    out_dir = os.path.abspath(safe_dirs(os.path.join(args.out, \"predictions\")))\n\n    logger.info(\"make predictions\")\n    data = is_tRNA(data, out_dir, args)\n\n    if args.coral:\n        logger.info(\"make CoRaL predictions\")\n        run_coral(data, out_dir, args)\n    write_data(data[0], out_file)\n    logger.info(\"Done\")", "response": "Create predictions of clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts loci according to number of sequences mapped there.", "response": "def sort_precursor(c, loci):\n    \"\"\"\n    Sort loci according to number of sequences mapped there.\n    \"\"\"\n    # Original Py 2.7 code\n    #data_loci = map(lambda (x): [x, loci[x].chr, int(loci[x].start), int(loci[x].end), loci[x].strand, len(c.loci2seq[x])], c.loci2seq.keys())\n    # 2to3 suggested Py 3 rewrite\n    data_loci = [[x, loci[x].chr, int(loci[x].start), int(loci[x].end), loci[x].strand, len(c.loci2seq[x])] for x in list(c.loci2seq.keys())]\n    data_loci = sorted(data_loci, key=itemgetter(5), reverse=True)\n    return data_loci"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects the best precursor asuming size around 100 nt", "response": "def best_precursor(clus, loci):\n    \"\"\"\n    Select best precursor asuming size around 100 nt\n    \"\"\"\n    data_loci = sort_precursor(clus, loci)\n    current_size = data_loci[0][5]\n    best = 0\n    for item, locus in enumerate(data_loci):\n        if locus[3] - locus[2] > 70:\n            if locus[5] > current_size * 0.8:\n                best = item\n                break\n    best_loci = data_loci[best]\n    del data_loci[best]\n    data_loci.insert(0, best_loci)\n    return data_loci"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse bedtools to intersect coordinates", "response": "def select_snps(mirna, snp, out):\n    \"\"\"\n    Use bedtools to intersect coordinates\n    \"\"\"\n    with open(out, 'w') as out_handle:\n        print(_create_header(mirna, snp, out), file=out_handle, end=\"\")\n        snp_in_mirna = pybedtools.BedTool(snp).intersect(pybedtools.BedTool(mirna), wo=True)\n        for single in snp_in_mirna:\n            if single[10] == \"miRNA\" and len(single[3]) + len(single[4]) == 2:\n                line = []\n                rel_p = _lift_positions(single)\n                line.append(_get_mirna_name(single[16]))\n                line.append(str(rel_p))\n                line.append(single[2])\n                line.append(_complement(single[3], single[14]))\n                line.append(_complement(single[4], single[14]))\n                line.append(single[5])\n                line.append(single[6])\n                line.append(single[7])\n                print(\"\\t\".join(line), file=out_handle, end=\"\")\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef up_threshold(x, s, p):\n    if 1.0 * x/s >= p:\n        return True\n    elif stat.binom_test(x, s, p) > 0.01:\n        return True\n    return False", "response": "function to decide if similarity is below cutoff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the region inside the vector with more expression", "response": "def _scan(positions):\n    \"\"\"get the region inside the vector with more expression\"\"\"\n    scores = []\n    for start in range(0, len(positions) - 17, 5):\n        end = start = 17\n        scores.add(_enrichment(positions[start:end], positions[:start], positions[end:]))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate clusters of sequences in a single file.", "response": "def cluster(args):\n    \"\"\"\n    Creating clusters\n    \"\"\"\n\n    args = _check_args(args)\n    read_stats_file = op.join(args.dir_out, \"read_stats.tsv\")\n    if file_exists(read_stats_file):\n        os.remove(read_stats_file)\n\n    bam_file, seq_obj = _clean_alignment(args)\n\n    logger.info(\"Parsing matrix file\")\n    seqL, y, l = parse_ma_file(seq_obj, args.ffile)\n    # y, l = _total_counts(seqL.keys(), seqL)\n    logger.info(\"counts after: %s\" % sum(y.values()))\n    logger.info(\"# sequences after: %s\" % l)\n    dt = pd.DataFrame({'sample': y.keys(), 'counts': y.values()})\n    dt['step'] = 'aligned'\n    dt.to_csv(read_stats_file, sep=\"\\t\", index=False, header=False, mode='a')\n\n    if len(seqL.keys()) < 10:\n        logger.error(\"It seems you have low coverage. Please check your fastq files have enough sequences.\")\n        raise ValueError(\"So few sequences.\")\n\n    logger.info(\"Cleaning bam file\")\n    y, l = _total_counts(seqL.keys(), seqL)\n    logger.info(\"counts after: %s\" % sum(y.values()))\n    logger.info(\"# sequences after: %s\" % l)\n    dt = pd.DataFrame({'sample': y.keys(), 'counts': y.values()})\n    dt['step'] = 'cleaned'\n    dt.to_csv(read_stats_file, sep=\"\\t\", index=False, header=False, mode='a')\n\n    clusL = _create_clusters(seqL, bam_file, args)\n    y, l = _total_counts(clusL.seq.keys(), clusL.seq, aligned=True)\n    logger.info(\"counts after: %s\" % sum(y.values()))\n    logger.info(\"# sequences after: %s\" % l)\n    dt = pd.DataFrame({'sample': y.keys(), 'counts': y.values()})\n    dt['step'] = 'clusters'\n    dt.to_csv(read_stats_file, sep=\"\\t\", index=False, header=False, mode='a')\n\n    logger.info(\"Solving multi-mapping events in the network of clusters\")\n    clusLred = _cleaning(clusL, args.dir_out)\n    y, l = _total_counts(clusLred.clus, seqL)\n    logger.info(\"counts after: %s\" % sum(y.values()))\n    logger.info(\"# sequences after: %s\" % l)\n    dt = pd.DataFrame({'sample': y.keys(), 'counts': y.values()})\n    dt['step'] = 'meta-cluster'\n    dt.to_csv(read_stats_file, sep=\"\\t\", index=False, header=False, mode='a')\n    logger.info(\"Clusters up to %s\" % (len(clusLred.clus.keys())))\n\n    if args.show:\n        logger.info(\"Creating sequences alignment to precursor\")\n        clusLred = show_seq(clusLred, args.index)\n\n    clusLred = peak_calling(clusLred)\n\n    clusLred = _annotate(args, clusLred)\n\n    logger.info(\"Creating json and count matrix\")\n    json_file = _create_json(clusLred, args)\n\n    logger.info(\"Output file in: %s\" % args.dir_out)\n\n    if args.db:\n        name = args.db + \".db\"\n        logger.info(\"Create database: database/\" + name)\n        data = load_data(json_file)\n        out_dir = op.join(args.dir_out, \"database\")\n        make_database(data, name, out_dir)\n    logger.info(\"Finished\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_args(args):\n    logger.info(\"Checking parameters and files\")\n    args.dir_out = args.out\n    args.samplename = \"pro\"\n    global decision_cluster\n    global similar\n    if not os.path.isdir(args.out):\n        logger.warning(\"the output folder doens't exists\")\n        os.mkdirs(args.out)\n    if args.bed and args.gtf:\n        logger.error(\"cannot provide -b and -g at the same time\")\n        raise SyntaxError\n    if args.debug:\n        logger.info(\"DEBUG messages will be showed in file.\")\n    if args.bed:\n        args.list_files = args.bed\n        args.type_ann = \"bed\"\n    if args.gtf:\n        args.list_files = args.gtf\n        args.type_ann = \"gtf\"\n    logger.info(\"Output dir will be: %s\" % args.dir_out)\n    if not all([file_exists(args.ffile), file_exists(args.afile)]):\n        logger.error(\"I/O error: Seqs.ma or Seqs.bam. \")\n        raise IOError(\"Seqs.ma or/and Seqs.bam doesn't exists.\")\n    if hasattr(args, 'list_files'):\n        beds = args.list_files.split(\",\")\n        for filebed in beds:\n            if not file_exists(filebed):\n                logger.error(\"I/O error: {0}\".format(filebed))\n                raise IOError(\"%s  annotation files doesn't exist\" % filebed)\n    param.decision_cluster = args.method\n    if args.similar:\n        param.similar = float(args.similar)\n    if args.min_seqs:\n        param.min_seqs = int(args.min_seqs)\n    return args", "response": "check arguments before starting analysis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncount total seqs after each step", "response": "def _total_counts(seqs, seqL, aligned=False):\n    \"\"\"\n    Counts total seqs after each step\n    \"\"\"\n    total = Counter()\n    if isinstance(seqs, list):\n        if not aligned:\n            l = len([total.update(seqL[s].freq) for s in seqs])\n        else:\n            l = len([total.update(seqL[s].freq) for s in seqs if seqL[s].align > 0])\n    elif isinstance(seqs, dict):\n        [total.update(seqs[s].get_freq(seqL)) for s in seqs]\n        l = sum(len(seqs[s].idmembers) for s in seqs)\n    return total, l"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_annotation(c, loci):\n    data_ann_temp = {}\n    data_ann = []\n    counts = Counter()\n    for lid in c.loci2seq:\n        # original Py 2.7 code\n        #for dbi in loci[lid].db_ann.keys():\n        #    data_ann_temp[dbi] = {dbi: map(lambda (x): loci[lid].db_ann[dbi].ann[x].name, loci[lid].db_ann[dbi].ann.keys())}\n        # suggestion by 2to3\n        for dbi in list(loci[lid].db_ann.keys()):\n            data_ann_temp[dbi] = {dbi: [loci[lid].db_ann[dbi].ann[x].name for x in list(loci[lid].db_ann[dbi].ann.keys())]}\n            logger.debug(\"_json_: data_ann_temp %s %s\" % (dbi, data_ann_temp[dbi]))\n            counts[dbi] += 1\n        # original Py 2.7 code\n        #data_ann = data_ann + map(lambda (x): data_ann_temp[x], data_ann_temp.keys())\n        # suggestion by 2to3\n        data_ann = data_ann + [data_ann_temp[x] for x in list(data_ann_temp.keys())]\n        logger.debug(\"_json_: data_ann %s\" % data_ann)\n    counts = {k: v for k, v in counts.iteritems()}\n    total_loci = sum([counts[db] for db in counts])\n    valid_ann = [k for k, v in counts.iteritems() if up_threshold(v, total_loci, 0.7)]\n    return data_ann, valid_ann", "response": "get annotation of transcriptional units"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsum the metacluster by samples.", "response": "def _sum_by_samples(seqs_freq, samples_order):\n    \"\"\"\n    Sum sequences of a metacluster by samples.\n    \"\"\"\n    n = len(seqs_freq[seqs_freq.keys()[0]].freq.keys())\n    y = np.array([0] * n)\n    for s in seqs_freq:\n        x = seqs_freq[s].freq\n        exp = [seqs_freq[s].freq[sam] for sam in samples_order]\n        y = list(np.array(exp) + y)\n    return y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _annotate(args, setclus):\n    logger.info(\"Creating bed file\")\n    bedfile = generate_position_bed(setclus)\n    a = pybedtools.BedTool(bedfile, from_string=True)\n    beds = []\n    logger.info(\"Annotating clusters\")\n    if hasattr(args, 'list_files'):\n        beds = args.list_files.split(\",\")\n        for filebed in beds:\n            logger.info(\"Using %s \" % filebed)\n            db = os.path.basename(filebed)\n            b = pybedtools.BedTool(filebed)\n            c = a.intersect(b, wo=True)\n            setclus = anncluster(c, setclus, db, args.type_ann, args.feature_id)\n    return setclus", "response": "annotate transcriptional units with\n    gtf file provided by - b option"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning alignment and detect complexity.", "response": "def _clean_alignment(args):\n    \"\"\"\n    Prepare alignment for cluster detection.\n    \"\"\"\n    logger.info(\"Clean bam file with highly repetitive reads with low counts. sum(counts)/n_hits > 1%\")\n    bam_file, seq_obj = clean_bam_file(args.afile, args.mask)\n    logger.info(\"Using %s file\" % bam_file)\n    detect_complexity(bam_file, args.ref, args.out)\n    return bam_file, seq_obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_clusters(seqL, bam_file, args):\n    clus_obj = []\n    cluster_file = op.join(args.out, \"cluster.bed\")\n    if not os.path.exists(op.join(args.out, 'list_obj.pk')):\n        if not file_exists(cluster_file):\n            logger.info(\"Parsing aligned file\")\n            logger.info(\"Merging sequences\")\n            bedtools = os.path.join(os.path.dirname(sys.executable), \"bedtools\")\n            bedtools = bedtools if os.path.exists(bedtools) else \"bedtools\"\n            parse_cmd = \"awk '{i=i+1;print $1\\\"\\\\t\\\"$2\\\"\\\\t\\\"$3\\\"\\\\t\\\"$4\\\"\\\\t\\\"i\\\"\\\\t\\\"$6}'\"\n            cmd = \"{bedtools} bamtobed -i {bam_file} | {parse_cmd} | {bedtools} cluster -s -d 20 -i - > {cluster_file}\"\n            do.run(cmd.format(**locals()))\n        c = pybedtools.BedTool(cluster_file)\n        logger.info(\"Creating clusters\")\n        clus_obj = detect_clusters(c, seqL, args.min_seqs, args.non_un_gl)\n        with open(op.join(args.out, 'list_obj.pk'), 'wb') as output:\n            pickle.dump(clus_obj, output, pickle.HIGHEST_PROTOCOL)\n    else:\n        logger.info(\"Loading previous clusters\")\n        with open(op.join(args.out, 'list_obj.pk'), 'rb') as input:\n            clus_obj = pickle.load(input)\n    # bedfile = pybedtools.BedTool(generate_position_bed(clus_obj), from_string=True)\n    # seqs_2_loci = bedfile.intersect(pybedtools.BedTool(aligned_bed, from_string=True), wo=True, s=True)\n    # seqs_2_position = add_seqs_position_to_loci(seqs_2_loci, seqL)\n    logger.info(\"%s clusters found\" % (len(clus_obj.clusid)))\n    return clus_obj", "response": "Create clusters for the given sequence L and bam file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload saved cluster and jump to next step", "response": "def _cleaning(clusL, path):\n    \"\"\"\n    Load saved cluster and jump to next step\n    \"\"\"\n    backup = op.join(path, \"list_obj_red.pk\")\n    if not op.exists(backup):\n        clus_obj = reduceloci(clusL, path)\n        with open(backup, 'wb') as output:\n            pickle.dump(clus_obj, output, pickle.HIGHEST_PROTOCOL)\n        return clus_obj\n    else:\n        logger.info(\"Loading previous reduced clusters\")\n        with open(backup, 'rb') as in_handle:\n            clus_obj = pickle.load(in_handle)\n        return clus_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef explore(args):\n    logger.info(\"reading sequeces\")\n    data = load_data(args.json)\n    logger.info(\"get sequences from json\")\n    #get_sequences_from_cluster()\n    c1, c2 = args.names.split(\",\")\n    seqs, names = get_sequences_from_cluster(c1, c2, data[0])\n    loci = get_precursors_from_cluster(c1, c2, data[0])\n    logger.info(\"map all sequences to all loci\")\n    print(\"%s\" % (loci))\n    map_to_precursors(seqs, names, loci, os.path.join(args.out, \"map.tsv\"), args)\n    #map_sequences_w_bowtie(sequences, precursors)\n    logger.info(\"plot sequences on loci\")\n    #get_matrix_position()\n    #plot_sequences()\n    logger.info(\"Done\")", "response": "Create mapping of sequences of two clusters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare(args):\n    try:\n        f = open(args.config, 'r')\n        seq_out = open(op.join(args.out, \"seqs.fastq\"), 'w')\n        ma_out = open(op.join(args.out, \"seqs.ma\"), 'w')\n    except IOError as e:\n        traceback.print_exc()\n        raise IOError(\"Can not create output files: %s, %s or read %s\" % (op.join(args.out, \"seqs.ma\"), op.join(args.out, \"seqs.fastq\"), args.config))\n    logger.info(\"Reading sequeces\")\n    seq_l, sample_l = _read_fastq_files(f, args)\n    logger.info(\"Creating matrix with unique sequences\")\n    logger.info(\"Filtering: min counts %s, min size %s, max size %s, min shared %s\" % (args.minc, args.minl, args.maxl, args.min_shared))\n    _create_matrix_uniq_seq(sample_l, seq_l, ma_out, seq_out, args.min_shared)\n    logger.info(\"Finish preprocessing. Get a sorted BAM file of seqs.fa and run seqcluster cluster.\")", "response": "Read all seq. fa files and create a matrix and unique fasta files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_fasta_files(f, args):\n    seq_l = {}\n    sample_l = []\n    idx = 1\n    for line1 in f:\n        line1 = line1.strip()\n        cols = line1.split(\"\\t\")\n        with open(cols[0], 'r') as fasta:\n            sample_l.append(cols[1])\n            for line in fasta:\n                if line.startswith(\">\"):\n                    idx += 1\n                    counts = int(re.search(\"x([0-9]+)\", line.strip()).group(1))\n                else:\n                    seq = line.strip()\n                    seq = seq[0:int(args.maxl)] if len(seq) > int(args.maxl) else seq\n                    if counts > int(args.minc) and len(seq) > int(args.minl):\n                        if seq not in seq_l:\n                            seq_l[seq] = sequence_unique(idx, seq)\n                        seq_l[seq].add_exp(cols[1], counts)\n    return seq_l, sample_l", "response": "read fasta files of each sample and generate a seq_obj for each unique sequence in each sample"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _read_fastq_files(f, args):\n    seq_l = {}\n    sample_l = []\n    idx = 1\n    p = re.compile(\"^[ATCGNU]+$\")\n    with open(op.join(args.out, \"stats_prepare.tsv\"), 'w') as out_handle:\n        for line1 in f:\n            line1 = line1.strip()\n            cols = line1.split(\"\\t\")\n            # if not is_fastq(cols[0]):\n            #    raise ValueError(\"file is not fastq: %s\" % cols[0])\n            with open_fastq(cols[0]) as handle:\n                sample_l.append(cols[1])\n                total = added = 0\n                for line in handle:\n                    if line.startswith(\"@\") or line.startswith(\">\"):\n                        seq = handle.next().strip()\n                        if not p.match(seq):\n                            continue\n                        idx += 1\n                        total += 1\n                        keep = {}\n                        counts = int(re.search(\"x([0-9]+)\", line.strip()).group(1))\n                        if is_fastq(cols[0]):\n                            handle.next().strip()\n                            qual = handle.next().strip()\n                        else:\n                            qual = \"I\" * len(seq)\n                        qual = qual[0:int(args.maxl)] if len(qual) > int(args.maxl) else qual\n                        seq = seq[0:int(args.maxl)] if len(seq) > int(args.maxl) else seq\n                        if counts > int(args.minc) and len(seq) > int(args.minl):\n                            added += 1\n                            if seq in keep:\n                                keep[seq].update(qual)\n                            else:\n                                keep[seq] = quality(qual)\n                            if seq not in seq_l:\n                                seq_l[seq] = sequence_unique(idx, seq)\n                            seq_l[seq].add_exp(cols[1], counts)\n                            seq_l[seq].quality = keep[seq].get()\n                print(\"total\\t%s\\t%s\" % (idx, cols[1]), file=out_handle, end=\"\")\n                print(\"added\\t%s\\t%s\" % (len(seq_l), cols[1]), file=out_handle, end=\"\")\n                logger.info(\"%s: Total read %s ; Total added %s\" % (cols[1], idx, len(seq_l)))\n    return seq_l, sample_l", "response": "read fasta files of each sample and generate a seq_obj for each unique sequence in each sample"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_matrix_uniq_seq(sample_l, seq_l, maout, out, min_shared):\n    skip = 0\n    if int(min_shared) > len(sample_l):\n        min_shared = len(sample_l)\n    maout.write(\"id\\tseq\")\n    for g in sample_l:\n        maout.write(\"\\t%s\" % g)\n    for s in seq_l.keys():\n        seen = sum([1 for g in seq_l[s].group if seq_l[s].group[g] > 0])\n        if seen < int(min_shared):\n            skip += 1\n            continue\n        maout.write(\"\\nseq_%s\\t%s\" % (seq_l[s].idx, seq_l[s].seq))\n        for g in sample_l:\n            if g in seq_l[s].group:\n                maout.write(\"\\t%s\" % seq_l[s].group[g])\n            else:\n                maout.write(\"\\t0\")\n        qual = \"\".join(seq_l[s].quality)\n        out.write(\"@seq_%s\\n%s\\n+\\n%s\\n\" % (seq_l[s].idx, seq_l[s].seq, qual))\n    out.close()\n    maout.close()\n    logger.info(\"Total skipped due to --min-shared parameter (%s) : %s\" % (min_shared, skip))", "response": "create matrix counts for each different sequence in all the fasta files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun some CoRaL modules to predict small RNA", "response": "def run_coral(clus_obj, out_dir, args):\n    \"\"\"\n    Run some CoRaL modules to predict small RNA function\n    \"\"\"\n    if not args.bed:\n        raise ValueError(\"This module needs the bed file output from cluster subcmd.\")\n    workdir = op.abspath(op.join(args.out, 'coral'))\n    safe_dirs(workdir)\n    bam_in = op.abspath(args.bam)\n    bed_in = op.abspath(args.bed)\n    reference = op.abspath(args.ref)\n    with chdir(workdir):\n        bam_clean = coral.prepare_bam(bam_in, bed_in)\n        out_dir = op.join(workdir, \"regions\")\n        safe_dirs(out_dir)\n        prefix = \"seqcluster\"\n        loci_file = coral.detect_regions(bam_clean, bed_in, out_dir, prefix)\n        coral.create_features(bam_clean, loci_file, reference, out_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_tRNA(clus_obj, out_dir, args):\n    ref = os.path.abspath(args.reference)\n    utils.safe_dirs(out_dir)\n    for nc in clus_obj[0]:\n        c = clus_obj[0][nc]\n        loci = c['loci']\n        out_fa = \"cluster_\" + nc\n        if loci[0][3] - loci[0][2] < 500:\n            with make_temp_directory() as tmpdir:\n                os.chdir(tmpdir)\n                get_loci_fasta({loci[0][0]: [loci[0][0:5]]}, out_fa, ref)\n                summary_file, str_file = _run_tRNA_scan(out_fa)\n                if \"predictions\" not in c:\n                    c['predictions'] = {}\n                c['predictions']['tRNA'] = _read_tRNA_scan(summary_file)\n                score = _read_tRNA_scan(summary_file)\n                logger.debug(score)\n                shutil.move(summary_file, op.join(out_dir, summary_file))\n                shutil.move(str_file, op.join(out_dir, str_file))\n        else:\n            c['errors'].add(\"precursor too long\")\n        clus_obj[0][nc] = c\n\n    return clus_obj", "response": "Checks if each cluster precursors have a tRNA type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_tRNA_scan(summary_file):\n    score = 0\n    if os.path.getsize(summary_file) == 0:\n        return 0\n    with open(summary_file) as in_handle:\n        # header = in_handle.next().strip().split()\n        for line in in_handle:\n            if not line.startswith(\"--\"):\n                pre = line.strip().split()\n                score = pre[-1]\n    return score", "response": "Parse output from tRNA_Scan\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _run_tRNA_scan(fasta_file):\n    out_file = fasta_file + \"_trnascan\"\n    se_file = fasta_file + \"_second_str\"\n    cmd = \"tRNAscan-SE -q -o {out_file} -f {se_file} {fasta_file}\"\n    run(cmd.format(**locals()))\n    return out_file, se_file", "response": "Run tRNA - scan - SE to predict tRNA\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_mut(mut):\n    multiplier = 1\n    if mut.startswith(\"-\"):\n        mut = mut[1:]\n        multiplier = -1\n    nt = mut.strip('0123456789')\n    pos = int(mut[:-2]) * multiplier\n    return nt, pos", "response": "Parse mutation field to get position and nts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_reference_position(isomir):\n    mut = isomir.split(\":\")[1]\n    if mut == \"0\":\n        return mut\n    nt, pos = _parse_mut(mut)\n    trim5 = isomir.split(\":\")[-2]\n    off = -1 * len(trim5)\n    if trim5.islower():\n        off = len(trim5)\n    if trim5 == \"NA\" or trim5 == \"0\":\n        off = 0\n    # print(isomir)\n    # print([mut, pos, off, nt])\n    return \"%s%s\" % (pos + off, nt)", "response": "Get the reference position of the isomir."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget pct of variants respect to the reference using reads and different sequences", "response": "def _get_pct(isomirs, mirna):\n    \"\"\"\n    Get pct of variants respect to the reference\n    using reads and different sequences\n    \"\"\"\n    pass_pos = []\n    for isomir in isomirs.iterrows():\n        mir = isomir[1][\"chrom\"]\n        mut = isomir[1][\"sv\"]\n        mut_counts = isomir[1][\"counts\"]\n        total = mirna.loc[mir, \"counts\"] * 1.0 - mut_counts\n        mut_diff = isomir[1][\"diff\"]\n        ratio = mut_counts / total\n        if mut_counts > 10 and ratio  > 0.4 and mut != \"0\" and mut_diff > 1:\n            isomir[1][\"ratio\"] = ratio\n            pass_pos.append(isomir[1])\n    return pass_pos"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _print_header(data):\n    print(\"##fileformat=VCFv4.2\", file=STDOUT, end=\"\")\n    print(\"##source=seqbuster2.3\", file=STDOUT, end=\"\")\n    print(\"##reference=mirbase\", file=STDOUT, end=\"\")\n    for pos in data:\n        print(\"##contig=<ID=%s>\" % pos[\"chrom\"], file=STDOUT, end=\"\")\n    print('##INFO=<ID=ID,Number=1,Type=String,Description=\"miRNA name\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=GT,Number=1,Type=Integer,Description=\"Genotype\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=NR,Number=A,Type=Integer,Description=\"Total reads supporting the variant\">', file=STDOUT, end=\"\")\n    print('##FORMAT=<ID=NS,Number=A,Type=Float,Description=\"Total number of different sequences supporting the variant\">', file=STDOUT, end=\"\")\n    print(\"#CHROM\\tPOS\\tID\\tREF\\tALT\\tQUAL\\tFILTER\\tINFO\\tFORMAT\\tSAMP001\", file=STDOUT, end=\"\")", "response": "Print the header of the sequence buster2. 3 vcf file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting vcf line following rules.", "response": "def print_vcf(data):\n    \"\"\"Print vcf line following rules.\"\"\"\n    id_name = \".\"\n    qual = \".\"\n    chrom = data['chrom']\n    pos = data['pre_pos']\n    nt_ref = data['nt'][1]\n    nt_snp = data['nt'][0]\n    flt = \"PASS\"\n    info = \"ID=%s\" % data['mature']\n    frmt = \"GT:NR:NS\"\n    gntp = \"%s:%s:%s\" % (_genotype(data), data[\"counts\"], data[\"diff\"])\n    print(\"\\t\".join(map(str, [chrom, pos, id_name, nt_ref, nt_snp, qual, flt, info, frmt, gntp])), file=STDOUT, end=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking position at precursor scale", "response": "def liftover(pass_pos, matures):\n    \"\"\"Make position at precursor scale\"\"\"\n    fixed_pos = []\n    _print_header(pass_pos)\n    for pos in pass_pos:\n        mir = pos[\"mature\"]\n        db_pos = matures[pos[\"chrom\"]]\n        mut = _parse_mut(pos[\"sv\"])\n        print([db_pos[mir], mut, pos[\"sv\"]])\n        pos['pre_pos'] = db_pos[mir][0] + mut[1] - 1\n        pos['nt'] = list(mut[0])\n        fixed_pos.append(pos)\n        print_vcf(pos)\n    return fixed_pos"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a vcf file of changes for all samples.", "response": "def create_vcf(isomirs, matures, gtf, vcf_file=None):\n    \"\"\"\n    Create vcf file of changes for all samples.\n    PASS will be ones with > 3 isomiRs supporting the position\n         and > 30% of reads, otherwise LOW\n    \"\"\"\n    global STDOUT\n    isomirs['sv'] = [_get_reference_position(m) for m in isomirs[\"isomir\"]]\n    mirna = isomirs.groupby(['chrom']).sum()\n    sv = isomirs.groupby(['chrom', 'mature', 'sv'], as_index=False).sum()\n    sv[\"diff\"] = isomirs.groupby(['chrom', 'mature', 'sv'], as_index=False).size().reset_index().loc[:,0]\n    pass_pos = _get_pct(sv, mirna)\n    if vcf_file:\n        with open(vcf_file, 'w') as out_handle:\n            STDOUT = out_handle\n            pass_pos = liftover(pass_pos, matures)\n\n    if gtf:\n        vcf_genome_file = vcf_file.replace(\".vcf\", \"_genome.vcf\")\n        with open(vcf_genome_file, 'w') as out_handle:\n            STDOUT = out_handle\n            pass_pos = liftover_to_genome(pass_pos, gtf)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_seqs_from_cluster(seqs, seen):\n    already_in = set()\n    not_in = []\n\n    already_in = map(seen.get, seqs)\n    # if isinstance(already_in, list):\n    already_in = filter(None, already_in)\n    not_in = set(seqs) - set(seen.keys())\n    # for s in seqs:\n    #    if s in seen:\n    #        already_in.add(seen[s])\n    #    else:\n    #        not_in.append(s)\n    return list(set(already_in)), list(not_in)", "response": "Returns the sequences that are already part of the cluster"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreduce number of loci a cluster has", "response": "def reduceloci(clus_obj,  path):\n    \"\"\"reduce number of loci a cluster has\n    :param clus_obj: cluster object object\n    :param path: output path\n    \"\"\"\n    filtered = {}\n    n_cluster = 0\n    large = 0\n    current = clus_obj.clusid\n    logger.info(\"Number of loci: %s\" % len(clus_obj.loci.keys()))\n    bar = ProgressBar(maxval=len(current))\n    bar.start()\n    bar.update(0)\n    for itern, idmc in enumerate(current):\n        bar.update(itern)\n        logger.debug(\"_reduceloci: cluster %s\" % idmc)\n        c = copy.deepcopy(list(current[idmc]))\n\n        n_loci = len(c)\n        if n_loci < 1000:\n            filtered, n_cluster = _iter_loci(c, clus_obj.clus, (clus_obj.loci, clus_obj.seq), filtered, n_cluster)\n        else:\n            large += 1\n            n_cluster += 1\n            _write_cluster(c, clus_obj.clus, clus_obj.loci, n_cluster, path)\n            filtered[n_cluster] = _add_complete_cluster(n_cluster, c, clus_obj.clus)\n    clus_obj.clus = filtered\n\n    seqs = 0\n    for idc in filtered:\n        seqs += len(filtered[idc].idmembers)\n    logger.info(\"seqs in clusters %s\" % (seqs))\n\n    logger.info(\"Clusters too long to be analized: %s\" % large)\n    logger.info(\"Number of clusters removed because low number of reads: %s\" % REMOVED)\n    logger.info(\"Number of clusters with conflicts: %s\" % CONFLICT)\n    return clus_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites all the loci for complex meta - clusters to a single file.", "response": "def _write_cluster(metacluster, cluster, loci, idx, path):\n    \"\"\"\n    For complex meta-clusters, write all the loci for further debug\n    \"\"\"\n    out_file = op.join(path, 'log', str(idx) + '.bed')\n    with utils.safe_run(out_file):\n        with open(out_file, 'w') as out_handle:\n            for idc in metacluster:\n                for idl in cluster[idc].loci2seq:\n                    pos = loci[idl].list()\n                    print(\"\\t\".join(pos[:4] + [str(len(cluster[idc].loci2seq[idl]))] + [pos[-1]]), file=out_handle, end=\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _iter_loci(meta, clusters, s2p, filtered, n_cluster):\n    global CONFLICT\n    loci = dict(zip(meta, [clusters[idc] for idc in meta]))\n\n    n_loci = len(meta)\n    n_loci_prev = n_loci + 1\n    cicle = 0\n    # [logger.note(\"BEFORE %s %s %s\" % (c.id, idl, len(c.loci2seq[idl]))) for idl in c.loci2seq]\n    internal_cluster = {}\n    if n_loci == 1:\n        n_cluster += 1\n        filtered[n_cluster] = clusters[meta[0]]\n        filtered[n_cluster].update(id=n_cluster)\n        filtered[n_cluster].set_freq(s2p[1])\n\n    while n_loci < n_loci_prev and n_loci != 1:\n        n_loci_prev = n_loci\n        cicle += 1\n        if (cicle % 1) == 0:\n            logger.debug(\"_iter_loci:number of cicle: %s with n_loci %s\" % (cicle, n_loci))\n        loci_similarity = _calculate_similarity(loci)\n        internal_cluster = _merge_similar(loci, loci_similarity)\n        n_loci = len(internal_cluster)\n        loci = internal_cluster\n        logger.debug(\"_iter_loci: n_loci %s\" % n_loci)\n\n    if n_loci > 1:\n        n_internal_cluster = sorted(internal_cluster.keys(), reverse=True)[0]\n        CONFLICT += 1\n        internal_cluster = _solve_conflict(internal_cluster, s2p, n_internal_cluster)\n\n    internal_cluster = _clean_cluster(internal_cluster)\n\n    for idc in internal_cluster:\n        n_cluster += 1\n        logger.debug(\"_iter_loci: add to filtered %s\" % n_cluster)\n        filtered[n_cluster] = internal_cluster[idc]\n        filtered[n_cluster].id = n_cluster\n        filtered[n_cluster].update(id=n_cluster)\n        filtered[n_cluster].set_freq(s2p[1])\n    logger.debug(\"_iter_loci: filtered %s\" % filtered.keys())\n\n    # for new_c in internal_cluster.values():\n    #    [logger.note(\"%s %s %s %s\" % (meta, new_c.id, idl, len(new_c.loci2seq[idl]))) for idl in new_c.loci2seq]\n    return filtered, n_cluster", "response": "Iterate through all loci and decide if they are part of the same TU or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn 1 cluster per loci", "response": "def _convert_to_clusters(c):\n    \"\"\"Return 1 cluster per loci\"\"\"\n    new_dict = {}\n    n_cluster = 0\n    logger.debug(\"_convert_to_cluster: loci %s\" % c.loci2seq.keys())\n    for idl in c.loci2seq:\n        n_cluster += 1\n        new_c = cluster(n_cluster)\n        #new_c.id_prev = c.id\n        new_c.loci2seq[idl] = c.loci2seq[idl]\n        new_dict[n_cluster] = new_c\n    logger.debug(\"_convert_to_cluster: new ids %s\" % new_dict.keys())\n    return new_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _calculate_similarity(c):\n    ma = {}\n    for idc in c:\n        set1 = _get_seqs(c[idc])\n        [ma.update({(idc, idc2): _common(set1, _get_seqs(c[idc2]), idc, idc2)}) for idc2 in c if idc != idc2 and (idc2, idc) not in ma]\n    # logger.debug(\"_calculate_similarity_ %s\" % ma)\n    return ma", "response": "Calculate a similarity matrix of % of shared sequence\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all sequences in a cluster knowing loci", "response": "def _get_seqs(list_idl):\n    \"\"\"get all sequences in a cluster knowing loci\"\"\"\n    seqs = set()\n    for idl in list_idl.loci2seq:\n        # logger.debug(\"_get_seqs_: loci %s\" % idl)\n        [seqs.add(s) for s in list_idl.loci2seq[idl]]\n    # logger.debug(\"_get_seqs_: %s\" % len(seqs))\n    return seqs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the common percentage of sequences", "response": "def _common(s1, s2, i1, i2):\n    \"\"\"calculate the common % percentage of sequences\"\"\"\n    c = len(set(s1).intersection(s2))\n    t = min(len(s1), len(s2))\n    pct = 1.0 * c / t * t\n    is_gt = up_threshold(pct, t * 1.0, parameters.similar)\n    logger.debug(\"_common: pct %s of clusters:%s %s = %s\" % (1.0 * c / t, i1, i2, is_gt))\n    if pct < parameters.similar and is_gt and pct > 0:\n        pct = parameters.similar\n    return pct / t"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _is_consistent(pairs, common, clus_seen, loci_similarity):\n    all_true1 = all([all([common and loci_similarity[(p, c)] > parameters.similar  for p in pairs if (p, c) in loci_similarity]) for c in clus_seen])\n    all_true2 = all([all([common and loci_similarity[(c, p)] > parameters.similar  for p in pairs if (c, p) in loci_similarity]) for c in clus_seen])\n    return all_true1 * all_true2", "response": "Check if loci shared that match sequences with all\n    clusters seen until now."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _merge_similar(loci, loci_similarity):\n    n_cluster = 0\n    internal_cluster = {}\n    clus_seen = {}\n    loci_sorted = sorted(loci_similarity.iteritems(), key=operator.itemgetter(1), reverse=True)\n    for pairs, sim in loci_sorted:\n        common = sim > parameters.similar\n        n_cluster += 1\n        logger.debug(\"_merge_similar:try new cluster %s\" % n_cluster)\n        new_c = cluster(n_cluster)\n        p_seen, p_unseen = [], []\n        size = min(len(_get_seqs(loci[pairs[0]])), len(_get_seqs(loci[pairs[1]])))\n        if common:\n            consistent = _is_consistent(pairs, common, clus_seen, loci_similarity)\n            logger.debug(\"_merge_similar: clusters seen: %s\" % clus_seen)\n            logger.debug(\"_merge_similar: id %s common %s|%s total  %s consistent %s\" % (pairs, sim, common, size, consistent))\n            if not consistent:\n                continue\n            if pairs[0] in clus_seen:\n                p_seen.append(pairs[0])\n                p_unseen.append(pairs[1])\n            if pairs[1] in clus_seen:\n                p_seen.append(pairs[1])\n                p_unseen.append(pairs[0])\n            if len(p_seen) == 0:\n                new_c = _merge_cluster(loci[pairs[0]], new_c)\n                new_c = _merge_cluster(loci[pairs[1]], new_c)\n                [clus_seen.update({p: n_cluster}) for p in pairs]\n                internal_cluster[n_cluster] = new_c\n            if len(p_seen) == 1:\n                idc_seen = clus_seen[p_seen[0]]\n                internal_cluster[idc_seen] = _merge_cluster(loci[p_unseen[0]], internal_cluster[idc_seen])\n                clus_seen[p_unseen[0]] = idc_seen\n        else:\n            logger.debug(\"_merge_similar: id %s %s  are different\" % pairs)\n            continue\n    internal_cluster.update(_add_unseen(loci, clus_seen, n_cluster))\n    logger.debug(\"_merge_similar: total clus %s\" %\n                 len(internal_cluster.keys()))\n    return internal_cluster", "response": "Internal function to reduce loci complexity"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _merge_cluster(old, new):\n    logger.debug(\"_merge_cluster: %s to %s\" % (old.id, new.id))\n    logger.debug(\"_merge_cluster: add idls %s\" % old.loci2seq.keys())\n    for idl in old.loci2seq:\n        # if idl in new.loci2seq:\n        #    new.loci2seq[idl] = list(set(new.loci2seq[idl] + old.loci2seq[idl]))\n        # new.loci2seq[idl] = old.loci2seq[idl]\n        new.add_id_member(old.loci2seq[idl], idl)\n    return new", "response": "merge one cluster to another"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a new set of cluster names that are not in the same cluster.", "response": "def _solve_conflict(list_c, s2p, n_cluster):\n    \"\"\"\n    Make sure sequences are counts once.\n    Resolve by most-vote or exclussion\n\n    :params list_c: dict of objects cluster\n    :param s2p: dict of [loci].coverage = # num of seqs\n    :param n_cluster: number of clusters\n\n    return dict: new set of clusters\n    \"\"\"\n    logger.debug(\"_solve_conflict: count once\")\n    if parameters.decision_cluster == \"bayes\":\n        return decide_by_bayes(list_c, s2p)\n    loci_similarity = _calculate_similarity(list_c)\n    loci_similarity = sorted(loci_similarity.iteritems(), key=operator.itemgetter(1), reverse=True)\n    common = sum([score for p, score in loci_similarity])\n    while common > 0:\n        n_cluster += 1\n        logger.debug(\"_solve_conflict: ma %s\" % loci_similarity)\n        pairs = loci_similarity[0][0]\n        score = loci_similarity[0][1]\n        logger.debug(\"_solve_conflict: common %s, new %s\" % (score, n_cluster))\n        if parameters.decision_cluster.startswith(\"most-voted\"):\n            list_c = _split_cluster_by_most_vote(list_c, pairs)\n        else:\n            list_c = _split_cluster(list_c, pairs, n_cluster)\n        list_c = {k: v for k, v in list_c.iteritems() if len(v.loci2seq) > 0}\n        loci_similarity = _calculate_similarity(list_c)\n        loci_similarity = sorted(loci_similarity.iteritems(), key=operator.itemgetter(1), reverse=True)\n        #logger.note(\"%s %s\" % (pairs, loci_similarity[0][1]))\n        common = sum([score for p, score in loci_similarity])\n        logger.debug(\"_solve_conflict: solved clusters %s\" % len(list_c.keys()))\n    return list_c"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits cluster by exclussion", "response": "def _split_cluster(c, pairs, n):\n    \"\"\"split cluster by exclussion\"\"\"\n    old = c[p[0]]\n    new = c[p[1]]\n    new_c = cluster(n)\n    common = set(_get_seqs(old)).intersection(_get_seqs(new))\n    for idl in old.loci2seq:\n        in_common = list(set(common).intersection(old.loci2seq[idl]))\n        if len(in_common) > 0:\n            logger.debug(\"_split_cluster: in_common %s with pair 1\" % (len(in_common)))\n            new_c.add_id_member(in_common, idl)\n            old.loci2seq[idl] = list(set(old.loci2seq[idl]) - set(common))\n            logger.debug(\"_split_cluster: len old %s with pair 1\" % (len(old.loci2seq)))\n    for idl in new.loci2seq:\n        in_common = list(set(common).intersection(new.loci2seq[idl]))\n        if len(in_common) > 0:\n            logger.debug(\"_split_cluster: in_common %s with pair 2\" % (len(in_common)))\n            new_c.add_id_member(in_common, idl)\n            new.loci2seq[idl] = list(set(new.loci2seq[idl]) - set(common))\n            logger.debug(\"_split_cluster: len old %s with pair 2\" % (len(new.loci2seq)))\n    old.update()\n    new.update()\n    old.loci2seq = {k: v for k, v in old.loci2seq.iteritems() if len(v) > 0}\n    new.loci2seq = {k: v for k, v in new.loci2seq.iteritems() if len(v) > 0}\n    c[n] = new\n    c[p[0]] = old\n    c[p[1]] = new\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsplit cluster by most - vote strategy", "response": "def _split_cluster_by_most_vote(c, p):\n    \"\"\"split cluster by most-vote strategy\"\"\"\n    old, new = c[p[0]], c[p[1]]\n    old_size = _get_seqs(old)\n    new_size = _get_seqs(new)\n    logger.debug(\"_most_vote: size of %s with %s - %s with %s\" % (old.id, len(old_size), new.id, len(new_size)))\n    if len(old_size) > len(new_size):\n        keep, remove = old, new\n    else:\n        keep, remove = new, old\n    common = list(set(old_size).intersection(new_size))\n    logger.debug(\"_most_vote: keep %s remove  %s with common %s\" % (keep.id, remove.id, len(common)))\n    for idl in remove.loci2seq:\n        if len(common) > 0:\n            remove.loci2seq[idl] = list(set(remove.loci2seq[idl]) - set(common))\n    keep.loci2seq = {k: v for k, v in keep.loci2seq.iteritems() if len(v) > 0}\n    remove.loci2seq = {k: v for k, v in remove.loci2seq.iteritems() if len(v) > 0}\n    keep.update()\n    remove.update()\n    c[keep.id] = keep\n    c[remove.id] = remove\n    return c"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _clean_cluster(list_c):\n    global REMOVED\n    init = len(list_c)\n    list_c = {k: v for k, v in list_c.iteritems() if len(_get_seqs(v)) > parameters.min_seqs}\n    logger.debug(\"_clean_cluster: number of clusters %s \" % len(list_c.keys()))\n    list_c = {k: _select_loci(v) for k, v in list_c.iteritems()}\n    end = len(list_c)\n    REMOVED += init - end\n    return list_c", "response": "Remove clusters with less than 10 sequences and\n    loci with size smaller than 60%"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _select_loci(c):\n    loci_len = {k: len(v) for k, v in c.loci2seq.iteritems()}\n    logger.debug(\"_select_loci: number of loci %s\" % len(c.loci2seq.keys()))\n    loci_len_sort = sorted(loci_len.iteritems(), key=operator.itemgetter(1), reverse=True)\n    max_size = loci_len_sort[0][1]\n    logger.debug(\"_select_loci: max size %s\" % max_size)\n    loci_clean = {locus: c.loci2seq[locus] for locus, size in loci_len_sort if size > 0.8 * max_size}\n    c.loci2seq = loci_clean\n    removed = list(set(c.idmembers.keys()) - set(_get_seqs(c)))\n    c.add_id_member(removed, loci_len_sort[0][0])\n    logger.debug(\"_select_loci: number of loci %s after cleaning\" % len(c.loci2seq.keys()))\n    return c", "response": "Select only loci with most abundant sequences"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _solve_loci_deprecated(c, locilen_sorted, seen_seqs, filtered, maxseq, n_cluster):\n    first_run = 0\n    seen_seqs = list()\n    n_cluster += 1\n    logger.debug(\"_solve_loci:new cluster %s\" % n_cluster)\n    new_c = cluster(n_cluster)\n    for idl, lenl in locilen_sorted:\n        locus_seqs = c.loci2seq[idl]\n        if first_run == 0:\n            seen_seqs = locus_seqs\n            first_run = 1\n            first_idl = idl\n        intersect = list(set(seen_seqs).intersection(locus_seqs))\n        common = 0\n        if intersect:\n            common = len(intersect)*1.0/min(len(seen_seqs), len(locus_seqs))\n        logger.debug(\"_sole_loci:id %s idl %s len %s max %s seen %s inter %s common %s \" % (c.id, idl, lenl, maxseq, len(seen_seqs), len(intersect), common))\n        if common*1.0 >= 0.6:\n            if lenl*1.0 >= 0.6*maxseq:\n                c, new_c, seen_seqs = _merge_loci_in_cluster(c, new_c, idl, seen_seqs)\n            else:\n                c, new_c, seen_seqs = _merge_with_first_loci(c, new_c, first_idl, idl, seen_seqs)\n        else:\n            c = _remove_seqs_from_loci(c, idl, seen_seqs)\n    filtered[n_cluster] = new_c\n    return c, seen_seqs, filtered, n_cluster", "response": "deprecated function to reduce loci complexity"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing annotation to get nice description", "response": "def _get_description(string):\n    \"\"\"\n    Parse annotation to get nice description\n    \"\"\"\n    ann = set()\n    if not string:\n        return \"This cluster is inter-genic.\"\n    for item in string:\n        for db in item:\n            ann = ann.union(set(item[db]))\n    return \"annotated as: %s ...\" % \",\".join(list(ann)[:3])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_format(profile):\n    x = set()\n    for sample in profile:\n        x = x.union(set(profile[sample].keys()))\n    if not x:\n        return ''\n    end, start = max(x), min(x)\n    x = range(start, end, 4)\n    scaled_profile = defaultdict(list)\n    for pos in x:\n        for sample in profile:\n            y = _get_closer(profile[sample], pos)\n            if y:\n                scaled_profile[sample].append(profile[sample][y])\n            else:\n                scaled_profile[sample].append(0)\n    return {'x': list(x), 'y': scaled_profile, 'names': scaled_profile.keys()}", "response": "Prepare dict to list of y values with same x\n   "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _insert_data(con, data):\n    with con:\n        cur = con.cursor()\n        cur.execute(\"DROP TABLE IF EXISTS clusters;\")\n        cur.execute(\"CREATE TABLE clusters(Id INT, Description TEXT, Locus TEXT, Annotation TEXT, Sequences TEXT, Profile TXT, Precursor TXT)\")\n        for c in data[0]:\n            locus = json.dumps(data[0][c]['loci'])\n            annotation = json.dumps(data[0][c]['ann'])\n            description = _get_description(data[0][c]['ann'])\n            sequences = json.dumps(_get_sequences(data[0][c]))\n            keys = data[0][c]['freq'][0].values()[0].keys()\n            profile = \"Not available.\"\n            if 'profile' in data[0][c]:\n                profile = json.dumps(_set_format(data[0][c]['profile']))\n            precursor = json.dumps(data[0][c].get('precursor'))\n            cur.execute(\"INSERT INTO clusters VALUES(%s, '%s', '%s', '%s', '%s', '%s', '%s')\" % (c, description, locus, annotation, sequences, profile, precursor))", "response": "insert data into the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_align_file(file_in):\n    loc_id = 1\n    bedfile_clusters = \"\"\n    bamfile = pybedtools.BedTool(file_in)\n    bed = pybedtools.BedTool.bam_to_bed(bamfile)\n    for c, start, end, name, q, strand in bed:\n        loc_id += 1\n        bedfile_clusters += \"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % \\\n                            (c, start, end, name, loc_id, strand)\n    return bedfile_clusters", "response": "Parse sam files with aligned sequences\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_ma_file(seq_obj, in_file):\n    name = \"\"\n    index = 1\n    total = defaultdict(int)\n    with open(in_file) as handle_in:\n        line = handle_in.readline().strip()\n        cols = line.split(\"\\t\")\n        samples = cols[2:]\n        for line in handle_in:\n            line = line.strip()\n            cols = line.split(\"\\t\")\n            name = int(cols[0].replace(\"seq_\", \"\"))\n            seq = cols[1]\n            exp = {}\n            for i in range(len(samples)):\n                exp[samples[i]] = int(cols[i+2])\n                total[samples[i]] += int(cols[i+2])\n            index = index+1\n            if name in seq_obj:\n                seq_obj[name].set_freq(exp)\n                seq_obj[name].set_seq(seq)\n            # new_s = sequence(seq, exp, index)\n            # seq_l[name] = new_s\n    seq_obj = _normalize_seqs(seq_obj, total)\n    return seq_obj, total, index", "response": "read seqs. ma file and create dict with sequence object"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_gtf_line(cols, field=\"name\"):\n    field = field.lower()\n    try:\n        group = cols[2]\n        attrs = cols[8].split(\";\")\n        name = [attr.strip().split(\" \")[1] for attr in attrs if attr.strip().split(\" \")[0].lower().endswith(field)]\n        if not name:\n            name = [attr.strip().split(\" \")[1] for attr in attrs if attr.strip().split(\" \")[0].lower().endswith(\"gene_id\")]\n        if not name:\n            name = [\"None\"]\n        biotype = [attr.strip().split(\" \")[1] for attr in attrs if attr.strip().split(\" \")[0].lower().endswith(\"biotype\")]\n        if biotype:\n            group = biotype[0]\n        c = cols[0]\n        s = int(cols[3])\n        e = int(cols[4])\n        st = cols[6]\n        return [c, s, e, st, group, name[0]]\n    except(Exception, e):\n        logger.error(cols)\n        logger.error(\"File is not in correct format\")\n        logger.error(\"Expect chr source feature start end . strand attributes\")\n        logger.error(\"Attributes are 'gene_name SNCA; gene_id ENSG; '\")\n        logger.error(\"The 3rd column is used as type of small RNA (like miRNA)\")\n        logger.error(\"at least should contains '; *name NAME; '\")\n        logger.error(e)\n        raise", "response": "parse a gtf line to get class and name information"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _position_in_feature(pos_a, pos_b):\n    strd = \"-\"\n    if pos_a[2] in pos_b[2]:\n        strd = \"+\"\n    if pos_a[2] in \"+\" and pos_b[2] in \"+\":\n        lento5 = pos_a[0] - pos_b[1] + 1\n        lento3 = pos_a[1] - pos_b[1] + 1\n    if pos_a[2] in \"+\" and pos_b[2] in \"-\":\n        lento5 = pos_a[1] - pos_b[0] + 1\n        lento3 = pos_a[0] - pos_b[1] + 1\n    if pos_a[2] in \"-\" and pos_b[2] in \"+\":\n        lento5 = pos_a[0] - pos_b[1] + 1\n        lento3 = pos_a[1] - pos_b[0] + 1\n    if pos_a[2] in \"-\" and pos_b[2] in \"-\":\n        lento3 = pos_a[0] - pos_b[0] + 1\n        lento5 = pos_a[1] - pos_b[1] + 1\n    else:\n        lento5 = pos_a[0] - pos_b[0] + 1\n        lento3 = pos_a[1] - pos_b[1] + 1\n    return lento5, lento3, strd", "response": "return distance to 3 and 5 end of the feature"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nintersect transcription position with annotation files", "response": "def anncluster(c, clus_obj, db, type_ann, feature_id=\"name\"):\n    \"\"\"intersect transcription position with annotation files\"\"\"\n    id_sa, id_ea, id_id, id_idl, id_sta = 1, 2, 3, 4, 5\n    if type_ann == \"bed\":\n        id_sb = 7\n        id_eb = 8\n        id_stb = 11\n        id_tag = 9\n    ida = 0\n    clus_id = clus_obj.clus\n    loci_id = clus_obj.loci\n    db = os.path.splitext(db)[0]\n    logger.debug(\"Type:%s\\n\" % type_ann)\n    for cols in c.features():\n        if type_ann == \"gtf\":\n            cb, sb, eb, stb, db, tag = read_gtf_line(cols[6:], feature_id)\n        else:\n            sb = int(cols[id_sb])\n            eb = int(cols[id_eb])\n            stb = cols[id_stb]\n            tag = cols[id_tag]\n        id = int(cols[id_id])\n        idl = int(cols[id_idl])\n        if (id in clus_id):\n            clus = clus_id[id]\n            sa = int(cols[id_sa])\n            ea = int(cols[id_ea])\n            ida += 1\n            lento5, lento3, strd = _position_in_feature([sa, ea, cols[id_sta]], [sb, eb, stb])\n            if db in loci_id[idl].db_ann:\n                ann = annotation(db, tag, strd, lento5, lento3)\n                tdb = loci_id[idl].db_ann[db]\n                tdb.add_db_ann(ida, ann)\n                loci_id[idl].add_db(db, tdb)\n            else:\n                ann = annotation(db, tag, strd, lento5, lento3)\n                tdb = dbannotation(1)\n                tdb.add_db_ann(ida, ann)\n                loci_id[idl].add_db(db, tdb)\n            clus_id[id] = clus\n    clus_obj.clus = clus_id\n    clus_obj.loci = loci_id\n    return clus_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect_complexity(bam_in, genome, out):\n    if not genome:\n        logger.info(\"No genome given. skipping.\")\n        return None\n    out_file = op.join(out, op.basename(bam_in) + \"_cov.tsv\")\n    if file_exists(out_file):\n        return None\n    fai = genome + \".fai\"\n    cov = pybedtools.BedTool(bam_in).genome_coverage(g=fai, max=1)\n    cov.saveas(out_file)\n    total = 0\n    for region in cov:\n        if region[0] == \"genome\" and int(region[1]) != 0:\n            total += float(region[4])\n    logger.info(\"Total genome with sequences: %s \" % total)", "response": "Detect complexity of small RNA sequences"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_bam_file(bam_in, mask=None):\n    seq_obj = defaultdict(int)\n    if mask:\n        mask_file = op.splitext(bam_in)[0] + \"_mask.bam\"\n        if not file_exists(mask_file):\n            pybedtools.BedTool(bam_file).intersect(b=mask, v=True).saveas(mask_file)\n        bam_in = mask_file\n    out_file = op.splitext(bam_in)[0] + \"_rmlw.bam\"\n    # bam.index(bam_in, {'algorithm':{}})\n    run(\"samtools index %s\" % bam_in)\n    if not file_exists(bam_in + \".bai\"):\n        raise IOError(\"Failed to created bam index of %s. Try to do it manually\" % bam_in)\n    bam_handle = pysam.AlignmentFile(bam_in, \"rb\")\n    with pysam.AlignmentFile(out_file, \"wb\", template=bam_handle) as out_handle:\n        for read in bam_handle.fetch():\n            seq_name = int(read.query_name.replace('seq_', ''))\n            match_size = [nts for oper, nts in read.cigartuples if oper == 0]\n            subs_size = [nts for oper, nts in read.cigartuples if oper == 4]\n            if match_size[0] < 17:\n                continue\n            if subs_size:\n                if subs_size[0] > 3:\n                    continue\n            try:\n                nh = read.get_tag('NH')\n            except KeyError:\n                nh = 1\n            seq_obj[seq_name] = sequence(seq_name)\n            seq_obj[seq_name].align = nh\n            out_handle.write(read)\n    return out_file, seq_obj", "response": "Remove from alignment reads with low counts and highly of hits from the bam file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the merge file of sequences position to create clusters that will have all sequences that shared any position on the genome.", "response": "def detect_clusters(c, current_seq, MIN_SEQ, non_un_gl=False):\n    \"\"\"\n    Parse the merge file of sequences position to create clusters that will have all\n    sequences that shared any position on the genome\n\n    :param c: file from bedtools with merge sequence positions\n    :param current_seq: list of sequences\n    :param MIN_SEQ: int cutoff to keep the cluster or not. 10 as default\n\n    :return: object with information about:\n        * cluster\n        * dict with sequences (as keys) and cluster_id (as value)\n        * sequences\n        * loci\n\n    \"\"\"\n    current_loci = {}\n    current_clus = {}\n    # sequence2clusters = [set()] * (max(current_seq.keys()) + 2)\n    sequence2clusters = defaultdict(set)\n    lindex = 0\n    eindex = 0\n    previous_id = 0\n    for line in c.features():\n        c, start, end, name, score, strand, c_id = line\n        name = int(name.replace('seq_', ''))\n        pos = int(start) if strand == \"+\" else int(end)\n        if name not in current_seq:\n            continue\n        if c.find('Un_gl') > -1 and non_un_gl:\n            continue\n        if c_id != previous_id:\n            if previous_id > 0:\n                if len(current_clus[eindex].idmembers) < MIN_SEQ:\n                    for s in current_clus[eindex].idmembers:\n                        sequence2clusters[s] = sequence2clusters[s] - set([eindex])\n                    del current_clus[eindex]\n\n            logger.debug(\"detect_cluster: %s %s %s\" % (c_id, previous_id, name))\n            lindex += 1\n            eindex += 1\n            current_clus[eindex] = cluster(eindex)\n            newpos = position(lindex, c, start, end, strand)\n            current_loci[lindex] = newpos\n\n        # update locus, sequences in each line\n        current_loci[lindex].end = int(end)\n        current_loci[lindex].coverage[pos] += 1\n        size = range(pos, pos + current_seq[name].len)\n        current_loci[lindex].counts.update(dict(zip(size, [current_seq[name].total()] * current_seq[name].len)))\n        current_clus[eindex].idmembers[name] = 1\n        current_clus[eindex].add_id_member([name], lindex)\n        current_seq[name].add_pos(lindex, pos)\n        # current_seq[name].align = 1\n        previous_id = c_id\n        sequence2clusters[name].add(eindex)\n    logger.info(\"%s Clusters read\" % eindex)\n    # merge cluster with shared sequences  \n    metacluster_obj, cluster_id = _find_metaclusters(current_clus, sequence2clusters, current_seq, MIN_SEQ)\n\n    return cluster_info_obj(current_clus, metacluster_obj, current_loci, current_seq)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all meta - clusters that share sequences", "response": "def _find_metaclusters(clus_obj, sequence2clusters, current_seq, min_seqs):\n    \"\"\"\n    Mask under same id all clusters that share sequences\n    :param clus_obj: cluster object coming from detect_cluster\n    :param min_seqs: int cutoff to keep the cluster or not. 10 as default\n\n    :return: updated clus_obj and dict with seq_id: cluster_id\n    \"\"\"\n    seen = defaultdict(int)\n    metacluster = defaultdict(set)\n    c_index = len(sequence2clusters)\n    logger.info(\"Creating meta-clusters based on shared sequences: %s\" % c_index)\n    meta_idx = 1\n    bar = ProgressBar(maxval=c_index)\n    bar.start()\n    bar.update()\n    for itern, name in enumerate(sequence2clusters):\n        clusters = sequence2clusters[name]\n        if len(clusters) == 0:\n            c_index -= 1\n            continue\n        current_seq[name].align = 1\n        meta_idx += 1\n        bar.update(itern)\n        already_in = _common(clusters, seen)\n        _update(clusters, meta_idx, seen)\n        metacluster[meta_idx] = metacluster[meta_idx].union(clusters)\n\n        if already_in:\n            for seen_metacluster in already_in:\n                clusters2merge = metacluster[seen_metacluster]\n                metacluster[meta_idx] = metacluster[meta_idx].union(clusters2merge)\n                _update(clusters2merge, meta_idx, seen)\n                # metacluster[seen_metacluster] = 0\n                del metacluster[seen_metacluster]\n    logger.info(\"%s metaclusters from %s sequences\" % (len(metacluster), c_index))\n\n    return metacluster, seen"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_families_deprecated(clus_obj, min_seqs):\n    logger.info(\"Creating meta-clusters based on shared sequences.\")\n    seen = defaultdict()\n    metacluster = defaultdict(list)\n    c_index = clus_obj.keys()\n    meta_idx = 0\n    with ProgressBar(maxval=len(c_index), redirect_stdout=True) as p:\n        for itern, c in enumerate(c_index):\n            p.update(itern)\n            clus = clus_obj[c]\n            if len(clus.idmembers.keys()) < min_seqs:\n                del clus_obj[c]\n                continue\n            logger.debug(\"reading cluster %s\" % c)\n            logger.debug(\"loci2seq  %s\" % clus.loci2seq)\n            already_in, not_in = _get_seqs_from_cluster(clus.idmembers.keys(), seen)\n            logger.debug(\"seen %s news %s\" % (already_in, not_in))\n            meta_idx += 1\n            metacluster[meta_idx].append(c)\n            seen.update(dict(zip(not_in, [meta_idx] * len(not_in))))\n            if len(already_in) > 0:\n                logger.debug(\"seen in %s\" % already_in)\n                for eindex in already_in:\n                    for cluster in metacluster[eindex]:\n                        metacluster[meta_idx].append(cluster)\n                        prev_clus = clus_obj[cluster]\n                        logger.debug(\"_find_families: prev %s current %s\" % (eindex, clus.id))\n                        # add current seqs to seen cluster\n                        seqs_in = prev_clus.idmembers.keys()\n                        seen.update(dict(zip(seqs_in, [meta_idx] * len(seqs_in))))\n                        # for s_in_clus in prev_clus.idmembers:\n                        #    seen[s_in_clus] = meta_idx\n                    #    clus.idmembers[s_in_clus] = 1\n                    # add current locus to seen cluster\n                    # for loci in prev_clus.loci2seq:\n                    #    logger.debug(\"adding %s\" % loci)\n                        # if not loci_old in current_clus[eindex].loci2seq:\n                    #    clus.add_id_member(list(prev_clus.loci2seq[loci]), loci)\n                    # logger.debug(\"loci %s\" % clus.loci2seq.keys())\n                    del metacluster[eindex]\n                # clus_obj[c] = clus\n\n                # logger.debug(\"num cluster %s\" % len(clus_obj.keys()))\n    logger.info(\"%s clusters merged\" % len(metacluster))\n\n    return metacluster, seen", "response": "Find all possible families of the cluster in which the cluster is not in the same id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef peak_calling(clus_obj):\n    new_cluster = {}\n    for cid in clus_obj.clus:\n        cluster = clus_obj.clus[cid]\n        cluster.update()\n        logger.debug(\"peak calling for %s\" % cid)\n        bigger = cluster.locimaxid\n        if bigger in clus_obj.loci:\n            s, e = min(clus_obj.loci[bigger].counts.keys()), max(clus_obj.loci[bigger].counts.keys())\n            scale = s\n            if clus_obj.loci[bigger].strand == \"-\":\n                scale = e\n            logger.debug(\"bigger %s at %s-%s\" % (bigger, s, e))\n            dt = np.array([0] * (abs(e - s) + 12))\n            for pos in clus_obj.loci[bigger].counts:\n                ss = abs(int(pos) - scale) + 5\n                dt[ss] += clus_obj.loci[bigger].counts[pos]\n        x = np.array(range(0, len(dt)))\n        logger.debug(\"x %s and y %s\" % (x, dt))\n        # tab = pd.DataFrame({'x': x, 'y': dt})\n        # tab.to_csv( str(cid) + \"peaks.csv\", mode='w', header=False, index=False)\n        if len(x) > 35 + 12:\n            peaks = list(np.array(pysen.pysenMMean(x, dt)) - 5)\n            logger.debug(peaks)\n        else:\n            peaks =  ['short']\n        cluster.peaks = peaks\n        new_cluster[cid] = cluster\n    clus_obj.clus = new_cluster\n    return clus_obj", "response": "Run peak calling inside each cluster"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_reads(seq, name):\n    reads = dict()\n    if len(seq) < 130 and len(seq) > 70:\n        reads.update(_mature(seq[:40], 0, name))\n        reads.update(_mature(seq[-40:], len(seq) - 40, name))\n        reads.update(_noise(seq, name))\n        reads.update(_noise(seq, name, 25))\n    return reads", "response": "Main function that creates reads from precursors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate mature sequences around start and end", "response": "def _mature(subseq, absolute, c,  size=33, total=5000):\n    \"\"\"Create mature sequences around start/end\"\"\"\n    reads = dict()\n    probs = [0.1, 0.2, 0.4, 0.2, 0.1]\n    end = 5 + size\n    error = [-2, -1, 0, 1, 2]\n    for error5 in error:\n        for error3 in error:\n            s = 5 - error5\n            e = end - error3\n            seen = subseq[s:e]\n            counts = int(probs[error5 + 2] * probs[error3 + 2] * total) + 1\n            name = \"seq_%s_%s_%s_x%s\" % (c, s + absolute, e + absolute, counts)\n            reads[name] = (seen, counts)\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates mature sequences around start and end", "response": "def _noise(seq, c, size=33, total=1000):\n    \"\"\"Create mature sequences around start/end\"\"\"\n    reads = dict()\n    seen = 0\n    while seen < total:\n        s = random.randint(0, len(seq) - size)\n        e = s + size + random.randint(-5,5)\n        p = random.uniform(0, 0.1)\n        counts = int(p * total) + 1\n        seen += counts\n        name = \"seq_%s_%s_%s_x%s\" % (c, s, e, counts)\n        reads[name] = (seq[s:e], counts)\n    return reads"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stats(args):\n    logger.info(\"Reading sequeces\")\n    data = parse_ma_file(args.ma)\n    logger.info(\"Get sequences from sam\")\n    is_align = _read_sam(args.sam)\n    is_json, is_db = _read_json(args.json)\n    res = _summarise_sam(data, is_align, is_json, is_db)\n    _write_suma(res, os.path.join(args.out, \"stats_align.dat\"))\n    logger.info(\"Done\")", "response": "Create stats from the analysis\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the provided command and return the result.", "response": "def run(cmd, data=None, checks=None, region=None, log_error=True,\n        log_stdout=False):\n    \"\"\"Run the provided command, logging details and checking for errors.\n    \"\"\"\n    try:\n        logger.debug(\" \".join(str(x) for x in cmd) if not isinstance(cmd, basestring) else cmd)\n        _do_run(cmd, checks, log_stdout)\n    except:\n        if log_error:\n            logger.info(\"error at command\")\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _normalize_cmd_args(cmd):\n    if isinstance(cmd, basestring):\n        # check for standard or anonymous named pipes\n        if cmd.find(\" | \") > 0 or cmd.find(\">(\") or cmd.find(\"<(\"):\n            return \"set -o pipefail; \" + cmd, True, find_bash()\n        else:\n            return cmd, True, None\n    else:\n        return [str(x) for x in cmd], False, None", "response": "Normalize subprocess arguments to handle list commands string and pipes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms running and check results.", "response": "def _do_run(cmd, checks, log_stdout=False):\n    \"\"\"Perform running and check results, raising errors for issues.\n    \"\"\"\n    cmd, shell_arg, executable_arg = _normalize_cmd_args(cmd)\n    s = subprocess.Popen(cmd, shell=shell_arg, executable=executable_arg,\n                         stdout=subprocess.PIPE,\n                         stderr=subprocess.STDOUT, close_fds=True)\n    debug_stdout = collections.deque(maxlen=100)\n    while 1:\n        line = s.stdout.readline()\n        if line:\n            debug_stdout.append(line)\n            if log_stdout:\n                logger.debug(line.rstrip())\n            else:\n                logger.debug(line.rstrip())\n        exitcode = s.poll()\n        if exitcode is not None:\n            for line in s.stdout:\n                debug_stdout.append(line)\n            if exitcode is not None and exitcode != 0:\n                error_msg = \" \".join(cmd) if not isinstance(cmd, basestring) else cmd\n                error_msg += \"\\n\"\n                error_msg += \"\".join(debug_stdout)\n                s.communicate()\n                s.stdout.close()\n                raise subprocess.CalledProcessError(exitcode, error_msg)\n            else:\n                break\n    s.communicate()\n    s.stdout.close()\n    # Check for problems not identified by shell return codes\n    if checks:\n        for check in checks:\n            if not check():\n                raise IOError(\"External command failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning dict with sequences = [ cluster1 cluster2... ]", "response": "def _dict_seq_locus(list_c, loci_obj, seq_obj):\n    \"\"\"\n    return dict with sequences = [ cluster1, cluster2 ...]\n    \"\"\"\n    seqs = defaultdict(set)\n    # n = len(list_c.keys())\n    for c in list_c.values():\n        for l in c.loci2seq:\n            [seqs[s].add(c.id) for s in c.loci2seq[l]]\n\n    common = [s for s in seqs if len(seqs[s]) > 1]\n    seqs_in_c = defaultdict(float)\n    for c in list_c.values():\n        for l in c.loci2seq:\n            # total = sum([v for v in loci_obj[l].coverage.values()])\n            for s in c.loci2seq[l]:\n                if s in common:\n                    pos = seq_obj[s].pos[l]\n                    # cov = 1.0 * loci_obj[l].coverage[pos] / total\n                    cov = 1.0 * loci_obj[l].coverage[pos]\n                    if seqs_in_c[(s, c.id)] < cov:\n                        seqs_in_c[(s, c.id)] = cov\n    seqs_in_c = _transform(seqs_in_c)\n    return seqs_in_c"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the PMF with new data.", "response": "def Update(self, data):\n        \"\"\"Updates the PMF with new data.\n        data: string cookie type\n        \"\"\"\n        for hypo in self.Values():\n            like = self.Likelihood(data, hypo)\n            self.Mult(hypo, like)\n        self.Normalize()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Likelihood(self, data, hypo):\n        mix = self.loci[hypo]\n        like = mix[data]\n        return like", "response": "The likelihood of the data under the hypothesis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_seq(clus_obj, index):\n    current = clus_obj.clus\n    clus_seqt = clus_obj.seq\n    clus_locit = clus_obj.loci\n\n    itern = 0\n    for idc in current.keys():\n        itern += 1\n        timestamp = str(idc)\n        seqListTemp = ()\n        f = open(\"/tmp/\"+timestamp+\".fa\",\"w\")\n        for idl in current[idc].loci2seq.keys():\n            seqListTemp = list(set(seqListTemp).union(current[idc].loci2seq[idl]))\n        maxscore = 0\n        for s in seqListTemp:\n            score = calculate_size(clus_seqt[s].freq)\n            maxscore = max(maxscore,score)\n            clus_seqt[s].score = score\n            seq = clus_seqt[s]\n            f.write(\">\"+s+\"\\n\"+seq.seq+\"\\n\")\n        f.close()\n\n        locilen_sorted = sorted(current[idc].locilen.iteritems(), key = operator.itemgetter(1),reverse = True)\n        lmax = clus_locit[locilen_sorted[0][0]]\n        f = open(\"/tmp/\"+timestamp+\".bed\",\"w\")\n        f.write(\"%s\\t%s\\t%s\\t.\\t.\\t%s\\n\" % (lmax.chr,lmax.start,lmax.end,lmax.strand))\n        f.close()\n        os.system(\"bedtools  getfasta -s -fi \"+index+\" -bed /tmp/\"+timestamp+\".bed -fo /tmp/\"+timestamp+\".pre.fa\")\n        os.system(\"bowtie2-build /tmp/\"+timestamp+\".pre.fa  /tmp/\"+timestamp+\".pre.ind >/dev/null 2>&1\")\n        os.system(\"bowtie2 --rdg 7,3 --mp 4 --end-to-end --no-head --no-sq  -D 20 -R 3 -N 0 -i S,1,0.8 -L 3 -f /tmp/\"+timestamp+\".pre.ind /tmp/\"+timestamp+\".fa -S /tmp/\"+timestamp+\".map  >>bowtie.log 2>&1\")\n        f = open(\"/tmp/\"+timestamp+\".map\",\"r\")\n        seqpos = {}\n        minv = 10000000\n        for line in f:\n            line = line.strip()\n            cols = line.split(\"\\t\")\n            seqpos[cols[0]] = int(cols[3])\n            if minv>int(cols[3]):\n                minv = int(cols[3])\n        f.close()\n        seqpos_sorted = sorted(seqpos.iteritems(), key = operator.itemgetter(1),reverse = False)\n        showseq = \"\"\n        showseq_plain = \"\"\n        for (s,pos) in seqpos_sorted:\n            ratio = (clus_seqt[s].score*1.0/maxscore*100.0)\n            realScore = (math.log(ratio,2)*2)\n            if realScore<0:\n                realScore = 0\n            # \"score %s max %s  ratio %s real %.0f\" % (clus_seqt[s].score,maxscore,ratio,realScore)\n            ##calculate the mean expression of the sequence and change size letter\n            showseq_plain += \"<br>%s<a style = \\\"font-size:%.0fpx;\\\"href = javascript:loadSeq(\\\"%s\\\")>%s</a>\" % (\"\".join(\".\" for i in range(pos-1)),realScore+10,s,clus_seqt[s].seq)\n            #showseq+ = seqviz.addseq(pos-1,clus_seqt[s].len,clus_seqt[s].seq)\n        #current[idc].showseq = showseq\n        current[idc].showseq_plain = showseq_plain\n        os.system(\"rm /tmp/\"+timestamp+\"*\")\n    clus_obj.clus = current\n    clus_obj.seq = clus_seqt\n    return clus_obj", "response": "Get the precursor and map sequences to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares BAM file for splitting into smaller cluster positions.", "response": "def prepare_bam(bam_in, precursors):\n    \"\"\"\n    Clean BAM file to keep only position inside the bigger cluster\n    \"\"\"\n    # use pybedtools to keep valid positions\n    # intersect option with -b bigger_cluster_loci\n    a = pybedtools.BedTool(bam_in)\n    b = pybedtools.BedTool(precursors)\n    c = a.intersect(b, u=True)\n    out_file = utils.splitext_plus(op.basename(bam_in))[0] + \"_clean.bam\"\n    c.saveas(out_file)\n    return op.abspath(out_file)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reorder_columns(bed_file):\n    new_bed = utils.splitext_plus(bed_file)[0] + '_order.bed'\n    with open(bed_file) as in_handle:\n        with open(new_bed, 'w') as out_handle:\n            for line in in_handle:\n                cols = line.strip().split(\"\\t\")\n                cols[3] = _select_anno(cols[3]) + \"_\" + cols[4]\n                cols[4] = \"0\"\n                print(\"\\t\".join(cols), file=out_handle, end=\"\")\n    return new_bed", "response": "Reorder columns to be compatible with CoRaL\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving counts to score columns in bed file", "response": "def _fix_score_column(cov_file):\n    \"\"\"\n    Move counts to score columns in bed file\n    \"\"\"\n    new_cov = utils.splitext_plus(cov_file)[0] + '_fix.cov'\n    with open(cov_file) as in_handle:\n        with open(new_cov, 'w') as out_handle:\n            for line in in_handle:\n                cols = line.strip().split(\"\\t\")\n                cols[4] = cols[6]\n                print(\"\\t\".join(cols[0:6]), file=out_handle, end=\"\")\n    return new_cov"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetecting regions using first CoRaL module.", "response": "def detect_regions(bam_in, bed_file, out_dir, prefix):\n    \"\"\"\n    Detect regions using first CoRaL module\n    \"\"\"\n    bed_file = _reorder_columns(bed_file)\n    counts_reads_cmd = (\"coverageBed -s -counts -b {bam_in} \"\n                        \"-a {bed_file} | sort -k4,4 \"\n                        \"> {out_dir}/loci.cov\")\n    # with tx_tmpdir() as temp_dir:\n    with utils.chdir(out_dir):\n        run(counts_reads_cmd.format(min_trimmed_read_len=min_trimmed_read_len, max_trimmed_read_len=max_trimmed_read_len, **locals()), \"Run counts_reads\")\n        loci_file = _fix_score_column(op.join(out_dir, \"loci.cov\"))\n        return loci_file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove counts to score columns in bed file Move counts to score columns in bed file", "response": "def _order_antisense_column(cov_file, min_reads):\n    \"\"\"\n    Move counts to score columns in bed file\n    \"\"\"\n    new_cov = op.join(op.dirname(cov_file), 'feat_antisense.txt')\n    with open(cov_file) as in_handle:\n        with open(new_cov, 'w') as out_handle:\n            print(\"name\\tantisense\", file=out_handle, end=\"\")\n            for line in in_handle:\n                cols = line.strip().split(\"\\t\")\n                cols[6] = 0 if cols[6] < min_reads else cols[6]\n                print(\"%s\\t%s\" % (cols[3], cols[6]), file=out_handle, end=\"\")\n    return new_cov"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the number of reads per position in a single loci file.", "response": "def _reads_per_position(bam_in, loci_file, out_dir):\n    \"\"\"\n    Create input for compute entropy\n    \"\"\"\n    data = Counter()\n    a = pybedtools.BedTool(bam_in)\n    b = pybedtools.BedTool(loci_file)\n    c = a.intersect(b, s=True, bed=True, wo=True)\n    for line in c:\n        end = int(line[1]) + 1 + int(line[2]) if line[5] == \"+\" else int(line[1]) + 1\n        start = int(line[1]) + 1 if line[5] == \"+\" else int(line[1]) + 1 + int(line[2])\n        side5 = \"%s\\t5p\\t%s\" % (line[15], start)\n        side3 = \"%s\\t3p\\t%s\" % (line[15], end)\n        data[side5] += 1\n        data[side3] += 1\n\n    counts_reads = op.join(out_dir, 'locus_readpos.counts')\n    with open(counts_reads, 'w') as out_handle:\n        for k in data:\n            print(k, file=out_handle, end=\"\")\n\n    return counts_reads"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating features from a CoRaL genomic file.", "response": "def create_features(bam_in, loci_file, reference, out_dir):\n    \"\"\"\n    Use feature extraction module from CoRaL\n    \"\"\"\n    lenvec_plus = op.join(out_dir, 'genomic_lenvec.plus')\n    lenvec_minus = op.join(out_dir, 'genomic_lenvec.minus')\n    compute_genomic_cmd = (\"compute_genomic_lenvectors \"\n                           \"{bam_in} {lenvec_plus} \"\n                           \"{lenvec_minus} \"\n                           \"{min_len} \"\n                           \"{max_len} \")\n    index_genomic_cmd = (\"index_genomic_lenvectors \"\n                         \"{lenvec} \")\n    genomic_lenvec = op.join(out_dir, 'genomic_lenvec')\n    feat_len_file = op.join(out_dir, 'feat_lengths.txt')\n    compute_locus_cmd = (\"compute_locus_lenvectors \"\n                         \"{loci_file} \"\n                         \"{genomic_lenvec} \"\n                         \"{min_len} \"\n                         \"{max_len} \"\n                         \"> {feat_len_file}\")\n    cov_S_file = op.join(out_dir, 'loci.cov_anti')\n    coverage_anti_cmd = (\"coverageBed -S -counts -b \"\n                         \"{bam_in} -a {loci_file} \"\n                         \"> {cov_S_file}\")\n    feat_posentropy = op.join(out_dir, 'feat_posentropy.txt')\n    entropy_cmd = (\"compute_locus_entropy.rb \"\n                   \"{counts_reads} \"\n                   \"> {feat_posentropy}\")\n    with utils.chdir(out_dir):\n        run(compute_genomic_cmd.format(min_len=min_trimmed_read_len, max_len=max_trimmed_read_len, **locals()), \"Run compute_genomic\")\n        run(index_genomic_cmd.format(lenvec=lenvec_plus), \"Run index in plus\")\n        run(index_genomic_cmd.format(lenvec=lenvec_minus), \"Run index in minus\")\n        run(compute_locus_cmd.format(min_len=min_trimmed_read_len, max_len=max_trimmed_read_len, **locals()), \"Run compute locus\")\n        run(coverage_anti_cmd.format(**locals()), \"Run coverage antisense\")\n        feat_antisense = _order_antisense_column(cov_S_file, min_trimmed_read_len)\n\n        counts_reads = _reads_per_position(bam_in, loci_file, out_dir)\n        run(entropy_cmd.format(**locals()), \"Run entropy\")\n\n        rnafold = calculate_structure(loci_file, reference)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef report(args):\n    logger.info(\"reading sequeces\")\n    data = load_data(args.json)\n\n    logger.info(\"create profile\")\n    data = make_profile(data, os.path.join(args.out, \"profiles\"), args)\n    logger.info(\"create database\")\n    make_database(data, \"seqcluster.db\", args.out)\n\n    logger.info(\"Done. Download https://github.com/lpantano/seqclusterViz/archive/master.zip to browse the output.\")", "response": "Create report in html format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of peaks that are closer than 10", "response": "def _summarize_peaks(peaks):\n    \"\"\"\n    merge peaks position if closer than 10\n    \"\"\"\n    previous = peaks[0]\n    new_peaks = [previous]\n    for pos in peaks:\n        if pos > previous + 10:\n            new_peaks.add(pos)\n        previous = pos\n    return new_peaks"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_mature(x, y, win=10):\n    previous = min(y)\n    peaks = []\n    intervals = range(x, y, win)\n    for pos in intervals:\n        if y[pos] > previous * 10:\n            previous = y[pos]\n            peaks.add(pos)\n    peaks = _summarize_peaks(peaks)", "response": "Find mature peaks in the expression profile."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncollapsing identical sequences and keep Q", "response": "def collapse(in_file):\n    \"\"\"collapse identical sequences and keep Q\"\"\"\n    keep = Counter()\n    with open_fastq(in_file) as handle:\n        for line in handle:\n            if line.startswith(\"@\"):\n                if line.find(\"UMI\") > -1:\n                    logger.info(\"Find UMI tags in read names, collapsing by UMI.\")\n                    return collapse_umi(in_file)\n                seq = handle.next().strip()\n                handle.next()\n                qual = handle.next().strip()\n                if seq in keep:\n                    keep[seq].update(qual)\n                else:\n                    keep[seq] = quality(qual)\n    logger.info(\"Sequences loaded: %s\" % len(keep))\n    return keep"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef collapse_umi(in_file):\n    keep = defaultdict(dict)\n    with open_fastq(in_file) as handle:\n        for line in handle:\n            if line.startswith(\"@\"):\n                m = re.search('UMI_([ATGC]*)', line.strip())\n                umis = m.group(0)\n                seq = handle.next().strip()\n                handle.next()\n                qual = handle.next().strip()\n                if (umis, seq) in keep:\n                    keep[(umis, seq)][1].update(qual)\n                    keep[(umis, seq)][0].update(seq)\n                else:\n                    keep[(umis, seq)] = [umi(seq), quality(qual)]\n    logger.info(\"Sequences loaded: %s\" % len(keep))\n    return keep", "response": "collapse reads using UMI tags"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening a fastq file using gzip if it is gzipped from bcbio package", "response": "def open_fastq(in_file):\n    \"\"\" open a fastq file, using gzip if it is gzipped\n    from bcbio package\n    \"\"\"\n    _, ext = os.path.splitext(in_file)\n    if ext == \".gz\":\n        return gzip.open(in_file, 'rb')\n    if ext in [\".fastq\", \".fq\", \".fasta\", \".fa\"]:\n        return open(in_file, 'r')\n    return ValueError(\"File needs to be fastq|fasta|fq|fa [.gz]\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef collapse_fastq(args):\n    try:\n        umi_fn = args.fastq\n        if _is_umi(args.fastq):\n            umis = collapse(args.fastq)\n            umi_fn = os.path.join(args.out, splitext_plus(os.path.basename(args.fastq))[0] + \"_umi_trimmed.fastq\")\n            write_output(umi_fn, umis, args.minimum)\n        seqs = collapse(umi_fn)\n        out_file = splitext_plus(os.path.basename(args.fastq))[0] + \"_trimmed.fastq\"\n    except IOError as e:\n        logger.error(\"I/O error({0}): {1}\".format(e.errno, e.strerror))\n        raise \"Can not read file\"\n    out_file = os.path.join(args.out, out_file)\n    write_output(out_file, seqs, args.minimum)\n    return out_file", "response": "collapse fasq files after adapter trimming\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a doctree remove all non - slide related elements from it.", "response": "def filter_doctree_for_slides(doctree):\n    \"\"\"Given a doctree, remove all non-slide related elements from it.\"\"\"\n\n    current = 0\n    num_children = len(doctree.children)\n    while current < num_children:\n\n        child = doctree.children[current]\n        child.replace_self(\n            child.traverse(no_autoslides_filter)\n        )\n\n        if len(doctree.children) == num_children:\n            # nothing removed, increment current\n            current += 1\n        else:\n            # a node was removed; retain current and update length\n            num_children = len(doctree.children)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_title_node(self, node, increment=True):\n\n        parent_title_node = node.parent.next_node(nodes.title)\n        nextslide_info = getattr(\n            parent_title_node, 'nextslide_info',\n            (parent_title_node.deepcopy().children, 1),\n        )\n        nextslide_info = (\n            nextslide_info[0],\n            nextslide_info[1] + 1,\n        )\n\n        if node.args:\n            textnodes, messages = node.state.inline_text(\n                node.args[0],\n                1,\n            )\n            new_title = nodes.title(node.args[0], '', *textnodes)\n\n        else:\n\n            title_nodes = nextslide_info[0][:]\n\n            if 'increment' in node.attributes:\n                title_nodes.append(\n                    nodes.Text(' (%s)' % nextslide_info[1])\n                )\n\n            new_title = nodes.title(\n                '', '',\n                *title_nodes\n            )\n\n        new_title.nextslide_info = nextslide_info\n        return new_title", "response": "Generate a new title node for node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef apply(self, builder):\n\n        if 'theme' in self.attributes:\n            builder.apply_theme(\n                self.attributes['theme'],\n                builder.theme_options,\n            )", "response": "Applies the Slide Configuration to a Builder."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_conf(cls, builder, doctree=None):\n\n        # set up the default conf\n        result = {\n            'theme': builder.config.slide_theme,\n            'autoslides': builder.config.autoslides,\n            'slide_classes': [],\n        }\n\n        # now look for a slideconf node in the doctree and update the conf\n        if doctree:\n            conf_node = cls.get(doctree)\n            if conf_node:\n                result.update(conf_node.attributes)\n\n        return result", "response": "Return a dictionary of slide configuration for this doctree."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new context dict based on original context.", "response": "def __fix_context(context):\n    \"\"\"Return a new context dict based on original context.\n\n    The new context will be a copy of the original, and some mutable\n    members (such as script and css files) will also be copied to\n    prevent polluting shared context.\n    \"\"\"\n\n    COPY_LISTS = ('script_files', 'css_files',)\n\n    for attr in COPY_LISTS:\n        if attr in context:\n            context[attr] = context[attr][:]\n\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ask_user(d):\n\n    # Print welcome message\n    msg = bold('Welcome to the Hieroglyph %s quickstart utility.') % (\n        version(),\n    )\n    print(msg)\n    msg = \"\"\"\nThis will ask questions for creating a Hieroglyph project, and then ask\nsome basic Sphinx questions.\n\"\"\"\n    print(msg)\n\n    # set a few defaults that we don't usually care about for Hieroglyph\n    d.update({\n        'version': datetime.date.today().strftime('%Y.%m.%d'),\n        'release': datetime.date.today().strftime('%Y.%m.%d'),\n        'make_mode': True,\n    })\n\n    if 'project' not in d:\n        print('''\nThe presentation title will be included on the title slide.''')\n        sphinx.quickstart.do_prompt(d, 'project', 'Presentation title')\n    if 'author' not in d:\n        sphinx.quickstart.do_prompt(d, 'author', 'Author name(s)')\n\n    # slide_theme\n    theme_entrypoints = pkg_resources.iter_entry_points('hieroglyph.theme')\n\n    themes = [\n        t.load()\n        for t in theme_entrypoints\n    ]\n\n    msg = \"\"\"\nAvailable themes:\n\n\"\"\"\n\n    for theme in themes:\n        msg += '\\n'.join([\n            bold(theme['name']),\n            theme['desc'],\n            '', '',\n        ])\n\n    msg += \"\"\"Which theme would you like to use?\"\"\"\n    print(msg)\n\n    sphinx.quickstart.do_prompt(\n        d, 'slide_theme', 'Slide Theme', themes[0]['name'],\n        sphinx.quickstart.choice(\n            *[t['name'] for t in themes]\n        ),\n    )\n\n    # Ask original questions\n    print(\"\")\n    sphinx.quickstart.ask_user(d)", "response": "Wrap sphinx. quickstart. ask_user and add additional questions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the context dict for rendering this slide.", "response": "def get_slide_context(self):\n        \"\"\"Return the context dict for rendering this slide.\"\"\"\n\n        return {\n            'title': self.title,\n            'level': self.level,\n            'content': self.content,\n            'classes': self.classes,\n            'slide_classes': self._filter_classes(exclude='content-'),\n            'content_classes': self._filter_classes(include='content-'),\n            'slide_number': self.slide_number,\n            'config': self._translator.builder.config,\n            'id': self.id,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds the slide number to the output if enabled.", "response": "def _add_slide_number(self, slide_no):\n        \"\"\"Add the slide number to the output if enabled.\"\"\"\n\n        if self.builder.config.slide_numbers:\n            self.body.append(\n                '\\n<div class=\"slide-no\">%s</div>\\n' % (slide_no,),\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_slide_footer(self, slide_no):\n\n        if self.builder.config.slide_footer:\n            self.body.append(\n                '\\n<div class=\"slide-footer\">%s</div>\\n' % (\n                    self.builder.config.slide_footer,\n                ),\n            )", "response": "Add the slide footer to the output if enabled."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninspects the Sphinx configuration and update for slide - linking.", "response": "def inspect_config(app):\n    \"\"\"Inspect the Sphinx configuration and update for slide-linking.\n\n    If links from HTML to slides are enabled, make sure the sidebar\n    configuration includes the template and add the necessary theme\n    directory as a loader so the sidebar template can be located.\n\n    If the sidebar configuration already includes ``slidelink.html``\n    (in any key), the configuration will not be changed. If the\n    configuration is not specified, we'll attempt to emulate what\n    Sphinx does by default.\n    \"\"\"\n\n    # avoid import cycles :/\n    from hieroglyph import writer\n\n    # only reconfigure Sphinx if we're generating HTML\n    if app.builder.name not in HTML_BUILDERS:\n        return\n\n    if app.config.slide_link_html_to_slides:\n\n        # add the slide theme dir as a Loader\n        app.builder.templates.loaders.append(\n            SphinxFileSystemLoader(\n                os.path.join(\n                    os.path.dirname(__file__), 'themes', 'slides',\n                )\n            )\n        )\n\n        # add the \"show slides\" sidebar template\n        if not app.config.html_sidebars:\n            # no sidebars explicitly defined, mimic the old style\n            # behavior + slide links\n            app.config.html_sidebars = {\n                '**': [\n                    'localtoc.html',\n                    'relations.html',\n                    'sourcelink.html',\n                    SLIDELINK_TEMPLATE,\n                    'searchbox.html',\n                ],\n            }\n        else:\n            # sidebars defined, add the template if needed\n            included = False\n            for glob, templates in app.config.html_sidebars:\n                if SLIDELINK_TEMPLATE in templates:\n                    included = True\n                    break\n\n            if not included:\n                # the slidelink template was not included; append it\n                # to the list of sidebars for all templates\n                app.config.html_sidebars.setdefault('**', []).append(\n                    SLIDELINK_TEMPLATE,\n                )\n\n    if app.config.slide_link_html_sections_to_slides:\n        # fix up the HTML Translator\n        if sphinx.version_info >= (1, 6, 0):\n            override_translator = type(\n                'SlideLinkTranslator',\n                (app.builder.get_translator_class(), object),\n                {\n                    'depart_title': writer.depart_title,\n                },\n            )\n            app.set_translator(app.builder, override_translator)\n        else:\n            app.builder.translator_class = type(\n                'SlideLinkTranslator',\n                (app.builder.translator_class, object),\n                {\n                    'depart_title': writer.depart_title,\n                },\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slide_path(builder, pagename=None):\n\n    return builder.get_relative_uri(\n        pagename or builder.current_docname,\n        os.path.join(\n            builder.app.config.slide_relative_path,\n            pagename or builder.current_docname,\n        ))", "response": "Calculate the relative path to the Slides for pagename."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef html_path(builder, pagename=None):\n\n    return builder.get_relative_uri(\n        pagename or builder.current_docname,\n        os.path.join(\n            builder.app.config.slide_html_relative_path,\n            pagename or builder.current_docname,\n        ))", "response": "Calculate the relative path to the Slides for pagename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_link(app, pagename, templatename, context, doctree):\n\n    # we can only show the slidelink if we can resolve the filename\n    context['show_slidelink'] = (\n        app.config.slide_link_html_to_slides and\n        hasattr(app.builder, 'get_outfilename')\n    )\n\n    if context['show_slidelink']:\n        context['slide_path'] = slide_path(app.builder, pagename)", "response": "Add the slides link to the HTML context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_theme(self, themename, themeoptions):\n\n        # push the existing values onto the Stack\n        self._theme_stack.append(\n            (self.theme, self.theme_options)\n        )\n\n        theme_factory = HTMLThemeFactory(self.app)\n        theme_factory.load_additional_themes(self.get_builtin_theme_dirs() + self.config.slide_theme_path)\n\n        self.theme = theme_factory.create(themename)\n        self.theme_options = themeoptions.copy()\n        self.templates.init(self, self.theme)\n        self.templates.environment.filters['json'] = json.dumps\n\n        if self.theme not in self._additional_themes:\n            self._additional_themes.append(self.theme)", "response": "This will store the existing theme configuration and apply a new one."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post_process_images(self, doctree):\n\n        super(AbstractSlideBuilder, self).post_process_images(doctree)\n\n        # figure out where this doctree is in relation to the srcdir\n        relative_base = (\n            ['..'] *\n            doctree.attributes.get('source')[len(self.srcdir) + 1:].count('/')\n        )\n\n        for node in doctree.traverse(nodes.image):\n\n            if node.get('candidates') is None:\n                node['candidates'] = ('*',)\n\n            # fix up images with absolute paths\n            if node['uri'].startswith(self.outdir):\n                node['uri'] = '/'.join(\n                    relative_base + [\n                        node['uri'][len(self.outdir) + 1:]\n                    ]\n                )", "response": "Pick the best candidate for all image URIs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_metadata(section):\n  metadata = {}\n  metadata_lines = section.split('\\n')\n  for line in metadata_lines:\n    colon_index = line.find(':')\n    if colon_index != -1:\n      key = line[:colon_index].strip()\n      val = line[colon_index + 1:].strip()\n      metadata[key] = val\n\n  return metadata", "response": "Given the first part of a slide returns metadata associated with it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef postprocess_html(html, metadata):\n  if metadata.get('build_lists') and metadata['build_lists'] == 'true':\n    html = html.replace('<ul>', '<ul class=\"build\">')\n    html = html.replace('<ol>', '<ol class=\"build\">')\n  return html", "response": "Returns processed HTML to fit into the slide template format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naugmenting a checker method with an augmentation function.", "response": "def augment_visit(linter, checker_method, augmentation):\n    \"\"\"\n    Augmenting a visit enables additional errors to be raised (although that case is\n    better served using a new checker) or to suppress all warnings in certain circumstances.\n\n    Augmenting functions should accept a 'chain' function, which runs the checker method\n    and possibly any other augmentations, and secondly an Astroid node. \"chain()\" can be\n    called at any point to trigger the continuation of other checks, or not at all to\n    prevent any further checking.\n    \"\"\"\n\n    if sys.version_info[0] <= 2:\n        checker = get_checker(linter, checker_method.im_class)\n    else:\n        try:\n            checker = get_checker(linter, checker_method.__self__.__class__)\n        except AttributeError:\n            checker = get_checker(linter, get_class(checker_method.__module__, checker_method.__qualname__))\n\n    old_method = getattr(checker, checker_method.__name__)\n\n    def augment_func(node):\n        def chain():\n            old_method(node)\n        augmentation(chain, node)\n\n    setattr(checker, checker_method.__name__, augment_func)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef suppress_message(linter, checker_method, message_id_or_symbol, test_func):\n    # At some point, pylint started preferring message symbols to message IDs. However this is not done\n    # consistently or uniformly - occasionally there are some message IDs with no matching symbols.\n    # We try to work around this here by suppressing both the ID and the symbol, if we can find it.\n    # This also gives us compatability with a broader range of pylint versions.\n\n    # Similarly, a commit between version 1.2 and 1.3 changed where the messages are stored - see:\n    # https://bitbucket.org/logilab/pylint/commits/0b67f42799bed08aebb47babdc9fb0e761efc4ff#chg-reporters/__init__.py\n    # Therefore here, we try the new attribute name, and fall back to the old version for\n    # compatability with <=1.2 and >=1.3\n    msgs_store = getattr(linter, 'msgs_store', linter)\n\n    def get_message_definitions(message_id_or_symbol):\n        if hasattr(msgs_store, 'check_message_id'):\n            return [msgs_store.check_message_id(message_id_or_symbol)]\n        # pylint 2.0 renamed check_message_id to get_message_definition in:\n        # https://github.com/PyCQA/pylint/commit/5ccbf9eaa54c0c302c9180bdfb745566c16e416d\n        elif hasattr(msgs_store, 'get_message_definition'):\n            return [msgs_store.get_message_definition(message_id_or_symbol)]\n        # pylint 2.3.0 renamed get_message_definition to get_message_definitions in:\n        # https://github.com/PyCQA/pylint/commit/da67a9da682e51844fbc674229ff6619eb9c816a\n        elif hasattr(msgs_store, 'get_message_definitions'):\n            return msgs_store.get_message_definitions(message_id_or_symbol)\n        else:\n            raise ValueError('pylint.utils.MessagesStore does not have a get_message_definition(s) method')\n\n    try:\n        pylint_messages = get_message_definitions(message_id_or_symbol)\n        symbols = [symbol\n                   for pylint_message in pylint_messages\n                   for symbol in (pylint_message.msgid, pylint_message.symbol)\n                   if symbol is not None]\n    except UnknownMessage:\n        # This can happen due to mismatches of pylint versions and plugin expectations of available messages\n        symbols = [message_id_or_symbol]\n\n    def do_suppress(chain, node):\n        with Suppress(linter) as s:\n            if test_func(node):\n                s.suppress(*symbols)\n            chain()\n    augment_visit(linter, checker_method, do_suppress)", "response": "Wrapper for suppress_message that allows the suppression of a message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquerying DB to see if hash is blacklisted", "response": "def lookup_full_hashes(self, hash_values):\n        \"\"\"Query DB to see if hash is blacklisted\"\"\"\n        q = '''SELECT threat_type,platform_type,threat_entry_type, expires_at < current_timestamp AS has_expired\n                FROM full_hash WHERE value IN ({})\n        '''\n        output = []\n        with self.get_cursor() as dbc:\n            placeholders = ','.join(['?'] * len(hash_values))\n            dbc.execute(q.format(placeholders), [sqlite3.Binary(hv) for hv in hash_values])\n            for h in dbc.fetchall():\n                threat_type, platform_type, threat_entry_type, has_expired = h\n                threat_list = ThreatList(threat_type, platform_type, threat_entry_type)\n                output.append((threat_list, has_expired))\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_hash_prefix(self, cues):\n        q = '''SELECT value, MAX(negative_expires_at < current_timestamp) AS negative_cache_expired\n                FROM hash_prefix WHERE cue IN ({}) GROUP BY 1\n        '''\n        output = []\n        with self.get_cursor() as dbc:\n            dbc.execute(q.format(','.join(['?'] * len(cues))), [sqlite3.Binary(cue) for cue in cues])\n            for h in dbc.fetchall():\n                value, negative_cache_expired = h\n                output.append((bytes(value), negative_cache_expired))\n        return output", "response": "Lookup hash prefixes by cue Returns a tuple of the first 4 bytes of hash and the negative cache expired."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef store_full_hash(self, threat_list, hash_value, cache_duration, malware_threat_type):\n        log.info('Storing full hash %s to list %s with cache duration %s',\n                 to_hex(hash_value), str(threat_list), cache_duration)\n        qi = '''INSERT OR IGNORE INTO full_hash\n                    (value, threat_type, platform_type, threat_entry_type, malware_threat_type, downloaded_at)\n                VALUES\n                    (?, ?, ?, ?, ?, current_timestamp)\n        '''\n        qu = \"UPDATE full_hash SET expires_at=datetime(current_timestamp, '+{} SECONDS') \\\n            WHERE value=? AND threat_type=? AND platform_type=? AND threat_entry_type=?\"\n\n        i_parameters = [sqlite3.Binary(hash_value), threat_list.threat_type,\n                        threat_list.platform_type, threat_list.threat_entry_type, malware_threat_type]\n        u_parameters = [sqlite3.Binary(hash_value), threat_list.threat_type,\n                        threat_list.platform_type, threat_list.threat_entry_type]\n\n        with self.get_cursor() as dbc:\n            dbc.execute(qi, i_parameters)\n            dbc.execute(qu.format(int(cache_duration)), u_parameters)", "response": "Store full hash found for the given hash prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cleanup_full_hashes(self, keep_expired_for=(60 * 60 * 12)):\n        q = '''DELETE FROM full_hash WHERE expires_at < datetime(current_timestamp, '-{} SECONDS')\n        '''\n        log.info('Cleaning up full_hash entries expired more than {} seconds ago.'.format(keep_expired_for))\n        with self.get_cursor() as dbc:\n            dbc.execute(q.format(int(keep_expired_for)))", "response": "Remove long expired full_hash entries."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_threat_lists(self):\n        q = '''SELECT threat_type,platform_type,threat_entry_type FROM threat_list'''\n        output = []\n        with self.get_cursor() as dbc:\n            dbc.execute(q)\n            for h in dbc.fetchall():\n                threat_type, platform_type, threat_entry_type = h\n                threat_list = ThreatList(threat_type, platform_type, threat_entry_type)\n                output.append(threat_list)\n        return output", "response": "Get a list of known threat lists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_client_state(self):\n        q = '''SELECT threat_type,platform_type,threat_entry_type,client_state FROM threat_list'''\n        output = {}\n        with self.get_cursor() as dbc:\n            dbc.execute(q)\n            for h in dbc.fetchall():\n                threat_type, platform_type, threat_entry_type, client_state = h\n                threat_list_tuple = (threat_type, platform_type, threat_entry_type)\n                output[threat_list_tuple] = client_state\n        return output", "response": "Get a dict of known threat lists including clientState values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a threat list entry if it does not exist.", "response": "def add_threat_list(self, threat_list):\n        \"\"\"Add threat list entry if it does not exist.\"\"\"\n        q = '''INSERT OR IGNORE INTO threat_list\n                    (threat_type, platform_type, threat_entry_type, timestamp)\n                VALUES\n                    (?, ?, ?, current_timestamp)\n        '''\n        params = [threat_list.threat_type, threat_list.platform_type, threat_list.threat_entry_type]\n        with self.get_cursor() as dbc:\n            dbc.execute(q, params)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_threat_list(self, threat_list):\n        log.info('Deleting cached threat list \"{}\"'.format(repr(threat_list)))\n        q = '''DELETE FROM threat_list\n                    WHERE threat_type=? AND platform_type=? AND threat_entry_type=?\n        '''\n        params = [threat_list.threat_type, threat_list.platform_type, threat_list.threat_entry_type]\n        with self.get_cursor() as dbc:\n            dbc.execute(q, params)", "response": "Delete the threat list entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hash_prefix_list_checksum(self, threat_list):\n        q = '''SELECT value FROM hash_prefix\n                WHERE threat_type=? AND platform_type=? AND threat_entry_type=?\n                ORDER BY value\n        '''\n        params = [threat_list.threat_type, threat_list.platform_type, threat_list.threat_entry_type]\n        with self.get_cursor() as dbc:\n            dbc.execute(q, params)\n            all_hashes = b''.join(bytes(h[0]) for h in dbc.fetchall())\n            checksum = hashlib.sha256(all_hashes).digest()\n        return checksum", "response": "Returns SHA256 checksum for alphabetically - sorted concatenated list of hash prefixes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving records matching idices from a lexicographically - sorted local threat list.", "response": "def remove_hash_prefix_indices(self, threat_list, indices):\n        \"\"\"Remove records matching idices from a lexicographically-sorted local threat list.\"\"\"\n        batch_size = 40\n        q = '''DELETE FROM hash_prefix\n                WHERE threat_type=? AND platform_type=? AND threat_entry_type=? AND value IN ({})\n        '''\n        prefixes_to_remove = self.get_hash_prefix_values_to_remove(threat_list, indices)\n        with self.get_cursor() as dbc:\n            for i in range(0, len(prefixes_to_remove), batch_size):\n                remove_batch = prefixes_to_remove[i:(i + batch_size)]\n                params = [\n                    threat_list.threat_type,\n                    threat_list.platform_type,\n                    threat_list.threat_entry_type\n                ] + [sqlite3.Binary(b) for b in remove_batch]\n                dbc.execute(q.format(','.join(['?'] * len(remove_batch))), params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports all hash prefix values. Returns a list of known hash prefix values.", "response": "def dump_hash_prefix_values(self):\n        \"\"\"Export all hash prefix values.\n\n        Returns a list of known hash prefix values\n        \"\"\"\n        q = '''SELECT distinct value from hash_prefix'''\n        output = []\n        with self.get_cursor() as dbc:\n            dbc.execute(q)\n            output = [bytes(r[0]) for r in dbc.fetchall()]\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks events in the queue on a given Tk instance", "response": "def _check_events(tk):\n    \"\"\"Checks events in the queue on a given Tk instance\"\"\"\n\n    used = False\n    try:\n        # Process all enqueued events, then exit.\n        while True:\n            try:\n                # Get an event request from the queue.\n                method, args, kwargs, response_queue = tk.tk._event_queue.get_nowait()\n            except queue.Empty:\n                # No more events to process.\n                break\n            else:\n                # Call the event with the given arguments, and then return\n                # the result back to the caller via the response queue.\n                used = True\n                if tk.tk._debug >= 2:\n                    print('Calling event from main thread:', method.__name__, args, kwargs)\n                try:\n                    response_queue.put((False, method(*args, **kwargs)))\n                except SystemExit:\n                    raise  # Raises original SystemExit\n                except Exception:\n                    # Calling the event caused an exception; return the\n                    # exception back to the caller so that it can be raised\n                    # in the caller's thread.\n                    from sys import exc_info  # Python 2 requirement\n                    ex_type, ex_value, ex_tb = exc_info()\n                    response_queue.put((True, (ex_type, ex_value, ex_tb)))\n    finally:\n        # Schedule to check again. If we just processed an event, check\n        # immediately; if we didn't, check later.\n        if used:\n            tk.after_idle(_check_events, tk)\n        else:\n            tk.after(tk.tk._check_period, _check_events, tk)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_hash_prefix_cache(self):\n        try:\n            self.storage.cleanup_full_hashes()\n            self.storage.commit()\n            self._sync_threat_lists()\n            self.storage.commit()\n            self._sync_hash_prefix_cache()\n        except Exception:\n            self.storage.rollback()\n            raise", "response": "Update locally cached threat lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndownload full hashes matching hash_prefixes. Also update cache expiration timestamps.", "response": "def _sync_full_hashes(self, hash_prefixes):\n        \"\"\"Download full hashes matching hash_prefixes.\n\n        Also update cache expiration timestamps.\n        \"\"\"\n        client_state = self.storage.get_client_state()\n        self.api_client.fair_use_delay()\n        fh_response = self.api_client.get_full_hashes(hash_prefixes, client_state)\n\n        # update negative cache for each hash prefix\n        # store full hash (insert or update) with positive cache bumped up\n        for m in fh_response.get('matches', []):\n            threat_list = ThreatList(m['threatType'], m['platformType'], m['threatEntryType'])\n            hash_value = b64decode(m['threat']['hash'])\n            cache_duration = int(m['cacheDuration'].rstrip('s'))\n            malware_threat_type = None\n            for metadata in m['threatEntryMetadata'].get('entries', []):\n                k = b64decode(metadata['key'])\n                v = b64decode(metadata['value'])\n                if k == 'malware_threat_type':\n                    malware_threat_type = v\n            self.storage.store_full_hash(threat_list, hash_value, cache_duration, malware_threat_type)\n\n        negative_cache_duration = int(fh_response['negativeCacheDuration'].rstrip('s'))\n        for prefix_value in hash_prefixes:\n            self.storage.update_hash_prefix_expiration(prefix_value, negative_cache_duration)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lookup_url(self, url):\n        if type(url) is not str:\n            url = url.encode('utf8')\n        if not url.strip():\n            raise ValueError(\"Empty input string.\")\n        url_hashes = URL(url).hashes\n        try:\n            list_names = self._lookup_hashes(url_hashes)\n            self.storage.commit()\n        except Exception:\n            self.storage.rollback()\n            raise\n        if list_names:\n            return list_names\n        return None", "response": "Look up specified URL in Safe Browsing threat lists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking up the full hash entries in blacklists and return the list of lists it was found in.", "response": "def _lookup_hashes(self, full_hashes):\n        \"\"\"Lookup URL hash in blacklists\n\n        Returns names of lists it was found in.\n        \"\"\"\n        full_hashes = list(full_hashes)\n        cues = [fh[0:4] for fh in full_hashes]\n        result = []\n        matching_prefixes = {}\n        matching_full_hashes = set()\n        is_potential_threat = False\n        # First lookup hash prefixes which match full URL hash\n        for (hash_prefix, negative_cache_expired) in self.storage.lookup_hash_prefix(cues):\n            for full_hash in full_hashes:\n                if full_hash.startswith(hash_prefix):\n                    is_potential_threat = True\n                    # consider hash prefix negative cache as expired if it is expired in at least one threat list\n                    matching_prefixes[hash_prefix] = matching_prefixes.get(hash_prefix, False) or negative_cache_expired\n                    matching_full_hashes.add(full_hash)\n        # if none matches, URL hash is clear\n        if not is_potential_threat:\n            return []\n        # if there is non-expired full hash, URL is blacklisted\n        matching_expired_threat_lists = set()\n        for threat_list, has_expired in self.storage.lookup_full_hashes(matching_full_hashes):\n            if has_expired:\n                matching_expired_threat_lists.add(threat_list)\n            else:\n                result.append(threat_list)\n        if result:\n            return result\n\n        # If there are no matching expired full hash entries\n        # and negative cache is still current for all prefixes, consider it safe\n        if len(matching_expired_threat_lists) == 0 and sum(map(int, matching_prefixes.values())) == 0:\n            log.info('Negative cache hit.')\n            return []\n\n        # Now we can assume that there are expired matching full hash entries and/or\n        # cache prefix entries with expired negative cache. Both require full hash sync.\n        self._sync_full_hashes(matching_prefixes.keys())\n        # Now repeat full hash lookup\n        for threat_list, has_expired in self.storage.lookup_full_hashes(matching_full_hashes):\n            if not has_expired:\n                result.append(threat_list)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_threats_lists(self):\n        response = self.service.threatLists().list().execute()\n        self.set_wait_duration(response.get('minimumWaitDuration'))\n        return response['threatLists']", "response": "Retrieve all available threat lists"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_threats_update(self, client_state):\n        request_body = {\n            \"client\": {\n                \"clientId\": self.client_id,\n                \"clientVersion\": self.client_version,\n            },\n            \"listUpdateRequests\": [],\n        }\n        for (threat_type, platform_type, threat_entry_type), current_state in client_state.items():\n            request_body['listUpdateRequests'].append(\n                {\n                    \"threatType\": threat_type,\n                    \"platformType\": platform_type,\n                    \"threatEntryType\": threat_entry_type,\n                    \"state\": current_state,\n                    \"constraints\": {\n                        \"supportedCompressions\": [\"RAW\"]\n                    }\n                }\n            )\n        response = self.service.threatListUpdates().fetch(body=request_body).execute()\n        self.set_wait_duration(response.get('minimumWaitDuration'))\n        return response['listUpdateResponses']", "response": "Fetch hash prefixes update for given threat list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds full hashes matching hash prefixes.", "response": "def get_full_hashes(self, prefixes, client_state):\n        \"\"\"Find full hashes matching hash prefixes.\n\n        client_state is a dict which looks like {(threatType, platformType, threatEntryType): clientState}\n        \"\"\"\n        request_body = {\n            \"client\": {\n                \"clientId\": self.client_id,\n                \"clientVersion\": self.client_version,\n            },\n            \"clientStates\": [],\n            \"threatInfo\": {\n                \"threatTypes\": [],\n                \"platformTypes\": [],\n                \"threatEntryTypes\": [],\n                \"threatEntries\": [],\n            }\n        }\n        for prefix in prefixes:\n            request_body['threatInfo']['threatEntries'].append({\"hash\": b64encode(prefix).decode()})\n        for ((threatType, platformType, threatEntryType), clientState) in client_state.items():\n            request_body['clientStates'].append(clientState)\n            if threatType not in request_body['threatInfo']['threatTypes']:\n                request_body['threatInfo']['threatTypes'].append(threatType)\n            if platformType not in request_body['threatInfo']['platformTypes']:\n                request_body['threatInfo']['platformTypes'].append(platformType)\n            if threatEntryType not in request_body['threatInfo']['threatEntryTypes']:\n                request_body['threatInfo']['threatEntryTypes'].append(threatEntryType)\n        response = self.service.fullHashes().find(body=request_body).execute()\n        self.set_wait_duration(response.get('minimumWaitDuration'))\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator of all possible permutations of the URL in canonical form.", "response": "def hashes(self):\n        \"\"\"Hashes of all possible permutations of the URL in canonical form\"\"\"\n        for url_variant in self.url_permutations(self.canonical):\n            url_hash = self.digest(url_variant)\n            yield url_hash"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts URL to its canonical form.", "response": "def canonical(self):\n        \"\"\"Convert URL to its canonical form.\"\"\"\n        def full_unescape(u):\n            uu = urllib.unquote(u)\n            if uu == u:\n                return uu\n            else:\n                return full_unescape(uu)\n\n        def full_unescape_to_bytes(u):\n            uu = urlparse.unquote_to_bytes(u)\n            if uu == u:\n                return uu\n            else:\n                return full_unescape_to_bytes(uu)\n\n        def quote(s):\n            safe_chars = '!\"$&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'\n            return urllib.quote(s, safe=safe_chars)\n\n        url = self.url.strip()\n        url = url.replace(b'\\n', b'').replace(b'\\r', b'').replace(b'\\t', b'')\n        url = url.split(b'#', 1)[0]\n        if url.startswith(b'//'):\n            url = b'http:' + url\n        if len(url.split(b'://')) <= 1:\n            url = b'http://' + url\n        # at python3 work with bytes instead of string\n        # as URL may contain invalid unicode characters\n        if self.__py3 and type(url) is bytes:\n            url = quote(full_unescape_to_bytes(url))\n        else:\n            url = quote(full_unescape(url))\n        url_parts = urlparse.urlsplit(url)\n        if not url_parts[0]:\n            url = 'http://{}'.format(url)\n            url_parts = urlparse.urlsplit(url)\n        protocol = url_parts.scheme\n        if self.__py3:\n            host = full_unescape_to_bytes(url_parts.hostname)\n            path = full_unescape_to_bytes(url_parts.path)\n        else:\n            host = full_unescape(url_parts.hostname)\n            path = full_unescape(url_parts.path)\n        query = url_parts.query\n        if not query and '?' not in url:\n            query = None\n        if not path:\n            path = b'/'\n        has_trailing_slash = (path[-1:] == b'/')\n        path = posixpath.normpath(path).replace(b'//', b'/')\n        if has_trailing_slash and path[-1:] != b'/':\n            path = path + b'/'\n        port = url_parts.port\n        host = host.strip(b'.')\n        host = re.sub(br'\\.+', b'.', host).lower()\n        if host.isdigit():\n            try:\n                host = socket.inet_ntoa(struct.pack(\"!I\", int(host)))\n            except Exception:\n                pass\n        elif host.startswith(b'0x') and b'.' not in host:\n            try:\n                host = socket.inet_ntoa(struct.pack(\"!I\", int(host, 16)))\n            except Exception:\n                pass\n        quoted_path = quote(path)\n        quoted_host = quote(host)\n        if port is not None:\n            quoted_host = '{}:{}'.format(quoted_host, port)\n        canonical_url = '{}://{}{}'.format(protocol, quoted_host, quoted_path)\n        if query is not None:\n            canonical_url = '{}?{}'.format(canonical_url, query)\n        return canonical_url"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry all permutations of hostname and path which can be applied to blacklisted URLs", "response": "def url_permutations(url):\n        \"\"\"Try all permutations of hostname and path which can be applied\n\n        to blacklisted URLs\n        \"\"\"\n        def url_host_permutations(host):\n            if re.match(r'\\d+\\.\\d+\\.\\d+\\.\\d+', host):\n                yield host\n                return\n            parts = host.split('.')\n            l = min(len(parts), 5)\n            if l > 4:\n                yield host\n            for i in range(l - 1):\n                yield '.'.join(parts[i - l:])\n\n        def url_path_permutations(path):\n            yield path\n            query = None\n            if '?' in path:\n                path, query = path.split('?', 1)\n            if query is not None:\n                yield path\n            path_parts = path.split('/')[0:-1]\n            curr_path = ''\n            for i in range(min(4, len(path_parts))):\n                curr_path = curr_path + path_parts[i] + '/'\n                yield curr_path\n\n        protocol, address_str = urllib.splittype(url)\n        host, path = urllib.splithost(address_str)\n        user, host = urllib.splituser(host)\n        host, port = urllib.splitport(host)\n        host = host.strip('/')\n        seen_permutations = set()\n        for h in url_host_permutations(host):\n            for p in url_path_permutations(path):\n                u = '{}{}'.format(h, p)\n                if u not in seen_permutations:\n                    yield u\n                    seen_permutations.add(u)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompare two version strings and return - 1 0 or 1 depending on the equality of the version numbers.", "response": "def _compare_versions(v1, v2):\n    \"\"\"\n    Compare two version strings and return -1, 0 or 1 depending on the equality\n    of the subset of matching version numbers.\n\n    The implementation is inspired by the top answer at\n    http://stackoverflow.com/a/1714190/997768.\n    \"\"\"\n    def normalize(v):\n        # strip trailing .0 or .00 or .0.0 or ...\n        v = re.sub(r'(\\.0+)*$', '', v)\n        result = []\n        for part in v.split('.'):\n            # just digits\n            m = re.match(r'^(\\d+)$', part)\n            if m:\n                result.append(int(m.group(1)))\n                continue\n            # digits letters\n            m = re.match(r'^(\\d+)([a-zA-Z]+)$', part)\n            if m:\n                result.append(int(m.group(1)))\n                result.append(m.group(2))\n                continue\n            # digits letters digits\n            m = re.match(r'^(\\d+)([a-zA-Z]+)(\\d+)$', part)\n            if m:\n                result.append(int(m.group(1)))\n                result.append(m.group(2))\n                result.append(int(m.group(3)))\n                continue\n        return tuple(result)\n\n    n1 = normalize(v1)\n    n2 = normalize(v2)\n\n    return (n1 > n2) - (n1 < n2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting version specifier into a tuple of 2 and 1.", "response": "def _split_version_specifier(spec):\n    \"\"\"Splits version specifiers in the form \">= 0.1.2\" into ('0.1.2', '>=')\"\"\"\n    m = re.search(r'([<>=]?=?)?\\s*([0-9.a-zA-Z]+)', spec)\n    return m.group(2), m.group(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exists(package):\n    pkg_config_exe = os.environ.get('PKG_CONFIG', None) or 'pkg-config'\n    cmd = '{0} --exists {1}'.format(pkg_config_exe, package).split()\n    return call(cmd) == 0", "response": "Check if package information is available."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef libs(package, static=False):\n    _raise_if_not_exists(package)\n    return _query(package, *_build_options('--libs', static=static))", "response": "Return the LDFLAGS string returned by pkg - config."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dictionary of all the variables defined in the. pc pkg - config file of package.", "response": "def variables(package):\n    \"\"\"\n    Return a dictionary of all the variables defined in the .pc pkg-config file\n    of 'package'.\n    \"\"\"\n    _raise_if_not_exists(package)\n    result = _query(package, '--print-variables')\n    names = (x.strip() for x in result.split('\\n') if x != '')\n    return dict(((x, _query(package, '--variable={0}'.format(x)).strip()) for x in names))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the package meets the required version.", "response": "def installed(package, version):\n    \"\"\"\n    Check if the package meets the required version.\n\n    The version specifier consists of an optional comparator (one of =, ==, >,\n    <, >=, <=) and an arbitrarily long version number separated by dots. The\n    should be as you would expect, e.g. for an installed version '0.1.2' of\n    package 'foo':\n\n    >>> installed('foo', '==0.1.2')\n    True\n    >>> installed('foo', '<0.1')\n    False\n    >>> installed('foo', '>= 0.0.4')\n    True\n\n    If ``pkg-config`` not on path, raises ``EnvironmentError``.\n    \"\"\"\n    if not exists(package):\n        return False\n\n    number, comparator = _split_version_specifier(version)\n    modversion = _query(package, '--modversion')\n\n    try:\n        result = _compare_versions(modversion, number)\n    except ValueError:\n        msg = \"{0} is not a correct version specifier\".format(version)\n        raise ValueError(msg)\n\n    if comparator in ('', '=', '=='):\n        return result == 0\n\n    if comparator == '>':\n        return result > 0\n\n    if comparator == '>=':\n        return result >= 0\n\n    if comparator == '<':\n        return result < 0\n\n    if comparator == '<=':\n        return result <= 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(packages, static=False):\n    for package in packages.split():\n        _raise_if_not_exists(package)\n\n    out = _query(packages, *_build_options('--cflags --libs', static=static))\n    out = out.replace('\\\\\"', '')\n    result = collections.defaultdict(list)\n\n    for token in re.split(r'(?<!\\\\) ', out):\n        key = _PARSE_MAP.get(token[:2])\n        if key:\n            result[key].append(token[2:].strip())\n\n    def split(m):\n        t = tuple(m.split('='))\n        return t if len(t) > 1 else (t[0], None)\n\n    result['define_macros'] = [split(m) for m in result['define_macros']]\n\n    # only have members with values not being the empty list (which is default\n    # anyway):\n    return collections.defaultdict(list, ((k, v) for k, v in result.items() if v))", "response": "Parse the output from pkg - config about the passed package or packages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend the data over UDP while taking the sample_rate in account", "response": "def send(self, data, sample_rate=None):\n        '''Send the data over UDP while taking the sample_rate in account\n\n        The sample rate should be a number between `0` and `1` which indicates\n        the probability that a message will be sent. The sample_rate is also\n        communicated to `statsd` so it knows what multiplier to use.\n\n        :keyword data: The data to send\n        :type data: dict\n        :keyword sample_rate: The sample rate, defaults to `1` (meaning always)\n        :type sample_rate: int\n        '''\n        if self._disabled:\n            self.logger.debug('Connection disabled, not sending data')\n            return False\n        if sample_rate is None:\n            sample_rate = self._sample_rate\n\n        sampled_data = {}\n        if sample_rate < 1:\n            if random.random() <= sample_rate:\n                # Modify the data so statsd knows our sample_rate\n                for stat, value in compat.iter_dict(data):\n                    sampled_data[stat] = '%s|@%s' % (data[stat], sample_rate)\n        else:\n            sampled_data = data\n\n        try:\n            for stat, value in compat.iter_dict(sampled_data):\n                send_data = ('%s:%s' % (stat, value)).encode(\"utf-8\")\n                self.udp_sock.send(send_data)\n            return True\n        except Exception as e:\n            self.logger.exception('unexpected error %r while sending data', e)\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the timer and store the start time", "response": "def start(self):\n        '''Start the timer and store the start time, this can only be executed\n        once per instance\n\n        It returns the timer instance so it can be chained when instantiating\n        the timer instance like this:\n        ``timer = Timer('application_name').start()``'''\n        assert self._start is None, (\n            'Unable to start, the timer is already running')\n        self._last = self._start = time.time()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, subname, delta):\n        '''Send the data to statsd via self.connection\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword delta: The time delta (time.time() - time.time()) to report\n        :type delta: float\n        '''\n        ms = delta * 1000\n        if ms > self.min_send_threshold:\n            name = self._get_name(self.name, subname)\n            self.logger.info('%s: %0.08fms', name, ms)\n            return statsd.Client._send(self, {name: '%0.08f|ms' % ms})\n        else:\n            return True", "response": "Send the data to statsd via self. connection\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef intermediate(self, subname):\n        '''Send the time that has passed since our last measurement\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        '''\n        t = time.time()\n        response = self.send(subname, t - self._last)\n        self._last = t\n        return response", "response": "Send the time that has passed since our last measurement\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstops the timer and send the total since start was run", "response": "def stop(self, subname='total'):\n        '''Stop the timer and send the total since `start()` was run\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        '''\n        assert self._stop is None, (\n            'Unable to stop, the timer is already stopped')\n        self._stop = time.time()\n        return self.send(subname, self._stop - self._start)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decorate(self, function_or_name):\n        '''Decorate a function to time the execution\n\n        The method can be called with or without a name. If no name is given\n        the function defaults to the name of the function.\n\n        :keyword function_or_name: The name to post to or the function to wrap\n\n        >>> from statsd import Timer\n        >>> timer = Timer('application_name')\n        >>>\n        >>> @timer.decorate\n        ... def some_function():\n        ...     # resulting timer name: application_name.some_function\n        ...     pass\n        >>>\n        >>> @timer.decorate('my_timer')\n        ... def some_other_function():\n        ...     # resulting timer name: application_name.my_timer\n        ...     pass\n\n        '''\n        if callable(function_or_name):\n            return self._decorate(function_or_name.__name__, function_or_name)\n        else:\n            return partial(self._decorate, function_or_name)", "response": "Decorate a function to time the execution\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a context manager to time execution of a block of code.", "response": "def time(self, subname=None, class_=None):\n        '''Returns a context manager to time execution of a block of code.\n\n        :keyword subname: The subname to report data to\n        :type subname: str\n        :keyword class_: The :class:`~statsd.client.Client` subclass to use\n            (e.g. :class:`~statsd.timer.Timer` or\n            :class:`~statsd.counter.Counter`)\n        :type class_: :class:`~statsd.client.Client`\n\n        >>> from statsd import Timer\n        >>> timer = Timer('application_name')\n        >>>\n        >>> with timer.time():\n        ...     # resulting timer name: application_name\n        ...     pass\n        >>>\n        >>>\n        >>> with timer.time('context_timer'):\n        ...     # resulting timer name: application_name.context_timer\n        ...     pass\n\n        '''\n        if class_ is None:\n            class_ = Timer\n        timer = self.get_client(subname, class_)\n        timer.start()\n        yield\n        timer.stop('')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending the data to statsd via self. connection", "response": "def send(self, subname, value, timestamp=None):\n        '''Send the data to statsd via self.connection\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword value: The raw value to send\n        '''\n        if timestamp is None:\n            ts = int(dt.datetime.now().strftime(\"%s\"))\n        else:\n            ts = timestamp\n        name = self._get_name(self.name, subname)\n        self.logger.info('%s: %s %s' % (name, value, ts))\n        return statsd.Client._send(self, {name: '%s|r|%s' % (value, ts)})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _send(self, subname, value):\n        '''Send the data to statsd via self.connection\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword value: The gauge value to send\n        '''\n        name = self._get_name(self.name, subname)\n        self.logger.info('%s: %s', name, value)\n        return statsd.Client._send(self, {name: '%s|g' % value})", "response": "Send the data to statsd via self. connection\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending the data to statsd via self. connection", "response": "def send(self, subname, value):\n        '''Send the data to statsd via self.connection\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword value: The gauge value to send\n        '''\n        assert isinstance(value, compat.NUM_TYPES)\n        return self._send(subname, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef increment(self, subname=None, delta=1):\n        '''Increment the gauge with `delta`\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword delta: The delta to add to the gauge\n        :type delta: int\n\n        >>> gauge = Gauge('application_name')\n        >>> gauge.increment('gauge_name', 10)\n        True\n        >>> gauge.increment(delta=10)\n        True\n        >>> gauge.increment('gauge_name')\n        True\n        '''\n        delta = int(delta)\n        sign = \"+\" if delta >= 0 else \"\"\n        return self._send(subname, \"%s%d\" % (sign, delta))", "response": "Increment the gauge with delta"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decrement(self, subname=None, delta=1):\n        '''Decrement the gauge with `delta`\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword delta: The delta to remove from the gauge\n        :type delta: int\n\n        >>> gauge = Gauge('application_name')\n        >>> gauge.decrement('gauge_name', 10)\n        True\n        >>> gauge.decrement(delta=10)\n        True\n        >>> gauge.decrement('gauge_name')\n        True\n        '''\n        delta = -int(delta)\n        sign = \"+\" if delta >= 0 else \"\"\n        return self._send(subname, \"%s%d\" % (sign, delta))", "response": "Decrement the gauge with delta"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set(self, subname, value):\n        '''\n        Set the data ignoring the sign, ie set(\"test\", -1) will set \"test\"\n        exactly to -1 (not decrement it by 1)\n\n        See https://github.com/etsy/statsd/blob/master/docs/metric_types.md\n        \"Adding a sign to the gauge value will change the value, rather\n        than setting it.\n\n            gaugor:-10|g\n            gaugor:+4|g\n\n        So if gaugor was 333, those commands would set it to 333 - 10 + 4, or\n        327.\n\n        Note: This implies you can't explicitly set a gauge to a negative\n        number without first setting it to zero.\"\n\n        :keyword subname: The subname to report the data to (appended to the\n            client name)\n        :type subname: str\n        :keyword value: The new gauge value\n        '''\n\n        assert isinstance(value, compat.NUM_TYPES)\n        if value < 0:\n            self._send(subname, 0)\n        return self._send(subname, value)", "response": "Set the value of the gauge entry in the specified subname."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a sub - client with a separate namespace", "response": "def get_client(self, name=None, class_=None):\n        '''Get a (sub-)client with a separate namespace\n        This way you can create a global/app based client with subclients\n        per class/function\n\n        :keyword name: The name to use, if the name for this client was `spam`\n            and the `name` argument is `eggs` than the resulting name will be\n            `spam.eggs`\n        :type name: str\n        :keyword class_: The :class:`~statsd.client.Client` subclass to use\n            (e.g. :class:`~statsd.timer.Timer` or\n            :class:`~statsd.counter.Counter`)\n        :type class_: :class:`~statsd.client.Client`\n        '''\n\n        # If the name was given, use it. Otherwise simply clone\n        name = self._get_name(self.name, name)\n\n        # Create using the given class, or the current class\n        if not class_:\n            class_ = self.__class__\n\n        return class_(\n            name=name,\n            connection=self.connection,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_average(self, name=None):\n        '''Shortcut for getting an :class:`~statsd.average.Average` instance\n\n        :keyword name: See :func:`~statsd.client.Client.get_client`\n        :type name: str\n        '''\n        return self.get_client(name=name, class_=statsd.Average)", "response": "Shortcut for getting an average. Average instance"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef init_git_version(cls, v_str):\n        if v_str is None:\n            cls._git_version = None\n            return\n\n        v_str = v_str.strip()\n        try:\n            version = cls._git_version = tuple(\n                int(x) for x in v_str.split()[2].split('.')[:3])\n        except Exception:\n            raise ValueError(\"Could not parse git version output %r. Please \"\n                             \"report this\" % v_str)\n        return version", "response": "Parse and store the parsed version tuple on self."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying remote repo about given ref.", "response": "def query_remote_ref(self, remote, ref):\n        \"\"\"Query remote repo about given ref.\n        :return: ``('tag', sha)`` if ref is a tag in remote\n                 ``('branch', sha)`` if ref is branch (aka \"head\") in remote\n                 ``(None, ref)`` if ref does not exist in remote. This happens\n                 notably if ref if a commit sha (they can't be queried)\n        \"\"\"\n        out = self.log_call(['git', 'ls-remote', remote, ref],\n                            cwd=self.cwd,\n                            callwith=subprocess.check_output).strip()\n        for sha, fullref in (l.split() for l in out.splitlines()):\n            if fullref == 'refs/heads/' + ref:\n                return 'branch', sha\n            elif fullref == 'refs/tags/' + ref:\n                return 'tag', sha\n            elif fullref == ref and ref == 'HEAD':\n                return 'HEAD', sha\n        return None, ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps a subprocess call with logging.", "response": "def log_call(self, cmd, callwith=subprocess.check_call,\n                 log_level=logging.DEBUG, **kw):\n        \"\"\"Wrap a subprocess call with logging\n        :param meth: the calling method to use.\n        \"\"\"\n        logger.log(log_level, \"%s> call %r\", self.cwd, cmd)\n        ret = callwith(cmd, **kw)\n        if callwith == subprocess.check_output:\n            ret = console_to_str(ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggregate(self):\n        logger.info('Start aggregation of %s', self.cwd)\n        target_dir = self.cwd\n\n        is_new = not os.path.exists(target_dir)\n        if is_new:\n            self.init_repository(target_dir)\n\n        self._switch_to_branch(self.target['branch'])\n        for r in self.remotes:\n            self._set_remote(**r)\n        self.fetch()\n        merges = self.merges\n        if not is_new:\n            # reset to the first merge\n            origin = merges[0]\n            merges = merges[1:]\n            self._reset_to(origin[\"remote\"], origin[\"ref\"])\n        for merge in merges:\n            self._merge(merge)\n        self._execute_shell_command_after()\n        logger.info('End aggregation of %s', self.cwd)", "response": "Aggregate all merges into the target branch\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks repo status and raise exception if dirty.", "response": "def _check_status(self):\n        \"\"\"Check repo status and except if dirty.\"\"\"\n        logger.info('Checking repo status')\n        status = self.log_call(\n            ['git', 'status', '--porcelain'],\n            callwith=subprocess.check_output,\n            cwd=self.cwd,\n        )\n        if status:\n            raise DirtyException(status)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fetch_options(self, merge):\n        cmd = tuple()\n        for option in FETCH_DEFAULTS:\n            value = merge.get(option, self.defaults.get(option))\n            if value:\n                cmd += (\"--%s\" % option, str(value))\n        return cmd", "response": "Get the fetch options from the given merge dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_remote(self, name, url):\n        remotes = self._get_remotes()\n        exising_url = remotes.get(name)\n        if exising_url == url:\n            logger.info('Remote already exists %s <%s>', name, url)\n            return\n        if not exising_url:\n            logger.info('Adding remote %s <%s>', name, url)\n            self.log_call(['git', 'remote', 'add', name, url], cwd=self.cwd)\n        else:\n            logger.info('Remote remote %s <%s> -> <%s>',\n                        name, exising_url, url)\n            self.log_call(['git', 'remote', 'rm', name], cwd=self.cwd)\n            self.log_call(['git', 'remote', 'add', name, url], cwd=self.cwd)", "response": "Add remote to the repository. It s equivalent to the command\n        git remote add <name > url"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting all pending merge PRs info.", "response": "def collect_prs_info(self):\n        \"\"\"Collect all pending merge PRs info.\n\n        :returns: mapping of PRs by state\n        \"\"\"\n        REPO_RE = re.compile(\n            '^(https://github.com/|git@github.com:)'\n            '(?P<owner>.*?)/(?P<repo>.*?)(.git)?$')\n        PULL_RE = re.compile(\n            '^(refs/)?pull/(?P<pr>[0-9]+)/head$')\n        remotes = {r['name']: r['url'] for r in self.remotes}\n        all_prs = {}\n        for merge in self.merges:\n            remote = merge['remote']\n            ref = merge['ref']\n            repo_url = remotes[remote]\n            repo_mo = REPO_RE.match(repo_url)\n            if not repo_mo:\n                logger.debug('%s is not a github repo', repo_url)\n                continue\n            pull_mo = PULL_RE.match(ref)\n            if not pull_mo:\n                logger.debug('%s is not a github pull reqeust', ref)\n                continue\n            pr_info = {\n                'owner': repo_mo.group('owner'),\n                'repo': repo_mo.group('repo'),\n                'pr': pull_mo.group('pr'),\n            }\n            pr_info['path'] = '{owner}/{repo}/pulls/{pr}'.format(**pr_info)\n            pr_info['url'] = 'https://github.com/{path}'.format(**pr_info)\n            pr_info['shortcut'] = '{owner}/{repo}#{pr}'.format(**pr_info)\n            r = self._github_api_get('/repos/{path}'.format(**pr_info))\n            if r.status_code != 200:\n                logger.warning(\n                    'Could not get status of {path}. '\n                    'Reason: {r.status_code} {r.reason}'.format(r=r, **pr_info)\n                )\n                continue\n            pr_info['state'] = r.json().get('state')\n            pr_info['merged'] = (\n                not r.json().get('merged') and 'not ' or ''\n            ) + 'merged'\n            all_prs.setdefault(pr_info['state'], []).append(pr_info)\n        return all_prs"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show_closed_prs(self):\n        all_prs = self.collect_prs_info()\n        for pr_info in all_prs.get('closed', []):\n            logger.info(\n                '{url} in state {state} ({merged})'.format(**pr_info)\n            )", "response": "Log only closed PRs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging all PRs grouped by state.", "response": "def show_all_prs(self):\n        \"\"\"Log all PRs grouped by state.\"\"\"\n        for __, prs in self.collect_prs_info().items():\n            for pr_info in prs:\n                logger.info(\n                    '{url} in state {state} ({merged})'.format(**pr_info)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_repos(config, force=False):\n    repo_list = []\n    for directory, repo_data in config.items():\n        if not os.path.isabs(directory):\n            directory = os.path.abspath(directory)\n        repo_dict = {\n            'cwd': directory,\n            'defaults': repo_data.get('defaults', dict()),\n            'force': force,\n        }\n        remote_names = set()\n        if 'remotes' in repo_data:\n            repo_dict['remotes'] = []\n            remotes_data = repo_data['remotes'] or {}\n            for remote_name, url in remotes_data.items():\n                if not url:\n                    raise ConfigException(\n                        '%s: No url defined for remote %s.' %\n                        (directory, remote_name))\n                remote_dict = {\n                    'name': remote_name,\n                    'url': url\n                }\n                repo_dict['remotes'].append(remote_dict)\n                remote_names.add(remote_name)\n            if not remote_names:\n                raise ConfigException(\n                    '%s: You should at least define one remote.' % directory)\n        else:\n            raise ConfigException('%s: remotes is not defined.' % directory)\n        if 'merges' in repo_data:\n            merges = []\n            merge_data = repo_data.get('merges') or []\n            for merge in merge_data:\n                try:\n                    # Assume parts is a str\n                    parts = merge.split(' ')\n                    if len(parts) != 2:\n                        raise ConfigException(\n                            '%s: Merge must be formatted as '\n                            '\"remote_name ref\".' % directory)\n                    merge = {\n                        \"remote\": parts[0],\n                        \"ref\": parts[1],\n                    }\n                except AttributeError:\n                    # Parts is a dict\n                    try:\n                        merge[\"remote\"] = str(merge[\"remote\"])\n                        merge[\"ref\"] = str(merge[\"ref\"])\n                    except KeyError:\n                        raise ConfigException(\n                            '%s: Merge lacks mandatory '\n                            '`remote` or `ref` keys.' % directory)\n                # Check remote is available\n                if merge[\"remote\"] not in remote_names:\n                    raise ConfigException(\n                        '%s: Merge remote %s not defined in remotes.' %\n                        (directory, merge[\"remote\"]))\n                merges.append(merge)\n            repo_dict['merges'] = merges\n            if not merges:\n                raise ConfigException(\n                    '%s: You should at least define one merge.' % directory)\n        else:\n            raise ConfigException(\n                '%s: merges is not defined.' % directory)\n        # Only fetch required remotes by default\n        repo_dict[\"fetch_all\"] = repo_data.get(\"fetch_all\", False)\n        if isinstance(repo_dict[\"fetch_all\"], string_types):\n            repo_dict[\"fetch_all\"] = frozenset((repo_dict[\"fetch_all\"],))\n        elif isinstance(repo_dict[\"fetch_all\"], list):\n            repo_dict[\"fetch_all\"] = frozenset(repo_dict[\"fetch_all\"])\n        if 'target' not in repo_data:\n            raise ConfigException('%s: No target defined.' % directory)\n        parts = (repo_data.get('target') or \"\") .split(' ')\n        if len(parts) != 2:\n            raise ConfigException(\n                '%s: Target must be formatted as '\n                '\"remote_name branch_name\"' % directory)\n\n        remote_name, branch = repo_data.get('target').split(' ')\n        if remote_name not in remote_names:\n            raise ConfigException(\n                '%s: Target remote %s not defined in remotes.' %\n                (directory, remote_name))\n        repo_dict['target'] = {\n            'remote': remote_name,\n            'branch': branch,\n        }\n        commands = []\n        if 'shell_command_after' in repo_data:\n            cmds = repo_data['shell_command_after']\n            # if str: turn to list\n            if cmds:\n                if isinstance(cmds, string_types):\n                    cmds = [cmds]\n                commands = cmds\n        repo_dict['shell_command_after'] = commands\n        repo_list.append(repo_dict)\n    return repo_list", "response": "Return a list of repos from a config file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_config(config, expand_env=False, force=False):\n    if not os.path.exists(config):\n        raise ConfigException('Unable to find configuration file: %s' % config)\n\n    file_extension = os.path.splitext(config)[1][1:]\n    conf = kaptan.Kaptan(handler=kaptan.HANDLER_EXT.get(file_extension))\n\n    if expand_env:\n        with open(config, 'r') as file_handler:\n            config = Template(file_handler.read())\n            config = config.substitute(os.environ)\n\n    conf.import_config(config)\n    return get_repos(conf.export('dict') or {}, force)", "response": "Load a config file and return repos."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_logger(log=None, level=logging.INFO):\n    if not log:\n        log = logging.getLogger()\n    if not log.handlers:\n        channel = logging.StreamHandler()\n        if level == logging.DEBUG:\n            channel.setFormatter(DebugLogFormatter())\n        else:\n            channel.setFormatter(LogFormatter())\n\n        log.setLevel(level)\n        log.addHandler(channel)", "response": "Setup logging for CLI use."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parser():\n\n    main_parser = argparse.ArgumentParser(\n        formatter_class=argparse.RawTextHelpFormatter)\n\n    main_parser.add_argument(\n        '-c', '--config',\n        dest='config',\n        type=str,\n        nargs='?',\n        help='Pull the latest repositories from config(s)'\n    ).completer = argcomplete.completers.FilesCompleter(\n        allowednames=('.yaml', '.yml', '.json'), directories=False\n    )\n\n    main_parser.add_argument(\n        '-p', '--push',\n        dest='do_push',\n        action='store_true', default=False,\n        help='Push result to target',\n    )\n\n    main_parser.add_argument(\n        '-d', '--dirmatch',\n        dest='dirmatch',\n        type=str,\n        nargs='?',\n        help='Pull only from the directories. Accepts fnmatch(1)'\n             'by commands'\n    )\n    main_parser.add_argument(\n        '--log-level',\n        default='INFO',\n        dest='log_level',\n        type=_log_level_string_to_int,\n        nargs='?',\n        help='Set the logging output level. {0}'.format(_LOG_LEVEL_STRINGS))\n\n    main_parser.add_argument(\n        '-e', '--expand-env',\n        dest='expand_env',\n        default=False,\n        action='store_true',\n        help='Expand environment variables in configuration file',\n    )\n    main_parser.add_argument(\n        '-f', '--force',\n        dest='force',\n        default=False,\n        action='store_true',\n        help='Force cleanup and aggregation on dirty repositories.',\n    )\n\n    main_parser.add_argument(\n        '-j', '--jobs',\n        dest='jobs',\n        default=1,\n        type=int,\n        help='Amount of processes to use when aggregating repos. '\n             'This is useful when there are a lot of large repos. '\n             'Set `1` or less to disable multiprocessing (default).',\n    )\n\n    main_parser.add_argument(\n        'command',\n        nargs='?',\n        default='aggregate',\n        help='aggregate (default): run the aggregation process.\\n'\n             'show-all-prs: show GitHub pull requests in merge sections\\n'\n             '              such pull requests are indentified as having\\n'\n             '              a github.com remote and a\\n'\n             '              refs/pull/NNN/head ref in the merge section.\\n'\n             'show-closed-prs: show pull requests that are not open anymore.\\n'\n    )\n\n    return main_parser", "response": "Return an argument parser for the CLI."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads YAML and JSON configs and begin creating or updating aggregating and pushing the repos", "response": "def load_aggregate(args):\n    \"\"\"Load YAML and JSON configs and begin creating / updating , aggregating\n    and pushing the repos (deprecated in favor or run())\"\"\"\n    repos = load_config(args.config, args.expand_env)\n    dirmatch = args.dirmatch\n    for repo_dict in repos:\n        r = Repo(**repo_dict)\n        logger.debug('%s' % r)\n        if not match_dir(r.cwd, dirmatch):\n            logger.info(\"Skip %s\", r.cwd)\n            continue\n        r.aggregate()\n        if args.do_push:\n            r.push()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\naggregate one repository according to the args.", "response": "def aggregate_repo(repo, args, sem, err_queue):\n    \"\"\"Aggregate one repo according to the args.\n\n    Args:\n         repo (Repo): The repository to aggregate.\n         args (argparse.Namespace): CLI arguments.\n    \"\"\"\n    try:\n        logger.debug('%s' % repo)\n        dirmatch = args.dirmatch\n        if not match_dir(repo.cwd, dirmatch):\n            logger.info(\"Skip %s\", repo.cwd)\n            return\n        if args.command == 'aggregate':\n            repo.aggregate()\n            if args.do_push:\n                repo.push()\n        elif args.command == 'show-closed-prs':\n            repo.show_closed_prs()\n        elif args.command == 'show-all-prs':\n            repo.show_all_prs()\n    except Exception:\n        err_queue.put_nowait(sys.exc_info())\n    finally:\n        sem.release()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(args):\n\n    repos = load_config(args.config, args.expand_env, args.force)\n\n    jobs = max(args.jobs, 1)\n    threads = []\n    sem = threading.Semaphore(jobs)\n    err_queue = Queue()\n\n    for repo_dict in repos:\n        if not err_queue.empty():\n            break\n\n        sem.acquire()\n        r = Repo(**repo_dict)\n        tname = os.path.basename(repo_dict['cwd'])\n\n        if jobs > 1:\n            t = threading.Thread(\n                target=aggregate_repo, args=(r, args, sem, err_queue))\n            t.daemon = True\n            t.name = tname\n            threads.append(t)\n            t.start()\n        else:\n            with ThreadNameKeeper():\n                threading.current_thread().name = tname\n                aggregate_repo(r, args, sem, err_queue)\n\n    for t in threads:\n        t.join()\n\n    if not err_queue.empty():\n        while True:\n            try:\n                exc_type, exc_obj, exc_trace = err_queue.get_nowait()\n            except EmptyQueue:\n                break\n            traceback.print_exception(exc_type, exc_obj, exc_trace)\n        sys.exit(1)", "response": "Load YAML and JSON configs and run the command specified\n    in args. command"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_no_error(\n    state, incorrect_msg=\"Your code generated an error. Fix it and try again!\"\n):\n    \"\"\"Check whether the submission did not generate a runtime error.\n\n    Simply use ``Ex().has_no_error()`` in your SCT whenever you want to check for errors.\n    By default, after the entire SCT finished executing, ``sqlwhat`` will check\n    for errors before marking the exercise as correct. You can disable this behavior\n    by using ``Ex().allow_error()``.\n\n    Args:\n        incorrect_msg: If specified, this overrides the automatically generated feedback message\n                       in case the student's query did not return a result.\n    \"\"\"\n\n    if state.reporter.get_errors():\n        state.do_test(incorrect_msg)\n\n    return state", "response": "Check whether the submission did not generate an error."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the student s query returned a result.", "response": "def has_result(state, incorrect_msg=\"Your query did not return a result.\"):\n    \"\"\"Checks if the student's query returned a result.\n    \n    Args:\n        incorrect_msg: If specified, this overrides the automatically generated feedback message\n                       in case the student's query did not return a result.\n    \"\"\"\n\n    # first check if there is no error\n    has_no_error(state)\n\n    if not state.solution_result:\n        raise NameError(\n            \"You are using has_result() to verify that the student query generated an error, but the solution query did not return a result either!\"\n        )\n\n    if not state.student_result:\n        state.do_test(incorrect_msg)\n\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest whether the student and solution query results have equal numbers of rows.", "response": "def has_nrows(\n    state,\n    incorrect_msg=\"Your query returned a table with {{n_stu}} row{{'s' if n_stu > 1 else ''}} while it should return a table with {{n_sol}} row{{'s' if n_sol > 1 else ''}}.\",\n):\n    \"\"\"Test whether the student and solution query results have equal numbers of rows.\n    \n    Args:\n        incorrect_msg: If specified, this overrides the automatically generated feedback message\n                       in case the number of rows in the student and solution query don't match.\n    \"\"\"\n\n    # check that query returned something\n    has_result(state)\n\n    # assumes that columns cannot be jagged in size\n    n_stu = len(next(iter(state.student_result.values())))\n    n_sol = len(next(iter(state.solution_result.values())))\n\n    if n_stu != n_sol:\n        _msg = state.build_message(\n            incorrect_msg, fmt_kwargs={\"n_stu\": n_stu, \"n_sol\": n_sol}\n        )\n        state.do_test(_msg)\n\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntests whether the student and solution query results have equal numbers of columns.", "response": "def has_ncols(\n    state,\n    incorrect_msg=\"Your query returned a table with {{n_stu}} column{{'s' if n_stu > 1 else ''}} while it should return a table with {{n_sol}} column{{'s' if n_sol > 1 else ''}}.\",\n):\n    \"\"\"Test whether the student and solution query results have equal numbers of columns.\n\n    Args:\n        incorrect_msg: If specified, this overrides the automatically generated feedback message\n                       in case the number of columns in the student and solution query don't match.\n\n    :Example:\n\n        Consider the following solution and SCT: ::\n\n            # solution\n            SELECT artist_id as id, name FROM artists\n\n            # sct\n            Ex().has_ncols()\n\n            # passing submission\n            SELECT artist_id as id, name FROM artists\n\n            # failing submission (too little columns)\n            SELECT artist_id as id FROM artists\n\n            # passing submission (two columns, even though not correct ones)\n            SELECT artist_id, label FROM artists\n\n    \"\"\"\n\n    # check that query returned something\n    has_result(state)\n\n    n_stu = len(state.student_result)\n    n_sol = len(state.solution_result)\n\n    if n_stu != n_sol:\n        _msg = state.build_message(\n            incorrect_msg, fmt_kwargs={\"n_stu\": n_stu, \"n_sol\": n_sol}\n        )\n        state.do_test(_msg)\n\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that a particular row in the query result is represented as a single - row query result.", "response": "def check_row(state, index, missing_msg=None, expand_msg=None):\n    \"\"\"Zoom in on a particular row in the query result, by index.\n\n    After zooming in on a row, which is represented as a single-row query result,\n    you can use ``has_equal_value()`` to verify whether all columns in the zoomed in solution\n    query result have a match in the student query result.\n\n    Args:\n        index: index of the row to zoom in on (zero-based indexed).\n        missing_msg: if specified, this overrides the automatically generated feedback\n                     message in case the row is missing in the student query result.\n        expand_msg: if specified, this overrides the automatically generated feedback \n                    message that is prepended to feedback messages that are thrown\n                    further in the SCT chain.\n\n    :Example:\n\n        Suppose we are testing the following SELECT statements\n\n        * solution: ``SELECT artist_id as id, name FROM artists LIMIT 5``\n        * student : ``SELECT artist_id, name       FROM artists LIMIT 2``\n\n        We can write the following SCTs: ::\n\n            # fails, since row 3 at index 2 is not in the student result\n            Ex().check_row(2)\n\n            # passes, since row 2 at index 1 is in the student result\n            Ex().check_row(0)\n\n    \"\"\"\n\n    if missing_msg is None:\n        missing_msg = \"The system wants to verify row {{index + 1}} of your query result, but couldn't find it. Have another look.\"\n    if expand_msg is None:\n        expand_msg = \"Have another look at row {{index + 1}} in your query result. \"\n    msg_kwargs = {\"index\": index}\n\n    # check that query returned something\n    has_result(state)\n\n    stu_res = state.student_result\n    sol_res = state.solution_result\n\n    n_sol = len(next(iter(sol_res.values())))\n    n_stu = len(next(iter(stu_res.values())))\n\n    if index >= n_sol:\n        raise BaseException(\n            \"There are only {} rows in the solution query result, and you're trying to fetch the row at index {}\".format(\n                n_sol, index\n            )\n        )\n\n    if index >= n_stu:\n        _msg = state.build_message(missing_msg, fmt_kwargs=msg_kwargs)\n        state.do_test(_msg)\n\n    return state.to_child(\n        append_message={\"msg\": expand_msg, \"kwargs\": msg_kwargs},\n        student_result={k: [v[index]] for k, v in stu_res.items()},\n        solution_result={k: [v[index]] for k, v in sol_res.items()},\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_column(state, name, missing_msg=None, expand_msg=None):\n\n    if missing_msg is None:\n        missing_msg = \"We expected to find a column named `{{name}}` in the result of your query, but couldn't.\"\n    if expand_msg is None:\n        expand_msg = \"Have another look at your query result. \"\n    msg_kwargs = {\"name\": name}\n\n    # check that query returned something\n    has_result(state)\n\n    stu_res = state.student_result\n    sol_res = state.solution_result\n\n    if name not in sol_res:\n        raise BaseException(\"name %s not in solution column names\" % name)\n\n    if name not in stu_res:\n        _msg = state.build_message(missing_msg, fmt_kwargs=msg_kwargs)\n        state.do_test(_msg)\n\n    return state.to_child(\n        append_message={\"msg\": expand_msg, \"kwargs\": msg_kwargs},\n        student_result={name: stu_res[name]},\n        solution_result={name: sol_res[name]},\n    )", "response": "Zoom in on a particular column in the query result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_all_columns(state, allow_extra=True, too_many_cols_msg=None, expand_msg=None):\n\n    if too_many_cols_msg is None:\n        too_many_cols_msg = (\n            \"Your query result contains the column {{col}} but shouldn't.\"\n        )\n    if expand_msg is None:\n        expand_msg = \"Have another look at your query result. \"\n\n    child_stu_result = {}\n    child_sol_result = {}\n\n    for col in state.solution_result:\n        child = check_column(state, col)\n        child_stu_result.update(**child.student_result)\n        child_sol_result.update(**child.solution_result)\n\n    cols_not_in_sol = list(\n        set(state.student_result.keys()) - set(child_stu_result.keys())\n    )\n    if not allow_extra and len(cols_not_in_sol) > 0:\n        _msg = state.build_message(\n            \"Your query result contains the column `{{col}}` but shouldn't.\",\n            fmt_kwargs={\"col\": cols_not_in_sol[0]},\n        )\n        state.do_test(_msg)\n\n    return state.to_child(\n        append_message={\"msg\": expand_msg, \"kwargs\": {}},\n        student_result=child_stu_result,\n        solution_result=child_sol_result,\n    )", "response": "This function checks all columns in the student query result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nverify if a student and solution query result match up.", "response": "def has_equal_value(state, ordered=False, ndigits=None, incorrect_msg=None):\n    \"\"\"Verify if a student and solution query result match up.\n\n    This function must always be used after 'zooming' in on certain columns or records (check_column, check_row or check_result).\n    ``has_equal_value`` then goes over all columns that are still left in the solution query result, and compares each column with the\n    corresponding column in the student query result.\n\n    Args:\n        ordered: if set to False, the default, all rows are sorted (according\n                 to the first column and the following columns as tie breakers).\n                 if set to True, the order of rows in student and solution query have to match.\n        digits: if specified, number of decimals to use when comparing column values.\n        incorrect_msg: if specified, this overrides the automatically generated feedback\n                       message in case a column in the student query result does not match\n                       a column in the solution query result.\n\n    :Example:\n\n        Suppose we are testing the following SELECT statements\n\n        * solution: ``SELECT artist_id as id, name FROM artists ORDER BY name``\n        * student : ``SELECT artist_id, name       FROM artists``\n\n        We can write the following SCTs: ::\n\n            # passes, as order is not important by default\n            Ex().check_column('name').has_equal_value()\n\n            # fails, as order is deemed important\n            Ex().check_column('name').has_equal_value(ordered=True)\n\n            # check_column fails, as id is not in the student query result\n            Ex().check_column('id').has_equal_value()\n\n            # check_all_columns fails, as id not in the student query result\n            Ex().check_all_columns().has_equal_value()\n    \"\"\"\n\n    if not hasattr(state, \"parent\"):\n        raise ValueError(\n            \"You can only use has_equal_value() on the state resulting from check_column, check_row or check_result.\"\n        )\n\n    if incorrect_msg is None:\n        incorrect_msg = \"Column `{{col}}` seems to be incorrect.{{' Make sure you arranged the rows correctly.' if ordered else ''}}\"\n\n    # First of all, check if number of rows correspond\n    has_nrows(state)\n\n    if not ordered:\n        stu_res, sol_res = sort_rows(state)\n    else:\n        stu_res = state.student_result\n        sol_res = state.solution_result\n\n    for sol_col_name, sol_col_vals in sol_res.items():\n        stu_col_vals = stu_res[sol_col_name]\n        if ndigits is not None:\n            try:\n                sol_col_vals = round_seq(sol_col_vals, ndigits)\n                stu_col_vals = round_seq(stu_col_vals, ndigits)\n            except:\n                pass\n\n        if sol_col_vals != stu_col_vals:\n            _msg = state.build_message(\n                incorrect_msg, fmt_kwargs={\"col\": sol_col_name, \"ordered\": ordered}\n            )\n            state.do_test(_msg)\n\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lowercase(state):\n    return state.to_child(\n        student_result={k.lower(): v for k, v in state.student_result.items()},\n        solution_result={k.lower(): v for k, v in state.solution_result.items()},\n    )", "response": "Convert all column names to their lower case versions to improve robustness\n    \n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_result(state):\n\n    state1 = lowercase(state)\n    state2 = check_all_columns(state1)\n    has_equal_value(state2)\n    return state2", "response": "High level function which wraps other SCTs for checking results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_query(state, query, error_msg=None, expand_msg=None):\n\n    if error_msg is None:\n        error_msg = \"Running `{{query}}` after your submission generated an error.\"\n    if expand_msg is None:\n        expand_msg = \"The autograder verified the result of running `{{query}}` against the database. \"\n\n    msg_kwargs = {\"query\": query}\n\n    # before redoing the query,\n    # make sure that it didn't generate any errors\n    has_no_error(state)\n\n    _msg = state.build_message(error_msg, fmt_kwargs=msg_kwargs)\n\n    # sqlbackend makes sure all queries are run in transactions.\n    # Rerun the solution code first, after which we run the provided query\n    with dbconn(state.solution_conn) as conn:\n        _ = runQuery(conn, state.solution_code)\n        sol_res = runQuery(conn, query)\n\n    if sol_res is None:\n        raise ValueError(\"Solution failed: \" + _msg)\n\n    # sqlbackend makes sure all queries are run in transactions.\n    # Rerun the student code first, after wich we run the provided query\n    with dbconn(state.student_conn) as conn:\n        _ = runQuery(conn, state.student_code)\n        stu_res = runQuery(conn, query)\n\n    if stu_res is None:\n        state.do_test(_msg)\n\n    return state.to_child(\n        append_message={\"msg\": expand_msg, \"kwargs\": msg_kwargs},\n        student_result=stu_res,\n        solution_result=sol_res,\n    )", "response": "Run arbitrary queries against the database and return the results."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_err(*args, **kwargs):\n    if kwargs.get('file', None) is None:\n        kwargs['file'] = sys.stderr\n    print(*args, **kwargs)", "response": "A wrapper for print that uses stderr by default."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwalk a directory and print status updates along the way.", "response": "def walk_dir_animated(path, maxdircnt=1000):\n    \"\"\" Walk a directory, printing status updates along the way. \"\"\"\n    p = AnimatedProgress(\n        'Walking {}...'.format(path),\n        frames=Frames.dots_orbit.as_rainbow(),\n        show_time=True,\n    )\n    rootcnt = 0\n    print('\\nStarting animated progress.')\n    with p:\n        for root, dirs, files in os.walk(path):\n            rootcnt += 1\n            if rootcnt % 50 == 0:\n                p.text = 'Walking {}...'.format(C(root, 'cyan'))\n            if rootcnt > maxdircnt:\n                # Stop is called because we are printing before the\n                # AnimatedProgress is finished running.\n                p.stop()\n                print('\\nFinished walking {} directories.'.format(\n                    C(maxdircnt, 'blue', style='bright')\n                ))\n                break\n        else:\n            # AnimatedProgress still running, `stop` it before printing.\n            p.stop()\n            print_err('\\nNever made it to {} directories ({}).'.format(\n                C(maxdircnt, 'blue', style='bright'),\n                C(rootcnt, 'red', style='bright'),\n            ))\n    print('\\nFinished with animated progress.')\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef walk_dir_progress(path, maxdircnt=5000, file=sys.stdout):\n    p = ProgressBar(\n        'Walking {}'.format(C(path, 'cyan')),\n        bars=Bars.numbers_blue.with_wrapper(('(', ')')),\n        show_time=True,\n        file=file,\n    )\n    rootcnt = 0\n    print('\\nStarting progress bar...')\n    p.start()\n\n    for root, dirs, files in os.walk(path):\n        rootcnt += 1\n        if rootcnt % 100 == 0:\n            p.update(\n                percent=min((rootcnt / maxdircnt) * 100, 100),\n                text='Walking {}...'.format(\n                    C(os.path.split(root)[-1], 'cyan'),\n                )\n            )\n\n        if rootcnt > maxdircnt:\n            # Stop is called because we are printing before the\n            # AnimatedProgress is finished running.\n            p.stop()\n            print(\n                '\\nFinished walking {} directories.'.format(\n                    C(maxdircnt, 'blue', style='bright')\n                ),\n                file=file,\n            )\n            break\n    else:\n        # AnimatedProgress still running, `stop` it before printing.\n        p.stop()\n        print_err(\n            '\\nNever made it to {} directories ({}).'.format(\n                C(maxdircnt, 'blue', style='bright'),\n                C(rootcnt, 'red', style='bright'),\n            )\n        )\n    print('\\nFinished with progress bar.')\n    return 0", "response": "Walk a directory and print progress updates along the way."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _coloredhelp(s):\n    newlines = []\n    bigindent = (' ' * 16)\n    in_opts = False\n    for line in s.split('\\n'):\n        linestripped = line.strip('\\n').strip().strip(':')\n        if linestripped == 'Usage':\n            # label\n            line = line.replace('Usage', str(C('Usage', **ARGS_LABEL)))\n        elif linestripped == 'Options':\n            line = line.replace('Options', str(C('Options', **ARGS_LABEL)))\n            in_opts = True\n        elif (':' in line) and (not line.startswith(bigindent)):\n            # opt,desc line. colorize it.\n            lineparts = line.split(':')\n            opt = lineparts[0]\n            vals = [lineparts[1]] if len(lineparts) == 2 else lineparts[1:]\n\n            # colorize opt\n            if ',' in opt:\n                opts = opt.split(',')\n            else:\n                opts = [opt]\n            optstr = ','.join(str(C(o, **ARGS_OPTIONS)) for o in opts)\n\n            # colorize desc\n            valstr = ':'.join(str(C(val, **ARGS_DESC)) for val in vals)\n            line = ':'.join((optstr, valstr))\n        elif in_opts and line.startswith(bigindent):\n            # continued desc string..\n            # Make any 'Default:Value' parts look the same as the opt,desc.\n\n            line = ':'.join(str(C(s, **ARGS_DESC)) for s in line.split(':'))\n        elif (not line.startswith('    ')):\n            # header line.\n            line = str(C(line, **ARGS_HEADER))\n        else:\n            # Everything else, usage mainly.\n            if SCRIPT:\n                line = line.replace(SCRIPT, str(C(SCRIPT, **ARGS_SCRIPT)))\n\n        newlines.append(\n            '{}{}'.format(line, C('', style='reset_all'))\n        )\n    return '\\n'.join(newlines)", "response": "Colorize the usage string for docopt\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef docopt(\n        doc, argv=None, help=True, version=None, options_first=False,\n        script=None, colors=None):\n    \"\"\"\nThis is a wrapper for docopt.docopt that also sets SCRIPT to `script`.\n    When SCRIPT is set, it can be colorized for the usage string.\n    A dict of Colr options can be passed with `colors` to alter the\n    styles.\n    Available color options keys:\n        desc     : Colr args for the description of options.\n        label    : Colr args for the 'Usage:' and 'Options:' labels.\n        header   : Colr args for the top line (program name), and any\n                   other line that is not indented at all.\n        options  : Colr args for the options themselves ('-h,--help').\n        script   : Colr args for the script name, if found in the usage\n                   text.\n        version  : Colr args for the version when --version is used.\n\n    Example:\n        # `colors` only updates the default settings. You must override\n        # them to change ALL the settings.\n        argd = docopt(\n            ...,\n            script=SCRIPT,\n            colors={'script': {'fore': 'red'}}\n        )\n\nOriginal docopt documentation follows:\n    \"\"\"\n    # docopt documentation is appended programmatically after this func def.\n\n    global SCRIPT\n    global ARGS_DESC, ARGS_HEADER, ARGS_LABEL, ARGS_OPTIONS\n    global ARGS_SCRIPT, ARGS_VERSION\n\n    SCRIPT = script\n    if colors:\n        # Setup colors, if any were given.\n        ARGS_DESC.update(\n            colors.get('desc', colors.get('description', {}))\n        )\n        ARGS_HEADER.update(colors.get('header', {}))\n        ARGS_LABEL.update(colors.get('label', {}))\n        ARGS_OPTIONS.update(colors.get('options', {}))\n        ARGS_SCRIPT.update(colors.get('script', {}))\n        ARGS_VERSION.update(colors.get('version', {}))\n\n    return _old_docopt(\n        doc,\n        argv=argv,\n        help=help,\n        version=version,\n        options_first=options_first,\n    )", "response": "Wrapper for docopt. docopt that sets SCRIPT to script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef display(method=EraseMethod.ALL_MOVE):\n        accepted_methods = ('0', '1', '2', '3', '4')\n        methodstr = str(method)\n        if methodstr not in accepted_methods:\n            raise ValueError('Invalid method, expected {}. Got: {!r}'.format(\n                ', '.join(accepted_methods),\n                method,\n            ))\n        if methodstr == '4':\n            methods = (2, 3)\n        else:\n            methods = (method, )\n        return EscapeCode(\n            ''.join(str(EscapeCode('{}J'.format(m))) for m in methods)\n        )", "response": "Display the current state of the current element in the screen."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef line(method=EraseMethod.ALL):\n        methods = ('0', '1', '2')\n        if str(method) not in methods:\n            raise ValueError('Invalid method, expected {}. Got: {!r}'.format(\n                ', '.join(methods),\n                method,\n            ))\n        return EscapeCode('{}K'.format(method))", "response": "Erase a line or part of a line."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef try_unbuffered_file(file, _alreadyopen={}):\n    try:\n        fileno = file.fileno()\n    except (AttributeError, UnsupportedOperation):\n        # Unable to use fileno to re-open unbuffered. Oh well.\n        # The output may be line buffered, which isn't that great for\n        # repeatedly drawing and erasing text, or hiding/showing the cursor.\n        return file\n    filedesc = _alreadyopen.get(fileno, None)\n    if filedesc is not None:\n        return filedesc\n\n    filedesc = fdopen(fileno, 'wb', 0)\n    _alreadyopen[fileno] = filedesc\n    # TODO: sys.stdout/stderr don't need to be closed.\n    #       But would it be worth it to try and close these opened files?\n    return filedesc", "response": "Try re - opening a file in an unbuffered mode and return it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _loop(self):\n        self.stop_flag.value = False\n        self.time_started.value = time()\n        self.time_elapsed.value = 0\n\n        while True:\n            if self.stop_flag.value:\n                break\n            self.update_text()\n            with self.time_started.get_lock():\n                start = self.time_started.value\n                with self.time_elapsed.get_lock():\n                    self.time_elapsed.value = time() - start\n                    if (\n                            self.timeout.value and\n                            (self.time_elapsed.value > self.timeout.value)):\n                        self.stop()\n                        raise ProgressTimedOut(\n                            self.name,\n                            self.time_elapsed.value,\n                        )", "response": "This is the main loop that runs in the subproces. It is called from the main thread. It is called from the main thread. It is called from the main thread. It is called from the main thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the main loop in a subprocess.", "response": "def run(self):\n        \"\"\" Runs the printer loop in a subprocess. This is called by\n            multiprocessing.\n        \"\"\"\n        try:\n            self._loop()\n        except Exception:\n            # Send the exception through the exc_queue, so the parent\n            # process can check it.\n            typ, val, tb = sys.exc_info()\n            tb_lines = traceback.format_exception(typ, val, tb)\n            self.exc_queue.put((val, tb_lines))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stop(self):\n        self.stop_flag.value = True\n        with self.lock:\n            (\n                Control().text(C(' ', style='reset_all'))\n                .pos_restore().move_column(1).erase_line()\n                .write(self.file)\n            )", "response": "Stop this WriterProcessBase and reset the cursor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting the current text and check for any new text changes.", "response": "def update_text(self):\n        \"\"\" Write the current text, and check for any new text changes.\n            This also updates the elapsed time.\n        \"\"\"\n        self.write()\n        try:\n            newtext = self.text_queue.get_nowait()\n            self._text = newtext\n        except Empty:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites the current text to self. file and flush it.", "response": "def write(self):\n        \"\"\" Write the current text to self.file, and flush it.\n            This can be overridden to handle custom writes.\n        \"\"\"\n        if self._text is not None:\n            with self.lock:\n                self.file.write(str(self._text).encode())\n                self.file.flush()\n        sleep(self.nice_delay)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntry retrieving the last subprocess exception.", "response": "def exception(self):\n        \"\"\" Try retrieving the last subprocess exception.\n            If set, the exception is returned. Otherwise None is returned.\n        \"\"\"\n        if self._exception is not None:\n            return self._exception\n        try:\n            exc, tblines = self.exc_queue.get_nowait()\n        except Empty:\n            self._exception, self.tb_lines = None, None\n        else:\n            # Raise any exception that the subprocess encountered and sent.\n            self._exception, self.tb_lines = exc, tblines\n        return self._exception"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting self.fmt, with some extra help for plain format strings.", "response": "def fmt(self, value):\n        \"\"\" Sets self.fmt, with some extra help for plain format strings. \"\"\"\n        if isinstance(value, str):\n            value = value.split(self.join_str)\n        if not (value and isinstance(value, (list, tuple))):\n            raise TypeError(\n                ' '.join((\n                    'Expecting str or list/tuple of formats {!r}.',\n                    'Got: ({}) {!r}'\n                )).format(\n                    self.default_format,\n                    type(value).__name__,\n                    value,\n                ))\n        self._fmt = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self):\n        try:\n            Control().cursor_hide().write(file=self.file)\n            super().run()\n        except KeyboardInterrupt:\n            self.stop()\n        finally:\n            Control().cursor_show().write(file=self.file)", "response": "Overrides WriterProcess. run to handle KeyboardInterrupts better."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops this animated progress and block until it is finished.", "response": "def stop(self):\n        \"\"\" Stop this animated progress, and block until it is finished. \"\"\"\n        super().stop()\n        while not self.stopped:\n            # stop() should block, so printing afterwards isn't interrupted.\n            sleep(0.001)\n        # Retrieve the latest exception, if any.\n        exc = self.exception\n        if exc is not None:\n            raise exc"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a single frame of the progress spinner to the terminal.", "response": "def write(self):\n        \"\"\" Writes a single frame of the progress spinner to the terminal.\n            This function updates the current frame before returning.\n        \"\"\"\n        if self.text is None:\n            # Text has not been sent through the pipe yet.\n            # Do not write anything until it is set to non-None value.\n            return None\n        if self._last_text == self.text:\n            char_delay = 0\n        else:\n            char_delay = self.char_delay\n            self._last_text = self.text\n        with self.lock:\n            ctl = Control().move_column(1).pos_save().erase_line()\n            if char_delay == 0:\n                ctl.text(str(self)).write(file=self.file)\n            else:\n                self.write_char_delay(ctl, char_delay)\n            ctl.delay(self.delay)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadvance to the next available frame.", "response": "def _advance_frame(self):\n        \"\"\" Sets `self.current_frame` to the next frame, looping to the\n            beginning if needed.\n        \"\"\"\n        self.current_frame += 1\n        if self.current_frame == self.frame_len:\n            self.current_frame = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_delay(self, userdelay, frameslist):\n        # User frameslists might not be a FrameSet.\n        delay = userdelay or getattr(frameslist, 'delay', None)\n        delay = (delay or self.default_delay) - self.nice_delay\n        if delay < 0:\n            delay = 0\n        return delay", "response": "Get the appropriate delay value to use for the user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_char_delay(self, ctl, delay):\n        for i, fmt in enumerate(self.fmt):\n            if '{text' in fmt:\n                # The text will use a write delay.\n                ctl.text(fmt.format(text=self.text))\n                if i != (self.fmt_len - 1):\n                    ctl.text(self.join_str)\n                ctl.write(\n                    file=self.file,\n                    delay=delay\n                )\n            else:\n                # Anything else is written with no delay.\n                ctl.text(fmt.format(\n                    frame=self.frames[self.current_frame],\n                    elapsed=self.elapsed\n                ))\n                if i != (self.fmt_len - 1):\n                    # Add the join_str to pieces, except the last piece.\n                    ctl.text(self.join_str)\n                ctl.write(file=self.file)\n        return ctl", "response": "Write the formatted format pieces in order applying a delay between characters for the text only."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the percentage and message.", "response": "def update(self, percent=None, text=None):\n        \"\"\" Update the progress bar percentage and message. \"\"\"\n        if percent is not None:\n            self.percent = percent\n        if text is not None:\n            self.message = text\n        super().update()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cls_get_by_name(cls, name):\n    try:\n        val = getattr(cls, name)\n    except AttributeError:\n        for attr in (a for a in dir(cls) if not a.startswith('_')):\n            try:\n                val = getattr(cls, attr)\n            except AttributeError:\n                # Is known to happen.\n                continue\n            valname = getattr(val, 'name', None)\n            if valname == name:\n                return val\n        else:\n            raise ValueError('No {} with that name: {}'.format(\n                cls.__name__,\n                name,\n            ))\n    else:\n        return val", "response": "Return a class attribute by searching the attributes name attribute."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cls_names(cls, wanted_cls, registered=True):\n    return [\n        fset.name\n        for fset in cls_sets(cls, wanted_cls, registered=registered)\n    ]", "response": "Return a list of all attributes for the wanted_cls attributes in this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a new FrameSet or FrameSet subclass as a member of a class.", "response": "def cls_register(cls, frameset, new_class, init_args, name=None):\n    \"\"\" Register a new FrameSet or FrameSet subclass as a member/attribute\n        of a class.\n        Returns the new FrameSet or FrameSet subclass.\n        Arguments:\n            frameset  : An existing FrameSet, or an iterable of strings.\n            init_args : A list of properties from the `frameset` to try to use\n                        for initializing the new FrameSet.\n            new_class : The class type to initialize.\n            name      : New name for the FrameSet, also used as the\n                        classes attribute name.\n                        If the `frameset` object has not `name` attribute,\n                        this argument is required. It must not be empty\n                        when given.\n    \"\"\"\n    name = name or getattr(frameset, 'name', None)\n    if name is None:\n        raise ValueError(\n            '`name` is needed when the `frameset` has no name attribute.'\n        )\n    kwargs = {'name': name}\n    for initarg in init_args:\n        kwargs[initarg] = getattr(frameset, initarg, None)\n\n    newframeset = new_class(frameset, **kwargs)\n    # Mark this FrameSet/BarSet as a registered item (not basic/original).\n    newframeset._registered = True\n    setattr(cls, name, newframeset)\n    return newframeset"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cls_sets(cls, wanted_cls, registered=True):\n    sets = []\n    for attr in dir(cls):\n        if attr.startswith('_'):\n            continue\n        val = getattr(cls, attr, None)\n        if not isinstance(val, wanted_cls):\n            continue\n        if (not registered) and getattr(val, '_registered', False):\n            continue\n        sets.append(val)\n    return sets", "response": "Return a list of all wanted_cls attributes in this\n        class where wanted_cls is the desired attribute type."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _build_color_variants(cls):\n    # Get the basic frame types first.\n    frametypes = cls.sets(registered=False)\n\n    _colornames = [\n        # 'black', disabled for now, it won't show on my terminal.\n        'red',\n        'green',\n        'yellow',\n        'blue',\n        'magenta',\n        'cyan',\n        'white',\n    ]\n    _colornames.extend('light{}'.format(s) for s in _colornames[:])\n    for colorname in _colornames:\n        for framesobj in frametypes:\n            framename = '{}_{}'.format(framesobj.name, colorname)\n            cls.register(\n                framesobj.as_colr(fore=colorname),\n                name=framename,\n            )", "response": "Build colorized variants of all frames and return a list of\n        all frame object names."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new FrameSet from an existing BarSet object.", "response": "def from_barset(\n            cls, barset, name=None, delay=None,\n            use_wrapper=True, wrapper=None):\n        \"\"\" Copy a BarSet's frames to create a new FrameSet.\n\n            Arguments:\n                barset      : An existing BarSet object to copy frames from.\n                name        : A name for the new FrameSet.\n                delay       : Delay for the animation.\n                use_wrapper : Whether to use the old barset's wrapper in the\n                              frames.\n                wrapper     : A new wrapper pair to use for each frame.\n                              This overrides the `use_wrapper` option.\n        \"\"\"\n        if wrapper:\n            data = tuple(barset.wrap_str(s, wrapper=wrapper) for s in barset)\n        elif use_wrapper:\n            data = tuple(barset.wrap_str(s) for s in barset)\n        else:\n            data = barset.data\n        return cls(\n            data,\n            name=name,\n            delay=delay\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_gradient(self, name=None, style=None, rgb_mode=False):\n        return self._as_gradient(\n            ('wrapper', ),\n            name=name,\n            style=style,\n            rgb_mode=rgb_mode,\n        )", "response": "Wrap each frame in a Colr object using Colr. gradient."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_percent(self, percent):\n        if not self:\n            return self.wrap_str()\n\n        length = len(self)\n        # Using mod 100, to provide some kind of \"auto reset\". 0 is 0 though.\n        percentmod = (int(percent) % 100) or min(percent, 100)\n        index = int((length / 100) * percentmod)\n        try:\n            barstr = str(self[index])\n        except IndexError:\n            barstr = self[-1]\n\n        return self.wrap_str(barstr)", "response": "Return a string representing a percentage of this progress bar."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_rainbow(self, offset=35, style=None, rgb_mode=False):\n        return self._as_rainbow(\n            ('wrapper', ),\n            offset=offset,\n            style=style,\n            rgb_mode=rgb_mode,\n        )", "response": "Wrap each frame in a Colr object using Colr. rainbow."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new instance of a nacarity. A nacarity. n", "response": "def from_char(\n            cls, char, name=None, width=None, fill_char=None,\n            bounce=False, reverse=False, back_char=None, wrapper=None):\n        \"\"\" Create progress bar frames from a \"moving\" character.\n            The frames simulate movement of the character, from left to\n            right through empty space (`fill_char`).\n\n            Arguments:\n                char           : Character to move across the bar.\n                name           : Name for the new BarSet.\n                width          : Width of the progress bar.\n                                 Default: 25\n                fill_char      : Character to fill empty space.\n                                 Default: ' ' (space)\n                bounce         : Whether the frames should simulate a bounce\n                                 from one side to another.\n                                 Default: False\n                reverse        : Whether the character should start on the\n                                 right.\n                                 Default: False\n                back_char  : Character to use when \"bouncing\" backward.\n                                 Default: `char`\n        \"\"\"\n        return cls(\n            cls._generate_move(\n                char,\n                width=width or cls.default_width,\n                fill_char=str(fill_char or cls.default_fill_char),\n                bounce=bounce,\n                reverse=reverse,\n                back_char=back_char,\n            ),\n            name=name,\n            wrapper=wrapper or cls.default_wrapper,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_str(cls, s, name=None, fill_char=None, wrapper=None):\n        fill_char = fill_char or cls.default_fill_char\n        maxlen = len(s)\n        frames = []\n        for pos in range(1, maxlen):\n            framestr = s[:pos]\n            # Not using ljust, because fill_char may be a str, not a char.\n            frames.append(\n                ''.join((\n                    framestr,\n                    fill_char * (maxlen - len(framestr))\n                ))\n            )\n        frames.append(s)\n        return cls(\n            frames,\n            name=name,\n            wrapper=wrapper,\n        )", "response": "Create a new BarSet from a single string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_move(\n            cls, char, width=None, fill_char=None,\n            bounce=False, reverse=True, back_char=None):\n        \"\"\" Yields strings that simulate movement of a character from left\n            to right. For use with `BarSet.from_char`.\n\n            Arguments:\n                char          : Character to move across the progress bar.\n                width         : Width for the progress bar.\n                                Default: cls.default_width\n                fill_char     : String for empty space.\n                                Default: cls.default_fill_char\n                bounce        : Whether to move the character in both\n                                directions.\n                reverse       : Whether to start on the right side.\n                back_char  : Character to use for the bounce's backward\n                                movement.\n                                Default: `char`\n        \"\"\"\n        width = width or cls.default_width\n        char = str(char)\n        filler = str(fill_char or cls.default_fill_char) * (width - len(char))\n\n        rangeargs = RangeMoveArgs(\n            (0, width, 1),\n            (width, 0, -1),\n        )\n        if reverse:\n            # Reverse the arguments for range to start from the right.\n            # Not using swap, because the stopping point is different.\n            rangeargs = RangeMoveArgs(\n                (width, -1, -1),\n                (0, width - 1, 1),\n            )\n\n        yield from (\n            ''.join((filler[:i], char, filler[i:]))\n            for i in range(*rangeargs.forward)\n        )\n\n        if bounce:\n            bouncechar = char if back_char is None else back_char\n            yield from (\n                ''.join((filler[:i], str(bouncechar), filler[i:]))\n                for i in range(*rangeargs.backward)\n            )", "response": "Generates a generator that generates the string that simulate a character from left to right."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy this BarSet and return a new BarSet with the specified wrapper.", "response": "def with_wrapper(self, wrapper=None, name=None):\n        \"\"\" Copy this BarSet, and return a new BarSet with the specified\n            name and wrapper.\n            If no name is given, `{self.name}_custom_wrapper` is used.\n            If no wrapper is given, the new BarSet will have no wrapper.\n        \"\"\"\n        name = name or '{}_custom_wrapper'.format(self.name)\n        return self.__class__(self.data, name=name, wrapper=wrapper)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_str(self, s=None, wrapper=None):\n        wrapper = wrapper or (self.wrapper or ('', ''))\n        return str('' if s is None else s).join(wrapper)", "response": "Wrap a string in self. wrapper with some extra handling for\n            empty or None strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a new BarSet as a member or attribute of this class.", "response": "def register(cls, barset, name=None):\n        \"\"\" Register a new BarSet as a member/attribute of this class.\n            Returns the new BarSet.\n            Arguments:\n                barset    : An existing BarSet, or an iterable of strings.\n                name      : New name for the BarSet, also used as the\n                            classes attribute name.\n                            If the `barset` object has not `name` attribute,\n                            this argument is required. It must not be empty\n                            when given.\n        \"\"\"\n        return cls_register(cls, barset, BarSet, ('wrapper', ), name=name)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(cls, frameset, name=None):\n        return cls_register(cls, frameset, FrameSet, ('delay', ), name=name)", "response": "Register a new FrameSet as a member or attribute of this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new instance from a pyuv. errno error code.", "response": "def from_errno(cls, errno):\n        \"\"\"Create a new instance from a :mod:`pyuv.errno` error code.\"\"\"\n        message = '{}: {}'.format(pyuv.errno.errorcode.get(errno, errno),\n                                    pyuv.errno.strerror(errno))\n        return cls(message, errno)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbind to protocol and start calling callbacks on it.", "response": "def start(self, protocol):\n        \"\"\"Bind to *protocol* and start calling callbacks on it. \"\"\"\n        if self._protocol is not None:\n            raise TransportError('already started')\n        self._protocol = protocol\n        self._protocol.connection_made(self)\n        if self._readable:\n            self.resume_reading()\n        if self._writable:\n            self._writing = True\n            self._can_write.set()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the low and high watermark for the write buffer.", "response": "def set_write_buffer_limits(self, high=None, low=None):\n        \"\"\"Set the low and high watermark for the write buffer.\"\"\"\n        if high is None:\n            high = self.write_buffer_size\n        if low is None:\n            low = high // 2\n        if low > high:\n            low = high\n        self._write_buffer_high = high\n        self._write_buffer_low = low"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        if self._closing or self._handle.closed:\n            return\n        elif self._protocol is None:\n            raise TransportError('transport not started')\n        # If the write buffer is empty, close now. Otherwise defer to\n        # _on_write_complete that will close when the buffer is empty.\n        if self._write_buffer_size == 0:\n            self._handle.close(self._on_close_complete)\n            assert self._handle.closed\n        else:\n            self._closing = True", "response": "Close the transport after all oustanding data has been written."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose the transport immediately.", "response": "def abort(self):\n        \"\"\"Close the transport immediately.\"\"\"\n        if self._handle.closed:\n            return\n        elif self._protocol is None:\n            raise TransportError('transport not started')\n        self._handle.close(self._on_close_complete)\n        assert self._handle.closed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, data, handle=None):\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data: expecting a bytes-like instance, got {!r}\"\n                                .format(type(data).__name__))\n        if handle is not None and not isinstance(self._handle, pyuv.Pipe):\n            raise ValueError('handle: can only be sent over pyuv.Pipe')\n        self._check_status()\n        if not self._writable:\n            raise TransportError('transport is not writable')\n        if self._closing:\n            raise TransportError('transport is closing')\n        try:\n            if handle:\n                self._handle.write(data, self._on_write_complete, handle)\n            else:\n                self._handle.write(data, self._on_write_complete)\n        except pyuv.error.UVError as e:\n            self._error = TransportError.from_errno(e.args[0])\n            self.abort()\n            raise compat.saved_exc(self._error)\n        # We only keep track of the number of outstanding write requests\n        # outselves. See note in get_write_buffer_size().\n        self._write_buffer_size += 1\n        self._maybe_pause_protocol()", "response": "Write data to the transport."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_eof(self):\n        self._check_status()\n        if not self._writable:\n            raise TransportError('transport is not writable')\n        if self._closing:\n            raise TransportError('transport is closing')\n        try:\n            self._handle.shutdown(self._on_write_complete)\n        except pyuv.error.UVError as e:\n            self._error = TransportError.from_errno(e.args[0])\n            self.abort()\n            raise compat.saved_exc(self._error)\n        self._write_buffer_size += 1", "response": "Shut down the write direction of the transport."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_extra_info(self, name, default=None):\n        if name == 'sockname':\n            if not hasattr(self._handle, 'getsockname'):\n                return default\n            try:\n                return self._handle.getsockname()\n            except pyuv.error.UVError:\n                return default\n        elif name == 'peername':\n            if not hasattr(self._handle, 'getpeername'):\n                return default\n            try:\n                return self._handle.getpeername()\n            except pyuv.error.UVError:\n                return default\n        elif name == 'winsize':\n            if not hasattr(self._handle, 'get_winsize'):\n                return default\n            try:\n                return self._handle.get_winsize()\n            except pyuv.error.UVError:\n                return default\n        elif name == 'unix_creds':\n            # In case you're wondering, DBUS needs this.\n            if not isinstance(self._handle, pyuv.Pipe) or not hasattr(socket, 'SO_PEERCRED'):\n                return default\n            try:\n                fd = self._handle.fileno()\n                sock = socket.fromfd(fd, socket.AF_UNIX, socket.SOCK_DGRAM)  # will dup()\n                with contextlib.closing(sock):\n                    creds = sock.getsockopt(socket.SOL_SOCKET, socket.SO_PEERCRED,\n                                            struct.calcsize('3i'))\n            except socket.error:\n                return default\n            return struct.unpack('3i', creds)\n        elif name == 'server_hostname':\n            return self._server_hostname\n        else:\n            return super(Transport, self).get_extra_info(name, default)", "response": "Get extra information for a specific transport class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_recv_complete(self, handle, addr, flags, data, error):\n        assert handle is self._handle\n        if error:\n            self._log.warning('pyuv error {} in recv callback', error)\n            self._protocol.error_received(TransportError.from_errno(error))\n        elif flags:\n            assert flags & pyuv.UV_UDP_PARTIAL\n            self._log.warning('ignoring partial datagram')\n        elif data:\n            self._protocol.datagram_received(data, addr)", "response": "Callback used with handle. start_recv."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_send_complete(self, handle, error):\n        assert handle is self._handle\n        self._write_buffer_size -= 1\n        assert self._write_buffer_size >= 0\n        if self._error:\n            self._log.debug('ignore sendto status {} after error', error)\n        # See note in _on_write_complete() about UV_ECANCELED\n        elif error and error != pyuv.errno.UV_ECANCELED:\n            self._log.warning('pyuv error {} in sendto callback', error)\n            self._protocol.error_received(TransportError.from_errno(error))\n        self._maybe_resume_protocol()\n        self._maybe_close()", "response": "Callback used with handle. send."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a datagram containing data to addr.", "response": "def sendto(self, data, addr=None):\n        \"\"\"Send a datagram containing *data* to *addr*.\n\n        The *addr* argument may be omitted only if the handle was bound to a\n        default remote address.\n        \"\"\"\n        if not isinstance(data, (bytes, bytearray, memoryview)):\n            raise TypeError(\"data: expecting a bytes-like instance, got {!r}\"\n                                .format(type(data).__name__))\n        self._check_status()\n        if not self._writable:\n            raise TransportError('transport is not writable')\n        try:\n            self._handle.send(addr, data, self._on_send_complete)\n        except pyuv.error.UVError as e:\n            error = TransportError.from_errno(e.args[0])\n            # Try to discern between permanent and transient errors. Permanent\n            # errors close the transport. This list is very likely not complete.\n            if error.errno != pyuv.errno.UV_EBADF:\n                raise error\n            self._error = error\n            self.abort()\n        self._write_buffer_size += 1\n        self._maybe_pause_protocol()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a D - BUS address string into a list of addresses.", "response": "def parse_dbus_address(address):\n    \"\"\"Parse a D-BUS address string into a list of addresses.\"\"\"\n    if address == 'session':\n        address = os.environ.get('DBUS_SESSION_BUS_ADDRESS')\n        if not address:\n            raise ValueError('$DBUS_SESSION_BUS_ADDRESS not set')\n    elif address == 'system':\n        address = os.environ.get('DBUS_SYSTEM_BUS_ADDRESS',\n                                 'unix:path=/var/run/dbus/system_bus_socket')\n    addresses = []\n    for addr in address.split(';'):\n        p1 = addr.find(':')\n        if p1 == -1:\n            raise ValueError('illegal address string: {}'.format(addr))\n        kind = addr[:p1]\n        args = dict((kv.split('=') for kv in addr[p1+1:].split(',')))\n        if kind == 'unix':\n            if 'path' in args:\n                addr = args['path']\n            elif 'abstract' in args:\n                addr = '\\0' + args['abstract']\n            else:\n                raise ValueError('require \"path\" or \"abstract\" for unix')\n        elif kind == 'tcp':\n            if 'host' not in args or 'port' not in args:\n                raise ValueError('require \"host\" and \"port\" for tcp')\n            addr = (args['host'], int(args['port']))\n        else:\n            raise ValueError('unknown transport: {}'.format(kind))\n        addresses.append(addr)\n    return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a D - BUS header. Return the message size.", "response": "def parse_dbus_header(header):\n    \"\"\"Parse a D-BUS header. Return the message size.\"\"\"\n    if six.indexbytes(header, 0) == ord('l'):\n        endian = '<'\n    elif six.indexbytes(header, 0) == ord('B'):\n        endian = '>'\n    else:\n        raise ValueError('illegal endianness')\n    if not 1 <= six.indexbytes(header, 1) <= 4:\n        raise ValueError('illegel message type')\n    if struct.unpack(endian + 'I', header[8:12])[0] == 0:\n        raise ValueError('illegal serial number')\n    harrlen = struct.unpack(endian + 'I', header[12:16])[0]\n    padlen = (8 - harrlen) % 8\n    bodylen = struct.unpack(endian + 'I', header[4:8])[0]\n    return 16 + harrlen + padlen + bodylen"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getMechanismName(self):\n        if self._server_side:\n            mech = self._authenticator.current_mech\n            return mech.getMechanismName() if mech else None\n        else:\n            return getattr(self._authenticator, 'authMech', None)", "response": "Return the authentication mechanism name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the authenticated user name.", "response": "def getUserName(self):\n        \"\"\"Return the authenticated user name (server side).\"\"\"\n        if not self._server_side:\n            return\n        mech = self._authenticator.current_mech\n        return mech.getUserName() if mech else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the unique name of the D - BUS connection.", "response": "def get_unique_name(self):\n        \"\"\"Return the unique name of the D-BUS connection.\"\"\"\n        self._name_acquired.wait()\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._transport is None:\n            raise DbusError('not connected')\n        return self._unique_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message to the D - BUS.", "response": "def send_message(self, message):\n        \"\"\"Send a D-BUS message.\n\n        The *message* argument must be ``gruvi.txdbus.DbusMessage`` instance.\n        \"\"\"\n        if not isinstance(message, txdbus.DbusMessage):\n            raise TypeError('message: expecting DbusMessage instance (got {!r})',\n                                type(message).__name__)\n        self._name_acquired.wait()\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._transport is None:\n            raise DbusError('not connected')\n        self._writer.write(message.rawMessage)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_method(self, service, path, interface, method, signature=None,\n                    args=None, no_reply=False, auto_start=False, timeout=-1):\n        \"\"\"Call a D-BUS method and wait for its reply.\n\n        This method calls the D-BUS method with name *method* that resides on\n        the object at bus address *service*, at path *path*, on interface\n        *interface*.\n\n        The *signature* and *args* are optional arguments that can be used to\n        add parameters to the method call. The signature is a D-BUS signature\n        string, while *args* must be a sequence of python types that can be\n        converted into the types specified by the signature. See the `D-BUS\n        specification\n        <http://dbus.freedesktop.org/doc/dbus-specification.html>`_ for a\n        reference on signature strings.\n\n        The flags *no_reply* and *auto_start* control the NO_REPLY_EXPECTED and\n        NO_AUTO_START flags on the D-BUS message.\n\n        The return value is the result of the D-BUS method call. This will be a\n        possibly empty sequence of values.\n        \"\"\"\n        message = txdbus.MethodCallMessage(path, method, interface=interface,\n                                destination=service, signature=signature, body=args,\n                                expectReply=not no_reply, autoStart=auto_start)\n        serial = message.serial\n        if timeout == -1:\n            timeout = self._timeout\n        try:\n            with switch_back(timeout) as switcher:\n                self._method_calls[serial] = switcher\n                self.send_message(message)\n                args, _ = self._hub.switch()\n        finally:\n            self._method_calls.pop(serial, None)\n        response = args[0]\n        assert response.reply_serial == serial\n        if isinstance(response, txdbus.ErrorMessage):\n            raise DbusMethodCallError(method, response)\n        args = tuple(response.body) if response.body else ()\n        return args", "response": "Call a method on the specified object at the specified bus address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconnect to the database and wait until the connection is established.", "response": "def connect(self, address='session'):\n        \"\"\"Connect to *address* and wait until the connection is established.\n\n        The *address* argument must be a D-BUS server address, in the format\n        described in the D-BUS specification. It may also be one of the special\n        addresses ``'session'`` or ``'system'``, to connect to the D-BUS\n        session and system bus, respectively.\n        \"\"\"\n        if isinstance(address, six.string_types):\n            addresses = parse_dbus_address(address)\n        else:\n            addresses = [address]\n        for addr in addresses:\n            try:\n                super(DbusClient, self).connect(addr)\n            except pyuv.error.UVError:\n                continue\n            break\n        else:\n            raise DbusError('could not connect to any address')\n        # Wait for authentication to complete\n        self.get_unique_name()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart listening on the given address for new connection.", "response": "def listen(self, address='session'):\n        \"\"\"Start listening on *address* for new connection.\n\n        The *address* argument must be a D-BUS server address, in the format\n        described in the D-BUS specification. It may also be one of the special\n        addresses ``'session'`` or ``'system'``, to connect to the D-BUS\n        session and system bus, respectively.\n        \"\"\"\n        if isinstance(address, six.string_types):\n            addresses = parse_dbus_address(address)\n        else:\n            addresses = [address]\n        for addr in addresses:\n            try:\n                super(DbusServer, self).listen(addr)\n            except pyuv.error.UVError:\n                self._log.error('skipping address {}', saddr(addr))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef docfrom(base):\n    def setdoc(func):\n        func.__doc__ = (getattr(base, '__doc__') or '') + (func.__doc__ or '')\n        return func\n    return setdoc", "response": "Decorator to set a function s docstring from another function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string that uniquely and compactly identifies an object.", "response": "def objref(obj):\n    \"\"\"Return a string that uniquely and compactly identifies an object.\"\"\"\n    ref = _objrefs.get(obj)\n    if ref is None:\n        clsname = obj.__class__.__name__.split('.')[-1]\n        seqno = _lastids.setdefault(clsname, 1)\n        ref = '{}-{}'.format(clsname, seqno)\n        _objrefs[obj] = ref\n        _lastids[clsname] += 1\n    return ref"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delegate_method(other, method, name=None):\n    frame = sys._getframe(1)\n    classdict = frame.f_locals\n\n    @functools.wraps(method)\n    def delegate(self, *args, **kwargs):\n        other_self = other.__get__(self)\n        return method(other_self, *args, **kwargs)\n\n    if getattr(method, '__switchpoint__', False):\n        delegate.__switchpoint__ = True\n\n    if name is None:\n        name = method.__name__\n    propname = None\n    for key in classdict:\n        if classdict[key] is other:\n            propname = key\n            break\n    # If we know the property name, replace the docstring with a small\n    # reference instead of copying the function docstring.\n    if propname:\n        qname = getattr(method, '__qualname__', method.__name__)\n        if '.' in qname:\n            delegate.__doc__ = 'A shorthand for ``self.{propname}.{name}()``.' \\\n                               .format(name=name, propname=propname)\n        else:\n            delegate.__doc__ = 'A shorthand for ``{name}({propname}, ...)``.' \\\n                               .format(name=name, propname=propname)\n    classdict[name] = delegate", "response": "Add a method to the current class that delegates to another method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef accept_ws(buf, pos):\n    match = re_ws.match(buf, pos)\n    if not match:\n        return None, pos\n    return buf[match.start(0):match.end(0)], match.end(0)", "response": "Skip whitespace at the current buffer position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\naccepting a literal character at the current buffer position.", "response": "def accept_lit(char, buf, pos):\n    \"\"\"Accept a literal character at the current buffer position.\"\"\"\n    if pos >= len(buf) or buf[pos] != char:\n        return None, pos\n    return char, pos+1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef expect_lit(char, buf, pos):\n    if pos >= len(buf) or buf[pos] != char:\n        return None, len(buf)\n    return char, pos+1", "response": "Expect a literal character at the current buffer position."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef accept_re(regexp, buf, pos):\n    match = regexp.match(buf, pos)\n    if not match:\n        return None, pos\n    return buf[match.start(1):match.end(1)], match.end(0)", "response": "Accept a regular expression at the current buffer position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrequire a regular expression at the current buffer position.", "response": "def expect_re(regexp, buf, pos):\n    \"\"\"Require a regular expression at the current buffer position.\"\"\"\n    match = regexp.match(buf, pos)\n    if not match:\n        return None, len(buf)\n    return buf[match.start(1):match.end(1)], match.end(0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the Content - Type header.", "response": "def parse_content_type(header):\n    \"\"\"Parse the \"Content-Type\" header.\"\"\"\n    typ = subtyp = None; options = {}\n    typ, pos = expect_re(re_token, header, 0)\n    _, pos = expect_lit('/', header, pos)\n    subtyp, pos = expect_re(re_token, header, pos)\n    ctype = header[:pos] if subtyp else ''\n    while pos < len(header):\n        _, pos = accept_ws(header, pos)\n        _, pos = expect_lit(';', header, pos)\n        _, pos = accept_ws(header, pos)\n        name, pos = expect_re(re_token, header, pos)\n        _, pos = expect_lit('=', header, pos)\n        char = lookahead(header, pos)\n        if char == '\"':\n            value, pos = expect_re(re_qstring, header, pos)\n            value = re_qpair.sub('\\\\1', value)\n        elif char:\n            value, pos = expect_re(re_token, header, pos)\n        if name and value is not None:\n            options[name] = value\n    return ctype, options"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_te(header):\n    pos = 0\n    names = []\n    while pos < len(header):\n        name, pos = expect_re(re_token, header, pos)\n        _, pos = accept_ws(header, pos)\n        _, pos = accept_lit(';', header, pos)\n        _, pos = accept_ws(header, pos)\n        qvalue, pos = accept_re(re_qvalue, header, pos)\n        if name:\n            names.append((name, qvalue))\n        _, pos = accept_ws(header, pos)\n        _, pos = expect_lit(',', header, pos)\n        _, pos = accept_ws(header, pos)\n    return names", "response": "Parse the TE header."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the Trailer header.", "response": "def parse_trailer(header):\n    \"\"\"Parse the \"Trailer\" header.\"\"\"\n    pos = 0\n    names = []\n    while pos < len(header):\n        name, pos = expect_re(re_token, header, pos)\n        if name:\n            names.append(name)\n        _, pos = accept_ws(header, pos)\n        _, pos = expect_lit(',', header, pos)\n        _, pos = accept_ws(header, pos)\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a RFC1123 style Date header for the given timestamp.", "response": "def rfc1123_date(timestamp=None):\n    \"\"\"Create a RFC1123 style Date header for *timestamp*.\"\"\"\n    if timestamp is None:\n        timestamp = time.time()\n    timestamp = int(timestamp)\n    global _cached_timestamp, _cached_datestring\n    if timestamp != _cached_timestamp:\n        # The time stamp must be GMT, and cannot be localized.\n        tm = time.gmtime(timestamp)\n        s = rfc1123_fmt.replace('%a', weekdays[tm.tm_wday]) \\\n                       .replace('%b', months[tm.tm_mon-1])\n        _cached_datestring = time.strftime(s, tm)\n        _cached_timestamp = timestamp\n    return _cached_datestring"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_url(url, default_scheme='http', is_connect=False):\n    # If this is not in origin-form, authority-form or asterisk-form and no\n    # scheme is present, assume it's in absolute-form with a missing scheme.\n    # See RFC7230 section 5.3.\n    if url[:1] not in '*/' and not is_connect and '://' not in url:\n        url = '{}://{}'.format(default_scheme, url)\n    burl = s2b(url)\n    parser = ffi.new('struct http_parser_url *')\n    lib.http_parser_url_init(parser)\n    res = lib.http_parser_parse_url(ffi.from_buffer(burl), len(burl), is_connect, parser)\n    if res != 0:\n        raise ValueError('invalid URL')\n    parsed = ParsedUrl.from_parser(parser, url)\n    return parsed", "response": "Parse an URL and return its components."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the value of header name*.", "response": "def get_header(headers, name, default=None):\n    \"\"\"Return the value of header *name*.\n\n    The *headers* argument must be a list of ``(name, value)`` tuples. If the\n    header is found its associated value is returned, otherwise *default* is\n    returned. Header names are matched case insensitively.\n    \"\"\"\n    name = name.lower()\n    for header in headers:\n        if header[0].lower() == name:\n            return header[1]\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_headers(headers, name):\n    i = 0\n    name = name.lower()\n    for j in range(len(headers)):\n        if headers[j][0].lower() != name:\n            if i != j:\n                headers[i] = headers[j]\n            i += 1\n    del headers[i:]\n    return headers", "response": "Remove all headers with name name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a chunk for the HTTP chunked transfer encoding.", "response": "def create_chunk(buf):\n    \"\"\"Create a chunk for the HTTP \"chunked\" transfer encoding.\"\"\"\n    chunk = []\n    chunk.append(s2b('{:X}\\r\\n'.format(len(buf))))\n    chunk.append(buf)\n    chunk.append(b'\\r\\n')\n    return b''.join(chunk)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the ending that terminates a chunked body.", "response": "def create_chunked_body_end(trailers=None):\n    \"\"\"Create the ending that terminates a chunked body.\"\"\"\n    chunk = []\n    chunk.append('0\\r\\n')\n    if trailers:\n        for name, value in trailers:\n            chunk.append(name)\n            chunk.append(': ')\n            chunk.append(value)\n            chunk.append('\\r\\n')\n    chunk.append('\\r\\n')\n    return s2b(''.join(chunk))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_request(version, method, url, headers):\n    # According to my measurements using b''.join is faster that constructing a\n    # bytearray.\n    message = []\n    message.append('{} {} HTTP/{}\\r\\n'.format(method, url, version))\n    for name, value in headers:\n        message.append(name)\n        message.append(': ')\n        message.append(value)\n        message.append('\\r\\n')\n    message.append('\\r\\n')\n    return s2b(''.join(message))", "response": "Create a HTTP request header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a HTTP response header.", "response": "def create_response(version, status, headers):\n    \"\"\"Create a HTTP response header.\"\"\"\n    message = []\n    message.append('HTTP/{} {}\\r\\n'.format(version, status))\n    for name, value in headers:\n        message.append(name)\n        message.append(': ')\n        message.append(value)\n        message.append('\\r\\n')\n    message.append('\\r\\n')\n    return s2b(''.join(message))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef addr(self):\n        port = self.port\n        if port:\n            port = int(port)\n        else:\n            port = default_ports.get(self.scheme or 'http')\n        return (self.host, port)", "response": "Address tuple that can be used with gruvi. create_connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_request(self, method, url, headers=None, bodylen=None):\n        self._headers = headers or []\n        agent = host = clen = trailer = None\n        # Check the headers provided, and capture some information about the\n        # request from them.\n        for name, value in self._headers:\n            lname = name.lower()\n            # Only HTTP applications are allowed to set \"hop-by-hop\" headers.\n            if lname in hop_by_hop:\n                raise ValueError('header {} is hop-by-hop'.format(name))\n            elif lname == 'user-agent':\n                agent = value\n            elif lname == 'host':\n                host = value\n            elif lname == 'content-length':\n                clen = int(value)\n            elif lname == 'trailer':\n                trailer = parse_trailer(value)\n            elif lname == 'content-type' and value.startswith('text/'):\n                ctype, params = parse_content_type(value)\n                self._charset = params.get('charset')\n        version = self._protocol._version\n        # The Host header is mandatory in 1.1. Add it if it's missing.\n        if host is None and version == '1.1' and self._protocol._server_name:\n            self._headers.append(('Host', self._protocol._server_name))\n        # Identify ourselves.\n        if agent is None:\n            self._headers.append(('User-Agent', self._protocol.identifier))\n        # Check if we need to use chunked encoding due to unknown body size.\n        if clen is None and bodylen is None:\n            if version == '1.0':\n                raise HttpError('body size unknown for HTTP/1.0')\n            self._chunked = True\n        self._content_length = clen\n        # Check if trailers are requested and if so need to switch to chunked.\n        if trailer:\n            if version == '1.0':\n                raise HttpError('cannot support trailers for HTTP/1.0')\n            if clen is not None:\n                remove_headers(self._headers, 'Content-Length')\n            self._chunked = True\n        self._trailer = trailer\n        # Add Content-Length if we know the body size and are not using chunked.\n        if not self._chunked and clen is None and bodylen >= 0:\n            self._headers.append(('Content-Length', str(bodylen)))\n            self._content_length = bodylen\n        # Complete the \"Hop by hop\" headers.\n        if version == '1.0':\n            self._headers.append(('Connection', 'keep-alive'))\n        elif version == '1.1':\n            self._headers.append(('Connection', 'te'))\n            self._headers.append(('TE', 'trailers'))\n        if self._chunked:\n            self._headers.append(('Transfer-Encoding', 'chunked'))\n        # Start the request\n        self._protocol._requests.append(method)\n        header = create_request(version, method, url, self._headers)\n        self._protocol.writer.write(header)", "response": "Start a new HTTP request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, buf):\n        if not isinstance(buf, six.binary_type):\n            raise TypeError('buf: must be a bytes instance')\n        # Be careful not to write zero-length chunks as they indicate the end of a body.\n        if len(buf) == 0:\n            return\n        if self._content_length and self._bytes_written > self._content_length:\n            raise RuntimeError('wrote too many bytes ({} > {})'\n                                    .format(self._bytes_written, self._content_length))\n        self._bytes_written += len(buf)\n        if self._chunked:\n            buf = create_chunk(buf)\n        self._protocol.writer.write(buf)", "response": "Write a single entry to the request body."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nending the request body.", "response": "def end_request(self):\n        \"\"\"End the request body.\"\"\"\n        if not self._chunked:\n            return\n        trailers = [(n, get_header(self._headers, n)) for n in self._trailer] \\\n                        if self._trailer else None\n        ending = create_chunked_body_end(trailers)\n        self._protocol.writer.write(ending)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a new HTTP request.", "response": "def request(self, method, url, headers=None, body=None):\n        \"\"\"Make a new HTTP request.\n\n        The *method* argument is the HTTP method as a string, for example\n        ``'GET'`` or ``'POST'``. The *url* argument specifies the URL.\n\n        The optional *headers* argument specifies extra HTTP headers to use in\n        the request. It must be a sequence of ``(name, value)`` tuples.\n\n        The optional *body* argument may be used to include a body in the\n        request. It must be a ``bytes`` instance, a file-like object opened in\n        binary mode, or an iterable producing ``bytes`` instances.  To send\n        potentially large bodies, use the file or iterator interfaces. This has\n        the benefit that only a single chunk is kept in memory at a time.\n\n        The response to the request can be obtained by calling the\n        :meth:`getresponse` method. You may make multiple requests before\n        reading a response. For every request that you make however, you must\n        call :meth:`getresponse` exactly once. The remote HTTP implementation\n        will send by the responses in the same order as the requests.\n\n        This method will use the \"chunked\" transfer encoding if here is a body\n        and the body size is unknown ahead of time. This happens when the file\n        or interator interface is used in the abence of a \"Content-Length\"\n        header.\n        \"\"\"\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._transport is None:\n            raise HttpError('not connected')\n        request = HttpRequest(self)\n        bodylen = -1 if body is None else \\\n                        len(body) if isinstance(body, bytes) else None\n        request.start_request(method, url, headers, bodylen)\n        if isinstance(body, bytes):\n            request.write(body)\n        elif hasattr(body, 'read'):\n            while True:\n                chunk = body.read(4096)\n                if not chunk:\n                    break\n                request.write(chunk)\n        elif hasattr(body, '__iter__'):\n            for chunk in body:\n                request.write(chunk)\n        request.end_request()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getresponse(self):\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._transport is None:\n            raise HttpError('not connected')\n        message = self._queue.get(timeout=self._timeout)\n        if isinstance(message, Exception):\n            raise compat.saved_exc(message)\n        return message", "response": "Wait for and return a HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unique_hash(filepath: str, blocksize: int=80)->str:\n    s = sha1()\n    with open(filepath, \"rb\") as f:\n        buf = f.read(blocksize)\n        s.update(buf)\n    return s.hexdigest()", "response": "Small function to generate a hash to uniquely generate a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(filename: str, limit: Optional[int]=None) -> Tuple[list, int]:\n\n    audiofile = AudioSegment.from_file(filename)\n\n    if limit:\n        audiofile = audiofile[:limit * 1000]\n\n    data = np.fromstring(audiofile._data, np.int16)\n\n    channels = []\n    for chn in range(audiofile.channels):\n        channels.append(data[chn::audiofile.channels])\n\n    fs = audiofile.frame_rate\n\n    return channels, fs", "response": "Reads any file supported by pydub and returns the data contained within the audio file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef path_to_songname(path: str)->str:\n    return os.path.splitext(os.path.basename(path))[0]", "response": "Extracts a song name from a filepath. Used to identify which songs have already been fingerprinted on disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_connection(protocol_factory, address, ssl=False, server_hostname=None,\n                      local_address=None, family=0, flags=0, ipc=False, timeout=None,\n                      mode='rw'):\n    \"\"\"Create a new client connection.\n\n    This method creates a new :class:`pyuv.Handle`, connects it to *address*,\n    and then waits for the connection to be established. When the connection is\n    established, the handle is wrapped in a transport, and a new protocol\n    instance is created by calling *protocol_factory*. The protocol is then\n    connected to the transport by calling the transport's\n    :meth:`~BaseTransport.start` method which in turn calls\n    :meth:`~BaseProtocol.connection_made` on the protocol. Finally the results\n    are returned as a ``(transport, protocol)`` tuple.\n\n    The address may be either be a string, a (host, port) tuple, a\n    ``pyuv.Stream`` handle or a file descriptor:\n\n    * If the address is a string, this method connects to a named pipe using a\n      :class:`pyuv.Pipe` handle. The address specifies the pipe name.\n    * If the address is a tuple, this method connects to a TCP/IP service using\n      a :class:`pyuv.TCP` handle. The first element of the tuple specifies the\n      IP address or DNS name, and the second element specifies the port number\n      or service name.\n    * If the address is a ``pyuv.Stream`` instance, it must be an already\n      connected stream.\n    * If the address is a file descriptor, then it is attached to a\n      :class:`pyuv.TTY` stream if :func:`os.isatty` returns true, or to a\n      :class:`pyuv.Pipe` instance otherwise.\n\n    The *ssl* parameter indicates whether an SSL/TLS connection is desired. If\n    so then an :class:`ssl.SSLContext` instance is used to wrap the connection\n    using the :mod:`ssl` module's asynchronous SSL support. The context is\n    created as follows. If *ssl* is an :class:`~ssl.SSLContext` instance, it is\n    used directly. If it is a function, it is called with the connection handle\n    as an argument and it must return a context . If it is ``True`` then a\n    default context is created using :func:`gruvi.create_default_context`. To\n    disable SSL (the default), pass ``False``. If SSL is active, the return\n    transport will be an :class:`SslTransport` instance, otherwise it will be a\n    :class:`Transport` instance.\n\n    The *server_hostname* parameter is only relevant for SSL connections, and\n    specifies the server hostname to use with SNI (Server Name Indication). If\n    no server hostname is provided, the hostname specified in *address* is\n    used, if available.\n\n    The *local_address* keyword argument is relevant only for TCP transports.\n    If provided, it specifies the local address to bind to.\n\n    The *family* and *flags* keyword arguments are used to customize address\n    resolution for TCP handles as described in :func:`socket.getaddrinfo`.\n\n    The *mode* parameter specifies if the transport should be put in read-only\n    (``'r'``), write-only (``'w'``) or read-write (``'rw'``) mode. For TTY\n    transports, the mode must be either read-only or write-only. For all other\n    transport the mode should usually be read-write.\n    \"\"\"\n    hub = get_hub()\n    log = logging.get_logger()\n    handle_args = ()\n    if isinstance(address, (six.binary_type, six.text_type)):\n        handle_type = pyuv.Pipe\n        handle_args = (ipc,)\n        addresses = [address]\n    elif isinstance(address, tuple):\n        handle_type = pyuv.TCP\n        result = getaddrinfo(address[0], address[1], family, socket.SOCK_STREAM,\n                             socket.IPPROTO_TCP, flags)\n        addresses = [res[4] for res in result]\n        if server_hostname is None:\n            server_hostname = address[0]\n            # Python 2.7 annoyingly gives a unicode IP address\n            if not isinstance(server_hostname, str):\n                server_hostname = server_hostname.encode('ascii')\n    elif isinstance(address, int):\n        if os.isatty(address):\n            if mode not in ('r', 'w'):\n                raise ValueError(\"mode: must be either 'r' or 'w' for tty\")\n            handle = pyuv.TTY(hub.loop, address, mode == 'r')\n        else:\n            handle = pyuv.Pipe(hub.loop, ipc)\n        handle.open(address)\n        addresses = []; error = None\n    elif isinstance(address, pyuv.Stream):\n        handle = address\n        addresses = []; error = None\n    else:\n        raise TypeError('expecting a string, tuple, fd, or pyuv.Stream')\n    for addr in addresses:\n        log.debug('trying address {}', saddr(addr))\n        handle = handle_type(hub.loop, *handle_args)\n        error = None\n        try:\n            if compat.pyuv_pipe_helper(handle, handle_args, 'connect', addr):\n                break\n            with switch_back(timeout) as switcher:\n                handle.connect(addr, switcher)\n                result = hub.switch()\n                _, error = result[0]\n        except pyuv.error.UVError as e:\n            error = e[0]\n        except Timeout:\n            error = pyuv.errno.UV_ETIMEDOUT\n        if not error:\n            break\n        handle.close()\n        log.warning('connect() failed with error {}', error)\n    if error:\n        log.warning('all addresses failed')\n        raise TransportError.from_errno(error)\n    if local_address:\n        handle.bind(*local_address)\n    protocol = protocol_factory()\n    protocol._timeout = timeout\n    if ssl:\n        context = ssl if hasattr(ssl, 'set_ciphers') \\\n                        else ssl(handle) if callable(ssl) \\\n                        else create_default_context(False)\n        transport = SslTransport(handle, context, False, server_hostname)\n    else:\n        transport = Transport(handle, server_hostname, mode)\n    events = transport.start(protocol)\n    if events:\n        for event in events:\n            event.wait()\n        transport._check_status()\n    return (transport, protocol)", "response": "Create a new connection to the given address."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new network server.", "response": "def create_server(protocol_factory, address=None, ssl=False, family=0, flags=0,\n                  ipc=False, backlog=128):\n    \"\"\"\n    Create a new network server.\n\n    This creates one or more :class:`pyuv.Handle` instances bound to *address*,\n    puts them in listen mode and starts accepting new connections. For each\n    accepted connection, a new transport is created which is connected to a new\n    protocol instance obtained by calling *protocol_factory*.\n\n    The *address* argument may be either be a string, a ``(host, port)`` tuple,\n    or a ``pyuv.Stream`` handle:\n\n    * If the address is a string, this method creates a new :class:`pyuv.Pipe`\n      instance and binds it to *address*.\n    * If the address is a tuple, this method creates one or more\n      :class:`pyuv.TCP` handles. The first element of the tuple specifies the\n      IP address or DNS name, and the second element specifies the port number\n      or service name. A transport is created for each resolved address.\n    * If the address is a ``pyuv.Stream`` handle, it must already be bound to\n      an address.\n\n    The *ssl* parameter indicates whether SSL should be used for accepted\n    connections. See :func:`create_connection` for a description.\n\n    The *family* and *flags* keyword arguments are used to customize address\n    resolution for TCP handles as described in :func:`socket.getaddrinfo`.\n\n    The *ipc* parameter indicates whether this server will accept new\n    connections via file descriptor passing. This works for `pyuv.Pipe` handles\n    only, and the user is required to call :meth:`Server.accept_connection`\n    whenever a new connection is pending.\n\n    The *backlog* parameter specifies the listen backlog i.e the maximum number\n    of not yet accepted active opens to queue. To disable listening for new\n    connections (useful when *ipc* was set), set the backlog to ``None``.\n\n    The return value is a :class:`Server` instance.\n    \"\"\"\n    server = Server(protocol_factory)\n    server.listen(address, ssl=ssl, family=family, flags=flags, backlog=backlog)\n    return server"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, address, **kwargs):\n        if self._transport:\n            raise RuntimeError('already connected')\n        kwargs.setdefault('timeout', self._timeout)\n        conn = create_connection(self._protocol_factory, address, **kwargs)\n        self._transport = conn[0]\n        self._transport._log = self._log\n        self._protocol = conn[1]\n        self._protocol._log = self._log", "response": "Connect to a new address and wait for the connection to be established."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_connection(self, client, ssl):\n        if ssl:\n            context = ssl if hasattr(ssl, 'set_ciphers') \\\n                            else ssl(client) if callable(ssl) \\\n                            else create_default_context(True)\n            transport = SslTransport(client, context, True)\n        else:\n            transport = Transport(client)\n        transport._log = self._log\n        transport._server = self\n        if DEBUG:\n            self._log.debug('new connection on {}', saddr(client.getsockname()))\n            if hasattr(client, 'getpeername'):\n                self._log.debug('remote peer is {}', saddr(client.getpeername()))\n        protocol = self._protocol_factory()\n        protocol._log = self._log\n        protocol._timeout = self._timeout\n        self._connections[transport] = protocol\n        self.connection_made(transport, protocol)\n        transport.start(protocol)", "response": "Handle a new connection with handle client."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new transport bind it to address and start listening for new connections.", "response": "def listen(self, address, ssl=False, family=0, flags=0, ipc=False, backlog=128):\n        \"\"\"Create a new transport, bind it to *address*, and start listening\n        for new connections.\n\n        See :func:`create_server` for a description of *address* and the\n        supported keyword arguments.\n        \"\"\"\n        handles = []\n        handle_args = ()\n        if isinstance(address, six.string_types):\n            handle_type = pyuv.Pipe\n            handle_args = (ipc,)\n            addresses = [address]\n        elif isinstance(address, tuple):\n            handle_type = pyuv.TCP\n            result = getaddrinfo(address[0], address[1], family, socket.SOCK_STREAM,\n                                 socket.IPPROTO_TCP, flags)\n            addresses = [res[4] for res in result]\n        elif isinstance(address, pyuv.Stream):\n            handles.append(address)\n            addresses = []\n        else:\n            raise TypeError('expecting a string, tuple or pyuv.Stream')\n        for addr in addresses:\n            handle = handle_type(self._hub.loop, *handle_args)\n            try:\n                if compat.pyuv_pipe_helper(handle, handle_args, 'bind', addr):\n                    handles.append(handle)\n                    break\n                handle.bind(addr)\n            except pyuv.error.UVError as e:\n                self._log.warning('bind error {!r}, skipping {}', e[0], saddr(addr))\n                continue\n            handles.append(handle)\n        addresses = []\n        for handle in handles:\n            if backlog is not None:\n                callback = functools.partial(self._on_new_connection, ssl=ssl)\n                handle.listen(callback, backlog)\n                addr = handle.getsockname()\n                self._log.debug('listen on {}', saddr(addr))\n            addresses.append(addr)\n        self._handles += handles\n        self._addresses += addresses"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        for handle in self._handles:\n            if not handle.closed:\n                handle.close()\n        del self._handles[:]\n        for transport, _ in self.connections:\n            transport.close()\n        self._all_closed.wait()", "response": "Close the listening sockets and all accepted connections."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef main(argd=None):\n    global DEBUG, debug\n\n    # The argd parameter for main() is for testing purposes only.\n    argd = argd or docopt(\n        USAGESTR,\n        version=VERSIONSTR,\n        script=SCRIPT,\n        # Example usage of colr_docopt colors.\n        colors={\n            'header': {'fore': 'yellow'},\n            'script': {'fore': 'lightblue', 'style': 'bright'},\n            'version': {'fore': 'lightblue'},\n        }\n    )\n\n    DEBUG = argd['--debug']\n    # Load real debug function if available.\n    if DEBUG:\n        load_debug_deps()\n    else:\n        debug = noop\n\n    if argd['--auto-disable']:\n        auto_disable()\n\n    if argd['--names']:\n        return list_names()\n    elif argd['--translate']:\n        # Just translate a simple code and exit.\n        try:\n            print('\\n'.join(\n                translate(\n                    argd['CODE'] or read_stdin().split(),\n                    rgb_mode=argd['--truecolor'],\n                )\n            ))\n        except ValueError as ex:\n            print_err('Translation error: {}'.format(ex))\n            return 1\n        return 0\n    elif argd['--listcodes']:\n        # List all escape codes found in some text and exit.\n        return list_known_codes(\n            argd['TEXT'] or read_stdin(),\n            unique=argd['--unique'],\n            rgb_mode=argd['--truecolor'],\n        )\n\n    txt = argd['TEXT'] or read_stdin()\n    fd = sys.stderr if argd['--err'] else sys.stdout\n    end = '' if argd['--newline'] else '\\n'\n\n    if argd['--stripcodes']:\n        txt = justify(strip_codes(txt), argd)\n        print(txt, file=fd, end=end)\n        return 0\n\n    clr = get_colr(txt, argd)\n\n    # Center, ljust, rjust, or not.\n    clr = justify(clr, argd)\n    if clr:\n        print(str(clr), file=fd, end=end)\n        return 0\n    # Error while building Colr.\n    return 1", "response": "Main entry point for the colr_docopt script."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef debug(*args, **kwargs):\n    if kwargs.get('file', None) is None:\n        kwargs['file'] = sys.stderr\n    msg = kwargs.get('sep', ' ').join(str(a) for a in args)\n    print('debug: {}'.format(msg), **kwargs)", "response": "Print to stderr unless printdebug is installed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntries popping a key from a dict. Otherwise return the default value.", "response": "def dict_pop_or(d, key, default=None):\n    \"\"\" Try popping a key from a dict.\n        Instead of raising KeyError, just return the default value.\n    \"\"\"\n    val = default\n    with suppress(KeyError):\n        val = d.pop(key)\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Colr instance based on user args.", "response": "def get_colr(txt, argd):\n    \"\"\" Return a Colr instance based on user args. \"\"\"\n    fore = parse_colr_arg(\n        get_name_arg(argd, '--fore', 'FORE', default=None),\n        rgb_mode=argd['--truecolor'],\n    )\n    back = parse_colr_arg(\n        get_name_arg(argd, '--back', 'BACK', default=None),\n        rgb_mode=argd['--truecolor'],\n    )\n    style = get_name_arg(argd, '--style', 'STYLE', default=None)\n    if argd['--gradient']:\n        # Build a gradient from user args.\n        return C(txt).gradient(\n            name=argd['--gradient'],\n            spread=try_int(argd['--spread'], 1, minimum=0),\n            fore=fore,\n            back=back,\n            style=style,\n            rgb_mode=argd['--truecolor'],\n        )\n    if argd['--gradientrgb']:\n        # Build an rgb gradient from user args.\n        rgb_start, rgb_stop = parse_gradient_rgb_args(argd['--gradientrgb'])\n        return C(txt).gradient_rgb(\n            fore=fore,\n            back=back,\n            style=style,\n            start=rgb_start,\n            stop=rgb_stop,\n        )\n    if argd['--rainbow']:\n        return C(txt).rainbow(\n            fore=fore,\n            back=back,\n            style=style,\n            freq=try_float(argd['--frequency'], 0.1, minimum=0),\n            offset=try_int(argd['--offset'], randint(0, 255), minimum=0),\n            spread=try_float(argd['--spread'], 3.0, minimum=0),\n            rgb_mode=argd['--truecolor'],\n        )\n\n    # Normal colored output.\n    return C(txt, fore=fore, back=back, style=style)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the first argument value given in a docopt arg dict.", "response": "def get_name_arg(argd, *argnames, default=None):\n    \"\"\" Return the first argument value given in a docopt arg dict.\n        When not given, return default.\n    \"\"\"\n    val = None\n    for argname in argnames:\n        if argd[argname]:\n            val = argd[argname].lower().strip()\n            break\n    return val if val else default"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_err(*args):\n    if DEBUG:\n        print_err(traceback.format_exc(), color=False)\n    else:\n        print_err(*args, newline=True)", "response": "Handle fatal errors caught in __main__ scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef justify(clr, argd):\n    methodmap = {\n        '--ljust': clr.ljust,\n        '--rjust': clr.rjust,\n        '--center': clr.center,\n    }\n    for flag in methodmap:\n        if argd[flag]:\n            if argd[flag] in ('0', '-'):\n                val = get_terminal_size(default=(80, 35))[0]\n            else:\n                val = try_int(argd[flag], minimum=None)\n                if val < 0:\n                    # Negative value, subtract from terminal width.\n                    val = get_terminal_size(default=(80, 35))[0] + val\n            return methodmap[flag](val)\n\n    # No justify args given.\n    return clr", "response": "Justify str or Colr based on user args."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_known_codes(s, unique=True, rgb_mode=False):\n    total = 0\n    for codedesc in get_known_codes(s, unique=unique, rgb_mode=rgb_mode):\n        total += 1\n        print(codedesc)\n    plural = 'code' if total == 1 else 'codes'\n    codetype = ' unique' if unique else ''\n    print('\\nFound {}{} escape {}.'.format(total, codetype, plural))\n    return 0 if total > 0 else 1", "response": "Find and print all known escape codes in a string"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all known color names.", "response": "def list_names():\n    \"\"\" List all known color names. \"\"\"\n    names = get_all_names()\n    # This is 375 right now. Probably won't ever change, but I'm not sure.\n    nameslen = len(names)\n    print('\\nListing {} names:\\n'.format(nameslen))\n    # Using 3 columns of names, still alphabetically sorted from the top down.\n    # Longest name so far: lightgoldenrodyellow (20 chars)\n    namewidth = 20\n    # namewidth * columns == 60, colorwidth * columns == 18, final == 78.\n    swatch = ' ' * 9\n\n    third = nameslen // 3\n    lastthird = third * 2\n    cols = (\n        names[0: third],\n        names[third: lastthird],\n        names[lastthird:],\n    )\n    # Exactly enough spaces to fill in a blank item (+2 for ': ').\n    # This may not ever be used, unless another 'known name' is added.\n    blankitem = ' ' * (namewidth + len(swatch) + 2)\n\n    for i in range(third):\n        nameset = []\n        for colset in cols:\n            try:\n                nameset.append(colset[i])\n            except IndexError:\n                nameset.append(None)\n                continue\n        line = C('').join(\n            C(': ').join(\n                C(name.rjust(namewidth)),\n                C(swatch, back=name),\n            ) if name else blankitem\n            for name in nameset\n        )\n        print(line)\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing one or two rgb args given with - G flag.", "response": "def parse_gradient_rgb_args(args):\n    \"\"\" Parse one or two rgb args given with --gradientrgb.\n        Raises InvalidArg for invalid rgb values.\n        Returns a tuple of (start_rgb, stop_rgb), where the stop_rgb may be\n        None if only one arg value was given and start_rgb may be None if\n        no values were given.\n    \"\"\"\n    arglen = len(args)\n    if arglen < 1 or arglen > 2:\n        raise InvalidArg(arglen, label='Expecting 1 or 2 \\'-G\\' flags, got')\n    start_rgb = try_rgb(args[0]) if args else None\n    stop_rgb = try_rgb(args[1]) if arglen > 1 else None\n    return start_rgb, stop_rgb"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_err(*args, **kwargs):\n    if kwargs.get('file', None) is None:\n        kwargs['file'] = sys.stderr\n\n    color = dict_pop_or(kwargs, 'color', True)\n    # Use color if asked, but only if the file is a tty.\n    if color and kwargs['file'].isatty():\n        # Keep any Colr args passed, convert strs into Colrs.\n        msg = kwargs.get('sep', ' ').join(\n            str(a) if isinstance(a, C) else str(C(a, 'red'))\n            for a in args\n        )\n    else:\n        # The file is not a tty anyway, no escape codes.\n        msg = kwargs.get('sep', ' ').join(\n            str(a.stripped() if isinstance(a, C) else a)\n            for a in args\n        )\n    newline = dict_pop_or(kwargs, 'newline', False)\n    if newline:\n        msg = '\\n{}'.format(msg)\n    print(msg, **kwargs)", "response": "A wrapper for print that uses stderr by default."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_stdin():\n    if sys.stdin.isatty() and sys.stdout.isatty():\n        print('\\nReading from stdin until end of file (Ctrl + D)...')\n\n    return sys.stdin.read()", "response": "Read text from stdin and print a helpful message for ttys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate(usercodes, rgb_mode=False):\n    for code in usercodes:\n        code = code.strip().lower()\n        if code.isalpha() and (code in codes['fore']):\n            # Basic color name.\n            yield translate_basic(code)\n        else:\n            if ',' in code:\n                try:\n                    r, g, b = (int(c.strip()) for c in code.split(','))\n                except (TypeError, ValueError):\n                    raise InvalidColr(code)\n                code = (r, g, b)\n\n            colorcode = ColorCode(code, rgb_mode=rgb_mode)\n\n            if disabled():\n                yield str(colorcode)\n            yield colorcode.example()", "response": "Translate one or more hex term or rgb value into the others."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntranslates a basic color name to color with explanation.", "response": "def translate_basic(usercode):\n    \"\"\" Translate a basic color name to color with explanation. \"\"\"\n    codenum = get_code_num(codes['fore'][usercode])\n    colorcode = codeformat(codenum)\n    msg = 'Name: {:>10}, Number: {:>3}, EscapeCode: {!r}'.format(\n        usercode,\n        codenum,\n        colorcode\n    )\n    if disabled():\n        return msg\n    return str(C(msg, fore=usercode))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef try_float(s, default=None, minimum=None):\n    if not s:\n        return default\n    try:\n        val = float(s)\n    except (TypeError, ValueError):\n        raise InvalidNumber(s, label='Invalid float value')\n    if (minimum is not None) and (val < minimum):\n        val = minimum\n    return val", "response": "Try parsing a string into a float. If the string is not a float default is returned. If the string is not a float default is returned. If the string is not a float default is returned."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntry parsing a string into an integer.", "response": "def try_int(s, default=None, minimum=None):\n    \"\"\" Try parsing a string into an integer.\n        If None is passed, default is returned.\n        On failure, InvalidNumber is raised.\n    \"\"\"\n    if not s:\n        return default\n    try:\n        val = int(s)\n    except (TypeError, ValueError):\n        raise InvalidNumber(s)\n    if (minimum is not None) and (val < minimum):\n        val = minimum\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrying parsing a string into an rgb value.", "response": "def try_rgb(s, default=None):\n    \"\"\" Try parsing a string into an rgb value (int, int, int),\n        where the ints are 0-255 inclusive.\n        If None is passed, default is returned.\n        On failure, InvalidArg is raised.\n    \"\"\"\n    if not s:\n        return default\n    try:\n        r, g, b = (int(x.strip()) for x in s.split(','))\n    except ValueError:\n        raise InvalidRgb(s)\n    if not all(in_range(x, 0, 255) for x in (r, g, b)):\n        raise InvalidRgb(s)\n\n    return r, g, b"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_default_context(purpose=None, **kwargs):\n    if hasattr(ssl, 'create_default_context'):\n        # Python 2.7.9+, Python 3.4+: take a server_side boolean or None, in\n        # addition to the ssl.Purpose.XX values. This allows a user to write\n        # code that works on all supported Python versions.\n        if purpose is None or purpose is False:\n            purpose = ssl.Purpose.SERVER_AUTH\n        elif purpose is True:\n            purpose = ssl.Purpose.CLIENT_AUTH\n        return ssl.create_default_context(purpose, **kwargs)\n    # Python 2.7.8, Python 3.3\n    context = SSLContext(ssl.PROTOCOL_SSLv23)\n    if kwargs.get('cafile'):\n        context.load_verify_locations(kwargs['cafile'])\n    return context", "response": "Create a new SSL context in the most secure way available on the current\n    Python version. See ssl. create_default_context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logger(context=None, name=None):\n    # Many class instances have their own logger. Share them to save memory if\n    # possible, i.e. when *context* is not set.\n    if name is None:\n        name = _logger_name\n    if context is None and name in _logger_dict:\n        return _logger_dict[name]\n    if context is not None and not isinstance(context, six.string_types):\n        context = util.objref(context)\n    logger = logging.getLogger(name)\n    logger = ContextLogger(logger, context)\n    if context is None:\n        _logger_dict[name] = logger\n    return logger", "response": "Return a logger for the given context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a string identifying the current thread and fiber.", "response": "def thread_info(self):\n        \"\"\"Return a string identifying the current thread and fiber.\"\"\"\n        tid = threading.current_thread().name\n        if tid == 'MainThread':\n            tid = 'Main'\n        current = fibers.current()\n        fid = getattr(current, 'name') if current.parent else 'Root'\n        return '{}/{}'.format(tid, fid)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef frame_info(self):\n        if not self._logger.isEnabledFor(logging.DEBUG):\n            return ''\n        f = sys._getframe(3)\n        fname = os.path.split(f.f_code.co_filename)[1]\n        return '{}:{}'.format(fname, f.f_lineno)", "response": "Return a string identifying the current frame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the high and low watermarks for the read buffer.", "response": "def set_buffer_limits(self, high=None, low=None):\n        \"\"\"Set the low and high watermarks for the read buffer.\"\"\"\n        if high is None:\n            high = self.default_buffer_size\n        if low is None:\n            low = high // 2\n        self._buffer_high = high\n        self._buffer_low = low"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef feed(self, data):\n        self._buffers.append(data)\n        self._buffer_size += len(data)\n        self._maybe_pause_transport()\n        self._can_read.set()", "response": "Add data to the buffer and update the internal buffer size."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping the stream with the specified encoding and keyword arguments.", "response": "def wrap(self, encoding, **textio_args):\n        \"\"\"Return a :class:`io.TextIOWrapper` that wraps the stream.\n\n        The wrapper provides text IO on top of the byte stream, using the\n        specified *encoding*. The *textio_args* keyword arguments are\n        additional keyword arguments passed to the :class:`~io.TextIOWrapper`\n        constructor. Unless another buffering scheme is specified, the\n        *write_through* option is enabled.\n        \"\"\"\n        # By default we want write_through behavior, unless the user specifies\n        # something else.\n        if 'line_buffering' not in textio_args and 'write_through' not in textio_args:\n            textio_args['write_through'] = True\n        return compat.TextIOWrapper(self, encoding, **textio_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self, size=-1):\n        self._check_readable()\n        chunks = []\n        bytes_read = 0\n        bytes_left = size\n        while True:\n            chunk = self._buffer.get_chunk(bytes_left)\n            if not chunk:\n                break\n            chunks.append(chunk)\n            bytes_read += len(chunk)\n            if bytes_read == size or not chunk:\n                break\n            if bytes_left > 0:\n                bytes_left -= len(chunk)\n        # If EOF was set, always return that instead of any error.\n        if not chunks and not self._buffer.eof and self._buffer.error:\n            raise compat.saved_exc(self._buffer.error)\n        return b''.join(chunks)", "response": "Read up to size bytes from the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read1(self, size=-1):\n        self._check_readable()\n        chunk = self._buffer.get_chunk(size)\n        if not chunk and not self._buffer.eof and self._buffer.error:\n            raise compat.saved_exc(self._buffer.error)\n        return chunk", "response": "Read up to size bytes from the internal buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a single line from the file.", "response": "def readline(self, limit=-1, delim=b'\\n'):\n        \"\"\"Read a single line.\n\n        If EOF is reached before a full line can be read, a partial line is\n        returned. If *limit* is specified, at most this many bytes will be read.\n        \"\"\"\n        self._check_readable()\n        chunks = []\n        while True:\n            chunk = self._buffer.get_chunk(limit, delim)\n            if not chunk:\n                break\n            chunks.append(chunk)\n            if chunk.endswith(delim):\n                break\n            if limit >= 0:\n                limit -= len(chunk)\n                if limit == 0:\n                    break\n        if not chunks and not self._buffer.eof and self._buffer.error:\n            raise compat.saved_exc(self._buffer.error)\n        return b''.join(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading lines until EOF and return them as a list.", "response": "def readlines(self, hint=-1):\n        \"\"\"Read lines until EOF, and return them as a list.\n\n        If *hint* is specified, then stop reading lines as soon as the total\n        size of all lines exceeds *hint*.\n        \"\"\"\n        self._check_readable()\n        lines = []\n        chunks = []\n        bytes_read = 0\n        while True:\n            chunk = self._buffer.get_chunk(-1, b'\\n')\n            if not chunk:\n                break\n            chunks.append(chunk)\n            if chunk.endswith(b'\\n'):\n                lines.append(b''.join(chunks))\n                del chunks[:]\n                bytes_read += len(lines[-1])\n            if hint >= 0 and bytes_read > hint:\n                break\n        if chunks:\n            lines.append(b''.join(chunks))\n        if not lines and not self._buffer.eof and self._buffer.error:\n            raise compat.saved_exc(self._buffer.error)\n        return lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, data):\n        self._check_writable()\n        self._transport._can_write.wait()\n        self._transport.write(data)", "response": "Write data to the transport."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the elements of the sequence seq to the transport.", "response": "def writelines(self, seq):\n        \"\"\"Write the elements of the sequence *seq* to the transport.\n\n        This method will block if the transport's write buffer is at capacity.\n        \"\"\"\n        self._check_writable()\n        for line in seq:\n            self._transport._can_write.wait()\n            self._transport.write(line)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_eof(self):\n        self._check_writable()\n        self._transport._can_write.wait()\n        self._transport.write_eof()", "response": "Close the write direction of the transport."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing the underlying transport and the underlying transport.", "response": "def close(self):\n        \"\"\"Close the stream.\n\n        If *autoclose* was passed to the constructor then the underlying\n        transport will be closed as well.\n        \"\"\"\n        if self._closed:\n            return\n        if self._autoclose:\n            self._transport.close()\n            self._transport._closed.wait()\n        self._transport = None\n        self._closed = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a function that uses blocking IO.", "response": "def blocking(func, *args, **kwargs):\n    \"\"\"Run a function that uses blocking IO.\n\n    The function is run in the IO thread pool.\n    \"\"\"\n    pool = get_io_pool()\n    fut = pool.submit(func, *args, **kwargs)\n    return fut.result()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield the objects as they become ready.", "response": "def as_completed(objects, count=None, timeout=None):\n    \"\"\"Wait for one or more waitable objects, yielding them as they become\n    ready.\n\n    This is the iterator/generator version of :func:`wait`.\n    \"\"\"\n    for obj in objects:\n        if not hasattr(obj, 'add_done_callback'):\n            raise TypeError('Expecting sequence of waitable objects')\n    if count is None:\n        count = len(objects)\n    if count < 0 or count > len(objects):\n        raise ValueError('count must be between 0 and len(objects)')\n    if count == 0:\n        return\n    pending = list(objects)\n    for obj in _wait(pending, timeout):\n        yield obj\n        count -= 1\n        if count == 0:\n            break"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait(objects, count=None, timeout=None):\n    for obj in objects:\n        if not hasattr(obj, 'add_done_callback'):\n            raise TypeError('Expecting sequence of waitable objects')\n    if count is None:\n        count = len(objects)\n    if count < 0 or count > len(objects):\n        raise ValueError('count must be between 0 and len(objects)')\n    if count == 0:\n        return [], objects\n    pending = list(objects)\n    done = []\n    try:\n        for obj in _wait(pending, timeout):\n            done.append(obj)\n            if len(done) == count:\n                break\n    except Timeout:\n        pass\n    return done, list(filter(bool, pending))", "response": "Wait for one or more waitable objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether this future was successfully cancelled.", "response": "def cancelled(self):\n        \"\"\"Return whether this future was successfully cancelled.\"\"\"\n        return self._state == self.S_EXCEPTION and isinstance(self._result, Cancelled)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel(self):\n        # We leverage/abuse our _done Event's thread lock as our own lock.\n        # Since it's a private copy it should be OK, and it saves some memory.\n        # Just be sure that we don't modify the event with the lock held.\n        with self._lock:\n            if self._state not in (self.S_PENDING, self.S_RUNNING):\n                return False\n            self._result = Cancelled('cancelled by Future.cancel()')\n            self._state = self.S_EXCEPTION\n            self._done.set()\n            return True", "response": "Cancel the execution of the async function if possible."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef result(self, timeout=None):\n        if not self._done.wait(timeout):\n            raise Timeout('timeout waiting for future')\n        # No more state changes after _done is set so no lock needed.\n        if self._state == self.S_EXCEPTION:\n            raise compat.saved_exc(self._result)\n        return self._result", "response": "Wait for the future to complete and return its result."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for the async function to complete and return its exception.", "response": "def exception(self, timeout=None):\n        \"\"\"Wait for the async function to complete and return its exception.\n\n        If the function did not raise an exception this returns ``None``.\n        \"\"\"\n        if not self._done.wait(timeout):\n            raise Timeout('timeout waiting for future')\n        if self._state == self.S_EXCEPTION:\n            return self._result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_done_callback(self, callback, *args):\n        with self._lock:\n            if self._state not in (self.S_DONE, self.S_EXCEPTION):\n                return add_callback(self, callback, args)\n        callback(*args)", "response": "Add a callback that gets called when the future completes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submit(self, func, *args):\n        with self._lock:\n            if self._closing:\n                raise RuntimeError('pool is closing/closed')\n            result = Future()\n            self._queue.put_nowait((func, args, result))\n            self._spawn_workers()\n        return result", "response": "Submit a function asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies a function to the elements of the sequences in iterables.", "response": "def map(self, func, *iterables, **kwargs):\n        \"\"\"Apply *func* to the elements of the sequences in *iterables*.\n\n        All invocations of *func* are run in the pool. If multiple iterables\n        are provided, then *func* must take this many arguments, and is applied\n        with one element from each iterable. All iterables must yield the same\n        number of elements.\n\n        An optional *timeout* keyword argument may be provided to specify a\n        timeout.\n\n        This returns a generator yielding the results.\n        \"\"\"\n        with self._lock:\n            if self._closing:\n                raise RuntimeError('pool is closing/closed')\n            timeout = kwargs.pop('timeout', None)\n            futures = []\n            for args in zip(*iterables):\n                result = Future()\n                self._queue.put_nowait((func, args, result))\n                futures.append(result)\n            self._spawn_workers()\n        try:\n            with switch_back(timeout):\n                for future in futures:\n                    yield future.result()\n        except Exception:\n            # Timeout, GeneratorExit or future.set_exception()\n            for future in futures:\n                if not future.done():\n                    future.cancel()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        with self._lock:\n            if self._closing:\n                return\n            self._closing = True\n            if not self._workers:\n                self._closed.set()\n                return\n            self._queue.put_nowait(self._PoolClosing)\n        self._closed.wait()", "response": "Close the pool and wait for all workers to exit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fingerprint(channel_samples: list, Fs: int = DEFAULT_FS,\n                wsize: int = DEFAULT_WINDOW_SIZE,\n                wratio: Union[int, float] = DEFAULT_OVERLAP_RATIO,\n                fan_value: int = DEFAULT_FAN_VALUE,\n                amp_min: Union[int, float] = DEFAULT_AMP_MIN)-> Iterator[tuple]:\n    \"\"\"\n    FFT the channel, log transform output, find local maxima, then return\n    locally sensitive hashes.\n    # \"\"\"\n    # FFT the signal and extract frequency components\n    arr2D = mlab.specgram(\n        channel_samples,\n        NFFT=wsize,\n        Fs=Fs,\n        window=mlab.window_hanning,\n        noverlap=int(wsize * wratio))[0]\n\n    arr2D = 10 * np.log10(arr2D)\n\n    arr2D[arr2D == -np.inf] = 0  # replace infs with zeros\n\n    # find local maxima\n    local_maxima = get_2D_peaks(arr2D, plot=False, amp_min=amp_min)\n\n    # return hashes\n    return generate_hashes(local_maxima, fan_value=fan_value)", "response": "Generate a fingerprint of the given samples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_hashes(peaks, fan_value: int = DEFAULT_FAN_VALUE):\n    if PEAK_SORT:\n        peaks = sorted(peaks, key=lambda x: x[1])\n        # peaks.sort(key=itemgetter(1))\n\n    for i in range(len(peaks)):\n        for j in range(1, fan_value):\n            if (i + j) < len(peaks):\n\n                freq1 = peaks[i][IDX_FREQ_I]\n                freq2 = peaks[i + j][IDX_FREQ_I]\n                t1 = peaks[i][IDX_TIME_J]\n                t2 = peaks[i + j][IDX_TIME_J]\n                t_delta = t2 - t1\n\n                if MIN_HASH_TIME_DELTA <= t_delta <= MAX_HASH_TIME_DELTA:\n                    key = \"{}|{}|{}\".format(freq1, freq2, t_delta)\n                    h = hashlib.sha1(key.encode('utf-8'))\n                    yield (h.hexdigest()[0:FINGERPRINT_REDUCTION], t1)", "response": "Generate a list of hashes for the given list of peaks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_handshake(self, callback=None):\n        if self._state != self.S_UNWRAPPED:\n            raise RuntimeError('handshake in progress or completed')\n        self._sslobj = sslcompat.wrap_bio(self._context, self._incoming, self._outgoing,\n                                          self._server_side, self._server_hostname)\n        self._state = self.S_DO_HANDSHAKE\n        self._handshake_cb = callback\n        ssldata, appdata = self.feed_ssldata(b'')\n        assert len(appdata) == 0\n        return ssldata", "response": "Start the SSL handshake. Return a list of ssldata."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstarting the SSL shutdown sequence. Return a list of ssldata.", "response": "def shutdown(self, callback=None):\n        \"\"\"Start the SSL shutdown sequence. Return a list of ssldata.\n\n        The optional *callback* argument can be used to install a callback that\n        will be called when the shutdown is complete. The callback will be\n        called without arguments.\n        \"\"\"\n        if self._state == self.S_UNWRAPPED:\n            raise RuntimeError('no security layer present')\n        self._state = self.S_SHUTDOWN\n        self._shutdown_cb = callback\n        ssldata, appdata = self.feed_ssldata(b'')\n        assert appdata == [] or appdata == [b'']\n        return ssldata"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef feed_eof(self):\n        self._incoming.write_eof()\n        ssldata, appdata = self.feed_ssldata(b'')\n        assert appdata == [] or appdata == [b'']", "response": "Send a potentially \"ragged\" EOF."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef feed_ssldata(self, data):\n        if self._state == self.S_UNWRAPPED:\n            # If unwrapped, pass plaintext data straight through.\n            return ([], [data] if data else [])\n        ssldata = []; appdata = []\n        self._need_ssldata = False\n        if data:\n            self._incoming.write(data)\n        try:\n            if self._state == self.S_DO_HANDSHAKE:\n                # Call do_handshake() until it doesn't raise anymore.\n                self._sslobj.do_handshake()\n                self._state = self.S_WRAPPED\n                if self._handshake_cb:\n                    self._handshake_cb()\n            if self._state == self.S_WRAPPED:\n                # Main state: read data from SSL until close_notify\n                while True:\n                    chunk = self._sslobj.read(self.bufsize)\n                    appdata.append(chunk)\n                    if not chunk:  # close_notify\n                        break\n            if self._state == self.S_SHUTDOWN:\n                # Call shutdown() until it doesn't raise anymore.\n                self._sslobj.unwrap()\n                self._sslobj = None\n                self._state = self.S_UNWRAPPED\n                if self._shutdown_cb:\n                    self._shutdown_cb()\n            if self._state == self.S_UNWRAPPED:\n                # Drain possible plaintext data after close_notify.\n                appdata.append(self._incoming.read())\n        except (ssl.SSLError, sslcompat.CertificateError) as e:\n            if getattr(e, 'errno', None) not in (ssl.SSL_ERROR_WANT_READ,\n                        ssl.SSL_ERROR_WANT_WRITE, ssl.SSL_ERROR_SYSCALL):\n                if self._state == self.S_DO_HANDSHAKE and self._handshake_cb:\n                    self._handshake_cb(e)\n                raise\n            self._need_ssldata = e.errno == ssl.SSL_ERROR_WANT_READ\n        # Check for record level data that needs to be sent back.\n        # Happens for the initial handshake and renegotiations.\n        if self._outgoing.pending:\n            ssldata.append(self._outgoing.read())\n        return (ssldata, appdata)", "response": "Feed SSL record level data into the pipe."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef feed_appdata(self, data, offset=0):\n        if self._state == self.S_UNWRAPPED:\n            # pass through data in unwrapped mode\n            return ([data[offset:]] if offset < len(data) else [], len(data))\n        ssldata = []\n        view = memoryview(data)\n        while True:\n            self._need_ssldata = False\n            try:\n                if offset < len(view):\n                    offset += self._sslobj.write(view[offset:])\n            except ssl.SSLError as e:\n                # It is not allowed to call write() after unwrap() until the\n                # close_notify is acknowledged. We return the condition to the\n                # caller as a short write.\n                if sslcompat.get_reason(e) == 'PROTOCOL_IS_SHUTDOWN':\n                    e.errno = ssl.SSL_ERROR_WANT_READ\n                if e.errno not in (ssl.SSL_ERROR_WANT_READ, ssl.SSL_ERROR_WANT_WRITE,\n                                   ssl.SSL_ERROR_SYSCALL):\n                    raise\n                self._need_ssldata = e.errno == ssl.SSL_ERROR_WANT_READ\n            # See if there's any record level data back for us.\n            if self._outgoing.pending:\n                ssldata.append(self._outgoing.read())\n            if offset == len(view) or self._need_ssldata:\n                break\n        return (ssldata, offset)", "response": "Feed plaintext data into the pipe."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the extra info for the given name.", "response": "def get_extra_info(self, name, default=None):\n        \"\"\"Return transport specific data.\n\n        The following fields are available, in addition to the information\n        exposed by :meth:`Transport.get_extra_info`.\n\n        ======================  ===============================================\n        Name                    Description\n        ======================  ===============================================\n        ``'ssl'``               The internal ``ssl.SSLObject`` instance used by\n                                this transport.\n        ``'sslctx'``            The ``ssl.SSLContext`` instance used to create\n                                the SSL object.\n        ======================  ===============================================\n        \"\"\"\n        if name == 'ssl':\n            return self._sslpipe.ssl_object\n        elif name == 'sslctx':\n            return self._sslpipe.context\n        else:\n            return super(SslTransport, self).get_extra_info(name, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nstarts the SSL handshake.", "response": "def do_handshake(self):\n        \"\"\"Start the SSL handshake.\n\n        This method only needs to be called if this transport was created with\n        *do_handshake_on_connect* set to False (the default is True).\n\n        The handshake needs to be synchronized between the both endpoints, so\n        that SSL record level data is not incidentially interpreted as\n        plaintext. Usually this is done by starting the handshake directly\n        after a connection is established, but you can also use an application\n        level protocol.\n        \"\"\"\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._closing or self._handle.closed:\n            raise TransportError('SSL transport is closing/closed')\n        self._write_backlog.append([b'', True])\n        self._process_write_backlog()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unwrap(self):\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._closing or self._handle.closed:\n            raise TransportError('SSL transport is closing/closed')\n        self._close_on_unwrap = False\n        self._write_backlog.append([b'', False])\n        self._process_write_backlog()", "response": "Unwrap the security layer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_locked(lock):\n    if hasattr(lock, 'locked'):\n        return lock.locked()\n    elif hasattr(lock, '_is_owned'):\n        return lock._is_owned()\n    else:\n        raise TypeError('expecting Lock/RLock')", "response": "Return whether a lock is locked."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nacquire a lock and restore its state.", "response": "def acquire_restore(lock, state):\n    \"\"\"Acquire a lock and restore its state.\"\"\"\n    if hasattr(lock, '_acquire_restore'):\n        lock._acquire_restore(state)\n    elif hasattr(lock, 'acquire'):\n        lock.acquire()\n    else:\n        raise TypeError('expecting Lock/RLock')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreleasing a lock and return its state.", "response": "def release_save(lock):\n    \"\"\"Release a lock and return its state.\"\"\"\n    if hasattr(lock, '_release_save'):\n        return lock._release_save()\n    elif hasattr(lock, 'release'):\n        lock.release()\n    else:\n        raise TypeError('expecting Lock/RLock')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the thread lock for lock.", "response": "def thread_lock(lock):\n    \"\"\"Return the thread lock for *lock*.\"\"\"\n    if hasattr(lock, '_lock'):\n        return lock._lock\n    elif hasattr(lock, 'acquire'):\n        return lock\n    else:\n        raise TypeError('expecting Lock/RLock')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nacquires the lock. If *blocking* is true (the default), then this will block until the lock can be acquired. The *timeout* parameter specifies an optional timeout in seconds. The return value is a boolean indicating whether the lock was acquired.", "response": "def acquire(self, blocking=True, timeout=None):\n        \"\"\"Acquire the lock.\n\n        If *blocking* is true (the default), then this will block until the\n        lock can be acquired. The *timeout* parameter specifies an optional\n        timeout in seconds.\n\n        The return value is a boolean indicating whether the lock was acquired.\n        \"\"\"\n        hub = get_hub()\n        try:\n            # switcher.__call__ needs to be synchronized with a lock IF it can\n            # be called from different threads. This is the case here because\n            # this method may be called from multiple threads and the callbacks\n            # are run in the calling thread. So pass it our _lock.\n            with switch_back(timeout, lock=self._lock) as switcher:\n                with self._lock:\n                    if not self._locked:\n                        self._locked = 1\n                        self._owner = fibers.current()\n                        return True\n                    elif self._reentrant and self._owner is fibers.current():\n                        self._locked += 1\n                        return True\n                    elif not blocking:\n                        return False\n                    handle = add_callback(self, switcher)\n                # It is safe to call hub.switch() outside the lock. Another\n                # thread could have called acquire()+release(), thereby firing\n                # the switchback. However the switchback only schedules the\n                # switchback in our hub, it won't execute it yet. So the\n                # switchback won't actually happen until we switch to the hub.\n                hub.switch()\n                # Here the lock should be ours because _release() wakes up only\n                # the fiber that it passed the lock.\n                assert self._locked > 0\n                assert self._owner is fibers.current()\n        except BaseException as e:\n            # Likely a Timeout but could also be e.g. Cancelled\n            with self._lock:\n                # Clean up the callback. It might have been popped by\n                # _release() but that is OK.\n                remove_callback(self, handle)\n                # This fiber was passed the lock but before that an exception\n                # was already scheduled with run_callback() (likely through\n                # Fiber.throw())\n                if self._owner is fibers.current():\n                    self._release()\n            if e is switcher.timeout:\n                return False\n            raise\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set(self):\n        with self._lock:\n            if self._flag:\n                return\n            self._flag = True\n            with assert_no_switchpoints():\n                run_callbacks(self)", "response": "Set the internal flag and wake up any fibers blocked on wait."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait(self, timeout=None):\n        # Optimization for the case the Event is already set.\n        if self._flag:\n            return True\n        hub = get_hub()\n        try:\n            with switch_back(timeout, lock=self._lock) as switcher:\n                with self._lock:\n                    # Need to check the flag again, now under the lock.\n                    if self._flag:\n                        return True\n                    # Allow other fibers to wake us up via callback in set().\n                    # The callback goes to switcher.switch directly() instead of\n                    # __call__(), because the latter would try to lock our lock\n                    # which is already held when callbacks are run by set().\n                    handle = add_callback(self, switcher.switch)\n                # See note in Lock.acquire() why we can call to hub.switch()\n                # outside the lock.\n                hub.switch()\n        except BaseException as e:\n            with self._lock:\n                remove_callback(self, handle)\n            if e is switcher.timeout:\n                return False\n            raise\n        return True", "response": "Block until the internal flag is set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nraising the condition and wake up fibers waiting on it.", "response": "def notify(self, n=1):\n        \"\"\"Raise the condition and wake up fibers waiting on it.\n\n        The optional *n* parameter specifies how many fibers will be notified.\n        By default, one fiber is notified.\n        \"\"\"\n        if not is_locked(self._lock):\n            raise RuntimeError('lock is not locked')\n        notified = [0]  # Work around lack of \"nonlocal\" in py27\n        def walker(switcher, predicate):\n            if not switcher.active:\n                return False  # not not keep switcher that timed out\n            if predicate and not predicate():\n                return True\n            if n >= 0 and notified[0] >= n:\n                return True\n            switcher.switch()\n            notified[0] += 1\n            return False  # only notify once\n        walk_callbacks(self, walker)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for(self, predicate, timeout=None):\n        if not is_locked(self._lock):\n            raise RuntimeError('lock is not locked')\n        hub = get_hub()\n        try:\n            with switch_back(timeout, lock=thread_lock(self._lock)) as switcher:\n                handle = add_callback(self, switcher, predicate)\n                # See the comment in Lock.acquire() why it is OK to release the\n                # lock here before calling hub.switch().\n                # Also if this is a reentrant lock make sure it is fully released.\n                state = release_save(self._lock)\n                hub.switch()\n        except BaseException as e:\n            with self._lock:\n                remove_callback(self, handle)\n            if e is switcher.timeout:\n                return False\n            raise\n        finally:\n            acquire_restore(self._lock, state)\n        return True", "response": "Like wait but additionally for predicate to be true."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nputs an item into the queue.", "response": "def put(self, item, block=True, timeout=None, size=None):\n        \"\"\"Put *item* into the queue.\n\n        If the queue is currently full and *block* is True (the default), then\n        wait up to *timeout* seconds for space to become available. If no\n        timeout is specified, then wait indefinitely.\n\n        If the queue is full and *block* is False or a timeout occurs, then\n        raise a :class:`QueueFull` exception.\n\n        The optional *size* argument may be used to specify a custom size for\n        the item. The total :meth:`qsize` of the queue is the sum of the sizes\n        of all the items. The default size for an item is 1.\n        \"\"\"\n        if size is None:\n            size = 1\n        with self._lock:\n            priority = self._get_item_priority(item)\n            while self._size + size > self.maxsize > 0:\n                if not block:\n                    raise QueueFull\n                if not self._notfull.wait_for(lambda: self._size+size <= self.maxsize, timeout):\n                    raise QueueFull\n            heapq.heappush(self._heap, (priority, size, item))\n            self._size += size\n            self._unfinished_tasks += 1\n            self._notempty.notify()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npop an item from the queue.", "response": "def get(self, block=True, timeout=None):\n        \"\"\"Pop an item from the queue.\n\n        If the queue is not empty, an item is returned immediately. Otherwise,\n        if *block* is True (the default), wait up to *timeout* seconds for an\n        item to become available. If not timeout is provided, then wait\n        indefinitely.\n\n        If the queue is empty and *block* is false or a timeout occurs, then\n        raise a :class:`QueueEmpty` exception.\n        \"\"\"\n        with self._lock:\n            while not self._heap:\n                if not block:\n                    raise QueueEmpty\n                if not self._notempty.wait(timeout):\n                    raise QueueEmpty\n            prio, size, item = heapq.heappop(self._heap)\n            self._size -= size\n            if 0 <= self._size < self.maxsize:\n                self._notfull.notify()\n        return item"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark a task as done.", "response": "def task_done(self):\n        \"\"\"Mark a task as done.\"\"\"\n        with self._lock:\n            unfinished = self._unfinished_tasks - 1\n            if unfinished < 0:\n                raise RuntimeError('task_done() called too many times')\n            elif unfinished == 0:\n                self._alldone.notify()\n            self._unfinished_tasks = unfinished"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nspawn a new child process.", "response": "def spawn(self, args, executable=None, stdin=None, stdout=None, stderr=None,\n              shell=False, cwd=None, env=None, flags=0, extra_handles=None):\n        \"\"\"Spawn a new child process.\n\n        The executable to spawn and its arguments are determined by *args*,\n        *executable* and *shell*.\n\n        When *shell* is set to ``False`` (the default), *args* is normally a\n        sequence and it contains both the program to execute (at index 0), and\n        its arguments.\n\n        When *shell* is set to ``True``, then *args* is normally a string and\n        it indicates the command to execute through the shell.\n\n        The *executable* argument can be used to override the executable to\n        execute. If *shell* is ``False``, it overrides ``args[0]``. This is\n        sometimes used on Unix to implement \"fat\" executables that behave\n        differently based on argv[0]. If *shell* is ``True``, it overrides the\n        shell to use. The default shell is ``'/bin/sh'`` on Unix, and the value\n        of $COMSPEC (or ``'cmd.exe'`` if it is unset) on Windows.\n\n        The *stdin*, *stdout* and *stderr* arguments specify how to handle\n        standard input, output, and error, respectively. If set to None, then\n        the child will inherit our respective stdio handle. If set to the\n        special constant ``PIPE`` then a pipe is created. The pipe will be\n        connected to a :class:`gruvi.StreamProtocol` which you can use to read\n        or write from it. The stream protocol instance is available under\n        either :attr:`stdin`, :attr:`stdout` or :attr:`stderr`. All 3 stdio\n        arguments can also be a file descriptor, a file-like object, or a pyuv\n        ``Stream`` instance.\n\n        The *extra_handles* specifies any extra handles to pass to the client.\n        It must be a sequence where each element is either a file descriptor, a\n        file-like objects, or a ``pyuv.Stream`` instance. The position in the\n        sequence determines the file descriptor in the client. The first\n        position corresponds to FD 3, the second to 4, etc. This places these\n        file descriptors directly after the stdio handles.\n\n        The *cwd* argument specifies the directory to change to before\n        executing the child. If not provided, the current directory is used.\n\n        The *env* argument specifies the environment to use when executing the\n        child. If provided, it must be a dictionary. By default, the current\n        environment is used.\n\n        The *flags* argument can be used to specify optional libuv\n        ``uv_process_flags``. The only relevant flags are\n        ``pyuv.UV_PROCESS_DETACHED`` and ``pyuv.UV_PROCESS_WINDOWS_HIDE``. Both\n        are Windows specific and are silently ignored on Unix.\n        \"\"\"\n        if self._process:\n            raise RuntimeError('child process already spawned')\n        self._child_exited.clear()\n        self._closed.clear()\n        self._exit_status = None\n        self._term_signal = None\n        hub = get_hub()\n        if isinstance(args, str):\n            args = [args]\n            flags |= pyuv.UV_PROCESS_WINDOWS_VERBATIM_ARGUMENTS\n        else:\n            args = list(args)\n        if shell:\n            if hasattr(os, 'fork'):\n                # Unix\n                if executable is None:\n                    executable = '/bin/sh'\n                args = [executable, '-c'] + args\n            else:\n                # Windows\n                if executable is None:\n                    executable = os.environ.get('COMSPEC', 'cmd.exe')\n                args = [executable, '/c'] + args\n        if executable is None:\n            executable = args[0]\n        kwargs = {}\n        if env is not None:\n            kwargs['env'] = env\n        if cwd is not None:\n            kwargs['cwd'] = cwd\n        kwargs['flags'] = flags\n        handles = self._get_child_handles(hub.loop, stdin, stdout, stderr, extra_handles)\n        kwargs['stdio'] = handles\n        process = pyuv.Process.spawn(hub.loop, args, executable,\n                                     exit_callback=self._on_child_exit, **kwargs)\n        # Create stdin/stdout/stderr transports/protocols.\n        if handles[0].stream:\n            self._stdin = self._connect_child_handle(handles[0])\n        if handles[1].stream:\n            self._stdout = self._connect_child_handle(handles[1])\n        if handles[2].stream:\n            self._stderr = self._connect_child_handle(handles[2])\n        self._process = process"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self._process is None:\n            return\n        waitfor = []\n        if not self._process.closed:\n            self._process.close(self._on_close_complete)\n            waitfor.append(self._closed)\n        # For each of stdin/stdout/stderr, close the transport. This schedules\n        # an on-close callback that will close the protocol, which we wait for.\n        if self._stdin:\n            self._stdin[1].close()\n            waitfor.append(self._stdin[1]._closed)\n        if self._stdout:\n            self._stdout[1].close()\n            waitfor.append(self._stdout[1]._closed)\n        if self._stderr:\n            self._stderr[1].close()\n            waitfor.append(self._stderr[1]._closed)\n        futures.wait(waitfor)\n        self._process = None\n        self._stdin = self._stdout = self._stderr = None", "response": "Close the process and frees its associated resources."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_signal(self, signum):\n        if self._process is None:\n            raise RuntimeError('no child process')\n        self._process.kill(signum)", "response": "Send the signal signum to the child process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef terminate(self):\n        try:\n            self.send_signal(signal.SIGTERM)\n        except pyuv.error.ProcessError as e:\n            if e.args[0] != pyuv.errno.UV_ESRCH:\n                raise", "response": "Terminate the child process."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits for the child process to exit.", "response": "def wait(self, timeout=-1):\n        \"\"\"Wait for the child to exit.\n\n        Wait for at most *timeout* seconds, or indefinitely if *timeout* is\n        None. Return the value of the :attr:`returncode` attribute.\n        \"\"\"\n        if self._process is None:\n            raise RuntimeError('no child process')\n        if timeout == -1:\n            timeout = self._timeout\n        if not self._child_exited.wait(timeout):\n            raise Timeout('timeout waiting for child to exit')\n        return self.returncode"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncommunicate with the child and return its output.", "response": "def communicate(self, input=None, timeout=-1):\n        \"\"\"Communicate with the child and return its output.\n\n        If *input* is provided, it is sent to the client. Concurrent with\n        sending the input, the child's standard output and standard error are\n        read, until the child exits.\n\n        The return value is a tuple ``(stdout_data, stderr_data)`` containing\n        the data read from standard output and standard error.\n        \"\"\"\n        if self._process is None:\n            raise RuntimeError('no child process')\n        if timeout == -1:\n            timeout = self._timeout\n        output = [[], []]\n        def writer(stream, data):\n            offset = 0\n            while offset < len(data):\n                buf = data[offset:offset+4096]\n                stream.write(buf)\n                offset += len(buf)\n            stream.close()\n        def reader(stream, data):\n            while True:\n                if self._encoding:\n                    buf = stream.read(4096)\n                else:\n                    buf = stream.read1()\n                if not buf:\n                    break\n                data.append(buf)\n        if self.stdin:\n            fibers.spawn(writer, self.stdin, input or b'')\n        if self.stdout:\n            fibers.spawn(reader, self.stdout, output[0])\n        if self.stderr:\n            fibers.spawn(reader, self.stderr, output[1])\n        self.wait(timeout)\n        empty = '' if self._encoding else b''\n        stdout_data = empty.join(output[0])\n        stderr_data = empty.join(output[1])\n        return (stdout_data, stderr_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a requirements. txt file and return as a list.", "response": "def get_requirements():\n    \"\"\"Parse a requirements.txt file and return as a list.\"\"\"\n    with open(os.path.join(topdir, 'requirements.txt')) as fin:\n        lines = fin.readlines()\n    lines = [line.strip() for line in lines]\n    return lines"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, node):\n        if not isinstance(node, Node):\n            raise TypeError('expecting Node instance')\n        if node._list is None:\n            return\n        if node._list is not self:\n            raise RuntimeError('node is not contained in list')\n        if node._next is None:\n            self._last = node._prev  # last node\n        else:\n            node._next._prev = node._prev\n        if node._prev is None:\n            self._first = node._next  # first node\n        else:\n            node._prev._next = node._next\n        node._list = node._prev = node._next = None\n        self._size -= 1", "response": "Removes a node from the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninsert a new node into the list.", "response": "def insert(self, node, before=None):\n        \"\"\"Insert a new node in the list.\n\n        If *before* is specified, the new node is inserted before this node.\n        Otherwise, the node is inserted at the end of the list.\n        \"\"\"\n        node._list = self\n        if self._first is None:\n            self._first = self._last = node  # first node in list\n            self._size += 1\n            return node\n        if before is None:\n            self._last._next = node  # insert as last node\n            node._prev = self._last\n            self._last = node\n        else:\n            node._next = before\n            node._prev = before._prev\n            if node._prev:\n                node._prev._next = node\n            else:\n                self._first = node  # inserting as first node\n            node._next._prev = node\n        self._size += 1\n        return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves all nodes from the list.", "response": "def clear(self):\n        \"\"\"Remove all nodes from the list.\"\"\"\n        node = self._first\n        while node is not None:\n            next_node = node._next\n            node._list = node._prev = node._next = None\n            node = next_node\n        self._size = 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a code map for all of the 256 - color variants.", "response": "def _build_codes() -> Dict[str, Dict[str, str]]:\n    \"\"\" Build code map, encapsulated to reduce module-level globals. \"\"\"\n    built = {\n        'fore': {},\n        'back': {},\n        'style': {},\n    }  # type: Dict[str, Dict[str, str]]\n\n    # Set codes for forecolors (30-37) and backcolors (40-47)\n    # Names are given to some of the 256-color variants as 'light' colors.\n    for name, number in _namemap:\n        # Not using format_* functions here, no validation needed.\n        built['fore'][name] = codeformat(30 + number)\n        built['back'][name] = codeformat(40 + number)\n        litename = 'light{}'.format(name)  # type: str\n        built['fore'][litename] = codeformat(90 + number)\n        built['back'][litename] = codeformat(100 + number)\n\n    # Set reset codes for fore/back.\n    built['fore']['reset'] = codeformat(39)\n    built['back']['reset'] = codeformat(49)\n\n    # Set style codes.\n    for code, names in _stylemap:\n        for alias in names:\n            built['style'][alias] = codeformat(code)\n\n    # Extended (256 color codes)\n    for i in range(256):\n        built['fore'][str(i)] = extforeformat(i)\n        built['back'][str(i)] = extbackformat(i)\n\n    return built"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a reverse escape - code to name map based on an existing name to escape - code map.", "response": "def _build_codes_reverse(\n        codes: Dict[str, Dict[str, str]]) -> Dict[str, Dict[str, str]]:\n    \"\"\" Build a reverse escape-code to name map, based on an existing\n        name to escape-code map.\n    \"\"\"\n    built = {}  # type: Dict[str, Dict[str, str]]\n    for codetype, codemap in codes.items():\n        for name, escapecode in codemap.items():\n            # Skip shorcut aliases to avoid overwriting long names.\n            if len(name) < 2:\n                continue\n            if built.get(codetype, None) is None:\n                built[codetype] = {}\n            built[codetype][escapecode] = name\n    return built"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auto_disable(\n        enabled: Optional[bool] = True,\n        fds: Optional[Sequence[IO]] = (sys.stdout, sys.stderr)) -> None:\n    \"\"\" Automatically decide whether to disable color codes if stdout or\n        stderr are not ttys.\n\n        Arguments:\n            enabled  : Whether to automatically disable color codes.\n                       When set to True, the fds will be checked for ttys.\n                       When set to False, enable() is called.\n            fds      : Open file descriptors to check for ttys.\n                       If any non-ttys are found, colors will be disabled.\n                       Objects must have a isatty() method.\n    \"\"\"\n    if enabled:\n        if not all(getattr(f, 'isatty', lambda: False)() for f in fds):\n            disable()\n    else:\n        enable()", "response": "Automatically disables color codes if stdout or stderr are not ttys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _format_code(\n        number: FormatArg,\n        backcolor: Optional[bool] = False,\n        light: Optional[bool] = False,\n        extended: Optional[bool] = False) -> str:\n    \"\"\" Return an escape code for a fore/back color, by number.\n        This is a convenience method for handling the different code types\n        all in one shot.\n        It also handles some validation.\n        format_fore/format_back wrap this function to reduce code duplication.\n\n        Arguments:\n            number    : Integer or RGB tuple to format into an escape code.\n            backcolor : Whether this is for a back color, otherwise it's fore.\n            light     : Whether this should be a 'light' color.\n            extended  : Whether this should be an extended (256) color.\n\n        If `light` and `extended` are both given, only `light` is used.\n    \"\"\"\n    if backcolor:\n        codetype = 'back'\n        # A dict of codeformat funcs. These funcs return an escape code str.\n        formatters = {\n            'code': lambda n: codeformat(40 + n),\n            'lightcode': lambda n: codeformat(100 + n),\n            'ext': lambda n: extbackformat(n),\n            'rgb': lambda r, g, b: rgbbackformat(r, g, b),\n        }  # type: Dict[str, Callable[..., str]]\n    else:\n        codetype = 'fore'\n        formatters = {\n            'code': lambda n: codeformat(30 + n),\n            'lightcode': lambda n: codeformat(90 + n),\n            'ext': lambda n: extforeformat(n),\n            'rgb': lambda r, g, b: rgbforeformat(r, g, b),\n        }\n    try:\n        r, g, b = (int(x) for x in number)  # type: ignore\n    except (TypeError, ValueError):\n        # Not an rgb code.\n        # This variable, and it's cast is only to satisfy the type checks.\n        try:\n            n = int(cast(int, number))\n        except ValueError:\n            # Not an rgb code, or a valid code number.\n            raise InvalidColr(\n                number,\n                'Expecting RGB or 0-255 for {} code.'.format(codetype)\n            )\n        if light:\n            if not in_range(n, 0, 9):\n                raise InvalidColr(\n                    n,\n                    'Expecting 0-9 for light {} code.'.format(codetype)\n                )\n            return formatters['lightcode'](n)\n        elif extended:\n            if not in_range(n, 0, 255):\n                raise InvalidColr(\n                    n,\n                    'Expecting 0-255 for ext. {} code.'.format(codetype)\n                )\n            return formatters['ext'](n)\n\n        if not in_range(n, 0, 9):\n            raise InvalidColr(\n                n,\n                'Expecting 0-9 for {} code.'.format(codetype)\n            )\n        return formatters['code'](n)\n\n    # Rgb code.\n    try:\n        if not all(in_range(x, 0, 255) for x in (r, g, b)):\n            raise InvalidColr(\n                (r, g, b),\n                'RGB value for {} not in range 0-255.'.format(codetype)\n            )\n    except TypeError:\n        # Was probably a 3-char string. Not an rgb code though.\n        raise InvalidColr(\n            (r, g, b),\n            'RGB value for {} contains invalid number.'.format(codetype)\n        )\n    return formatters['rgb'](r, g, b)", "response": "Return an escape code for a specific color by number."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef format_back(\n        number: FormatArg,\n        light: Optional[bool] = False,\n        extended: Optional[bool] = False) -> str:\n    \"\"\" Return an escape code for a back color, by number.\n        This is a convenience method for handling the different code types\n        all in one shot.\n        It also handles some validation.\n    \"\"\"\n    return _format_code(\n        number,\n        backcolor=True,\n        light=light,\n        extended=extended\n    )", "response": "Return an escape code for a back color by number."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an escape code for a fore color by number.", "response": "def format_fore(\n        number: FormatArg,\n        light: Optional[bool] = False,\n        extended: Optional[bool] = False) -> str:\n    \"\"\" Return an escape code for a fore color, by number.\n        This is a convenience method for handling the different code types\n        all in one shot.\n        It also handles some validation.\n    \"\"\"\n    return _format_code(\n        number,\n        backcolor=False,\n        light=light,\n        extended=extended\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an escape code for a style by number.", "response": "def format_style(number: int) -> str:\n    \"\"\" Return an escape code for a style, by number.\n        This handles invalid style numbers.\n    \"\"\"\n    if str(number) not in _stylenums:\n        raise InvalidStyle(number)\n    return codeformat(number)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving a tuple of all known color names basic and known names.", "response": "def get_all_names() -> Tuple[str]:\n    \"\"\" Retrieve a tuple of all known color names, basic and 'known names'.\n    \"\"\"\n    names = list(basic_names)\n    names.extend(name_data)\n    return tuple(sorted(set(names)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the code number from an escape code string.", "response": "def get_code_num(s: str) -> Optional[int]:\n    \"\"\" Get code number from an escape code.\n        Raises InvalidEscapeCode if an invalid number is found.\n    \"\"\"\n    if ';' in s:\n        # Extended fore/back codes.\n        numberstr = s.rpartition(';')[-1][:-1]\n    else:\n        # Fore, back, style, codes.\n        numberstr = s.rpartition('[')[-1][:-1]\n\n    num = try_parse_int(\n        numberstr,\n        default=None,\n        minimum=0,\n        maximum=255\n    )\n    if num is None:\n        raise InvalidEscapeCode(numberstr)\n    return num"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget rgb code numbers from an RGB escape code.", "response": "def get_code_num_rgb(s: str) -> Optional[Tuple[int, int, int]]:\n    \"\"\" Get rgb code numbers from an RGB escape code.\n        Raises InvalidRgbEscapeCode if an invalid number is found.\n    \"\"\"\n    parts = s.split(';')\n    if len(parts) != 5:\n        raise InvalidRgbEscapeCode(s, reason='Count is off.')\n    rgbparts = parts[-3:]\n    if not rgbparts[2].endswith('m'):\n        raise InvalidRgbEscapeCode(s, reason='Missing \\'m\\' on the end.')\n\n    rgbparts[2] = rgbparts[2].rstrip('m')\n    try:\n        r, g, b = [int(x) for x in rgbparts]\n    except ValueError as ex:\n        raise InvalidRgbEscapeCode(s) from ex\n\n    if not all(in_range(x, 0, 255) for x in (r, g, b)):\n        raise InvalidRgbEscapeCode(s, reason='Not in range 0-255.')\n    return r, g, b"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_known_codes(\n        s: Union[str, 'Colr'],\n        unique: Optional[bool] = True,\n        rgb_mode: Optional[bool] = False):\n    \"\"\" Get all known escape codes from a string, and yield the explanations.\n    \"\"\"\n\n    isdisabled = disabled()\n    orderedcodes = tuple((c, get_known_name(c)) for c in get_codes(s))\n    codesdone = set()  # type: Set[str]\n\n    for code, codeinfo in orderedcodes:\n        # Do the codes in order, but don't do the same code twice.\n\n        if unique:\n            if code in codesdone:\n                continue\n            codesdone.add(code)\n\n        if codeinfo is None:\n            continue\n        codetype, name = codeinfo\n\n        typedesc = '{:>13}: {!r:<23}'.format(codetype.title(), code)\n        if codetype.startswith(('extended', 'rgb')):\n            if isdisabled:\n                codedesc = str(ColorCode(name, rgb_mode=rgb_mode))\n            else:\n                codedesc = ColorCode(name, rgb_mode=rgb_mode).example()\n        else:\n            codedesc = ''.join((\n                code,\n                str(name).lstrip('(').rstrip(')'),\n                codes['style']['reset_all']\n            ))\n\n        yield ' '.join((\n            typedesc,\n            codedesc\n        ))", "response": "Get all known escape codes from a string and yield the explanations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_known_name(s: str) -> Optional[Tuple[str, ColorArg]]:\n\n    if not s.endswith('m'):\n        # All codes end with 'm', so...\n        return None\n    if s.startswith('\\033[38;5;'):\n        # Extended fore.\n        name = codes_reverse['fore'].get(s, None)\n        if name is None:\n            num = get_code_num(s)\n            return ('extended fore', num)\n        else:\n            return ('extended fore', name)\n    elif s.startswith('\\033[48;5;'):\n        # Extended back.\n        name = codes_reverse['back'].get(s, None)\n        if name is None:\n            num = get_code_num(s)\n            return ('extended back', num)\n        else:\n            return ('extended back', name)\n    elif s.startswith('\\033[38;2'):\n        # RGB fore.\n        vals = get_code_num_rgb(s)\n        if vals is not None:\n            return ('rgb fore', vals)\n    elif s.startswith('\\033[48;2'):\n        # RGB back.\n        vals = get_code_num_rgb(s)\n        if vals is not None:\n            return ('rgb back', vals)\n    elif s.startswith('\\033['):\n        # Fore, back, style.\n        number = get_code_num(s)\n        # Get code type based on number.\n        if (number <= 7) or (number == 22):\n            codetype = 'style'\n        elif (((number >= 30) and (number < 40)) or\n                ((number >= 90) and (number < 100))):\n            codetype = 'fore'\n        elif (((number >= 40) and (number < 50)) or\n                ((number >= 100) and (number < 110))):\n            codetype = 'back'\n        else:\n            raise InvalidEscapeCode(\n                number,\n                'Expecting 0-7, 22, 30-39, or 40-49 for escape code',\n            )\n\n        name = codes_reverse[codetype].get(s, None)\n        if name is not None:\n            return (codetype, name)\n\n    # Not a known escape code.\n    return None", "response": "Reverse translate a terminal code to a known color name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef in_range(x: int, minimum: int, maximum: int) -> bool:\n    return (x >= minimum and x <= maximum)", "response": "Return True if x is in the given range."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_colr_arg(\n        s: str,\n        default: Optional[Any] = None,\n        rgb_mode: Optional[bool] = False) -> ColorArg:\n    \"\"\" Parse a user argument into a usable fore/back color value for Colr.\n        If a falsey value is passed, default is returned.\n        Raises InvalidColr if the argument is unusable.\n        Returns: A usable value for Colr(fore/back).\n\n        This validates basic/extended color names.\n        This validates the range for basic/extended values (0-255).\n        This validates the length/range for rgb values (0-255, 0-255, 0-255).\n\n        Arguments:\n            s  : User's color value argument.\n                 Example: \"1\", \"255\", \"black\", \"25,25,25\"\n    \"\"\"\n    if not s:\n        return default\n\n    val = s.strip().lower()\n    try:\n        # Try as int.\n        intval = int(val)\n    except ValueError:\n        # Try as rgb.\n        try:\n            r, g, b = (int(x.strip()) for x in val.split(','))\n        except ValueError:\n            if ',' in val:\n                # User tried rgb value and failed.\n                raise InvalidColr(val)\n\n            # Try as name (fore/back have the same names)\n            code = codes['fore'].get(val, None)\n            if code:\n                # Valid basic code from fore, bask, or style.\n                return val\n\n            # Not a basic code, try known names.\n            named_data = name_data.get(val, None)\n            if named_data is not None:\n                # A known named color.\n                return val\n\n            # Not a basic/extended/known name, try as hex.\n            try:\n                if rgb_mode:\n                    return hex2rgb(val, allow_short=True)\n                return hex2termhex(val, allow_short=True)\n            except ValueError:\n                raise InvalidColr(val)\n        else:\n            # Got rgb. Do some validation.\n            if not all((in_range(x, 0, 255) for x in (r, g, b))):\n                raise InvalidColr(val)\n            # Valid rgb.\n            return r, g, b\n    else:\n        # Int value.\n        if not in_range(intval, 0, 255):\n            # May have been a hex value confused as an int.\n            if len(val) in (3, 6):\n                try:\n                    if rgb_mode:\n                        return hex2rgb(val, allow_short=True)\n                    return hex2termhex(val, allow_short=True)\n                except ValueError:\n                    raise InvalidColr(val)\n            raise InvalidColr(intval)\n        # Valid int value.\n        return intval", "response": "Parses a user argument into a usable color value for the given color."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying parsing a string into an integer.", "response": "def try_parse_int(\n        s: str,\n        default: Optional[Any] = None,\n        minimum: Optional[int] = None,\n        maximum: Optional[int] = None) -> Optional[Any]:\n    \"\"\" Try parsing a string into an integer.\n        On failure, return `default`.\n        If the number is less then `minimum` or greater than `maximum`,\n        return `default`.\n        Returns an integer on success.\n    \"\"\"\n    try:\n        n = int(s)\n    except ValueError:\n        return default\n    if (minimum is not None) and (n < minimum):\n        return default\n    elif (maximum is not None) and (n > maximum):\n        return default\n    return n"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _attr_to_method(self, attr):\n\n        if attr in codes['fore']:\n            # Fore method\n            return partial(self.chained, fore=attr)\n        elif attr in codes['style']:\n            # Style method\n            return partial(self.chained, style=attr)\n        elif attr.startswith('bg'):\n            # Back method\n            name = attr[2:].lstrip('_')\n            if name in codes['back']:\n                return partial(self.chained, back=name)\n        elif attr.startswith(('b256_', 'b_')):\n            # Back 256 method\n            # Remove the b256_ portion.\n            name = attr.partition('_')[2]\n            return self._ext_attr_to_partial(name, 'back')\n        elif attr.startswith(('f256_', 'f_')):\n            # Fore 256 method\n            name = attr.partition('_')[2]\n            return self._ext_attr_to_partial(name, 'fore')\n\n        return None", "response": "Return the correct color function by method name."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls the Colr class method on an object.", "response": "def _call_dunder_colr(cls, obj):\n        \"\"\" Call __colr__ on an object, after some checks.\n            If color is disabled, the object itself is returned.\n            If __colr__ doesn't return a Colr instance, TypeError is raised.\n            On success, a Colr instance is returned from obj.__colr__().\n        \"\"\"\n        if _disabled:\n            # No colorization when disabled. Just use str.\n            return obj\n        clr = obj.__colr__()\n        if not isinstance(clr, cls):\n            # __colr__ should always return a Colr.\n            # Future development may assume a Colr was returned.\n            raise TypeError(\n                ' '.join((\n                    '__colr__ methods should return a {} instance.',\n                    'Got: {}',\n                )).format(\n                    cls.__name__,\n                    type(clr).__name__,\n                )\n            )\n        return clr"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a string like 2223 or aliceblue into a partial for self. chained.", "response": "def _ext_attr_to_partial(self, name, kwarg_key):\n        \"\"\" Convert a string like '233' or 'aliceblue' into partial for\n            self.chained.\n        \"\"\"\n        try:\n            intval = int(name)\n        except ValueError:\n            # Try as an extended name_data name.\n            info = name_data.get(name, None)\n            if info is None:\n                # Not an int value or name_data name.\n                return None\n            kws = {kwarg_key: info['code']}\n            return partial(self.chained, **kws)\n        # Integer str passed, use the int value.\n        kws = {kwarg_key: intval}\n        return partial(self.chained, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _gradient_black_line(\n            self, text, start, step=1,\n            fore=None, back=None, style=None, reverse=False, rgb_mode=False):\n        \"\"\" Yield colorized characters,\n            within the 24-length black gradient.\n        \"\"\"\n        if start < 232:\n            start = 232\n        elif start > 255:\n            start = 255\n        if reverse:\n            codes = list(range(start, 231, -1))\n        else:\n            codes = list(range(start, 256))\n        return ''.join((\n            self._iter_text_wave(\n                text,\n                codes,\n                step=step,\n                fore=fore,\n                back=back,\n                style=style,\n                rgb_mode=rgb_mode\n            )\n        ))", "response": "Yields black lines within the 24 - length black gradient."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields colorized characters within the 24 - length black gradient.", "response": "def _gradient_black_lines(\n            self, text, start, step=1,\n            fore=None, back=None, style=None, reverse=False,\n            movefactor=2, rgb_mode=False):\n        \"\"\" Yield colorized characters,\n            within the 24-length black gradient,\n            treating each line separately.\n        \"\"\"\n        if not movefactor:\n            def factor(i):\n                return start\n        else:\n            # Increase the start for each line.\n            def factor(i):\n                return start + (i * movefactor)\n        return '\\n'.join((\n            self._gradient_black_line(\n                line,\n                start=factor(i),\n                step=step,\n                fore=fore,\n                back=back,\n                style=style,\n                reverse=reverse,\n                rgb_mode=rgb_mode,\n            )\n            for i, line in enumerate(text.splitlines())\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gradient_rgb_line(\n            self, text, start, stop, step=1,\n            fore=None, back=None, style=None):\n        \"\"\" Yield colorized characters, morphing from one rgb value to\n            another.\n        \"\"\"\n        return self._gradient_rgb_line_from_morph(\n            text,\n            list(self._morph_rgb(start, stop, step=step)),\n            fore=fore,\n            back=back,\n            style=style\n        )", "response": "Yield colorized characters from one rgb value to\n            another."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gradient_rgb_line_from_morph(\n            self, text, morphlist, fore=None, back=None, style=None):\n        \"\"\" Yield colorized characters, morphing from one rgb value to\n            another.\n        \"\"\"\n        try:\n            listlen = len(morphlist)\n        except TypeError:\n            morphlist = list(morphlist)\n            listlen = len(morphlist)\n        neededsteps = listlen // len(text)\n        iterstep = 1\n        if neededsteps > iterstep:\n            # Skip some members of morphlist, to be sure to reach the end.\n            iterstep = neededsteps\n        usevals = morphlist\n        if iterstep > 1:\n            # Rebuild the morphlist, skipping some.\n            usevals = [usevals[i] for i in range(0, listlen, iterstep)]\n        return ''.join((\n            self._iter_text_wave(\n                text,\n                usevals,\n                fore=fore,\n                back=back,\n                style=style,\n                rgb_mode=False,\n            )\n        ))", "response": "Yield colorized characters from one rgb value to\n            another."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _gradient_rgb_lines(\n            self, text, start, stop, step=1,\n            fore=None, back=None, style=None, movefactor=None):\n        \"\"\" Yield colorized characters, morphing from one rgb value to\n            another. This treats each line separately.\n        \"\"\"\n        morphlist = list(self._morph_rgb(start, stop, step=step))\n        if movefactor:\n            # Moving means we need the morph to wrap around.\n            morphlist.extend(self._morph_rgb(stop, start, step=step))\n            if movefactor < 0:\n                # Increase the start for each line.\n                def move():\n                    popped = []\n                    for _ in range(abs(movefactor)):\n                        try:\n                            popped.append(morphlist.pop(0))\n                        except IndexError:\n                            pass\n                    morphlist.extend(popped)\n                    return morphlist\n            else:\n                # Decrease start for each line.\n                def move():\n                    for _ in range(movefactor):\n                        try:\n                            val = morphlist.pop(-1)\n                        except IndexError:\n                            pass\n                        else:\n                            morphlist.insert(0, val)\n                    return morphlist\n\n        return '\\n'.join((\n            self._gradient_rgb_line_from_morph(\n                line,\n                move() if movefactor else morphlist,\n                fore=fore,\n                back=back,\n                style=style,\n            )\n            for i, line in enumerate(text.splitlines())\n        ))", "response": "Yield colorized characters from one rgb value to\n            another."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding colorized characters from text using a wave of numbers.", "response": "def _iter_text_wave(\n            self, text, numbers, step=1,\n            fore=None, back=None, style=None, rgb_mode=False):\n        \"\"\" Yield colorized characters from `text`, using a wave of `numbers`.\n            Arguments:\n                text      : String to be colorized.\n                numbers   : A list/tuple of numbers (256 colors).\n                step      : Number of characters to colorize per color.\n                fore      : Fore color to use (name or number).\n                            (Back will be gradient)\n                back      : Background color to use (name or number).\n                            (Fore will be gradient)\n                style     : Style name to use.\n                rgb_mode  : Use number for rgb value.\n                            This should never be used when the numbers\n                            are rgb values themselves.\n        \"\"\"\n        if fore and back:\n            raise ValueError('Both fore and back colors cannot be specified.')\n\n        pos = 0\n        end = len(text)\n        numbergen = self._iter_wave(numbers)\n\n        def make_color(n):\n            try:\n                r, g, b = n\n            except TypeError:\n                if rgb_mode:\n                    return n, n, n\n                return n\n            return r, g, b\n\n        for value in numbergen:\n            lastchar = pos + step\n            yield self.color(\n                text[pos:lastchar],\n                fore=make_color(value) if fore is None else fore,\n                back=make_color(value) if fore is not None else back,\n                style=style\n            )\n            if lastchar >= end:\n                numbergen.send(True)\n            pos = lastchar"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _iter_wave(iterable, count=0):\n        up = True\n        pos = 0\n        i = 0\n        try:\n            end = len(iterable)\n        except TypeError:\n            iterable = list(iterable)\n            end = len(iterable)\n\n        # Stop on count, or run forever.\n        while (i < count) if count > 0 else True:\n            try:\n                stop = yield iterable[pos]\n                # End of generator (user sent the stop signal)\n                if stop:\n                    break\n            except IndexError:\n                # End of iterable, when len(iterable) is < count.\n                up = False\n\n            # Change directions if needed, otherwise increment/decrement.\n            if up:\n                pos += 1\n                if pos == end:\n                    up = False\n                    pos = end - 2\n            else:\n                pos -= 1\n                if pos < 0:\n                    up = True\n                    pos = 1\n            i += 1", "response": "A generator that yields the given iterable of A B C and D."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _morph_rgb(self, rgb1, rgb2, step=1):\n        pos1, pos2 = list(rgb1), list(rgb2)\n        indexes = [i for i, _ in enumerate(pos1)]\n\n        def step_value(a, b):\n            \"\"\" Returns the amount to add to `a` to make it closer to `b`,\n                multiplied by `step`.\n            \"\"\"\n            if a < b:\n                return step\n            if a > b:\n                return -step\n            return 0\n\n        steps = [step_value(pos1[x], pos2[x]) for x in indexes]\n        stepcnt = 0\n        while (pos1 != pos2):\n            stepcnt += 1\n            stop = yield tuple(pos1)\n            if stop:\n                break\n            for x in indexes:\n                if pos1[x] != pos2[x]:\n                    pos1[x] += steps[x]\n                    if (steps[x] < 0) and (pos1[x] < pos2[x]):\n                        # Over stepped, negative.\n                        pos1[x] = pos2[x]\n                    if (steps[x] > 0) and (pos1[x] > pos2[x]):\n                        # Over stepped, positive.\n                        pos1[x] = pos2[x]\n        yield tuple(pos1)", "response": "Yields the rgb value from rgb1 to rgb2 in a loop."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _rainbow_hex_chars(self, s, freq=0.1, spread=3.0, offset=0):\n        return (\n            (c, self._rainbow_color(freq, offset + i / spread))\n            for i, c in enumerate(s)\n        )", "response": "Iterate over characters in a string to build data needed for rainbow effect."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a rainbow line from a string.", "response": "def _rainbow_line(\n            self, text, freq=0.1, spread=3.0, offset=0,\n            rgb_mode=False, **colorargs):\n        \"\"\" Create rainbow using the same offset for all text.\n            Arguments:\n                text       : String to colorize.\n                freq       : Frequency/\"tightness\" of colors in the rainbow.\n                             Best results when in the range 0.0-1.0.\n                             Default: 0.1\n                spread     : Spread/width of colors.\n                             Default: 3.0\n                offset     : Offset for start of rainbow.\n                             Default: 0\n                rgb_mode   : If truthy, use RGB escape codes instead of\n                             extended 256 and approximate hex match.\n            Keyword Arguments:\n                colorargs  : Any extra arguments for the color function,\n                             such as fore, back, style.\n                             These need to be treated carefully to not\n                            'overwrite' the rainbow codes.\n        \"\"\"\n        fore = colorargs.get('fore', None)\n        back = colorargs.get('back', None)\n        style = colorargs.get('style', None)\n        if fore:\n            color_args = (lambda value: {\n                'back': value if rgb_mode else hex2term(value),\n                'style': style,\n                'fore': fore\n            })\n        else:\n            color_args = (lambda value: {\n                'fore': value if rgb_mode else hex2term(value),\n                'style': style,\n                'back': back\n            })\n\n        if rgb_mode:\n            method = self._rainbow_rgb_chars\n        else:\n            method = self._rainbow_hex_chars\n        return ''.join(\n            self.color(c, **color_args(hval))\n            for c, hval in method(\n                text,\n                freq=freq,\n                spread=spread,\n                offset=offset)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating rainbow text from text.", "response": "def _rainbow_lines(\n            self, text, freq=0.1, spread=3.0, offset=0, movefactor=0,\n            rgb_mode=False, **colorargs):\n        \"\"\" Create rainbow text, using the same offset for each line.\n            Arguments:\n                text       : String to colorize.\n                freq       : Frequency/\"tightness\" of colors in the rainbow.\n                             Best results when in the range 0.0-1.0.\n                             Default: 0.1\n                spread     : Spread/width of colors.\n                             Default: 3.0\n                offset     : Offset for start of rainbow.\n                             Default: 0\n                movefactor : Factor for offset increase on each new line.\n                             Default: 0\n                rgb_mode   : If truthy, use RGB escape codes instead of\n                             extended 256 and approximate hex match.\n\n            Keyword Arguments:\n                fore, back, style  : Other args for the color() function.\n        \"\"\"\n        if not movefactor:\n            def factor(i):\n                return offset\n        else:\n            # Increase the offset for each line.\n            def factor(i):\n                return offset + (i * movefactor)\n        return '\\n'.join(\n            self._rainbow_line(\n                line,\n                freq=freq,\n                spread=spread,\n                offset=factor(i),\n                rgb_mode=rgb_mode,\n                **colorargs)\n            for i, line in enumerate(text.splitlines()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _rainbow_rgb(self, freq, i):\n        # Borrowed from lolcat, translated from ruby.\n        red = math.sin(freq * i + 0) * 127 + 128\n        green = math.sin(freq * i + 2 * math.pi / 3) * 127 + 128\n        blue = math.sin(freq * i + 4 * math.pi / 3) * 127 + 128\n        return int(red), int(green), int(blue)", "response": "Calculate a single rgb value for a piece of a rainbow."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _rainbow_rgb_chars(self, s, freq=0.1, spread=3.0, offset=0):\n        return (\n            (c, self._rainbow_rgb(freq, offset + i / spread))\n            for i, c in enumerate(s)\n        )", "response": "Iterate over characters in a string to build data needed for rainbow effect."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef b_rgb(self, r, g, b, text=None, fore=None, style=None):\n        return self.chained(text=text, fore=fore, back=(r, g, b), style=style)", "response": "A chained method that sets the back color to an RGB value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by the various 'color' methods to colorize a single string. The RESET_ALL code is appended to the string unless text is empty. Raises ValueError on invalid color names. Arguments: text : String to colorize, or None for BG/Style change. fore : Name of fore color to use. back : Name of back color to use. style : Name of style to use.", "response": "def chained(self, text=None, fore=None, back=None, style=None):\n        \"\"\" Called by the various 'color' methods to colorize a single string.\n            The RESET_ALL code is appended to the string unless text is empty.\n            Raises ValueError on invalid color names.\n\n            Arguments:\n                text  : String to colorize, or None for  BG/Style change.\n                fore  : Name of fore color to use.\n                back  : Name of back color to use.\n                style : Name of style to use.\n        \"\"\"\n        self.data = ''.join((\n            self.data,\n            self.color(text=text, fore=fore, back=back, style=style),\n        ))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef color_code(self, fore=None, back=None, style=None):\n        # Map from style type to raw code formatter function.\n        colorcodes = []\n        resetcodes = []\n        userstyles = {'style': style, 'back': back, 'fore': fore}\n        for stype in userstyles:\n            stylearg = userstyles.get(stype, None)\n            if not stylearg:\n                # No value for this style name, don't use it.\n                continue\n            # Get escape code for this style.\n            code = self.get_escape_code(stype, stylearg)\n            stylename = str(stylearg).lower()\n            if (stype == 'style') and (stylename in ('0', )):\n                resetcodes.append(code)\n            elif stylename.startswith('reset'):\n                resetcodes.append(code)\n            else:\n                colorcodes.append(code)\n        # Reset codes come first, to not override colors.\n        return ''.join((''.join(resetcodes), ''.join(colorcodes)))", "response": "Return the color codes for this style and colors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(self, *args, **kwargs):\n        return self.__class__(self.data.format(*args, **kwargs))", "response": "Like str. format except it returns a Colr"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts user arg to escape code.", "response": "def get_escape_code(self, codetype, value):\n        \"\"\" Convert user arg to escape code. \"\"\"\n        valuefmt = str(value).lower()\n        code = codes[codetype].get(valuefmt, None)\n        if code:\n            # Basic code from fore, back, or style.\n            return code\n\n        named_funcs = {\n            'fore': format_fore,\n            'back': format_back,\n            'style': format_style,\n        }\n\n        # Not a basic code, try known names.\n        converter = named_funcs.get(codetype, None)\n        if converter is None:\n            raise ValueError(\n                'Invalid code type. Expecting {}, got: {!r}'.format(\n                    ', '.join(named_funcs),\n                    codetype\n                )\n            )\n        # Try as hex.\n        with suppress(ValueError):\n            value = int(hex2term(value, allow_short=True))\n            return converter(value, extended=True)\n\n        named_data = name_data.get(valuefmt, None)\n        if named_data is not None:\n            # A known named color.\n            try:\n                return converter(named_data['code'], extended=True)\n            except TypeError:\n                # Passing a known name as a style?\n                if codetype == 'style':\n                    raise InvalidStyle(value)\n                raise\n        # Not a known color name/value, try rgb.\n        try:\n            r, g, b = (int(x) for x in value)\n            # This does not mean we have a 3 int tuple. It could '111'.\n            # The converter should catch it though.\n        except (TypeError, ValueError):\n            # Not an rgb value.\n            if codetype == 'style':\n                raise InvalidStyle(value)\n        try:\n            escapecode = converter(value)\n        except ValueError as ex:\n            raise InvalidColr(value) from ex\n        return escapecode"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gradient(\n            self, text=None, name=None, fore=None, back=None, style=None,\n            freq=0.1, spread=None, linemode=True,\n            movefactor=2, rgb_mode=False):\n        \"\"\" Return a gradient by color name. Uses rainbow() underneath to\n            build the gradients, starting at a known offset.\n            Arguments:\n                text       : Text to make gradient (self.data when not given).\n                             The gradient text is joined to self.data when\n                             this is used.\n                name       : Color name for the gradient (same as fore names).\n                             Default: black\n                fore       : Fore color. Back will be gradient when used.\n                             Default: None (fore is gradient)\n                back       : Back color. Fore will be gradient when used.\n                             Default: None (back=reset/normal)\n                style      : Style for the gradient.\n                             Default: None (reset/normal)\n                freq       : Frequency of color change.\n                             Higher means more colors.\n                             Best when in the 0.0-1.0 range.\n                             Default: 0.1\n                spread     : Spread/width of each color (in characters).\n                             Default: 3.0 for colors, 1 for black/white\n                linemode   : Colorize each line in the input.\n                             Default: True\n                movefactor : Factor for offset increase on each line when\n                             using linemode.\n                             Minimum value: 0\n                             Default: 2\n                rgb_mode   : Use true color (rgb) codes.\n        \"\"\"\n        try:\n            # Try explicit offset (passed in with `name`).\n            offset = int(name)\n        except (TypeError, ValueError):\n            name = name.lower().strip() if name else 'black'\n            # Black and white are separate methods.\n            if name == 'black':\n                return self.gradient_black(\n                    text=text,\n                    fore=fore,\n                    back=back,\n                    style=style,\n                    step=int(spread) if spread else 1,\n                    linemode=linemode,\n                    movefactor=movefactor,\n                    rgb_mode=rgb_mode\n                )\n            elif name == 'white':\n                return self.gradient_black(\n                    text=text,\n                    fore=fore,\n                    back=back,\n                    style=style,\n                    step=int(spread) if spread else 1,\n                    linemode=linemode,\n                    movefactor=movefactor,\n                    reverse=True,\n                    rgb_mode=rgb_mode\n                )\n            try:\n                # Get rainbow offset from known name.\n                offset = self.gradient_names[name]\n            except KeyError:\n                raise ValueError('Unknown gradient name: {}'.format(name))\n\n        return self.rainbow(\n            text=text,\n            fore=fore,\n            back=back,\n            style=style,\n            offset=offset,\n            freq=freq,\n            spread=spread or 3.0,\n            linemode=linemode,\n            movefactor=movefactor,\n            rgb_mode=rgb_mode,\n        )", "response": "Return a gradient of the current color."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new instance of the color black gradient.", "response": "def gradient_black(\n            self, text=None, fore=None, back=None, style=None,\n            start=None, step=1, reverse=False,\n            linemode=True, movefactor=2, rgb_mode=False):\n        \"\"\" Return a black and white gradient.\n            Arguments:\n                text       : String to colorize.\n                             This will always be greater than 0.\n                fore       : Foreground color, background will be gradient.\n                back       : Background color, foreground will be gradient.\n                style      : Name of style to use for the gradient.\n                start      : Starting 256-color number.\n                             The `start` will be adjusted if it is not within\n                             bounds.\n                             This will always be > 15.\n                             This will be adjusted to fit within a 6-length\n                             gradient, or the 24-length black/white gradient.\n                step       : Number of characters to colorize per color.\n                             This allows a \"wider\" gradient.\n                linemode   : Colorize each line in the input.\n                             Default: True\n                movefactor : Factor for offset increase on each line when\n                             using linemode.\n                             Minimum value: 0\n                             Default: 2\n                rgb_mode   : Use true color (rgb) method and codes.\n        \"\"\"\n        gradargs = {\n            'step': step,\n            'fore': fore,\n            'back': back,\n            'style': style,\n            'reverse': reverse,\n            'rgb_mode': rgb_mode,\n        }\n\n        if linemode:\n            gradargs['movefactor'] = 2 if movefactor is None else movefactor\n            method = self._gradient_black_lines\n        else:\n            method = self._gradient_black_line\n\n        if text:\n            return self.__class__(\n                ''.join((\n                    self.data or '',\n                    method(\n                        text,\n                        start or (255 if reverse else 232),\n                        **gradargs)\n                ))\n            )\n\n        # Operating on self.data.\n        return self.__class__(\n            method(\n                self.stripped(),\n                start or (255 if reverse else 232),\n                **gradargs)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a gradient of rgb values.", "response": "def gradient_rgb(\n            self, text=None, fore=None, back=None, style=None,\n            start=None, stop=None, step=1, linemode=True, movefactor=0):\n        \"\"\" Return a black and white gradient.\n            Arguments:\n                text       : String to colorize.\n                fore       : Foreground color, background will be gradient.\n                back       : Background color, foreground will be gradient.\n                style      : Name of style to use for the gradient.\n                start      : Starting rgb value.\n                stop       : Stopping rgb value.\n                step       : Number of characters to colorize per color.\n                             This allows a \"wider\" gradient.\n                             This will always be greater than 0.\n                linemode   : Colorize each line in the input.\n                             Default: True\n                movefactor : Amount to shift gradient for each line when\n                             `linemode` is set.\n\n       \"\"\"\n        gradargs = {\n            'step': step,\n            'fore': fore,\n            'back': back,\n            'style': style,\n        }\n        start = start or (0, 0, 0)\n        stop = stop or (255, 255, 255)\n        if linemode:\n            method = self._gradient_rgb_lines\n            gradargs['movefactor'] = movefactor\n        else:\n            method = self._gradient_rgb_line\n\n        if text:\n            return self.__class__(\n                ''.join((\n                    self.data or '',\n                    method(\n                        text,\n                        start,\n                        stop,\n                        **gradargs\n                    ),\n                ))\n            )\n\n        # Operating on self.data.\n        return self.__class__(\n            method(\n                self.stripped(),\n                start,\n                stop,\n                **gradargs\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hex(self, value, text=None, back=None, style=None, rgb_mode=False):\n        if rgb_mode:\n            try:\n                colrval = hex2rgb(value, allow_short=True)\n            except ValueError:\n                raise InvalidColr(value)\n        else:\n            try:\n                colrval = hex2term(value, allow_short=True)\n            except ValueError:\n                raise InvalidColr(value)\n        return self.chained(text=text, fore=colrval, back=back, style=style)", "response": "A chained method that sets the fore color to an hex value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef join(self, *colrs, **colorkwargs):\n        flat = []\n        for clr in colrs:\n            if isinstance(clr, (list, tuple, GeneratorType)):\n                # Flatten any lists, at least once.\n                flat.extend(str(c) for c in clr)\n            else:\n                flat.append(str(clr))\n\n        if colorkwargs:\n            fore = colorkwargs.get('fore', None)\n            back = colorkwargs.get('back', None)\n            style = colorkwargs.get('style', None)\n            flat = (\n                self.color(s, fore=fore, back=back, style=style)\n                for s in flat\n            )\n        return self.__class__(self.data.join(flat))", "response": "Like str. join except it returns a Colr."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking str. lstrip except it returns the Colr instance.", "response": "def lstrip(self, chars=None):\n        \"\"\" Like str.lstrip, except it returns the Colr instance. \"\"\"\n        return self.__class__(\n            self._str_strip('lstrip', chars),\n            no_closing=chars and (closing_code in chars),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new instance of the object with the rainbow gradient text.", "response": "def rainbow(\n            self, text=None, fore=None, back=None, style=None,\n            freq=0.1, offset=30, spread=3.0,\n            linemode=True, movefactor=2, rgb_mode=False):\n        \"\"\" Make rainbow gradient text.\n            Arguments:\n                text       : Text to make gradient.\n                             Default: self.data\n                fore       : Fore color to use (makes back the rainbow).\n                             Default: None\n                back       : Back color to use (makes fore the rainbow).\n                             Default: None\n                style      : Style for the rainbow.\n                             Default: None\n                freq       : Frequency of color change, a higher value means\n                             more colors.\n                             Best results when in the range 0.0-1.0.\n                             Default: 0.1\n                offset     : Offset for start of rainbow.\n                             Default: 30\n                spread     : Spread/width of each color.\n                             Default: 3.0,\n                linemode   : Colorize each line in the input.\n                             Default: True\n                movefactor : Factor for offset increase on each line when\n                             using linemode.\n                             Minimum value: 0\n                             Default: 2\n                rgb_mode   : Use RGB escape codes instead of extended 256 and\n                             approximate hex matches.\n        \"\"\"\n        if fore and back:\n            raise ValueError('Cannot use both fore and back with rainbow()')\n\n        rainbowargs = {\n            'freq': freq,\n            'spread': spread,\n            'offset': offset,\n            'fore': fore,\n            'back': back,\n            'style': style,\n            'rgb_mode': rgb_mode,\n        }\n        if linemode:\n            rainbowargs['movefactor'] = movefactor\n            method = self._rainbow_lines\n        else:\n            method = self._rainbow_line\n\n        if text:\n            # Prepend existing self.data to the rainbow text.\n            return self.__class__(\n                ''.join((\n                    self.data,\n                    method(text, **rainbowargs)\n                ))\n            )\n\n        # Operate on self.data.\n        return self.__class__(\n            method(self.stripped(), **rainbowargs)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rstrip(self, chars=None):\n        return self.__class__(\n            self._str_strip('rstrip', chars),\n            no_closing=chars and (closing_code in chars),\n        )", "response": "Like str. rstrip except it returns the Colr instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlike str. strip except it returns the Colr instance.", "response": "def strip(self, chars=None):\n        \"\"\" Like str.strip, except it returns the Colr instance. \"\"\"\n        return self.__class__(\n            self._str_strip('strip', chars),\n            no_closing=chars and (closing_code in chars),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nliking str except it returns a colorized Colr instance.", "response": "def as_colr(self, label_args=None, value_args=None):\n        \"\"\" Like __str__, except it returns a colorized Colr instance. \"\"\"\n        label_args = label_args or {'fore': 'red'}\n        value_args = value_args or {'fore': 'blue', 'style': 'bright'}\n        return Colr(self.default_format.format(\n            label=Colr(self.label, **label_args),\n            value=Colr(repr(self.value), **value_args),\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nliking str except it returns a colorized Colr instance.", "response": "def as_colr(\n            self, label_args=None, type_args=None, type_val_args=None,\n            value_args=None):\n        \"\"\" Like __str__, except it returns a colorized Colr instance. \"\"\"\n        label_args = label_args or {'fore': 'red'}\n        type_args = type_args or {'fore': 'yellow'}\n        type_val_args = type_val_args or {'fore': 'grey'}\n        value_args = value_args or {'fore': 'blue', 'style': 'bright'}\n\n        return Colr(self.default_format.format(\n            label=Colr(':\\n    ').join(\n                Colr('Expecting color name/value', **label_args),\n                ',\\n    '.join(\n                    '{lbl:<5} ({val})'.format(\n                        lbl=Colr(l, **type_args),\n                        val=Colr(v, **type_val_args),\n                    )\n                    for l, v in self.accepted_values\n                )\n            ),\n            value=Colr(repr(self.value), **value_args)\n        ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_colr(\n            self, label_args=None, type_args=None, type_val_args=None,\n            value_args=None, spec_args=None):\n        \"\"\" Like __str__, except it returns a colorized Colr instance. \"\"\"\n        label_args = label_args or {'fore': 'red'}\n        type_args = type_args or {'fore': 'yellow'}\n        type_val_args = type_val_args or {'fore': 'grey'}\n        value_args = value_args or {'fore': 'blue', 'style': 'bright'}\n        spec_args = spec_args or {'fore': 'blue'}\n        spec_repr = repr(self.spec)\n        spec_quote = spec_repr[0]\n        val_repr = repr(self.value)\n        val_quote = val_repr[0]\n        return Colr(self.default_format.format(\n            label=Colr(':\\n    ').join(\n                Colr('Bad format spec. color name/value', **label_args),\n                ',\\n    '.join(\n                    '{lbl:<5} ({val})'.format(\n                        lbl=Colr(l, **type_args),\n                        val=Colr(v, **type_val_args),\n                    )\n                    for l, v in self.accepted_values\n                )\n            ),\n            spec=Colr('=').join(\n                Colr(v, **spec_args)\n                for v in spec_repr[1:-1].split('=')\n            ).join((spec_quote, spec_quote)),\n            value=Colr(\n                val_repr[1:-1],\n                **value_args\n            ).join((val_quote, val_quote)),\n        ))", "response": "Returns a string representation of the object as a Colr instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_colr(\n            self, label_args=None, type_args=None, value_args=None):\n        \"\"\" Like __str__, except it returns a colorized Colr instance. \"\"\"\n        label_args = label_args or {'fore': 'red'}\n        type_args = type_args or {'fore': 'yellow'}\n        value_args = value_args or {'fore': 'blue', 'style': 'bright'}\n\n        return Colr(self.default_format.format(\n            label=Colr(':\\n    ').join(\n                Colr('Expecting style value', **label_args),\n                Colr(',\\n    ').join(\n                    Colr(', ').join(\n                        Colr(v, **type_args)\n                        for v in t[1]\n                    )\n                    for t in _stylemap\n                )\n            ),\n            value=Colr(repr(self.value), **value_args)\n        ))", "response": "Like str except it returns a colorized Colr instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nensure a file object is a tty.", "response": "def ensure_tty(file=sys.stdout):\n    \"\"\" Ensure a file object is a tty. It must have an `isatty` method that\n        returns True.\n        TypeError is raised if the method doesn't exist, or returns False.\n    \"\"\"\n    isatty = getattr(file, 'isatty', None)\n    if isatty is None:\n        raise TypeError(\n            'Cannot detect tty, file has no `isatty` method: {}'.format(\n                getattr(file, 'name', type(file).__name__)\n            )\n        )\n    if not isatty():\n        raise TypeError(\n            'This will not work, file object is not a tty: {}'.format(\n                getattr(file, 'name', type(file).__name__)\n            )\n        )\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef erase_display(method=EraseMethod.ALL_MOVE, file=sys.stdout):\n    erase.display(method).write(file=file)", "response": "Erases the screen or part of the screen and then moves the screen to the home position."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef erase_line(method=EraseMethod.ALL, file=sys.stdout):\n    erase.line(method).write(file=file)", "response": "Erase a line or part of a language file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef move_back(columns=1, file=sys.stdout):\n    move.back(columns).write(file=file)", "response": "Moves the cursor back by the specified number of columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef move_column(column=1, file=sys.stdout):\n    move.column(column).write(file=file)", "response": "Move the cursor to the specified column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmoving the cursor down a number of lines without changing columns.", "response": "def move_down(lines=1, file=sys.stdout):\n    \"\"\" Move the cursor down a number of lines.\n\n        Esc[<lines>B:\n        Moves the cursor down by the specified number of lines without\n        changing columns. If the cursor is already on the bottom line,\n        ANSI.SYS ignores this sequence.\n    \"\"\"\n    move.down(lines).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_forward(columns=1, file=sys.stdout):\n    move.forward(columns).write(file=file)", "response": "Moves the cursor forward by columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove the cursor to the next line in the sequence.", "response": "def move_next(lines=1, file=sys.stdout):\n    \"\"\" Move the cursor to the beginning of the line, a number of lines down.\n        Default: 1\n\n        Esc[<lines>E\n    \"\"\"\n    move.next(lines).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmoving the cursor to a new position.", "response": "def move_pos(line=1, column=1, file=sys.stdout):\n    \"\"\" Move the cursor to a new position. Values are 1-based, and default\n        to 1.\n\n        Esc[<line>;<column>H\n        or\n        Esc[<line>;<column>f\n    \"\"\"\n    move.pos(line=line, col=column).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmove the cursor to the beginning of the line.", "response": "def move_prev(lines=1, file=sys.stdout):\n    \"\"\" Move the cursor to the beginning of the line, a number of lines up.\n        Default: 1\n\n        Esc[<lines>F\n    \"\"\"\n    move.prev(lines).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the cursor up a number of lines without changing columns.", "response": "def move_up(lines=1, file=sys.stdout):\n    \"\"\" Move the cursor up a number of lines.\n\n        Esc[ValueA:\n        Moves the cursor up by the specified number of lines without changing\n        columns. If the cursor is already on the top line, ANSI.SYS ignores\n        this sequence.\n    \"\"\"\n    move.up(lines).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint some text in a single line in a single line.", "response": "def print_inplace(*args, **kwargs):\n    \"\"\" Save cursor position, write some text, and then restore the position.\n        Arguments:\n            Same as `print()`.\n\n        Keyword Arguments:\n            Same as `print()`, except `end` defaults to '' (empty str),\n            and these:\n                delay : Time in seconds between character writes.\n    \"\"\"\n    kwargs.setdefault('file', sys.stdout)\n    kwargs.setdefault('end', '')\n    pos_save(file=kwargs['file'])\n    delay = None\n    with suppress(KeyError):\n        delay = kwargs.pop('delay')\n    if delay is None:\n        print(*args, **kwargs)\n    else:\n        for c in kwargs.get('sep', ' ').join(str(a) for a in args):\n            kwargs['file'].write(c)\n            kwargs['file'].flush()\n            sleep(delay)\n        if kwargs['end']:\n            kwargs['file'].write(kwargs['end'])\n    pos_restore(file=kwargs['file'])\n    # Must flush to see changes.\n    kwargs['file'].flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_flush(*args, **kwargs):\n    kwargs.setdefault('file', sys.stdout)\n    print(*args, **kwargs)\n    kwargs['file'].flush()", "response": "Like print except the file is. flushed afterwards."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_overwrite(*args, **kwargs):\n    kwargs.setdefault('file', sys.stdout)\n    kwargs.setdefault('end', '')\n    delay = None\n    with suppress(KeyError):\n        delay = kwargs.pop('delay')\n    erase_line()\n    # Move to the beginning of the line.\n    move_column(1, file=kwargs['file'])\n    if delay is None:\n        print(*args, **kwargs)\n    else:\n        for c in kwargs.get('sep', ' ').join(str(a) for a in args):\n            kwargs['file'].write(c)\n            kwargs['file'].flush()\n            sleep(delay)\n        if kwargs['end']:\n            kwargs['file'].write(kwargs['end'])\n    kwargs['file'].flush()", "response": "Print some text to the current line and overwrite the current line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scroll_down(lines=1, file=sys.stdout):\n    scroll.down(lines).write(file=file)", "response": "Scroll the whole page down a number of lines"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nscrolling the whole page up a number of lines", "response": "def scroll_up(lines=1, file=sys.stdout):\n    \"\"\" Scroll the whole page up a number of lines, new lines are added to\n        the bottom.\n\n        Esc[<lines>S\n    \"\"\"\n    scroll.up(lines).write(file=file)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nerase the screen or part of the screen.", "response": "def erase_display(self, method=EraseMethod.ALL_MOVE):\n        \"\"\" Clear the screen or part of the screen.\n            Arguments:\n                method: One of these possible values:\n                            EraseMethod.END or 0:\n                                Clear from cursor to the end of the screen.\n                            EraseMethod.START or 1:\n                                Clear from cursor to the start of the screen.\n                            EraseMethod.ALL_MOVE or 2:\n                                Clear all, and move home.\n                            EraseMethod.ALL_ERASE or 3:\n                                Clear all, and erase scrollback buffer.\n                            EraseMethod.ALL_MOVE_ERASE or 4:\n                                Like doing 2 and 3 in succession.\n                                This is a feature of Colr. It is not standard.\n                        Default: EraseMethod.ALL_MOVE (2)\n        \"\"\"\n        return self.chained(erase.display(method))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nerase a line or part of a line.", "response": "def erase_line(self, method=EraseMethod.ALL):\n        \"\"\" Erase a line, or part of a line.\n            Arguments:\n                method : One of these possible values:\n                            EraseMethod.END or 0:\n                                Clear from cursor to the end of the line.\n                            EraseMethod.START or 1:\n                                Clear from cursor to the start of the line.\n                            EraseMethod.ALL or 2:\n                                Clear the entire line.\n                         Default: EraseMethod.ALL (2)\n        \"\"\"\n        return self.chained(erase.line(method=method))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the last escape code in self. data.", "response": "def last_code(self):\n        \"\"\" Return the last escape code in `self.data`.\n            If no escape codes are found, '' is returned.\n        \"\"\"\n        codes = self.data.split(escape_sequence)\n        if not codes:\n            return ''\n        return ''.join((escape_sequence, codes[-1]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves the cursor to a new position.", "response": "def move_pos(self, line=1, column=1):\n        \"\"\" Move the cursor to a new position.\n            Default: line 1, column 1\n        \"\"\"\n        return self.chained(move.pos(line=line, column=column))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef repeat(self, count=2):\n        # Subtracting one from the count means the code mentioned is\n        # truly repeated exactly `count` times.\n        # Control().move_up().repeat(3) ==\n        # Control().move_up().move_up().move_up()\n        try:\n            return self.__class__(''.join((\n                str(self),\n                self.last_code() * (count - 1),\n            )))\n        except TypeError as ex:\n            raise TypeError(\n                '`count` must be an integer. Got: {!r}'.format(count)\n            ) from ex", "response": "Repeat the last 3 control codes a number of times. Returns a new Control with this one s data and the repeated code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrepeating this entire Control code a number of times.", "response": "def repeat_all(self, count=2):\n        \"\"\" Repeat this entire Control code a number of times.\n            Returns a new Control with this one's data repeated.\n        \"\"\"\n        try:\n            return self.__class__(''.join(str(self) * count))\n        except TypeError:\n            raise TypeError(\n                '`count` must be an integer. Got: {!r}'.format(count)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a tuple of r g b from a hex color.", "response": "def hex2rgb(hexval: str, allow_short: bool = False) -> RGB:\n    \"\"\" Return a tuple of (R, G, B) from a hex color. \"\"\"\n    if not hexval:\n        raise ValueError(\n            'Expecting hex string (#RGB, #RRGGBB), got nothing: {!r}'.format(\n                hexval\n            )\n        )\n    try:\n        hexval = hexval.strip().lstrip('#')\n    except AttributeError:\n        raise ValueError(\n            'Expecting hex string (#RGB, #RRGGBB), got: ({}) {!r}'.format(\n                type(hexval).__name__,\n                hexval\n            )\n        )\n    if allow_short:\n        hexval = fix_hex(hexval)\n    if not len(hexval) == 6:\n        raise ValueError(\n            'Not a length 3 or 6 hex string (#RGB, #RRGGBB), got: {}'.format(\n                hexval\n            )\n        )\n    try:\n        val = tuple(\n            int(''.join(hexval[i:i + 2]), 16)\n            for i in range(0, len(hexval), 2)\n        )\n    except ValueError:\n        # Bad hex string.\n        raise ValueError('Invalid hex value: {}'.format(hexval))\n    # Only needed to satisft typing. `return val` would work fine.\n    r, g, b = val\n    return r, g, b"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a hex value into a nearest terminal code number.", "response": "def hex2term(hexval: str, allow_short: bool = False) -> str:\n    \"\"\" Convert a hex value into the nearest terminal code number. \"\"\"\n    return rgb2term(*hex2rgb(hexval, allow_short=allow_short))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hex2termhex(hexval: str, allow_short: bool = False) -> str:\n    return rgb2termhex(*hex2rgb(hexval, allow_short=allow_short))", "response": "Convert a hex value into a nearest terminal color matched hex."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint all 256 xterm color codes.", "response": "def print_all() -> None:\n    \"\"\" Print all 256 xterm color codes. \"\"\"\n    for code in sorted(term2hex_map):\n        print(' '.join((\n            '\\033[48;5;{code}m{code:<3}:{hexval:<6}\\033[0m',\n            '\\033[38;5;{code}m{code:<3}:{hexval:<6}\\033[0m'\n        )).format(code=code, hexval=term2hex_map[code]))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rgb2hex(r: int, g: int, b: int) -> str:\n    return '{:02x}{:02x}{:02x}'.format(r, g, b)", "response": "Convert rgb values to hex code."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rgb2term(r: int, g: int, b: int) -> str:\n    return hex2term_map[rgb2termhex(r, g, b)]", "response": "Convert an rgb value to a terminal code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rgb2termhex(r: int, g: int, b: int) -> str:\n    incs = [0x00, 0x5f, 0x87, 0xaf, 0xd7, 0xff]\n\n    res = []\n    parts = r, g, b\n    for part in parts:\n        if (part < 0) or (part > 255):\n            raise ValueError(\n                'Expecting 0-255 for RGB code, got: {!r}'.format(parts)\n            )\n        i = 0\n        while i < len(incs) - 1:\n            s, b = incs[i], incs[i + 1]  # smaller, bigger\n            if s <= part <= b:\n                s1 = abs(s - part)\n                b1 = abs(b - part)\n                if s1 < b1:\n                    closest = s\n                else:\n                    closest = b\n                res.append(closest)\n                break\n            i += 1\n\n    # Convert back into nearest hex value.\n    return rgb2hex(*res)", "response": "Convert an rgb value to the nearest hex value that matches a term code."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef term2hex(code: Numeric, default: Optional[str] = None) -> str:\n    try:\n        val = term2hex_map.get('{:02}'.format(int(code), default))\n    except ValueError:\n        raise ValueError(\n            'Expecting an int or number string, got: {} ({})'.format(\n                code,\n                getattr(code, '__name__', type(code).__name__)))\n    return val", "response": "Convert a term code to a hex string."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninitialize from an int terminal code.", "response": "def _init_code(self, code: int) -> None:\n        \"\"\" Initialize from an int terminal code. \"\"\"\n        if -1 < code < 256:\n            self.code = '{:02}'.format(code)\n            self.hexval = term2hex(code)\n            self.rgb = hex2rgb(self.hexval)\n        else:\n            raise ValueError(' '.join((\n                'Code must be in the range 0-255, inclusive.',\n                'Got: {} ({})'\n            )).format(code, getattr(code, '__name__', type(code).__name__)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_hex(self, hexval: str) -> None:\n        self.hexval = hex2termhex(fix_hex(hexval))\n        self.code = hex2term(self.hexval)\n        self.rgb = hex2rgb(self.hexval)", "response": "Initialize from a hex value string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing from red green blue args.", "response": "def _init_rgb(self, r: int, g: int, b: int) -> None:\n        \"\"\" Initialize from red, green, blue args. \"\"\"\n        if self.rgb_mode:\n            self.rgb = (r, g, b)\n            self.hexval = rgb2hex(r, g, b)\n        else:\n            self.rgb = hex2rgb(rgb2termhex(r, g, b))\n            self.hexval = rgb2termhex(r, g, b)\n\n        self.code = hex2term(self.hexval)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_code(cls, code: int) -> 'ColorCode':\n        c = cls()\n        c._init_code(code)\n        return c", "response": "Return a ColorCode from a terminal code."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a ColorCode from a hex string.", "response": "def from_hex(cls, hexval: str) -> 'ColorCode':\n        \"\"\" Return a ColorCode from a hex string. \"\"\"\n        c = cls()\n        c._init_hex(hexval)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a ColorCode from a RGB tuple.", "response": "def from_rgb(cls, r: int, g: int, b: int) -> 'ColorCode':\n        \"\"\" Return a ColorCode from a RGB tuple. \"\"\"\n        c = cls()\n        c._init_rgb(r, g, b)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef commit(func):\n    '''Used as a decorator for automatically making session commits'''\n    def wrap(**kwarg):\n        with session_withcommit() as session:\n            a = func(**kwarg)\n            session.add(a)\n        return session.query(songs).order_by(\n            songs.song_id.desc()).first().song_id\n    return wrap", "response": "Used as a decorator for automatically making session commits"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_songs()->Iterator:\n    with session_withcommit() as session:\n        val = session.query(songs).all()\n        for row in val:\n            yield row", "response": "Return an iterator over the songs that have the fingerprinted flag set TRUE ( 1 )."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the instance of the hub.", "response": "def get_hub():\n    \"\"\"Return the instance of the hub.\"\"\"\n    try:\n        hub = _local.hub\n    except AttributeError:\n        # The Hub can only be instantiated from the root fiber. No other fibers\n        # can run until the Hub is there, so the root will always be the first\n        # one to call get_hub().\n        assert fibers.current().parent is None\n        hub = _local.hub = Hub()\n    return hub"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsleep for *secs* seconds.", "response": "def sleep(secs):\n    \"\"\"Sleep for *secs* seconds. The *secs* argument can be an int or a float.\"\"\"\n    hub = get_hub()\n    try:\n        with switch_back(secs, hub):\n            hub.switch()\n    except Timeout:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef switch(self, value=None):\n        if self._hub is None or not self._fiber.is_alive():\n            return\n        self._hub.run_callback(self._fiber.switch, value)\n        self._hub = self._fiber = None", "response": "Switch to the origin fiber."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nthrowing an exception into the origin fiber. The exception is thrown the next time the event loop runs.", "response": "def throw(self, typ, val=None, tb=None):\n        \"\"\"Throw an exception into the origin fiber. The exception is thrown\n        the next time the event loop runs.\"\"\"\n        # The might seem redundant with self._fiber.cancel(exc), but it isn't\n        # as self._fiber might be a \"raw\" fibers.Fiber() that doesn't have a\n        # cancel() method.\n        if self._hub is None or not self._fiber.is_alive():\n            return\n        self._hub.run_callback(self._fiber.throw, typ, val, tb)\n        self._hub = self._fiber = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if self._loop is None:\n            return\n        if fibers.current().parent is not None:\n            raise RuntimeError('close() may only be called in the root fiber')\n        elif compat.get_thread_ident() != self._thread:\n            raise RuntimeError('cannot close() from a different thread')\n        self._closing = True\n        self._interrupt_loop()\n        # Note how we are switching to the Hub without a switchback condition\n        # being in place. This works because the hub is our child and upon\n        # a child fiber exit its parent is switched in.\n        self.switch()", "response": "Close the hub and wait for it to be closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nswitches the current fiber to the hub.", "response": "def switch(self):\n        \"\"\"Switch to the hub.\n\n        This method pauses the current fiber and runs the event loop. The\n        caller should ensure that it has set up appropriate callbacks so that\n        it will get scheduled again, preferably using :class:`switch_back`. In\n        this case then return value of this method will be an ``(args,\n        kwargs)`` tuple containing the arguments passed to the switch back\n        instance.\n\n        If this method is called from the root fiber then there are two\n        additional cases. If the hub exited due to a call to :meth:`close`,\n        then this method returns None. And if the hub exited due to a\n        exception, that exception is re-raised here.\n        \"\"\"\n        if self._loop is None or not self.is_alive():\n            raise RuntimeError('hub is closed/dead')\n        elif fibers.current() is self:\n            raise RuntimeError('cannot switch to myself')\n        elif compat.get_thread_ident() != self._thread:\n            raise RuntimeError('cannot switch from a different thread')\n        value = super(Hub, self).switch()\n        # A fiber exit will cause its parent to be switched to. All fibers in\n        # the system should be children of the Hub, *except* the Hub itself\n        # which is a child of the root fiber. So do an explicit check here to\n        # see if the Hub exited unexpectedly, and if so raise an error.\n        if fibers.current().parent is None and not self.is_alive() \\\n                    and self._loop is not None:\n            raise RuntimeError('hub exited unexpectedly')\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nqueue a callback. The *callback* will be called with positional arguments *args* in the next iteration of the event loop. If you add multiple callbacks, they will be called in the order that you added them. The callback will run in the Hub's fiber. This method is thread-safe: it is allowed to queue a callback from a different thread than the one running the Hub.", "response": "def run_callback(self, callback, *args):\n        \"\"\"Queue a callback.\n\n        The *callback* will be called with positional arguments *args* in the\n        next iteration of the event loop. If you add multiple callbacks, they\n        will be called in the order that you added them. The callback will run\n        in the Hub's fiber.\n\n        This method is thread-safe: it is allowed to queue a callback from a\n        different thread than the one running the Hub.\n        \"\"\"\n        if self._loop is None:\n            raise RuntimeError('hub is closed')\n        elif not callable(callback):\n            raise TypeError('\"callback\": expecting a callable')\n        self._callbacks.append((callback, args))  # thread-safe\n        self._interrupt_loop()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string describing a message for debugging purposes.", "response": "def message_info(message):\n    \"\"\"Return a string describing a message, for debugging purposes.\"\"\"\n    method = message.get('method')\n    msgid = message.get('id')\n    error = message.get('error')\n    if method and msgid is not None:\n        return 'method call \"{}\", id = \"{}\"'.format(method, msgid)\n    elif method:\n        return 'notification \"{}\"'.format(method)\n    elif error is not None and msgid is not None:\n        return 'error reply to id = \"{}\"'.format(msgid)\n    elif error is not None:\n        code = error.get('code', '(none)')\n        return 'error reply: {}'.format(errorcode.get(code, code))\n    else:\n        return 'method return for id = \"{}\"'.format(msgid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a unique message ID.", "response": "def next_id(self):\n        \"\"\"Return a unique message ID.\"\"\"\n        msgid = self._id_template.format(self._next_id)\n        self._next_id += 1\n        return msgid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new instance for version.", "response": "def create(version):\n        \"\"\"Return a new instance for *version*, which can be either `'1.0'`\n        or `'2.0'`.\"\"\"\n        clsname = 'JsonRpcV{}'.format(version.rstrip('.0'))\n        cls = globals()[clsname]\n        return cls(version)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_message(self, message):\n        if self._error:\n            raise compat.saved_exc(self._error)\n        elif self._transport is None:\n            raise JsonRpcError('not connected')\n        self._version.check_message(message)\n        self._writer.write(serialize(message))", "response": "Send a raw JSON - RPC message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef call_method(self, method, *args):\n        message = self._version.create_request(method, args)\n        msgid = message['id']\n        try:\n            with switch_back(self._timeout) as switcher:\n                self._method_calls[msgid] = switcher\n                self.send_message(message)\n                args, _ = self._hub.switch()\n        finally:\n            self._method_calls.pop(msgid, None)\n        response = args[0]\n        assert response['id'] == msgid\n        error = response.get('error')\n        if error is not None:\n            raise JsonRpcError('error response calling \"{}\"'.format(method), error)\n        return response['result']", "response": "Call a JSON - RPC method and wait for its result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a JSON - RPC notification.", "response": "def send_notification(self, method, *args):\n        \"\"\"Send a JSON-RPC notification.\n\n        The notification *method* is sent with positional arguments *args*.\n        \"\"\"\n        message = self._version.create_request(method, args, notification=True)\n        self.send_message(message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_response(self, request, result=None, error=None):\n        message = self._version.create_response(request, result, error)\n        self.send_message(message)", "response": "Respond to a JSON - RPC method call."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the DBus signature for the supplied Python object.", "response": "def sigFromPy( pobj ):\n    \"\"\"\n    Returns the DBus signature type for the argument. If the argument is an\n    instance of one of the type wrapper classes, the exact type signature\n    corresponding to the wrapper class will be used. If the object has a\n    variable named 'dbusSignature', the value of that variable will be\n    used. Otherwise, a generic type will be used (i.e \"i\" for a Python int)\n\n    @rtype: C{string}\n    @returns: The DBus signature for the supplied Python object\n    \"\"\"\n    sig = getattr(pobj, 'dbusSignature', None)\n    \n    if sig is not None:\n        return sig\n    \n    elif isinstance(pobj,        int): return 'i'\n    elif isinstance(pobj, six.integer_types): return 'x'\n    elif isinstance(pobj,      float): return 'd'\n    elif isinstance(pobj, six.string_types): return 's'\n    \n    elif isinstance(pobj,       list):\n        vtype = type(pobj[0])\n        same = True\n        for v in pobj[1:]:\n            if not vtype is type(v):\n                same = False\n        if same:\n            return 'a' + sigFromPy(pobj[0])\n        else:\n            return 'av'\n    \n    elif isinstance(pobj,       dict):\n        same = True\n        vtype = None\n        for k,v in six.iteritems(pobj):\n            if vtype is None:\n                vtype = type(v)\n            elif not vtype is type(v):\n                same = False\n        if same:\n            return 'a{' + sigFromPy(k) + sigFromPy(v) + '}'\n        else:\n            return 'a{' + sigFromPy(k) + 'v}'\n    \n    else:\n        raise MarshallingError('Invalid Python type for variant: ' + repr(pobj))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef marshal( compoundSignature, variableList, startByte = 0, lendian=True ):\n    chunks = list()\n    bstart = startByte\n\n    if hasattr(variableList, 'dbusOrder'):\n        order = getattr(variableList, 'dbusOrder')\n        variableList = [ getattr(variableList, attr_name) for attr_name in order ]\n\n    for ct, var in zip(genCompleteTypes( compoundSignature ), variableList):\n        tcode   = ct[0]\n        padding = pad[tcode]( startByte )\n        \n        if padding:\n            startByte += len(padding)\n            chunks.append( padding )\n        \n        nbytes, vchunks = marshallers[ tcode ]( ct, var, startByte, lendian )\n\n        startByte += nbytes\n        \n        chunks.extend( vchunks )\n        \n\n    return startByte - bstart, chunks", "response": "This function encodes the Python objects in variableList into the DBus wire - format and returns a list of binary strings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spawn(func, *args, **kwargs):\n    fiber = Fiber(func, args, **kwargs)\n    fiber.start()\n    return fiber", "response": "Spawn a new fiber."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start(self):\n        target = getattr(self._target, '__qualname__', self._target.__name__)\n        self._log.debug('starting fiber {}, target {}', self.name, target)\n        self._hub.run_callback(self.switch)", "response": "Schedule the fiber to be started in the next iteration of the\n        event loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nschedule the fiber to be cancelled in the next iteration of the fiber event loop.", "response": "def cancel(self, message=None):\n        \"\"\"Schedule the fiber to be cancelled in the next iteration of the\n        event loop.\n\n        Cancellation works by throwing a :class:`~gruvi.Cancelled` exception\n        into the fiber. If *message* is provided, it will be set as the value\n        of the exception.\n        \"\"\"\n        if not self.is_alive():\n            return\n        if message is None:\n            message = 'cancelled by Fiber.cancel()'\n        self._hub.run_callback(self.throw, Cancelled, Cancelled(message))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef align_matches(self, matches: list)->Optional[dict]:\n        # align by diffs\n        diff_counter: dict = {}\n        largest = 0\n        largest_count = 0\n        song_id = -1\n        for sid, diff in matches:\n\n            if diff not in diff_counter:\n                diff_counter[diff] = {}\n            if sid not in diff_counter[diff]:\n                diff_counter[diff][sid] = 0\n            diff_counter[diff][sid] += 1\n\n            if diff_counter[diff][sid] > largest_count:\n                largest = diff\n                largest_count = diff_counter[diff][sid]\n                song_id = sid\n\n        # extract idenfication\n        song = database.get_song_by_id(song_id)\n        if song:\n            songname = song.song_name\n        else:\n            return None\n\n        # return match info\n        nseconds = round(\n            float(largest) / fingerprint.DEFAULT_FS *\n            fingerprint.DEFAULT_WINDOW_SIZE * fingerprint.DEFAULT_OVERLAP_RATIO,\n            5\n        )\n        song = {\n            'song_id': song_id,\n            'song_name': songname,\n            MetaMusic.CONFIDENCE: largest_count,\n            MetaMusic.OFFSET: int(largest),\n            'offset_seconds': nseconds,\n            'file_sha1': binascii.hexlify(song.file_sha1).decode('utf-8'),\n        }\n        return song", "response": "Aligns matches in time with other matches and returns a dictionary with match information."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string representation of an IP address tuple or a socket address.", "response": "def saddr(address):\n    \"\"\"Return a string representation for an address.\n\n    The *address* paramater can be a pipe name, an IP address tuple, or a\n    socket address.\n\n    The return value is always a ``str`` instance.\n    \"\"\"\n    if isinstance(address, six.string_types):\n        return address\n    elif isinstance(address, tuple) and len(address) >= 2 and ':' in address[0]:\n        return '[{}]:{}'.format(address[0], address[1])\n    elif isinstance(address, tuple) and len(address) >= 2:\n        return '{}:{}'.format(*address)\n    else:\n        raise TypeError('illegal address type: {!s}'.format(type(address)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef paddr(address):\n    if not isinstance(address, six.string_types):\n        raise TypeError('expecting a string')\n    if address.startswith('['):\n        p1 = address.find(']:')\n        if p1 == -1:\n            raise ValueError\n        return (address[1:p1], int(address[p1+2:]))\n    elif ':' in address:\n        p1 = address.find(':')\n        return (address[:p1], int(address[p1+1:]))\n    else:\n        return address", "response": "Parse a string representation of an address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresolves an Internet node name and service into a socket address.", "response": "def getaddrinfo(node, service=0, family=0, socktype=0, protocol=0, flags=0, timeout=30):\n    \"\"\"Resolve an Internet *node* name and *service* into a socket address.\n\n    The *family*, *socktype* and *protocol* are optional arguments that specify\n    the address family, socket type and protocol, respectively. The *flags*\n    argument allows you to pass flags to further modify the resolution process.\n    See the :func:`socket.getaddrinfo` function for a detailed description of\n    these arguments.\n\n    The return value is a list of ``(family, socktype, proto, canonname,\n    sockaddr)`` tuples. The fifth element (``sockaddr``) is the socket address.\n    It will be a 2-tuple ``(addr, port)`` for an IPv4 address, and a 4-tuple\n    ``(addr, port, flowinfo, scopeid)`` for an IPv6 address.\n\n    The address resolution is performed in the libuv thread pool.\n    \"\"\"\n    hub = get_hub()\n    with switch_back(timeout) as switcher:\n        request = pyuv.dns.getaddrinfo(hub.loop, node, service, family,\n                                       socktype, protocol, flags, callback=switcher)\n        switcher.add_cleanup(request.cancel)\n        result = hub.switch()\n    result, error = result[0]\n    if error:\n        message = pyuv.errno.strerror(error)\n        raise pyuv.error.UVError(error, message)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getnameinfo(sockaddr, flags=0, timeout=30):\n    hub = get_hub()\n    with switch_back(timeout) as switcher:\n        request = pyuv.dns.getnameinfo(hub.loop, sockaddr, flags, callback=switcher)\n        switcher.add_cleanup(request.cancel)\n        result = hub.switch()\n    result, error = result[0]\n    if error:\n        message = pyuv.errno.strerror(error)\n        raise pyuv.error.UVError(error, message)\n    return result", "response": "Resolve a socket address sockaddr back to a node service tuple."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_codes(s: Union[str, 'ChainedBase']) -> List[str]:\n    return codegrabpat.findall(str(s))", "response": "Grab all escape codes from a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_code_indices(s: Union[str, 'ChainedBase']) -> Dict[int, str]:\n    indices = {}\n    i = 0\n    codes = get_codes(s)\n    for code in codes:\n        codeindex = s.index(code)\n        realindex = i + codeindex\n        indices[realindex] = code\n        codelen = len(code)\n        i = realindex + codelen\n        s = s[codeindex + codelen:]\n    return indices", "response": "Retrieve a dict of index and escape code for a given string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_indices(s: Union[str, 'ChainedBase']) -> Dict[int, str]:\n    codes = get_code_indices(s)\n    if not codes:\n        # This function is not for non-escape-code stuff, but okay.\n        return {i: c for i, c in enumerate(s)}\n\n    indices = {}\n    for codeindex in sorted(codes):\n        code = codes[codeindex]\n        if codeindex == 0:\n            indices[codeindex] = code\n            continue\n        # Grab characters before codeindex.\n        start = max(indices or {0: ''}, key=int)\n        startcode = indices.get(start, '')\n        startlen = start + len(startcode)\n        indices.update({i: s[i] for i in range(startlen, codeindex)})\n        indices[codeindex] = code\n\n    if not indices:\n        return {i: c for i, c in enumerate(s)}\n    lastindex = max(indices, key=int)\n    lastitem = indices[lastindex]\n    start = lastindex + len(lastitem)\n    textlen = len(s)\n    if start < (textlen - 1):\n        # Grab chars after last code.\n        indices.update({i: s[i] for i in range(start, textlen)})\n    return indices", "response": "Retrieve a dict of characters and escape codes with their real index\n        into the string as the key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strip_codes(s: Union[str, 'ChainedBase']) -> str:\n    return codepat.sub('', str(s) if (s or (s == 0)) else '')", "response": "Strip all color codes from a string."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new callback.", "response": "def add_callback(self, events, callback):\n        \"\"\"Add a new callback.\"\"\"\n        if self._poll is None:\n            raise RuntimeError('poll instance is closed')\n        if events & ~(READABLE|WRITABLE):\n            raise ValueError('illegal event mask: {}'.format(events))\n        if events & READABLE:\n            self._readers += 1\n        if events & WRITABLE:\n            self._writers += 1\n        handle = add_callback(self, callback, events)\n        self._sync()\n        return handle"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a callback from the poll.", "response": "def remove_callback(self, handle):\n        \"\"\"Remove a callback.\"\"\"\n        if self._poll is None:\n            raise RuntimeError('poll instance is closed')\n        remove_callback(self, handle)\n        if handle.extra & READABLE:\n            self._readers -= 1\n        if handle.extra & WRITABLE:\n            self._writers -= 1\n        self._sync()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_callback(self, handle, events):\n        if self._poll is None:\n            raise RuntimeError('poll instance is closed')\n        if not has_callback(self, handle):\n            raise ValueError('no such callback')\n        if events & ~(READABLE|WRITABLE):\n            raise ValueError('illegal event mask: {}'.format(events))\n        if handle.extra == events:\n            return\n        if handle.extra & READABLE:\n            self._readers -= 1\n        if handle.extra & WRITABLE:\n            self._writers -= 1\n        if events & READABLE:\n            self._readers += 1\n        if events & WRITABLE:\n            self._writers += 1\n        handle.extra = events\n        self._sync()", "response": "Update the event mask for a callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef close(self):\n        if self._poll is None:\n            return\n        self._poll.close()\n        self._poll = None\n        self._readers = 0\n        self._writers = 0\n        self._events = 0\n        clear_callbacks(self)", "response": "Close the poll instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_callback(self, fd, events, callback):\n        if self._mpoll is None:\n            raise RuntimeError('Poller instance is closed')\n        try:\n            mpoll = self._mpoll[fd]\n        except KeyError:\n            mpoll = self._mpoll[fd] = MultiPoll(self._loop, fd)\n        handle = mpoll.add_callback(events, callback)\n        return handle", "response": "Add a callback to be called when events occur."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a callback from the Poller.", "response": "def remove_callback(self, fd, handle):\n        \"\"\"Remove a callback added by :meth:`~Poller.add_callback`.\n\n        If this is the last callback that is registered for the fd then this\n        will deallocate the ``MultiPoll`` instance and close the libuv handle.\n        \"\"\"\n        if self._mpoll is None:\n            raise RuntimeError('Poller instance is closed')\n        mpoll = self._mpoll.get(fd)\n        if mpoll is None:\n            raise ValueError('not watching fd {}'.format(fd))\n        mpoll.remove_callback(handle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the event mask associated with an existing callback.", "response": "def update_callback(self, fd, handle, events):\n        \"\"\"Update the event mask associated with an existing callback.\n\n        If you want to temporarily disable a callback then you can use this\n        method with an *events* argument of ``0``. This is more efficient than\n        removing the callback and adding it again later.\n        \"\"\"\n        if self._mpoll is None:\n            raise RuntimeError('Poller instance is closed')\n        mpoll = self._mpoll.get(fd)\n        if mpoll is None:\n            raise ValueError('not watching fd {}'.format(fd))\n        mpoll.update_callback(handle, events)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        if self._mpoll is None:\n            return\n        for mpoll in self._mpoll.values():\n            mpoll.close()\n        self._mpoll.clear()\n        self._mpoll = None", "response": "Close all active poll instances and remove all callbacks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nattempting the next authentication method or raises a failure if all mechanisms have been tried.", "response": "def authTryNextMethod(self):\n        \"\"\"\n        Tries the next authentication method or raises a failure if all mechanisms\n        have been tried.\n        \"\"\"\n        if not self.authOrder:\n            raise DBusAuthenticationFailed()\n        \n        self.authMech  = self.authOrder.pop()\n            \n        if self.authMech == 'DBUS_COOKIE_SHA1':\n            self.sendAuthMessage('AUTH ' + self.authMech + ' ' +\n                                 hexlify(getpass.getuser()))\n        elif self.authMech == 'ANONYMOUS':\n            self.sendAuthMessage('AUTH ' + self.authMech + ' ' +\n                                 hexlify(\"txdbus\"))\n        else:\n            self.sendAuthMessage('AUTH ' + self.authMech)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the DBus cookie from the cookie_context file and returns the hex cookie of the DBus cookie.", "response": "def _authGetDBusCookie(self, cookie_context, cookie_id):\n        \"\"\"\n        Reads the requested cookie_id from the cookie_context file\n        \"\"\"\n        # XXX   Ensure we obtain the correct directory for the\n        #       authenticating user and that that user actually\n        #       owns the keyrings directory\n\n        if self.cookie_dir is None:\n            cookie_dir = os.path.expanduser('~/.dbus-keyrings')\n        else:\n            cookie_dir = self.cookie_dir\n\n        dstat = os.stat(cookie_dir)\n\n        if dstat.st_mode & 0x36:  # 066\n            raise Exception('User keyrings directory is writeable by other users. Aborting authentication')\n\n        import pwd\n        if dstat.st_uid != pwd.getpwuid(os.geteuid()).pw_uid:\n            raise Exception('Keyrings directory is not owned by the current user. Aborting authentication!')\n        \n        f = open(os.path.join(cookie_dir, cookie_context), 'r')\n\n        try:\n            for line in f:\n                try:\n                    k_id, k_time, k_cookie_hex = line.split()\n                    if k_id == cookie_id:\n                        return k_cookie_hex\n                except:\n                    pass\n        finally:\n            f.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_callback(obj, callback, args=()):\n    callbacks = obj._callbacks\n    node = Node(callback, args)\n    # Store a single callback directly in _callbacks\n    if callbacks is None:\n        obj._callbacks = node\n        return node\n    # Otherwise use a dllist.\n    if not isinstance(callbacks, dllist):\n        obj._callbacks = dllist()\n        obj._callbacks.insert(callbacks)\n        callbacks = obj._callbacks\n    callbacks.insert(node)\n    return node", "response": "Add a callback to an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a callback from an object.", "response": "def remove_callback(obj, handle):\n    \"\"\"Remove a callback from an object.\"\"\"\n    callbacks = obj._callbacks\n    if callbacks is handle:\n        obj._callbacks = None\n    elif isinstance(callbacks, dllist):\n        callbacks.remove(handle)\n        if not callbacks:\n            obj._callbacks = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning whether a callback is currently registered for an object.", "response": "def has_callback(obj, handle):\n    \"\"\"Return whether a callback is currently registered for an object.\"\"\"\n    callbacks = obj._callbacks\n    if not callbacks:\n        return False\n    if isinstance(callbacks, Node):\n        return handle is callbacks\n    else:\n        return handle in callbacks"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npop a single callback from the list of callbacks.", "response": "def pop_callback(obj):\n    \"\"\"Pop a single callback.\"\"\"\n    callbacks = obj._callbacks\n    if not callbacks:\n        return\n    if isinstance(callbacks, Node):\n        node = callbacks\n        obj._callbacks = None\n    else:\n        node = callbacks.first\n        callbacks.remove(node)\n        if not callbacks:\n            obj._callbacks = None\n    return node.data, node.extra"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clear_callbacks(obj):\n    callbacks = obj._callbacks\n    if isinstance(callbacks, dllist):\n        # Help the garbage collector by clearing all links.\n        callbacks.clear()\n    obj._callbacks = None", "response": "Remove all callbacks from an object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walk_callbacks(obj, func, log=None):\n    callbacks = obj._callbacks\n    if isinstance(callbacks, Node):\n        node = callbacks\n        try:\n            if not func(node.data, node.extra):\n                obj._callbacks = None\n        except Exception:\n            if log is None:\n                log = logging.get_logger()\n            log.exception('uncaught exception in callback')\n    elif isinstance(callbacks, dllist):\n        for node in callbacks:\n            try:\n                if func(node.data, node.extra):\n                    continue\n                callbacks.remove(node)\n            except Exception:\n                if log is None:\n                    log = logging.get_logger()\n                log.exception('uncaught exception in callback')\n        if not callbacks:\n            obj._callbacks = None", "response": "Walks the callbacks list and calls func for all those those\n    callbacks for which the function returns True."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning callbacks on the object.", "response": "def run_callbacks(obj, log=None):\n    \"\"\"Run callbacks.\"\"\"\n    def run_callback(callback, args):\n        return callback(*args)\n    return walk_callbacks(obj, run_callback, log)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a ConfigurationModel serializer class for the supplied configuration_model.", "response": "def get_serializer_class(configuration_model):\n    \"\"\" Returns a ConfigurationModel serializer class for the supplied configuration_model. \"\"\"\n    class AutoConfigModelSerializer(ModelSerializer):\n        \"\"\"Serializer class for configuration models.\"\"\"\n\n        class Meta(object):\n            \"\"\"Meta information for AutoConfigModelSerializer.\"\"\"\n            model = configuration_model\n            fields = '__all__'\n\n        def create(self, validated_data):\n            if \"changed_by_username\" in self.context:\n                model = get_user_model()\n                validated_data['changed_by'] = model.objects.get(username=self.context[\"changed_by_username\"])\n            return super(AutoConfigModelSerializer, self).create(validated_data)\n\n    return AutoConfigModelSerializer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a stream containing JSON, deserializers the JSON into ConfigurationModel instances. The stream is expected to be in the following format: { \"model\": \"config_models.ExampleConfigurationModel\", \"data\": [ { \"enabled\": True, \"color\": \"black\" ... }, { \"enabled\": False, \"color\": \"yellow\" ... }, ... ] } If the provided stream does not contain valid JSON for the ConfigurationModel specified, an Exception will be raised. Arguments: stream: The stream of JSON, as described above. username: The username of the user making the change. This must match an existing user. Returns: the number of created entries", "response": "def deserialize_json(stream, username):\n    \"\"\"\n    Given a stream containing JSON, deserializers the JSON into ConfigurationModel instances.\n\n    The stream is expected to be in the following format:\n        { \"model\": \"config_models.ExampleConfigurationModel\",\n          \"data\":\n            [\n              { \"enabled\": True,\n                \"color\": \"black\"\n                ...\n              },\n              { \"enabled\": False,\n                \"color\": \"yellow\"\n                ...\n              },\n              ...\n            ]\n        }\n\n    If the provided stream does not contain valid JSON for the ConfigurationModel specified,\n    an Exception will be raised.\n\n    Arguments:\n            stream: The stream of JSON, as described above.\n            username: The username of the user making the change. This must match an existing user.\n\n    Returns: the number of created entries\n    \"\"\"\n    parsed_json = JSONParser().parse(stream)\n    serializer_class = get_serializer_class(apps.get_model(parsed_json[\"model\"]))\n    list_serializer = serializer_class(data=parsed_json[\"data\"], context={\"changed_by_username\": username}, many=True)\n    if list_serializer.is_valid():\n        model_class = serializer_class.Meta.model\n        for data in reversed(list_serializer.validated_data):\n            if model_class.equal_to_current(data):\n                list_serializer.validated_data.remove(data)\n\n        entries_created = len(list_serializer.validated_data)\n        list_serializer.save()\n        return entries_created\n    else:\n        raise Exception(list_serializer.error_messages)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all field names excluding reverse foreign key relationships.", "response": "def get_displayable_field_names(self):\n        \"\"\"\n        Return all field names, excluding reverse foreign key relationships.\n        \"\"\"\n        return [\n            f.name\n            for f in self.model._meta.get_fields()\n            if not f.one_to_many\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choices(self, changelist):\n        show_all = self.used_parameters.get(self.parameter_name) == \"1\"\n        return (\n            {\n                'display': _('Current Configuration'),\n                'selected': not show_all,\n                'query_string': changelist.get_query_string({}, [self.parameter_name]),\n            },\n            {\n                'display': _('All (Show History)'),\n                'selected': show_all,\n                'query_string': changelist.get_query_string({self.parameter_name: \"1\"}, []),\n            }\n        )", "response": "Returns a list of choices ready to be output in the template."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_queryset(self, request):\n        if request.GET.get(ShowHistoryFilter.parameter_name) == '1':\n            queryset = self.model.objects.with_active_flag()\n        else:\n            # Show only the most recent row for each key.\n            queryset = self.model.objects.current_set()\n        ordering = self.get_ordering(request)\n        if ordering:\n            return queryset.order_by(*ordering)\n        return queryset", "response": "Returns the queryset for the most recent entry for the given request."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nedit link for the change view", "response": "def edit_link(self, inst):\n        \"\"\" Edit link for the change view \"\"\"\n        if not inst.is_active:\n            return u'--'\n        update_url = reverse('admin:{}_{}_add'.format(self.model._meta.app_label, self.model._meta.model_name))\n        update_url += \"?source={}\".format(inst.pk)\n        return u'<a href=\"{}\">{}</a>'.format(update_url, _('Update'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides django. contrib. admin. templatetags. admin_modify. submit_row.", "response": "def submit_row(context):\n    \"\"\"\n    Overrides 'django.contrib.admin.templatetags.admin_modify.submit_row'.\n\n    Manipulates the context going into that function by hiding all of the buttons\n    in the submit row if the key `readonly` is set in the context.\n    \"\"\"\n    ctx = original_submit_row(context)\n\n    if context.get('readonly', False):\n        ctx.update({\n            'show_delete_link': False,\n            'show_save_as_new': False,\n            'show_save_and_add_another': False,\n            'show_save_and_continue': False,\n            'show_save': False,\n        })\n    else:\n        return ctx"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef current_set(self):\n        assert self.model.KEY_FIELDS != (), \"Just use model.current() if there are no KEY_FIELDS\"\n        return self.get_queryset().filter(\n            pk__in=self._current_ids_subquery()\n        ).annotate(\n            is_active=models.Value(1, output_field=models.IntegerField())\n        )", "response": "A queryset for the active configuration entries only. Only useful if KEY_FIELDS is set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving the current configuration entry to the database.", "response": "def save(self, force_insert=False, force_update=False, using=None,\n             update_fields=None):\n        \"\"\"\n        Clear the cached value when saving a new configuration entry\n        \"\"\"\n        # Always create a new entry, instead of updating an existing model\n        self.pk = None  # pylint: disable=invalid-name\n        super(ConfigurationModel, self).save(\n            force_insert,\n            force_update,\n            using,\n            update_fields\n        )\n        cache.delete(self.cache_key_name(*[getattr(self, key) for key in self.KEY_FIELDS]))\n        if self.KEY_FIELDS:\n            cache.delete(self.key_values_cache_key_name())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cache_key_name(cls, *args):\n        if cls.KEY_FIELDS != ():\n            if len(args) != len(cls.KEY_FIELDS):\n                raise TypeError(\n                    \"cache_key_name() takes exactly {} arguments ({} given)\".format(len(cls.KEY_FIELDS), len(args))\n                )\n            # pylint: disable=unicode-builtin\n            return 'configuration/{}/current/{}'.format(cls.__name__, ','.join(six.text_type(arg) for arg in args))\n        else:\n            return 'configuration/{}/current'.format(cls.__name__)", "response": "Return the name of the key to use to cache the current configuration"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef current(cls, *args):\n        cached = cache.get(cls.cache_key_name(*args))\n        if cached is not None:\n            return cached\n\n        key_dict = dict(zip(cls.KEY_FIELDS, args))\n        try:\n            current = cls.objects.filter(**key_dict).order_by('-change_date')[0]\n        except IndexError:\n            current = cls(**key_dict)\n\n        cache.set(cls.cache_key_name(*args), current, cls.cache_timeout)\n        return current", "response": "Return the active configuration entry from the database or by creating a new one if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nkeys for fetching unique key values from the cache", "response": "def key_values_cache_key_name(cls, *key_fields):\n        \"\"\" Key for fetching unique key values from the cache \"\"\"\n        key_fields = key_fields or cls.KEY_FIELDS\n        return 'configuration/{}/key_values/{}'.format(cls.__name__, ','.join(key_fields))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of unique values for the given key_fields.", "response": "def key_values(cls, *key_fields, **kwargs):\n        \"\"\"\n        Get the set of unique values in the configuration table for the given\n        key[s]. Calling cls.current(*value) for each value in the resulting\n        list should always produce an entry, though any such entry may have\n        enabled=False.\n\n        Arguments:\n            key_fields: The positional arguments are the KEY_FIELDS to return. For example if\n                you had a course embargo configuration where each entry was keyed on (country,\n                course), then you might want to know \"What countries have embargoes configured?\"\n                with cls.key_values('country'), or \"Which courses have country restrictions?\"\n                with cls.key_values('course'). You can also leave this unspecified for the\n                default, which returns the distinct combinations of all keys.\n            flat: If you pass flat=True as a kwarg, it has the same effect as in Django's\n                'values_list' method: Instead of returning a list of lists, you'll get one list\n                of values. This makes sense to use whenever there is only one key being queried.\n\n        Return value:\n            List of lists of each combination of keys found in the database.\n            e.g. [(\"Italy\", \"course-v1:SomeX+some+2015\"), ...] for the course embargo example\n        \"\"\"\n        flat = kwargs.pop('flat', False)\n        assert not kwargs, \"'flat' is the only kwarg accepted\"\n        key_fields = key_fields or cls.KEY_FIELDS\n        cache_key = cls.key_values_cache_key_name(*key_fields)\n        cached = cache.get(cache_key)\n        if cached is not None:\n            return cached\n        values = list(cls.objects.values_list(*key_fields, flat=flat).order_by().distinct())\n        cache.set(cache_key, values, cls.cache_timeout)\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the fields of this instance are equal False otherwise.", "response": "def fields_equal(self, instance, fields_to_ignore=(\"id\", \"change_date\", \"changed_by\")):\n        \"\"\"\n        Compares this instance's fields to the supplied instance to test for equality.\n        This will ignore any fields in `fields_to_ignore`.\n\n        Note that this method ignores many-to-many fields.\n\n        Args:\n            instance: the model instance to compare\n            fields_to_ignore: List of fields that should not be compared for equality. By default\n            includes `id`, `change_date`, and `changed_by`.\n\n        Returns: True if the checked fields are all equivalent, else False\n        \"\"\"\n        for field in self._meta.get_fields():\n            if not field.many_to_many and field.name not in fields_to_ignore:\n                if getattr(instance, field.name) != getattr(self, field.name):\n                    return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompare this instance to a model instance constructed from the supplied JSON.", "response": "def equal_to_current(cls, json, fields_to_ignore=(\"id\", \"change_date\", \"changed_by\")):\n        \"\"\"\n        Compares for equality this instance to a model instance constructed from the supplied JSON.\n        This will ignore any fields in `fields_to_ignore`.\n\n        Note that this method cannot handle fields with many-to-many associations, as those can only\n        be set on a saved model instance (and saving the model instance will create a new entry).\n        All many-to-many field entries will be removed before the equality comparison is done.\n\n        Args:\n            json: json representing an entry to compare\n            fields_to_ignore: List of fields that should not be compared for equality. By default\n            includes `id`, `change_date`, and `changed_by`.\n\n        Returns: True if the checked fields are all equivalent, else False\n        \"\"\"\n\n        # Remove many-to-many relationships from json.\n        # They require an instance to be already saved.\n        info = model_meta.get_field_info(cls)\n        for field_name, relation_info in info.relations.items():\n            if relation_info.to_many and (field_name in json):\n                json.pop(field_name)\n\n        new_instance = cls(**json)\n        key_field_args = tuple(getattr(new_instance, key) for key in cls.KEY_FIELDS)\n        current = cls.current(*key_field_args)\n        # If current.id is None, no entry actually existed and the \"current\" method created it.\n        if current.id is not None:\n            return current.fields_equal(new_instance, fields_to_ignore)\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_atomic_wrapper(cls, wrapped_func):\n        def _create_atomic_wrapper(*args, **kwargs):\n            \"\"\"Actual wrapper.\"\"\"\n            # When a view call fails due to a permissions error, it raises an exception.\n            # An uncaught exception breaks the DB transaction for any following DB operations\n            # unless it's wrapped in a atomic() decorator or context manager.\n            with transaction.atomic():\n                return wrapped_func(*args, **kwargs)\n\n        return _create_atomic_wrapper", "response": "Returns a wrapped function that wraps the given function with atomic operations."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_view(cls, **initkwargs):\n        view = super(AtomicMixin, cls).as_view(**initkwargs)\n        return cls.create_atomic_wrapper(view)", "response": "Overrides as_view to add atomic transaction."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef require_config(config_model):\n    def _decorator(func):\n        \"\"\"\n        Decorator implementation.\n        \"\"\"\n        @wraps(func)\n        def _inner(*args, **kwargs):\n            \"\"\"\n            Wrapper implementation.\n            \"\"\"\n            if not config_model.current().enabled:\n                return HttpResponseNotFound()\n            return func(*args, **kwargs)\n        return _inner\n    return _decorator", "response": "Decorator that enables or disables a view based on the current configuration model."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge a list of feature layers into one.", "response": "def merge(file, feature_layers):\n    ''' Retrieve a list of OSciMap4 tile responses and merge them into one.\n\n        get_tiles() retrieves data and performs basic integrity checks.\n    '''\n    tile = VectorTile(extents)\n\n    for layer in feature_layers:\n        tile.addFeatures(layer['features'], layer['name'])\n\n    tile.complete()\n\n    data = tile.out.SerializeToString()\n    file.write(struct.pack(\">I\", len(data)))\n    file.write(data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_valid_if_necessary(shape):\n    if shape.type in ('Polygon', 'MultiPolygon') and not shape.is_valid:\n        shape = shape.buffer(0)\n\n        # return value from buffer is usually valid, but it's\n        # not clear from the docs whether this is guaranteed,\n        # so return None if not.\n        if not shape.is_valid:\n            return None\n\n    return shape", "response": "Make valid shape if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _accumulate_props(dest_props, src_props):\n    props_size = 0\n    if src_props:\n        for k, v in src_props.items():\n            if v is not None:\n                props_size += len(k) + _sizeof(v)\n                dest_props[k] = v\n    return props_size", "response": "Helper function to accumulate a dict of properties\nWorkItem and non - None src_props into dest_props"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef metatile_children_with_size(coord, metatile_zoom, nominal_zoom, tile_size):\n\n    from tilequeue.tile import coord_children_subrange\n    from tilequeue.tile import metatile_zoom_from_size\n\n    assert tile_size >= 256\n    assert tile_size <= 256 * (1 << metatile_zoom)\n    assert _is_power_of_2(tile_size)\n\n    # delta is how many zoom levels _lower_ we want the child tiles, based on\n    # their tile size. 256px tiles are defined as being at nominal zoom, so\n    # delta = 0 for them.\n    delta = metatile_zoom_from_size(tile_size // 256)\n\n    zoom = nominal_zoom - delta\n\n    return list(coord_children_subrange(coord, zoom, zoom))", "response": "Returns a list of all the coords which are children of the input metatile at the given coord with the given zoom and size tile_size corrected for the given nominal zoom."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the tile sizes for a given zoom level.", "response": "def calculate_sizes_by_zoom(coord, metatile_zoom, cfg_tile_sizes, max_zoom):\n    \"\"\"\n    Returns a map of nominal zoom to the list of tile sizes to generate at that\n    zoom.\n\n    This is because we want to generate different metatile contents at\n    different zoom levels. At the most detailed zoom level, we want to generate\n    the smallest tiles possible, as this allows \"overzooming\" by simply\n    extracting the smaller tiles. At the minimum zoom, we want to get as close\n    as we can to zero nominal zoom by using any \"unused\" space in the metatile\n    for larger tile sizes that we're not generating.\n\n    For example, with 1x1 metatiles, the tile size is always 256px, and the\n    function will return {coord.zoom: [256]}\n\n    Note that max_zoom should be the maximum *coordinate* zoom, not nominal\n    zoom.\n    \"\"\"\n\n    from tilequeue.tile import metatile_zoom_from_size\n\n    tile_size_by_zoom = {}\n    nominal_zoom = coord.zoom + metatile_zoom\n\n    # check that the tile sizes are correct and within range.\n    for tile_size in cfg_tile_sizes:\n        assert tile_size >= 256\n        assert tile_size <= 256 * (1 << metatile_zoom)\n        assert _is_power_of_2(tile_size)\n\n    if coord.zoom >= max_zoom:\n        # all the tile_sizes down to 256 at the nominal zoom.\n        tile_sizes = []\n        tile_sizes.extend(cfg_tile_sizes)\n\n        lowest_tile_size = min(tile_sizes)\n        while lowest_tile_size > 256:\n            lowest_tile_size //= 2\n            tile_sizes.append(lowest_tile_size)\n\n        tile_size_by_zoom[nominal_zoom] = tile_sizes\n\n    elif coord.zoom <= 0:\n        # the tile_sizes, plus max(tile_sizes) size at nominal zooms decreasing\n        # down to 0 (or as close as we can get)\n        tile_size_by_zoom[nominal_zoom] = cfg_tile_sizes\n\n        max_tile_size = max(cfg_tile_sizes)\n        max_tile_zoom = metatile_zoom_from_size(max_tile_size // 256)\n        assert max_tile_zoom <= metatile_zoom\n        for delta in range(0, metatile_zoom - max_tile_zoom):\n            z = nominal_zoom - (delta + 1)\n            tile_size_by_zoom[z] = [max_tile_size]\n\n    else:\n        # the tile_sizes at nominal zoom only.\n        tile_size_by_zoom[nominal_zoom] = cfg_tile_sizes\n\n    return tile_size_by_zoom"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the cut coordinates at a given zoom.", "response": "def calculate_cut_coords_by_zoom(\n        coord, metatile_zoom, cfg_tile_sizes, max_zoom):\n    \"\"\"\n    Returns a map of nominal zoom to the list of cut coordinates at that\n    nominal zoom.\n\n    Note that max_zoom should be the maximum coordinate zoom, not nominal\n    zoom.\n    \"\"\"\n\n    tile_sizes_by_zoom = calculate_sizes_by_zoom(\n        coord, metatile_zoom, cfg_tile_sizes, max_zoom)\n\n    cut_coords_by_zoom = {}\n    for nominal_zoom, tile_sizes in tile_sizes_by_zoom.iteritems():\n        cut_coords = []\n        for tile_size in tile_sizes:\n            cut_coords.extend(metatile_children_with_size(\n                coord, metatile_zoom, nominal_zoom, tile_size))\n\n        cut_coords_by_zoom[nominal_zoom] = cut_coords\n\n    return cut_coords_by_zoom"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tiles_are_equal(tile_data_1, tile_data_2, fmt):\n\n    if fmt and fmt == zip_format:\n        return metatiles_are_equal(tile_data_1, tile_data_2)\n\n    else:\n        return tile_data_1 == tile_data_2", "response": "Returns True if the tile data is equal in tile_data_1 and tile_data_2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the tile data if the data has changed.", "response": "def write_tile_if_changed(store, tile_data, coord, format):\n    \"\"\"\n    Only write tile data if different from existing.\n\n    Try to read the tile data from the store first. If the existing\n    data matches, don't write. Returns whether the tile was written.\n    \"\"\"\n\n    existing_data = store.read_tile(coord, format)\n    if not existing_data or \\\n       not tiles_are_equal(existing_data, tile_data, format):\n        store.write_tile(tile_data, coord, format)\n        return True\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _override_cfg(container, yamlkeys, value):\n\n    key = yamlkeys[0]\n    rest = yamlkeys[1:]\n\n    if len(rest) == 0:\n        # no rest means we found the key to update.\n        container[key] = value\n\n    elif key in container:\n        # still need to find the leaf in the tree, so recurse.\n        _override_cfg(container[key], rest, value)\n\n    else:\n        # need to create a sub-tree down to the leaf to insert into.\n        subtree = {}\n        _override_cfg(subtree, rest, value)\n        container[key] = subtree", "response": "Override a hierarchical key in the config setting it to the value."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a full pyramid for a single coordinate.", "response": "def coord_pyramid(coord, zoom_start, zoom_stop):\n    \"\"\"\n    generate full pyramid for coord\n\n    Generate the full pyramid for a single coordinate. Note that zoom_stop is\n    exclusive.\n    \"\"\"\n    if zoom_start <= coord.zoom:\n        yield coord\n    for child_coord in coord_children_range(coord, zoom_stop):\n        if zoom_start <= child_coord.zoom:\n            yield child_coord"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef coord_pyramids(coords, zoom_start, zoom_stop):\n    for coord in coords:\n        for child in coord_pyramid(coord, zoom_start, zoom_stop):\n            yield child", "response": "Generate full pyramid for coords"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef emit_toi_stats(toi_set, peripherals):\n\n    count_by_zoom = defaultdict(int)\n    total = 0\n    for coord_int in toi_set:\n        coord = coord_unmarshall_int(coord_int)\n        count_by_zoom[coord.zoom] += 1\n        total += 1\n\n    peripherals.stats.gauge('tiles-of-interest.count', total)\n    for zoom, count in count_by_zoom.items():\n        peripherals.stats.gauge(\n            'tiles-of-interest.by-zoom.z{:02d}'.format(zoom),\n            count\n        )", "response": "Calculates new TOI stats and emits them via statsd."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads all tiles of interest into the top level hierarchy.", "response": "def tilequeue_load_tiles_of_interest(cfg, peripherals):\n    \"\"\"\n    Given a newline-delimited file containing tile coordinates in\n    `zoom/column/row` format, load those tiles into the tiles of interest.\n    \"\"\"\n    logger = make_logger(cfg, 'load_tiles_of_interest')\n\n    toi_filename = \"toi.txt\"\n    logger.info('Loading tiles of interest from %s ... ', toi_filename)\n\n    with open(toi_filename, 'r') as f:\n        new_toi = load_set_from_fp(f)\n\n    logger.info('Loading tiles of interest from %s ... done', toi_filename)\n    logger.info('Setting new TOI (with %s tiles) ... ', len(new_toi))\n\n    peripherals.toi.set_tiles_of_interest(new_toi)\n    emit_toi_stats(new_toi, peripherals)\n\n    logger.info('Setting new TOI (with %s tiles) ... done', len(new_toi))\n\n    logger.info('Loading tiles of interest ... done')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking which files exist on s3 but are not in toi.", "response": "def tilequeue_stuck_tiles(cfg, peripherals):\n    \"\"\"\n    Check which files exist on s3 but are not in toi.\n    \"\"\"\n    store = _make_store(cfg)\n    format = lookup_format_by_extension('zip')\n    layer = 'all'\n\n    assert peripherals.toi, 'Missing toi'\n    toi = peripherals.toi.fetch_tiles_of_interest()\n\n    for coord in store.list_tiles(format, layer):\n        coord_int = coord_marshall_int(coord)\n        if coord_int not in toi:\n            print serialize_coord(coord)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tilequeue_tile_status(cfg, peripherals, args):\n    logger = make_logger(cfg, 'tile_status')\n\n    # friendly warning to avoid confusion when this command outputs nothing\n    # at all when called with no positional arguments.\n    if not args.coords:\n        logger.warning('No coordinates given on the command line.')\n        return\n\n    # pre-load TOI to avoid having to do it for each coordinate\n    toi = None\n    if peripherals.toi:\n        toi = peripherals.toi.fetch_tiles_of_interest()\n\n    # TODO: make these configurable!\n    tile_format = lookup_format_by_extension('zip')\n    store = _make_store(cfg)\n\n    for coord_str in args.coords:\n        coord = deserialize_coord(coord_str)\n\n        # input checking! make sure that the coordinate is okay to use in\n        # the rest of the code.\n        if not coord:\n            logger.warning('Could not deserialize %r as coordinate', coord_str)\n            continue\n\n        if not coord_is_valid(coord):\n            logger.warning('Coordinate is not valid: %r (parsed from %r)',\n                           coord, coord_str)\n            continue\n\n        # now we think we probably have a valid coordinate. go look up\n        # whether it exists in various places.\n\n        logger.info(\"=== %s ===\", coord_str)\n        coord_int = coord_marshall_int(coord)\n\n        if peripherals.inflight_mgr:\n            is_inflight = peripherals.inflight_mgr.is_inflight(coord)\n            logger.info('inflight: %r', is_inflight)\n\n        if toi:\n            in_toi = coord_int in toi\n            logger.info('in TOI: %r' % (in_toi,))\n\n        data = store.read_tile(coord, tile_format)\n        logger.info('tile in store: %r', bool(data))", "response": "Report the status of the given tiles in the store queue and TOI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncommanding to take tile expiry path and enqueue for rawr tile generation", "response": "def tilequeue_rawr_enqueue(cfg, args):\n    \"\"\"command to take tile expiry path and enqueue for rawr tile generation\"\"\"\n    from tilequeue.stats import RawrTileEnqueueStatsHandler\n    from tilequeue.rawr import make_rawr_enqueuer_from_cfg\n\n    msg_marshall_yaml = cfg.yml.get('message-marshall')\n    assert msg_marshall_yaml, 'Missing message-marshall config'\n    msg_marshaller = make_message_marshaller(msg_marshall_yaml)\n\n    logger = make_logger(cfg, 'rawr_enqueue')\n    stats = make_statsd_client_from_cfg(cfg)\n    stats_handler = RawrTileEnqueueStatsHandler(stats)\n    rawr_enqueuer = make_rawr_enqueuer_from_cfg(\n        cfg, logger, stats_handler, msg_marshaller)\n\n    with open(args.expiry_path) as fh:\n        coords = create_coords_generator_from_tiles_file(fh)\n        rawr_enqueuer(coords)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tilequeue_rawr_setup(cfg):\n    rawr_yaml = cfg.yml.get('rawr')\n    assert rawr_yaml is not None, 'Missing rawr configuration in yaml'\n\n    rawr_postgresql_yaml = rawr_yaml.get('postgresql')\n    assert rawr_postgresql_yaml, 'Missing rawr postgresql config'\n\n    from raw_tiles.formatter.msgpack import Msgpack\n    from raw_tiles.gen import RawrGenerator\n    from raw_tiles.source.conn import ConnectionContextManager\n    from raw_tiles.source import parse_sources\n    from raw_tiles.source import DEFAULT_SOURCES as DEFAULT_RAWR_SOURCES\n    from tilequeue.rawr import RawrS3Sink\n    from tilequeue.rawr import RawrStoreSink\n    import boto3\n    # pass through the postgresql yaml config directly\n    conn_ctx = ConnectionContextManager(rawr_postgresql_yaml)\n\n    rawr_source_list = rawr_yaml.get('sources', DEFAULT_RAWR_SOURCES)\n    assert isinstance(rawr_source_list, list), \\\n        'RAWR source list should be a list'\n    assert len(rawr_source_list) > 0, \\\n        'RAWR source list should be non-empty'\n\n    rawr_store = rawr_yaml.get('store')\n    if rawr_store:\n        store = make_store(\n            rawr_store, credentials=cfg.subtree('aws credentials'))\n        rawr_sink = RawrStoreSink(store)\n\n    else:\n        rawr_sink_yaml = rawr_yaml.get('sink')\n        assert rawr_sink_yaml, 'Missing rawr sink config'\n        sink_type = rawr_sink_yaml.get('type')\n        assert sink_type, 'Missing rawr sink type'\n        if sink_type == 's3':\n            s3_cfg = rawr_sink_yaml.get('s3')\n            assert s3_cfg, 'Missing s3 config'\n            bucket = s3_cfg.get('bucket')\n            assert bucket, 'Missing rawr sink bucket'\n            sink_region = s3_cfg.get('region')\n            assert sink_region, 'Missing rawr sink region'\n            prefix = s3_cfg.get('prefix')\n            assert prefix, 'Missing rawr sink prefix'\n            extension = s3_cfg.get('extension')\n            assert extension, 'Missing rawr sink extension'\n            tags = s3_cfg.get('tags')\n            from tilequeue.store import make_s3_tile_key_generator\n            tile_key_gen = make_s3_tile_key_generator(s3_cfg)\n\n            s3_client = boto3.client('s3', region_name=sink_region)\n            rawr_sink = RawrS3Sink(\n                s3_client, bucket, prefix, extension, tile_key_gen, tags)\n        elif sink_type == 'none':\n            from tilequeue.rawr import RawrNullSink\n            rawr_sink = RawrNullSink()\n        else:\n            assert 0, 'Unknown rawr sink type %s' % sink_type\n\n    rawr_source = parse_sources(rawr_source_list)\n    rawr_formatter = Msgpack()\n    rawr_gen = RawrGenerator(rawr_source, rawr_formatter, rawr_sink)\n\n    return rawr_gen, conn_ctx", "response": "command to read from rawr queue and generate rawr tiles"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tilequeue_rawr_seed_toi(cfg, peripherals):\n    tiles_of_interest = peripherals.toi.fetch_tiles_of_interest()\n    coords = map(coord_unmarshall_int, tiles_of_interest)\n    _tilequeue_rawr_seed(cfg, peripherals, coords)", "response": "command to read the toi and enqueue the corresponding rawr tiles"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef tilequeue_rawr_seed_all(cfg, peripherals):\n\n    rawr_yaml = cfg.yml.get('rawr')\n    assert rawr_yaml is not None, 'Missing rawr configuration in yaml'\n\n    group_by_zoom = rawr_yaml.get('group-zoom')\n    assert group_by_zoom is not None, 'Missing group-zoom rawr config'\n\n    max_coord = 2 ** group_by_zoom\n\n    # creating the list of all coordinates here might be a lot of memory, but\n    # if we handle the TOI okay then we should be okay with z10. if the group\n    # by zoom is much larger, then it might start running into problems.\n    coords = []\n    for x in xrange(0, max_coord):\n        for y in xrange(0, max_coord):\n            coords.append(Coordinate(zoom=group_by_zoom, column=x, row=y))\n\n    _tilequeue_rawr_seed(cfg, peripherals, coords)", "response": "command to enqueue all the tiles at the group - by zoom"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_single_layer(out, features, zoom):\n    precision = precision_for_zoom(zoom)\n    fs = create_layer_feature_collection(features, precision)\n    json.dump(fs, out)", "response": "Encode a list of features into a GeoJSON stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a list of features_by_layer into a JSON file.", "response": "def encode_multiple_layers(out, features_by_layer, zoom):\n    \"\"\"\n    features_by_layer should be a dict: layer_name -> feature tuples\n    \"\"\"\n    precision = precision_for_zoom(zoom)\n    geojson = {}\n    for layer_name, features in features_by_layer.items():\n        fs = create_layer_feature_collection(features, precision)\n        geojson[layer_name] = fs\n    json.dump(geojson, out)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_arc_indexes(geometry, merged_arcs, old_arcs):\n    if geometry['type'] in ('Point', 'MultiPoint'):\n        return\n\n    elif geometry['type'] == 'LineString':\n        for arc_index, old_arc in enumerate(geometry['arcs']):\n            geometry['arcs'][arc_index] = len(merged_arcs)\n            merged_arcs.append(old_arcs[old_arc])\n\n    elif geometry['type'] == 'Polygon':\n        for ring in geometry['arcs']:\n            for arc_index, old_arc in enumerate(ring):\n                ring[arc_index] = len(merged_arcs)\n                merged_arcs.append(old_arcs[old_arc])\n\n    elif geometry['type'] == 'MultiLineString':\n        for part in geometry['arcs']:\n            for arc_index, old_arc in enumerate(part):\n                part[arc_index] = len(merged_arcs)\n                merged_arcs.append(old_arcs[old_arc])\n\n    elif geometry['type'] == 'MultiPolygon':\n        for part in geometry['arcs']:\n            for ring in part:\n                for arc_index, old_arc in enumerate(ring):\n                    ring[arc_index] = len(merged_arcs)\n                    merged_arcs.append(old_arcs[old_arc])\n\n    else:\n        raise NotImplementedError(\"Can't do %s geometries\" % geometry['type'])", "response": "Updates arc indexes and add arcs to merged_arcs along the way."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a TopoJSON transform dictionary and a point - transforming function.", "response": "def get_transform(bounds, size=4096):\n    \"\"\" Return a TopoJSON transform dictionary and a point-transforming function.\n\n        Size is the tile size in pixels and sets the implicit output\n        resolution.\n    \"\"\"\n    tx, ty = bounds[0], bounds[1]\n    sx, sy = (bounds[2] - bounds[0]) / size, (bounds[3] - bounds[1]) / size\n\n    def forward(lon, lat):\n        \"\"\" Transform a longitude and latitude to TopoJSON integer space.\n        \"\"\"\n        return int(round((lon - tx) / sx)), int(round((lat - ty) / sy))\n\n    return dict(translate=(tx, ty), scale=(sx, sy)), forward"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a list of features into a TopoJSON file.", "response": "def encode(file, features_by_layer, bounds, size=4096):\n    \"\"\" Encode a dict of layername: (shape, props, id) features into a\n        TopoJSON stream.\n\n        If no id is available, pass in None\n\n        Geometries in the features list are assumed to be unprojected\n        lon, lats.  Bounds are given in geographic coordinates as\n        (xmin, ymin, xmax, ymax).\n\n        Size is the number of integer coordinates which span the extent\n        of the tile.\n    \"\"\"\n    transform, forward = get_transform(bounds, size=size)\n    arcs = []\n\n    geometries_by_layer = {}\n\n    for layer, features in features_by_layer.iteritems():\n        geometries = []\n        for shape, props, fid in features:\n            if shape.type == 'GeometryCollection':\n                continue\n\n            geometry = dict(properties=props)\n\n            if fid is not None:\n                geometry['id'] = fid\n\n            elif shape.type == 'Point':\n                geometry.update(dict(\n                    type='Point',\n                    coordinates=forward(shape.x, shape.y)))\n\n            elif shape.type == 'LineString':\n                geometry.update(dict(type='LineString', arcs=[len(arcs)]))\n                arcs.append(diff_encode(shape, forward))\n\n            elif shape.type == 'Polygon':\n                geometry.update(dict(type='Polygon', arcs=[]))\n\n                rings = [shape.exterior] + list(shape.interiors)\n\n                for ring in rings:\n                    geometry['arcs'].append([len(arcs)])\n                    arcs.append(diff_encode(ring, forward))\n\n            elif shape.type == 'MultiPoint':\n                geometry.update(dict(type='MultiPoint', coordinates=[]))\n\n                for point in shape.geoms:\n                    geometry['coordinates'].append(forward(point.x, point.y))\n\n            elif shape.type == 'MultiLineString':\n                geometry.update(dict(type='MultiLineString', arcs=[]))\n\n                for line in shape.geoms:\n                    geometry['arcs'].append([len(arcs)])\n                    arcs.append(diff_encode(line, forward))\n\n            elif shape.type == 'MultiPolygon':\n                geometry.update(dict(type='MultiPolygon', arcs=[]))\n\n                for polygon in shape.geoms:\n                    rings = [polygon.exterior] + list(polygon.interiors)\n                    polygon_arcs = []\n\n                    for ring in rings:\n                        polygon_arcs.append([len(arcs)])\n                        arcs.append(diff_encode(ring, forward))\n\n                    geometry['arcs'].append(polygon_arcs)\n\n            else:\n                raise NotImplementedError(\"Can't do %s geometries\" %\n                                          shape.type)\n\n            geometries.append(geometry)\n\n        geometries_by_layer[layer] = dict(\n            type='GeometryCollection',\n            geometries=geometries,\n        )\n\n    result = dict(\n        type='Topology',\n        transform=transform,\n        objects=geometries_by_layer,\n        arcs=arcs,\n    )\n\n    json.dump(result, file)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter out any bounding box that overlaps with the bounding box.", "response": "def jinja_filter_bbox_overlaps(bounds, geometry_col_name, srid=3857):\n    \"\"\"\n    Check whether the boundary of the geometry intersects with the bounding\n    box.\n\n    Note that the usual meaning of \"overlaps\" in GIS terminology is that the\n    boundaries of the box and polygon intersect, but not the interiors. This\n    means that if the box or polygon is completely within the other, then\n    st_overlaps will be false.\n\n    However, that's not what we want. This is used for boundary testing, and\n    while we don't want to pull out a whole country boundary if the bounding\n    box is fully within it, we _do_ want to if the country boundary is within\n    the bounding box.\n\n    Therefore, this test has an extra \"or st_contains\" test to also pull in any\n    boundaries which are completely within the bounding box.\n    \"\"\"\n\n    min_point = 'ST_MakePoint(%.12f, %.12f)' % (bounds[0], bounds[1])\n    max_point = 'ST_MakePoint(%.12f, %.12f)' % (bounds[2], bounds[3])\n    bbox_no_srid = 'ST_MakeBox2D(%s, %s)' % (min_point, max_point)\n    bbox = 'ST_SetSrid(%s, %d)' % (bbox_no_srid, srid)\n    bbox_filter = \\\n        '((%(col)s && %(bbox)s) AND (' \\\n        '  st_overlaps(%(col)s, %(bbox)s) OR' \\\n        '  st_contains(%(bbox)s, %(col)s)' \\\n        '))' \\\n        % dict(col=geometry_col_name, bbox=bbox)\n    return bbox_filter"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a DataFetcher object which is a function which returns a list of rows.", "response": "def make_db_data_fetcher(postgresql_conn_info, template_path, reload_templates,\n                         query_cfg, io_pool):\n    \"\"\"\n    Returns an object which is callable with the zoom and unpadded bounds and\n    which returns a list of rows.\n    \"\"\"\n\n    sources = parse_source_data(query_cfg)\n    queries_generator = make_queries_generator(\n        sources, template_path, reload_templates)\n    return DataFetcher(\n        postgresql_conn_info, queries_generator, io_pool)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_multi_metatile(parent, tiles, date_time=None):\n\n    assert parent is not None, \\\n        \"Parent tile must be provided and not None to make a metatile.\"\n\n    if len(tiles) == 0:\n        return []\n\n    if date_time is None:\n        date_time = gmtime()[0:6]\n\n    layer = tiles[0]['layer']\n\n    buf = StringIO.StringIO()\n    with zipfile.ZipFile(buf, mode='w') as z:\n        for tile in tiles:\n            assert tile['layer'] == layer\n\n            coord = tile['coord']\n\n            # change in zoom level from parent to coord. since parent should\n            # be a parent, its zoom should always be equal or smaller to that\n            # of coord.\n            delta_z = coord.zoom - parent.zoom\n            assert delta_z >= 0, \"Coordinates must be descendents of parent\"\n\n            # change in row/col coordinates are relative to the upper left\n            # coordinate at that zoom. both should be positive.\n            delta_row = coord.row - (int(parent.row) << delta_z)\n            delta_column = coord.column - (int(parent.column) << delta_z)\n            assert delta_row >= 0, \\\n                \"Coordinates must be contained by their parent, but \" + \\\n                \"row is not.\"\n            assert delta_column >= 0, \\\n                \"Coordinates must be contained by their parent, but \" + \\\n                \"column is not.\"\n\n            tile_name = '%d/%d/%d.%s' % \\\n                (delta_z, delta_column, delta_row, tile['format'].extension)\n            tile_data = tile['tile']\n            info = zipfile.ZipInfo(tile_name, date_time)\n            z.writestr(info, tile_data, zipfile.ZIP_DEFLATED)\n\n    return [dict(tile=buf.getvalue(), format=zip_format, coord=parent,\n                 layer=layer)]", "response": "Makes a multi - metatile file containing a list of tiles with the same layer and coordinates relative to the given parent tile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef common_parent(a, b):\n\n    if a.zoom < b.zoom:\n        b = b.zoomTo(a.zoom).container()\n\n    elif a.zoom > b.zoom:\n        a = a.zoomTo(b.zoom).container()\n\n    while a.row != b.row or a.column != b.column:\n        a = a.zoomBy(-1).container()\n        b = b.zoomBy(-1).container()\n\n    # by this point a == b.\n    return a", "response": "Find the common parent tile of both a and b."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parent_tile(tiles):\n    parent = None\n    for t in tiles:\n        if parent is None:\n            parent = t\n\n        else:\n            parent = common_parent(parent, t)\n\n    return parent", "response": "Find the common parent tile for a sequence of tiles."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a list of metatiles out of the given size and tiles.", "response": "def make_metatiles(size, tiles, date_time=None):\n    \"\"\"\n    Group by layers, and make metatiles out of all the tiles which share those\n    properties relative to the \"top level\" tile which is parent of them all.\n    Provide a 6-tuple date_time to set the timestamp on each tile within the\n    metatile, or leave it as None to use the current time.\n    \"\"\"\n\n    groups = defaultdict(list)\n    for tile in tiles:\n        key = tile['layer']\n        groups[key].append(tile)\n\n    metatiles = []\n    for group in groups.itervalues():\n        parent = _parent_tile(t['coord'] for t in group)\n        metatiles.extend(make_multi_metatile(parent, group, date_time))\n\n    return metatiles"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract the metatile from the given file - like object io and format fmt.", "response": "def extract_metatile(io, fmt, offset=None):\n    \"\"\"\n    Extract the tile at the given offset (defaults to 0/0/0) and format from\n    the metatile in the file-like object io.\n    \"\"\"\n\n    ext = fmt.extension\n    if offset is None:\n        tile_name = '0/0/0.%s' % ext\n    else:\n        tile_name = '%d/%d/%d.%s' % (offset.zoom, offset.column, offset.row,\n                                     ext)\n\n    with zipfile.ZipFile(io, mode='r') as zf:\n        if tile_name in zf.namelist():\n            return zf.read(tile_name)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the contents of two open zip files are byte - wise identical to the ones in the zip.", "response": "def _metatile_contents_equal(zip_1, zip_2):\n    \"\"\"\n    Given two open zip files as arguments, this returns True if the zips\n    both contain the same set of files, having the same names, and each\n    file within the zip is byte-wise identical to the one with the same\n    name in the other zip.\n    \"\"\"\n\n    names_1 = set(zip_1.namelist())\n    names_2 = set(zip_2.namelist())\n\n    if names_1 != names_2:\n        return False\n\n    for n in names_1:\n        bytes_1 = zip_1.read(n)\n        bytes_2 = zip_2.read(n)\n\n        if bytes_1 != bytes_2:\n            return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if the two tiles are both zipped metatiles and contain the same set of files with the same contents.", "response": "def metatiles_are_equal(tile_data_1, tile_data_2):\n    \"\"\"\n    Return True if the two tiles are both zipped metatiles and contain the\n    same set of files with the same contents. This ignores the timestamp of\n    the individual files in the zip files, as well as their order or any\n    other metadata.\n    \"\"\"\n\n    try:\n        buf_1 = StringIO.StringIO(tile_data_1)\n        buf_2 = StringIO.StringIO(tile_data_2)\n\n        with zipfile.ZipFile(buf_1, mode='r') as zip_1:\n            with zipfile.ZipFile(buf_2, mode='r') as zip_2:\n                return _metatile_contents_equal(zip_1, zip_2)\n\n    except (StandardError, zipfile.BadZipFile, zipfile.LargeZipFile):\n        # errors, such as files not being proper zip files, or missing\n        # some attributes or contents that we expect, are treated as not\n        # equal.\n        pass\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_feature_layers_to_dict(feature_layers):\n    features_by_layer = {}\n    for feature_layer in feature_layers:\n        layer_name = feature_layer['name']\n        features = feature_layer['features']\n        features_by_layer[layer_name] = features\n    return features_by_layer", "response": "takes a list of feature_layer objects and converts it to a dict containing the key of the layer name and the values of the feature_layer objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef group(self, coords):\n\n        groups = []\n        for i in range(len(self.zoom_range_items)):\n            groups.append([])\n\n        # first group the coordinates based on their queue\n        for coord in coords:\n            for i, zri in enumerate(self.zoom_range_items):\n                toi_match = zri.in_toi is None or \\\n                            (coord in self.toi_set) == zri.in_toi\n                if zri.start <= coord.zoom < zri.end and toi_match:\n                    groups[i].append(coord)\n                    break\n\n        # now, we need to just verify that for each particular group,\n        # should they be further grouped, eg by a particular zoom 10\n        # tile\n        for i, zri in enumerate(self.zoom_range_items):\n            group = groups[i]\n            if not group:\n                continue\n            if zri.group_by_zoom is None:\n                for coord in group:\n                    yield CoordGroup([coord], zri.queue_id)\n            else:\n                by_parent_coords = defaultdict(list)\n                for coord in group:\n                    if coord.zoom >= zri.group_by_zoom:\n                        group_coord = coord.zoomTo(zri.group_by_zoom)\n                        group_key = coord_marshall_int(group_coord)\n                        by_parent_coords[group_key].append(coord)\n                    else:\n                        # this means that a coordinate belonged to a\n                        # particular queue but the zoom was lower than\n                        # the group by zoom\n                        # this probably shouldn't happen\n                        # should it be an assert instead?\n                        yield CoordGroup([coord], zri.queue_id)\n\n                for group_key, coords in by_parent_coords.iteritems():\n                    yield CoordGroup(coords, zri.queue_id)", "response": "return a list of CoordGroups that can be used to send to queues"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the common parent for coords.", "response": "def common_parent(coords, parent_zoom):\n    \"\"\"\n    Return the common parent for coords\n\n    Also check that all coords do indeed share the same parent coordinate.\n    \"\"\"\n    parent = None\n    for coord in coords:\n        assert parent_zoom <= coord.zoom\n        coord_parent = coord.zoomTo(parent_zoom).container()\n        if parent is None:\n            parent = coord_parent\n        else:\n            assert parent == coord_parent\n    assert parent is not None, 'No coords?'\n    return parent"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert ModestMaps. Core. Coordinate object to raw_tiles. tile. Tile object.", "response": "def convert_coord_object(coord):\n    \"\"\"Convert ModestMaps.Core.Coordinate -> raw_tiles.tile.Tile\"\"\"\n    assert isinstance(coord, Coordinate)\n    coord = coord.container()\n    return Tile(int(coord.zoom), int(coord.column), int(coord.row))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts rawr_tiles. tile. Tile to ModestMaps. Core. Coordinate.", "response": "def unconvert_coord_object(tile):\n    \"\"\"Convert rawr_tiles.tile.Tile -> ModestMaps.Core.Coordinate\"\"\"\n    assert isinstance(tile, Tile)\n    return Coordinate(zoom=tile.z, column=tile.x, row=tile.y)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_rawr_zip_payload(rawr_tile, date_time=None):\n    if date_time is None:\n        date_time = gmtime()[0:6]\n\n    buf = StringIO()\n    with zipfile.ZipFile(buf, mode='w') as z:\n        for fmt_data in rawr_tile.all_formatted_data:\n            zip_info = zipfile.ZipInfo(fmt_data.name, date_time)\n            z.writestr(zip_info, fmt_data.data, zipfile.ZIP_DEFLATED)\n    return buf.getvalue()", "response": "make a zip file from the rawr tile formatted data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunpacks a zipfile and turn it into a callable", "response": "def unpack_rawr_zip_payload(table_sources, payload):\n    \"\"\"unpack a zipfile and turn it into a callable \"tables\" object.\"\"\"\n    # the io we get from S3 is streaming, so we can't seek on it, but zipfile\n    # seems to require that. so we buffer it all in memory. RAWR tiles are\n    # generally up to around 100MB in size, which should be safe to store in\n    # RAM.\n    from tilequeue.query.common import Table\n    from io import BytesIO\n\n    zfh = zipfile.ZipFile(BytesIO(payload), 'r')\n\n    def get_table(table_name):\n        # need to extract the whole compressed file from zip reader, as it\n        # doesn't support .tell() on the filelike, which gzip requires.\n        data = zfh.open(table_name, 'r').read()\n        unpacker = Unpacker(file_like=BytesIO(data))\n        source = table_sources[table_name]\n        return Table(source, unpacker)\n\n    return get_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, payloads, logger, num_tries=5):\n        from time import sleep\n\n        backoff_interval = 1\n        backoff_factor = 2\n\n        for try_counter in xrange(0, num_tries):\n            failed_messages = self.send_without_retry(payloads)\n\n            # success!\n            if not failed_messages:\n                payloads = []\n                break\n\n            # output some information about the failures for debugging\n            # purposes. we expect failures to be quite rare, so we can be\n            # pretty verbose.\n            if logger:\n                for msg in failed_messages:\n                    logger.warning(\"Failed to send message on try %d: Id=%r, \"\n                                   \"SenderFault=%r, Code=%r, Message=%r\" %\n                                   (try_counter, msg['Id'],\n                                    msg.get('SenderFault'), msg.get('Code'),\n                                    msg.get('Message')))\n\n            # wait a little while, in case the problem is that we're talking\n            # too fast.\n            sleep(backoff_interval)\n            backoff_interval *= backoff_factor\n\n            # filter out the failed payloads for retry\n            retry_payloads = []\n            for msg in failed_messages:\n                i = int(msg['Id'])\n                retry_payloads.append(payloads[i])\n            payloads = retry_payloads\n\n        if payloads:\n            raise Exception('Messages failed to send to sqs after %d '\n                            'retries: %s' % (num_tries, len(payloads)))", "response": "Send a list of payloads to the SQS queue."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single message from the queue", "response": "def read(self):\n        \"\"\"read a single message from the queue\"\"\"\n        resp = self.sqs_client.receive_message(\n            QueueUrl=self.queue_url,\n            MaxNumberOfMessages=1,\n            AttributeNames=('SentTimestamp',),\n            WaitTimeSeconds=self.recv_wait_time_seconds,\n        )\n        if resp['ResponseMetadata']['HTTPStatusCode'] != 200:\n            raise Exception('Invalid status code from sqs: %s' %\n                            resp['ResponseMetadata']['HTTPStatusCode'])\n        msgs = resp.get('Messages')\n        if not msgs:\n            return None\n        assert len(msgs) == 1\n        msg = msgs[0]\n        payload = msg['Body']\n        handle = msg['ReceiptHandle']\n        timestamp = msg['Attributes']['SentTimestamp']\n        metadata = dict(timestamp=timestamp)\n        msg_handle = MessageHandle(handle, payload, metadata)\n        return msg_handle"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nacknowledging completion of a message", "response": "def done(self, msg_handle):\n        \"\"\"acknowledge completion of message\"\"\"\n        self.sqs_client.delete_message(\n            QueueUrl=self.queue_url,\n            ReceiptHandle=msg_handle.handle,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nacknowledging a single coordinate", "response": "def _ack_coord_handle(\n        coord, coord_handle, queue_mapper, msg_tracker, timing_state,\n        tile_proc_logger, stats_handler):\n    \"\"\"share code for acknowledging a coordinate\"\"\"\n\n    # returns tuple of (handle, error), either of which can be None\n\n    track_result = msg_tracker.done(coord_handle)\n    queue_handle = track_result.queue_handle\n    if not queue_handle:\n        return None, None\n\n    tile_queue = queue_mapper.get_queue(queue_handle.queue_id)\n    assert tile_queue, \\\n        'Missing tile_queue: %s' % queue_handle.queue_id\n\n    parent_tile = None\n    if track_result.all_done:\n        parent_tile = track_result.parent_tile\n\n        try:\n            tile_queue.job_done(queue_handle.handle)\n        except Exception as e:\n            stacktrace = format_stacktrace_one_line()\n            tile_proc_logger.error_job_done(\n                'tile_queue.job_done', e, stacktrace,\n                coord, parent_tile,\n            )\n            return queue_handle, e\n\n        if parent_tile is not None:\n            # we completed a tile pyramid and should log appropriately\n\n            start_time = timing_state['start']\n            stop_time = convert_seconds_to_millis(time.time())\n            tile_proc_logger.log_processed_pyramid(\n                parent_tile, start_time, stop_time)\n            stats_handler.processed_pyramid(\n                parent_tile, start_time, stop_time)\n    else:\n        try:\n            tile_queue.job_progress(queue_handle.handle)\n        except Exception as e:\n            stacktrace = format_stacktrace_one_line()\n            err_details = {\"queue_handle\": queue_handle.handle}\n            if isinstance(e, JobProgressException):\n                err_details = e.err_details\n            tile_proc_logger.error_job_progress(\n                'tile_queue.job_progress', e, stacktrace,\n                coord, parent_tile, err_details,\n            )\n            return queue_handle, e\n\n    return queue_handle, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _snapping_round(num, eps, resolution):\n\n    rounded = round(num)\n    delta = abs(num - rounded)\n    if delta < eps:\n        return int(rounded)\n    else:\n        return int(resolution(num))", "response": "Return num snapped to within eps of an integer or int ( resolution ( num )."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of LineStrings which make up the shape.", "response": "def _explode_lines(shape):\n    \"\"\"\n    Return a list of LineStrings which make up the shape.\n    \"\"\"\n\n    if shape.geom_type == 'LineString':\n        return [shape]\n\n    elif shape.geom_type == 'MultiLineString':\n        return shape.geoms\n\n    elif shape.geom_type == 'GeometryCollection':\n        lines = []\n        for geom in shape.geoms:\n            lines.extend(_explode_lines(geom))\n        return lines\n\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _lines_only(shape):\n\n    lines = _explode_lines(shape)\n    if len(lines) == 1:\n        return lines[0]\n    else:\n        return MultiLineString(lines)", "response": "Extract the lines from any geometry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a geometry string and sets the properties of the object to the appropriate attributes.", "response": "def parseGeometry(self, geometry):\n        \n\n        \"\"\"\n        A factory method for creating objects of the correct OpenGIS type.\n        \"\"\"\n\n        self.coordinates = []\n        self.index = []\n        self.position = 0\n        self.lastX = 0 \n        self.lastY = 0\n        self.isPoly = False\n        self.isPoint = True;\n        self.dropped = 0;\n        self.first = True\n        # Used for exception strings\n        self._current_string = geometry\n        \n        reader = _ExtendedUnPacker(geometry)\n                \n        # Start the parsing\n        self._dispatchNextType(reader)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dispatchNextType(self,reader):\n        \n        # Need to check endianess here!\n        endianness = reader.unpack_byte()\n        if endianness == 0:\n            reader.setEndianness('XDR')\n        elif endianness == 1:\n            reader.setEndianness('NDR')\n        else:\n            raise ExceptionWKBParser(\"Invalid endianness in WKB format.\\n\"\\\n                                     \"The parser can only cope with XDR/big endian WKB format.\\n\"\\\n                                     \"To force the WKB format to be in XDR use AsBinary(<fieldname>,'XDR'\")\n            \n        \n        geotype = reader.unpack_uint32() \n\n        mask = geotype & 0x80000000 # This is used to mask of the dimension flag.\n\n        srid = geotype & 0x20000000\n        # ignore srid ...\n        if srid != 0:\n            reader.unpack_uint32()\n\n        dimensions = 2\n        if mask == 0:\n            dimensions = 2\n        else:\n            dimensions = 3\n       \n        geotype = geotype & 0x1FFFFFFF\n        # Despatch to a method on the type id.\n        if self._typemap.has_key(geotype):\n            self._typemap[geotype](reader, dimensions)\n        else:\n            raise ExceptionWKBParser('Error type to dispatch with geotype = %s \\n'\\\n                                     'Invalid geometry in WKB string: %s' % (str(geotype),\n                                                                             str(self._current_string),))", "response": "Dispatches the next type of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_buffered_bounds(\n        format, bounds, meters_per_pixel_dim, layer_name, geometry_type,\n        buffer_cfg):\n    \"\"\"\n    Calculate the buffered bounds per format per layer based on config.\n    \"\"\"\n\n    if not buffer_cfg:\n        return bounds\n\n    format_buffer_cfg = buffer_cfg.get(format.extension)\n    if format_buffer_cfg is None:\n        return bounds\n\n    geometry_type = normalize_geometry_type(geometry_type)\n\n    per_layer_cfg = format_buffer_cfg.get('layer', {}).get(layer_name)\n    if per_layer_cfg is not None:\n        layer_geom_pixels = per_layer_cfg.get(geometry_type)\n        if layer_geom_pixels is not None:\n            assert isinstance(layer_geom_pixels, Number)\n            result = bounds_buffer(\n                bounds, meters_per_pixel_dim * layer_geom_pixels)\n            return result\n\n    by_geometry_pixels = format_buffer_cfg.get('geometry', {}).get(\n        geometry_type)\n    if by_geometry_pixels is not None:\n        assert isinstance(by_geometry_pixels, Number)\n        result = bounds_buffer(\n            bounds, meters_per_pixel_dim * by_geometry_pixels)\n        return result\n\n    return bounds", "response": "Calculate the buffered bounds per format per layer based on config."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the parts of the multipolygon which overlap the tile_bounds and clip_bounds.", "response": "def _intersect_multipolygon(shape, tile_bounds, clip_bounds):\n    \"\"\"\n    Return the parts of the MultiPolygon shape which overlap the tile_bounds,\n    each clipped to the clip_bounds. This can be used to extract only the\n    parts of a multipolygon which are actually visible in the tile, while\n    keeping those parts which extend beyond the tile clipped to avoid huge\n    polygons.\n    \"\"\"\n\n    polys = []\n    for poly in shape.geoms:\n        if tile_bounds.intersects(poly):\n            if not clip_bounds.contains(poly):\n                poly = clip_bounds.intersection(poly)\n\n            # the intersection operation can make the resulting polygon\n            # invalid. including it in a MultiPolygon would make that\n            # invalid too. instead, we skip it, and hope it wasn't too\n            # important.\n            if not poly.is_valid:\n                continue\n\n            if poly.type == 'Polygon':\n                polys.append(poly)\n            elif poly.type == 'MultiPolygon':\n                polys.extend(poly.geoms)\n\n    return geometry.MultiPolygon(polys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _clip_shape(shape, buffer_padded_bounds, is_clipped, clip_factor):\n\n    shape_buf_bounds = geometry.box(*buffer_padded_bounds)\n\n    if not shape_buf_bounds.intersects(shape):\n        return None\n\n    if is_clipped:\n        # now we know that we should include the geometry, but\n        # if the geometry should be clipped, we'll clip to the\n        # layer-specific padded bounds\n        layer_padded_bounds = calculate_padded_bounds(\n            clip_factor, buffer_padded_bounds)\n\n        if shape.type == 'MultiPolygon':\n            shape = _intersect_multipolygon(\n                shape, shape_buf_bounds, layer_padded_bounds)\n        else:\n            try:\n                shape = shape.intersection(layer_padded_bounds)\n            except shapely.errors.TopologicalError:\n                return None\n\n    return shape", "response": "Clip the shape to a clip_factor expansion of buffer_padded_bounds and return the original shape or None."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef now(self):\n        if self._now is None:\n            # Compute the current time only once per instance\n            self._now = datetime.utcnow()\n        return self._now", "response": "Return the current time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate and clean the command s arguments.", "response": "def _clean_required_args(self, url, redirect_uri, client_type):\n        \"\"\"\n        Validate and clean the command's arguments.\n\n        Arguments:\n            url (str): Client's application URL.\n            redirect_uri (str): Client application's OAuth2 callback URI.\n            client_type (str): Client's type, indicating whether the Client application\n                is capable of maintaining the confidentiality of its credentials (e.g., running on a\n                secure server) or is incapable of doing so (e.g., running in a browser).\n\n        Raises:\n            CommandError, if the URLs provided are invalid, or if the client type provided is invalid.\n        \"\"\"\n        # Validate URLs\n        for url_to_validate in (url, redirect_uri):\n            try:\n                URLValidator()(url_to_validate)\n            except ValidationError:\n                raise CommandError(\"URLs provided are invalid. Please provide valid application and redirect URLs.\")\n\n        # Validate and map client type to the appropriate django-oauth2-provider constant\n        client_type = client_type.lower()\n        client_type = {\n            'confidential': CONFIDENTIAL,\n            'public': PUBLIC\n        }.get(client_type)\n\n        if client_type is None:\n            raise CommandError(\"Client type provided is invalid. Please use one of 'confidential' or 'public'.\")\n\n        self.fields = {  # pylint: disable=attribute-defined-outside-init\n            'url': url,\n            'redirect_uri': redirect_uri,\n            'client_type': client_type,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_options(self, options):\n        for key in ('username', 'client_name', 'client_id', 'client_secret', 'trusted', 'logout_uri'):\n            value = options.get(key)\n            if value is not None:\n                self.fields[key] = value\n\n        username = self.fields.pop('username', None)\n        if username is not None:\n            try:\n                user_model = get_user_model()\n                self.fields['user'] = user_model.objects.get(username=username)\n            except user_model.DoesNotExist:\n                raise CommandError(\"User matching the provided username does not exist.\")\n\n        # The keyword argument 'name' conflicts with that of `call_command()`. We instead\n        # use 'client_name' up to this point, then swap it out for the expected field, 'name'.\n        client_name = self.fields.pop('client_name', None)\n        if client_name is not None:\n            self.fields['name'] = client_name\n\n        logout_uri = self.fields.get('logout_uri')\n\n        if logout_uri:\n            try:\n                URLValidator()(logout_uri)\n            except ValidationError:\n                raise CommandError(\"The logout_uri is invalid.\")", "response": "Parse the command s options."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef id_token(access_token, nonce=None, claims_request=None):\n\n    handlers = HANDLERS['id_token']\n\n    # Select only the relevant section of the claims request.\n    claims_request_section = claims_request.get('id_token', {}) if claims_request else {}\n\n    scope_request = provider.scope.to_names(access_token.scope)\n\n    if nonce:\n        claims_request_section.update({'nonce': {'value': nonce}})\n\n    scopes, claims = collect(\n        handlers,\n        access_token,\n        scope_request=scope_request,\n        claims_request=claims_request_section,\n    )\n\n    return IDToken(access_token, scopes, claims)", "response": "Returns a new IDToken object that can be used to generate a new IDToken."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a IDToken object that represents a UserInfo response for the given access token.", "response": "def userinfo(access_token, scope_request=None, claims_request=None):\n    \"\"\"\n    Returns data required for an OpenID Connect UserInfo response, according to:\n\n    http://openid.net/specs/openid-connect-basic-1_0.html#UserInfoResponse\n\n    Supports scope and claims request parameter as described in:\n\n    - http://openid.net/specs/openid-connect-core-1_0.html#ScopeClaims\n    - http://openid.net/specs/openid-connect-core-1_0.html#ClaimsParameter\n\n    Arguments: access_token (:class:`AccessToken`): Associated access\n        token.  scope_request (list): Optional list of requested\n        scopes. Only scopes authorized in the `access_token` will be\n            considered.  claims_request\n        (dict): Optional dictionary with a claims request parameter.\n\n    Information on the claims request parameter specification:\n\n    - http://openid.net/specs/openid-connect-core-1_0.html#ClaimsParameter\n\n    As a convinience, if neither `scope_request` or user_info claim is\n    specified in the `claims_request`, it will return the claims for\n    all the scopes in the `access_token`.\n\n    Returns an :class:`IDToken` instance with the scopes from the\n    `scope_request` and the corresponding claims. Claims in the\n    `claims_request` paramater userinfo section will be included *in\n    addition* to the ones corresponding to `scope_request`.\n\n    \"\"\"\n\n    handlers = HANDLERS['userinfo']\n\n    # Select only the relevant section of the claims request.\n    claims_request_section = claims_request.get('userinfo', {}) if claims_request else {}\n\n    # If nothing is requested, return the claims for the scopes in the access token.\n    if not scope_request and not claims_request_section:\n        scope_request = provider.scope.to_names(access_token.scope)\n    else:\n        scope_request = scope_request\n\n    scopes, claims = collect(\n        handlers,\n        access_token,\n        scope_request=scope_request,\n        claims_request=claims_request_section,\n    )\n\n    return IDToken(access_token, scopes, claims)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef encode(self, secret, algorithm='HS256'):\n\n        return jwt.encode(self.claims, secret, algorithm)", "response": "Encodes the claims to the JWT token string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the data for an OAuth2 response.", "response": "def access_token_response_data(self, access_token, response_type=None, nonce=''):\n        \"\"\"\n        Return `access_token` fields for OAuth2, and add `id_token` fields for\n        OpenID Connect according to the `access_token` scope.\n\n        \"\"\"\n\n        # Clear the scope for requests that do not use OpenID Connect.\n        # Scopes for pure OAuth2 request are currently not supported.\n        scope = constants.DEFAULT_SCOPE\n\n        extra_data = {}\n\n        # Add OpenID Connect `id_token` if requested.\n        #\n        # TODO: Unfourtunately because of how django-oauth2-provider implements\n        # scopes, we cannot check if `openid` is the first scope to be\n        # requested, as required by OpenID Connect specification.\n\n        if provider.scope.check(constants.OPEN_ID_SCOPE, access_token.scope):\n            id_token = self.get_id_token(access_token, nonce)\n            extra_data['id_token'] = self.encode_id_token(id_token)\n            scope = provider.scope.to_int(*id_token.scopes)\n\n        # Update the token scope, so it includes only authorized values.\n        access_token.scope = scope\n        access_token.save()\n\n        # Get the main fields for OAuth2 response.\n        response_data = super(AccessTokenView, self).access_token_response_data(access_token)\n\n        # Add any additional fields if OpenID Connect is requested. The order of\n        # the addition makes sures the OAuth2 values are not overrided.\n        response_data = dict(extra_data.items() + response_data.items())\n\n        return response_data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_id_token(self, access_token, nonce):\n\n        claims_string = self.request.POST.get('claims')\n        claims_request = json.loads(claims_string) if claims_string else {}\n\n        return oidc.id_token(access_token, nonce, claims_request)", "response": "Return an ID token for the given Access Token and nonce."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_id_token(self, id_token):\n\n        # Encode the ID token using the `client_secret`.\n        #\n        # TODO: Using the `client_secret` is not ideal, since it is transmitted\n        # over the wire in some authentication flows.  A better alternative is\n        # to use the public key of the issuer, which also allows the ID token to\n        # be shared among clients. Doing so however adds some operational\n        # costs. We should consider this for the future.\n\n        secret = id_token.access_token.client.client_secret\n\n        return id_token.encode(secret)", "response": "Encode the ID token using the client secret."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrespond to a UserInfo request.", "response": "def get(self, request, *_args, **_kwargs):\n        \"\"\"\n        Respond to a UserInfo request.\n\n        Two optional query parameters are accepted, scope and claims.\n        See the references above for more details.\n\n        \"\"\"\n\n        access_token = self.access_token\n\n        scope_string = request.GET.get('scope')\n        scope_request = scope_string.split() if scope_string else None\n\n        claims_string = request.GET.get('claims')\n        claims_request = json.loads(claims_string) if claims_string else None\n\n        if not provider.scope.check(constants.OPEN_ID_SCOPE, access_token.scope):\n            return self._bad_request('Missing openid scope.')\n\n        try:\n            claims = self.userinfo_claims(access_token, scope_request, claims_request)\n        except ValueError, exception:\n            return self._bad_request(str(exception))\n\n        # TODO: Encode and sign responses if requested.\n\n        response = JsonResponse(claims)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the claims for the requested parameters.", "response": "def userinfo_claims(self, access_token, scope_request, claims_request):\n        \"\"\" Return the claims for the requested parameters. \"\"\"\n        id_token = oidc.userinfo(access_token, scope_request, claims_request)\n        return id_token.claims"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncollect all the claims values from the handlers.", "response": "def collect(handlers, access_token, scope_request=None, claims_request=None):\n    \"\"\"\n    Collect all the claims values from the `handlers`.\n\n    Arguments:\n      handlers (list): List of claim :class:`Handler` classes.\n      access_token (:class:AccessToken): Associated access token.\n      scope_request (list): List of requested scopes.\n      claims_request (dict): Dictionary with only the relevant section of a\n          OpenID Connect claims request.\n\n    Returns a list of the scopes from `scope_request` that are authorized, and a\n    dictionary of the claims associated with the authorized scopes in\n    `scope_request`, and additionally, the authorized claims listed in\n    `claims_request`.\n\n    \"\"\"\n    user = access_token.user\n    client = access_token.client\n\n    # Instantiate handlers. Each handler is instanciated only once, allowing the\n    # handler to keep state in-between calls to its scope and claim methods.\n\n    handlers = [cls() for cls in handlers]\n\n    # Find all authorized scopes by including the access_token scopes.  Note\n    # that the handlers determine if a scope is authorized, not its presense in\n    # the access_token.\n\n    required_scopes = set(REQUIRED_SCOPES)\n    token_scopes = set(provider.scope.to_names(access_token.scope))\n    authorized_scopes = _collect_scopes(handlers, required_scopes | token_scopes, user, client)\n\n    # Select only the authorized scopes from the requested scopes.\n\n    scope_request = set(scope_request) if scope_request else set()\n    scopes = required_scopes | (authorized_scopes & scope_request)\n\n    # Find all authorized claims names for the authorized_scopes.\n\n    authorized_names = _collect_names(handlers, authorized_scopes, user, client)\n\n    # Select only the requested claims if no scope has been requested. Selecting\n    # scopes has prevalence over selecting claims.\n\n    claims_request = _validate_claim_request(claims_request)\n\n    # Add the requested claims that are authorized to the response.\n\n    requested_names = set(claims_request.keys()) & authorized_names\n    names = _collect_names(handlers, scopes, user, client) | requested_names\n\n    # Get the values for the claims.\n\n    claims = _collect_values(\n        handlers,\n        names=names,\n        user=user,\n        client=client,\n        values=claims_request or {}\n    )\n\n    return authorized_scopes, claims"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncollecting all the authorized scopes according to the handlers.", "response": "def _collect_scopes(handlers, scopes, user, client):\n    \"\"\" Get a set of all the authorized scopes according to the handlers. \"\"\"\n    results = set()\n\n    data = {'user': user, 'client': client}\n\n    def visitor(scope_name, func):\n        claim_names = func(data)\n        # If the claim_names is None, it means that the scope is not authorized.\n        if claim_names is not None:\n            results.add(scope_name)\n\n    _visit_handlers(handlers, visitor, 'scope', scopes)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _collect_names(handlers, scopes, user, client):\n\n    results = set()\n\n    data = {'user': user, 'client': client}\n\n    def visitor(_scope_name, func):\n        claim_names = func(data)\n        # If the claim_names is None, it means that the scope is not authorized.\n        if claim_names is not None:\n            results.update(claim_names)\n\n    _visit_handlers(handlers, visitor, 'scope', scopes)\n\n    return results", "response": "Collect the names of the claims supported by the handlers for the requested scope."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _collect_values(handlers, names, user, client, values):\n\n    results = {}\n\n    def visitor(claim_name, func):\n        data = {'user': user, 'client': client}\n        data.update(values.get(claim_name) or {})\n        claim_value = func(data)\n        # If the claim_value is None, it means that the claim is not authorized.\n        if claim_value is not None:\n            # New values overwrite previous results\n            results[claim_name] = claim_value\n\n    _visit_handlers(handlers, visitor, 'claim', names)\n\n    return results", "response": "Collect the values from the handlers of the requested claims."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating a claim request section.", "response": "def _validate_claim_request(claims, ignore_errors=False):\n    \"\"\"\n    Validates a claim request section (`userinfo` or `id_token`) according\n    to section 5.5 of the OpenID Connect specification:\n\n    - http://openid.net/specs/openid-connect-core-1_0.html#ClaimsParameter\n\n    Returns a copy of the claim request with only the valid fields and values.\n\n    Raises ValueError is the claim request is invalid and `ignore_errors` is False\n\n    \"\"\"\n\n    results = {}\n    claims = claims if claims else {}\n\n    for name, value in claims.iteritems():\n        if value is None:\n            results[name] = None\n        elif isinstance(value, dict):\n            results[name] = _validate_claim_values(name, value, ignore_errors)\n        else:\n            if not ignore_errors:\n                msg = 'Invalid claim {}.'.format(name)\n                raise ValueError(msg)\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing visitor partern to collect information from handlers.", "response": "def _visit_handlers(handlers, visitor, prefix, suffixes):\n    \"\"\" Use visitor partern to collect information from handlers \"\"\"\n\n    results = []\n    for handler in handlers:\n        for suffix in suffixes:\n            func = getattr(handler, '{}_{}'.format(prefix, suffix).lower(), None)\n            if func:\n                results.append(visitor(suffix, func))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the model with the data in the form.", "response": "def update_model(self, model):\n        \"\"\"\n        trivial implementation for simple data in the form,\n        using the model prefix.\n        \"\"\"\n        for k, v in self.parse_form().items():\n            setattr(model, k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef locale_negotiator(request):\n    locale = 'en'\n    if request.accept_language:\n        locale = request.accept_language.best_match(LANGUAGES)\n        locale = LANGUAGES.get(locale, 'en')\n    return locale", "response": "Locale negotiator base on the Accept - Language header"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a PyShop WSGI application configured with settings.", "response": "def main(global_config, **settings):\n    \"\"\"\n    Get a PyShop WSGI application configured with settings.\n    \"\"\"\n    if sys.version_info[0] < 3:\n        reload(sys)\n        sys.setdefaultencoding('utf-8')\n\n    settings = dict(settings)\n    # Scoping sessions for Pyramid ensure session are commit/rollback\n    # after the template has been rendered\n    create_engine(settings, scoped=True)\n\n    authn_policy = RouteSwitchAuthPolicy(secret=settings['pyshop.cookie_key'],\n                                         callback=groupfinder)\n    authz_policy = ACLPolicy()\n    route_prefix = settings.get('pyshop.route_prefix')\n\n    config = Configurator(settings=settings,\n                          root_factory=RootFactory,\n                          route_prefix=route_prefix,\n                          locale_negotiator=locale_negotiator,\n                          authentication_policy=authn_policy,\n                          authorization_policy=authz_policy)\n    config.end()\n\n    return config.make_wsgi_app()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef by_name(cls, session, name):\n        return cls.first(session, where=(cls.name == name,))", "response": "Get a group from a given name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a user from a given login.", "response": "def by_login(cls, session, login, local=True):\n        \"\"\"\n        Get a user from a given login.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param login: the user login\n        :type login: unicode\n\n        :return: the associated user\n        :rtype: :class:`pyshop.models.User`\n        \"\"\"\n        user = cls.first(session,\n                         where=((cls.login == login),\n                                (cls.local == local),)\n                         )\n        # XXX it's appear that this is not case sensitive !\n        return user if user and user.login == login else None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a user from given credentials.", "response": "def by_credentials(cls, session, login, password):\n        \"\"\"\n        Get a user from given credentials\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param login: username\n        :type login: unicode\n\n        :param password: user password\n        :type password: unicode\n\n        :return: associated user\n        :rtype: :class:`pyshop.models.User`\n        \"\"\"\n        user = cls.by_login(session, login, local=True)\n        if not user:\n            return None\n        if crypt.check(user.password, password):\n            return user"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a user object by LDAP credentials", "response": "def by_ldap_credentials(cls, session, login, password, settings):\n        \"\"\"if possible try to contact the LDAP for authentification if success\n        and login don't exist localy create one and return it\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param login: username\n        :type login: unicode\n\n        :param password: user password\n        :type password: unicode\n\n        :param settings: settings from self.request.registry.settings in views\n        :type settings: dict\n\n        :return: associated user\n        :rtype: :class:`pyshop.models.User`\n\n        \"\"\"\n        if not asbool(settings.get('pyshop.ldap.use_for_auth', 'False')):\n            return None\n\n        if ldap is None:\n            raise ImportError(\n                \"no module name ldap. Install python-ldap package\")\n\n        try:\n            if hasattr(ldap, 'OPT_X_TLS_CACERTDIR'):\n                ldap.set_option(\n                    ldap.OPT_X_TLS_CACERTDIR, '/etc/openldap/cacerts')\n            ldap.set_option(ldap.OPT_REFERRALS, ldap.OPT_OFF)\n            ldap.set_option(ldap.OPT_RESTART, ldap.OPT_ON)\n            ldap.set_option(ldap.OPT_TIMEOUT, 20)\n            ldap.set_option(ldap.OPT_NETWORK_TIMEOUT, 10)\n            ldap.set_option(ldap.OPT_TIMELIMIT, 15)\n\n            ldap_server_type = settings.get('pyshop.ldap.type', 'ldap')\n            host = settings['pyshop.ldap.host'].strip()\n            port = settings.get('pyshop.ldap.port', None).strip()\n            if ldap_server_type in [\"ldaps\", \"start_tls\"]:\n                port = port or 689\n                ldap_type = \"ldaps\"\n                certreq = settings.get('pyshop.ldap.certreq', 'DEMAND').strip()\n                if certreq not in ['DEMAND', 'ALLOW', 'HARD', 'TRY', 'NEVER']:\n                    certreq = 'DEMAND'\n                tls_cert = getattr(ldap, 'OPT_X_TLS_%s' % certreq)\n                ldap.set_option(ldap.OPT_X_TLS_REQUIRE_CERT, tls_cert)\n            else:\n                port = port or 389\n                ldap_type = 'ldap'\n            server_url = \"{ldap_type}://{host}:{port}\".format(\n                ldap_type=ldap_type, host=host, port=port)\n            server = ldap.initialize(server_url)\n            if ldap_server_type == \"start_tls\":\n                server.start_tls_s()\n            server.protocol = ldap.VERSION3\n            # bind the account if needed\n            if settings['pyshop.ldap.account'] and \\\n                    settings['pyshop.ldap.password']:\n                server.simple_bind_s(settings['pyshop.ldap.account'],\n                                     settings['pyshop.ldap.password'])\n\n            filter_ = settings['pyshop.ldap.search_filter'].format(\n                username=login)\n            results = server.search_ext_s(\n                settings['pyshop.ldap.bind_dn'],\n                getattr(ldap,\n                        'SCOPE_%s' % settings['pyshop.ldap.search_scope']),\n                filter_)\n            if results is None:\n                log.debug(\"LDAP rejected password for user %s\", (login))\n                return None\n\n            for (dn, _attrs) in results:\n                if dn is None:\n                    continue\n                log.debug('Trying simple bind with %s', dn)\n                server.simple_bind_s(dn, password)\n                attrs = server.search_ext_s(\n                    dn, ldap.SCOPE_BASE, '(objectClass=*)')[0][1]\n                break\n            else:\n                log.debug(\"No matching LDAP objects for authentication of \"\n                          \"'%s'\", login)\n                return None\n\n            log.debug('LDAP authentication OK')\n            # we may create a new user if it don't exist\n            user_ldap = User.by_login(session, login)\n            if user_ldap is None:\n                log.debug('create user %s', login)\n                user_ldap = User()\n                user_ldap.login = login\n                # when creating a User, do not copy the ldap password\n                user_ldap.password = ''\n                user_ldap.local = True\n                user_ldap.firstname = attrs[\n                    settings['pyshop.ldap.first_name_attr']][0]\n                user_ldap.lastname = attrs[\n                    settings['pyshop.ldap.last_name_attr']][0]\n                user_ldap.email = attrs[\n                    settings['pyshop.ldap.email_attr']][0]\n                for groupname in [\"developer\", \"installer\"]:\n                    user_ldap.groups.append(Group.by_name(session, groupname))\n                if user_ldap.validate(session):\n                    session.add(user_ldap)\n                    log.debug('User \"%s\" added', login)\n                    transaction.commit()\n            # its OK\n            return user_ldap\n        except ldap.NO_SUCH_OBJECT:\n            log.debug(\"LDAP says no such user '%s'\", login)\n        except ldap.SERVER_DOWN:\n            log.error(\"LDAP can't access authentication server\")\n        except ldap.LDAPError:\n            log.error('ERROR while using LDAP connection')\n        except Exception as exc:\n            log.error('Unmanaged exception %s', exc, exc_info=True)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_locals(cls, session, **kwargs):\n        return cls.find(session,\n                        where=(cls.local == True,),\n                        order_by=cls.login,\n                        **kwargs)", "response": "Get all local users."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that the current user can be saved.", "response": "def validate(self, session):\n        \"\"\"\n        Validate that the current user can be saved.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :return: ``True``\n        :rtype: bool\n\n        :raise: :class:`pyshop.helpers.sqla.ModelError` if user is not valid\n        \"\"\"\n\n        errors = []\n        if not self.login:\n            errors.append(u'login is required')\n        else:\n            other = User.by_login(session, self.login)\n            if other and other.id != self.id:\n                errors.append(u'duplicate login %s' % self.login)\n        if not self.password:\n            errors.append(u'password is required')\n        if not self.email:\n            errors.append(u'email is required')\n        elif not re_email.match(self.email):\n            errors.append(u'%s is not a valid email' % self.email)\n\n        if len(errors):\n            raise ModelError(errors)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a classifier from a given name.", "response": "def by_name(cls, session, name, **kwargs):\n        \"\"\"\n        Get a classifier from a given name.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param name: name of the classifier\n        :type name: `unicode\n\n        :return: classifier instance\n        :rtype: :class:`pyshop.models.Classifier`\n        \"\"\"\n        classifier = cls.first(session, where=(cls.name == name,))\n\n        if not kwargs.get('create_if_not_exists', False):\n            return classifier\n\n        if not classifier:\n            splitted_names = [n.strip() for n in name.split(u'::')]\n            classifiers = [u' :: '.join(splitted_names[:i + 1])\n                           for i in range(len(splitted_names))]\n            parent_id = None\n            category = splitted_names[0]\n\n            for c in classifiers:\n                classifier = cls.first(session, where=(cls.name == c,))\n                if not classifier:\n                    classifier = Classifier(name=c, parent_id=parent_id,\n                                            category=category)\n                    session.add(classifier)\n                session.flush()\n                parent_id = classifier.id\n\n        return classifier"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sorted_releases(self):\n        releases = [(parse_version(release.version), release)\n                    for release in self.releases]\n        releases.sort(reverse=True)\n        return [release[1] for release in releases]", "response": "Returns a list of all the release records in the cache sorted by version."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a package from a given name.", "response": "def by_name(cls, session, name):\n        \"\"\"\n        Get a package from a given name.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param name: name of the package\n        :type name: `unicode\n\n        :return: package instance\n        :rtype: :class:`pyshop.models.Package`\n        \"\"\"\n        # XXX the field \"name\" should be created with a\n        # case insensitive collation.\n        pkg = cls.first(session, where=(cls.name.like(name),))\n        if not pkg:\n            name = name.replace(u'-', u'_').upper()\n            pkg = cls.first(session,\n                            where=(cls.name.like(name),))\n            # XXX _ is a like operator\n            if pkg and pkg.name.upper().replace(u'-', u'_') != name:\n                pkg = None\n        return pkg"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets packages from given filters.", "response": "def by_filter(cls, session, opts, **kwargs):\n        \"\"\"\n        Get packages from given filters.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param opts: filtering options\n        :type opts: `dict\n\n        :return: package instances\n        :rtype: generator of :class:`pyshop.models.Package`\n        \"\"\"\n        where = []\n\n        if opts.get('local_only'):\n            where.append(cls.local == True)\n\n        if opts.get('names'):\n            where.append(cls.name.in_(opts['names']))\n\n        if opts.get('classifiers'):\n            ids = [c.id for c in opts.get('classifiers')]\n            cls_pkg = classifier__package\n            qry = session.query(cls_pkg.c.package_id,\n                                func.count('*'))\n            qry = qry.filter(cls_pkg.c.classifier_id.in_(ids))\n            qry = qry.group_by(cls_pkg.c.package_id)\n            qry = qry.having(func.count('*') >= len(ids))\n            where.append(cls.id.in_([r[0] for r in qry.all()]))\n\n        return cls.find(session, where=where, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef by_owner(cls, session, owner_name):\n        return cls.find(session,\n                        join=(cls.owners),\n                        where=(User.login == owner_name,),\n                        order_by=cls.name)", "response": "Get packages from a given owner username."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef by_maintainer(cls, session, maintainer_name):\n        return cls.find(session,\n                        join=(cls.maintainers),\n                        where=(User.login == maintainer_name,),\n                        order_by=cls.name)", "response": "Get package instances from a given maintainer name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets all local packages.", "response": "def get_locals(cls, session):\n        \"\"\"\n        Get all local packages.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :return: package instances\n        :rtype: generator of :class:`pyshop.models.Package`\n        \"\"\"\n        return cls.find(session,\n                        where=(cls.local == True,))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all mirrored packages.", "response": "def get_mirrored(cls, session):\n        \"\"\"\n        Get all mirrored packages.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :return: package instances\n        :rtype: generator of :class:`pyshop.models.Package`\n        \"\"\"\n        return cls.find(session,\n                        where=(cls.local == False,))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef by_version(cls, session, package_name, version):\n        return cls.first(session,\n                         join=(Package,),\n                         where=((Package.name == package_name),\n                                (cls.version == version)))", "response": "Get release instance by a given version."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget releases for given classifiers.", "response": "def by_classifiers(cls, session, classifiers):\n        \"\"\"\n        Get releases for given classifiers.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param classifiers: classifiers\n        :type classifiers: unicode\n\n        :return: release instances\n        :rtype: generator of :class:`pyshop.models.Release`\n        \"\"\"\n        return cls.find(session,\n                        join=(cls.classifiers,),\n                        where=(Classifier.name.in_(classifiers),),\n                        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch releases for given filters.", "response": "def search(cls, session, opts, operator):\n        \"\"\"\n        Get releases for given filters.\n\n        :param session: SQLAlchemy session\n        :type session: :class:`sqlalchemy.Session`\n\n        :param opts: filtering options\n        :type opts: dict\n\n        :param operator: filtering options joining operator (`and` or `or`)\n        :type operator: basestring\n\n        :return: release instances\n        :rtype: generator of :class:`pyshop.models.Release`\n        \"\"\"\n        available = {'name': Package.name,\n                     'version': cls.version,\n                     'author': User.login,\n                     'author_email': User.email,\n                     'maintainer': User.login,\n                     'maintainer_email': User.email,\n                     'home_page': cls.home_page,\n                     'license': cls.license,\n                     'summary': cls.summary,\n                     'description': cls.description,\n                     'keywords': cls.keywords,\n                     'platform': cls.platform,\n                     'download_url': cls.download_url\n                     }\n        oper = {'or': or_, 'and': and_}\n        join_map = {'name': Package,\n                    'author': cls.author,\n                    'author_email': cls.author,\n                    'maintainer': cls.maintainer,\n                    'maintainer_email': cls.maintainer,\n                    }\n        where = []\n        join = []\n        for opt, val in opts.items():\n            field = available[opt]\n            if hasattr(val, '__iter__') and len(val) > 1:\n                stmt = or_(*[field.like(u'%%%s%%' % v) for v in val])\n            else:\n                stmt = field.like(u'%%%s%%' % val)\n            where.append(stmt)\n            if opt in join_map:\n                join.append(join_map[opt])\n        return cls.find(session, join=join,\n                        where=(oper[operator](*where),))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef by_release(cls, session, package_name, version):\n        return cls.find(session,\n                        join=(Release, Package),\n                        where=(Package.name == package_name,\n                               Release.version == version,\n                               ))", "response": "Get release files for a given package and version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef by_filename(cls, session, release, filename):\n        return cls.first(session,\n                         where=(ReleaseFile.release_id == release.id,\n                                ReleaseFile.filename == filename,\n                                ))", "response": "Get a release file for a given release and a given filename."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding url helpers to the template engine.", "response": "def add_urlhelpers(event):\n    \"\"\"\n    Add helpers to the template engine.\n    \"\"\"\n    event['static_url'] = lambda x: static_path(x, event['request'])\n    event['route_url'] = lambda name, *args, **kwargs: \\\n        route_path(name, event['request'], *args, **kwargs)\n    event['parse_rest'] = parse_rest\n    event['has_permission'] = event['request'].has_permission"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef includeme(config):\n    settings = config.registry.settings\n\n    # config.add_renderer('json', JSONP())\n    # release file download\n    config.add_renderer('repository', dl_renderer_factory)\n\n    # Jinja configuration\n    # We don't use jinja2 filename, .html instead\n    config.add_renderer('.html', renderer_factory)\n    # helpers\n    config.add_subscriber(add_urlhelpers, IBeforeRender)\n    # i18n\n    config.add_translation_dirs('locale/')\n\n    pypi_url = settings.get('pyshop.pypi.url', 'https://pypi.python.org/pypi')\n    # PyPI url for XML RPC service consume\n    pypi.set_proxy(pypi_url, settings.get('pyshop.pypi.transport_proxy'))\n\n    # Javascript + Media\n    config.add_static_view('static', 'static', cache_max_age=3600)\n    # config.add_static_view('repository', 'repository', cache_max_age=3600)\n\n    config.add_route(u'login', u'/login',)\n    config.add_view(u'pyshop.views.credentials.Login',\n                    route_name=u'login',\n                    renderer=u'shared/login.html')\n\n    config.add_route(u'logout', u'/logout')\n    config.add_view(u'pyshop.views.credentials.Logout',\n                    route_name=u'logout',\n                    permission=u'user_view')\n\n    # Home page\n    config.add_route(u'index', u'/')\n    config.add_view(u'pyshop.views.Index',\n                    route_name=u'index',\n                    permission=u'user_view')\n\n    # Archive downloads\n    config.add_route(u'show_external_release_file',\n                     u'/repository/ext/{release_id}/{filename:.*}',\n                     request_method=u'GET')\n    config.add_view(u'pyshop.views.repository.show_external_release_file',\n                    route_name=u'show_external_release_file',\n                    renderer=u'repository',\n                    permission=u'download_releasefile')\n\n    config.add_route(u'show_release_file',\n                     u'/repository/{file_id}/{filename:.*}',\n                     request_method=u'GET')\n    config.add_view(u'pyshop.views.repository.show_release_file',\n                    route_name=u'show_release_file',\n                    renderer=u'repository',\n                    permission=u'download_releasefile')\n\n    # Simple views used by pip\n    config.add_route(u'list_simple', u'/simple/', request_method=u'GET')\n\n    config.add_view(u'pyshop.views.simple.List',\n                    route_name=u'list_simple',\n                    renderer=u'pyshop/simple/list.html',\n                    permission=u'download_releasefile')\n\n    config.add_route(u'show_simple', u'/simple/{package_name}/')\n    config.add_view(u'pyshop.views.simple.Show',\n                    route_name=u'show_simple',\n                    renderer=u'pyshop/simple/show.html',\n                    permission=u'download_releasefile')\n\n    try:\n        config.add_notfound_view(notfound, append_slash=True)\n    except AttributeError:\n        # Pyramid < 1.4\n        pass\n\n\n    # Used by setup.py sdist upload\n\n    config.add_route(u'upload_releasefile', u'/simple/',\n                     request_method=u'POST')\n\n    config.add_view(u'pyshop.views.simple.UploadReleaseFile',\n                    renderer=u'pyshop/simple/create.html',\n                    route_name=u'upload_releasefile',\n                    permission=u'upload_releasefile')\n\n    # Web Services\n\n    config.add_renderer('pyshopxmlrpc', XMLRPCRenderer(allow_none=True))\n    config.add_xmlrpc_endpoint(\n        'api', '/pypi/xmlrpc', default_renderer='pyshopxmlrpc')\n    config.scan('pyshop.views.xmlrpc')\n\n    # Backoffice Views\n\n    config.add_route(u'list_package', u'/pyshop/package')\n    config.add_view(u'pyshop.views.package.List',\n                    route_name='list_package',\n                    renderer=u'pyshop/package/list.html',\n                    permission=u'user_view')\n\n    config.add_route(u'list_package_page', u'/pyshop/package/p/{page_no}')\n    config.add_view(u'pyshop.views.package.List',\n                    route_name='list_package_page',\n                    renderer=u'pyshop/package/list.html',\n                    permission=u'user_view')\n\n    config.add_route(u'show_package',\n                     u'/pyshop/package/{package_name}')\n\n    config.add_route(u'show_package_version',\n                     u'/pyshop/package/{package_name}/{release_version}')\n\n    config.add_view(u'pyshop.views.package.Show',\n                    route_name=u'show_package',\n                    renderer=u'pyshop/package/show.html',\n                    permission=u'user_view')\n\n    config.add_view(u'pyshop.views.package.Show',\n                    route_name=u'show_package_version',\n                    renderer=u'pyshop/package/show.html',\n                    permission=u'user_view')\n\n    # Admin  view\n    config.add_route(u'list_account', u'/pyshop/account')\n    config.add_view(u'pyshop.views.account.List',\n                    route_name=u'list_account',\n                    renderer=u'pyshop/account/list.html',\n                    permission=u'admin_view')\n\n    config.add_route(u'create_account', u'/pyshop/account/new')\n    config.add_view(u'pyshop.views.account.Create',\n                    route_name=u'create_account',\n                    renderer=u'pyshop/account/create.html',\n                    permission=u'admin_view')\n\n    config.add_route(u'edit_account', u'/pyshop/account/{user_id}')\n    config.add_view(u'pyshop.views.account.Edit',\n                    route_name=u'edit_account',\n                    renderer=u'pyshop/account/edit.html',\n                    permission=u'admin_view')\n\n    config.add_route(u'delete_account', u'/pyshop/delete/account/{user_id}')\n    config.add_view(u'pyshop.views.account.Delete',\n                    route_name=u'delete_account',\n                    renderer=u'pyshop/account/delete.html',\n                    permission=u'admin_view')\n\n    config.add_route(u'purge_package', u'/pyshop/purge/package/{package_id}')\n    config.add_view(u'pyshop.views.package.Purge',\n                    route_name=u'purge_package',\n                    renderer=u'pyshop/package/purge.html',\n                    permission=u'admin_view')\n\n    # Current user can update it's information\n    config.add_route(u'edit_user', u'/pyshop/user')\n    config.add_view(u'pyshop.views.user.Edit',\n                    route_name=u'edit_user',\n                    renderer=u'pyshop/user/edit.html',\n                    permission=u'user_view')\n\n    config.add_route(u'change_password', u'/pyshop/user/password')\n    config.add_view(u'pyshop.views.user.ChangePassword',\n                    route_name=u'change_password',\n                    renderer=u'pyshop/user/change_password.html',\n                    permission=u'user_view')\n\n    # Credentials\n    for route in ('list_simple', 'show_simple',\n                  'show_release_file', 'show_external_release_file',\n                  'upload_releasefile'):\n        config.add_view('pyshop.views.credentials.authbasic',\n                        route_name=route,\n                        context='pyramid.exceptions.Forbidden'\n                        )\n\n    config.add_view('pyshop.views.credentials.Login',\n                    renderer=u'shared/login.html',\n                    context=u'pyramid.exceptions.Forbidden')", "response": "A Pyramid includeme file for the Pyramid project."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the packages registered with the package index.", "response": "def list_packages(request):\n    \"\"\"\n    Retrieve a list of the package names registered with the package index.\n    Returns a list of name strings.\n    \"\"\"\n    session = DBSession()\n    names = [p.name for p in Package.all(session, order_by=Package.name)]\n    return names"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef package_releases(request, package_name, show_hidden=False):\n    session = DBSession()\n    package = Package.by_name(session, package_name)\n    return [rel.version for rel in package.sorted_releases]", "response": "Retrieve a list of the releases registered for the given package_name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a list of users and their attributes roles for a given package_name. Role is either Maintainer or Owner.", "response": "def package_roles(request, package_name):\n    \"\"\"\n    Retrieve a list of users and their attributes roles for a given\n    package_name. Role is either 'Maintainer' or 'Owner'.\n    \"\"\"\n    session = DBSession()\n    package = Package.by_name(session, package_name)\n    owners = [('Owner', o.name) for o in package.owners]\n    maintainers = [('Maintainer', o.name) for o in package.maintainers]\n    return owners + maintainers"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a list of role_name package_name for a given user.", "response": "def user_packages(request, user):\n    \"\"\"\n    Retrieve a list of [role_name, package_name] for a given username.\n    Role is either 'Maintainer' or 'Owner'.\n    \"\"\"\n    session = DBSession()\n    owned = Package.by_owner(session, user)\n    maintained = Package.by_maintainer(session, user)\n    owned = [('Owner', p.name) for p in owned]\n    maintained = [('Maintainer', p.name) for p in maintained]\n    return owned + maintained"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a list of files and download count for a given package and version.", "response": "def release_downloads(request, package_name, version):\n    \"\"\"\n    Retrieve a list of files and download count for a given package and\n    release version.\n    \"\"\"\n    session = DBSession()\n    release_files = ReleaseFile.by_release(session, package_name, version)\n    if release_files:\n        release_files = [(f.release.package.name,\n                            f.filename) for f in release_files]\n    return release_files"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef release_urls(request, package_name, version):\n    session = DBSession()\n    release_files = ReleaseFile.by_release(session, package_name, version)\n    return [{'url': f.url,\n                'packagetype': f.package_type,\n                'filename': f.filename,\n                'size': f.size,\n                'md5_digest': f.md5_digest,\n                'downloads': f.downloads,\n                'has_sig': f.has_sig,\n                'comment_text': f.comment_text,\n                'python_version': f.python_version\n                }\n            for f in release_files]", "response": "Retrieve a list of download URLs for the given package release."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef release_data(request, package_name, version):\n    session = DBSession()\n    release = Release.by_version(session, package_name, version)\n\n    if release:\n        result = {'name': release.package.name,\n                    'version': release.version,\n                    'stable_version': '',\n                    'author': release.author.name,\n                    'author_email': release.author.email,\n                    'home_page': release.home_page,\n                    'license': release.license,\n                    'summary': release.summary,\n                    'description': release.description,\n                    'keywords': release.keywords,\n                    'platform': release.platform,\n                    'download_url': release.download_url,\n                    'classifiers': [c.name for c in release.classifiers],\n                    #'requires': '',\n                    #'requires_dist': '',\n                    #'provides': '',\n                    #'provides_dist': '',\n                    #'requires_external': '',\n                    #'requires_python': '',\n                    #'obsoletes': '',\n                    #'obsoletes_dist': '',\n                    'bugtrack_url': release.bugtrack_url,\n                    'docs_url': release.docs_url,\n                    }\n\n    if release.maintainer:\n        result.update({'maintainer': release.maintainer.name,\n                        'maintainer_email': release.maintainer.email,\n                        })\n\n    return dict([(key, val or '') for key, val in result.items()])", "response": "Returns a dictionary containing the metadata for a specific package release."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching the package database using the specified search spec.", "response": "def search(request, spec, operator='and'):\n    \"\"\"\n    Search the package database using the indicated search spec.\n\n    The spec may include any of the keywords described in the above list\n    (except 'stable_version' and 'classifiers'),\n    for example: {'description': 'spam'} will search description fields.\n    Within the spec, a field's value can be a string or a list of strings\n    (the values within the list are combined with an OR),\n    for example: {'name': ['foo', 'bar']}.\n    Valid keys for the spec dict are listed here. Invalid keys are ignored:\n        name\n        version\n        author\n        author_email\n        maintainer\n        maintainer_email\n        home_page\n        license\n        summary\n        description\n        keywords\n        platform\n        download_url\n    Arguments for different fields are combined using either \"and\"\n    (the default) or \"or\".\n    Example: search({'name': 'foo', 'description': 'bar'}, 'or').\n    The results are returned as a list of dicts\n    {'name': package name,\n        'version': package release version,\n        'summary': package release summary}\n    \"\"\"\n    api = pypi.proxy\n    rv = []\n    # search in proxy\n    for k, v in spec.items():\n        rv += api.search({k: v}, True)\n\n    # search in local\n    session = DBSession()\n    release = Release.search(session, spec, operator)\n    rv += [{'name': r.package.name,\n            'version': r.version,\n            'summary': r.summary,\n            # hack https://mail.python.org/pipermail/catalog-sig/2012-October/004633.html\n            '_pypi_ordering':'',\n            } for r in release]\n    return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef browse(request, classifiers):\n    session = DBSession()\n    release = Release.by_classifiers(session, classifiers)\n    rv = [(r.package.name, r.version) for r in release]\n    return rv", "response": "Browse a list of release names and versions for a given Trove classifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the proxy to PyPI XML - RPC Server", "response": "def set_proxy(proxy_url, transport_proxy=None):\n    \"\"\"Create the proxy to PyPI XML-RPC Server\"\"\"\n    global proxy, PYPI_URL\n    PYPI_URL = proxy_url\n    proxy = xmlrpc.ServerProxy(\n        proxy_url,\n        transport=RequestsTransport(proxy_url.startswith('https://')),\n        allow_none=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes an xmlrpc request.", "response": "def request(self, host, handler, request_body, verbose):\n        \"\"\"\n        Make an xmlrpc request.\n        \"\"\"\n        headers = {'User-Agent': self.user_agent,\n                   #Proxy-Connection': 'Keep-Alive',\n                   #'Content-Range': 'bytes oxy1.0/-1',\n                   'Accept': 'text/xml',\n                   'Content-Type': 'text/xml' }\n        url = self._build_url(host, handler)\n        try:\n            resp = requests.post(url, data=request_body, headers=headers)\n        except ValueError:\n            raise\n        except Exception:\n            raise # something went wrong\n        else:\n            try:\n                resp.raise_for_status()\n            except requests.RequestException as e:\n                raise xmlrpc.ProtocolError(url, resp.status_code,\n                                                        str(e), resp.headers)\n            else:\n                return self.parse_response(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authbasic(request):\n    if len(request.environ.get('HTTP_AUTHORIZATION', '')) > 0:\n        auth = request.environ.get('HTTP_AUTHORIZATION')\n        scheme, data = auth.split(None, 1)\n        assert scheme.lower() == 'basic'\n        data = base64.b64decode(data)\n        if not isinstance(data, unicode):\n            data = data.decode('utf-8')\n        username, password = data.split(':', 1)\n        if User.by_ldap_credentials(\n                DBSession(), username, password, request.registry.settings):\n            return HTTPFound(location=request.url)\n        if User.by_credentials(DBSession(), username, password):\n            return HTTPFound(location=request.url)\n    return Response(status=401,\n                    headerlist=[(str('WWW-Authenticate'),\n                                 str('Basic realm=\"pyshop repository access\"'),\n                                 )],\n                    )", "response": "Authentification basic Upload pyshop repository access"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_acl(self, request):\n        if RootFactory._acl is None:\n            acl = []\n            session = DBSession()\n            groups = Group.all(session)\n            for g in groups:\n                acl.extend([(Allow, g.name, p.name) for p in g.permissions])\n            RootFactory._acl = acl\n\n        return RootFactory._acl", "response": "Get the ACLs from the SQL database once."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef show_release_file(root, request):\n    settings = request.registry.settings\n    whlify = asbool(settings.get('pyshop.mirror.wheelify', '0'))\n    session = DBSession()\n\n    f = ReleaseFile.by_id(session, int(request.matchdict['file_id']))\n    whlify = whlify and f.package_type == 'sdist'\n\n    filename = f.filename_whlified if whlify else f.filename\n    url = f.url\n    if url and url.startswith('http://pypi.python.org'):\n        url = 'https' + url[4:]\n\n    rv = {'url': url,\n          'filename': filename,\n          'original': f.filename,\n          'whlify': whlify\n          }\n    f.downloads += 1\n    f.release.downloads += 1\n    f.release.package.downloads += 1\n    session.add(f.release.package)\n    session.add(f.release)\n    session.add(f)\n    request.response.etag = f.md5_digest\n    request.response.cache_control = 'max-age=31557600, public'\n    request.response.date = datetime.datetime.utcnow()\n    return rv", "response": "Download a release file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef show_external_release_file(root, request):\n    session = DBSession()\n\n    settings = request.registry.settings\n    whlify = asbool(settings.get('pyshop.mirror.wheelify', '0'))\n    release = Release.by_id(session, int(request.matchdict['release_id']))\n\n    filename = (release.whlify_download_url_file if whlify else\n                release.download_url_file)\n\n    rv = {'url': release.download_url,\n          'filename': filename,\n          'original': release.download_url_file,\n          'whlify': whlify\n          }\n\n    release.downloads += 1\n    release.package.downloads += 1\n    session.add(release.package)\n    session.add(release)\n    request.response.date = datetime.datetime.utcnow()\n    return rv", "response": "Show a release file from a download url from its package information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_b64(data):\n    '''Wrapper for b64decode, without having to struggle with bytestrings.'''\n    byte_string = data.encode('utf-8')\n    decoded = base64.b64decode(byte_string)\n    return decoded.decode('utf-8')", "response": "Wrapper for b64decode without having to struggle with bytestrings."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_b64(data):\n    '''Wrapper for b64encode, without having to struggle with bytestrings.'''\n    byte_string = data.encode('utf-8')\n    encoded = base64.b64encode(byte_string)\n    return encoded.decode('utf-8')", "response": "Wrapper for b64encode without having to struggle with bytestrings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef array2image(self, array, subgrid_res=1):\n        nx, ny = self._nx * subgrid_res, self._ny * subgrid_res\n        if self._idex_mask_bool is True:\n            idex_mask = self._idex_mask\n            grid1d = np.zeros((nx * ny))\n            if subgrid_res > 1:\n                idex_mask_subgrid = self._idex_mask_sub\n            else:\n                idex_mask_subgrid = idex_mask\n            grid1d[idex_mask_subgrid == 1] = array\n        else:\n            grid1d = array\n        grid2d = util.array2image(grid1d, nx, ny)\n        return grid2d", "response": "Maps a 1d array into a 2d grid with array populating the idex_mask indices"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef image2array(self, image):\n        idex_mask = self._idex_mask\n        array = util.image2array(image)\n        if self._idex_mask_bool is True:\n            return array[idex_mask == 1]\n        else:\n            return array", "response": "converts an image into a 1d array of values in idex_mask"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the minimum and maximum PSF values for the idex_mask.", "response": "def _init_mask_psf(self):\n        \"\"\"\n        smaller frame that encolses all the idex_mask\n        :param idex_mask:\n        :param nx:\n        :param ny:\n        :return:\n        \"\"\"\n        if not hasattr(self, '_x_min_psf'):\n            idex_2d = self._idex_mask_2d\n            self._x_min_psf = np.min(np.where(idex_2d == 1)[0])\n            self._x_max_psf = np.max(np.where(idex_2d == 1)[0])\n            self._y_min_psf = np.min(np.where(idex_2d == 1)[1])\n            self._y_max_psf = np.max(np.where(idex_2d == 1)[1])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmanages the convention from an iterative index to the specific polynomial n, m, (real/imaginary part) :param index: int, index of list :return: n, m bool", "response": "def index2poly(self, index):\n        \"\"\"\n        manages the convention from an iterative index to the specific polynomial n, m, (real/imaginary part)\n\n        :param index: int, index of list\n        :return: n, m bool\n        \"\"\"\n\n        n = self._index2n(index)\n\n        num_prev = n * (n + 1) / 2\n        num = index + 1\n        delta = int(num - num_prev - 1)\n        if n % 2 == 0:\n            if delta == 0:\n                m = delta\n                complex_bool = False\n            elif delta % 2 == 0:\n                complex_bool = True\n                m = delta\n            else:\n                complex_bool = False\n                m = delta + 1\n        else:\n            if delta % 2 == 0:\n                complex_bool = False\n                m = delta + 1\n            else:\n                complex_bool = True\n                m = delta\n        return n, m, complex_bool"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndecompose an image into the shapelet coefficients in same order as for the function call :param image: :param x: :param y: :param n_max: :param beta: :param center_x: :param center_y: :return:", "response": "def decomposition(self, image, x, y, n_max, beta, deltaPix, center_x=0, center_y=0):\n        \"\"\"\n        decomposes an image into the shapelet coefficients in same order as for the function call\n        :param image:\n        :param x:\n        :param y:\n        :param n_max:\n        :param beta:\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n        num_param = self.shapelets.num_param(n_max)\n        param_list = np.zeros(num_param)\n        amp_norm = 1. * deltaPix**2\n        L_list = self._pre_calc(x, y, beta, n_max, center_x, center_y)\n        for i in range(num_param):\n            base = self._pre_calc_function(L_list, i) * amp_norm\n            param = np.sum(image*base)\n            n, m, complex_bool = self.shapelets.index2poly(i)\n            if m != 0:\n                param *= 2\n            param_list[i] = param\n        return param_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef de_shift_kernel(kernel, shift_x, shift_y, iterations=20):\n    nx, ny = np.shape(kernel)\n    kernel_new = np.zeros((nx+2, ny+2)) + (kernel[0, 0] + kernel[0, -1] + kernel[-1, 0] + kernel[-1, -1]) / 4.\n    kernel_new[1:-1, 1:-1] = kernel\n    int_shift_x = int(round(shift_x))\n    frac_x_shift = shift_x - int_shift_x\n    int_shift_y = int(round(shift_y))\n    frac_y_shift = shift_y - int_shift_y\n    kernel_init = copy.deepcopy(kernel_new)\n    kernel_init_shifted = copy.deepcopy(interp.shift(kernel_init, [int_shift_y, int_shift_x], order=1))\n    kernel_new = interp.shift(kernel_new, [int_shift_y, int_shift_x], order=1)\n    norm = np.sum(kernel_init_shifted)\n    for i in range(iterations):\n        kernel_shifted_inv = interp.shift(kernel_new, [-frac_y_shift, -frac_x_shift], order=1)\n        delta = kernel_init_shifted - kernel_norm(kernel_shifted_inv) * norm\n        kernel_new += delta * 1.\n        kernel_new = kernel_norm(kernel_new) * norm\n    return kernel_new[1:-1, 1:-1]", "response": "This function de - shifts a shifted kernel to the center of a pixel. This is performed iteratively."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a higher resolution kernel with subgrid resolution", "response": "def subgrid_kernel(kernel, subgrid_res, odd=False, num_iter=100):\n    \"\"\"\n    creates a higher resolution kernel with subgrid resolution as an interpolation of the original kernel in an\n    iterative approach\n\n    :param kernel: initial kernel\n    :param subgrid_res: subgrid resolution required\n    :return: kernel with higher resolution (larger)\n    \"\"\"\n    subgrid_res = int(subgrid_res)\n    if subgrid_res == 1:\n        return kernel\n    nx, ny = np.shape(kernel)\n    d_x = 1. / nx\n    x_in = np.linspace(d_x/2, 1-d_x/2, nx)\n    d_y = 1. / nx\n    y_in = np.linspace(d_y/2, 1-d_y/2, ny)\n    nx_new = nx * subgrid_res\n    ny_new = ny * subgrid_res\n    if odd is True:\n        if nx_new % 2 == 0:\n            nx_new -= 1\n        if ny_new % 2 == 0:\n            ny_new -= 1\n\n    d_x_new = 1. / nx_new\n    d_y_new = 1. / ny_new\n    x_out = np.linspace(d_x_new/2., 1-d_x_new/2., nx_new)\n    y_out = np.linspace(d_y_new/2., 1-d_y_new/2., ny_new)\n    kernel_input = copy.deepcopy(kernel)\n    kernel_subgrid = image_util.re_size_array(x_in, y_in, kernel_input, x_out, y_out)\n    kernel_subgrid = kernel_norm(kernel_subgrid)\n    for i in range(max(num_iter, 1)):\n        # given a proposition, re-size it to original pixel size\n        if subgrid_res % 2 == 0:\n            kernel_pixel = averaging_even_kernel(kernel_subgrid, subgrid_res)\n        else:\n            kernel_pixel = util.averaging(kernel_subgrid, numGrid=nx_new, numPix=nx)\n        delta = kernel - kernel_pixel\n        #plt.matshow(delta)\n        #plt.colorbar()\n        #plt.show()\n        temp_kernel = kernel_input + delta\n        kernel_subgrid = image_util.re_size_array(x_in, y_in, temp_kernel, x_out, y_out)#/norm_subgrid\n        kernel_subgrid = kernel_norm(kernel_subgrid)\n        kernel_input = temp_kernel\n\n    #from scipy.ndimage import zoom\n\n    #ratio = subgrid_res\n    #kernel_subgrid = zoom(kernel, ratio, order=4) / ratio ** 2\n    #print(np.shape(kernel_subgrid))\n    # whatever has not been matched is added to zeroth order (in squares of the undersampled PSF)\n    if subgrid_res % 2 == 0:\n        return kernel_subgrid\n    kernel_pixel = util.averaging(kernel_subgrid, numGrid=nx_new, numPix=nx)\n    kernel_pixel = kernel_norm(kernel_pixel)\n    delta_kernel = kernel_pixel - kernel_norm(kernel)\n    id = np.ones((subgrid_res, subgrid_res))\n    delta_kernel_sub = np.kron(delta_kernel, id)/subgrid_res**2\n    return kernel_norm(kernel_subgrid - delta_kernel_sub)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef averaging_even_kernel(kernel_high_res, subgrid_res):\n    n_high = len(kernel_high_res)\n    n_low = int((n_high + 1) / subgrid_res)\n    kernel_low_res = np.zeros((n_low, n_low))\n    # adding pixels that are fully within a single re-binned pixel\n    for i in range(subgrid_res-1):\n        for j in range(subgrid_res-1):\n            kernel_low_res += kernel_high_res[i::subgrid_res, j::subgrid_res]\n    # adding half of a pixel that has over-lap with two pixels\n    i = subgrid_res - 1\n    for j in range(subgrid_res - 1):\n        kernel_low_res[1:, :] += kernel_high_res[i::subgrid_res, j::subgrid_res] / 2\n        kernel_low_res[:-1, :] += kernel_high_res[i::subgrid_res, j::subgrid_res] / 2\n    j = subgrid_res - 1\n    for i in range(subgrid_res - 1):\n        kernel_low_res[:, 1:] += kernel_high_res[i::subgrid_res, j::subgrid_res] / 2\n        kernel_low_res[:, :-1] += kernel_high_res[i::subgrid_res, j::subgrid_res] / 2\n    # adding a quarter of a pixel value that is at the boarder of four pixels\n    i = subgrid_res - 1\n    j = subgrid_res - 1\n    kernel_edge = kernel_high_res[i::subgrid_res, j::subgrid_res]\n    kernel_low_res[1:, 1:] += kernel_edge / 4\n    kernel_low_res[:-1, 1:] += kernel_edge / 4\n    kernel_low_res[1:, :-1] += kernel_edge / 4\n    kernel_low_res[:-1, :-1] += kernel_edge / 4\n    return kernel_low_res", "response": "This function averages the kernel_high_res and subgrid_res into a lower resolution kernel."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the pixel size of a given kernel", "response": "def kernel_pixelsize_change(kernel, deltaPix_in, deltaPix_out):\n    \"\"\"\n    change the pixel size of a given kernel\n    :param kernel:\n    :param deltaPix_in:\n    :param deltaPix_out:\n    :return:\n    \"\"\"\n    numPix = len(kernel)\n    numPix_new = int(round(numPix * deltaPix_in/deltaPix_out))\n    if numPix_new % 2 == 0:\n        numPix_new -= 1\n    x_in = np.linspace(-(numPix-1)/2*deltaPix_in, (numPix-1)/2*deltaPix_in, numPix)\n    x_out = np.linspace(-(numPix_new-1)/2*deltaPix_out, (numPix_new-1)/2*deltaPix_out, numPix_new)\n    kernel_out = image_util.re_size_array(x_in, x_in, kernel, x_out, x_out)\n    kernel_out = kernel_norm(kernel_out)\n    return kernel_out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncut the psf properly", "response": "def cut_psf(psf_data, psf_size):\n    \"\"\"\n    cut the psf properly\n    :param psf_data: image of PSF\n    :param psf_size: size of psf\n    :return: re-sized and re-normalized PSF\n    \"\"\"\n    kernel = image_util.cut_edges(psf_data, psf_size)\n    kernel = kernel_norm(kernel)\n    return kernel"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a pixelised kernel of a point source to a kernel representing a uniform extended pixel", "response": "def pixel_kernel(point_source_kernel, subgrid_res=7):\n    \"\"\"\n    converts a pixelised kernel of a point source to a kernel representing a uniform extended pixel\n\n    :param point_source_kernel:\n    :param subgrid_res:\n    :return: convolution kernel for an extended pixel\n    \"\"\"\n    kernel_subgrid = subgrid_kernel(point_source_kernel, subgrid_res, num_iter=10)\n    kernel_size = len(point_source_kernel)\n    kernel_pixel = np.zeros((kernel_size*subgrid_res, kernel_size*subgrid_res))\n    for i in range(subgrid_res):\n        k_x = int((kernel_size-1) / 2 * subgrid_res + i)\n        for j in range(subgrid_res):\n            k_y = int((kernel_size-1) / 2 * subgrid_res + j)\n            kernel_pixel = image_util.add_layer2image(kernel_pixel, k_x, k_y, kernel_subgrid)\n    kernel_pixel = util.averaging(kernel_pixel, numGrid=kernel_size*subgrid_res, numPix=kernel_size)\n    return kernel_norm(kernel_pixel)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncutting out point source (e.g. PSF estimate) out of image and shift it to the center of a pixel :param x_pos: :param y_pos: :param image: :param kernelsize: :return:", "response": "def cutout_source(x_pos, y_pos, image, kernelsize, shift=True):\n    \"\"\"\n    cuts out point source (e.g. PSF estimate) out of image and shift it to the center of a pixel\n    :param x_pos:\n    :param y_pos:\n    :param image:\n    :param kernelsize:\n    :return:\n    \"\"\"\n    if kernelsize % 2 == 0:\n        raise ValueError(\"even pixel number kernel size not supported!\")\n    x_int = int(round(x_pos))\n    y_int = int(round(y_pos))\n    n = len(image)\n    d = (kernelsize - 1)/2\n    x_max = int(np.minimum(x_int + d + 1, n))\n    x_min = int(np.maximum(x_int - d, 0))\n    y_max = int(np.minimum(y_int + d + 1, n))\n    y_min = int(np.maximum(y_int - d, 0))\n    image_cut = copy.deepcopy(image[y_min:y_max, x_min:x_max])\n    shift_x = x_int - x_pos\n    shift_y = y_int - y_pos\n    if shift is True:\n        kernel_shift = de_shift_kernel(image_cut, shift_x, shift_y, iterations=50)\n    else:\n        kernel_shift = image_cut\n    kernel_final = np.zeros((kernelsize, kernelsize))\n\n    k_l2_x = int((kernelsize - 1) / 2)\n    k_l2_y = int((kernelsize - 1) / 2)\n\n    xk_min = np.maximum(0, -x_int + k_l2_x)\n    yk_min = np.maximum(0, -y_int + k_l2_y)\n    xk_max = np.minimum(kernelsize, -x_int + k_l2_x + n)\n    yk_max = np.minimum(kernelsize, -y_int + k_l2_y + n)\n\n    kernel_final[yk_min:yk_max, xk_min:xk_max] = kernel_shift\n    return kernel_final"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the full width at half maximum of a kernel", "response": "def fwhm_kernel(kernel):\n    \"\"\"\n    computes the full width at half maximum of a (PSF) kernel\n    :param kernel: (psf) kernel, 2d numpy array\n    :return: fwhm in units of pixels\n    \"\"\"\n    n = len(kernel)\n    if n % 2 == 0:\n        raise ValueError('only works with odd number of pixels in kernel!')\n    max_flux = kernel[int((n-1)/2), int((n-1)/2)]\n    I_2 = max_flux/2.\n    I_r = kernel[int((n-1)/2), int((n-1)/2):]\n    r = np.linspace(0, (n-1)/2, int((n + 1) / 2))\n    for i in range(1, len(r)):\n        if I_r[i] < I_2:\n            fwhm_2 = (I_2 - I_r[i-1])/(I_r[i] - I_r[i-1]) + r[i-1]\n            return fwhm_2 * 2\n    raise ValueError('The kernel did not drop to half the max value - fwhm not determined!')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef estimate_amp(data, x_pos, y_pos, psf_kernel):\n    numPix_x, numPix_y = np.shape(data)\n    #data_center = int((numPix-1.)/2)\n    x_int = int(round(x_pos-0.49999))#+data_center\n    y_int = int(round(y_pos-0.49999))#+data_center\n    if x_int > 2 and x_int < numPix_x-2 and y_int > 2 and y_int < numPix_y-2:\n        mean_image = max(np.sum(data[y_int-2:y_int+3, x_int-2:x_int+3]), 0)\n        num = len(psf_kernel)\n        center = int((num-0.5)/2)\n        mean_kernel = np.sum(psf_kernel[center-2:center+3, center-2:center+3])\n        amp_estimated = mean_image/mean_kernel\n    else:\n        amp_estimated = 0\n    return amp_estimated", "response": "estimate the amplitude of a point source located at x_pos y_pos"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconstructs the CDF and draws from it random realizations of projected radii R", "response": "def draw_light_2d_linear(self, kwargs_list, n=1, new_compute=False, r_eff=1.):\n        \"\"\"\n        constructs the CDF and draws from it random realizations of projected radii R\n        :param kwargs_list:\n        :return:\n        \"\"\"\n        if not hasattr(self, '_light_cdf') or new_compute is True:\n            r_array = np.linspace(self._min_interpolate, self._max_interpolate, self._interp_grid_num)\n            cum_sum = np.zeros_like(r_array)\n            sum = 0\n            for i, r in enumerate(r_array):\n                if i == 0:\n                    cum_sum[i] = 0\n                else:\n                    sum += self.light_2d(r, kwargs_list) * r\n                    cum_sum[i] = copy.deepcopy(sum)\n            cum_sum_norm = cum_sum/cum_sum[-1]\n            f = interp1d(cum_sum_norm, r_array)\n            self._light_cdf = f\n        cdf_draw = np.random.uniform(0., 1, n)\n        r_draw = self._light_cdf(cdf_draw)\n        return r_draw"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct the CDF and draws from it random realizations of projected radii R", "response": "def draw_light_2d(self, kwargs_list, n=1, new_compute=False):\n        \"\"\"\n        constructs the CDF and draws from it random realizations of projected radii R\n        :param kwargs_list:\n        :return:\n        \"\"\"\n        if not hasattr(self, '_light_cdf_log') or new_compute is True:\n            r_array = np.logspace(np.log10(self._min_interpolate), np.log10(self._max_interpolate), self._interp_grid_num)\n            cum_sum = np.zeros_like(r_array)\n            sum = 0\n            for i, r in enumerate(r_array):\n                if i == 0:\n                    cum_sum[i] = 0\n                else:\n                    sum += self.light_2d(r, kwargs_list) * r * r\n                    cum_sum[i] = copy.deepcopy(sum)\n            cum_sum_norm = cum_sum/cum_sum[-1]\n            f = interp1d(cum_sum_norm, np.log(r_array))\n            self._light_cdf_log = f\n        cdf_draw = np.random.uniform(0., 1, n)\n        r_log_draw = self._light_cdf_log(cdf_draw)\n        return np.exp(r_log_draw)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn df x y of the function", "response": "def derivatives(self, x, y, Rs, theta_Rs, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function (integral of NFW)\n        \"\"\"\n        rho0_input = self._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs)\n        if Rs < 0.0000001:\n            Rs = 0.0000001\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_**2 + y_**2)\n        f_x, f_y = self.nfwAlpha(R, Rs, rho0_input, x_, y_)\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef density(self, R, Rs, rho0):\n        return rho0/(R/Rs*(1+R/Rs)**2)", "response": "calculate the density of the NFW profile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef density_2d(self, x, y, Rs, rho0, center_x=0, center_y=0):\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_**2 + y_**2)\n        x = R/Rs\n        Fx = self.F_(x)\n        return 2*rho0*Rs*Fx", "response": "returns the density at radius R"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mass_3d(self, R, Rs, rho0):\n        Rs = float(Rs)\n        m_3d = 4. * np.pi * rho0 * Rs**3 *(np.log((Rs + R)/Rs) - R/(Rs + R))\n        return m_3d", "response": "r Returns the mass enclosed a 3d sphere or radius r\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mass_2d(self, R, Rs, rho0):\n        x = R/Rs\n        gx = self.g_(x)\n        m_2d = 4*rho0*Rs*R**2*gx/x**2 * np.pi\n        return m_2d", "response": "calculate the mass of a 3d sphere or radius r\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef F_(self, X):\n        if self._interpol:\n            if not hasattr(self, '_F_interp'):\n\n                if self._lookup:\n                    x = self._x_lookup\n                    F_x = self._f_lookup\n                else:\n                    x = np.linspace(0, self._max_interp_X, self._num_interp_X)\n                    F_x = self._F(x)\n                self._F_interp = interp.interp1d(x, F_x, kind='linear', axis=-1, copy=False, bounds_error=False,\n                                                 fill_value=0, assume_sorted=True)\n            return self._F_interp(X)\n        else:\n            return self._F(X)", "response": "computes the F function for a single item"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _F(self, X):\n        if isinstance(X, int) or isinstance(X, float):\n            if X < 1 and X > 0:\n                a = 1/(X**2-1)*(1-2/np.sqrt(1-X**2)*np.arctanh(np.sqrt((1-X)/(1+X))))\n            elif X == 1:\n                a = 1./3\n            elif X > 1:\n                a = 1/(X**2-1)*(1-2/np.sqrt(X**2-1)*np.arctan(np.sqrt((X-1)/(1+X))))\n            else:  # X == 0:\n                c = 0.0000001\n                a = 1/(-1)*(1-2/np.sqrt(1)*np.arctanh(np.sqrt((1-c)/(1+c))))\n\n        else:\n            a = np.empty_like(X)\n            x = X[(X < 1) & (X > 0)]\n            a[(X < 1) & (X > 0)] = 1/(x**2-1)*(1-2/np.sqrt(1-x**2)*np.arctanh(np.sqrt((1-x)/(1+x))))\n\n            a[X == 1] = 1./3.\n\n            x = X[X > 1]\n            a[X > 1] = 1/(x**2-1)*(1-2/np.sqrt(x**2-1)*np.arctan(np.sqrt((x-1)/(1+x))))\n            # a[X>y] = 0\n\n            c = 0.0000001\n            a[X == 0] = 1/(-1)*(1-2/np.sqrt(1)*np.arctanh(np.sqrt((1-c)/(1+c))))\n        return a", "response": "F is the analytic solution of the projection integral"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef g_(self, X):\n        if self._interpol:\n            if not hasattr(self, '_g_interp'):\n\n                if self._lookup:\n                    x = self._x_lookup\n                    g_x = self._g_lookup\n                else:\n                    x = np.linspace(0, self._max_interp_X, self._num_interp_X)\n                    g_x = self._g(x)\n                self._g_interp = interp.interp1d(x, g_x, kind='linear', axis=-1, copy=False, bounds_error=False,\n                                                 fill_value=0, assume_sorted=True)\n            return self._g_interp(X)\n        else:\n            return self._g(X)", "response": "computes the h function for the current state of the object"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert angle at Rs into rho0", "response": "def _alpha2rho0(self, theta_Rs, Rs):\n\n        \"\"\"\n        convert angle at Rs into rho0\n        \"\"\"\n\n        rho0 = theta_Rs / (4. * Rs ** 2 * (1. + np.log(1. / 2.)))\n        return rho0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rho02alpha(self, rho0, Rs):\n\n        \"\"\"\n        convert rho0 to angle at Rs\n\n        :param rho0:\n        :param Rs:\n        :return:\n        \"\"\"\n\n        theta_Rs = rho0 * (4 * Rs ** 2 * (1 + np.log(1. / 2.)))\n        return theta_Rs", "response": "convert rho0 to angle at Rs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef param_name_list(self):\n        name_list = []\n        for func in self.func_list:\n            name_list.append(func.param_names)\n        return name_list", "response": "returns the list of all parameter names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the surface brightness of the image", "response": "def surface_brightness(self, x, y, kwargs_list, k=None):\n        \"\"\"\n        :param x: coordinate in units of arcsec relative to the center of the image\n        :type x: set or single 1d numpy array\n        \"\"\"\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        flux = np.zeros_like(x)\n        for i, func in enumerate(self.func_list):\n            if k is None or k == i:\n                out = np.array(func.function(x, y, **kwargs_list[i]), dtype=float)\n                flux += out\n        return flux"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef light_3d(self, r, kwargs_list, k=None):\n        r = np.array(r, dtype=float)\n        flux = np.zeros_like(r)\n        for i, func in enumerate(self.func_list):\n            if k is None or k == i:\n                kwargs = {k: v for k, v in kwargs_list[i].items() if not k in ['center_x', 'center_y']}\n                if self.profile_type_list[i] in ['HERNQUIST', 'HERNQUIST_ELLIPSE', 'PJAFFE', 'PJAFFE_ELLIPSE',\n                                                     'GAUSSIAN', 'GAUSSIAN_ELLIPSE', 'MULTI_GAUSSIAN',\n                                                     'MULTI_GAUSSIAN_ELLIPSE', 'POWER_LAW']:\n                    flux += func.light_3d(r, **kwargs)\n                else:\n                    raise ValueError('Light model %s does not support a 3d light distribution!'\n                                         % self.profile_type_list[i])\n        return flux", "response": "compute 3d density at radius r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef total_flux(self, kwargs_list, norm=False, k=None):\n        norm_flux_list = []\n        for i, model in enumerate(self.profile_type_list):\n            if k is None or k == i:\n                if model in ['SERSIC', 'SERSIC_ELLIPSE', 'INTERPOL', 'GAUSSIAN', 'GAUSSIAN_ELLIPSE',\n                             'MULTI_GAUSSIAN', 'MULTI_GAUSSIAN_ELLIPSE']:\n                    kwargs_new = kwargs_list[i].copy()\n                    if norm is True:\n                        if model in ['MULTI_GAUSSIAN', 'MULTI_GAUSSIAN_ELLIPSE']:\n                            new = {'amp': np.array(kwargs_new['amp'])/kwargs_new['amp'][0]}\n                        else:\n                            new = {'amp': 1}\n                        kwargs_new.update(new)\n                    norm_flux = self.func_list[i].total_flux(**kwargs_new)\n                    norm_flux_list.append(norm_flux)\n                else:\n                    raise ValueError(\"profile %s does not support flux normlization.\" % model)\n                #  TODO implement total flux for e.g. 'HERNQUIST', 'HERNQUIST_ELLIPSE', 'PJAFFE', 'PJAFFE_ELLIPSE',\n                    # 'GAUSSIAN', 'GAUSSIAN_ELLIPSE', 'POWER_LAW', 'NIE', 'CHAMELEON', 'DOUBLE_CHAMELEON', 'UNIFORM'\n        return norm_flux_list", "response": "Compute the total flux of each individual light profile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef k_bn(self, n, Re):\n        bn = self.b_n(n)\n        k = bn*Re**(-1./n)\n        return k, bn", "response": "returns the k and bn of the sersic profile such that Re is the half light radius given n_sersic slope\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the reduced x - axis", "response": "def _x_reduced(self, x, y, n_sersic, r_eff, center_x, center_y):\n        \"\"\"\n        coordinate transform to normalized radius\n        :param x:\n        :param y:\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        if isinstance(r, int) or isinstance(r, float):\n            r = max(self._s, r)\n        else:\n            r[r < self._s] = self._s\n        x_reduced = (r/r_eff)**(1./n_sersic)\n        return x_reduced"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the alpha effect of a set of sersic entries at a given deflection angle at r_eff.", "response": "def _alpha_eff(self, r_eff, n_sersic, k_eff):\n        \"\"\"\n        deflection angle at r_eff\n        :param r_eff:\n        :param n_sersic:\n        :param k_eff:\n        :return:\n        \"\"\"\n        b = self.b_n(n_sersic)\n        alpha_eff = n_sersic * r_eff * k_eff * b**(-2*n_sersic) * np.exp(b) * special.gamma(2*n_sersic)\n        return -alpha_eff"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef density(self, x, y, n_sersic, r_eff, k_eff, center_x=0, center_y=0):\n        raise ValueError(\"not implemented! Use a Multi-Gaussian-component decomposition.\")", "response": "This method computes the density of the Sersic profile at the given coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _total_flux(self, r_eff, I_eff, n_sersic):\n        bn = self.b_n(n_sersic)\n        return I_eff * r_eff**2 * 2 * np.pi * n_sersic * np.exp(bn) / bn**(2*n_sersic) * scipy.special.gamma(2*n_sersic)", "response": "Compute the total flux of a Sersic profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef function(self, x, y, theta_E, gamma, center_x=0, center_y=0):\n        gamma = self._gamma_limit(gamma)\n\n        x_ = x - center_x\n        y_ = y - center_y\n        E = theta_E / ((3. - gamma) / 2.) ** (1. / (1. - gamma))\n        # E = phi_E_spp\n        eta= -gamma + 3\n\n        p2 = x_**2+y_**2\n        s2 = 0. # softening\n        return 2 * E**2/eta**2 * ((p2 + s2)/E**2)**(eta/2)", "response": "Function that calculates the function of the lense logarithmic logarithm."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting 3d density into 2d projected density parameter", "response": "def rho2theta(self, rho0, gamma):\n        \"\"\"\n        converts 3d density into 2d projected density parameter\n        :param rho0:\n        :param gamma:\n        :return:\n        \"\"\"\n        fac = np.sqrt(np.pi) * special.gamma(1. / 2 * (-1 + gamma)) / special.gamma(gamma / 2.) * 2 / (3 - gamma) * rho0\n\n        #fac = theta_E**(gamma - 1)\n        theta_E = fac**(1. / (gamma - 1))\n        return theta_E"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef theta2rho(self, theta_E, gamma):\n        fac1 = np.sqrt(np.pi) * special.gamma(1. / 2 * (-1 + gamma)) / special.gamma(gamma / 2.) * 2 / (3 - gamma)\n        fac2 = theta_E**(gamma - 1)\n        rho0 = fac2 / fac1\n        return rho0", "response": "converts projected density parameter into 3d density parameter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mass_3d(self, r, rho0, gamma):\n        mass_3d = 4 * np.pi * rho0 /(-gamma + 3) * r ** (-gamma + 3)\n        return mass_3d", "response": "calculate the mass of a 3d sphere or radius r"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the mass of a 2d sphere of radius r", "response": "def mass_2d(self, r, rho0, gamma):\n        \"\"\"\n        mass enclosed projected 2d sphere of radius r\n        :param r:\n        :param rho0:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        alpha = np.sqrt(np.pi) * special.gamma(1. / 2 * (-1 + gamma)) / special.gamma(gamma / 2.) * r ** (2 - gamma)/(3 - gamma) *np.pi * 2 * rho0\n        mass_2d = alpha*r\n        return mass_2d"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the density of the n - tuple", "response": "def density(self, r, rho0, gamma):\n        \"\"\"\n        computes the density\n        :param x:\n        :param y:\n        :param rho0:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        rho = rho0 / r**gamma\n        return rho"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef density_2d(self, x, y, rho0, gamma, center_x=0, center_y=0):\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        sigma = np.sqrt(np.pi) * special.gamma(1./2*(-1+gamma))/special.gamma(gamma/2.) * r**(1-gamma) * rho0\n        return sigma", "response": "projected density of a 2D logarithm"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the averaged LOS velocity dispersion in the slit", "response": "def vel_disp(self, kwargs_mass, kwargs_light, kwargs_anisotropy, kwargs_apertur):\n        \"\"\"\n        computes the averaged LOS velocity dispersion in the slit (convolved)\n\n        :param kwargs_mass: mass model parameters (following lenstronomy lens model conventions)\n        :param kwargs_light: deflector light parameters (following lenstronomy light model conventions)\n        :param kwargs_anisotropy: anisotropy parameters, may vary according to anisotropy type chosen.\n            We refer to the Anisotropy() class for details on the parameters.\n        :param kwargs_apertur: Aperture parameters, may vary depending on aperture type chosen.\n            We refer to the Aperture() class for details on the parameters.\n        :return: integrated LOS velocity dispersion in units [km/s]\n        \"\"\"\n        sigma2_R_sum = 0\n        for i in range(0, self._num_sampling):\n            sigma2_R = self.draw_one_sigma2(kwargs_mass, kwargs_light, kwargs_anisotropy, kwargs_apertur)\n            sigma2_R_sum += sigma2_R\n        sigma_s2_average = sigma2_R_sum / self._num_sampling\n        # apply unit conversion from arc seconds and deflections to physical velocity disperison in (km/s)\n        sigma_s2_average *= 2 * const.G  # correcting for integral prefactor\n        return np.sqrt(sigma_s2_average/(const.arcsec**2 * self.cosmo.D_d**2 * const.Mpc))/1000."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sigma2_R(self, R, kwargs_mass, kwargs_light, kwargs_anisotropy):\n        I_R_sigma2 = self.I_R_simga2(R, kwargs_mass, kwargs_light, kwargs_anisotropy)\n        I_R = self.lightProfile.light_2d(R, kwargs_light)\n        return I_R_sigma2 / I_R", "response": "returns unweighted los velocity dispersion for a specified projected radius"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns df x y of the function", "response": "def derivatives(self, x, y, n_sersic, R_sersic, k_eff, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        if isinstance(r, int) or isinstance(r, float):\n            r = max(self._s, r)\n        else:\n            r[r < self._s] = self._s\n        alpha = -self.alpha_abs(x, y, n_sersic, R_sersic, k_eff, center_x, center_y)\n        f_x = alpha * x_ / r\n        f_y = alpha * y_ / r\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derivatives(self, x, y, theta_E, s, q):\n        if q >= 1:\n            q = 0.999999\n        psi = self._psi(x, y, q, s)\n        f_x = theta_E / np.sqrt(1. - q ** 2) * np.arctan(np.sqrt(1. - q ** 2) * x / (psi+s))\n        f_y = theta_E / np.sqrt(1. - q ** 2) * np.arctanh(np.sqrt(1. - q ** 2) * y / (psi + q**2*s))\n        return f_x, f_y", "response": "returns df x y of the function\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning Hessian matrix of function d^2f x y theta_E s q", "response": "def hessian(self, x, y, theta_E, s, q):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        alpha_ra, alpha_dec = self.derivatives(x, y, theta_E, s, q)\n        diff = self._diff\n        alpha_ra_dx, alpha_dec_dx = self.derivatives(x + diff, y, theta_E, s, q)\n        alpha_ra_dy, alpha_dec_dy = self.derivatives(x, y + diff, theta_E, s, q)\n\n        f_xx = (alpha_ra_dx - alpha_ra) / diff\n        f_xy = (alpha_ra_dy - alpha_ra) / diff\n        # f_yx = (alpha_dec_dx - alpha_dec)/diff\n        f_yy = (alpha_dec_dy - alpha_dec) / diff\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _psi(self, x, y, q, s):\n        return np.sqrt(q**2 * (s**2 + x**2) + y**2)", "response": "expression after equation (8) in Keeton&Kochanek 1998\n\n        :param x:\n        :param y:\n        :param q:\n        :param s:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef function(self, x, y, kappa_ext, ra_0=0, dec_0=0):\n        theta, phi = param_util.cart2polar(x - ra_0, y - dec_0)\n        f_ = 1./2 * kappa_ext * theta**2\n        return f_", "response": "calculate the function of the convergence set"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the derivatives of the convergence species", "response": "def derivatives(self, x, y, kappa_ext, ra_0=0, dec_0=0):\n        \"\"\"\n        deflection angle\n\n        :param x: x-coordinate\n        :param y: y-coordinate\n        :param kappa_ext: external convergence\n        :return: deflection angles (first order derivatives)\n        \"\"\"\n        x_ = x - ra_0\n        y_ = y - dec_0\n        f_x = kappa_ext * x_\n        f_y = kappa_ext * y_\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hessian(self, x, y, kappa_ext, ra_0=0, dec_0=0):\n        gamma1 = 0\n        gamma2 = 0\n        kappa = kappa_ext\n        f_xx = kappa + gamma1\n        f_yy = kappa - gamma1\n        f_xy = gamma2\n        return f_xx, f_yy, f_xy", "response": "Hessian matrix for the current convergence set."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map_coord2pix(ra, dec, x_0, y_0, M):\n    x, y = M.dot(np.array([ra, dec]))\n    return x + x_0, y + y_0", "response": "This routine maps a coordinate system to a pixel coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a 1d array into an n * n 2d array", "response": "def array2image(array, nx=0, ny=0):\n    \"\"\"\n    returns the information contained in a 1d array into an n*n 2d array (only works when lenght of array is n**2)\n\n    :param array: image values\n    :type array: array of size n**2\n    :returns:  2d array\n    :raises: AttributeError, KeyError\n    \"\"\"\n    if nx == 0 or ny == 0:\n        n = int(np.sqrt(len(array)))\n        if n**2 != len(array):\n            raise ValueError(\"lenght of input array given as %s is not square of integer number!\" %(len(array)))\n        nx, ny = n, n\n    image = array.reshape(int(nx), int(ny))\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef image2array(image):\n    nx, ny = image.shape  # find the size of the array\n    imgh = np.reshape(image, nx*ny)  # change the shape to be 1d\n    return imgh", "response": "returns the information contained in a 2d array into an n 1d array"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake a grid with linear transformation", "response": "def make_grid_transformed(numPix, Mpix2Angle):\n    \"\"\"\n    returns grid with linear transformation (deltaPix and rotation)\n    :param numPix: number of Pixels\n    :param Mpix2Angle: 2-by-2 matrix to mat a pixel to a coordinate\n    :return: coordinate grid\n    \"\"\"\n    x_grid, y_grid = make_grid(numPix, deltapix=1)\n    ra_grid, dec_grid = map_coord2pix(x_grid, y_grid, 0, 0, Mpix2Angle)\n    return ra_grid, dec_grid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef grid_from_coordinate_transform(numPix, Mpix2coord, ra_at_xy_0, dec_at_xy_0):\n    a = np.arange(numPix)\n    matrix = np.dstack(np.meshgrid(a, a)).reshape(-1, 2)\n    x_grid = matrix[:, 0]\n    y_grid = matrix[:, 1]\n    ra_grid = x_grid * Mpix2coord[0, 0] + y_grid * Mpix2coord[0, 1] + ra_at_xy_0\n    dec_grid = x_grid * Mpix2coord[1, 0] + y_grid * Mpix2coord[1, 1] + dec_at_xy_0\n    return ra_grid, dec_grid", "response": "return a grid in x and y coordinates that satisfy the coordinate system"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the axes x and y of a given 2d grid", "response": "def get_axes(x, y):\n    \"\"\"\n    computes the axis x and y of a given 2d grid\n    :param x:\n    :param y:\n    :return:\n    \"\"\"\n    n=int(np.sqrt(len(x)))\n    if n**2 != len(x):\n        raise ValueError(\"lenght of input array given as %s is not square of integer number!\" % (len(x)))\n    x_image = x.reshape(n,n)\n    y_image = y.reshape(n,n)\n    x_axes = x_image[0,:]\n    y_axes = y_image[:,0]\n    return x_axes, y_axes"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef averaging(grid, numGrid, numPix):\n\n    Nbig = numGrid\n    Nsmall = numPix\n    small = grid.reshape([int(Nsmall), int(Nbig/Nsmall), int(Nsmall), int(Nbig/Nsmall)]).mean(3).mean(1)\n    return small", "response": "resizes 2d pixel grid with numGrid to numPix and averages over the pixels"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef displaceAbs(x, y, sourcePos_x, sourcePos_y):\n    x_mapped = x - sourcePos_x\n    y_mapped = y - sourcePos_y\n    absmapped = np.sqrt(x_mapped**2+y_mapped**2)\n    return absmapped", "response": "Displace the observer in angel\n    by the absolute distance of the observer in sourcePos_x and sourcePos_y."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef min_square_dist(x_1, y_1, x_2, y_2):\n    dist = np.zeros_like(x_1)\n    for i in range(len(x_1)):\n        dist[i] = np.min((x_1[i] - x_2)**2 + (y_1[i] - y_2)**2)\n    return dist", "response": "returns minimum of quadratic distance of pairs x1 y1 to pairs x2 y2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef points_on_circle(radius, points):\n    angle = np.linspace(0, 2*np.pi, points)\n    x_coord = np.cos(angle)*radius\n    y_coord = np.sin(angle)*radius\n    return x_coord, y_coord", "response": "Returns a set of uniform points around a circle"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind local minima in a 2d grid and returns the indices of local minima that are in the source positions x y", "response": "def neighborSelect(a, x, y):\n    \"\"\"\n    finds (local) minima in a 2d grid\n\n    :param a: 1d array of displacements from the source positions\n    :type a: numpy array with length numPix**2 in float\n    :returns:  array of indices of local minima, values of those minima\n    :raises: AttributeError, KeyError\n    \"\"\"\n    dim = int(np.sqrt(len(a)))\n    values = []\n    x_mins = []\n    y_mins = []\n    for i in range(dim+1,len(a)-dim-1):\n        if (a[i] < a[i-1]\n            and a[i] < a[i+1]\n            and a[i] < a[i-dim]\n            and a[i] < a[i+dim]\n            and a[i] < a[i-(dim-1)]\n            and a[i] < a[i-(dim+1)]\n            and a[i] < a[i+(dim-1)]\n            and a[i] < a[i+(dim+1)]):\n                if(a[i] < a[(i-2*dim-1)%dim**2]\n                    and a[i] < a[(i-2*dim+1)%dim**2]\n                    and a[i] < a[(i-dim-2)%dim**2]\n                    and a[i] < a[(i-dim+2)%dim**2]\n                    and a[i] < a[(i+dim-2)%dim**2]\n                    and a[i] < a[(i+dim+2)%dim**2]\n                    and a[i] < a[(i+2*dim-1)%dim**2]\n                    and a[i] < a[(i+2*dim+1)%dim**2]):\n                    if(a[i] < a[(i-3*dim-1)%dim**2]\n                        and a[i] < a[(i-3*dim+1)%dim**2]\n                        and a[i] < a[(i-dim-3)%dim**2]\n                        and a[i] < a[(i-dim+3)%dim**2]\n                        and a[i] < a[(i+dim-3)%dim**2]\n                        and a[i] < a[(i+dim+3)%dim**2]\n                        and a[i] < a[(i+3*dim-1)%dim**2]\n                        and a[i] < a[(i+3*dim+1)%dim**2]):\n                        x_mins.append(x[i])\n                        y_mins.append(y[i])\n                        values.append(a[i])\n    return np.array(x_mins), np.array(y_mins), np.array(values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_subgrid(ra_coord, dec_coord, subgrid_res=2):\n    ra_array = array2image(ra_coord)\n    dec_array = array2image(dec_coord)\n    n = len(ra_array)\n    d_ra_x = ra_array[0][1] - ra_array[0][0]\n    d_ra_y = ra_array[1][0] - ra_array[0][0]\n    d_dec_x = dec_array[0][1] - dec_array[0][0]\n    d_dec_y = dec_array[1][0] - dec_array[0][0]\n\n    ra_array_new = np.zeros((n*subgrid_res, n*subgrid_res))\n    dec_array_new = np.zeros((n*subgrid_res, n*subgrid_res))\n    for i in range(0, subgrid_res):\n        for j in range(0, subgrid_res):\n            ra_array_new[i::subgrid_res, j::subgrid_res] = ra_array + d_ra_x * (-1/2. + 1/(2.*subgrid_res) + j/float(subgrid_res)) + d_ra_y * (-1/2. + 1/(2.*subgrid_res) + i/float(subgrid_res))\n            dec_array_new[i::subgrid_res, j::subgrid_res] = dec_array + d_dec_x * (-1/2. + 1/(2.*subgrid_res) + j/float(subgrid_res)) + d_dec_y * (-1/2. + 1/(2.*subgrid_res) + i/float(subgrid_res))\n\n    ra_coords_sub = image2array(ra_array_new)\n    dec_coords_sub = image2array(dec_array_new)\n    return ra_coords_sub, dec_coords_sub", "response": "make a subgrid of a single site"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction that calculates the function of the lense logarithmic mass", "response": "def function(self, x, y, theta_E, gamma, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        :param x: set of x-coordinates\n        :type x: array of size (n)\n        :param theta_E: Einstein radius of lense\n        :type theta_E: float.\n        :param gamma: power law slope of mass profifle\n        :type gamma: <2 float\n        :param q: Axis ratio\n        :type q: 0<q<1\n        :param phi_G: position angel of SES\n        :type q: 0<phi_G<pi/2\n        :returns:  function\n        :raises: AttributeError, KeyError\n        \"\"\"\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        gamma, q = self._param_bounds(gamma, q)\n        theta_E *= q\n        x_shift = x - center_x\n        y_shift = y - center_y\n        E = theta_E / (((3 - gamma) / 2.) ** (1. / (1 - gamma)) * np.sqrt(q))\n        #E = phi_E\n        eta = -gamma+3\n        xt1 = np.cos(phi_G)*x_shift+np.sin(phi_G)*y_shift\n        xt2 = -np.sin(phi_G)*x_shift+np.cos(phi_G)*y_shift\n        p2 = xt1**2+xt2**2/q**2\n        s2 = 0. # softening\n        return 2 * E**2/eta**2 * ((p2 + s2)/E**2)**(eta/2)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mass_3d_lens(self, r, theta_E, gamma, e1, e2):\n        return self.spp.mass_3d_lens(r, theta_E, gamma)", "response": "computes the spherical power - law mass enclosed with SPP routiune"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _param_bounds(self, gamma, q):\n        if gamma < 1.4:\n            gamma = 1.4\n        if gamma > 2.9:\n            gamma = 2.9\n        if q < 0.01:\n            q = 0.01\n        return float(gamma), q", "response": "get the parameter bounds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntest whether the image positions map back to the same source position", "response": "def solver_penalty(self, kwargs_lens, kwargs_ps, kwargs_cosmo, tolerance):\n        \"\"\"\n        test whether the image positions map back to the same source position\n        :param kwargs_lens:\n        :param kwargs_ps:\n        :return: add penalty when solver does not find a solution\n        \"\"\"\n        dist = self._param.check_solver(kwargs_lens, kwargs_ps, kwargs_cosmo)\n        if dist > tolerance:\n            return dist * 10**10\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_additional_images(self, kwargs_ps, kwargs_lens):\n        ra_image_list, dec_image_list = self._pointSource.image_position(kwargs_ps=kwargs_ps, kwargs_lens=kwargs_lens)\n        if len(ra_image_list) > 0:\n            if len(ra_image_list[0]) > self._param.num_point_source_images:\n                return True\n        return False", "response": "checks whether additional images have been found and placed in kwargs_ps\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfunction that computes the Sersic profile for a specific set of sources.", "response": "def function(self, x, y, amp, R_sersic, n_sersic, center_x=0, center_y=0):\n        \"\"\"\n        returns Sersic profile\n        \"\"\"\n        #if n_sersic < 0.2:\n        #    n_sersic = 0.2\n        #if R_sersic < 10.**(-6):\n        #    R_sersic = 10.**(-6)\n        R_sersic = np.maximum(0, R_sersic)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        R = np.sqrt(x_shift*x_shift + y_shift*y_shift)\n        if isinstance(R, int) or isinstance(R, float):\n            R = max(self._smoothing, R)\n        else:\n            R[R < self._smoothing] = self._smoothing\n        _, bn = self.k_bn(n_sersic, R_sersic)\n        R_frac = R/R_sersic\n        #R_frac = R_frac.astype(np.float32)\n        if isinstance(R, int) or isinstance(R, float):\n            if R_frac > 100:\n                result = 0\n            else:\n                exponent = -bn*(R_frac**(1./n_sersic)-1.)\n                result = amp * np.exp(exponent)\n        else:\n            R_frac_real = R_frac[R_frac <= 100]\n            exponent = -bn*(R_frac_real**(1./n_sersic)-1.)\n            result = np.zeros_like(R)\n            result[R_frac <= 100] = amp * np.exp(exponent)\n        return np.nan_to_num(result)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction that computes the Sersic profile for a specific ephemeris system.", "response": "def function(self, x, y, amp, R_sersic, n_sersic, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        returns Sersic profile\n        \"\"\"\n        #if n_sersic < 0.2:\n        #    n_sersic = 0.2\n        #if R_sersic < 10.**(-6):\n        #    R_sersic = 10.**(-6)\n        R_sersic = np.maximum(0, R_sersic)\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n\n        xt1 = cos_phi*x_shift+sin_phi*y_shift\n        xt2 = -sin_phi*x_shift+cos_phi*y_shift\n        xt2difq2 = xt2/(q*q)\n        R_ = np.sqrt(xt1*xt1+xt2*xt2difq2)\n        if isinstance(R_, int) or isinstance(R_, float):\n            R_ = max(self._smoothing, R_)\n        else:\n            R_[R_ < self._smoothing] = self._smoothing\n        k, bn = self.k_bn(n_sersic, R_sersic)\n        R_frac = R_/R_sersic\n        R_frac = R_frac.astype(np.float32)\n        if isinstance(R_, int) or isinstance(R_, float):\n            if R_frac > 100:\n                result = 0\n            else:\n                exponent = -bn*(R_frac**(1./n_sersic)-1.)\n                result = amp * np.exp(exponent)\n        else:\n            R_frac_real = R_frac[R_frac <= 100]\n            exponent = -bn*(R_frac_real**(1./n_sersic)-1.)\n            result = np.zeros_like(R_)\n            result[R_frac <= 100] = amp * np.exp(exponent)\n        return np.nan_to_num(result)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the function of the Core - Sersic system", "response": "def function(self, x, y, amp, R_sersic, Re, n_sersic, gamma, e1, e2, center_x=0, center_y=0, alpha=3.):\n        \"\"\"\n        returns Core-Sersic function\n        \"\"\"\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        Rb = R_sersic\n        x_shift = x - center_x\n        y_shift = y - center_y\n\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n\n        xt1 = cos_phi*x_shift+sin_phi*y_shift\n        xt2 = -sin_phi*x_shift+cos_phi*y_shift\n        xt2difq2 = xt2/(q*q)\n        R_ = np.sqrt(xt1*xt1+xt2*xt2difq2)\n        #R_ = R_.astype(np.float32)\n        if isinstance(R_, int) or isinstance(R_, float):\n            R_ = max(self._smoothing, R_)\n        else:\n            R_[R_ < self._smoothing] = self._smoothing\n        if isinstance(R_, int) or isinstance(R_, float):\n            R = max(self._smoothing, R_)\n        else:\n            R=np.empty_like(R_)\n            _R = R_[R_ > self._smoothing]  #in the SIS regime\n            R[R_ <= self._smoothing] = self._smoothing\n            R[R_ > self._smoothing] = _R\n\n        k, bn = self.k_bn(n_sersic, Re)\n        result = amp * (1 + (Rb / R) ** alpha) ** (gamma / alpha) * np.exp(-bn * (((R ** alpha + Rb ** alpha) / Re ** alpha) ** (1. / (alpha * n_sersic)) - 1.))\n        return np.nan_to_num(result)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn df x y of the function that is derivatives of the function that is at center_x and center_y", "response": "def derivatives(self, x, y, theta_E, r_trunc, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        x_shift = x - center_x\n        y_shift = y - center_y\n\n        dphi_dr = self._dphi_dr(x_shift, y_shift, theta_E, r_trunc)\n        dr_dx, dr_dy = self._dr_dx(x_shift, y_shift)\n        f_x = dphi_dr * dr_dx\n        f_y = dphi_dr * dr_dy\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning Hessian matrix of function d^2f x y theta_E r_trunc center_x center_y", "response": "def hessian(self, x, y, theta_E, r_trunc, center_x=0, center_y=0):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        x_shift = x - center_x\n        y_shift = y - center_y\n        dphi_dr = self._dphi_dr(x_shift, y_shift, theta_E, r_trunc)\n        d2phi_dr2 = self._d2phi_dr2(x_shift, y_shift, theta_E, r_trunc)\n        dr_dx, dr_dy = self._dr_dx(x, y)\n        d2r_dx2, d2r_dy2, d2r_dxy = self._d2r_dx2(x_shift, y_shift)\n        f_xx = d2r_dx2*dphi_dr + dr_dx**2*d2phi_dr2\n        f_yy = d2r_dy2*dphi_dr + dr_dy**2*d2phi_dr2\n        f_xy = d2r_dxy*dphi_dr + dr_dx*dr_dy*d2phi_dr2\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the function of the NFW profile", "response": "def function(self, x, y, sigma0, Rs, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        returns double integral of NFW profile\n        \"\"\"\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        e = abs(1 - q)\n        x_ = (cos_phi*x_shift+sin_phi*y_shift)*np.sqrt(1 - e)\n        y_ = (-sin_phi*x_shift+cos_phi*y_shift)*np.sqrt(1 + e)\n        f_ = self.spherical.function(x_, y_, sigma0, Rs)\n        return f_"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef derivatives(self, x, y, sigma0, Rs, e1, e2, center_x=0, center_y=0):\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        e = abs(1 - q)\n        x_ = (cos_phi*x_shift+sin_phi*y_shift)*np.sqrt(1 - e)\n        y_ = (-sin_phi*x_shift+cos_phi*y_shift)*np.sqrt(1 + e)\n\n        f_x_prim, f_y_prim = self.spherical.derivatives(x_, y_, sigma0, Rs)\n        f_x_prim *= np.sqrt(1 - e)\n        f_y_prim *= np.sqrt(1 + e)\n        f_x = cos_phi*f_x_prim-sin_phi*f_y_prim\n        f_y = sin_phi*f_x_prim+cos_phi*f_y_prim\n        return f_x, f_y", "response": "returns df x y of the function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _nfw_func(self, x):\n\n        c = 0.000001\n\n        if isinstance(x, np.ndarray):\n            x[np.where(x<c)] = c\n            nfwvals = np.ones_like(x)\n            inds1 = np.where(x < 1)\n            inds2 = np.where(x > 1)\n\n            nfwvals[inds1] = (1 - x[inds1] ** 2) ** -.5 * np.arctanh((1 - x[inds1] ** 2) ** .5)\n            nfwvals[inds2] = (x[inds2] ** 2 - 1) ** -.5 * np.arctan((x[inds2] ** 2 - 1) ** .5)\n\n            return nfwvals\n\n        elif isinstance(x, float) or isinstance(x, int):\n            x = max(x, c)\n            if x == 1:\n                return 1\n            if x < 1:\n                return (1 - x ** 2) ** -.5 * np.arctanh((1 - x ** 2) ** .5)\n            else:\n                return (x ** 2 - 1) ** -.5 * np.arctan((x ** 2 - 1) ** .5)", "response": "This function returns the Classic NFW function in terms of arctanh and arctanh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef density(self, R, Rs, rho0, r_core):\n\n        M0 = 4*np.pi*rho0 * Rs ** 3\n        return (M0/4/np.pi) * ((r_core + R)*(R + Rs)**2) ** -1", "response": "calculate the density of the NFW profile"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprojects two dimenstional NFW profile at radius R", "response": "def density_2d(self, x, y, Rs, rho0, r_core, center_x=0, center_y=0):\n        \"\"\"\n        projected two dimenstional NFW profile (kappa*Sigma_crit)\n\n        :param R: radius of interest\n        :type R: float/numpy array\n        :param Rs: scale radius\n        :type Rs: float\n        :param rho0: density normalization (characteristic density)\n        :type rho0: float\n        :param r200: radius of (sub)halo\n        :type r200: float>0\n        :return: Epsilon(R) projected density at radius R\n        \"\"\"\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_ ** 2 + y_ ** 2)\n        b = r_core * Rs ** -1\n        x = R * Rs ** -1\n        Fx = self._F(x, b)\n\n        return 2 * rho0 * Rs * Fx"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the mass of a 3d sphere or radius r", "response": "def mass_3d(self, R, Rs, rho0, r_core):\n        \"\"\"\n        mass enclosed a 3d sphere or radius r\n\n        :param r:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n        b = r_core * Rs ** -1\n        x = R * Rs ** -1\n\n        M_0 = 4 * np.pi * Rs**3 * rho0\n\n        return M_0 * (x * (1+x) ** -1 * (-1+b) ** -1 + (-1+b) ** -2 *\n                      ((2*b-1)*np.log(1/(1+x)) + b **2 * np.log(x / b + 1)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the alpha of the NFW profile at radius R", "response": "def cnfwAlpha(self, R, Rs, rho0, r_core, ax_x, ax_y):\n        \"\"\"\n        deflection angel of NFW profile along the projection to coordinate axis\n\n        :param R: radius of interest\n        :type R: float/numpy array\n        :param Rs: scale radius\n        :type Rs: float\n        :param rho0: density normalization (characteristic density)\n        :type rho0: float\n        :param r200: radius of (sub)halo\n        :type r200: float>0\n        :param axis: projection to either x- or y-axis\n        :type axis: same as R\n        :return: Epsilon(R) projected density at radius R\n        \"\"\"\n        if isinstance(R, int) or isinstance(R, float):\n            R = max(R, 0.00001)\n        else:\n            R[R <= 0.00001] = 0.00001\n\n        x = R / Rs\n        b = r_core * Rs ** -1\n        b = max(b, 0.000001)\n        gx = self._G(x, b)\n\n        a = 4*rho0*Rs*gx/x**2\n        return a * ax_x, a * ax_y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef function(self, x, y, Rs, theta_Rs, e1, e2, center_x=0, center_y=0):\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        e = min(abs(1. - q), 0.99)\n        xt1 = (cos_phi*x_shift+sin_phi*y_shift)*np.sqrt(1 - e)\n        xt2 = (-sin_phi*x_shift+cos_phi*y_shift)*np.sqrt(1 + e)\n        R_ = np.sqrt(xt1**2 + xt2**2)\n        rho0_input = self.nfw._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs)\n        if Rs < 0.0000001:\n            Rs = 0.0000001\n        f_ = self.nfw.nfwPot(R_, Rs, rho0_input)\n        return f_", "response": "function returns the function of the NFW profile"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns df x y and df y of the function", "response": "def derivatives(self, x, y, Rs, theta_Rs, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function (integral of NFW)\n        \"\"\"\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        x_shift = x - center_x\n        y_shift = y - center_y\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        e = min(abs(1. - q), 0.99)\n        xt1 = (cos_phi*x_shift+sin_phi*y_shift)*np.sqrt(1 - e)\n        xt2 = (-sin_phi*x_shift+cos_phi*y_shift)*np.sqrt(1 + e)\n        R_ = np.sqrt(xt1**2 + xt2**2)\n        rho0_input = self.nfw._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs)\n        if Rs < 0.0000001:\n            Rs = 0.0000001\n        f_x_prim, f_y_prim = self.nfw.nfwAlpha(R_, Rs, rho0_input, xt1, xt2)\n        f_x_prim *= np.sqrt(1 - e)\n        f_y_prim *= np.sqrt(1 + e)\n        f_x = cos_phi*f_x_prim-sin_phi*f_y_prim\n        f_y = sin_phi*f_x_prim+cos_phi*f_y_prim\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pso(self, n_particles=10, n_iterations=10, lowerLimit=-0.2, upperLimit=0.2, threadCount=1, mpi=False, print_key='default'):\n        init_pos = self.chain.get_args(self.chain.kwargs_data_init)\n        num_param = self.chain.num_param\n        lowerLimit = [lowerLimit] * num_param\n        upperLimit = [upperLimit] * num_param\n        if mpi is True:\n            pso = MpiParticleSwarmOptimizer(self.chain, lowerLimit, upperLimit, n_particles, threads=1)\n        else:\n            pso = ParticleSwarmOptimizer(self.chain, lowerLimit, upperLimit, n_particles, threads=threadCount)\n        if not init_pos is None:\n            pso.gbest.position = init_pos\n            pso.gbest.velocity = [0]*len(init_pos)\n            pso.gbest.fitness, _ = self.chain.likelihood(init_pos)\n        X2_list = []\n        vel_list = []\n        pos_list = []\n        time_start = time.time()\n        if pso.isMaster():\n            print('Computing the %s ...' % print_key)\n        num_iter = 0\n        for swarm in pso.sample(n_iterations):\n            X2_list.append(pso.gbest.fitness*2)\n            vel_list.append(pso.gbest.velocity)\n            pos_list.append(pso.gbest.position)\n            num_iter += 1\n            if pso.isMaster():\n                if num_iter % 10 == 0:\n                    print(num_iter)\n        if not mpi:\n            result = pso.gbest.position\n        else:\n            result = MpiUtil.mpiBCast(pso.gbest.position)\n        kwargs_data = self.chain.update_data(result)\n        if mpi is True and not pso.isMaster():\n            pass\n        else:\n            time_end = time.time()\n            print(\"Shifts found: \", result)\n            print(time_end - time_start, 'time used for PSO', print_key)\n        return kwargs_data, [X2_list, pos_list, vel_list, []]", "response": "Returns the best fit for the lense model on catalogue basis with particle swarm optimizer."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes likelihood for a MCMC chainF", "response": "def _likelihood(self, args):\n        \"\"\"\n        routine to compute X2 given variable parameters for a MCMC/PSO chainF\n        \"\"\"\n        #generate image and computes likelihood\n        kwargs_data = self.update_data(args)\n        imageModel = class_creator.create_image_model(kwargs_data, self._kwargs_psf, self._kwargs_numerics, self._kwargs_model)\n        logL = imageModel.likelihood_data_given_model(self._kwargs_lens, self._kwargs_source, self._kwargs_lens_light, self._kwargs_else, source_marg=self._source_marg)\n        return logL, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef optimize(self, n_particles=50, n_iterations=250, restart=1):\n\n        \"\"\"\n        the best result of all optimizations will be returned.\n        total number of lens models sovled: n_particles*n_iterations\n\n        :param n_particles: number of particle swarm particles\n        :param n_iterations: number of particle swarm iternations\n        :param restart: number of times to execute the optimization;\n\n        :return: lens model keywords, [optimized source position], best fit image positions\n        \"\"\"\n\n        if restart < 0:\n            raise ValueError(\"parameter 'restart' must be integer of value > 0\")\n\n        # particle swarm optimization\n        penalties, parameters, src_pen_best = [],[], []\n\n        for run in range(0, restart):\n\n            penalty, params = self._single_optimization(n_particles, n_iterations)\n            penalties.append(penalty)\n            parameters.append(params)\n            src_pen_best.append(self._optimizer.src_pen_best)\n\n        # select the best optimization\n        best_index = np.argmin(penalties)\n\n        # combine the optimized parameters with the parameters kept fixed during the optimization to obtain full kwargs_lens\n        kwargs_varied = self._params.argstovary_todictionary(parameters[best_index])\n        kwargs_lens_final = kwargs_varied + self._params.argsfixed_todictionary()\n\n        # solve for the optimized image positions\n        srcx, srcy = self._optimizer.lensing._ray_shooting_fast(kwargs_varied)\n        source_x, source_y = np.mean(srcx), np.mean(srcy)\n\n        # if we have a good enough solution, no point in recomputing the image positions since this can be quite slow\n        # and will give the same answer\n        if src_pen_best[best_index] < self._tol_src_penalty:\n            x_image, y_image = self.x_pos, self.y_pos\n        else:\n            # Here, the solver has the instance of \"lensing_class\" or \"LensModel\" for multiplane/singleplane respectively.\n            print('Warning: possibly a bad fit.')\n            x_image, y_image = self.solver.findBrightImage(source_x, source_y, kwargs_lens_final, arrival_time_sort=False)\n            #x_image, y_image = self.solver.image_position_from_source(source_x, source_y, kwargs_lens_final, arrival_time_sort = False)\n        if self._verbose:\n            print('optimization done.')\n            print('Recovered source position: ', (srcx, srcy))\n\n        return kwargs_lens_final, [source_x, source_y], [x_image, y_image]", "response": "This function performs a single optimization of the lens model and returns the best fit image positions and the best fit image keywords."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef de_projection_3d(amplitudes, sigmas):\n    amplitudes_3d = amplitudes / sigmas / np.sqrt(2*np.pi)\n    return amplitudes_3d, sigmas", "response": "de - projects a 2d Gaussians from a 2d projected to a 3d profile"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_pixel_size(self, deltaPix):\n        self._pixel_size = deltaPix\n        if self.psf_type == 'GAUSSIAN':\n            try:\n                del self._kernel_point_source\n            except:\n                pass", "response": "update the pixel size of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates center of mass", "response": "def com(self, center1_x, center1_y, center2_x, center2_y, Fm):\n        \"\"\"\n        :return: center of mass\n        \"\"\"\n        com_x = (Fm * center1_x + center2_x)/(Fm + 1.)\n        com_y = (Fm * center1_y + center2_y)/(Fm + 1.)\n        return com_x, com_y"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef angle(self, center1_x, center1_y, center2_x, center2_y):\n        phi_G = np.arctan2(center2_y - center1_y, center2_x - center1_x)\n        return phi_G", "response": "compute the rotation angle of the dipole"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logL(self, kwargs_lens, kwargs_ps, kwargs_cosmo):\n        x_pos, y_pos = self._pointSource.image_position(kwargs_ps=kwargs_ps, kwargs_lens=kwargs_lens)\n        x_pos, y_pos = self._param.real_image_positions(x_pos[0], y_pos[0], kwargs_cosmo)\n        x_source, y_source = self._lensModel.ray_shooting(x_pos, y_pos, kwargs_lens)\n        delay_arcsec = self._lensModel.fermat_potential(x_pos, y_pos, x_source, y_source, kwargs_lens)\n        D_dt_model = kwargs_cosmo['D_dt']\n        delay_days = const.delay_arcsec2days(delay_arcsec, D_dt_model)\n        logL = self._logL_delays(delay_days, self._delays_measured, self._delays_errors)\n        return logL", "response": "compute the log likelihood of the model given the time delay"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the log likelihood of modeled delays vs measured time delays under considerations of errors", "response": "def _logL_delays(self, delays_model, delays_measured, delays_errors):\n        \"\"\"\n        log likelihood of modeled delays vs measured time delays under considerations of errors\n\n        :param delays_model: n delays of the model (not relative delays)\n        :param delays_measured: relative delays (1-2,1-3,1-4) relative to the first in the list\n        :param delays_errors: gaussian errors on the measured delays\n        :return: log likelihood of data given model\n        \"\"\"\n        delta_t_model = np.array(delays_model[1:]) - delays_model[0]\n        logL = np.sum(-(delta_t_model - delays_measured) ** 2 / (2 * delays_errors ** 2))\n        return logL"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shift_coordinate_grid(self, x_shift, y_shift, pixel_unit=False):\n        if pixel_unit is True:\n            ra_shift, dec_shift = self.map_pix2coord(x_shift, y_shift)\n        else:\n            ra_shift, dec_shift = x_shift, y_shift\n        self._ra_at_xy_0 += ra_shift\n        self._dec_at_xy_0 += dec_shift\n        self._x_at_radec_0, self._y_at_radec_0 = util.map_coord2pix(-self._ra_at_xy_0, -self._dec_at_xy_0, 0, 0,\n                                                                    self._Ma2pix)", "response": "shifts the coordinate grid of the data class with the given shift in x and y"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts the array of parameters from kwargs_list", "response": "def _extract_array(self, kwargs_list):\n        \"\"\"\n        inverse of _update_kwargs\n        :param kwargs_list:\n        :return:\n        \"\"\"\n        if self._solver_type == 'PROFILE_SHEAR':\n            e1 = kwargs_list[1]['e1']\n            e2 = kwargs_list[1]['e2']\n            phi_ext, gamma_ext = param_util.ellipticity2phi_gamma(e1, e2)\n        else:\n            phi_ext = 0\n        lens_model = self._lens_mode_list[0]\n        if lens_model in ['SPEP', 'SPEMD', 'SIE', 'NIE']:\n            e1 = kwargs_list[0]['e1']\n            e2 = kwargs_list[0]['e2']\n            center_x = kwargs_list[0]['center_x']\n            center_y = kwargs_list[0]['center_y']\n            theta_E = kwargs_list[0]['theta_E']\n            x = [theta_E, e1, e2, center_x, center_y, phi_ext]\n        elif lens_model in ['NFW_ELLIPSE']:\n            e1 = kwargs_list[0]['e1']\n            e2 = kwargs_list[0]['e2']\n            center_x = kwargs_list[0]['center_x']\n            center_y = kwargs_list[0]['center_y']\n            theta_Rs = kwargs_list[0]['theta_Rs']\n            x = [theta_Rs, e1, e2, center_x, center_y, phi_ext]\n        elif lens_model in ['SHAPELETS_CART']:\n            coeffs = list(kwargs_list[0]['coeffs'])\n            [c10, c01, c20, c11, c02] = coeffs[1: 6]\n            x = [c10, c01, c20, c11, c02, phi_ext]\n        else:\n            raise ValueError(\"Lens model %s not supported for 4-point solver!\" % lens_model)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning df dx and dy of the function", "response": "def derivatives(self, x, y, grid_interp_x=None, grid_interp_y=None, f_=None, f_x=None, f_y=None, f_xx=None, f_yy=None, f_xy=None):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        #self._check_interp(grid_interp_x, grid_interp_y, f_, f_x, f_y, f_xx, f_yy, f_xy)\n        n = len(np.atleast_1d(x))\n        if n <= 1 and np.shape(x) == ():\n        #if type(x) == float or type(x) == int or type(x) == type(np.float64(1)) or len(x) <= 1:\n            f_x_out = self.f_x_interp(x, y, grid_interp_x, grid_interp_y, f_x)\n            f_y_out = self.f_y_interp(x, y, grid_interp_x, grid_interp_y, f_y)\n            return f_x_out[0][0], f_y_out[0][0]\n        else:\n            if self._grid and n >= self._min_grid_number:\n                x_, y_ = util.get_axes(x, y)\n                f_x_out = self.f_x_interp(x_, y_, grid_interp_x, grid_interp_y, f_x)\n                f_y_out = self.f_y_interp(x_, y_, grid_interp_x, grid_interp_y, f_y)\n                f_x_out = util.image2array(f_x_out)\n                f_y_out = util.image2array(f_y_out)\n            else:\n                #n = len(x)\n                f_x_out, f_y_out = np.zeros(n), np.zeros(n)\n                for i in range(n):\n                    f_x_out[i] = self.f_x_interp(x[i], y[i], grid_interp_x, grid_interp_y, f_x)\n                    f_y_out[i] = self.f_y_interp(x[i], y[i], grid_interp_x, grid_interp_y, f_y)\n        return f_x_out, f_y_out"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Hessian matrix of function f", "response": "def hessian(self, x, y, grid_interp_x=None, grid_interp_y=None, f_=None, f_x=None, f_y=None, f_xx=None, f_yy=None, f_xy=None):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        #self._check_interp(grid_interp_x, grid_interp_y, f_, f_x, f_y, f_xx, f_yy, f_xy)\n        n = len(np.atleast_1d(x))\n        if n <= 1 and np.shape(x) == ():\n        #if type(x) == float or type(x) == int or type(x) == type(np.float64(1)) or len(x) <= 1:\n            f_xx_out = self.f_xx_interp(x, y, grid_interp_x, grid_interp_y, f_xx)\n            f_yy_out = self.f_yy_interp(x, y, grid_interp_x, grid_interp_y, f_yy)\n            f_xy_out = self.f_xy_interp(x, y, grid_interp_x, grid_interp_y, f_xy)\n            return f_xx_out[0][0], f_yy_out[0][0], f_xy_out[0][0]\n        else:\n            if self._grid and n >= self._min_grid_number:\n                x_, y_ = util.get_axes(x, y)\n                f_xx_out = self.f_xx_interp(x_, y_, grid_interp_x, grid_interp_y, f_xx)\n                f_yy_out = self.f_yy_interp(x_, y_, grid_interp_x, grid_interp_y, f_yy)\n                f_xy_out = self.f_xy_interp(x_, y_, grid_interp_x, grid_interp_y, f_xy)\n                f_xx_out = util.image2array(f_xx_out)\n                f_yy_out = util.image2array(f_yy_out)\n                f_xy_out = util.image2array(f_xy_out)\n            else:\n                #n = len(x)\n                f_xx_out, f_yy_out, f_xy_out = np.zeros(n), np.zeros(n), np.zeros(n)\n                for i in range(n):\n                    f_xx_out[i] = self.f_xx_interp(x[i], y[i], grid_interp_x, grid_interp_y, f_xx)\n                    f_yy_out[i] = self.f_yy_interp(x[i], y[i], grid_interp_x, grid_interp_y, f_yy)\n                    f_xy_out[i] = self.f_xy_interp(x[i], y[i], grid_interp_x, grid_interp_y, f_xy)\n        return f_xx_out, f_yy_out, f_xy_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots a single lens model and the critical curves and caustics.", "response": "def lens_model_plot(ax, lensModel, kwargs_lens, numPix=500, deltaPix=0.01, sourcePos_x=0, sourcePos_y=0,\n                    point_source=False, with_caustics=False):\n    \"\"\"\n    plots a lens model (convergence) and the critical curves and caustics\n\n    :param ax:\n    :param kwargs_lens:\n    :param numPix:\n    :param deltaPix:\n    :return:\n    \"\"\"\n    kwargs_data = sim_util.data_configure_simple(numPix, deltaPix)\n    data = Data(kwargs_data)\n    _frame_size = numPix * deltaPix\n    _coords = data._coords\n    x_grid, y_grid = data.coordinates\n    lensModelExt = LensModelExtensions(lensModel)\n    #ra_crit_list, dec_crit_list, ra_caustic_list, dec_caustic_list = lensModelExt.critical_curve_caustics(\n    #    kwargs_lens, compute_window=_frame_size, grid_scale=deltaPix/2.)\n    x_grid1d = util.image2array(x_grid)\n    y_grid1d = util.image2array(y_grid)\n    kappa_result = lensModel.kappa(x_grid1d, y_grid1d, kwargs_lens)\n    kappa_result = util.array2image(kappa_result)\n    im = ax.matshow(np.log10(kappa_result), origin='lower', extent=[0, _frame_size, 0, _frame_size], cmap='Greys',\n                    vmin=-1, vmax=1) #, cmap=self._cmap, vmin=v_min, vmax=v_max)\n    if with_caustics is True:\n        ra_crit_list, dec_crit_list = lensModelExt.critical_curve_tiling(kwargs_lens, compute_window=_frame_size,\n                                                                         start_scale=deltaPix, max_order=10)\n        ra_caustic_list, dec_caustic_list = lensModel.ray_shooting(ra_crit_list, dec_crit_list, kwargs_lens)\n        plot_line_set(ax, _coords, ra_caustic_list, dec_caustic_list, color='g')\n        plot_line_set(ax, _coords, ra_crit_list, dec_crit_list, color='r')\n    if point_source:\n        from lenstronomy.LensModel.Solver.lens_equation_solver import LensEquationSolver\n        solver = LensEquationSolver(lensModel)\n        theta_x, theta_y = solver.image_position_from_source(sourcePos_x, sourcePos_y, kwargs_lens,\n                                                             min_distance=deltaPix, search_window=deltaPix*numPix)\n        mag_images = lensModel.magnification(theta_x, theta_y, kwargs_lens)\n        x_image, y_image = _coords.map_coord2pix(theta_x, theta_y)\n        abc_list = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K']\n        for i in range(len(x_image)):\n            x_ = (x_image[i] + 0.5) * deltaPix\n            y_ = (y_image[i] + 0.5) * deltaPix\n            ax.plot(x_, y_, 'dk', markersize=4*(1 + np.log(np.abs(mag_images[i]))), alpha=0.5)\n            ax.text(x_, y_, abc_list[i], fontsize=20, color='k')\n        x_source, y_source = _coords.map_coord2pix(sourcePos_x, sourcePos_y)\n        ax.plot((x_source + 0.5) * deltaPix, (y_source + 0.5) * deltaPix, '*k', markersize=10)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    ax.autoscale(False)\n    #image_position_plot(ax, _coords, self._kwargs_else)\n    #source_position_plot(ax, self._coords, self._kwargs_source)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_mcmc_behaviour(ax, samples_mcmc, param_mcmc, dist_mcmc, num_average=100):\n    num_samples = len(samples_mcmc[:, 0])\n    num_average = int(num_average)\n    n_points = int((num_samples - num_samples % num_average) / num_average)\n    for i, param_name in enumerate(param_mcmc):\n        samples = samples_mcmc[:, i]\n        samples_averaged = np.average(samples[:int(n_points * num_average)].reshape(n_points, num_average), axis=1)\n        end_point = np.mean(samples_averaged)\n        samples_renormed = (samples_averaged - end_point) / np.std(samples_averaged)\n        ax.plot(samples_renormed, label=param_name)\n\n    dist_averaged = -np.max(dist_mcmc[:int(n_points * num_average)].reshape(n_points, num_average), axis=1)\n    dist_normed = (dist_averaged - np.max(dist_averaged)) / (np.max(dist_averaged) - np.min(dist_averaged))\n    ax.plot(dist_normed, label=\"logL\", color='k', linewidth=2)\n    ax.legend()\n    return ax", "response": "Plots the MCMC behaviour and looks for convergence of the chain\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the main plots together in a joint frame", "response": "def plot_main(self, with_caustics=False, image_names=False):\n        \"\"\"\n        print the main plots together in a joint frame\n\n        :return:\n        \"\"\"\n\n        f, axes = plt.subplots(2, 3, figsize=(16, 8))\n        self.data_plot(ax=axes[0, 0])\n        self.model_plot(ax=axes[0, 1], image_names=True)\n        self.normalized_residual_plot(ax=axes[0, 2], v_min=-6, v_max=6)\n        self.source_plot(ax=axes[1, 0], deltaPix_source=0.01, numPix=100, with_caustics=with_caustics)\n        self.convergence_plot(ax=axes[1, 1], v_max=1)\n        self.magnification_plot(ax=axes[1, 2])\n        f.tight_layout()\n        f.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0., hspace=0.05)\n        return f, axes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_separate(self):\n        f, axes = plt.subplots(2, 3, figsize=(16, 8))\n\n        self.decomposition_plot(ax=axes[0, 0], text='Lens light', lens_light_add=True, unconvolved=True)\n        self.decomposition_plot(ax=axes[1, 0], text='Lens light convolved', lens_light_add=True)\n        self.decomposition_plot(ax=axes[0, 1], text='Source light', source_add=True, unconvolved=True)\n        self.decomposition_plot(ax=axes[1, 1], text='Source light convolved', source_add=True)\n        self.decomposition_plot(ax=axes[0, 2], text='All components', source_add=True, lens_light_add=True,\n                                    unconvolved=True)\n        self.decomposition_plot(ax=axes[1, 2], text='All components convolved', source_add=True,\n                                    lens_light_add=True, point_source_add=True)\n        f.tight_layout()\n        f.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0., hspace=0.05)\n        return f, axes", "response": "plot the different model components separately"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubtract model components from data", "response": "def plot_subtract_from_data_all(self):\n        \"\"\"\n        subtract model components from data\n\n        :return:\n        \"\"\"\n        f, axes = plt.subplots(2, 3, figsize=(16, 8))\n\n        self.subtract_from_data_plot(ax=axes[0, 0], text='Data')\n        self.subtract_from_data_plot(ax=axes[0, 1], text='Data - Point Source', point_source_add=True)\n        self.subtract_from_data_plot(ax=axes[0, 2], text='Data - Lens Light', lens_light_add=True)\n        self.subtract_from_data_plot(ax=axes[1, 0], text='Data - Source Light', source_add=True)\n        self.subtract_from_data_plot(ax=axes[1, 1], text='Data - Source Light - Point Source', source_add=True,\n                                         point_source_add=True)\n        self.subtract_from_data_plot(ax=axes[1, 2], text='Data - Lens Light - Point Source', lens_light_add=True,\n                                         point_source_add=True)\n        f.tight_layout()\n        f.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0., hspace=0.05)\n        return f, axes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef derivatives(self, x, y, coeffs, beta, center_x=0, center_y=0):\n        shapelets = self._createShapelet(coeffs)\n        r, phi = param_util.cart2polar(x, y, center=np.array([center_x, center_y]))\n        alpha1_shapelets, alpha2_shapelets = self._alphaShapelets(shapelets, beta)\n        f_x = self._shapeletOutput(r, phi, beta, alpha1_shapelets)\n        f_y = self._shapeletOutput(r, phi, beta, alpha2_shapelets)\n        return f_x, f_y", "response": "returns df dx dy of the function\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns Hessian matrix of function d^2f / dx^2 y d^2f / dy^2", "response": "def hessian(self, x, y, coeffs, beta, center_x=0, center_y=0):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        shapelets = self._createShapelet(coeffs)\n        r, phi = param_util.cart2polar(x, y, center=np.array([center_x, center_y]))\n        kappa_shapelets=self._kappaShapelets(shapelets, beta)\n        gamma1_shapelets, gamma2_shapelets=self._gammaShapelets(shapelets, beta)\n        kappa_value=self._shapeletOutput(r, phi, beta, kappa_shapelets)\n        gamma1_value=self._shapeletOutput(r, phi, beta, gamma1_shapelets)\n        gamma2_value=self._shapeletOutput(r, phi, beta, gamma2_shapelets)\n        f_xx = kappa_value + gamma1_value\n        f_xy = gamma2_value\n        f_yy = kappa_value - gamma1_value\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a shapelet array out of the coefficients a and b.", "response": "def _createShapelet(self,coeff):\n        \"\"\"\n        returns a shapelet array out of the coefficients *a, up to order l\n\n        :param num_l: order of shapelets\n        :type num_l: int.\n        :param coeff: shapelet coefficients\n        :type coeff: floats\n        :returns:  complex array\n        :raises: AttributeError, KeyError\n        \"\"\"\n        n_coeffs = len(coeff)\n        num_l = self._get_num_l(n_coeffs)\n        shapelets=np.zeros((num_l+1,num_l+1),'complex')\n        nl=0\n        k=0\n        i=0\n        while i < len(coeff):\n            if i%2==0:\n                shapelets[nl][k]+=coeff[i]/2.\n                shapelets[k][nl]+=coeff[i]/2.\n                if k==nl:\n                    nl+=1\n                    k=0\n                    i+=1\n                    continue\n                else:\n                    k+=1\n                    i+=1\n                    continue\n            else:\n                shapelets[nl][k] += 1j*coeff[i]/2.\n                shapelets[k][nl] -= 1j*coeff[i]/2.\n                i+=1\n        return shapelets"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the the numerical values of a set of shapelets at polar coordinates :param shapelets: set of shapelets [l=,r=,a_lr=] :type shapelets: array of size (n,3) :param coordPolar: set of coordinates in polar units :type coordPolar: array of size (n,2) :returns: array of same size with coords [r,phi] :raises: AttributeError, KeyError", "response": "def _shapeletOutput(self, r, phi, beta, shapelets):\n        \"\"\"\n        returns the the numerical values of a set of shapelets at polar coordinates\n        :param shapelets: set of shapelets [l=,r=,a_lr=]\n        :type shapelets: array of size (n,3)\n        :param coordPolar: set of coordinates in polar units\n        :type coordPolar: array of size (n,2)\n        :returns:  array of same size with coords [r,phi]\n        :raises: AttributeError, KeyError\n        \"\"\"\n        if type(r) == float or type(r) == int or type(r) == type(np.float64(1)) or len(r) <= 1:\n            values = 0.\n        else:\n            values = np.zeros(len(r), 'complex')\n        for nl in range(0,len(shapelets)): #sum over different shapelets\n            for nr in range(0,len(shapelets)):\n                value = shapelets[nl][nr]*self._chi_lr(r, phi, nl, nr, beta)\n                values += value\n        return values.real"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _chi_lr(self,r, phi, nl,nr,beta):\n        m=int((nr-nl).real)\n        n=int((nr+nl).real)\n        p=int((n-abs(m))/2)\n        p2=int((n+abs(m))/2)\n        q=int(abs(m))\n        if p % 2==0: #if p is even\n            prefac=1\n        else:\n            prefac=-1\n        prefactor=prefac/beta**(abs(m)+1)*np.sqrt(math.factorial(p)/(np.pi*math.factorial(p2)))\n        poly=self.poly[p][q]\n        return prefactor*r**q*poly((r/beta)**2)*np.exp(-(r/beta)**2/2)*np.exp(-1j*m*phi)", "response": "Compute the generalized polar basis function in the convention of Massey & Refregier eqn 8\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate convergence kappa given lensing potential shapelet coefficients laplacian and beta", "response": "def _kappaShapelets(self, shapelets, beta):\n        \"\"\"\n        calculates the convergence kappa given lensing potential shapelet coefficients (laplacian/2)\n        :param shapelets: set of shapelets [l=,r=,a_lr=]\n        :type shapelets: array of size (n,3)\n        :returns:  set of kappa shapelets.\n        :raises: AttributeError, KeyError\n        \"\"\"\n        output=np.zeros((len(shapelets)+1,len(shapelets)+1),'complex')\n        for nl in range(0,len(shapelets)):\n            for nr in range(0,len(shapelets)):\n                a_lr=shapelets[nl][nr]\n                if nl>0:\n                    output[nl-1][nr+1]+=a_lr*np.sqrt(nl*(nr+1))/2\n                    if nr>0:\n                        output[nl-1][nr-1]+=a_lr*np.sqrt(nl*nr)/2\n                output[nl+1][nr+1]+=a_lr*np.sqrt((nl+1)*(nr+1))/2\n                if nr>0:\n                    output[nl+1][nr-1]+=a_lr*np.sqrt((nl+1)*nr)/2\n        return output/beta**2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _alphaShapelets(self,shapelets, beta):\n        output_x = np.zeros((len(shapelets)+1, len(shapelets)+1), 'complex')\n        output_y = np.zeros((len(shapelets)+1, len(shapelets)+1), 'complex')\n        for nl in range(0,len(shapelets)):\n            for nr in range(0,len(shapelets)):\n                a_lr=shapelets[nl][nr]\n                output_x[nl][nr+1]-=a_lr*np.sqrt(nr+1)/2\n                output_y[nl][nr+1]-=a_lr*np.sqrt(nr+1)/2*1j\n                output_x[nl+1][nr]-=a_lr*np.sqrt(nl+1)/2\n                output_y[nl+1][nr]+=a_lr*np.sqrt(nl+1)/2*1j\n                if nl>0:\n                    output_x[nl-1][nr]+=a_lr*np.sqrt(nl)/2\n                    output_y[nl-1][nr]-=a_lr*np.sqrt(nl)/2*1j\n                if nr>0:\n                    output_x[nl][nr-1]+=a_lr*np.sqrt(nr)/2\n                    output_y[nl][nr-1]+=a_lr*np.sqrt(nr)/2*1j\n        return output_x/beta,output_y/beta", "response": "Calculates the deflection angles given lensing potential shapelet coefficients"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the critical projected mass density in units of M_sun Mpc^2", "response": "def epsilon_crit(self):\n        \"\"\"\n        returns the critical projected mass density in units of M_sun/Mpc^2 (physical units)\n        \"\"\"\n        const_SI = const.c**2 / (4*np.pi * const.G)  #c^2/(4*pi*G) in units of [kg/m]\n        conversion = const.Mpc / const.M_sun  # converts [kg/m] to [M_sun/Mpc]\n        pre_const = const_SI*conversion   #c^2/(4*pi*G) in units of [M_sun/Mpc]\n        Epsilon_Crit = self.D_s/(self.D_d*self.D_ds) * pre_const #[M_sun/Mpc^2]\n        return Epsilon_Crit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef F(self, x):\n        if isinstance(x, np.ndarray):\n            nfwvals = np.ones_like(x)\n            inds1 = np.where(x < 1)\n            inds2 = np.where(x > 1)\n            nfwvals[inds1] = (1 - x[inds1] ** 2) ** -.5 * np.arctanh((1 - x[inds1] ** 2) ** .5)\n            nfwvals[inds2] = (x[inds2] ** 2 - 1) ** -.5 * np.arctan((x[inds2] ** 2 - 1) ** .5)\n            return nfwvals\n\n        elif isinstance(x, float) or isinstance(x, int):\n            if x == 1:\n                return 1\n            if x < 1:\n                return (1 - x ** 2) ** -.5 * np.arctanh((1 - x ** 2) ** .5)\n            else:\n                return (x ** 2 - 1) ** -.5 * np.arctan((x ** 2 - 1) ** .5)", "response": "r - > r"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nproject two dimenstional NFW profile at radius R", "response": "def density_2d(self, x, y, Rs, rho0, r_trunc, center_x=0, center_y=0):\n        \"\"\"\n        projected two dimenstional NFW profile (kappa*Sigma_crit)\n\n        :param R: radius of interest\n        :type R: float/numpy array\n        :param Rs: scale radius\n        :type Rs: float\n        :param rho0: density normalization (characteristic density)\n        :type rho0: float\n        :param r200: radius of (sub)halo\n        :type r200: float>0\n        :return: Epsilon(R) projected density at radius R\n        \"\"\"\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_ ** 2 + y_ ** 2)\n        x = R * Rs ** -1\n        tau = float(r_trunc) * Rs ** -1\n        Fx = self._F(x, tau)\n        return 2 * rho0 * Rs * Fx"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the mass of a 3d sphere or radius r", "response": "def mass_3d(self, R, Rs, rho0, r_trunc):\n        \"\"\"\n        mass enclosed a 3d sphere or radius r\n\n        :param r:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n\n        x = R * Rs ** -1\n\n        func = (r_trunc ** 2 * (-2 * x * (1 + r_trunc ** 2) + 4 * (1 + x) * r_trunc * np.arctan(x / r_trunc) -\n                                2 * (1 + x) * (-1 + r_trunc ** 2) * np.log(Rs) + 2 * (1 + x) * (-1 + r_trunc ** 2) * np.log(Rs * (1 + x)) +\n                                2 * (1 + x) * (-1 + r_trunc ** 2) * np.log(Rs * r_trunc) -\n                                (1 + x) * (-1 + r_trunc ** 2) * np.log(Rs ** 2 * (x ** 2 + r_trunc ** 2)))) / (2. * (1 + x) * (1 + r_trunc ** 2) ** 2)\n\n        m_3d = 4*np.pi*Rs ** 3 * rho0 * func\n        return m_3d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nfwPot(self, R, Rs, rho0, r_trunc):\n        x = R / Rs\n        tau = float(r_trunc) / Rs\n        hx = self._h(x, tau)\n        return 2 * rho0 * Rs ** 3 * hx", "response": "calculate lensing potential of NFW profile"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the alpha of the NFW profile at radius R", "response": "def nfwAlpha(self, R, Rs, rho0, r_trunc, ax_x, ax_y):\n        \"\"\"\n        deflection angel of NFW profile along the projection to coordinate axis\n\n        :param R: radius of interest\n        :type R: float/numpy array\n        :param Rs: scale radius\n        :type Rs: float\n        :param rho0: density normalization (characteristic density)\n        :type rho0: float\n        :param r200: radius of (sub)halo\n        :type r200: float>0\n        :param axis: projection to either x- or y-axis\n        :type axis: same as R\n        :return: Epsilon(R) projected density at radius R\n        \"\"\"\n        if isinstance(R, int) or isinstance(R, float):\n            R = max(R, 0.00001)\n        else:\n            R[R <= 0.00001] = 0.00001\n\n        x = R / Rs\n        tau = float(r_trunc) / Rs\n        gx = self._g(x, tau)\n        a = 4 * rho0 * Rs * gx / x ** 2\n        return a * ax_x, a * ax_y"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate mass of a 2D system from a set of R and Rs", "response": "def mass_2d(self,R,Rs,rho0,r_trunc):\n\n        \"\"\"\n        analytic solution of the projection integral\n        (convergence)\n\n        :param x: R/Rs\n        :type x: float >0\n        \"\"\"\n\n        x = R / Rs\n        tau = r_trunc / Rs\n        gx = self._g(x,tau)\n        m_2d = 4 * rho0 * Rs * R ** 2 * gx / x ** 2 * np.pi\n        return m_2d"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing analytic solution of integral for NFW profile", "response": "def _g(self, x, tau):\n        \"\"\"\n        analytic solution of integral for NFW profile to compute deflection angel and gamma\n\n        :param x: R/Rs\n        :type x: float >0\n        \"\"\"\n        return tau ** 2 * (tau ** 2 + 1) ** -2 * (\n                (tau ** 2 + 1 + 2 * (x ** 2 - 1)) * self.F(x) + tau * np.pi + (tau ** 2 - 1) * np.log(tau) +\n                np.sqrt(tau ** 2 + x ** 2) * (-np.pi + self.L(x, tau) * (tau ** 2 - 1) * tau ** -1))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _h(self, X, tau):\n\n        \"\"\"\n        a horrible expression for the integral to compute potential\n\n        :param x: R/Rs\n        :param tau: t/Rs\n        :type x: float >0\n        \"\"\"\n\n        def cos_func(y):\n            if isinstance(y, float) or isinstance(y, int):\n                if y > 1:\n                    return np.arccosh(y)\n                else:\n                    return np.arccos(y)\n            else:\n                values = np.ones_like(y)\n                inds1 = np.where(y < 1)\n                inds2 = np.where(y > 1)\n                values[inds1] = np.arccos(y[inds1])\n                # values[inds2] = np.arccosh(y[inds2])\n                values[inds2] = np.arccos((1 - y[inds2]) ** .5)\n                return values\n\n        def nfw_func(x):\n            if isinstance(x,float) or isinstance(x,int):\n                if x<1:\n                    return np.log(x / 2.) ** 2 - np.arccosh(1. / x) ** 2\n                else:\n                    return np.log(x / 2.)**2 - np.arccos(1. /x)**2\n            else:\n                inds1 = np.where(x < 1)\n                inds2 = np.where(x >= 1)\n                y = np.zeros_like(x)\n                y[inds1] = np.log(x[inds1] / 2.) ** 2 - np.arccosh(1. / x[inds1]) ** 2\n                y[inds2] = np.log(x[inds2] / 2.) ** 2 - np.arccosh(1. / x[inds2]) ** 2\n\n                return y\n\n        def tnfw_func(x,tau):\n\n            u = x**2\n            t2 = tau**2\n            Lx = self.L(u**.5, tau)\n\n            return (t2 + 1) ** -2 * (\n                2 * t2 * np.pi * (tau - (t2 + u) ** .5 + tau * np.log(tau + (t2 + u) ** .5))\n                +\n                2 * (t2 - 1) * tau * (t2 + u) ** .5 * Lx\n                +\n                t2 * (t2 - 1) * Lx ** 2\n                +\n                4 * t2 * (u - 1) * self.F(u**.5)\n                +\n                t2 * (t2 - 1) * (cos_func(u ** -0.5)) ** 2\n                +\n                t2 * ((t2 - 1) * np.log(tau) - t2 - 1) * np.log(u)\n                -\n                t2 * (\n                (t2 - 1) * np.log(tau) * np.log(4 * tau) + 2 * np.log(0.5 * tau) - 2 * tau * (tau - np.pi) * np.log(\n                    tau * 2)))\n\n        c = 1e-9\n        rescale = 1\n\n        if np.any(X-c<1):\n\n            warnings.warn('Truncated NFW potential not yet implemented for x<1. Using the expression for the NFW '\n                          'potential in this regime isntead.')\n            rescale = tnfw_func(1.00001,tau)*nfw_func(1)**-1\n\n        if isinstance(X,float) or isinstance(X,int):\n            if X<1:\n                X = min(X,0.001)\n                return nfw_func(X)*rescale\n            else:\n                return tnfw_func(X,tau)\n        else:\n\n            X[np.where(X<0.001)] = 0.001\n\n            values = np.zeros_like(X)\n            inds1,inds2 = np.where(X<1),np.where(X>=1)\n            values[inds1] = nfw_func(X[inds1])*rescale\n            values[inds2] = tnfw_func(X[inds2],tau)\n\n            return values", "response": "a horrible expression for the integral to compute potential\n           "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform cartesian coords [x,y] into polar coords [r,phi] in the frame of the lense center :param coord: set of coordinates :type coord: array of size (n,2) :param center: rotation point :type center: array of size (2) :returns: array of same size with coords [r,phi] :raises: AttributeError, KeyError", "response": "def cart2polar(x, y, center=np.array([0, 0])):\n    \"\"\"\n    transforms cartesian coords [x,y] into polar coords [r,phi] in the frame of the lense center\n\n    :param coord: set of coordinates\n    :type coord: array of size (n,2)\n    :param center: rotation point\n    :type center: array of size (2)\n    :returns:  array of same size with coords [r,phi]\n    :raises: AttributeError, KeyError\n    \"\"\"\n    coordShift_x = x - center[0]\n    coordShift_y = y - center[1]\n    r = np.sqrt(coordShift_x**2+coordShift_y**2)\n    phi = np.arctan2(coordShift_y, coordShift_x)\n    return r, phi"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform polar coords [r,phi] into cartesian coords [x,y] in the frame of the lense center :param coord: set of coordinates :type coord: array of size (n,2) :param center: rotation point :type center: array of size (2) :returns: array of same size with coords [x,y] :raises: AttributeError, KeyError", "response": "def polar2cart(r, phi, center):\n    \"\"\"\n    transforms polar coords [r,phi] into cartesian coords [x,y] in the frame of the lense center\n\n    :param coord: set of coordinates\n    :type coord: array of size (n,2)\n    :param center: rotation point\n    :type center: array of size (2)\n    :returns:  array of same size with coords [x,y]\n    :raises: AttributeError, KeyError\n    \"\"\"\n    x = r*np.cos(phi)\n    y = r*np.sin(phi)\n    return x - center[0], y - center[1]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ellipticity2phi_gamma(e1, e2):\n    phi = np.arctan2(e2, e1)/2\n    gamma = np.sqrt(e1**2+e2**2)\n    return phi, gamma", "response": "returns the angle and abs value of the ellipticity component"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transform_e1e2(x, y, e1, e2, center_x=0, center_y=0):\n    x_shift = x - center_x\n    y_shift = y - center_y\n    x_ = (1-e1) * x_shift - e2 * y_shift\n    y_ = -e2 * x_shift + (1 + e1) * y_shift\n    det = np.sqrt((1-e1)*(1+e1) + e2**2)\n    return x_ / det, y_ / det", "response": "This function transforms the coordinates x y with eccentricities e1 e2 into an elliptical coordiante system."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the phi and q of the ellipticity", "response": "def ellipticity2phi_q(e1, e2):\n    \"\"\"\n    :param e1:\n    :param e2:\n    :return:\n    \"\"\"\n    phi = np.arctan2(e2, e1)/2\n    c = np.sqrt(e1**2+e2**2)\n    if c > 0.999:\n        c = 0.999\n    q = (1-c)/(1+c)\n    return phi, q"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef time_delays(self, kwargs_lens, kwargs_ps, kappa_ext=0):\n        fermat_pot = self.lens_analysis.fermat_potential(kwargs_lens, kwargs_ps)\n        time_delay = self.lensCosmo.time_delay_units(fermat_pot, kappa_ext)\n        return time_delay", "response": "predicts the time delays at the image positions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the velocity dispersion of the LOS lens model", "response": "def velocity_dispersion(self, kwargs_lens, kwargs_lens_light, lens_light_model_bool_list=None, aniso_param=1,\n                            r_eff=None, R_slit=0.81, dR_slit=0.1, psf_fwhm=0.7, num_evaluate=1000):\n        \"\"\"\n        computes the LOS velocity dispersion of the lens within a slit of size R_slit x dR_slit and seeing psf_fwhm.\n        The assumptions are a Hernquist light profile and the spherical power-law lens model at the first position.\n\n        Further information can be found in the AnalyticKinematics() class.\n\n        :param kwargs_lens: lens model parameters\n        :param kwargs_lens_light: deflector light parameters\n        :param aniso_param: scaled r_ani with respect to the half light radius\n        :param r_eff: half light radius, if not provided, will be computed from the lens light model\n        :param R_slit: width of the slit\n        :param dR_slit: length of the slit\n        :param psf_fwhm: full width at half maximum of the seeing condition\n        :param num_evaluate: number of spectral rendering of the light distribution that end up on the slit\n        :return: velocity dispersion in units [km/s]\n        \"\"\"\n        gamma = kwargs_lens[0]['gamma']\n        if 'center_x' in kwargs_lens_light[0]:\n            center_x, center_y = kwargs_lens_light[0]['center_x'], kwargs_lens_light[0]['center_y']\n        else:\n            center_x, center_y = 0, 0\n        if r_eff is None:\n            r_eff = self.lens_analysis.half_light_radius_lens(kwargs_lens_light, center_x=center_x, center_y=center_y, model_bool_list=lens_light_model_bool_list)\n        theta_E = kwargs_lens[0]['theta_E']\n        r_ani = aniso_param * r_eff\n        sigma2 = self.analytic_kinematics.vel_disp(gamma, theta_E, r_eff, r_ani, R_slit, dR_slit, FWHM=psf_fwhm, rendering_number=num_evaluate)\n        return sigma2"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef velocity_dispersion_numerical(self, kwargs_lens, kwargs_lens_light, kwargs_anisotropy, kwargs_aperture, psf_fwhm,\n                                      aperture_type, anisotropy_model, r_eff=None, kwargs_numerics={}, MGE_light=False,\n                                      MGE_mass=False, lens_model_kinematics_bool=None, light_model_kinematics_bool=None,\n                                      Hernquist_approx=False):\n        \"\"\"\n        Computes the LOS velocity dispersion of the deflector galaxy with arbitrary combinations of light and mass models.\n        For a detailed description, visit the description of the Galkin() class.\n        Additionaly to executing the Galkin routine, it has an optional Multi-Gaussian-Expansion decomposition of lens\n        and light models that do not have a three-dimensional distribution built in, such as Sersic profiles etc.\n\n        The center of all the lens and lens light models that are part of the kinematic estimate must be centered on the\n        same point.\n\n        :param kwargs_lens: lens model parameters\n        :param kwargs_lens_light: lens light parameters\n        :param kwargs_anisotropy: anisotropy parameters (see Galkin module)\n        :param kwargs_aperture: aperture parameters (see Galkin module)\n        :param psf_fwhm: full width at half maximum of the seeing (Gaussian form)\n        :param aperture_type: type of aperture (see Galkin module\n        :param anisotropy_model: stellar anisotropy model (see Galkin module)\n        :param r_eff: a rough estimate of the half light radius of the lens light in case of computing the MGE of the\n         light profile\n        :param kwargs_numerics: keyword arguments that contain numerical options (see Galkin module)\n        :param MGE_light: bool, if true performs the MGE for the light distribution\n        :param MGE_mass: bool, if true performs the MGE for the mass distribution\n        :param lens_model_kinematics_bool: bool list of length of the lens model. Only takes a subset of all the models\n            as part of the kinematics computation (can be used to ignore substructure, shear etc that do not describe the\n            main deflector potential\n        :param light_model_kinematics_bool: bool list of length of the light model. Only takes a subset of all the models\n            as part of the kinematics computation (can be used to ignore light components that do not describe the main\n            deflector\n        :return: LOS velocity dispersion [km/s]\n        \"\"\"\n\n        kwargs_cosmo = {'D_d': self.lensCosmo.D_d, 'D_s': self.lensCosmo.D_s, 'D_ds': self.lensCosmo.D_ds}\n        mass_profile_list = []\n        kwargs_profile = []\n        if lens_model_kinematics_bool is None:\n            lens_model_kinematics_bool = [True] * len(kwargs_lens)\n        for i, lens_model in enumerate(self.kwargs_options['lens_model_list']):\n            if lens_model_kinematics_bool[i] is True:\n                mass_profile_list.append(lens_model)\n                if lens_model in ['INTERPOL', 'INTERPOL_SCLAED']:\n                    center_x, center_y = self._lensModelExt.lens_center(kwargs_lens, k=i)\n                    kwargs_lens_i = copy.deepcopy(kwargs_lens[i])\n                    kwargs_lens_i['grid_interp_x'] -= center_x\n                    kwargs_lens_i['grid_interp_y'] -= center_y\n                else:\n                    kwargs_lens_i = {k: v for k, v in kwargs_lens[i].items() if not k in ['center_x', 'center_y']}\n                kwargs_profile.append(kwargs_lens_i)\n\n        if MGE_mass is True:\n            lensModel = LensModel(lens_model_list=mass_profile_list)\n            massModel = LensModelExtensions(lensModel)\n            theta_E = massModel.effective_einstein_radius(kwargs_profile)\n            r_array = np.logspace(-4, 2, 200) * theta_E\n            mass_r = lensModel.kappa(r_array, np.zeros_like(r_array), kwargs_profile)\n            amps, sigmas, norm = mge.mge_1d(r_array, mass_r, N=20)\n            mass_profile_list = ['MULTI_GAUSSIAN_KAPPA']\n            kwargs_profile = [{'amp': amps, 'sigma': sigmas}]\n\n        light_profile_list = []\n        kwargs_light = []\n        if light_model_kinematics_bool is None:\n            light_model_kinematics_bool = [True] * len(kwargs_lens_light)\n        for i, light_model in enumerate(self.kwargs_options['lens_light_model_list']):\n            if light_model_kinematics_bool[i]:\n                light_profile_list.append(light_model)\n                kwargs_lens_light_i = {k: v for k, v in kwargs_lens_light[i].items() if not k in ['center_x', 'center_y']}\n                if 'q' in kwargs_lens_light_i:\n                    kwargs_lens_light_i['q'] = 1\n                kwargs_light.append(kwargs_lens_light_i)\n        if r_eff is None:\n            lensAnalysis = LensAnalysis({'lens_light_model_list': light_profile_list})\n            r_eff = lensAnalysis.half_light_radius_lens(kwargs_light, model_bool_list=light_model_kinematics_bool)\n        if Hernquist_approx is True:\n            light_profile_list = ['HERNQUIST']\n            kwargs_light = [{'Rs':  r_eff, 'amp': 1.}]\n        else:\n            if MGE_light is True:\n                lightModel = LightModel(light_profile_list)\n                r_array = np.logspace(-3, 2, 200) * r_eff * 2\n                flux_r = lightModel.surface_brightness(r_array, 0, kwargs_light)\n                amps, sigmas, norm = mge.mge_1d(r_array, flux_r, N=20)\n                light_profile_list = ['MULTI_GAUSSIAN']\n                kwargs_light = [{'amp': amps, 'sigma': sigmas}]\n\n        galkin = Galkin(mass_profile_list, light_profile_list, aperture_type=aperture_type,\n                        anisotropy_model=anisotropy_model, fwhm=psf_fwhm, kwargs_cosmo=kwargs_cosmo, **kwargs_numerics)\n        sigma2 = galkin.vel_disp(kwargs_profile, kwargs_light, kwargs_anisotropy, kwargs_aperture)\n        return sigma2", "response": "This method computes the LOS velocity dispersion of the deflector galaxy with arbitrary combinations of lens and light models."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef function(self, x, y, amp, sigma, center_x=0, center_y=0):\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        sigma_x, sigma_y = sigma, sigma\n        c = 1. / (2 * sigma_x * sigma_y)\n        num_int = self._num_integral(r, c)\n        amp_density = self._amp2d_to_3d(amp, sigma_x, sigma_y)\n        amp2d = amp_density / (np.sqrt(np.pi) * np.sqrt(sigma_x * sigma_y * 2))\n        amp2d *= 2 * 1. / (2 * c)\n        return num_int * amp2d", "response": "returns the function of the Gaussian"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _num_integral(self, r, c):\n        out = integrate.quad(lambda x: (1-np.exp(-c*x**2))/x, 0, r)\n        return out[0]", "response": "get the num_integral of the logarithm of the logarithm"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn df dx dy of the function", "response": "def derivatives(self, x, y, amp, sigma, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_**2 + y_**2)\n        sigma_x, sigma_y = sigma, sigma\n        if isinstance(R, int) or isinstance(R, float):\n            R = max(R, self.ds)\n        else:\n            R[R <= self.ds] = self.ds\n        alpha = self.alpha_abs(R, amp, sigma)\n        return alpha / R * x_, alpha / R * y_"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hessian(self, x, y, amp, sigma, center_x=0, center_y=0):\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        sigma_x, sigma_y = sigma, sigma\n        if isinstance(r, int) or isinstance(r, float):\n            r = max(r, self.ds)\n        else:\n            r[r <= self.ds] = self.ds\n        d_alpha_dr = -self.d_alpha_dr(r, amp, sigma_x, sigma_y)\n        alpha = self.alpha_abs(r, amp, sigma)\n\n        f_xx = -(d_alpha_dr/r + alpha/r**2) * x_**2/r + alpha/r\n        f_yy = -(d_alpha_dr/r + alpha/r**2) * y_**2/r + alpha/r\n        f_xy = -(d_alpha_dr/r + alpha/r**2) * x_*y_/r\n        return f_xx, f_yy, f_xy", "response": "returns Hessian matrix of function d^2f x y and sigma"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef alpha_abs(self, R, amp, sigma):\n        sigma_x, sigma_y = sigma, sigma\n        amp_density = self._amp2d_to_3d(amp, sigma_x, sigma_y)\n        alpha = self.mass_2d(R, amp_density, sigma) / np.pi / R\n        return alpha", "response": "Calculates the absolute value of the deflections log - mass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _amp3d_to_2d(self, amp, sigma_x, sigma_y):\n        return amp * np.sqrt(np.pi) * np.sqrt(sigma_x * sigma_y * 2)", "response": "Convert 3d density into 2d density parameter"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _amp2d_to_3d(self, amp, sigma_x, sigma_y):\n        return amp / (np.sqrt(np.pi) * np.sqrt(sigma_x * sigma_y * 2))", "response": "Convert 2d density into 3d density"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef function(self, x, y, amp, alpha, beta, center_x, center_y):\n        x_shift = x - center_x\n        y_shift = y - center_y\n        return amp * (1. + (x_shift**2+y_shift**2)/alpha**2)**(-beta)", "response": "Function that computes the Moffat profile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parameter_constraints(self, theta_E, gamma, q, phi_G, s_scale):\n        if theta_E < 0:\n            theta_E = 0\n        if s_scale < 0.00000001:\n            s_scale = 0.00000001\n        if gamma < 1.2:\n            gamma = 1.2\n            theta_E = 0\n        if gamma > 2.9:\n            gamma = 2.9\n            theta_E = 0\n        if q < 0.01:\n            q = 0.01\n            theta_E = 0\n        if q > 1:\n            q = 1.\n            theta_E = 0\n        return theta_E, gamma, q, phi_G, s_scale", "response": "Sets the bounds to parameters due to numerical stability."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_lower_upper_errors(sample, num_sigma=1):\n    if num_sigma > 3:\n        raise ValueError(\"Number of sigma-constraints restircted to three. %s not valid\" % num_sigma)\n    num = len(sample)\n    num_threshold1 = int(round((num-1)*0.833))\n    num_threshold2 = int(round((num-1)*0.977249868))\n    num_threshold3 = int(round((num-1)*0.998650102))\n\n    median = np.median(sample)\n    sorted_sample = np.sort(sample)\n    if num_sigma > 0:\n        upper_sigma1 = sorted_sample[num_threshold1-1]\n        lower_sigma1 = sorted_sample[num-num_threshold1-1]\n    else:\n        return median, [[]]\n    if num_sigma > 1:\n        upper_sigma2 = sorted_sample[num_threshold2-1]\n        lower_sigma2 = sorted_sample[num-num_threshold2-1]\n    else:\n        return median, [[median-lower_sigma1, upper_sigma1-median]]\n    if num_sigma > 2:\n        upper_sigma3 = sorted_sample[num_threshold3-1]\n        lower_sigma3 = sorted_sample[num-num_threshold3-1]\n        return median, [[median-lower_sigma1, upper_sigma1-median], [median-lower_sigma2, upper_sigma2-median],\n                      [median-lower_sigma3, upper_sigma3-median]]\n    else:\n        return median, [[median-lower_sigma1, upper_sigma1-median], [median-lower_sigma2, upper_sigma2-median]]", "response": "This function computes the upper and lower sigma from the median value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pdf_new(self, x, mu, sigma, skw):\n        if skw > 1 or skw < -1:\n            print(\"skewness %s out of range\" % skw)\n            skw = 1.\n        e, w, a = self.map_mu_sigma_skw(mu, sigma, skw)\n        pdf = self.pdf(x, e, w, a)\n        return pdf", "response": "function with different parameterisation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the weight of the logarithm of the variance and the delta", "response": "def _w_sigma_delta(self, sigma, delta):\n        \"\"\"\n        invert variance\n        :param sigma:\n        :param delta:\n        :return: w parameter\n        \"\"\"\n        sigma2=sigma**2\n        w2 = sigma2/(1-2*delta**2/np.pi)\n        w = np.sqrt(w2)\n        return w"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map_mu_sigma_skw(self, mu, sigma, skw):\n        delta = self._delta_skw(skw)\n        a = self._alpha_delta(delta)\n        w = self._w_sigma_delta(sigma, delta)\n        e = self._e_mu_w_delta(mu, w, delta)\n        return e, w, a", "response": "Map mu sigma skw to e w a"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the lense model of the catalogue basis with particle swarm optimizer.", "response": "def pso(self, n_particles, n_iterations, lower_start=None, upper_start=None, threadCount=1, init_pos=None,\n            mpi=False, print_key='PSO'):\n        \"\"\"\n        returns the best fit for the lense model on catalogue basis with particle swarm optimizer\n        \"\"\"\n        if lower_start is None or upper_start is None:\n            lower_start, upper_start = np.array(self.lower_limit), np.array(self.upper_limit)\n            print(\"PSO initialises its particles with default values\")\n        else:\n            lower_start = np.maximum(lower_start, self.lower_limit)\n            upper_start = np.minimum(upper_start, self.upper_limit)\n        if mpi is True:\n            pso = MpiParticleSwarmOptimizer(self.chain, lower_start, upper_start, n_particles, threads=1)\n            if pso.isMaster():\n                print('MPI option chosen')\n        else:\n            pso = ParticleSwarmOptimizer(self.chain, lower_start, upper_start, n_particles, threads=threadCount)\n        if init_pos is None:\n            init_pos = (upper_start - lower_start) / 2 + lower_start\n        if not init_pos is None:\n            pso.gbest.position = init_pos\n            pso.gbest.velocity = [0]*len(init_pos)\n            pso.gbest.fitness, _ = self.chain.likelihood(init_pos)\n        X2_list = []\n        vel_list = []\n        pos_list = []\n        time_start = time.time()\n        if pso.isMaster():\n            print('Computing the %s ...' % print_key)\n        num_iter = 0\n        for swarm in pso.sample(n_iterations):\n            X2_list.append(pso.gbest.fitness*2)\n            vel_list.append(pso.gbest.velocity)\n            pos_list.append(pso.gbest.position)\n            num_iter += 1\n            if pso.isMaster():\n                if num_iter % 10 == 0:\n                    print(num_iter)\n        if not mpi:\n            result = pso.gbest.position\n        else:\n            result = MpiUtil.mpiBCast(pso.gbest.position)\n\n        #if (pso.isMaster() and mpi is True) or self.chain.sampling_option == 'X2_catalogue':\n        if mpi is True and not pso.isMaster():\n            pass\n        else:\n            lens_dict, source_dict, lens_light_dict, ps_dict, kwargs_cosmo = self.chain.param.args2kwargs(result)\n            print(pso.gbest.fitness * 2 / (self.chain.effectiv_num_data_points(lens_dict, source_dict, lens_light_dict, ps_dict)), 'reduced X^2 of best position')\n            print(pso.gbest.fitness, 'logL')\n            print(self.chain.effectiv_num_data_points(lens_dict, source_dict, lens_light_dict, ps_dict), 'effective number of data points')\n            print(lens_dict, 'lens result')\n            print(source_dict, 'source result')\n            print(lens_light_dict, 'lens light result')\n            print(ps_dict, 'point source result')\n            print(kwargs_cosmo, 'cosmo result')\n            time_end = time.time()\n            print(time_end - time_start, 'time used for PSO', print_key)\n            print('===================')\n        return result, [X2_list, pos_list, vel_list, []]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns mcmc on the parameter space given parameter bounds with CosmoHammerSampler returns the chain", "response": "def mcmc_CH(self, walkerRatio, n_run, n_burn, mean_start, sigma_start, threadCount=1, init_pos=None, mpi=False):\n        \"\"\"\n        runs mcmc on the parameter space given parameter bounds with CosmoHammerSampler\n        returns the chain\n        \"\"\"\n        lowerLimit, upperLimit = self.lower_limit, self.upper_limit\n\n        mean_start = np.maximum(lowerLimit, mean_start)\n        mean_start = np.minimum(upperLimit, mean_start)\n\n        low_start = mean_start - sigma_start\n        high_start = mean_start + sigma_start\n        low_start = np.maximum(lowerLimit, low_start)\n        high_start = np.minimum(upperLimit, high_start)\n        sigma_start = (high_start - low_start) / 2\n        mean_start = (high_start + low_start) / 2\n        params = np.array([mean_start, lowerLimit, upperLimit, sigma_start]).T\n\n        chain = LikelihoodComputationChain(\n            min=lowerLimit,\n            max=upperLimit)\n\n        temp_dir = tempfile.mkdtemp(\"Hammer\")\n        file_prefix = os.path.join(temp_dir, \"logs\")\n        #file_prefix = \"./lenstronomy_debug\"\n        # chain.addCoreModule(CambCoreModule())\n        chain.addLikelihoodModule(self.chain)\n        chain.setup()\n\n        store = InMemoryStorageUtil()\n        #store = None\n        if mpi is True:\n            sampler = MpiCosmoHammerSampler(\n            params=params,\n            likelihoodComputationChain=chain,\n            filePrefix=file_prefix,\n            walkersRatio=walkerRatio,\n            burninIterations=n_burn,\n            sampleIterations=n_run,\n            threadCount=1,\n            initPositionGenerator=init_pos,\n            storageUtil=store)\n        else:\n            sampler = CosmoHammerSampler(\n                params=params,\n                likelihoodComputationChain=chain,\n                filePrefix=file_prefix,\n                walkersRatio=walkerRatio,\n                burninIterations=n_burn,\n                sampleIterations=n_run,\n                threadCount=threadCount,\n                initPositionGenerator=init_pos,\n                storageUtil=store)\n        time_start = time.time()\n        if sampler.isMaster():\n            print('Computing the MCMC...')\n            print('Number of walkers = ', len(mean_start)*walkerRatio)\n            print('Burn-in iterations: ', n_burn)\n            print('Sampling iterations:', n_run)\n        sampler.startSampling()\n        if sampler.isMaster():\n            time_end = time.time()\n            print(time_end - time_start, 'time taken for MCMC sampling')\n        # if sampler._sampler.pool is not None:\n        #     sampler._sampler.pool.close()\n        try:\n            shutil.rmtree(temp_dir)\n        except Exception as ex:\n            print(ex, 'shutil.rmtree did not work')\n            pass\n        #samples = np.loadtxt(file_prefix+\".out\")\n        #prob = np.loadtxt(file_prefix+\"prob.out\")\n        return store.samples, store.prob"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bkg_noise(readout_noise, exposure_time, sky_brightness, pixel_scael, num_exposures=1):\n    exposure_time_tot = num_exposures * exposure_time\n    readout_noise_tot = num_exposures * readout_noise ** 2\n    sky_per_pixel = sky_brightness * pixel_scael ** 2\n    sigma_bkg = np.sqrt(readout_noise_tot + exposure_time_tot * sky_per_pixel ** 2) / exposure_time_tot\n    return sigma_bkg", "response": "Compute the expected background noise of a pixel in units of counts per second per pixel"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset_point_source_cache(self, bool=True):\n        if self.PointSource is not None:\n            self.PointSource.delete_lens_model_cach()\n            self.PointSource.set_save_cache(bool)", "response": "Resets all the cache in the point source class and saves it from then on."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef linear_response_matrix(self, kwargs_lens=None, kwargs_source=None, kwargs_lens_light=None, kwargs_ps=None):\n        A = self._response_matrix(self.ImageNumerics.ra_grid_ray_shooting, self.ImageNumerics.dec_grid_ray_shooting,\n                                      kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, self.ImageNumerics.mask)\n        return A", "response": "This method computes the linear response matrix for the given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the 1d array of the data element that is fitted for including masking", "response": "def data_response(self):\n        \"\"\"\n        returns the 1d array of the data element that is fitted for (including masking)\n\n        :return: 1d numpy array\n        \"\"\"\n        d = self.ImageNumerics.image2array(self.Data.data * self.ImageNumerics.mask)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef error_response(self, kwargs_lens, kwargs_ps):\n        model_error = self.error_map(kwargs_lens, kwargs_ps)\n        error_map_1d = self.ImageNumerics.image2array(model_error)\n        C_D_response = self.ImageNumerics.C_D_response + error_map_1d\n        return C_D_response, model_error", "response": "returns the 1d array of response and 2d array of additonal errors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reduced_chi2(self, model, error_map=0):\n        chi2 = self.reduced_residuals(model, error_map)\n        return np.sum(chi2**2) / self.num_data_evaluate()", "response": "returns reduced chi2\n        :param model:\n        :param error_map:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert keyword arguments depending on model options to tuple of parameters", "response": "def kwargs2args(self, kwargs_lens=None, kwargs_source=None, kwargs_lens_light=None, kwargs_ps=None, kwargs_cosmo=None):\n        \"\"\"\n        inverse of getParam function\n        :param kwargs_lens: keyword arguments depending on model options\n        :param kwargs_source: keyword arguments depending on model options\n        :return: tuple of parameters\n        \"\"\"\n\n        args = self.lensParams.setParams(kwargs_lens)\n        args += self.souceParams.setParams(kwargs_source)\n        args += self.lensLightParams.setParams(kwargs_lens_light)\n        args += self.pointSourceParams.setParams(kwargs_ps)\n        args += self.cosmoParams.setParams(kwargs_cosmo)\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmapping the image plane position definition of the source plane to the image plane position definition of the source plane", "response": "def image2source_plane(self, kwargs_source, kwargs_lens, image_plane=False):\n        \"\"\"\n        maps the image plane position definition of the source plane\n\n        :param kwargs_source:\n        :param kwargs_lens:\n        :return:\n        \"\"\"\n        kwargs_source_copy = copy.deepcopy(kwargs_source)\n        for i, kwargs in enumerate(kwargs_source_copy):\n            if self._image_plane_source_list[i] is True and not image_plane:\n                if 'center_x' in kwargs:\n                    x_mapped, y_mapped = self._image2SourceMapping.image2source(kwargs['center_x'], kwargs['center_y'],\n                                                                                kwargs_lens, idex_source=i)\n                    kwargs['center_x'] = x_mapped\n                    kwargs['center_y'] = y_mapped\n        return kwargs_source_copy"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_lens_scaling(self, kwargs_cosmo, kwargs_lens, inverse=False):\n        kwargs_lens_updated = copy.deepcopy(kwargs_lens)\n        if self._mass_scaling is False:\n            return kwargs_lens_updated\n        scale_factor_list = np.array(kwargs_cosmo['scale_factor'])\n        if inverse is True:\n            scale_factor_list = 1. / np.array(kwargs_cosmo['scale_factor'])\n        for i, kwargs in enumerate(kwargs_lens_updated):\n            if self._mass_scaling_list[i] is not False:\n                scale_factor = scale_factor_list[self._mass_scaling_list[i] - 1]\n                if 'theta_E' in kwargs:\n                    kwargs['theta_E'] *= scale_factor\n                elif 'theta_Rs' in kwargs:\n                    kwargs['theta_Rs'] *= scale_factor\n                elif 'sigma0' in kwargs:\n                    kwargs['sigma0'] *= scale_factor\n                elif 'k_eff' in kwargs:\n                    kwargs['k_eff'] *= scale_factor\n        return kwargs_lens_updated", "response": "multiplies the scaling parameters of the profiles of the lens"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntesting whether the image positions map back to the same source position", "response": "def check_solver(self, kwargs_lens, kwargs_ps, kwargs_cosmo={}):\n        \"\"\"\n        test whether the image positions map back to the same source position\n        :param kwargs_lens:\n        :param kwargs_ps:\n        :return: Euclidean distance between the rayshooting of the image positions\n        \"\"\"\n        if self._solver is True:\n            image_x, image_y = kwargs_ps[0]['ra_image'], kwargs_ps[0]['dec_image']\n            image_x, image_y = self.real_image_positions(image_x, image_y, kwargs_cosmo)\n            dist = self._solver_module.check_solver(image_x, image_y, kwargs_lens)\n            return np.max(dist)\n        else:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the current setting of the parameter class.", "response": "def print_setting(self):\n        \"\"\"\n        prints the setting of the parameter class\n\n        :return:\n        \"\"\"\n        num, param_list = self.num_param()\n        num_linear = self.num_param_linear()\n\n        print(\"The following model options are chosen:\")\n        print(\"Lens models:\", self._lens_model_list)\n        print(\"Source models:\", self._source_light_model_list)\n        print(\"Lens light models:\", self._lens_light_model_list)\n        print(\"Point source models:\", self._point_source_model_list)\n        print(\"===================\")\n        print(\"The following parameters are being fixed:\")\n        print(\"Lens:\", self.lensParams.kwargs_fixed)\n        print(\"Source:\", self.souceParams.kwargs_fixed)\n        print(\"Lens light:\", self.lensLightParams.kwargs_fixed)\n        print(\"Point source:\", self.pointSourceParams.kwargs_fixed)\n        print(\"===================\")\n        print(\"Joint parameters for different models\")\n        print(\"Joint lens with lens:\", self._joint_lens_with_lens)\n        print(\"Joint lens with lens light:\", self._joint_lens_light_with_lens_light)\n        print(\"Joint source with source:\", self._joint_source_with_source)\n        print(\"Joint lens with light:\", self._joint_lens_with_light)\n        print(\"Joint source with point source:\", self._joint_source_with_point_source)\n        print(\"===================\")\n        print(\"Number of non-linear parameters being sampled: \", num)\n        print(\"Parameters being sampled: \", param_list)\n        print(\"Number of linear parameters being solved for: \", num_linear)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef derivatives(self, x, y, Rs, theta_Rs, r_core, center_x=0, center_y=0):\n\n        rho0 = self._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs, r_core=r_core)\n\n        if Rs < 0.0000001:\n            Rs = 0.0000001\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_ ** 2 + y_ ** 2)\n\n        dx, dy = self.coreBurkAlpha(R, Rs, rho0, r_core, x_, y_)\n\n        return dx, dy", "response": "returns the derivative of the core Burk at the given coordinates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hessian(self, x, y, Rs, theta_Rs, r_core, center_x=0, center_y=0):\n\n        \"\"\"\n        :param x: x coordinate\n        :param y: y coordinate\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n\n        if Rs < 0.0001:\n            Rs = 0.0001\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_ ** 2 + y_ ** 2)\n\n        rho0 = self._alpha2rho0(theta_Rs=theta_Rs, Rs=Rs, r_core=r_core)\n\n        kappa = self.density_2d(x_, y_, Rs, rho0, r_core)\n\n        gamma1, gamma2 = self.cBurkGamma(R, Rs, rho0, r_core, x_, y_)\n        f_xx = kappa + gamma1\n        f_yy = kappa - gamma1\n        f_xy = gamma2\n        return f_xx, f_yy, f_xy", "response": "calculate the hessian of a set of attributes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate mass of a 2D system from a distance and radius", "response": "def mass_2d(self, R, Rs, rho0, r_core):\n\n        \"\"\"\n        analytic solution of the projection integral\n        (convergence)\n\n        :param R: projected distance\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius\n        \"\"\"\n\n        x = R * Rs ** -1\n        p = Rs * r_core ** -1\n        gx = self._G(x, p)\n\n        m_2d = 2 * np.pi * rho0 * Rs ** 3 * gx\n\n        return m_2d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef coreBurkAlpha(self, R, Rs, rho0, r_core, ax_x, ax_y):\n        x = R * Rs ** -1\n        p = Rs * r_core ** -1\n\n        gx = self._G(x, p)\n\n        a = 2 * rho0 * Rs ** 2 * gx / x\n\n        return a * ax_x / R, a * ax_y / R", "response": "calculate core burk alpha"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates 3D mass of a set of species", "response": "def mass_3d(self, R, Rs, rho0, r_core):\n        \"\"\"\n        :param R: projected distance\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius\n        \"\"\"\n\n        Rs = float(Rs)\n        b = r_core * Rs ** -1\n        c = R * Rs ** -1\n\n        M0 = 4*np.pi*Rs**3 * rho0\n\n        return M0 * (1+b**2) ** -1 * (0.5*np.log(1+c**2) + b**2*np.log(c*b**-1 + 1) - b*np.arctan(c))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cBurkPot(self, R, Rs, rho0, r_core):\n\n        \"\"\"\n        :param R: projected distance\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius\n        \"\"\"\n        x = R * Rs ** -1\n        p = Rs * r_core ** -1\n        hx = self._H(x, p)\n\n        return 2 * rho0 * Rs ** 3 * hx", "response": ":param R: projected distance\n        :param Rs: scale radius\n        :param rho0: central core density\n        :param r_core: core radius"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef linear_response_matrix(self, kwargs_lens=None, kwargs_source=None, kwargs_lens_light=None, kwargs_ps=None):\n        A = []\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                A_i = self._imageModel_list[i].linear_response_matrix(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps)\n                if A == []:\n                    A = A_i\n                else:\n                    A = np.append(A, A_i, axis=1)\n        return A", "response": "compute the linear response matrix for the image model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data_response(self):\n        d = []\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                d_i = self._imageModel_list[i].data_response\n                if d == []:\n                    d = d_i\n                else:\n                    d = np.append(d, d_i)\n        return d", "response": "returns the 1d numpy array of the data element that is fitted for ( including masking"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmap 1d vector of joint exposures in list of 2d images of single exposures in list of 2d images of single exposures", "response": "def _array2image_list(self, array):\n        \"\"\"\n        maps 1d vector of joint exposures in list of 2d images of single exposures\n\n        :param array: 1d numpy array\n        :return: list of 2d numpy arrays of size  of exposures\n        \"\"\"\n        image_list = []\n        k = 0\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                num_data = self.num_response_list[i]\n                array_i = array[k:k + num_data]\n                image_i = self._imageModel_list[i].ImageNumerics.array2image(array_i)\n                image_list.append(image_i)\n                k += num_data\n        return image_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef likelihood_data_given_model(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, source_marg=False):\n        # generate image\n        im_sim_list, model_error_list, cov_matrix, param = self.image_linear_solve(kwargs_lens, kwargs_source,\n                                                                         kwargs_lens_light, kwargs_ps,\n                                                                         inv_bool=source_marg)\n        # compute X^2\n        logL = 0\n        index = 0\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                logL += self._imageModel_list[i].Data.log_likelihood(im_sim_list[index], self._imageModel_list[i].ImageNumerics.mask, model_error_list[index])\n                index += 1\n        if cov_matrix is not None and source_marg:\n            marg_const = de_lens.marginalisation_const(cov_matrix)\n            logL += marg_const\n        return logL", "response": "computes the likelihood of the data given a model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the image (lens and source surface brightness with a given lens model). The linear parameters are computed with a weighted linear least square optimization (i.e. flux normalization of the brightness profiles) :param kwargs_lens: list of keyword arguments corresponding to the superposition of different lens profiles :param kwargs_source: list of keyword arguments corresponding to the superposition of different source light profiles :param kwargs_lens_light: list of keyword arguments corresponding to different lens light surface brightness profiles :param kwargs_ps: keyword arguments corresponding to \"other\" parameters, such as external shear and point source image positions :param inv_bool: if True, invert the full linear solver Matrix Ax = y for the purpose of the covariance matrix. :return: 1d array of surface brightness pixels of the optimal solution of the linear parameters to match the data", "response": "def image_linear_solve(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, inv_bool=False):\n        \"\"\"\n        computes the image (lens and source surface brightness with a given lens model).\n        The linear parameters are computed with a weighted linear least square optimization (i.e. flux normalization of the brightness profiles)\n        :param kwargs_lens: list of keyword arguments corresponding to the superposition of different lens profiles\n        :param kwargs_source: list of keyword arguments corresponding to the superposition of different source light profiles\n        :param kwargs_lens_light: list of keyword arguments corresponding to different lens light surface brightness profiles\n        :param kwargs_ps: keyword arguments corresponding to \"other\" parameters, such as external shear and point source image positions\n        :param inv_bool: if True, invert the full linear solver Matrix Ax = y for the purpose of the covariance matrix.\n        :return: 1d array of surface brightness pixels of the optimal solution of the linear parameters to match the data\n        \"\"\"\n        wls_list, error_map_list, cov_param_list, param_list = [], [], [], []\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                kwargs_source_i = [kwargs_source[k] for k in self._index_source_list[i]]\n                kwargs_lens_light_i = [kwargs_lens_light[k] for k in self._index_lens_light_list[i]]\n                wls_model, error_map, cov_param, param = self._imageModel_list[i].image_linear_solve(kwargs_lens,\n                                                                                                     kwargs_source_i,\n                                                                                                     kwargs_lens_light_i,\n                                                                                                     kwargs_ps,\n                                                                                                     inv_bool=inv_bool)\n                wls_list.append(wls_model)\n                error_map_list.append(error_map)\n                cov_param_list.append(cov_param)\n                param_list.append(param)\n        return wls_list, error_map_list, cov_param_list, param_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef likelihood_data_given_model(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, source_marg=False):\n        # generate image\n        logL = 0\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                kwargs_source_i = [kwargs_source[k] for k in self._index_source_list[i]]\n                kwargs_lens_light_i = [kwargs_lens_light[k] for k in self._index_lens_light_list[i]]\n                logL += self._imageModel_list[i].likelihood_data_given_model(kwargs_lens, kwargs_source_i,\n                                                                             kwargs_lens_light_i, kwargs_ps,\n                                                                             source_marg=source_marg)\n        return logL", "response": "computes the likelihood of the data given a model"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_solver(self, image_x, image_y, kwargs_lens):\n        source_x, source_y = self._lensModel.ray_shooting(image_x, image_y, kwargs_lens)\n        dist = np.sqrt((source_x - source_x[0]) ** 2 + (source_y - source_y[0]) ** 2)\n        return dist", "response": "returns the precision of the solver to match the image position\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding fixed lens to kwargs that are kept fixed during run depending on options", "response": "def add_fixed_lens(self, kwargs_fixed_lens, kwargs_lens_init):\n        \"\"\"\n        returns kwargs that are kept fixed during run, depending on options\n        :param kwargs_options:\n        :param kwargs_lens:\n        :return:\n        \"\"\"\n        kwargs_fixed_lens = self._solver.add_fixed_lens(kwargs_fixed_lens, kwargs_lens_init)\n        return kwargs_fixed_lens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef epsilon_crit(self):\n        if not hasattr(self, '_Epsilon_Crit'):\n            const_SI = const.c ** 2 / (4 * np.pi * const.G)  #c^2/(4*pi*G) in units of [kg/m]\n            conversion = const.Mpc / const.M_sun  # converts [kg/m] to [M_sun/Mpc]\n            factor = const_SI*conversion   #c^2/(4*pi*G) in units of [M_sun/Mpc]\n            self._Epsilon_Crit = self.D_s/(self.D_d*self.D_ds) * factor #[M_sun/Mpc^2]\n        return self._Epsilon_Crit", "response": "returns the critical projected mass density in units of M_sun Mpc^2"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the mass of the object in the given Einstein radius", "response": "def mass_in_theta_E(self, theta_E):\n        \"\"\"\n        mass within Einstein radius (area * epsilon crit) [M_sun]\n        :param theta_E: Einstein radius [arcsec]\n        :return: mass within Einstein radius [M_sun]\n        \"\"\"\n        mass = self.arcsec2phys_lens(theta_E) ** 2 * np.pi * self.epsilon_crit\n        return mass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nfw_angle2physical(self, Rs_angle, theta_Rs):\n        Rs = Rs_angle * const.arcsec * self.D_d\n        theta_scaled = theta_Rs * self.epsilon_crit * self.D_d * const.arcsec\n        rho0 = theta_scaled / (4 * Rs ** 2 * (1 + np.log(1. / 2.)))\n        rho0_com = rho0 / self.h**2 * self.a_z(self.z_lens)**3\n        c = self.nfw_param.c_rho0(rho0_com)\n        r200 = c * Rs\n        M200 = self.nfw_param.M_r200(r200 * self.h / self.a_z(self.z_lens)) / self.h\n        return rho0, Rs, c, r200, M200", "response": "converts the angular parameters into the physical ones for an NFW profile"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nfw_physical2angle(self, M, c):\n        rho0, Rs, r200 = self.nfwParam_physical(M, c)\n        Rs_angle = Rs / self.D_d / const.arcsec  # Rs in arcsec\n        theta_Rs = rho0 * (4 * Rs ** 2 * (1 + np.log(1. / 2.)))\n        return Rs_angle,  theta_Rs / self.epsilon_crit / self.D_d / const.arcsec", "response": "converts the physical mass and concentration parameter of an NFW profile into the lensing quantities"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nfwParam_physical(self, M, c):\n        r200 = self.nfw_param.r200_M(M * self.h) / self.h * self.a_z(self.z_lens)  # physical radius r200\n        rho0 = self.nfw_param.rho0_c(c) * self.h**2 / self.a_z(self.z_lens)**3 # physical density in M_sun/Mpc**3\n        Rs = r200/c\n        return rho0, Rs, r200", "response": "returns the NFW parameters in physical units"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the lensing Einstein radius into a physical velocity dispersion in units", "response": "def sis_theta_E2sigma_v(self, theta_E):\n        \"\"\"\n        converts the lensing Einstein radius into a physical velocity dispersion\n        :param theta_E: Einstein radius (in arcsec)\n        :return: velocity dispersion in units (km/s)\n        \"\"\"\n        v_sigma_c2 = theta_E * const.arcsec / (4*np.pi) * self.D_s / self.D_ds\n        return np.sqrt(v_sigma_c2)*const.c / 1000"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the velocity dispersion into an Einstein radius for a SIS profile", "response": "def sis_sigma_v2theta_E(self, v_sigma):\n        \"\"\"\n        converts the velocity dispersion into an Einstein radius for a SIS profile\n        :param v_sigma: velocity dispersion (km/s)\n        :return: theta_E (arcsec)\n        \"\"\"\n        theta_E = 4 * np.pi * (v_sigma * 1000./const.c)**2 * self.D_ds / self.D_s / const.arcsec\n        return theta_E"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the angular diameter of the class", "response": "def D_d(self, H_0, Om0, Ode0=None):\n        \"\"\"\n        angular diameter to deflector\n        :param H_0: Hubble parameter [km/s/Mpc]\n        :param Om0: normalized matter density at present time\n        :return: float [Mpc]\n        \"\"\"\n        lensCosmo = self._get_cosom(H_0, Om0, Ode0)\n        return lensCosmo.D_d"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef D_s(self, H_0, Om0, Ode0=None):\n        lensCosmo = self._get_cosom(H_0, Om0, Ode0)\n        return lensCosmo.D_s", "response": "get the angular diameter of source timecluster"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the angular diameter from deflector to source", "response": "def D_ds(self, H_0, Om0, Ode0=None):\n        \"\"\"\n        angular diameter from deflector to source\n        :param H_0: Hubble parameter [km/s/Mpc]\n        :param Om0: normalized matter density at present time\n        :return: float [Mpc]\n        \"\"\"\n        lensCosmo = self._get_cosom(H_0, Om0, Ode0)\n        return lensCosmo.D_ds"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the time delay distance between the base and the lens cosmology", "response": "def D_dt(self, H_0, Om0, Ode0=None):\n        \"\"\"\n        time delay distance\n        :param H_0: Hubble parameter [km/s/Mpc]\n        :param Om0: normalized matter density at present time\n        :return: float [Mpc]\n        \"\"\"\n        lensCosmo = self._get_cosom(H_0, Om0, Ode0)\n        return lensCosmo.D_dt"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn df dx dy of the function", "response": "def derivatives(self, x, y, coeffs, beta, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        shapelets = self._createShapelet(coeffs)\n        n_order = self._get_num_n(len(coeffs))\n        dx_shapelets = self._dx_shapelets(shapelets, beta)\n        dy_shapelets = self._dy_shapelets(shapelets, beta)\n        n = len(np.atleast_1d(x))\n        if n <= 1:\n            f_x = self._shapeletOutput(x, y, beta, dx_shapelets, precalc=False)\n            f_y = self._shapeletOutput(x, y, beta, dy_shapelets, precalc=False)\n        else:\n            H_x, H_y = self.pre_calc(x, y, beta, n_order+1, center_x, center_y)\n            f_x = self._shapeletOutput(H_x, H_y, beta, dx_shapelets)\n            f_y = self._shapeletOutput(H_x, H_y, beta, dy_shapelets)\n        return f_x, f_y"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef hessian(self, x, y, coeffs, beta, center_x=0, center_y=0):\n        shapelets = self._createShapelet(coeffs)\n        n_order = self._get_num_n(len(coeffs))\n        dxx_shapelets = self._dxx_shapelets(shapelets, beta)\n        dyy_shapelets = self._dyy_shapelets(shapelets, beta)\n        dxy_shapelets = self._dxy_shapelets(shapelets, beta)\n        n = len(np.atleast_1d(x))\n        if n <= 1:\n            f_xx = self._shapeletOutput(x, y, beta, dxx_shapelets, precalc=False)\n            f_yy = self._shapeletOutput(x, y, beta, dyy_shapelets, precalc=False)\n            f_xy = self._shapeletOutput(x, y, beta, dxy_shapelets, precalc=False)\n        else:\n            H_x, H_y = self.pre_calc(x, y, beta, n_order+2, center_x, center_y)\n            f_xx = self._shapeletOutput(H_x, H_y, beta, dxx_shapelets)\n            f_yy = self._shapeletOutput(H_x, H_y, beta, dyy_shapelets)\n            f_xy = self._shapeletOutput(H_x, H_y, beta, dxy_shapelets)\n        return f_xx, f_yy, f_xy", "response": "returns Hessian matrix of function d^2f / dx^2 dy^2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a shapelet array out of the coefficients a and b.", "response": "def _createShapelet(self, coeffs):\n        \"\"\"\n        returns a shapelet array out of the coefficients *a, up to order l\n\n        :param num_l: order of shapelets\n        :type num_l: int.\n        :param coeff: shapelet coefficients\n        :type coeff: floats\n        :returns:  complex array\n        :raises: AttributeError, KeyError\n        \"\"\"\n        n_coeffs = len(coeffs)\n        num_n = self._get_num_n(n_coeffs)\n        shapelets=np.zeros((num_n+1, num_n+1))\n        n = 0\n        k = 0\n        for coeff in coeffs:\n            shapelets[n-k][k] = coeff\n            k += 1\n            if k == n + 1:\n                n += 1\n                k = 0\n        return shapelets"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the the numerical values of a set of shapelets at polar coordinates :param shapelets: set of shapelets [l=,r=,a_lr=] :type shapelets: array of size (n,3) :param coordPolar: set of coordinates in polar units :type coordPolar: array of size (n,2) :returns: array of same size with coords [r,phi] :raises: AttributeError, KeyError", "response": "def _shapeletOutput(self, x, y, beta, shapelets, precalc=True):\n        \"\"\"\n        returns the the numerical values of a set of shapelets at polar coordinates\n        :param shapelets: set of shapelets [l=,r=,a_lr=]\n        :type shapelets: array of size (n,3)\n        :param coordPolar: set of coordinates in polar units\n        :type coordPolar: array of size (n,2)\n        :returns:  array of same size with coords [r,phi]\n        :raises: AttributeError, KeyError\n        \"\"\"\n        n = len(np.atleast_1d(x))\n        if n <= 1:\n            values = 0.\n        else:\n            values = np.zeros(len(x[0]))\n        n = 0\n        k = 0\n        i = 0\n        num_n = len(shapelets)\n        while i < num_n * (num_n+1)/2:\n            values += self._function(x, y, shapelets[n-k][k], beta, n-k, k, precalc=precalc)\n            k += 1\n            if k == n + 1:\n                n += 1\n                k = 0\n            i += 1\n        return values"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dx_shapelets(self, shapelets, beta):\n        num_n = len(shapelets)\n        dx = np.zeros((num_n+1, num_n+1))\n        for n1 in range(num_n):\n            for n2 in range(num_n):\n                amp = shapelets[n1][n2]\n                dx[n1+1][n2] -= np.sqrt((n1+1)/2.) * amp\n                if n1 > 0:\n                    dx[n1-1][n2] += np.sqrt(n1/2.) * amp\n        return dx/beta", "response": "Compute the derivative d / dx of the shapelets"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the derivative d / dx of the shapelets", "response": "def _dy_shapelets(self, shapelets, beta):\n        \"\"\"\n        computes the derivative d/dx of the shapelet coeffs\n        :param shapelets:\n        :param beta:\n        :return:\n        \"\"\"\n        num_n = len(shapelets)\n        dy = np.zeros((num_n+1, num_n+1))\n        for n1 in range(num_n):\n            for n2 in range(num_n):\n                amp = shapelets[n1][n2]\n                dy[n1][n2+1] -= np.sqrt((n2+1)/2.) * amp\n                if n2 > 0:\n                    dy[n1][n2-1] += np.sqrt(n2/2.) * amp\n        return dy/beta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef H_n(self, n, x):\n        n_array = np.zeros(n+1)\n        n_array[n] = 1\n        return hermite.hermval(x, n_array, tensor=False)", "response": "Returns the Hermite polynomial of order n at position x"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the 1 - dimenional basis function for the given n - dimensional array and x - dimensional array.", "response": "def phi_n(self,n,x):\n        \"\"\"\n        constructs the 1-dim basis function (formula (1) in Refregier et al. 2001)\n\n        :param n: The n'the basis function.\n        :type name: int.\n        :param x: 1-dim position (dimensionless)\n        :type state: float or numpy array.\n        :returns:  array-- phi_n(x).\n        :raises: AttributeError, KeyError\n        \"\"\"\n        prefactor = 1./np.sqrt(2**n*np.sqrt(np.pi)*math.factorial(n))\n        return prefactor*self.H_n(n,x)*np.exp(-x**2/2.)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract the array of parameters from kwargs_list", "response": "def _extract_array(self, kwargs_list):\n        \"\"\"\n        inverse of _update_kwargs\n        :param kwargs_list:\n        :return:\n        \"\"\"\n        lens_model = self._lens_mode_list[0]\n        if self._solver_type == 'CENTER':\n            center_x = kwargs_list[0]['center_x']\n            center_y = kwargs_list[0]['center_y']\n            x = [center_x, center_y]\n        elif self._solver_type == 'ELLIPSE':\n            e1 = kwargs_list[0]['e1']\n            e2 = kwargs_list[0]['e2']\n            x = [e1, e2]\n        elif self._solver_type == 'SHAPELETS':\n            coeffs = list(kwargs_list[0]['coeffs'])\n            [c10, c01] = coeffs[1: 3]\n            x = [c10, c01]\n        elif self._solver_type == 'THETA_E_PHI':\n            theta_E = kwargs_list[0]['theta_E']\n            e1 = kwargs_list[1]['e1']\n            e2 = kwargs_list[1]['e2']\n            phi_ext, gamma_ext = param_util.ellipticity2phi_gamma(e1, e2)\n            x = [theta_E, phi_ext]\n        else:\n            raise ValueError(\"Solver type %s not supported for 2-point solver!\" % self._solver_type)\n        return x"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the scaled effective exposure time of IID counts.", "response": "def scaled_exposure_time(self):\n        \"\"\"\n        scaled \"effective\" exposure time of IID counts. This can be used by lenstronomy to estimate the Poisson errors\n        keeping the assumption that the counts are IIDs (even if they are not).\n\n        :return: scaled exposure time\n        \"\"\"\n        if self._data_count_unit == 'ADU':\n            exp_time = self.ccd_gain * self.exposure_time\n        else:\n            exp_time = self.exposure_time\n        return exp_time"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting an apparent magnitude to counts per second", "response": "def magnitude2cps(self, magnitude):\n        \"\"\"\n        converts an apparent magnitude to counts per second (in units of the data)\n\n        The zero point of an instrument, by definition, is the magnitude of an object that produces one count\n        (or data number, DN) per second. The magnitude of an arbitrary object producing DN counts in an observation of\n        length EXPTIME is therefore:\n        m = -2.5 x log10(DN / EXPTIME) + ZEROPOINT\n\n        :param magnitude: magnitude of object\n        :return: counts per second of object\n        \"\"\"\n        # compute counts in units of ADS (as magnitude zero point is defined)\n        cps = data_util.magnitude2cps(magnitude, magnitude_zero_point=self._magnitude_zero_point)\n        if self._data_count_unit == 'e-':\n            cps *= self.ccd_gain\n        return cps"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the density of the n - tuple", "response": "def density(self, r, rho0, Ra, Rs):\n        \"\"\"\n        computes the density\n        :param x:\n        :param y:\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n        Ra, Rs = self._sort_ra_rs(Ra, Rs)\n        rho = rho0 / ((1 + (r / Ra) ** 2) * (1 + (r / Rs) ** 2))\n        return rho"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprojecting density of a 2D resource set", "response": "def density_2d(self, x, y, rho0, Ra, Rs, center_x=0, center_y=0):\n        \"\"\"\n        projected density\n        :param x:\n        :param y:\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n        Ra, Rs = self._sort_ra_rs(Ra, Rs)\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        sigma0 = self.rho2sigma(rho0, Ra, Rs)\n        sigma = sigma0 * Ra * Rs / (Rs - Ra) * (1 / np.sqrt(Ra ** 2 + r ** 2) - 1 / np.sqrt(Rs ** 2 + r ** 2))\n        return sigma"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the mass of a 3d sphere or radius r", "response": "def mass_3d(self, r, rho0, Ra, Rs):\n        \"\"\"\n        mass enclosed a 3d sphere or radius r\n        :param r:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n        m_3d = 4 * np.pi * rho0 * Ra ** 2 * Rs ** 2 / (Rs ** 2 - Ra ** 2) * (Rs * np.arctan(r / Rs) - Ra * np.arctan(r / Ra))\n        return m_3d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mass_2d(self, r, rho0, Ra, Rs):\n        Ra, Rs = self._sort_ra_rs(Ra, Rs)\n        sigma0 = self.rho2sigma(rho0, Ra, Rs)\n        m_2d = 2 * np.pi * sigma0 * Ra * Rs / (Rs - Ra) * (np.sqrt(Ra ** 2 + r ** 2) - Ra - np.sqrt(Rs ** 2 + r ** 2) + Rs)\n        return m_2d", "response": "return the mass of the 2d sphere of radius r"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate total mass within the profile", "response": "def mass_tot(self, rho0, Ra, Rs):\n        \"\"\"\n        total mass within the profile\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n        Ra, Rs = self._sort_ra_rs(Ra, Rs)\n        sigma0 = self.rho2sigma(rho0, Ra, Rs)\n        m_tot = 2 * np.pi * sigma0 * Ra * Rs\n        return m_tot"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _f_A20(self, r_a, r_s):\n        return r_a/(1+np.sqrt(1 + r_a**2)) - r_s/(1+np.sqrt(1 + r_s**2))", "response": "Equation A20 in Eliasdottir 2004."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting 3d density into 2d projected density", "response": "def rho2sigma(self, rho0, Ra, Rs):\n        \"\"\"\n        converts 3d density into 2d projected density parameter\n        :param rho0:\n        :param Ra:\n        :param Rs:\n        :return:\n        \"\"\"\n        return np.pi * rho0 * Ra * Rs / (Rs + Ra)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming sqrt scaling of the input numpy array.", "response": "def sqrt(inputArray, scale_min=None, scale_max=None):\n    \"\"\"Performs sqrt scaling of the input numpy array.\n\n    @type inputArray: numpy array\n    @param inputArray: image data array\n    @type scale_min: float\n    @param scale_min: minimum data value\n    @type scale_max: float\n    @param scale_max: maximum data value\n    @rtype: numpy array\n    @return: image data array\n\n    \"\"\"\n\n    imageData = np.array(inputArray, copy=True)\n\n    if scale_min is None:\n        scale_min = imageData.min()\n    if scale_max is None:\n        scale_max = imageData.max()\n\n    imageData = imageData.clip(min=scale_min, max=scale_max)\n    imageData = imageData - scale_min\n    indices = np.where(imageData < 0)\n    imageData[indices] = 0.0\n    imageData = np.sqrt(imageData)\n    imageData = imageData / math.sqrt(scale_max - scale_min)\n\n    return imageData"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the log likelihood of a MCMC or PSO chain", "response": "def logL(self, args):\n        \"\"\"\n        routine to compute X2 given variable parameters for a MCMC/PSO chain\n        \"\"\"\n        #extract parameters\n        kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, kwargs_cosmo = self.param.args2kwargs(args)\n        #generate image and computes likelihood\n        self._reset_point_source_cache(bool=True)\n        logL = 0\n        if self._check_bounds is True:\n            penalty, bound_hit = self.check_bounds(args, self._lower_limit, self._upper_limit)\n            logL -= penalty\n            if bound_hit:\n                return logL, None\n        if self._image_likelihood is True:\n            logL += self.image_likelihood.logL(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps)\n        if self._time_delay_likelihood is True:\n            logL += self.time_delay_likelihood.logL(kwargs_lens, kwargs_ps, kwargs_cosmo)\n        if self._check_positive_flux is True:\n            bool = self.param.check_positive_flux(kwargs_source, kwargs_lens_light, kwargs_ps)\n            if bool is False:\n                logL -= 10**10\n        if self._flux_ratio_likelihood is True:\n            logL += self.flux_ratio_likelihood.logL(kwargs_lens, kwargs_ps, kwargs_cosmo)\n        logL += self._position_likelihood.logL(kwargs_lens, kwargs_ps, kwargs_cosmo)\n        logL += self._prior_likelihood.logL(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, kwargs_cosmo)\n        self._reset_point_source_cache(bool=False)\n        return logL, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks whether the parameter vector has left its bound and adds a big number if so.", "response": "def check_bounds(args, lowerLimit, upperLimit):\n        \"\"\"\n        checks whether the parameter vector has left its bound, if so, adds a big number\n        \"\"\"\n        penalty = 0\n        bound_hit = False\n        for i in range(0, len(args)):\n            if args[i] < lowerLimit[i] or args[i] > upperLimit[i]:\n                penalty = 10**15\n                bound_hit = True\n        return penalty, bound_hit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the effective number of data points considered in the X2 estimation to compute the reduced X2 value", "response": "def effectiv_num_data_points(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps):\n        \"\"\"\n        returns the effective number of data points considered in the X2 estimation to compute the reduced X2 value\n        \"\"\"\n        num_linear = 0\n        if self._image_likelihood is True:\n            num_linear = self.image_likelihood.num_param_linear(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps)\n        num_param, _ = self.param.num_param()\n        return self.num_data - num_param - num_linear"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef image2source(self, x, y, kwargs_lens, idex_source):\n        if self._multi_source_plane is False:\n            x_source, y_source = self._lensModel.ray_shooting(x, y, kwargs_lens)\n        else:\n            if self._multi_lens_plane is False:\n                x_alpha, y_alpha = self._lensModel.alpha(x, y, kwargs_lens)\n                scale_factor = self._deflection_scaling_list[idex_source]\n                x_source = x - x_alpha * scale_factor\n                y_source = y - y_alpha * scale_factor\n            else:\n                z_stop = self._source_redshift_list[idex_source]\n                x_comov, y_comov, alpha_x, alpha_y = self._lensModel.lens_model.ray_shooting_partial(0, 0, x, y,\n                                                                                                     0, z_stop,\n                                                                                                     kwargs_lens,\n                                                                                                     keep_range=False,\n                                                                                                     include_z_start=False)\n\n                T_z = self._bkg_cosmo.T_xy(0, z_stop)\n                x_source = x_comov / T_z\n                y_source = y_comov / T_z\n        return x_source, y_source", "response": "This function maps an image plane to the source plane coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn Hessian matrix of function f_xx y_xx f_yy f_xy", "response": "def hessian(self, x, y, amp, sigma, e1, e2, center_x=0, center_y=0):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        alpha_ra, alpha_dec = self.derivatives(x, y, amp, sigma, e1, e2, center_x, center_y)\n        diff = self._diff\n        alpha_ra_dx, alpha_dec_dx = self.derivatives(x + diff, y, amp, sigma, e1, e2, center_x, center_y)\n        alpha_ra_dy, alpha_dec_dy = self.derivatives(x, y + diff, amp, sigma, e1, e2, center_x, center_y)\n\n        f_xx = (alpha_ra_dx - alpha_ra) / diff\n        f_xy = (alpha_ra_dy - alpha_ra) / diff\n        # f_yx = (alpha_dec_dx - alpha_dec)/diff\n        f_yy = (alpha_dec_dy - alpha_dec) / diff\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the average LOS velocity dispersion in the slit", "response": "def vel_disp(self, kwargs_profile, kwargs_aperture, kwargs_light, kwargs_anisotropy, num=1000):\n        \"\"\"\n        computes the averaged LOS velocity dispersion in the slit (convolved)\n        :param gamma:\n        :param phi_E:\n        :param r_eff:\n        :param r_ani:\n        :param R_slit:\n        :param FWHM:\n        :return:\n        \"\"\"\n        sigma_s2_sum = 0\n        for i in range(0, num):\n            sigma_s2_draw = self._vel_disp_one(kwargs_profile, kwargs_aperture, kwargs_light, kwargs_anisotropy)\n            sigma_s2_sum += sigma_s2_draw\n        sigma_s2_average = sigma_s2_sum/num\n        return np.sqrt(sigma_s2_average)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _vel_disp_one(self, kwargs_profile, kwargs_aperture, kwargs_light, kwargs_anisotropy):\n\n        while True:\n            r = self.lightProfile.draw_light(kwargs_light)  # draw r\n            R, x, y = util.R_r(r)  # draw projected R\n            x_, y_ = util.displace_PSF(x, y, self.FWHM)  # displace via PSF\n            bool = self.aperture.aperture_select(x_, y_, kwargs_aperture)\n            if bool is True:\n                break\n        sigma_s2 = self.sigma_s2(r, R, kwargs_profile, kwargs_anisotropy, kwargs_light)\n        return sigma_s2", "response": "Compute one realisation of the velocity dispersion realized in the slit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sigma_s2(self, r, R, kwargs_profile, kwargs_anisotropy, kwargs_light):\n        beta = self.anisotropy.beta_r(r, kwargs_anisotropy)\n        return (1 - beta * R**2/r**2) * self.sigma_r2(r, kwargs_profile, kwargs_anisotropy, kwargs_light)", "response": "projected velocity dispersion\n        :param r:\n        :param R:\n        :param r_ani:\n        :param a:\n        :param gamma:\n        :param phi_E:\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the radial velocity dispersion at radius r", "response": "def sigma_r2(self, r, kwargs_profile, kwargs_anisotropy, kwargs_light):\n        \"\"\"\n        computes radial velocity dispersion at radius r (solving the Jeans equation\n        :param r:\n        :return:\n        \"\"\"\n        return self.jeans_solver.sigma_r2(r, kwargs_profile, kwargs_anisotropy, kwargs_light)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving radial Jeans equation", "response": "def sigma_r2(self, r, kwargs_profile, kwargs_anisotropy, kwargs_light):\n        \"\"\"\n        solves radial Jeans equation\n        \"\"\"\n        if self._mass_profile == 'power_law':\n            if self._anisotropy_type == 'r_ani':\n                if self._light_profile == 'Hernquist':\n                    sigma_r = self.power_law_anisotropy(r, kwargs_profile, kwargs_anisotropy, kwargs_light)\n                else:\n                    raise ValueError('light profile %s not supported for Jeans solver' % self._light_profile)\n            else:\n                raise ValueError('anisotropy type %s not implemented in Jeans equation modelling' % self._anisotropy_type)\n        else:\n            raise ValueError('mass profile type %s not implemented in Jeans solver' % self._mass_profile)\n        return sigma_r"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a bool list if the coordinate is within the aperture ( list", "response": "def aperture_select(self, ra, dec, kwargs_aperture):\n        \"\"\"\n        returns a bool list if the coordinate is within the aperture (list)\n        :param ra:\n        :param dec:\n        :return:\n        \"\"\"\n        if self._aperture_type == 'shell':\n            bool_list = self.shell_select(ra, dec, **kwargs_aperture)\n        elif self._aperture_type == 'slit':\n            bool_list = self.slit_select(ra, dec, **kwargs_aperture)\n        else:\n            raise ValueError(\"aperture type %s not implemented!\" % self._aperture_type)\n        return bool_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the image (lens and source surface brightness with a given lens model). The linear parameters are computed with a weighted linear least square optimization (i.e. flux normalization of the brightness profiles) :param kwargs_lens: list of keyword arguments corresponding to the superposition of different lens profiles :param kwargs_source: list of keyword arguments corresponding to the superposition of different source light profiles :param kwargs_lens_light: list of keyword arguments corresponding to different lens light surface brightness profiles :param kwargs_else: keyword arguments corresponding to \"other\" parameters, such as external shear and point source image positions :param inv_bool: if True, invert the full linear solver Matrix Ax = y for the purpose of the covariance matrix. :return: 1d array of surface brightness pixels of the optimal solution of the linear parameters to match the data", "response": "def image_linear_solve(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_else, inv_bool=False):\n        \"\"\"\n        computes the image (lens and source surface brightness with a given lens model).\n        The linear parameters are computed with a weighted linear least square optimization (i.e. flux normalization of the brightness profiles)\n        :param kwargs_lens: list of keyword arguments corresponding to the superposition of different lens profiles\n        :param kwargs_source: list of keyword arguments corresponding to the superposition of different source light profiles\n        :param kwargs_lens_light: list of keyword arguments corresponding to different lens light surface brightness profiles\n        :param kwargs_else: keyword arguments corresponding to \"other\" parameters, such as external shear and point source image positions\n        :param inv_bool: if True, invert the full linear solver Matrix Ax = y for the purpose of the covariance matrix.\n        :return: 1d array of surface brightness pixels of the optimal solution of the linear parameters to match the data\n        \"\"\"\n        wls_list, error_map_list, cov_param_list, param_list = [], [], [], []\n        for i in range(self._num_bands):\n            wls_model, error_map, cov_param, param = self._imageModel_list[i].image_linear_solve(kwargs_lens,\n                                                                                                 kwargs_source,\n                                                                                                 kwargs_lens_light,\n                                                                                                 kwargs_else,\n                                                                                                 inv_bool=inv_bool)\n            wls_list.append(wls_model)\n            error_map_list.append(error_map)\n            cov_param_list.append(cov_param)\n            param_list.append(param)\n        return wls_list, error_map_list, cov_param_list, param_list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef likelihood_data_given_model(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, source_marg=False):\n        # generate image\n        logL = 0\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                logL += self._imageModel_list[i].likelihood_data_given_model(kwargs_lens, kwargs_source,\n                                                                             kwargs_lens_light, kwargs_ps,\n                                                                             source_marg=source_marg)\n        return logL", "response": "computes the likelihood of the data given a model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef image_linear_solve(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, inv_bool=False):\n        A = self.linear_response_matrix(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps)\n        C_D_response, model_error_list = self.error_response(kwargs_lens, kwargs_ps)\n        d = self.data_response\n        param, cov_param, wls_model = de_lens.get_param_WLS(A.T, 1 / C_D_response, d, inv_bool=inv_bool)\n        kwargs_lens_0 = [kwargs_lens[k] for k in self._idex_lens_list[0]]\n        _, _, _, _ = self._imageModel_list[0]._update_linear_kwargs(param, kwargs_lens_0, kwargs_source, kwargs_lens_light, kwargs_ps)\n        wls_list = self._array2image_list(wls_model)\n        return wls_list, model_error_list, cov_param, param", "response": "This method computes the image linear parameters for a given set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the 1d array of the error estimate corresponding to the data response MimeType", "response": "def error_response(self, kwargs_lens, kwargs_ps):\n        \"\"\"\n        returns the 1d array of the error estimate corresponding to the data response\n\n        :return: 1d numpy array of response, 2d array of additonal errors (e.g. point source uncertainties)\n        \"\"\"\n        C_D_response, model_error = [], []\n        for i in range(self._num_bands):\n            if self._compute_bool[i] is True:\n                kwargs_lens_i = [kwargs_lens[k] for k in self._idex_lens_list[i]]\n                C_D_response_i, model_error_i = self._imageModel_list[i].error_response(kwargs_lens_i, kwargs_ps)\n                model_error.append(model_error_i)\n                if C_D_response == []:\n                    C_D_response = C_D_response_i\n                else:\n                    C_D_response = np.append(C_D_response, C_D_response_i)\n        return C_D_response, model_error"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfunction to calculate the Gaussian logarithm of the logarithm of the current logarithm.", "response": "def function(self, x, y, amp, sigma_x, sigma_y, center_x=0, center_y=0):\n        \"\"\"\n        returns Gaussian\n        \"\"\"\n        c = amp/(2*np.pi*sigma_x*sigma_y)\n        delta_x = x - center_x\n        delta_y = y - center_y\n        exponent = -((delta_x/sigma_x)**2+(delta_y/sigma_y)**2)/2.\n        return c * np.exp(exponent)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn df dx and df dy of the function", "response": "def derivatives(self, x, y, amp, sigma_x, sigma_y, center_x=0, center_y=0):\n        \"\"\"\n        returns df/dx and df/dy of the function\n        \"\"\"\n        f_ = self.function(x, y, amp, sigma_x, sigma_y, center_x, center_y)\n        return f_ * (center_x-x)/sigma_x**2, f_ * (center_y-y)/sigma_y**2"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Hessian matrix of function f", "response": "def hessian(self, x, y, amp, sigma_x, sigma_y, center_x = 0, center_y = 0):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        \"\"\"\n        f_ = self.function(x, y, amp, sigma_x, sigma_y, center_x, center_y)\n        f_xx = f_ * ( (-1./sigma_x**2) + (center_x-x)**2/sigma_x**4 )\n        f_yy = f_ * ( (-1./sigma_y**2) + (center_y-y)**2/sigma_y**4 )\n        f_xy = f_ * (center_x-x)/sigma_x**2 * (center_y-y)/sigma_y**2\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating M_200 for NFW profile", "response": "def M200(self, Rs, rho0, c):\n        \"\"\"\n        M(R_200) calculation for NFW profile\n\n        :param Rs: scale radius\n        :type Rs: float\n        :param rho0: density normalization (characteristic density)\n        :type rho0: float\n        :param c: concentration\n        :type c: float [4,40]\n        :return: M(R_200) density\n        \"\"\"\n        return 4*np.pi*rho0*Rs**3*(np.log(1.+c)-c/(1.+c))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rho0_c(self, c):\n        return 200./3*self.rhoc*c**3/(np.log(1.+c)-c/(1.+c))", "response": "computes density normalization as a function of concentration parameter\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the concentration given a comoving overdensity rho0", "response": "def c_rho0(self, rho0):\n        \"\"\"\n        computes the concentration given a comoving overdensity rho0 (inverse of function rho0_c)\n        :param rho0: density normalization in h^2/Mpc^3 (comoving)\n        :return: concentration parameter c\n        \"\"\"\n        if not hasattr(self, '_c_rho0_interp'):\n            c_array = np.linspace(0.1, 10, 100)\n            rho0_array = self.rho0_c(c_array)\n            from scipy import interpolate\n            self._c_rho0_interp = interpolate.InterpolatedUnivariateSpline(rho0_array, c_array, w=None, bbox=[None, None], k=3)\n        return self._c_rho0_interp(rho0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef c_M_z(self, M, z):\n        # fitted parameter values\n        A = 5.22\n        B = -0.072\n        C = -0.42\n        M_pivot = 2.*10**12\n        return A*(M/M_pivot)**B*(1+z)**C", "response": "fitting function of http://moriond. in2p3. fr/J08_proceedings_duffy. pdf for the mass and redshift dependence of the concentration parameter M_sun and redshift dependence of the concentration parameter z"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning all needed parameter in comoving units modulo h to draw the profile of the main halo Arc Arc", "response": "def profileMain(self, M, z):\n        \"\"\"\n        returns all needed parameter (in comoving units modulo h) to draw the profile of the main halo\n        r200 in co-moving Mpc/h\n        rho_s in  h^2/Mpc^3 (co-moving)\n        Rs in Mpc/h co-moving\n        c unit less\n        \"\"\"\n        c = self.c_M_z(M, z)\n        r200 = self.r200_M(M)\n        rho0 = self.rho0_c(c)\n        Rs = r200/c\n        return r200, rho0, c, Rs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction to reconstruct the model without including the point source contributions as a list", "response": "def image_single_point_source(self, image_model_class, kwargs_lens, kwargs_source, kwargs_lens_light,\n                                  kwargs_ps):\n        \"\"\"\n        return model without including the point source contributions as a list (for each point source individually)\n        :param image_model_class: ImageModel class instance\n        :param kwargs_lens: lens model kwargs list\n        :param kwargs_source: source model kwargs list\n        :param kwargs_lens_light: lens light model kwargs list\n        :param kwargs_ps: point source model kwargs list\n        :return: list of images with point source isolated\n        \"\"\"\n        # reconstructed model with given psf\n        model, error_map, cov_param, param = image_model_class.image_linear_solve(kwargs_lens, kwargs_source,\n                                                                              kwargs_lens_light, kwargs_ps)\n        #model = image_model_class.image(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps)\n        data = image_model_class.Data.data\n        mask = image_model_class.ImageNumerics.mask\n        point_source_list = image_model_class.point_sources_list(kwargs_ps, kwargs_lens)\n        n = len(point_source_list)\n        model_single_source_list = []\n        for i in range(n):\n            model_single_source = (data - model + point_source_list[i]) * mask\n            model_single_source_list.append(model_single_source)\n        return model_single_source_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef combine_psf(kernel_list_new, kernel_old, sigma_bkg, factor=1, stacking_option='median', symmetry=1):\n\n        n = int(len(kernel_list_new) * symmetry)\n        angle = 360. / symmetry\n        kernelsize = len(kernel_old)\n        kernel_list = np.zeros((n, kernelsize, kernelsize))\n        i = 0\n        for kernel_new in kernel_list_new:\n            for k in range(symmetry):\n                kernel_rotated = image_util.rotateImage(kernel_new, angle * k)\n                kernel_norm = kernel_util.kernel_norm(kernel_rotated)\n                kernel_list[i, :, :] = kernel_norm\n                i += 1\n\n        kernel_old_rotated = np.zeros((symmetry, kernelsize, kernelsize))\n        for i in range(symmetry):\n            kernel_old_rotated[i, :, :] = kernel_old\n\n        kernel_list_new = np.append(kernel_list, kernel_old_rotated, axis=0)\n        if stacking_option == 'median':\n            kernel_new = np.median(kernel_list_new, axis=0)\n        elif stacking_option == 'mean':\n            kernel_new = np.mean(kernel_list_new, axis=0)\n        else:\n            raise ValueError(\" stack_option must be 'median' or 'mean', %s is not supported.\" % stacking_option)\n        kernel_new[kernel_new < 0] = 0\n        kernel_new = kernel_util.kernel_norm(kernel_new)\n        kernel_return = factor * kernel_new + (1.-factor)* kernel_old\n\n        kernel_bkg = copy.deepcopy(kernel_return)\n        kernel_bkg[kernel_bkg < sigma_bkg] = sigma_bkg\n        error_map = np.var(kernel_list_new, axis=0) / kernel_bkg**2 / 2.\n        return kernel_return, error_map", "response": "combine_psf computes the new PSF kernels and error_map for the image"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef logLikelihood(self, D_d, D_delta_t):\n        if self._kde_type == 'scipy_gaussian':\n            density = self._PDF_kernel([D_d, D_delta_t])\n            logL = np.log(density)\n        else:\n            x = np.array([[D_d], [D_delta_t]])\n            logL = self._kde.score_samples(x.T)\n        return logL", "response": "This method calculates the log likelihood of the data given a model with predicted angular diameter distances and time - delays."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the density of the n - tuple", "response": "def density(self, r, rho0, Rs):\n        \"\"\"\n        computes the density\n        :param x:\n        :param y:\n        :param rho0:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        rho = rho0 / (r/Rs * (1 + (r/Rs))**3)\n        return rho"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef density_2d(self, x, y, rho0, Rs, center_x=0, center_y=0):\n        x_ = x - center_x\n        y_ = y - center_y\n        r = np.sqrt(x_**2 + y_**2)\n        X = r/Rs\n        sigma0 = self.rho2sigma(rho0, Rs)\n        if isinstance(X, int) or isinstance(X, float):\n            if X == 1:\n                X = 1.000001\n        else:\n            X[X == 1] = 1.000001\n        sigma = sigma0 / (X**2-1)**2 * (-3 + (2+X**2)*self._F(X))\n        return sigma", "response": "projected density of a 2D logarithmic resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the mass of a 3d sphere or radius r", "response": "def mass_3d(self, r, rho0, Rs):\n        \"\"\"\n        mass enclosed a 3d sphere or radius r\n        :param r:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        mass_3d = 2*np.pi*Rs**3*rho0 * r**2/(r + Rs)**2\n        return mass_3d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mass_3d_lens(self, r, sigma0, Rs):\n        rho0 = self.sigma2rho(sigma0, Rs)\n        return self.mass_3d(r, rho0, Rs)", "response": "mass enclosed a 3d sphere or radius r for lens parameterisation\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mass_2d(self, r, rho0, Rs):\n\n        sigma0 = self.rho2sigma(rho0, Rs)\n        return self.mass_2d_lens(r, sigma0, Rs)", "response": "return the mass enclosed projected 2d sphere of radius r"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate 2d mass of a 2d sphere of radius r", "response": "def mass_2d_lens(self, r, sigma0, Rs):\n        \"\"\"\n        mass enclosed projected 2d sphere of radius r\n        :param r:\n        :param rho0:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        X = r/Rs\n        alpha_r = 2*sigma0 * Rs * X * (1-self._F(X)) / (X**2-1)\n        mass_2d = alpha_r * r * np.pi\n        return mass_2d"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates total mass within the profile", "response": "def mass_tot(self, rho0, Rs):\n        \"\"\"\n        total mass within the profile\n        :param rho0:\n        :param a:\n        :param s:\n        :return:\n        \"\"\"\n        m_tot = 2*np.pi*rho0*Rs**3\n        return m_tot"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ray_shooting(self, x, y, kwargs, k=None):\n        return self.lens_model.ray_shooting(x, y, kwargs, k=k)", "response": "ray shooting for the image"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the fermat potential of the image at the given location", "response": "def fermat_potential(self, x_image, y_image, x_source, y_source, kwargs_lens):\n        \"\"\"\n        fermat potential (negative sign means earlier arrival time)\n\n        :param x_image: image position\n        :param y_image: image position\n        :param x_source: source position\n        :param y_source: source position\n        :param kwargs_lens: list of keyword arguments of lens model parameters matching the lens model classes\n        :return: fermat potential in arcsec**2 without geometry term (second part of Eqn 1 in Suyu et al. 2013) as a list\n        \"\"\"\n        if hasattr(self.lens_model, 'fermat_potential'):\n            return self.lens_model.fermat_potential(x_image, y_image, x_source, y_source, kwargs_lens)\n        else:\n            raise ValueError(\"Fermat potential is not defined in multi-plane lensing. Please use single plane lens models.\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef potential(self, x, y, kwargs, k=None):\n        return self.lens_model.potential(x, y, kwargs, k=k)", "response": "lensing potential\n\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: lensing potential in units of arcsec^2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the deflection angles of the current set of lens models", "response": "def alpha(self, x, y, kwargs, k=None):\n        \"\"\"\n        deflection angles\n\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: deflection angles in units of arcsec\n        \"\"\"\n        return self.lens_model.alpha(x, y, kwargs, k=k)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hessian(self, x, y, kwargs, k=None):\n        return self.lens_model.hessian(x, y, kwargs, k=k)", "response": "compute the hessian of the current set of components"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef flexion(self, x, y, kwargs, diff=0.000001):\n        f_xx, f_xy, f_yx, f_yy = self.hessian(x, y, kwargs)\n\n        f_xx_dx, f_xy_dx, f_yx_dx, f_yy_dx = self.hessian(x + diff, y, kwargs)\n        f_xx_dy, f_xy_dy, f_yx_dy, f_yy_dy = self.hessian(x, y + diff, kwargs)\n\n        f_xxx = (f_xx_dx - f_xx) / diff\n        f_xxy = (f_xx_dy - f_xx) / diff\n        f_xyy = (f_xy_dy - f_xy) / diff\n        f_yyy = (f_yy_dy - f_yy) / diff\n        return f_xxx, f_xxy, f_xyy, f_yyy", "response": "flexion is the inverse of the hessian function"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef X2_chain_omega_mh2(self, args):\n        H0 = args[0]\n        h = H0/100.\n        omega_m = self.omega_mh2_fixed / h**2\n        Ode0 = self._omega_lambda_fixed\n        logL, bool = self.prior_omega_mh2(h, omega_m)\n        if bool is True:\n            logL += self.LCDM_lensLikelihood(H0, omega_m, Ode0)\n        return logL, None", "response": "X2_chain_omega_mh2 routine to compute the log likelihood given a omega_m h ** 2 prior fixed\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prior_H0(self, H0, H0_min=0, H0_max=200):\n        if H0 < H0_min or H0 > H0_max:\n            penalty = -10**15\n            return penalty, False\n        else:\n            return 0, True", "response": "checks whether the parameter vector has left its bound adds a big number to the prior H0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck whether the parameter omega_m is within the given bounds and returns the penalty and a boolean indicating if the parameter omega_m is prior to the given omega_m_min and omega_m_max.", "response": "def prior_omega_m(self, omega_m, omega_m_min=0, omega_m_max=1):\n        \"\"\"\n        checks whether the parameter omega_m is within the given bounds\n        :param omega_m:\n        :param omega_m_min:\n        :param omega_m_max:\n        :return:\n        \"\"\"\n        if omega_m < omega_m_min or omega_m > omega_m_max:\n            penalty = -10**15\n            return penalty, False\n        else:\n            return 0, True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mcmc_emcee(self, n_walkers, n_run, n_burn, mean_start, sigma_start):\n        sampler = emcee.EnsembleSampler(n_walkers, self.cosmoParam.numParam, self.chain.likelihood)\n        p0 = emcee.utils.sample_ball(mean_start, sigma_start, n_walkers)\n        new_pos, _, _, _ = sampler.run_mcmc(p0, n_burn)\n        sampler.reset()\n        store = InMemoryStorageUtil()\n        for pos, prob, _, _ in sampler.sample(new_pos, iterations=n_run):\n            store.persistSamplingValues(pos, prob, None)\n        return store.samples", "response": "Runs the mcmc analysis of the parameter space."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconfigures the data keyword arguments with a coordinate grid centered at zero. :param numPix: number of pixel (numPix x numPix) :param deltaPix: pixel size (in angular units) :param exposure_time: exposure time :param sigma_bkg: background noise (Gaussian sigma) :param inverse: if True, coordinate system is ra to the left, if False, to the right :return: keyword arguments that can be used to construct a Data() class instance of lenstronomy", "response": "def data_configure_simple(numPix, deltaPix, exposure_time=1, sigma_bkg=1, inverse=False):\n    \"\"\"\n    configures the data keyword arguments with a coordinate grid centered at zero.\n\n    :param numPix: number of pixel (numPix x numPix)\n    :param deltaPix: pixel size (in angular units)\n    :param exposure_time: exposure time\n    :param sigma_bkg: background noise (Gaussian sigma)\n    :param inverse: if True, coordinate system is ra to the left, if False, to the right\n    :return: keyword arguments that can be used to construct a Data() class instance of lenstronomy\n    \"\"\"\n    mean = 0.  # background mean flux (default zero)\n    # 1d list of coordinates (x,y) of a numPix x numPix square grid, centered to zero\n    x_grid, y_grid, ra_at_xy_0, dec_at_xy_0, x_at_radec_0, y_at_radec_0, Mpix2coord, Mcoord2pix = util.make_grid_with_coordtransform(numPix=numPix, deltapix=deltaPix, subgrid_res=1, inverse=inverse)\n    # mask (1= model this pixel, 0= leave blanck)\n    exposure_map = np.ones((numPix, numPix)) * exposure_time  # individual exposure time/weight per pixel\n\n    kwargs_data = {\n        'background_rms': sigma_bkg,\n        'exposure_map': exposure_map\n        , 'ra_at_xy_0': ra_at_xy_0, 'dec_at_xy_0': dec_at_xy_0, 'transform_pix2angle': Mpix2coord\n        , 'image_data': np.zeros((numPix, numPix))\n        }\n    return kwargs_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef psf_configure_simple(psf_type=\"GAUSSIAN\", fwhm=1, kernelsize=11, deltaPix=1, truncate=6, kernel=None):\n\n    if psf_type == 'GAUSSIAN':\n        sigma = util.fwhm2sigma(fwhm)\n        sigma_axis = sigma\n        gaussian = Gaussian()\n        x_grid, y_grid = util.make_grid(kernelsize, deltaPix)\n        kernel_large = gaussian.function(x_grid, y_grid, amp=1., sigma_x=sigma_axis, sigma_y=sigma_axis, center_x=0, center_y=0)\n        kernel_large /= np.sum(kernel_large)\n        kernel_large = util.array2image(kernel_large)\n        kernel_pixel = kernel_util.pixel_kernel(kernel_large)\n        kwargs_psf = {'psf_type': psf_type, 'fwhm': fwhm, 'truncation': truncate*fwhm, 'kernel_point_source': kernel_large, 'kernel_pixel': kernel_pixel, 'pixel_size': deltaPix}\n    elif psf_type == 'PIXEL':\n        kernel_large = copy.deepcopy(kernel)\n        kernel_large = kernel_util.cut_psf(kernel_large, psf_size=kernelsize)\n        kwargs_psf = {'psf_type': \"PIXEL\", 'kernel_point_source': kernel_large}\n    elif psf_type == 'NONE':\n        kwargs_psf = {'psf_type': 'NONE'}\n    else:\n        raise ValueError(\"psf type %s not supported!\" % psf_type)\n    return kwargs_psf", "response": "This routine initializes a PSF class in lenstronomy with the given parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _theta_E_convert(self, theta_E, w_c, w_t):\n        if not w_t > w_c:\n            w_t, w_c = w_c, w_t\n        s_scale_1 = w_c\n        s_scale_2 = w_t\n        f_x_1, f_y_1 = self.nie.derivatives(1, 0, theta_E=1, e1=0, e2=0, s_scale=s_scale_1)\n        f_x_2, f_y_2 = self.nie.derivatives(1, 0, theta_E=1, e1=0, e2=0, s_scale=s_scale_2)\n        f_x = f_x_1 - f_x_2\n        theta_E_convert = theta_E / f_x\n        return theta_E_convert, w_c, w_t", "response": "convert the parameter theta_E into the parameter w_c w_t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef derivatives(self, x, y, center_x = 0, center_y = 0, **kwargs):\n\n        \"\"\"\n        returns df/dx and df/dy (un-normalized!!!) interpolated from the numerical deflection table\n        \"\"\"\n\n        assert 'norm' in kwargs.keys(), \"key word arguments must contain 'norm', \" \\\n                                        \"the normalization of deflection angle in units of arcsec.\"\n\n        x_ = x - center_x\n        y_ = y - center_y\n        R = np.sqrt(x_**2 + y_**2)\n\n        alpha = self._interp(x_, y_, **kwargs)\n\n        cos_theta = x_ * R ** -1\n        sin_theta = y_ * R ** -1\n\n        f_x, f_y = alpha * cos_theta, alpha * sin_theta\n\n        return f_x, f_y", "response": "returns df dx dy interpolated from the numerical deflection table\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the Hessian matrix of function d^2f dx y d^2 f dy and f xy", "response": "def hessian(self, x, y, center_x = 0, center_y = 0, **kwargs):\n        \"\"\"\n        returns Hessian matrix of function d^2f/dx^2, d^f/dy^2, d^2/dxdy\n        (un-normalized!!!) interpolated from the numerical deflection table\n        \"\"\"\n\n        diff = 1e-6\n        alpha_ra, alpha_dec = self.derivatives(x, y, center_x = center_x, center_y = center_y,\n                                               **kwargs)\n\n        alpha_ra_dx, alpha_dec_dx = self.derivatives(x + diff, y, center_x = center_x, center_y = center_y,\n                                               **kwargs)\n        alpha_ra_dy, alpha_dec_dy = self.derivatives(x, y + diff, center_x = center_x, center_y = center_y,\n                                               **kwargs)\n\n        dalpha_rara = (alpha_ra_dx - alpha_ra) / diff\n        dalpha_radec = (alpha_ra_dy - alpha_ra) / diff\n        dalpha_decdec = (alpha_dec_dy - alpha_dec) / diff\n\n        f_xx = dalpha_rara\n        f_yy = dalpha_decdec\n        f_xy = dalpha_radec\n\n        return f_xx, f_yy, f_xy"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef derivatives(self, x, y, n_sersic, R_sersic, k_eff, e1, e2, center_x=0, center_y=0):\n        phi_G, q = param_util.ellipticity2phi_q(e1, e2)\n        e = abs(1. - q)\n        cos_phi = np.cos(phi_G)\n        sin_phi = np.sin(phi_G)\n        x_, y_ = self._coord_transf(x, y, q, phi_G, center_x, center_y)\n        f_x_prim, f_y_prim = self.sersic.derivatives(x_, y_, n_sersic, R_sersic, k_eff)\n        f_x_prim *= np.sqrt(1 - e)\n        f_y_prim *= np.sqrt(1 + e)\n        f_x = cos_phi*f_x_prim-sin_phi*f_y_prim\n        f_y = sin_phi*f_x_prim+cos_phi*f_y_prim\n        return f_x, f_y", "response": "returns df x y of the function that is derivatives of the sersic function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_layer2image(grid2d, x_pos, y_pos, kernel, order=1):\n\n    x_int = int(round(x_pos))\n    y_int = int(round(y_pos))\n    shift_x = x_int - x_pos\n    shift_y = y_int - y_pos\n    kernel_shifted = interp.shift(kernel, [-shift_y, -shift_x], order=order)\n    return add_layer2image_int(grid2d, x_int, y_int, kernel_shifted)", "response": "Adds a kernel on the grid2d image at position x_pos y_pos and returns the image with the added layer cut to original size"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_layer2image_int(grid2d, x_pos, y_pos, kernel):\n    nx, ny = np.shape(kernel)\n    if nx % 2 == 0:\n        raise ValueError(\"kernel needs odd numbers of pixels\")\n\n    num_x, num_y = np.shape(grid2d)\n    x_int = int(round(x_pos))\n    y_int = int(round(y_pos))\n\n    k_x, k_y = np.shape(kernel)\n    k_l2_x = int((k_x - 1) / 2)\n    k_l2_y = int((k_y - 1) / 2)\n\n    min_x = np.maximum(0, x_int-k_l2_x)\n    min_y = np.maximum(0, y_int-k_l2_y)\n    max_x = np.minimum(num_x, x_int+k_l2_x + 1)\n    max_y = np.minimum(num_y, y_int+k_l2_y + 1)\n\n    min_xk = np.maximum(0, -x_int + k_l2_x)\n    min_yk = np.maximum(0, -y_int + k_l2_y)\n    max_xk = np.minimum(k_x, -x_int + k_l2_x + num_x)\n    max_yk = np.minimum(k_y, -y_int + k_l2_y + num_y)\n    if min_x >= max_x or min_y >= max_y or min_xk >= max_xk or min_yk >= max_yk or (max_x-min_x != max_xk-min_xk) or (max_y-min_y != max_yk-min_yk):\n        return grid2d\n    kernel_re_sized = kernel[min_yk:max_yk, min_xk:max_xk]\n    new = grid2d.copy()\n    new[min_y:max_y, min_x:max_x] += kernel_re_sized\n    return new", "response": "Adds a kernel on the grid2d image at position x_pos y_pos at integer positions of pixel\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd background noise to the image", "response": "def add_background(image, sigma_bkd):\n    \"\"\"\n    adds background noise to image\n    :param image: pixel values of image\n    :param sigma_bkd: background noise (sigma)\n    :return: a realisation of Gaussian noise of the same size as image\n    \"\"\"\n    if sigma_bkd < 0:\n        raise ValueError(\"Sigma background is smaller than zero! Please use positive values.\")\n    nx, ny = np.shape(image)\n    background = np.random.randn(nx, ny) * sigma_bkd\n    return background"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_poisson(image, exp_time):\n    \"\"\"\n    adds a poison (or Gaussian) distributed noise with mean given by surface brightness\n    \"\"\"\n    if isinstance(exp_time, int) or isinstance(exp_time, float):\n        if exp_time <= 0:\n            exp_time = 1\n    else:\n        mean_exp_time = np.mean(exp_time)\n        exp_time[exp_time < mean_exp_time/10] = mean_exp_time/10\n    sigma = np.sqrt(np.abs(image)/exp_time) # Gaussian approximation for Poisson distribution, normalized to exposure time\n    nx, ny = np.shape(image)\n    poisson = np.random.randn(nx, ny) * sigma\n    return poisson", "response": "Adds a poison or Gaussian realization of input image to the next image in the next image."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef re_size_array(x_in, y_in, input_values, x_out, y_out):\n    interp_2d = scipy.interpolate.interp2d(x_in, y_in, input_values, kind='linear')\n    #interp_2d = scipy.interpolate.RectBivariateSpline(x_in, y_in, input_values, kx=1, ky=1)\n    out_values = interp_2d.__call__(x_out, y_out)\n    return out_values", "response": "Resizes 2d array to new coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the symmetrical image of the image", "response": "def symmetry_average(image, symmetry):\n    \"\"\"\n    symmetry averaged image\n    :param image:\n    :param symmetry:\n    :return:\n    \"\"\"\n    img_sym = np.zeros_like(image)\n    angle = 360./symmetry\n    for i in range(symmetry):\n        img_sym += rotateImage(image, angle*i)\n    img_sym /= symmetry\n    return img_sym"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef findOverlap(x_mins, y_mins, min_distance):\n    n = len(x_mins)\n    idex = []\n    for i in range(n):\n        if i == 0:\n            pass\n        else:\n            for j in range(0, i):\n                if (abs(x_mins[i] - x_mins[j]) < min_distance and abs(y_mins[i] - y_mins[j]) < min_distance):\n                    idex.append(i)\n                    break\n    x_mins = np.delete(x_mins, idex, axis=0)\n    y_mins = np.delete(y_mins, idex, axis=0)\n    return x_mins, y_mins", "response": "Find overlapping solutions deletes multiples and deletes non - solutions and deletes multiples and deletes non - solutions and returns x_mins y_mins"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether image positions are within the image in units of arcsec", "response": "def coordInImage(x_coord, y_coord, numPix, deltapix):\n    \"\"\"\n    checks whether image positions are within the pixel image in units of arcsec\n    if not: remove it\n\n    :param imcoord: image coordinate (in units of angels)  [[x,y,delta,magnification][...]]\n    :type imcoord: (n,4) numpy array\n    :returns: image positions within the pixel image\n    \"\"\"\n    idex=[]\n    min = -deltapix*numPix/2\n    max = deltapix*numPix/2\n    for i in range(len(x_coord)): #sum over image positions\n        if (x_coord[i] < min or x_coord[i] > max or y_coord[i] < min or y_coord[i] > max):\n            idex.append(i)\n    x_coord = np.delete(x_coord, idex, axis=0)\n    y_coord = np.delete(y_coord, idex, axis=0)\n    return x_coord, y_coord"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadopting coordinate system and transformation between angular and pixel coordinates of a re-binned image :param bin_size: :param ra_0: :param dec_0: :param x_0: :param y_0: :param Matrix: :param Matrix_inv: :return:", "response": "def rebin_coord_transform(factor, x_at_radec_0, y_at_radec_0, Mpix2coord, Mcoord2pix):\n    \"\"\"\n    adopt coordinate system and transformation between angular and pixel coordinates of a re-binned image\n    :param bin_size:\n    :param ra_0:\n    :param dec_0:\n    :param x_0:\n    :param y_0:\n    :param Matrix:\n    :param Matrix_inv:\n    :return:\n    \"\"\"\n    factor = int(factor)\n    Mcoord2pix_resized = Mcoord2pix / factor\n    Mpix2coord_resized = Mpix2coord * factor\n    x_at_radec_0_resized = (x_at_radec_0 + 0.5) / factor - 0.5\n    y_at_radec_0_resized = (y_at_radec_0 + 0.5) / factor - 0.5\n    ra_at_xy_0_resized, dec_at_xy_0_resized = util.map_coord2pix(-x_at_radec_0_resized, -y_at_radec_0_resized, 0, 0, Mpix2coord_resized)\n    return ra_at_xy_0_resized, dec_at_xy_0_resized, x_at_radec_0_resized, y_at_radec_0_resized, Mpix2coord_resized, Mcoord2pix_resized"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stack_images(image_list, wht_list, sigma_list):\n    image_stacked = np.zeros_like(image_list[0])\n    wht_stacked = np.zeros_like(image_stacked)\n    sigma_stacked = 0.\n    for i in range(len(image_list)):\n        image_stacked += image_list[i]*wht_list[i]\n        sigma_stacked += sigma_list[i]**2 * np.median(wht_list[i])\n        wht_stacked += wht_list[i]\n    image_stacked /= wht_stacked\n    sigma_stacked /= np.median(wht_stacked)\n    wht_stacked /= len(wht_list)\n    return image_stacked, wht_stacked, np.sqrt(sigma_stacked)", "response": "stacks images and saves new image as a fits file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncuts out the edges of a 2d image and returns re-sized image to numPix center is well defined for odd pixel sizes. :param image: 2d numpy array :param numPix: square size of cut out image :return: cutout image with size numPix", "response": "def cut_edges(image, numPix):\n    \"\"\"\n    cuts out the edges of a 2d image and returns re-sized image to numPix\n    center is well defined for odd pixel sizes.\n    :param image: 2d numpy array\n    :param numPix: square size of cut out image\n    :return: cutout image with size numPix\n    \"\"\"\n    nx, ny = image.shape\n    if nx < numPix or ny < numPix:\n        print('WARNING: image can not be resized, in routine cut_edges.')\n        return image\n    if nx % 2 == 0 or ny % 2 == 0 or numPix % 2 == 0:\n        #pass\n        print(\"WARNING: image or cutout side are even number. This routine only works for odd numbers %s %s %s\"\n                         % (nx, ny, numPix))\n    cx = int((nx-1)/2)\n    cy = int((ny-1)/2)\n    d = int((numPix-1)/2)\n    if nx % 2 == 0:\n        cx += 1\n    if ny % 2 == 0:\n        cy += 1\n    resized = image[cx-d:cx+d+1, cy-d:cy+d+1]\n    return copy.deepcopy(resized)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_interpolation(self):\n        H0_range = np.linspace(10, 100, 90)\n        omega_m_range = np.linspace(0.05, 1, 95)\n        grid2d = np.dstack(np.meshgrid(H0_range, omega_m_range)).reshape(-1, 2)\n        H0_grid = grid2d[:, 0]\n        omega_m_grid = grid2d[:, 1]\n        Dd_grid = np.zeros_like(H0_grid)\n        Ds_Dds_grid = np.zeros_like(H0_grid)\n        for i in range(len(H0_grid)):\n            Dd, Ds_Dds = self.cosmo2Dd_Ds_Dds(H0_grid[i], omega_m_grid[i])\n            Dd_grid[i] = Dd\n            Ds_Dds_grid[i] = Ds_Dds\n        self._f_H0 = interpolate.interp2d(Dd_grid, Ds_Dds_grid, H0_grid, kind='linear', copy=False, bounds_error=False, fill_value=-1)\n        print(\"H0 interpolation done\")\n        self._f_omega_m = interpolate.interp2d(Dd_grid, Ds_Dds_grid, omega_m_grid, kind='linear', copy=False, bounds_error=False, fill_value=-1)\n        print(\"omega_m interpolation done\")", "response": "Creates an interpolation grid in H_0 omega_m and computes quantities in Dd and Ds_Dds\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cosmo(self, Dd, Ds_Dds):\n        if not hasattr(self, '_f_H0') or not hasattr(self, '_f_omega_m'):\n            self._make_interpolation()\n        H0 = self._f_H0(Dd, Ds_Dds)\n        print(H0, 'H0')\n        omega_m = self._f_omega_m(Dd, Ds_Dds)\n        Dd_new, Ds_Dds_new = self.cosmo2Dd_Ds_Dds(H0[0], omega_m[0])\n        if abs(Dd - Dd_new)/Dd > 0.01 or abs(Ds_Dds - Ds_Dds_new)/Ds_Dds > 0.01:\n            return [-1], [-1]\n        else:\n            return H0[0], omega_m[0]", "response": "get_cosmo - Get the values of H0 and omega_m computed with an interpolation\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mass_enclosed_3d(self, r, kwargs_profile):\n        kwargs = copy.deepcopy(kwargs_profile)\n        try:\n            del kwargs['center_x']\n            del kwargs['center_y']\n        except:\n            pass\n        # integral of self._profile.density(x)* 4*np.pi * x^2 *dx, 0,r\n        out = integrate.quad(lambda x: self._profile.density(x, **kwargs)*4*np.pi*x**2, 0, r)\n        return out[0]", "response": "computes the 3d mass enclosed within a sphere of radius r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef density_2d(self, r, kwargs_profile):\n        kwargs = copy.deepcopy(kwargs_profile)\n        try:\n            del kwargs['center_x']\n            del kwargs['center_y']\n        except:\n            pass\n        # integral of self._profile.density(np.sqrt(x^2+r^2))* dx, 0, infty\n        out = integrate.quad(lambda x: 2*self._profile.density(np.sqrt(x**2+r**2), **kwargs), 0, 100)\n        return out[0]", "response": "computes the projected density along the line - of - sight\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mass_enclosed_2d(self, r, kwargs_profile):\n        kwargs = copy.deepcopy(kwargs_profile)\n        try:\n            del kwargs['center_x']\n            del kwargs['center_y']\n        except:\n            pass\n        # integral of self.density_2d(x)* 2*np.pi * x *dx, 0, r\n        out = integrate.quad(lambda x: self.density_2d(x, kwargs)*2*np.pi*x, 0, r)\n        return out[0]", "response": "computes the mass enclosed by projected line - of - sight"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef shift_coordinate_grid(self, x_shift, y_shift, pixel_unit=False):\n        self._coords.shift_coordinate_grid(x_shift, y_shift, pixel_unit=pixel_unit)\n        self._x_grid, self._y_grid = self._coords.coordinate_grid(self.nx)", "response": "shifts the coordinate grid of the data class with the given shift in x and y"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef C_D(self):\n        if not hasattr(self, '_C_D'):\n            if self._noise_map is not None:\n                self._C_D = self._noise_map**2\n            else:\n                self._C_D = self.covariance_matrix(self.data, self.background_rms, self.exposure_map)\n        return self._C_D", "response": "Returns the covariance matrix of all pixel values in 2d numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef covariance_matrix(self, data, background_rms=1, exposure_map=1, noise_map=None, verbose=False):\n        if noise_map is not None:\n            return noise_map**2\n        if isinstance(exposure_map, int) or isinstance(exposure_map, float):\n            if exposure_map <= 0:\n                exposure_map = 1\n        else:\n            mean_exp_time = np.mean(exposure_map)\n            exposure_map[exposure_map < mean_exp_time / 10] = mean_exp_time / 10\n        if verbose:\n            if background_rms * np.max(exposure_map) < 1:\n                print(\"WARNING! sigma_b*f %s < 1 count may introduce unstable error estimates\" % (background_rms * np.max(exposure_map)))\n        d_pos = np.zeros_like(data)\n        #threshold = 1.5*sigma_b\n        d_pos[data >= 0] = data[data >= 0]\n        #d_pos[d < threshold] = 0\n        sigma = d_pos / exposure_map + background_rms ** 2\n        return sigma", "response": "returns the diagonal matrix that describes the error of the background and Poisson components of the noise estimate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef image_position_stochastic(self, source_x, source_y, kwargs_lens, search_window=10,\n                                  precision_limit=10**(-10), arrival_time_sort=True, x_center=0,\n                                  y_center=0, num_random=1000, verbose=False):\n        \"\"\"\n        Solves the lens equation stochastically with the scipy minimization routine on the quadratic distance between\n        the backwards ray-shooted proposed image position and the source position.\n        Credits to Giulia Pagano\n\n        :param source_x: source position\n        :param source_y: source position\n        :param kwargs_lens: lens model list of keyword arguments\n        :param search_window: angular size of search window\n        :param precision_limit: limit required on the precision in the source plane\n        :param arrival_time_sort: bool, if True sorts according to arrival time\n        :param x_center: center of search window\n        :param y_center: center of search window\n        :param num_random: number of random starting points of the non-linear solver in the search window\n        :param verbose: bool, if True, prints performance information\n        :return: x_image, y_image\n        \"\"\"\n        x_solve, y_solve = [], []\n        for i in range(num_random):\n            x_init = np.random.uniform(-search_window / 2., search_window / 2) + x_center\n            y_init = np.random.uniform(-search_window / 2., search_window / 2) + y_center\n            xinitial = np.array([x_init, y_init])\n            result = minimize(self._root, xinitial, args=(kwargs_lens, source_x, source_y), tol=precision_limit ** 2, method='Nelder-Mead')\n            if self._root(result.x, kwargs_lens, source_x, source_y) < precision_limit**2:\n                x_solve.append(result.x[0])\n                y_solve.append(result.x[1])\n\n        x_mins, y_mins = image_util.findOverlap(x_solve, y_solve, precision_limit)\n        if arrival_time_sort is True:\n            x_mins, y_mins = self.sort_arrival_times(x_mins, y_mins, kwargs_lens)\n        return x_mins, y_mins", "response": "Solves the lens equation stochastically with the scipy minimization routine on the quadratic distance between the source position and the proposed image position."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef image_position_from_source(self, sourcePos_x, sourcePos_y, kwargs_lens, min_distance=0.1, search_window=10,\n                                   precision_limit=10**(-10), num_iter_max=100, arrival_time_sort=True,\n                                   initial_guess_cut=True, verbose=False, x_center=0, y_center=0, num_random=0):\n        \"\"\"\n        finds image position source position and lense model\n\n        :param sourcePos_x: source position in units of angle\n        :param sourcePos_y: source position in units of angle\n        :param kwargs_lens: lens model parameters as keyword arguments\n        :param min_distance: minimum separation to consider for two images in units of angle\n        :param search_window: window size to be considered by the solver. Will not find image position outside this window\n        :param precision_limit: required precision in the lens equation solver (in units of angle in the source plane).\n        :param num_iter_max: maximum iteration of lens-source mapping conducted by solver to match the required precision\n        :param arrival_time_sort: bool, if True, sorts image position in arrival time (first arrival photon first listed)\n        :param initial_guess_cut: bool, if True, cuts initial local minima selected by the grid search based on distance criteria from the source position\n        :param verbose: bool, if True, prints some useful information for the user\n        :param x_center: float, center of the window to search for point sources\n        :param y_center: float, center of the window to search for point sources\n        :returns: (exact) angular position of (multiple) images ra_pos, dec_pos in units of angle\n        :raises: AttributeError, KeyError\n        \"\"\"\n\n        # compute number of pixels to cover the search window with the required min_distance\n        numPix = int(round(search_window / min_distance) + 0.5)\n        x_grid, y_grid = util.make_grid(numPix, min_distance)\n        x_grid += x_center\n        y_grid += y_center\n        # ray-shoot to find the relative distance to the required source position for each grid point\n        x_mapped, y_mapped = self.lensModel.ray_shooting(x_grid, y_grid, kwargs_lens)\n        absmapped = util.displaceAbs(x_mapped, y_mapped, sourcePos_x, sourcePos_y)\n        # select minima in the grid points and select grid points that do not deviate more than the\n        # width of the grid point to a solution of the lens equation\n        x_mins, y_mins, delta_map = util.neighborSelect(absmapped, x_grid, y_grid)\n        if verbose is True:\n            print(\"There are %s regions identified that could contain a solution of the lens equation\" % len(x_mins))\n        #mag = np.abs(mag)\n        #print(x_mins, y_mins, 'before requirement of min_distance')\n        if initial_guess_cut is True:\n            mag = np.abs(self.lensModel.magnification(x_mins, y_mins, kwargs_lens))\n            mag[mag < 1] = 1\n            x_mins = x_mins[delta_map <= min_distance*mag*5]\n            y_mins = y_mins[delta_map <= min_distance*mag*5]\n            if verbose is True:\n                print(\"The number of regions that meet the plausibility criteria are %s\" % len(x_mins))\n        x_mins = np.append(x_mins, np.random.uniform(low=-search_window/2+x_center, high=search_window/2+x_center,\n                                                     size=num_random))\n        y_mins = np.append(y_mins, np.random.uniform(low=-search_window / 2 + y_center, high=search_window / 2 + y_center,\n                                             size=num_random))\n        # iterative solving of the lens equation for the selected grid points\n        x_mins, y_mins, solver_precision = self._findIterative(x_mins, y_mins, sourcePos_x, sourcePos_y, kwargs_lens,\n                                                               precision_limit, num_iter_max, verbose=verbose,\n                                                               min_distance=min_distance)\n        # only select iterative results that match the precision limit\n        x_mins = x_mins[solver_precision <= precision_limit]\n        y_mins = y_mins[solver_precision <= precision_limit]\n        # find redundant solutions within the min_distance criterion\n        x_mins, y_mins = image_util.findOverlap(x_mins, y_mins, min_distance)\n        if arrival_time_sort is True:\n            x_mins, y_mins = self.sort_arrival_times(x_mins, y_mins, kwargs_lens)\n        #x_mins, y_mins = lenstronomy_util.coordInImage(x_mins, y_mins, numPix, deltapix)\n        return x_mins, y_mins", "response": "finds image position from source position and lense model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsorting arrival times of images in increasing order of light travel time", "response": "def sort_arrival_times(self, x_mins, y_mins, kwargs_lens):\n        \"\"\"\n        sort arrival times (fermat potential) of image positions in increasing order of light travel time\n        :param x_mins: ra position of images\n        :param y_mins: dec position of images\n        :param kwargs_lens: keyword arguments of lens model\n        :return: sorted lists of x_mins and y_mins\n        \"\"\"\n\n        if hasattr(self.lensModel, '_no_potential'):\n            raise Exception('Instance of lensModel passed to this class does not compute the lensing potential, '\n                            'and therefore cannot compute time delays.')\n\n        if len(x_mins) <= 1:\n            return x_mins, y_mins\n        x_source, y_source = self.lensModel.ray_shooting(x_mins, y_mins, kwargs_lens)\n        x_source = np.mean(x_source)\n        y_source = np.mean(y_source)\n        if self.lensModel.multi_plane is True:\n            arrival_time = self.lensModel.arrival_time(x_mins, y_mins, kwargs_lens)\n        else:\n            fermat_pot = self.lensModel.fermat_potential(x_mins, y_mins, x_source, y_source, kwargs_lens)\n            arrival_time = fermat_pot\n        idx = np.argsort(arrival_time)\n        x_mins = np.array(x_mins)[idx]\n        y_mins = np.array(y_mins)[idx]\n        return x_mins, y_mins"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_positive_flux(cls, kwargs_ps):\n        pos_bool = True\n        for kwargs in kwargs_ps:\n            point_amp = kwargs['point_amp']\n            for amp in point_amp:\n                if amp < 0:\n                    pos_bool = False\n                    break\n        return pos_bool", "response": "check whether inferred linear parameters are positive\nCOOKIEID"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the log likelihood of the best fit model of the current state of this class", "response": "def best_fit_likelihood(self):\n        \"\"\"\n        returns the log likelihood of the best fit model of the current state of this class\n\n        :return: log likelihood, float\n        \"\"\"\n        kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, kwargs_cosmo = self.best_fit(bijective=False)\n        param_class = self._param_class\n        likelihoodModule = self.likelihoodModule\n        logL, _ = likelihoodModule.logL(param_class.kwargs2args(kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps,\n                                                             kwargs_cosmo))\n        return logL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mcmc(self, n_burn, n_run, walkerRatio, sigma_scale=1, threadCount=1, init_samples=None, re_use_samples=True):\n\n        param_class = self._param_class\n        # run PSO\n        mcmc_class = Sampler(likelihoodModule=self.likelihoodModule)\n        mean_start = param_class.kwargs2args(self._lens_temp, self._source_temp, self._lens_light_temp, self._ps_temp,\n                                           self._cosmo_temp)\n        lens_sigma, source_sigma, lens_light_sigma, ps_sigma, cosmo_sigma = self._updateManager.sigma_kwargs\n        sigma_start = param_class.kwargs2args(lens_sigma, source_sigma, lens_light_sigma, ps_sigma, cosmo_sigma)\n        num_param, param_list = param_class.num_param()\n        # run MCMC\n        if not init_samples is None and re_use_samples is True:\n            print(\"test that you are here!\")\n            num_samples, num_param_prev = np.shape(init_samples)\n            print(num_samples, num_param_prev, num_param, 'shape of init_sample')\n            if num_param_prev == num_param:\n                print(\"re-using previous samples to initialize the next MCMC run.\")\n                initpos = ReusePositionGenerator(init_samples)\n            else:\n                print(\"Can not re-use previous MCMC samples due to change in option\")\n                initpos = None\n        else:\n            initpos = None\n        samples, dist = mcmc_class.mcmc_CH(walkerRatio, n_run, n_burn, mean_start, np.array(sigma_start) * sigma_scale,\n                                           threadCount=threadCount,\n                                           mpi=self._mpi, init_pos=initpos)\n        return samples, param_list, dist", "response": "This routine runs the MCMC routine."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pso(self, n_particles, n_iterations, sigma_scale=1, print_key='PSO', threadCount=1):\n\n        param_class = self._param_class\n        init_pos = param_class.kwargs2args(self._lens_temp, self._source_temp, self._lens_light_temp, self._ps_temp,\n                                           self._cosmo_temp)\n        lens_sigma, source_sigma, lens_light_sigma, ps_sigma, cosmo_sigma = self._updateManager.sigma_kwargs\n        sigma_start = param_class.kwargs2args(lens_sigma, source_sigma, lens_light_sigma, ps_sigma, cosmo_sigma)\n        lowerLimit = np.array(init_pos) - np.array(sigma_start) * sigma_scale\n        upperLimit = np.array(init_pos) + np.array(sigma_start) * sigma_scale\n        num_param, param_list = param_class.num_param()\n\n        # run PSO\n        sampler = Sampler(likelihoodModule=self.likelihoodModule)\n        result, chain = sampler.pso(n_particles, n_iterations, lowerLimit, upperLimit, init_pos=init_pos,\n                                       threadCount=threadCount, mpi=self._mpi, print_key=print_key)\n        lens_result, source_result, lens_light_result, ps_result, cosmo_result = param_class.args2kwargs(result,\n                                                                                                         bijective=True)\n        return lens_result, source_result, lens_light_result, ps_result, cosmo_result, chain, param_list", "response": "This routine runs the PSO algorithm and returns the best fit parameter for each particle system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef psf_iteration(self, num_iter=10, no_break=True, stacking_method='median', block_center_neighbour=0, keep_psf_error_map=True,\n                 psf_symmetry=1, psf_iter_factor=1, verbose=True, compute_bands=None):\n        \"\"\"\n        iterative PSF reconstruction\n\n        :param num_iter: number of iterations in the process\n        :param no_break: bool, if False will break the process as soon as one step lead to a wors reconstruction then the previous step\n        :param stacking_method: string, 'median' and 'mean' supported\n        :param block_center_neighbour: radius of neighbouring point source to be blocked in the reconstruction\n        :param keep_psf_error_map: bool, whether or not to keep the previous psf_error_map\n        :param psf_symmetry: int, number of invariant rotations in the reconstructed PSF\n        :param psf_iter_factor: factor of new estimated PSF relative to the old one PSF_updated = (1-psf_iter_factor) * PSF_old + psf_iter_factor*PSF_new\n        :param verbose: bool, print statements\n        :param compute_bands: bool list, if multiple bands, this process can be limited to a subset of bands\n        :return: 0, updated PSF is stored in self.mult_iband_list\n        \"\"\"\n        #lens_temp = copy.deepcopy(lens_input)\n        kwargs_model = self._updateManager.kwargs_model\n        param_class = self._param_class\n        lens_updated = param_class.update_lens_scaling(self._cosmo_temp, self._lens_temp)\n        source_updated = param_class.image2source_plane(self._source_temp, lens_updated)\n        if compute_bands is None:\n            compute_bands = [True] * len(self.multi_band_list)\n\n        for i in range(len(self.multi_band_list)):\n            if compute_bands[i] is True:\n                kwargs_data = self.multi_band_list[i][0]\n                kwargs_psf = self.multi_band_list[i][1]\n                kwargs_numerics = self.multi_band_list[i][2]\n                image_model = class_creator.create_image_model(kwargs_data=kwargs_data,\n                                                               kwargs_psf=kwargs_psf,\n                                                               kwargs_numerics=kwargs_numerics,\n                                                               kwargs_model=kwargs_model)\n                psf_iter = PsfFitting(image_model_class=image_model)\n                kwargs_psf = psf_iter.update_iterative(kwargs_psf, lens_updated, source_updated,\n                                                       self._lens_light_temp, self._ps_temp, num_iter=num_iter,\n                                                       no_break=no_break, stacking_method=stacking_method,\n                                                       block_center_neighbour=block_center_neighbour,\n                                                       keep_psf_error_map=keep_psf_error_map,\n                 psf_symmetry=psf_symmetry, psf_iter_factor=psf_iter_factor, verbose=verbose)\n                self.multi_band_list[i][1] = kwargs_psf\n        return 0", "response": "This function is used to update the PSF in a single iteration of the process."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\naligns the coordinate systems of different exposures within a fixed model parameterisation by executing a PSOAlignmentFitting function on the specified set of parameters.", "response": "def align_images(self, n_particles=10, n_iterations=10, lowerLimit=-0.2, upperLimit=0.2, threadCount=1,\n                     compute_bands=None):\n        \"\"\"\n        aligns the coordinate systems of different exposures within a fixed model parameterisation by executing a PSO\n        with relative coordinate shifts as free parameters\n\n        :param n_particles: number of particles in the Particle Swarm Optimization\n        :param n_iterations: number of iterations in the optimization process\n        :param lowerLimit: lower limit of relative shift\n        :param upperLimit: upper limit of relative shift\n        :param verbose: bool, print statements\n        :param compute_bands: bool list, if multiple bands, this process can be limited to a subset of bands\n        :return:\n        \"\"\"\n        kwargs_model = self._updateManager.kwargs_model\n        param_class = self._updateManager.param_class(self._lens_temp)\n        lens_updated = param_class.update_lens_scaling(self._cosmo_temp, self._lens_temp)\n        source_updated = param_class.image2source_plane(self._source_temp, lens_updated)\n        if compute_bands is None:\n            compute_bands = [True] * len(self.multi_band_list)\n\n        for i in range(len(self.multi_band_list)):\n            if compute_bands[i] is True:\n                kwargs_data = self.multi_band_list[i][0]\n                kwargs_psf = self.multi_band_list[i][1]\n                kwargs_numerics = self.multi_band_list[i][2]\n                alignmentFitting = AlignmentFitting(kwargs_data, kwargs_psf, kwargs_numerics, kwargs_model, lens_updated, source_updated,\n                                                        self._lens_light_temp, self._ps_temp)\n\n                kwargs_data, chain = alignmentFitting.pso(n_particles=n_particles, n_iterations=n_iterations,\n                                                          lowerLimit=lowerLimit, upperLimit=upperLimit,\n                                                          threadCount=threadCount, mpi=self._mpi,\n                                                          print_key='Alignment fitting for band %s ...' % i)\n                print('Align completed for band %s.' % i)\n                print('ra_shift: %s,  dec_shift: %s' %(kwargs_data['ra_shift'], kwargs_data['dec_shift']))\n                self.multi_band_list[i][0] = kwargs_data\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate lenstronomy settings on the fly", "response": "def update_settings(self, kwargs_model={}, kwargs_constraints={}, kwargs_likelihood={}, lens_add_fixed=[],\n                     source_add_fixed=[], lens_light_add_fixed=[], ps_add_fixed=[], cosmo_add_fixed=[], lens_remove_fixed=[],\n                     source_remove_fixed=[], lens_light_remove_fixed=[], ps_remove_fixed=[], cosmo_remove_fixed=[],\n                        change_source_lower_limit=None, change_source_upper_limit=None):\n        \"\"\"\n        updates lenstronomy settings \"on the fly\"\n\n        :param kwargs_model: kwargs, specified keyword arguments overwrite the existing ones\n        :param kwargs_constraints: kwargs, specified keyword arguments overwrite the existing ones\n        :param kwargs_likelihood: kwargs, specified keyword arguments overwrite the existing ones\n        :param lens_add_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param source_add_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param lens_light_add_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param ps_add_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param cosmo_add_fixed: ['param1', 'param2',...]\n        :param lens_remove_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param source_remove_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param lens_light_remove_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param ps_remove_fixed: [[i_model, ['param1', 'param2',...], [...]]\n        :param cosmo_remove_fixed: ['param1', 'param2',...]\n        :return: 0, the settings are overwritten for the next fitting step to come\n        \"\"\"\n        self._updateManager.update_options(kwargs_model, kwargs_constraints, kwargs_likelihood)\n        self._updateManager.update_fixed(self._lens_temp, self._source_temp, self._lens_light_temp, self._ps_temp,\n                                         self._cosmo_temp, lens_add_fixed, source_add_fixed, lens_light_add_fixed,\n                                         ps_add_fixed, cosmo_add_fixed, lens_remove_fixed, source_remove_fixed,\n                                         lens_light_remove_fixed, ps_remove_fixed, cosmo_remove_fixed)\n        self._updateManager.update_limits(change_source_lower_limit, change_source_upper_limit)\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the parameter values given A C_D_inv d", "response": "def get_param_WLS(A, C_D_inv, d, inv_bool=True):\n    \"\"\"\n    returns the parameter values given\n    :param A: response matrix Nd x Ns (Nd = # data points, Ns = # parameters)\n    :param C_D_inv: inverse covariance matrix of the data, Nd x Nd, diagonal form\n    :param d: data array, 1-d Nd\n    :param inv_bool: boolean, wheter returning also the inverse matrix or just solve the linear system\n    :return: 1-d array of parameter values\n    \"\"\"\n    M = A.T.dot(np.multiply(C_D_inv, A.T).T)\n    if inv_bool:\n        if np.linalg.cond(M) < 5/sys.float_info.epsilon:\n            try:\n                M_inv = np.linalg.inv(M)\n            except:\n                M_inv = np.zeros_like(M)\n        else:\n            M_inv = np.zeros_like(M)\n        R = A.T.dot(np.multiply(C_D_inv, d))\n        B = M_inv.dot(R)\n    else:\n        if np.linalg.cond(M) < 5/sys.float_info.epsilon:\n            R = A.T.dot(np.multiply(C_D_inv, d))\n            try:\n                B = np.linalg.solve(M, R).T\n            except:\n                B = np.zeros(len(A.T))\n        else:\n            B = np.zeros(len(A.T))\n        M_inv = None\n    image = A.dot(B)\n    return B, M_inv, image"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the anisotorpy parameter at a given radius", "response": "def beta_r(self, r, kwargs):\n        \"\"\"\n        returns the anisotorpy parameter at a given radius\n        :param r:\n        :return:\n        \"\"\"\n        if self._type == 'const':\n            return self.const_beta(kwargs)\n        elif self._type == 'OsipkovMerritt':\n            return self.ospikov_meritt(r, kwargs)\n        elif self._type == 'Colin':\n            return self.colin(r, kwargs)\n        elif self._type == 'isotropic':\n            return self.isotropic()\n        elif self._type == 'radial':\n            return self.radial()\n        else:\n            raise ValueError('anisotropy type %s not supported!' % self._type)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _B(self, x, a, b):\n        return special.betainc(a, b, x) * special.beta(a, b)", "response": "incomplete Beta function as described in Mamon & Lokas A13."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the anisotorpy parameter at a given radius", "response": "def beta_r(self, r, kwargs):\n        \"\"\"\n        returns the anisotorpy parameter at a given radius\n        :param r:\n        :return:\n        \"\"\"\n        if self._type == 'const':\n            return self.const_beta(kwargs)\n        elif self._type == 'r_ani':\n            return self.beta_r_ani(r, kwargs)\n        else:\n            raise ValueError('anisotropy type %s not supported!' % self._type)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a Data instance of lenstronomy based on knowledge of the observation", "response": "def data_class(self):\n        \"\"\"\n        creates a Data() instance of lenstronomy based on knowledge of the observation\n\n        :return: instance of Data() class\n        \"\"\"\n        x_grid, y_grid, ra_at_xy_0, dec_at_xy_0, x_at_radec_0, y_at_radec_0, Mpix2coord, Mcoord2pix = util.make_grid_with_coordtransform(\n            numPix=self.numpix, deltapix=self.pixel_scale, subgrid_res=1, left_lower=False, inverse=False)\n        kwargs_data = {'numPix': self.numpix, 'ra_at_xy_0': ra_at_xy_0, 'dec_at_xy_0': dec_at_xy_0,\n                       'transform_pix2angle': Mpix2coord,\n                       'background_rms': self.background_noise,\n                       'exp_time': self.scaled_exposure_time}\n        data_class = Data(kwargs_data)\n        return data_class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef psf_class(self):\n        if self._psf_type == 'GAUSSIAN':\n            psf_type = \"GAUSSIAN\"\n            fwhm = self._seeing\n            kwargs_psf = {'psf_type': psf_type, 'fwhm': fwhm}\n        elif self._psf_type == 'PIXEL':\n            if self._psf_model is not None:\n                kwargs_psf = {'psf_type': \"PIXEL\", 'kernel_point_source': self._psf_model}\n            else:\n                raise ValueError(\"You need to create the class instance with a psf_model!\")\n        else:\n            raise ValueError(\"psf_type %s not supported!\" % self._psf_type)\n        psf_class = PSF(kwargs_psf)\n        return psf_class", "response": "Creates an instance of the PSF class based on knowledge of the observations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_options(self, kwargs_model, kwargs_constraints, kwargs_likelihood):\n        kwargs_model_updated = self.kwargs_model.update(kwargs_model)\n        kwargs_constraints_updated = self.kwargs_constraints.update(kwargs_constraints)\n        kwargs_likelihood_updated = self.kwargs_likelihood.update(kwargs_likelihood)\n        return kwargs_model_updated, kwargs_constraints_updated, kwargs_likelihood_updated", "response": "Updates the kwargs_model kwargs_constraints and kwargs_likelihood by updating the kwargs_model and kwargs_constraints and kwargs_likelihood."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_limits(self, change_source_lower_limit=None, change_source_upper_limit=None):\n        if not change_source_lower_limit is None:\n            self._source_lower = self._update_limit(change_source_lower_limit, self._source_lower)\n        if not change_source_upper_limit is None:\n            self._source_upper = self._update_limit(change_source_upper_limit, self._source_upper)", "response": "Updates the limits of the internal state of the source instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating the kwargs fixed with the values of the keyword arguments that are stated in _add_fixed to the existing kwargs fixed.", "response": "def update_fixed(self, kwargs_lens, kwargs_source, kwargs_lens_light, kwargs_ps, kwargs_cosmo, lens_add_fixed=[],\n                     source_add_fixed=[], lens_light_add_fixed=[], ps_add_fixed=[], cosmo_add_fixed=[], lens_remove_fixed=[],\n                     source_remove_fixed=[], lens_light_remove_fixed=[], ps_remove_fixed=[], cosmo_remove_fixed=[]):\n        \"\"\"\n        adds the values of the keyword arguments that are stated in the _add_fixed to the existing fixed arguments.\n\n        :param kwargs_lens:\n        :param kwargs_source:\n        :param kwargs_lens_light:\n        :param kwargs_ps:\n        :param kwargs_cosmo:\n        :param lens_add_fixed:\n        :param source_add_fixed:\n        :param lens_light_add_fixed:\n        :param ps_add_fixed:\n        :param cosmo_add_fixed:\n        :return: updated kwargs fixed\n        \"\"\"\n        lens_fixed = self._add_fixed(kwargs_lens, self._lens_fixed, lens_add_fixed)\n        lens_fixed = self._remove_fixed(lens_fixed, lens_remove_fixed)\n        source_fixed = self._add_fixed(kwargs_source, self._source_fixed, source_add_fixed)\n        source_fixed = self._remove_fixed(source_fixed, source_remove_fixed)\n        lens_light_fixed = self._add_fixed(kwargs_lens_light, self._lens_light_fixed, lens_light_add_fixed)\n        lens_light_fixed = self._remove_fixed(lens_light_fixed, lens_light_remove_fixed)\n        ps_fixed = self._add_fixed(kwargs_ps, self._ps_fixed, ps_add_fixed)\n        ps_fixed = self._remove_fixed(ps_fixed, ps_remove_fixed)\n        cosmo_fixed = copy.deepcopy(self._cosmo_fixed)\n        for param_name in cosmo_add_fixed:\n            if param_name in cosmo_fixed:\n                pass\n            else:\n                cosmo_fixed[param_name] = kwargs_cosmo[param_name]\n        for param_name in cosmo_remove_fixed:\n            if param_name in cosmo_fixed:\n                del cosmo_fixed[param_name]\n                self._lens_fixed, self._source_fixed, self._lens_light_fixed, self._ps_fixed, self._cosmo_fixed = lens_fixed, source_fixed, lens_light_fixed, ps_fixed, cosmo_fixed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the average LOS integrated velocity dispersion in the slit", "response": "def vel_disp(self, gamma, theta_E, r_eff, r_ani, R_slit, dR_slit, FWHM, rendering_number=1000):\n        \"\"\"\n        computes the averaged LOS velocity dispersion in the slit (convolved)\n\n        :param gamma: power-law slope of the mass profile (isothermal = 2)\n        :param theta_E: Einstein radius of the lens (in arcseconds)\n        :param r_eff: half light radius of the Hernquist profile (or as an approximation of any other profile to be described as a Hernquist profile\n        :param r_ani: anisotropy radius\n        :param R_slit: length of the slit/box\n        :param dR_slit: width of the slit/box\n        :param FWHM: full width at half maximum of the seeing conditions, described as a Gaussian\n        :param rendering_number: number of spectral renderings drawn from the light distribution that go through the\n            slit of the observations\n\n        :return: LOS integrated velocity dispersion in units [km/s]\n        \"\"\"\n        sigma_s2_sum = 0\n        rho0_r0_gamma = self._rho0_r0_gamma(theta_E, gamma)\n        for i in range(0, rendering_number):\n            sigma_s2_draw = self.vel_disp_one(gamma, rho0_r0_gamma, r_eff, r_ani, R_slit, dR_slit, FWHM)\n            sigma_s2_sum += sigma_s2_draw\n        sigma_s2_average = sigma_s2_sum / rendering_number\n        return np.sqrt(sigma_s2_average)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vel_disp_one(self, gamma, rho0_r0_gamma, r_eff, r_ani, R_slit, dR_slit, FWHM):\n        a = 0.551 * r_eff\n        while True:\n            r = self.P_r(a)  # draw r\n            R, x, y = self.R_r(r)  # draw projected R\n            x_, y_ = self.displace_PSF(x, y, FWHM)  # displace via PSF\n            bool = self.check_in_slit(x_, y_, R_slit, dR_slit)\n            if bool is True:\n                break\n        sigma_s2 = self.sigma_s2(r, R, r_ani, a, gamma, rho0_r0_gamma)\n        return sigma_s2", "response": "Compute one realisation of the velocity dispersion realized in the potential"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndrawing a random projection from radius r in 2d and 1d radius draws a random projection from radius r in 2d and 1d radius", "response": "def R_r(self, r):\n        \"\"\"\n        draws a random projection from radius r in 2d and 1d\n        :param r: 3d radius\n        :return: R, x, y\n        \"\"\"\n        phi = np.random.uniform(0, 2*np.pi)\n        theta = np.random.uniform(0, np.pi)\n        x = r * np.sin(theta) * np.cos(phi)\n        y = r * np.sin(theta) * np.sin(phi)\n        R = np.sqrt(x**2 + y**2)\n        return R, x, y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sigma_s2(self, r, R, r_ani, a, gamma, rho0_r0_gamma):\n        beta = self._beta_ani(r, r_ani)\n        return (1 - beta * R**2/r**2) * self.sigma_r2(r, a, gamma, rho0_r0_gamma, r_ani)", "response": "calculate sigma_s2 for a given projected velocity dispersion"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sigma_r2(self, r, a, gamma, rho0_r0_gamma, r_ani):\n        # first term\n        prefac1 = 4*np.pi * const.G * a**(-gamma) * rho0_r0_gamma / (3-gamma)\n        prefac2 = r * (r + a)**3/(r**2 + r_ani**2)\n        hyp1 = vel_util.hyp_2F1(a=2+gamma, b=gamma, c=3+gamma, z=1./(1+r/a))\n        hyp2 = vel_util.hyp_2F1(a=3, b=gamma, c=1+gamma, z=-a/r)\n        fac = r_ani**2/a**2 * hyp1 / ((2+gamma) * (r/a + 1)**(2+gamma)) + hyp2 / (gamma*(r/a)**gamma)\n        return prefac1 * prefac2 * fac * (self._cosmo.arcsec2phys_lens(1.) * const.Mpc / 1000) ** 2", "response": "calculate sigma of an object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking sure that the window covers all the light, otherwise the moments may give to low answers. :param kwargs_lens_light: :param center_x: :param center_y: :param model_bool_list: :param deltaPix: :param numPix: :return:", "response": "def ellipticity_lens_light(self, kwargs_lens_light, center_x=0, center_y=0, model_bool_list=None, deltaPix=None,\n                               numPix=None):\n        \"\"\"\n        make sure that the window covers all the light, otherwise the moments may give to low answers.\n\n        :param kwargs_lens_light:\n        :param center_x:\n        :param center_y:\n        :param model_bool_list:\n        :param deltaPix:\n        :param numPix:\n        :return:\n        \"\"\"\n        if model_bool_list is None:\n            model_bool_list = [True] * len(kwargs_lens_light)\n        if numPix is None:\n            numPix = 100\n        if deltaPix is None:\n            deltaPix = 0.05\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        I_xy = self._lens_light_internal(x_grid, y_grid, kwargs_lens_light, model_bool_list=model_bool_list)\n        e1, e2 = analysis_util.ellipticities(I_xy, x_grid, y_grid)\n        return e1, e2"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef half_light_radius_lens(self, kwargs_lens_light, center_x=0, center_y=0, model_bool_list=None, deltaPix=None, numPix=None):\n        if model_bool_list is None:\n            model_bool_list = [True] * len(kwargs_lens_light)\n        if numPix is None:\n            numPix = 1000\n        if deltaPix is None:\n            deltaPix = 0.05\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        lens_light = self._lens_light_internal(x_grid, y_grid, kwargs_lens_light, model_bool_list=model_bool_list)\n        R_h = analysis_util.half_light_radius(lens_light, x_grid, y_grid, center_x, center_y)\n        return R_h", "response": "Calculates the half - light - radius of the deflector light and the total photon flux."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef half_light_radius_source(self, kwargs_source, center_x=0, center_y=0, deltaPix=None, numPix=None):\n        if numPix is None:\n            numPix = 1000\n        if deltaPix is None:\n            deltaPix = 0.005\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        source_light = self.SourceModel.surface_brightness(x_grid, y_grid, kwargs_source)\n        R_h = analysis_util.half_light_radius(source_light, x_grid, y_grid, center_x=center_x, center_y=center_y)\n        return R_h", "response": "Calculates the half - light - radius of the deflector light and the total photon flux."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _lens_light_internal(self, x_grid, y_grid, kwargs_lens_light, model_bool_list=None):\n        if model_bool_list is None:\n            model_bool_list = [True] * len(kwargs_lens_light)\n        lens_light = np.zeros_like(x_grid)\n        for i, bool in enumerate(model_bool_list):\n            if bool is True:\n                lens_light_i = self.LensLightModel.surface_brightness(x_grid, y_grid, kwargs_lens_light, k=i)\n                lens_light += lens_light_i\n        return lens_light", "response": "Compute the lens light for each entry in the list of model_bool_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multi_gaussian_lens_light(self, kwargs_lens_light, model_bool_list=None, e1=0, e2=0, n_comp=20, deltaPix=None, numPix=None):\n        if 'center_x' in kwargs_lens_light[0]:\n            center_x = kwargs_lens_light[0]['center_x']\n            center_y = kwargs_lens_light[0]['center_y']\n        else:\n            center_x, center_y = 0, 0\n        r_h = self.half_light_radius_lens(kwargs_lens_light, center_x=center_x, center_y=center_y,\n                                          model_bool_list=model_bool_list, deltaPix=deltaPix, numPix=numPix)\n        r_array = np.logspace(-3, 2, 200) * r_h * 2\n        x_coords, y_coords = param_util.transform_e1e2(r_array, np.zeros_like(r_array), e1=-e1, e2=-e2)\n        x_coords += center_x\n        y_coords += center_y\n        #r_array = np.logspace(-2, 1, 50) * r_h\n        flux_r = self._lens_light_internal(x_coords, y_coords, kwargs_lens_light,\n                                           model_bool_list=model_bool_list)\n        amplitudes, sigmas, norm = mge.mge_1d(r_array, flux_r, N=n_comp)\n        return amplitudes, sigmas, center_x, center_y", "response": "This function computes the multi - Gaussian decomposition of the lens light profile."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef multi_gaussian_lens(self, kwargs_lens, model_bool_list=None, e1=0, e2=0, n_comp=20):\n        if 'center_x' in kwargs_lens[0]:\n            center_x = kwargs_lens[0]['center_x']\n            center_y = kwargs_lens[0]['center_y']\n        else:\n            raise ValueError('no keyword center_x defined!')\n        theta_E = self._lensModelExtensions.effective_einstein_radius(kwargs_lens)\n        r_array = np.logspace(-4, 2, 200) * theta_E\n        x_coords, y_coords = param_util.transform_e1e2(r_array, np.zeros_like(r_array), e1=-e1, e2=-e2)\n        x_coords += center_x\n        y_coords += center_y\n        #r_array = np.logspace(-2, 1, 50) * theta_E\n        if model_bool_list is None:\n            model_bool_list = [True] * len(kwargs_lens)\n        kappa_s = np.zeros_like(r_array)\n        for i in range(len(kwargs_lens)):\n            if model_bool_list[i] is True:\n                kappa_s += self.LensModel.kappa(x_coords, y_coords, kwargs_lens, k=i)\n        amplitudes, sigmas, norm = mge.mge_1d(r_array, kappa_s, N=n_comp)\n        return amplitudes, sigmas, center_x, center_y", "response": "This function computes the amplitudes and sigmas of a multi - Gaussian lens model in convergence space."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flux_components(self, kwargs_light, n_grid=400, delta_grid=0.01, deltaPix=0.05, type=\"lens\"):\n        flux_list = []\n        R_h_list = []\n        x_grid, y_grid = util.make_grid(numPix=n_grid, deltapix=delta_grid)\n        kwargs_copy = copy.deepcopy(kwargs_light)\n        for k, kwargs in enumerate(kwargs_light):\n            if 'center_x' in kwargs_copy[k]:\n                kwargs_copy[k]['center_x'] = 0\n                kwargs_copy[k]['center_y'] = 0\n            if type == 'lens':\n                light = self.LensLightModel.surface_brightness(x_grid, y_grid, kwargs_copy, k=k)\n            elif type == 'source':\n                light = self.SourceModel.surface_brightness(x_grid, y_grid, kwargs_copy, k=k)\n            else:\n                raise ValueError(\"type %s not supported!\" % type)\n            flux = np.sum(light)*delta_grid**2 / deltaPix**2\n            R_h = analysis_util.half_light_radius(light, x_grid, y_grid)\n            flux_list.append(flux)\n            R_h_list.append(R_h)\n        return flux_list, R_h_list", "response": "computes the total flux in each component of the model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the error map for a source model", "response": "def error_map_source(self, kwargs_source, x_grid, y_grid, cov_param):\n        \"\"\"\n        variance of the linear source reconstruction in the source plane coordinates,\n        computed by the diagonal elements of the covariance matrix of the source reconstruction as a sum of the errors\n        of the basis set.\n\n        :param kwargs_source: keyword arguments of source model\n        :param x_grid: x-axis of positions to compute error map\n        :param y_grid: y-axis of positions to compute error map\n        :param cov_param: covariance matrix of liner inversion parameters\n        :return: diagonal covariance errors at the positions (x_grid, y_grid)\n        \"\"\"\n\n        error_map = np.zeros_like(x_grid)\n        basis_functions, n_source = self.SourceModel.functions_split(x_grid, y_grid, kwargs_source)\n        basis_functions = np.array(basis_functions)\n\n        if cov_param is not None:\n            for i in range(len(error_map)):\n                error_map[i] = basis_functions[:, i].T.dot(cov_param[:n_source, :n_source]).dot(basis_functions[:, i])\n        return error_map"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a lens light model and turns it numerically in a lens model (with all lensmodel quantities computed on a grid). Then provides an interpolated grid for the quantities. :param kwargs_lens_light: lens light keyword argument list :param numPix: number of pixels per axis for the return interpolation :param deltaPix: interpolation/pixel size :param center_x: center of the grid :param center_y: center of the grid :param subgrid: subgrid for the numerical integrals :return:", "response": "def light2mass_interpol(lens_light_model_list, kwargs_lens_light, numPix=100, deltaPix=0.05, subgrid_res=5, center_x=0, center_y=0):\n        \"\"\"\n        takes a lens light model and turns it numerically in a lens model\n        (with all lensmodel quantities computed on a grid). Then provides an interpolated grid for the quantities.\n\n        :param kwargs_lens_light: lens light keyword argument list\n        :param numPix: number of pixels per axis for the return interpolation\n        :param deltaPix: interpolation/pixel size\n        :param center_x: center of the grid\n        :param center_y: center of the grid\n        :param subgrid: subgrid for the numerical integrals\n        :return:\n        \"\"\"\n        # make sugrid\n        x_grid_sub, y_grid_sub = util.make_grid(numPix=numPix*5, deltapix=deltaPix, subgrid_res=subgrid_res)\n        import lenstronomy.Util.mask as mask_util\n        mask = mask_util.mask_sphere(x_grid_sub, y_grid_sub, center_x, center_y, r=1)\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        # compute light on the subgrid\n        lightModel = LightModel(light_model_list=lens_light_model_list)\n        flux = lightModel.surface_brightness(x_grid_sub, y_grid_sub, kwargs_lens_light)\n        flux_norm = np.sum(flux[mask == 1]) / np.sum(mask)\n        flux /= flux_norm\n        from lenstronomy.LensModel.numerical_profile_integrals import ConvergenceIntegrals\n        integral = ConvergenceIntegrals()\n\n        # compute lensing quantities with subgrid\n        convergence_sub = flux\n        f_x_sub, f_y_sub = integral.deflection_from_kappa(convergence_sub, x_grid_sub, y_grid_sub,\n                                                          deltaPix=deltaPix/float(subgrid_res))\n        f_sub = integral.potential_from_kappa(convergence_sub, x_grid_sub, y_grid_sub,\n                                                          deltaPix=deltaPix/float(subgrid_res))\n        # interpolation function on lensing quantities\n        x_axes_sub, y_axes_sub = util.get_axes(x_grid_sub, y_grid_sub)\n        from lenstronomy.LensModel.Profiles.interpol import Interpol\n        interp_func = Interpol()\n        interp_func.do_interp(x_axes_sub, y_axes_sub, f_sub, f_x_sub, f_y_sub)\n        # compute lensing quantities on sparser grid\n        x_axes, y_axes = util.get_axes(x_grid, y_grid)\n        f_ = interp_func.function(x_grid, y_grid)\n        f_x, f_y = interp_func.derivatives(x_grid, y_grid)\n        # numerical differentials for second order differentials\n        from lenstronomy.LensModel.numeric_lens_differentials import NumericLens\n        lens_differential = NumericLens(lens_model_list=['INTERPOL'])\n        kwargs = [{'grid_interp_x': x_axes_sub, 'grid_interp_y': y_axes_sub, 'f_': f_sub,\n                   'f_x': f_x_sub, 'f_y': f_y_sub}]\n        f_xx, f_xy, f_yx, f_yy = lens_differential.hessian(x_grid, y_grid, kwargs)\n        kwargs_interpol = {'grid_interp_x': x_axes, 'grid_interp_y': y_axes, 'f_': util.array2image(f_),\n                   'f_x': util.array2image(f_x), 'f_y': util.array2image(f_y), 'f_xx': util.array2image(f_xx),\n                           'f_xy': util.array2image(f_xy), 'f_yy': util.array2image(f_yy)}\n        return kwargs_interpol"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mass_fraction_within_radius(self, kwargs_lens, center_x, center_y, theta_E, numPix=100):\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=2.*theta_E / numPix)\n        x_grid += center_x\n        y_grid += center_y\n        mask = mask_util.mask_sphere(x_grid, y_grid, center_x, center_y, theta_E)\n        kappa_list = []\n        for i in range(len(kwargs_lens)):\n            kappa = self.LensModel.kappa(x_grid, y_grid, kwargs_lens, k=i)\n            kappa_mean = np.sum(kappa * mask) / np.sum(mask)\n            kappa_list.append(kappa_mean)\n        return kappa_list", "response": "computes the mean convergence of all the different lens model components within a spherical aperture"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the search area for the lens equation solver", "response": "def update_search_window(self, search_window, x_center, y_center):\n        \"\"\"\n        update the search area for the lens equation solver\n\n        :param search_window: search_window: window size of the image position search with the lens equation solver.\n        :param x_center: center of search window\n        :param y_center: center of search window\n        :return: updated self instances\n        \"\"\"\n        self._search_window, self._x_center, self._y_center = search_window, x_center, y_center"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef point_source_list(self, kwargs_ps, kwargs_lens, k=None):\n        ra_list, dec_list = self.image_position(kwargs_ps, kwargs_lens, k=k)\n        amp_list = self.image_amplitude(kwargs_ps, kwargs_lens)\n        ra_array, dec_array, amp_array = [], [], []\n        for i, ra in enumerate(ra_list):\n            for j in range(len(ra)):\n                ra_array.append(ra_list[i][j])\n                dec_array.append(dec_list[i][j])\n                amp_array.append(amp_list[i][j])\n        return ra_array, dec_array, amp_array", "response": "returns the coordinates and amplitudes of all point sources in a single array\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the image amplitudes of the point sources", "response": "def image_amplitude(self, kwargs_ps, kwargs_lens, k=None):\n        \"\"\"\n        returns the image amplitudes\n\n        :param kwargs_ps:\n        :param kwargs_lens:\n        :return:\n        \"\"\"\n        amp_list = []\n        for i, model in enumerate(self._point_source_list):\n            if k is None or k == i:\n                amp_list.append(model.image_amplitude(kwargs_ps=kwargs_ps[i], kwargs_lens=kwargs_lens, min_distance=self._min_distance,\n                                                        search_window=self._search_window,\n                                                        precision_limit=self._precision_limit,\n                                                        num_iter_max=self._num_iter_max, x_center=self._x_center,\n                                                        y_center=self._y_center))\n        return amp_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the source amplitudes of the current set of points", "response": "def source_amplitude(self, kwargs_ps, kwargs_lens):\n        \"\"\"\n        returns the source amplitudes\n\n        :param kwargs_ps:\n        :param kwargs_lens:\n        :return:\n        \"\"\"\n        amp_list = []\n        for i, model in enumerate(self._point_source_list):\n            amp_list.append(model.source_amplitude(kwargs_ps=kwargs_ps[i], kwargs_lens=kwargs_lens))\n        return amp_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_image_positions(self, kwargs_ps, kwargs_lens, tolerance=0.001):\n        x_image_list, y_image_list = self.image_position(kwargs_ps, kwargs_lens)\n        for i, model in enumerate(self._point_source_list):\n            if model in ['LENSED_POSITION', 'SOURCE_POSITION']:\n                x_pos = x_image_list[i]\n                y_pos = y_image_list[i]\n                x_source, y_source = self._lensModel.ray_shooting(x_pos, y_pos, kwargs_lens)\n                dist = np.sqrt((x_source - x_source[0]) ** 2 + (y_source - y_source[0]) ** 2)\n                if np.max(dist) > tolerance:\n                    return False\n        return True", "response": "checks whether the point sources in kwargs_ps satisfy the lens equation with a tolerance"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the finite magnification of an extended source with Gaussian light profile.", "response": "def magnification_finite(self, x_pos, y_pos, kwargs_lens, source_sigma=0.003, window_size=0.1, grid_number=100,\n                             shape=\"GAUSSIAN\", polar_grid=False, aspect_ratio=0.5):\n        \"\"\"\n        returns the magnification of an extended source with Gaussian light profile\n        :param x_pos: x-axis positons of point sources\n        :param y_pos: y-axis position of point sources\n        :param kwargs_lens: lens model kwargs\n        :param source_sigma: Gaussian sigma in arc sec in source\n        :param window_size: size of window to compute the finite flux\n        :param grid_number: number of grid cells per axis in the window to numerically comute the flux\n        :return: numerically computed brightness of the sources\n        \"\"\"\n\n        mag_finite = np.zeros_like(x_pos)\n        deltaPix = float(window_size)/grid_number\n        if shape == 'GAUSSIAN':\n            from lenstronomy.LightModel.Profiles.gaussian import Gaussian\n            quasar = Gaussian()\n        elif shape == 'TORUS':\n            import lenstronomy.LightModel.Profiles.torus as quasar\n        else:\n            raise ValueError(\"shape %s not valid for finite magnification computation!\" % shape)\n        x_grid, y_grid = util.make_grid(numPix=grid_number, deltapix=deltaPix, subgrid_res=1)\n\n        if polar_grid:\n            a = window_size*0.5\n            b = window_size*0.5*aspect_ratio\n            ellipse_inds = (x_grid*a**-1) **2 + (y_grid*b**-1) **2 <= 1\n            x_grid, y_grid = x_grid[ellipse_inds], y_grid[ellipse_inds]\n\n        for i in range(len(x_pos)):\n            ra, dec = x_pos[i], y_pos[i]\n\n            center_x, center_y = self._lensModel.ray_shooting(ra, dec, kwargs_lens)\n\n            if polar_grid:\n                theta = np.arctan2(dec,ra)\n                xcoord, ycoord = util.rotate(x_grid, y_grid, theta)\n            else:\n                xcoord, ycoord = x_grid, y_grid\n\n            betax, betay = self._lensModel.ray_shooting(xcoord + ra, ycoord + dec, kwargs_lens)\n\n            I_image = quasar.function(betax, betay, 1., source_sigma, source_sigma, center_x, center_y)\n            mag_finite[i] = np.sum(I_image) * deltaPix**2\n\n        return mag_finite"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _tiling_crit(self, edge1, edge2, edge_90, max_order, kwargs_lens):\n        ra_1, dec_1, mag_1 = edge1\n        ra_2, dec_2, mag_2 = edge2\n        ra_3, dec_3, mag_3 = edge_90\n        sign_list = np.sign([mag_1, mag_2, mag_3])\n        if sign_list[0] == sign_list[1] and sign_list[0] == sign_list[2]:  # if all signs are the same\n            return [], []\n        else:\n            # split triangle along the long axis\n            # execute tiling twice\n            # add ra_crit and dec_crit together\n            # if max depth has been reached, return the mean value in the triangle\n            max_order -= 1\n            if max_order <= 0:\n                return [(ra_1 + ra_2 + ra_3)/3], [(dec_1 + dec_2 + dec_3)/3]\n            else:\n                # split triangle\n                ra_90_ = (ra_1 + ra_2)/2  # find point in the middle of the long axis to split triangle\n                dec_90_ = (dec_1 + dec_2)/2\n                mag_90_ = self._lensModel.magnification(ra_90_, dec_90_, kwargs_lens)\n                edge_90_ = [ra_90_, dec_90_, mag_90_]\n                ra_crit, dec_crit = self._tiling_crit(edge1=edge_90, edge2=edge1, edge_90=edge_90_, max_order=max_order,\n                                                      kwargs_lens=kwargs_lens)\n                ra_crit_2, dec_crit_2 = self._tiling_crit(edge1=edge_90, edge2=edge2, edge_90=edge_90_, max_order=max_order,\n                                                          kwargs_lens=kwargs_lens)\n                ra_crit += ra_crit_2\n                dec_crit += dec_crit_2\n                return ra_crit, dec_crit", "response": "tiling a rectangular triangle and compares the signs of the magnification"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the radius of the effective Einstein radius estimate", "response": "def effective_einstein_radius(self, kwargs_lens_list, k=None, spacing=1000):\n        \"\"\"\n        computes the radius with mean convergence=1\n\n        :param kwargs_lens:\n        :param spacing: number of annular bins to compute the convergence (resolution of the Einstein radius estimate)\n        :return:\n        \"\"\"\n        if 'center_x' in kwargs_lens_list[0]:\n            center_x = kwargs_lens_list[0]['center_x']\n            center_y = kwargs_lens_list[0]['center_y']\n        elif self._lensModel.lens_model_list[0] in ['INTERPOL', 'INTERPOL_SCALED']:\n            center_x, center_y = 0, 0\n        else:\n            center_x, center_y = 0, 0\n        numPix = 200\n        deltaPix = 0.05\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x\n        y_grid += center_y\n        kappa = self._lensModel.kappa(x_grid, y_grid, kwargs_lens_list, k=k)\n        if self._lensModel.lens_model_list[0] in ['INTERPOL', 'INTERPOL_SCALED']:\n            center_x = x_grid[kappa == np.max(kappa)]\n            center_y = y_grid[kappa == np.max(kappa)]\n        kappa = util.array2image(kappa)\n        r_array = np.linspace(0.0001, numPix*deltaPix/2., spacing)\n        for r in r_array:\n            mask = np.array(1 - mask_util.mask_center_2d(center_x, center_y, r, x_grid, y_grid))\n            sum_mask = np.sum(mask)\n            if sum_mask > 0:\n                kappa_mean = np.sum(kappa*mask)/np.sum(mask)\n                if kappa_mean < 1:\n                    return r\n        print(kwargs_lens_list, \"Warning, no Einstein radius computed!\")\n        return r_array[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef external_lensing_effect(self, kwargs_lens, lens_model_internal_bool=None):\n        alpha0_x, alpha0_y = 0, 0\n        kappa_ext = 0\n        shear1, shear2 = 0, 0\n        if lens_model_internal_bool is None:\n            lens_model_internal_bool = [True] * len(kwargs_lens)\n        for i, kwargs in enumerate(kwargs_lens):\n            if not lens_model_internal_bool[i] is True:\n                f_x, f_y = self._lensModel.alpha(0, 0, kwargs_lens, k=i)\n                f_xx, f_xy, f_yx, f_yy = self._lensModel.hessian(0, 0, kwargs_lens, k=i)\n                alpha0_x += f_x\n                alpha0_y += f_y\n                kappa_ext += (f_xx + f_yy)/2.\n                shear1 += 1./2 * (f_xx - f_yy)\n                shear2 += f_xy\n        return alpha0_x, alpha0_y, kappa_ext, shear1, shear2", "response": "computes the external deflection shear and convergence at ( 0 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lens_center(self, kwargs_lens, k=None, bool_list=None, numPix=200, deltaPix=0.01, center_x_init=0, center_y_init=0):\n        x_grid, y_grid = util.make_grid(numPix=numPix, deltapix=deltaPix)\n        x_grid += center_x_init\n        y_grid += center_y_init\n\n        if bool_list is None:\n            kappa = self._lensModel.kappa(x_grid, y_grid, kwargs_lens, k=k)\n        else:\n            kappa = np.zeros_like(x_grid)\n            for k in range(len(kwargs_lens)):\n                if bool_list[k] is True:\n                    kappa += self._lensModel.kappa(x_grid, y_grid, kwargs_lens, k=k)\n        center_x = x_grid[kappa == np.max(kappa)]\n        center_y = y_grid[kappa == np.max(kappa)]\n        return center_x, center_y", "response": "computes convergence weighted center of a lens model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the logarithmic power - law slope of a profile", "response": "def profile_slope(self, kwargs_lens_list, lens_model_internal_bool=None, num_points=10):\n        \"\"\"\n        computes the logarithmic power-law slope of a profile\n\n        :param kwargs_lens_list: lens model keyword argument list\n        :param lens_model_internal_bool: bool list, indicate which part of the model to consider\n        :param num_points: number of estimates around the Einstein radius\n        :return:\n        \"\"\"\n        theta_E = self.effective_einstein_radius(kwargs_lens_list)\n        x0 = kwargs_lens_list[0]['center_x']\n        y0 = kwargs_lens_list[0]['center_y']\n        x, y = util.points_on_circle(theta_E, num_points)\n        dr = 0.01\n        x_dr, y_dr = util.points_on_circle(theta_E + dr, num_points)\n        if lens_model_internal_bool is None:\n            lens_model_internal_bool = [True]*len(kwargs_lens_list)\n\n        alpha_E_x_i, alpha_E_y_i = self._lensModel.alpha(x0 + x, y0 + y, kwargs_lens_list, k=lens_model_internal_bool)\n        alpha_E_r = np.sqrt(alpha_E_x_i**2 + alpha_E_y_i**2)\n        alpha_E_dr_x_i, alpha_E_dr_y_i = self._lensModel.alpha(x0 + x_dr, y0 + y_dr, kwargs_lens_list,\n                                                               k=lens_model_internal_bool)\n        alpha_E_dr = np.sqrt(alpha_E_dr_x_i ** 2 + alpha_E_dr_y_i ** 2)\n        slope = np.mean(np.log(alpha_E_dr / alpha_E_r) / np.log((theta_E + dr) / theta_E))\n        gamma = -slope + 2\n        return gamma"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef kappa(self, x, y, kwargs, diff=diff):\n        f_xx, f_xy, f_yx, f_yy = self.hessian(x, y, kwargs, diff=diff)\n        kappa = 1./2 * (f_xx + f_yy)\n        return kappa", "response": "computes the convergence\n        :return: kappa"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef gamma(self, x, y, kwargs, diff=diff):\n        f_xx, f_xy, f_yx, f_yy = self.hessian(x, y, kwargs, diff=diff)\n        gamma1 = 1./2 * (f_xx - f_yy)\n        gamma2 = f_xy\n        return gamma1, gamma2", "response": "computes the shear gamma"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef magnification(self, x, y, kwargs, diff=diff):\n        f_xx, f_xy, f_yx, f_yy = self.hessian(x, y, kwargs, diff=diff)\n        det_A = (1 - f_xx) * (1 - f_yy) - f_xy*f_yx\n        return 1/det_A", "response": "computes the magnification of the potential species"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hessian(self, x, y, kwargs, diff=diff):\n        alpha_ra, alpha_dec = self.alpha(x, y, kwargs)\n\n        alpha_ra_dx, alpha_dec_dx = self.alpha(x + diff, y, kwargs)\n        alpha_ra_dy, alpha_dec_dy = self.alpha(x, y + diff, kwargs)\n\n        dalpha_rara = (alpha_ra_dx - alpha_ra)/diff\n        dalpha_radec = (alpha_ra_dy - alpha_ra)/diff\n        dalpha_decra = (alpha_dec_dx - alpha_dec)/diff\n        dalpha_decdec = (alpha_dec_dy - alpha_dec)/diff\n\n        f_xx = dalpha_rara\n        f_yy = dalpha_decdec\n        f_xy = dalpha_radec\n        f_yx = dalpha_decra\n        return f_xx, f_xy, f_yx, f_yy", "response": "computes the differentials f_xx f_yy f_yx f_yy from f_x and f_ycluster."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntimes - delay distance in units of Mpc", "response": "def D_dt(self, z_lens, z_source):\n        \"\"\"\n        time-delay distance\n\n        :param z_lens: redshift of lens\n        :param z_source: redshift of source\n        :return: time-delay distance in units of Mpc\n        \"\"\"\n        return self.D_xy(0, z_lens) * self.D_xy(0, z_source) / self.D_xy(z_lens, z_source) * (1 + z_lens)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rho_crit(self):\n        h = self.cosmo.H(0).value / 100.\n        return 3 * h ** 2 / (8 * np.pi * const.G) * 10 ** 10 * const.Mpc / const.M_sun", "response": "return value in M_sol or Mpc^3\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the Hermite polynomial for the given value x", "response": "def hermval(self, x, n_array, tensor=True):\n        \"\"\"\n        computes the Hermit polynomial as numpy.polynomial.hermite.hermval\n        difference: for values more than sqrt(n_max + 1) * cut_scale, the value is set to zero\n        this should be faster and numerically stable\n\n        :param x: array of values\n        :param n_array: list of coeffs in H_n\n        :param cut_scale: scale where the polynomial will be set to zero\n        :return: see numpy.polynomial.hermite.hermval\n        \"\"\"\n        if not self._stable_cut:\n            return hermite.hermval(x, n_array)\n        else:\n            n_max = len(n_array)\n            x_cut = np.sqrt(n_max + 1) * self._cut_scale\n            if isinstance(x, int) or isinstance(x, float):\n                if x >= x_cut:\n                    return 0\n                else:\n                    return hermite.hermval(x, n_array)\n            else:\n                out = np.zeros_like(x)\n                out[x < x_cut] = hermite.hermval(x[x < x_cut], n_array, tensor=tensor)\n                return out"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the Hermite polynomial of order n at position x.", "response": "def H_n(self, n, x):\n        \"\"\"\n        constructs the Hermite polynomial of order n at position x (dimensionless)\n\n        :param n: The n'the basis function.\n        :type name: int.\n        :param x: 1-dim position (dimensionless)\n        :type state: float or numpy array.\n        :returns:  array-- H_n(x).\n        :raises: AttributeError, KeyError\n        \"\"\"\n        if not self._interpolation:\n            n_array = np.zeros(n+1)\n            n_array[n] = 1\n            return self.hermval(x, n_array, tensor=False)  # attention, this routine calculates every single hermite polynomial and multiplies it with zero (exept the right one)\n        else:\n            return np.interp(x, self.x_grid, self.H_interp[n])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_calc(self, x, y, beta, n_order, center_x, center_y):\n        x_ = x - center_x\n        y_ = y - center_y\n        n = len(np.atleast_1d(x))\n        H_x = np.empty((n_order+1, n))\n        H_y = np.empty((n_order+1, n))\n        if n_order > 170:\n            raise ValueError('polynomial order to large', n_order)\n        for n in range(0, n_order+1):\n\n            prefactor = 1./np.sqrt(2**n*np.sqrt(np.pi)*math.factorial(n))\n            n_array = np.zeros(n+1)\n            n_array[n] = 1\n            H_x[n] = self.hermval(x_/beta, n_array, tensor=False) * prefactor * np.exp(-(x_/beta)**2/2.)\n            H_y[n] = self.hermval(y_/beta, n_array, tensor=False) * prefactor * np.exp(-(y_/beta)**2/2.)\n        return H_x, H_y", "response": "Calculates the H_n x and H_n y for a given x - array and y - array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecompose an image into the shapelet coefficients in same order as for the function call :param image: :param x: :param y: :param n_max: :param beta: :param center_x: :param center_y: :return:", "response": "def decomposition(self, image, x, y, n_max, beta, deltaPix, center_x=0, center_y=0):\n        \"\"\"\n        decomposes an image into the shapelet coefficients in same order as for the function call\n        :param image:\n        :param x:\n        :param y:\n        :param n_max:\n        :param beta:\n        :param center_x:\n        :param center_y:\n        :return:\n        \"\"\"\n        num_param = int((n_max+1)*(n_max+2)/2)\n        param_list = np.zeros(num_param)\n        amp_norm = 1./beta**2*deltaPix**2\n        n1 = 0\n        n2 = 0\n        H_x, H_y = self.shapelets.pre_calc(x, y, beta, n_max, center_x, center_y)\n        for i in range(num_param):\n            kwargs_source_shapelet = {'center_x': center_x, 'center_y': center_y, 'n1': n1, 'n2': n2, 'beta': beta, 'amp': amp_norm}\n            base = self.shapelets.function(H_x, H_y, **kwargs_source_shapelet)\n            param = np.sum(image*base)\n            param_list[i] = param\n            if n1 == 0:\n                n1 = n2 + 1\n                n2 = 0\n            else:\n                n1 -= 1\n                n2 += 1\n        return param_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sample(self, maxIter=1000, c1=1.193, c2=1.193, lookback = 0.25, standard_dev = None):\n        self._get_fitness(self.swarm)\n        i = 0\n        self.i = i\n        while True:\n\n            for particle in self.swarm:\n                if ((self._gbest.fitness)<particle.fitness):\n\n                    self._gbest = particle.copy()\n\n                if (particle.fitness > particle.pbest.fitness):\n                    particle.updatePBest()\n\n            if(i>=maxIter):\n                if self._verbose:\n                    print(\"max iteration reached! stoping\")\n                return\n\n            if self._func.is_converged:\n                return\n\n            if self._converged_likelihood(maxIter*lookback, self._particleCount, standard_dev):\n                return\n\n            for particle in self.swarm:\n\n                w = 0.5 + numpy.random.uniform(0, 1, size=self._paramCount) / 2\n                #w=0.72\n                part_vel = w * particle.velocity\n                cog_vel = c1 * numpy.random.uniform(0, 1, size=self._paramCount) * (particle.pbest.position - particle.position)\n                soc_vel = c2 * numpy.random.uniform(0, 1, size=self._paramCount) * (self._gbest.position - particle.position)\n                particle.velocity = part_vel + cog_vel + soc_vel\n                particle.position = particle.position + particle.velocity\n\n            self._get_fitness(self.swarm)\n\n            swarm = []\n            for particle in self.swarm:\n                swarm.append(particle.copy())\n            yield swarm\n\n            i+=1\n            self.i = i", "response": "Generate a random set of random particles for a particular set of attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, paramCount):\n\n        return Particle(numpy.array([[]]*paramCount),\n                 numpy.array([[]]*paramCount),\n                 -numpy.Inf)", "response": "Creates a new particle with position velocity and - inf as fitness\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a copy of itself", "response": "def copy(self):\n        \"\"\"\n        Creates a copy of itself\n        \"\"\"\n        return Particle(copy(self.position),\n                        copy(self.velocity),\n                        self.fitness)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets all the cache in the point source class and saves it from then on", "response": "def reset_point_source_cache(self, bool=True):\n        \"\"\"\n        deletes all the cache in the point source class and saves it from then on\n\n        :return:\n        \"\"\"\n        for imageModel in self._imageModel_list:\n            imageModel.reset_point_source_cache(bool=bool)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing quadrupole moments from a light distribution I_xy", "response": "def moments(I_xy_input, x, y):\n    \"\"\"\n    compute quadrupole moments from a light distribution\n\n    :param I_xy: light distribution\n    :param x: x-coordinates of I_xy\n    :param y: y-coordinates of I_xy\n    :return: Q_xx, Q_xy, Q_yy\n    \"\"\"\n    I_xy = copy.deepcopy(I_xy_input)\n    background = np.minimum(0, np.min(I_xy))\n    I_xy -= background\n    x_ = np.sum(I_xy * x)\n    y_ = np.sum(I_xy * y)\n    r = (np.max(x) - np.min(x)) / 3.\n    mask = mask_util.mask_sphere(x, y, center_x=x_, center_y=y_, r=r)\n    Q_xx = np.sum(I_xy * mask * (x - x_) ** 2)\n    Q_xy = np.sum(I_xy * mask * (x - x_) * (y - y_))\n    Q_yy = np.sum(I_xy * mask * (y - y_) ** 2)\n    return Q_xx, Q_xy, Q_yy, background / np.mean(I_xy)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ellipticities(I_xy, x, y):\n    Q_xx, Q_xy, Q_yy, bkg = moments(I_xy, x, y)\n    norm = Q_xx + Q_yy + 2 * np.sqrt(Q_xx*Q_yy - Q_xy**2)\n    e1 = (Q_xx - Q_yy) / norm\n    e2 = 2 * Q_xy / norm\n    return e1 / (1+bkg), e2 / (1+bkg)", "response": "compute ellipticities of a light distribution"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ray_shooting(self, theta_x, theta_y, kwargs_lens, k=None):\n        x = np.zeros_like(theta_x)\n        y = np.zeros_like(theta_y)\n        alpha_x = theta_x\n        alpha_y = theta_y\n        i = -1\n        for i, idex in enumerate(self._sorted_redshift_index):\n            delta_T = self._T_ij_list[i]\n            x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n            alpha_x, alpha_y = self._add_deflection(x, y, alpha_x, alpha_y, kwargs_lens, i)\n        delta_T = self._T_ij_list[i+1]\n        x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n        beta_x, beta_y = self._co_moving2angle_source(x, y)\n        return beta_x, beta_y", "response": "ray shooting for the image"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nray shooting for a set of objects", "response": "def ray_shooting_partial(self, x, y, alpha_x, alpha_y, z_start, z_stop, kwargs_lens, keep_range=False,\n                             include_z_start=False):\n        \"\"\"\n        ray-tracing through parts of the coin, starting with (x,y) and angles (alpha_x, alpha_y) at redshift z_start\n        and then backwards to redshfit z_stop\n\n        :param x: co-moving position [Mpc]\n        :param y: co-moving position [Mpc]\n        :param alpha_x: ray angle at z_start [arcsec]\n        :param alpha_y: ray angle at z_start [arcsec]\n        :param z_start: redshift of start of computation\n        :param z_stop: redshift where output is computed\n        :param kwargs_lens: lens model keyword argument list\n        :param keep_range: bool, if True, only computes the angular diameter ratio between the first and last step once\n        :return: co-moving position and angles at redshift z_stop\n        \"\"\"\n        z_lens_last = z_start\n        first_deflector = True\n        for i, idex in enumerate(self._sorted_redshift_index):\n            z_lens = self._redshift_list[idex]\n            if self._start_condition(include_z_start, z_lens, z_start) and z_lens <= z_stop:\n            #if z_lens > z_start and z_lens <= z_stop:\n                if first_deflector is True:\n                    if keep_range is True:\n                        if not hasattr(self, '_cosmo_bkg_T_start'):\n                            self._cosmo_bkg_T_start = self._cosmo_bkg.T_xy(z_start, z_lens)\n                        delta_T = self._cosmo_bkg_T_start\n                    else:\n                        delta_T = self._cosmo_bkg.T_xy(z_start, z_lens)\n                    first_deflector = False\n                else:\n                    delta_T = self._T_ij_list[i]\n                x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n                alpha_x, alpha_y = self._add_deflection(x, y, alpha_x, alpha_y, kwargs_lens, i)\n                z_lens_last = z_lens\n        if keep_range is True:\n            if not hasattr(self, '_cosmo_bkg_T_stop'):\n                self._cosmo_bkg_T_stop = self._cosmo_bkg.T_xy(z_lens_last, z_stop)\n            delta_T = self._cosmo_bkg_T_stop\n        else:\n            delta_T = self._cosmo_bkg.T_xy(z_lens_last, z_stop)\n        x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n        return x, y, alpha_x, alpha_y"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ray_shooting_partial_steps(self, x, y, alpha_x, alpha_y, z_start, z_stop, kwargs_lens,\n                             include_z_start=False):\n        \"\"\"\n        ray-tracing through parts of the coin, starting with (x,y) and angles (alpha_x, alpha_y) at redshift z_start\n        and then backwards to redshfit z_stop.\n\n        This function differs from 'ray_shooting_partial' in that it returns the angular position of the ray\n        at each lens plane.\n\n        :param x: co-moving position [Mpc]\n        :param y: co-moving position [Mpc]\n        :param alpha_x: ray angle at z_start [arcsec]\n        :param alpha_y: ray angle at z_start [arcsec]\n        :param z_start: redshift of start of computation\n        :param z_stop: redshift where output is computed\n        :param kwargs_lens: lens model keyword argument list\n        :param keep_range: bool, if True, only computes the angular diameter ratio between the first and last step once\n        :return: co-moving position and angles at redshift z_stop\n        \"\"\"\n        z_lens_last = z_start\n        first_deflector = True\n\n        pos_x, pos_y, redshifts, Tz_list = [], [], [], []\n        pos_x.append(x)\n        pos_y.append(y)\n        redshifts.append(z_start)\n        Tz_list.append(self._cosmo_bkg.T_xy(0, z_start))\n\n        current_z = z_lens_last\n\n        for i, idex in enumerate(self._sorted_redshift_index):\n\n            z_lens = self._redshift_list[idex]\n\n            if self._start_condition(include_z_start,z_lens,z_start) and z_lens <= z_stop:\n\n                if z_lens != current_z:\n                    new_plane = True\n                    current_z = z_lens\n\n                else:\n                    new_plane = False\n\n                if first_deflector is True:\n                    delta_T = self._cosmo_bkg.T_xy(z_start, z_lens)\n\n                    first_deflector = False\n                else:\n                    delta_T = self._T_ij_list[i]\n                x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n                alpha_x, alpha_y = self._add_deflection(x, y, alpha_x, alpha_y, kwargs_lens, i)\n                z_lens_last = z_lens\n\n                if new_plane:\n\n                    pos_x.append(x)\n                    pos_y.append(y)\n                    redshifts.append(z_lens)\n                    Tz_list.append(self._T_z_list[i])\n\n        delta_T = self._cosmo_bkg.T_xy(z_lens_last, z_stop)\n\n        x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n\n        pos_x.append(x)\n        pos_y.append(y)\n        redshifts.append(self._z_source)\n        Tz_list.append(self._T_z_source)\n\n        return pos_x, pos_y, redshifts, Tz_list", "response": "This function is used to perform ray shooting on a set of regions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef arrival_time(self, theta_x, theta_y, kwargs_lens, k=None):\n        dt_grav = np.zeros_like(theta_x)\n        dt_geo = np.zeros_like(theta_x)\n        x = np.zeros_like(theta_x)\n        y = np.zeros_like(theta_y)\n        alpha_x = theta_x\n        alpha_y = theta_y\n        i = 0\n        for i, idex in enumerate(self._sorted_redshift_index):\n            z_lens = self._redshift_list[idex]\n            delta_T = self._T_ij_list[i]\n            dt_geo_new = self._geometrical_delay(alpha_x, alpha_y, delta_T)\n            x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n            dt_grav_new = self._gravitational_delay(x, y, kwargs_lens, i, z_lens)\n            alpha_x, alpha_y = self._add_deflection(x, y, alpha_x, alpha_y, kwargs_lens, i)\n            dt_geo = dt_geo + dt_geo_new\n            dt_grav = dt_grav + dt_grav_new\n        delta_T = self._T_ij_list[i + 1]\n        dt_geo += self._geometrical_delay(alpha_x, alpha_y, delta_T)\n        x, y = self._ray_step(x, y, alpha_x, alpha_y, delta_T)\n        beta_x, beta_y = self._co_moving2angle_source(x, y)\n        dt_geo -= self._geometrical_delay(beta_x, beta_y, self._T_z_source)\n        return dt_grav + dt_geo", "response": "calculate light travel time relative to a straight path through the coordinate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef alpha(self, theta_x, theta_y, kwargs_lens, k=None):\n        beta_x, beta_y = self.ray_shooting(theta_x, theta_y, kwargs_lens)\n        alpha_x = theta_x - beta_x\n        alpha_y = theta_y - beta_y\n        return alpha_x, alpha_y", "response": "reduced deflection angle\n\n        :param theta_x: angle in x-direction\n        :param theta_y: angle in y-direction\n        :param kwargs_lens: lens model kwargs\n        :return:"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the hessian components f_xx f_yy f_yx f_yy from f_x and f_y with numerical differentiation", "response": "def hessian(self, theta_x, theta_y, kwargs_lens, k=None, diff=0.00000001):\n        \"\"\"\n        computes the hessian components f_xx, f_yy, f_xy from f_x and f_y with numerical differentiation\n\n        :param theta_x: x-position (preferentially arcsec)\n        :type theta_x: numpy array\n        :param theta_y: y-position (preferentially arcsec)\n        :type theta_y: numpy array\n        :param kwargs_lens: list of keyword arguments of lens model parameters matching the lens model classes\n        :param diff: numerical differential step (float)\n        :return: f_xx, f_xy, f_yx, f_yy\n        \"\"\"\n\n        alpha_ra, alpha_dec = self.alpha(theta_x, theta_y, kwargs_lens)\n\n        alpha_ra_dx, alpha_dec_dx = self.alpha(theta_x + diff, theta_y, kwargs_lens)\n        alpha_ra_dy, alpha_dec_dy = self.alpha(theta_x, theta_y + diff, kwargs_lens)\n\n        dalpha_rara = (alpha_ra_dx - alpha_ra)/diff\n        dalpha_radec = (alpha_ra_dy - alpha_ra)/diff\n        dalpha_decra = (alpha_dec_dx - alpha_dec)/diff\n        dalpha_decdec = (alpha_dec_dy - alpha_dec)/diff\n\n        f_xx = dalpha_rara\n        f_yy = dalpha_decdec\n        f_xy = dalpha_radec\n        f_yx = dalpha_decra\n        return f_xx, f_xy, f_yx, f_yy"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reduced2physical_deflection(self, alpha_reduced, idex_lens):\n        factor = self._reduced2physical_factor[idex_lens]\n        #factor = self._cosmo_bkg.D_xy(0, z_source) / self._cosmo_bkg.D_xy(z_lens, z_source)\n        return alpha_reduced * factor", "response": "Convert reduced deflection angle to physical deflection angle"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _geometrical_delay(self, alpha_x, alpha_y, delta_T):\n        dt_days = (alpha_x**2 + alpha_y**2) / 2. * delta_T * const.Mpc / const.c / const.day_s * const.arcsec**2\n        return dt_days", "response": "Compute the geometrical delay of a light ray with an angle relative to the shortest path\n        and a transversal diameter distance between the start and end of the ray with an angle relative to the shortest path\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _lensing_potential2time_delay(self, potential, z_lens, z_source):\n        D_dt = self._cosmo_bkg.D_dt(z_lens, z_source)\n        delay_days = const.delay_arcsec2days(potential, D_dt)\n        return delay_days", "response": "transforms the lensing potential to a gravitational time - delay as measured at z = 0\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _co_moving2angle(self, x, y, idex):\n        T_z = self._T_z_list[idex]\n        #T_z = self._cosmo_bkg.T_xy(0, z_lens)\n        theta_x = x / T_z\n        theta_y = y / T_z\n        return theta_x, theta_y", "response": "transforms co - moving distances Mpc into angles on the sky"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _co_moving2angle_source(self, x, y):\n        T_z = self._T_z_source\n        theta_x = x / T_z\n        theta_y = y / T_z\n        return theta_x, theta_y", "response": "special case of the co_moving2angle definition at the source redshift"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _ray_step(self, x, y, alpha_x, alpha_y, delta_T):\n        x_ = x + alpha_x * delta_T\n        y_ = y + alpha_y * delta_T\n        return x_, y_", "response": "ray propagation with small angle approximation"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_deflection(self, x, y, alpha_x, alpha_y, kwargs_lens, idex):\n        theta_x, theta_y = self._co_moving2angle(x, y, idex)\n        alpha_x_red, alpha_y_red = self._lens_model.alpha(theta_x, theta_y, kwargs_lens, k=self._sorted_redshift_index[idex])\n        alpha_x_phys = self._reduced2physical_deflection(alpha_x_red, idex)\n        alpha_y_phys = self._reduced2physical_deflection(alpha_y_red, idex)\n        alpha_x_new = alpha_x - alpha_x_phys\n        alpha_y_new = alpha_y - alpha_y_phys\n        return alpha_x_new, alpha_y_new", "response": "Adds the pyhsical deflection angle to the deflection field of a single lens model"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef derivatives(self, x, y, theta_E, center_x=0, center_y=0):\n        x_shift = x - center_x\n        y_shift = y - center_y\n        R = np.sqrt(x_shift*x_shift + y_shift*y_shift)\n        if isinstance(R, int) or isinstance(R, float):\n            a = theta_E / max(0.000001, R)\n        else:\n            a=np.empty_like(R)\n            r = R[R > 0]  #in the SIS regime\n            a[R == 0] = 0\n            a[R > 0] = theta_E / r\n        f_x = a * x_shift\n        f_y = a * y_shift\n        return f_x, f_y", "response": "returns df x y of the function\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef hessian(self, x, y, theta_E, center_x=0, center_y=0):\n        x_shift = x - center_x\n        y_shift = y - center_y\n        R = (x_shift*x_shift + y_shift*y_shift)**(3./2)\n        if isinstance(R, int) or isinstance(R, float):\n            prefac = theta_E / max(0.000001, R)\n        else:\n            prefac = np.empty_like(R)\n            r = R[R>0]  #in the SIS regime\n            prefac[R==0] = 0.\n            prefac[R>0] = theta_E / r\n\n        f_xx = y_shift*y_shift * prefac\n        f_yy = x_shift*x_shift * prefac\n        f_xy = -x_shift*y_shift * prefac\n        return f_xx, f_yy, f_xy", "response": "returns Hessian matrix of function d^2f x y theta_E"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfunctions that computes the profile of a single node.", "response": "def function(x, y, amp, a_x, a_y, center_x, center_y):\n    \"\"\"\n    returns torus (ellipse with constant surface brightnes) profile\n    \"\"\"\n    x_shift = x - center_x\n    y_shift = y - center_y\n    A = np.pi * a_x * a_y\n    dist = (x_shift/a_x)**2 + (y_shift/a_y)**2\n    torus = np.zeros_like(x)\n    torus[dist <= 1] = 1\n    return amp/A * torus"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to compute the source plane positions corresponding to the image", "response": "def ray_shooting(self, x, y, kwargs, k=None):\n        \"\"\"\n        maps image to source position (inverse deflection)\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: source plane positions corresponding to (x, y) in the image plane\n        \"\"\"\n        dx, dy = self.alpha(x, y, kwargs, k=k)\n        return x - dx, y - dy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the fermat potential of the image at the given coordinates", "response": "def fermat_potential(self, x_image, y_image, x_source, y_source, kwargs_lens, k=None):\n        \"\"\"\n        fermat potential (negative sign means earlier arrival time)\n\n        :param x_image: image position\n        :param y_image: image position\n        :param x_source: source position\n        :param y_source: source position\n        :param kwargs_lens: list of keyword arguments of lens model parameters matching the lens model classes\n        :return: fermat potential in arcsec**2 without geometry term (second part of Eqn 1 in Suyu et al. 2013) as a list\n        \"\"\"\n\n        potential = self.potential(x_image, y_image, kwargs_lens, k=k)\n        geometry = ((x_image - x_source)**2 + (y_image - y_source)**2) / 2.\n        return geometry - potential"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlenses potential for a given set of points x y and kwargs", "response": "def potential(self, x, y, kwargs, k=None):\n        \"\"\"\n        lensing potential\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: lensing potential in units of arcsec^2\n        \"\"\"\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        bool_list = self._bool_list(k)\n        x_, y_, kwargs_copy = self._update_foreground(x, y, kwargs)\n        potential = np.zeros_like(x)\n        for i, func in enumerate(self.func_list):\n            if bool_list[i] is True:\n                if self._model_list[i] == 'SHEAR':\n                    potential += func.function(x, y, **kwargs[i])\n                else:\n                    potential += func.function(x_, y_, **kwargs_copy[i])\n        return potential"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef alpha(self, x, y, kwargs, k=None):\n\n        \"\"\"\n        deflection angles\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: deflection angles in units of arcsec\n        \"\"\"\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        bool_list = self._bool_list(k)\n        x_, y_, kwargs_copy = self._update_foreground(x, y, kwargs)\n        f_x, f_y = np.zeros_like(x_), np.zeros_like(x_)\n        for i, func in enumerate(self.func_list):\n            if bool_list[i] is True:\n                if self._model_list[i] == 'SHEAR':\n                    f_x_i, f_y_i = func.derivatives(x, y, **kwargs[i])\n                else:\n                    f_x_i, f_y_i = func.derivatives(x_, y_, **kwargs_copy[i])\n                f_x += f_x_i\n                f_y += f_y_i\n        return f_x, f_y", "response": "compute the deflection angles for the current lens model class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the hessian of the lens model class", "response": "def hessian(self, x, y, kwargs, k=None):\n        \"\"\"\n        hessian matrix\n        :param x: x-position (preferentially arcsec)\n        :type x: numpy array\n        :param y: y-position (preferentially arcsec)\n        :type y: numpy array\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param k: only evaluate the k-th lens model\n        :return: f_xx, f_xy, f_yy components\n        \"\"\"\n        x = np.array(x, dtype=float)\n        y = np.array(y, dtype=float)\n        if self._foreground_shear:\n            # needs to be computed numerically due to non-linear effects\n            f_xx, f_xy, f_yx, f_yy = self.hessian_differential(x, y, kwargs, k=k)\n        else:\n            bool_list = self._bool_list(k)\n            x_ = x\n            y_ = y\n            f_xx, f_yy, f_xy = np.zeros_like(x_), np.zeros_like(x_), np.zeros_like(x_)\n            for i, func in enumerate(self.func_list):\n                if bool_list[i] is True:\n                    f_xx_i, f_yy_i, f_xy_i = func.hessian(x_, y_, **kwargs[i])\n                    f_xx += f_xx_i\n                    f_yy += f_yy_i\n                    f_xy += f_xy_i\n        f_yx = f_xy\n        return f_xx, f_xy, f_yx, f_yy"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompute the mass of a 3d sphere of radius r", "response": "def mass_3d(self, r, kwargs, bool_list=None):\n        \"\"\"\n        computes the mass within a 3d sphere of radius r\n\n        :param r: radius (in angular units)\n        :param kwargs: list of keyword arguments of lens model parameters matching the lens model classes\n        :param bool_list: list of bools that are part of the output\n        :return: mass (in angular units, modulo epsilon_crit)\n        \"\"\"\n        bool_list = self._bool_list(bool_list)\n        mass_3d = 0\n        for i, func in enumerate(self.func_list):\n            if bool_list[i] is True:\n                kwargs_i = {k:v for k, v in kwargs[i].items() if not k in ['center_x', 'center_y']}\n                mass_3d_i = func.mass_3d_lens(r, **kwargs_i)\n                mass_3d += mass_3d_i\n                #except:\n                #    raise ValueError('Lens profile %s does not support a 3d mass function!' % self.model_list[i])\n        return mass_3d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mass_2d(self, r, kwargs, bool_list=None):\n        bool_list = self._bool_list(bool_list)\n        mass_2d = 0\n        for i, func in enumerate(self.func_list):\n            if bool_list[i] is True:\n                kwargs_i = {k: v for k, v in kwargs[i].items() if not k in ['center_x', 'center_y']}\n                mass_2d_i = func.mass_2d_lens(r, **kwargs_i)\n                mass_2d += mass_2d_i\n                #except:\n                #    raise ValueError('Lens profile %s does not support a 2d mass function!' % self.model_list[i])\n        return mass_2d", "response": "computes the projected mass of a 2d radius r"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _bool_list(self, k=None):\n        n = len(self.func_list)\n        if k is None:\n            bool_list = [True] * n\n        elif isinstance(k, (int, np.integer)):\n            bool_list = [False] * n\n            bool_list[k] = True\n        else:\n            bool_list = [False] * n\n            for i, k_i in enumerate(k):\n                if k_i is not False:\n                    if k_i is True:\n                        bool_list[i] = True\n                    elif k_i < n:\n                        bool_list[k_i] = True\n                    else:\n                        raise ValueError(\"k as set by %s is not convertable in a bool string!\" % k)\n        if self._foreground_shear is True:\n            bool_list[self._foreground_shear_idex] = False\n        return bool_list", "response": "returns a list of the length of the lens models\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating a open namespace from symbol namespace x y z", "response": "def open_ns(symbol):\n        ''' generates a open namespace from symbol namespace x { y { z {'''\n        blocks = ['namespace {0} {{'.format(x) for x in symbol.module.name_parts]\n        return ' '.join(blocks)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a closing names statement from a symbol", "response": "def close_ns(symbol):\n        '''generates a closing names statement from a symbol'''\n        closing = ' '.join(['}' for x in symbol.module.name_parts])\n        name = '::'.join(symbol.module.name_parts)\n        return '{0} // namespace {1}'.format(closing, name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ns(symbol):\n        '''generates a namespace x::y::z statement from a symbol'''\n        if symbol.type and symbol.type.is_primitive:\n            return ''\n        return '{0}::'.format('::'.join(symbol.module.name_parts))", "response": "generates a namespace x :: y :: z statement from a symbol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking several files or directories as src and generates the code in the given dst directory.", "response": "def app(config, src, dst, features, reload, force):\n    \"\"\"Takes several files or directories as src and generates the code\n    in the given dst directory.\"\"\"\n    config = Path(config)\n    if reload:\n        argv = sys.argv.copy()\n        argv.remove('--reload')\n        monitor(config.dirname(), src, dst, argv)\n    else:\n        run(config, src, dst, force)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reload(script, input, output):\n    script = Path(script).expand().abspath()\n    output = Path(output).expand().abspath()\n    input = input if isinstance(input, (list, tuple)) else [input]\n    output.makedirs_p()\n    _script_reload(script, input, output)", "response": "Reloads the generator script when the script files\n    or the input files changes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _script_reload(script, input, output):\n    input = [Path(entry).expand().abspath() for entry in input]\n    output = Path(output).expand().abspath()\n    cmd = 'python3 {0} {1} {2}'.format(script, ' '.join(input), output)\n    event_handler = RunScriptChangeHandler(cmd)\n    event_handler.run()  # run always once\n    observer = Observer()\n    path = script.dirname().expand().abspath()\n    click.secho('watch: {0}'.format(path), fg='blue')\n    observer.schedule(event_handler, path, recursive=True)\n    for entry in input:\n        entry = entry.dirname().expand().abspath()\n        click.secho('watch: {0}'.format(entry), fg='blue')\n        observer.schedule(event_handler, entry, recursive=True)\n    path = Path(__file__).parent / 'qface'\n    click.secho('watch: {0}'.format(path), fg='blue')\n    observer.schedule(event_handler, path, recursive=True)\n    observer.start()\n\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()", "response": "run the named generator and monitor the input and generator folder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninstalling the script onto the system using pip3", "response": "def install(editable):\n    \"\"\"install the script onto the system using pip3\"\"\"\n    script_dir = str(Path(__file__).parent.abspath())\n    click.secho(script_dir, fg='blue')\n    if editable:\n        sh('pip3 install --editable {0} --upgrade'.format(script_dir))\n    else:\n        sh('pip3 install {0} --upgrade'.format(script_dir))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef merge(a, b):\n    \"merges b into a recursively if a and b are dicts\"\n    for key in b:\n        if isinstance(a.get(key), dict) and isinstance(b.get(key), dict):\n            merge(a[key], b[key])\n        else:\n            a[key] = b[key]\n    return a", "response": "merges b into a recursively if a and b are dicts"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves a single template file from the template loader", "response": "def get_template(self, name):\n        \"\"\"Retrieves a single template file from the template loader\"\"\"\n        source = name\n        if name and name[0] is '/':\n            source = name[1:]\n        elif self.source is not None:\n            source = '/'.join((self.source, name))\n        return self.env.get_template(source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render(self, name, context):\n        if Generator.strict:\n            self.env.undefined = TestableUndefined\n        else:\n            self.env.undefined = Undefined\n        template = self.get_template(name)\n        return template.render(context)", "response": "Returns the rendered text from a single template file from the base class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the rendered text of a template instance", "response": "def apply(self, template, context={}):\n        context.update(self.context)\n        \"\"\"Return the rendered text of a template instance\"\"\"\n        return self.env.from_string(template).render(context)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a template into a file given a context.", "response": "def write(self, file_path, template, context={}, preserve=False, force=False):\n        \"\"\"Using a template file name it renders a template\n           into a file given a context\n        \"\"\"\n        if not file_path or not template:\n            click.secho('source or target missing for document')\n            return\n        if not context:\n            context = self.context\n        error = False\n        try:\n            self._write(file_path, template, context, preserve, force)\n        except TemplateSyntaxError as exc:\n            message = '{0}:{1}: error: {2}'.format(exc.filename, exc.lineno, exc.message)\n            click.secho(message, fg='red', err=True)\n            error = True\n        except TemplateNotFound as exc:\n            message = '{0}: error: Template not found'.format(exc.name)\n            click.secho(message, fg='red', err=True)\n            error = True\n        except TemplateError as exc:\n            # Just return with an error, the generic template_error_handler takes care of printing it\n            error = True\n\n        if error and Generator.strict:\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the templates read from the rules document", "response": "def process_rules(self, path: Path, system: System):\n        \"\"\"writes the templates read from the rules document\"\"\"\n        self.context.update({\n            'system': system,\n        })\n        document = FileSystem.load_yaml(path, required=True)\n        for module, rules in document.items():\n            click.secho('process: {0}'.format(module), fg='green')\n            self._process_rules(rules, system)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a set of rules for a target", "response": "def _process_rules(self, rules: dict, system: System):\n        \"\"\" process a set of rules for a target \"\"\"\n        self._source = None # reset the template source\n        if not self._shall_proceed(rules):\n            return\n        self.context.update(rules.get('context', {}))\n        self.path = rules.get('path', '')\n        self.source = rules.get('source', None)\n        self._process_rule(rules.get('system', None), {'system': system})\n        for module in system.modules:\n            self._process_rule(rules.get('module', None), {'module': module})\n            for interface in module.interfaces:\n                self._process_rule(rules.get('interface', None), {'interface': interface})\n            for struct in module.structs:\n                self._process_rule(rules.get('struct', None), {'struct': struct})\n            for enum in module.enums:\n                self._process_rule(rules.get('enum', None), {'enum': enum})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _process_rule(self, rule: dict, context: dict):\n        if not rule or not self._shall_proceed(rule):\n            return\n        self.context.update(context)\n        self.context.update(rule.get('context', {}))\n        self.path = rule.get('path', None)\n        self.source = rule.get('source', None)\n        for entry in rule.get('documents', []):\n            target, source = self._resolve_rule_document(entry)\n            self.write(target, source)\n        for entry in rule.get('preserve', []):\n            target, source = self._resolve_rule_document(entry)\n            self.write(target, source, preserve=True)", "response": "process a single rule"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a document and returns the resulting domain", "response": "def _parse_document(document: Path, system: System = None, profile=EProfile.FULL):\n        \"\"\"Parses a document and returns the resulting domain system\n\n        :param path: document path to parse\n        :param system: system to be used (optional)\n        \"\"\"\n        logger.debug('parse document: {0}'.format(document))\n        stream = FileStream(str(document), encoding='utf-8')\n        system = FileSystem._parse_stream(stream, system, document, profile)\n        FileSystem.merge_annotations(system, document.stripext() + '.yaml')\n        return system"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads a YAML document and for each root symbol identifier updates the tag information of that root symbol identifier", "response": "def merge_annotations(system, document):\n        \"\"\"Read a YAML document and for each root symbol identifier\n        updates the tag information of that symbol\n        \"\"\"\n        if not Path(document).exists():\n            return\n        meta = FileSystem.load_yaml(document)\n        click.secho('merge: {0}'.format(document.name), fg='blue')\n        for identifier, data in meta.items():\n            symbol = system.lookup(identifier)\n            if symbol:\n                merge(symbol.tags, data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(input, identifier: str = None, use_cache=False, clear_cache=True, pattern=\"*.qface\", profile=EProfile.FULL):\n        inputs = input if isinstance(input, (list, tuple)) else [input]\n        logger.debug('parse input={0}'.format(inputs))\n        identifier = 'system' if not identifier else identifier\n        system = System()\n        cache = None\n        if use_cache:\n            cache = shelve.open('qface.cache')\n            if identifier in cache and clear_cache:\n                del cache[identifier]\n            if identifier in cache:\n                # use the cached domain model\n                system = cache[identifier]\n        # if domain model not cached generate it\n        for input in inputs:\n            path = Path.getcwd() / str(input)\n            if path.isfile():\n                FileSystem.parse_document(path, system)\n            else:\n                for document in path.walkfiles(pattern):\n                    FileSystem.parse_document(document, system)\n\n        if use_cache:\n            cache[identifier] = system\n        return system", "response": "Parse a directory or a list of files or directories and return a System object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef monitor(args, watch):\n    watch = watch if isinstance(watch, (list, tuple)) else [watch]\n    watch = [Path(entry).expand().abspath() for entry in watch]\n    event_handler = RunScriptChangeHandler(args)\n    observer = Observer()\n    for entry in watch:\n        if entry.isfile():\n            entry = entry.parent\n        click.secho('watch recursive: {0}'.format(entry), fg='blue')\n        observer.schedule(event_handler, entry, recursive=True)\n    event_handler.run()  # run always once\n    observer.start()\n\n    try:\n        while True:\n            time.sleep(1)\n    except KeyboardInterrupt:\n        observer.stop()\n    observer.join()", "response": "watch the src files of the current node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qualified_name(self):\n        '''return the fully qualified name (`<module>.<name>`)'''\n        if self.module == self:\n            return self.module.name\n        else:\n            if \".\" not in self.name:\n                return '{0}.{1}'.format(self.module.name, self.name)\n            else:\n                # We have a fully qualified reference, just return it\n                return self.name", "response": "return the fully qualified name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_tag(self, tag):\n        if tag not in self._tags:\n            self._tags[tag] = dict()", "response": "add a tag to the tag list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an attribute to the named tag", "response": "def add_attribute(self, tag, name, value):\n        \"\"\" add an attribute (nam, value pair) to the named tag \"\"\"\n        self.add_tag(tag)\n        d = self._tags[tag]\n        d[name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn attribute by tag and attribute name", "response": "def attribute(self, tag, name):\n        \"\"\" return attribute by tag and attribute name \"\"\"\n        if tag in self._tags and name in self._tags[tag]:\n            return self._tags[tag][name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if type is a valid type", "response": "def is_valid(self):\n        '''checks if type is a valid type'''\n        return (self.is_primitive and self.name) \\\n            or (self.is_complex and self.name) \\\n            or (self.is_list and self.nested) \\\n            or (self.is_map and self.nested) \\\n            or (self.is_model and self.nested)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve(self):\n        self.__is_resolved = True\n        if self.is_complex:\n            type = self.nested if self.nested else self\n            type.__reference = self.module.lookup(type.name)", "response": "resolve the type symbol from name by doing a lookup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup(self, name: str, fragment: str = None):\n        '''lookup a symbol by name. If symbol is not local\n        it will be looked up system wide'''\n        if name in self._contentMap:\n            symbol = self._contentMap[name]\n            if fragment:\n                return symbol._contentMap[fragment]\n            return symbol\n        return self.system.lookup(name)", "response": "lookup a symbol by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef qualified_name(self):\n        '''return the fully qualified name (`<module>.<interface>#<operation>`)'''\n        return '{0}.{1}#{2}'.format(self.module.name, self.interface.name, self.name)", "response": "return the fully qualified name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qualified_name(self):\n        '''return the fully qualified name (`<module>.<struct>#<field>`)'''\n        return '{0}.{1}#{2}'.format(self.module.name, self.struct.name, self.name)", "response": "return the fully qualified name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the fully qualified name", "response": "def qualified_name(self):\n        '''return the fully qualified name (`<module>.<enum>#<member>`)'''\n        return '{0}.{1}#{2}'.format(self.module.name, self.enum.name, self.name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef jsonify(symbol):\n    try:\n        # all symbols have a toJson method, try it\n        return json.dumps(symbol.toJson(), indent='  ')\n    except AttributeError:\n        pass\n    return json.dumps(symbol, indent='  ')", "response": "returns json format for symbol"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hash(symbol, hash_type='sha1'):\n    code = hashlib.new(hash_type)\n    code.update(str(symbol).encode('utf-8'))\n    return code.hexdigest()", "response": "create a hash code from symbol"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the given cmd as shell command", "response": "def sh(args, **kwargs):\n    \"\"\"\n    runs the given cmd as shell command\n    \"\"\"\n    if isinstance(args, str):\n        args = args.split()\n    if not args:\n        return\n    click.echo('$ {0}'.format(' '.join(args)))\n    try:\n        return subprocess.check_call(args, **kwargs)\n    except subprocess.CalledProcessError as exc:\n        click.secho('run error {}'.format(exc))\n    except OSError as exc:\n        click.secho('not found error {}'.format(exc))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a JavaDoc string and returns an object where each JavaDoc tag is a property of the object.", "response": "def parse_doc(s):\n    \"\"\" parse a comment in the format of JavaDoc and returns an object, where each JavaDoc tag\n        is a property of the object. \"\"\"\n    if not s:\n        return\n    doc = DocObject()\n    tag = None\n    s = s[3:-2]  # remove '/**' and '*/'\n    for line in s.splitlines():\n        line = line.lstrip(' *')  # strip a ' ' and '*' from start\n        if not line:\n            tag = None  # on empty line reset the tag information\n        elif line[0] == '@':\n            line = line[1:]\n            res = line.split(maxsplit=1)\n            if len(res) == 0:\n                continue\n            tag = res[0]\n            if len(res) == 1:\n                doc.add_tag(tag, True)\n            elif len(res) == 2:\n                value = res[1]\n                doc.add_tag(tag, value)\n        elif tag:  # append to previous matched tag\n            doc.add_tag(tag, line)\n        else: # append any loose lines to description\n            doc.add_tag('description', line)\n    return doc"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_permission(self, request, view):\n        url_username = request.parser_context.get('kwargs', {}).get('username', '')\n        if request.user.username.lower() != url_username.lower():\n            if request.user.is_staff:\n                return False  # staff gets 403\n            raise Http404()\n        return True", "response": "Returns true if the current request is by the user itself."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef db_type(self, connection):\n        conn_module = type(connection).__module__\n        if \"mysql\" in conn_module:\n            return \"bigint AUTO_INCREMENT\"\n        elif \"postgres\" in conn_module:\n            return \"bigserial\"\n        return super(BigAutoField, self).db_type(connection)", "response": "Returns the type of the field to insert into the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the completion value for the specified record. Parameters: * user (django.contrib.auth.models.User): The user for whom the completion is being submitted. * course_key (opaque_keys.edx.keys.CourseKey): The course in which the submitted block is found. * block_key (opaque_keys.edx.keys.UsageKey): The block that has had its completion changed. * completion (float in range [0.0, 1.0]): The fractional completion value of the block (0.0 = incomplete, 1.0 = complete). Return Value: (BlockCompletion, bool): A tuple comprising the created or updated BlockCompletion object and a boolean value indicating whether the object was newly created by this call. Raises: ValueError: If the wrong type is passed for one of the parameters. django.core.exceptions.ValidationError: If a float is passed that is not between 0.0 and 1.0. django.db.DatabaseError: If there was a problem getting, creating, or updating the BlockCompletion record in the database. This will also be a more specific error, as described here: https://docs.djangoproject.com/en/1.11/ref/exceptions/#database-exceptions. IntegrityError and OperationalError are relatively common subclasses.", "response": "def submit_completion(self, user, course_key, block_key, completion):\n        \"\"\"\n        Update the completion value for the specified record.\n\n        Parameters:\n            * user (django.contrib.auth.models.User): The user for whom the\n              completion is being submitted.\n            * course_key (opaque_keys.edx.keys.CourseKey): The course in\n              which the submitted block is found.\n            * block_key (opaque_keys.edx.keys.UsageKey): The block that has had\n              its completion changed.\n            * completion (float in range [0.0, 1.0]): The fractional completion\n              value of the block (0.0 = incomplete, 1.0 = complete).\n\n        Return Value:\n            (BlockCompletion, bool): A tuple comprising the created or updated\n            BlockCompletion object and a boolean value indicating whether the\n            object was newly created by this call.\n\n        Raises:\n\n            ValueError:\n                If the wrong type is passed for one of the parameters.\n\n            django.core.exceptions.ValidationError:\n                If a float is passed that is not between 0.0 and 1.0.\n\n            django.db.DatabaseError:\n                If there was a problem getting, creating, or updating the\n                BlockCompletion record in the database.\n\n                This will also be a more specific error, as described here:\n                https://docs.djangoproject.com/en/1.11/ref/exceptions/#database-exceptions.\n                IntegrityError and OperationalError are relatively common\n                subclasses.\n        \"\"\"\n\n        # Raise ValueError to match normal django semantics for wrong type of field.\n        if not isinstance(course_key, CourseKey):\n            raise ValueError(\n                \"course_key must be an instance of `opaque_keys.edx.keys.CourseKey`.  Got {}\".format(type(course_key))\n            )\n        try:\n            block_type = block_key.block_type\n        except AttributeError:\n            raise ValueError(\n                \"block_key must be an instance of `opaque_keys.edx.keys.UsageKey`.  Got {}\".format(type(block_key))\n            )\n        if waffle.waffle().is_enabled(waffle.ENABLE_COMPLETION_TRACKING):\n            try:\n                with transaction.atomic():\n                    obj, is_new = self.get_or_create(  # pylint: disable=unpacking-non-sequence\n                        user=user,\n                        course_key=course_key,\n                        block_key=block_key,\n                        defaults={\n                            'completion': completion,\n                            'block_type': block_type,\n                        },\n                    )\n            except IntegrityError:\n                # The completion was created concurrently by another process\n                log.info(\n                    \"An IntegrityError was raised when trying to create a BlockCompletion for %s:%s:%s.  \"\n                    \"Falling back to get().\",\n                    user,\n                    course_key,\n                    block_key,\n                )\n                obj = self.get(\n                    user=user,\n                    course_key=course_key,\n                    block_key=block_key,\n                )\n                is_new = False\n            if not is_new and obj.completion != completion:\n                obj.completion = completion\n                obj.full_clean()\n                obj.save(update_fields={'completion', 'modified'})\n        else:\n            # If the feature is not enabled, this method should not be called.\n            # Error out with a RuntimeError.\n            raise RuntimeError(\n                \"BlockCompletion.objects.submit_completion should not be \\\n                called when the feature is disabled.\"\n            )\n        return obj, is_new"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsubmit a batch of completion objects for the given user and course.", "response": "def submit_batch_completion(self, user, course_key, blocks):\n        \"\"\"\n        Performs a batch insertion of completion objects.\n\n        Parameters:\n            * user (django.contrib.auth.models.User): The user for whom the\n              completions are being submitted.\n            * course_key (opaque_keys.edx.keys.CourseKey): The course in\n              which the submitted blocks are found.\n            * blocks: A list of tuples of UsageKey to float completion values.\n              (float in range [0.0, 1.0]): The fractional completion\n              value of the block (0.0 = incomplete, 1.0 = complete).\n\n        Return Value:\n            Dict of (BlockCompletion, bool): A dictionary with a\n            BlockCompletion object key and a value of bool. The boolean value\n            indicates whether the object was newly created by this call.\n\n        Raises:\n\n            ValueError:\n                If the wrong type is passed for one of the parameters.\n\n            django.core.exceptions.ValidationError:\n                If a float is passed that is not between 0.0 and 1.0.\n\n            django.db.DatabaseError:\n                If there was a problem getting, creating, or updating the\n                BlockCompletion record in the database.\n        \"\"\"\n        block_completions = {}\n        for block, completion in blocks:\n            (block_completion, is_new) = self.submit_completion(user, course_key, block, completion)\n            block_completions[block_completion] = is_new\n        return block_completions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the full block key value with the run filled in.", "response": "def full_block_key(self):\n        \"\"\"\n        Returns the \"correct\" usage key value with the run filled in.\n        \"\"\"\n        if self.block_key.run is None:\n            # pylint: disable=unexpected-keyword-arg, no-value-for-parameter\n            return self.block_key.replace(course_key=self.course_key)\n        return self.block_key"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_course_completions(cls, user, course_key):\n        user_course_completions = cls.user_course_completion_queryset(user, course_key)\n        return cls.completion_by_block_key(user_course_completions)", "response": "Returns a dictionary mapping BlockKeys to completion values for the given user and course_key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Queryset of completions for a given user and course_key.", "response": "def user_course_completion_queryset(cls, user, course_key):\n        \"\"\"\n        Returns a Queryset of completions for a given user and course_key.\n        \"\"\"\n        return cls.objects.filter(user=user, course_key=course_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates the batch object to make sure it is in the proper format.", "response": "def _validate_and_parse(self, batch_object):\n        \"\"\"\n        Performs validation on the batch object to make sure it is in the proper format.\n\n        Parameters:\n            * batch_object: The data provided to a POST. The expected format is the following:\n            {\n                \"username\": \"username\",\n                \"course_key\": \"course-key\",\n                \"blocks\": {\n                    \"block_key1\": 0.0,\n                    \"block_key2\": 1.0,\n                    \"block_key3\": 1.0,\n                }\n            }\n\n\n        Return Value:\n            * tuple: (User, CourseKey, List of tuples (UsageKey, completion_float)\n\n        Raises:\n\n            django.core.exceptions.ValidationError:\n                If any aspect of validation fails a ValidationError is raised.\n\n            ObjectDoesNotExist:\n                If a database object cannot be found an ObjectDoesNotExist is raised.\n        \"\"\"\n        if not waffle.waffle().is_enabled(waffle.ENABLE_COMPLETION_TRACKING):\n            raise ValidationError(\n                _(\"BlockCompletion.objects.submit_batch_completion should not be called when the feature is disabled.\")\n            )\n\n        for key in self.REQUIRED_KEYS:\n            if key not in batch_object:\n                raise ValidationError(_(\"Key '{key}' not found.\").format(key=key))\n\n        username = batch_object['username']\n        user = User.objects.get(username=username)\n\n        course_key_obj = self._validate_and_parse_course_key(batch_object['course_key'])\n\n        if not CourseEnrollment.is_enrolled(user, course_key_obj):\n            raise ValidationError(_('User is not enrolled in course.'))\n\n        blocks = batch_object['blocks']\n        block_objs = []\n        for block_key in blocks:\n            block_key_obj = self._validate_and_parse_block_key(block_key, course_key_obj)\n            completion = float(blocks[block_key])\n            block_objs.append((block_key_obj, completion))\n\n        return user, course_key_obj, block_objs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_and_parse_course_key(self, course_key):\n        try:\n            return CourseKey.from_string(course_key)\n        except InvalidKeyError:\n            raise ValidationError(_(\"Invalid course key: {}\").format(course_key))", "response": "Validate and parse a course key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_and_parse_block_key(self, block_key, course_key_obj):\n        try:\n            block_key_obj = UsageKey.from_string(block_key)\n        except InvalidKeyError:\n            raise ValidationError(_(\"Invalid block key: {}\").format(block_key))\n\n        if block_key_obj.run is None:\n            expected_matching_course_key = course_key_obj.replace(run=None)\n        else:\n            expected_matching_course_key = course_key_obj\n\n        if block_key_obj.course_key != expected_matching_course_key:\n            raise ValidationError(\n                _(\"Block with key: '{key}' is not in course {course}\").format(key=block_key, course=course_key_obj)\n            )\n\n        return block_key_obj", "response": "Validate and parse a given block key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(self, request, *args, **kwargs):  # pylint: disable=unused-argument\n        batch_object = request.data or {}\n        try:\n            user, course_key, blocks = self._validate_and_parse(batch_object)\n            BlockCompletion.objects.submit_batch_completion(user, course_key, blocks)\n        except ValidationError as exc:\n            return Response({\n                \"detail\": _(' ').join(text_type(msg) for msg in exc.messages),\n            }, status=status.HTTP_400_BAD_REQUEST)\n        except ValueError as exc:\n            return Response({\n                \"detail\": text_type(exc),\n            }, status=status.HTTP_400_BAD_REQUEST)\n        except ObjectDoesNotExist as exc:\n            return Response({\n                \"detail\": text_type(exc),\n            }, status=status.HTTP_404_NOT_FOUND)\n        except DatabaseError as exc:\n            return Response({\n                \"detail\": text_type(exc),\n            }, status=status.HTTP_500_INTERNAL_SERVER_ERROR)\n\n        return Response({\"detail\": _(\"ok\")}, status=status.HTTP_200_OK)", "response": "A POST method that inserts a batch of completions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the completion for a user in a course.", "response": "def get(self, request, username, course_key, subsection_id):\n        \"\"\"\n        Returns completion for a (user, subsection, course).\n        \"\"\"\n        def get_completion(course_completions, all_blocks, block_id):\n            \"\"\"\n            Recursively get the aggregate completion for a subsection,\n            given the subsection block and a list of all blocks.\n\n            Parameters:\n                course_completions: a dictionary of completion values by block IDs\n                all_blocks: a dictionary of the block structure for a subsection\n                block_id: an ID of a block for which to get completion\n            \"\"\"\n            block = all_blocks.get(block_id)\n            child_ids = block.get('children', [])\n            if not child_ids:\n                return course_completions.get(block.serializer.instance, 0)\n\n            completion = 0\n            total_children = 0\n            for child_id in child_ids:\n                completion += get_completion(course_completions, all_blocks, child_id)\n                total_children += 1\n\n            return int(completion == total_children)\n\n        user_id = User.objects.get(username=username).id\n        block_types_filter = [\n            'course',\n            'chapter',\n            'sequential',\n            'vertical',\n            'html',\n            'problem',\n            'video',\n            'discussion',\n            'drag-and-drop-v2'\n        ]\n\n        blocks = get_blocks(\n            request,\n            UsageKey.from_string(subsection_id),\n            nav_depth=2,\n            requested_fields=[\n                'children'\n            ],\n            block_types_filter=block_types_filter\n        )\n\n        course_completions = BlockCompletion.get_course_completions(user_id, CourseKey.from_string(course_key))\n        aggregated_completion = get_completion(course_completions, blocks['blocks'], blocks['root'])\n\n        return Response({\"completion\": aggregated_completion}, status=status.HTTP_200_OK)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_completions(self, candidates):\n        queryset = BlockCompletion.user_course_completion_queryset(self._user, self._course_key).filter(\n            block_key__in=candidates\n        )\n        completions = BlockCompletion.completion_by_block_key(queryset)\n        candidates_with_runs = [candidate.replace(course_key=self._course_key) for candidate in candidates]\n        for candidate in candidates_with_runs:\n            if candidate not in completions:\n                completions[candidate] = 0.0\n        return completions", "response": "Returns a dictionary mapping of the block keys to the present completion values of the associated blocks."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vertical_is_complete(self, item):\n        if item.location.block_type != 'vertical':\n            raise ValueError('The passed in xblock is not a vertical type!')\n\n        if not self.completion_tracking_enabled():\n            return None\n\n        # this is temporary local logic and will be removed when the whole course tree is included in completion\n        child_locations = [\n            child.location for child in item.get_children() if child.location.block_type != 'discussion'\n        ]\n        completions = self.get_completions(child_locations)\n        for child_location in child_locations:\n            if completions[child_location] < 1.0:\n                return False\n        return True", "response": "Calculates and returns whether a particular vertical is complete."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef can_mark_block_complete_on_view(self, block):\n        return (\n            XBlockCompletionMode.get_mode(block) == XBlockCompletionMode.COMPLETABLE\n            and not getattr(block, 'has_custom_completion', False)\n            and not getattr(block, 'has_score', False)\n        )", "response": "Returns True if the xblock can be marked complete on view."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef blocks_to_mark_complete_on_view(self, blocks):\n        blocks = {block for block in blocks if self.can_mark_block_complete_on_view(block)}\n        completions = self.get_completions({block.location for block in blocks})\n        return {block for block in blocks if completions.get(block.location, 0) < 1.0}", "response": "Returns a set of blocks which should be marked complete on view and haven t yet been marked."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit a completion for a group of users.", "response": "def submit_group_completion(self, block_key, completion, users=None, user_ids=None):\n        \"\"\"\n        Submit a completion for a group of users.\n\n        Arguments:\n\n            block_key (opaque_key.edx.keys.UsageKey): The block to submit completions for.\n            completion (float): A value in the range [0.0, 1.0]\n            users ([django.contrib.auth.models.User]): An optional iterable of Users that completed the block.\n            user_ids ([int]): An optional iterable of ids of Users that completed the block.\n\n        Returns a list of (BlockCompletion, bool) where the boolean indicates\n        whether the given BlockCompletion was newly created.\n        \"\"\"\n        if users is None:\n            users = []\n        if user_ids is None:\n            user_ids = []\n        more_users = User.objects.filter(id__in=user_ids)\n        if len(more_users) < len(user_ids):\n            found_ids = {u.id for u in more_users}\n            not_found_ids = [pk for pk in user_ids if pk not in found_ids]\n            raise User.DoesNotExist(\"User not found with id(s): {}\".format(not_found_ids))\n        users.extend(more_users)\n\n        submitted = []\n        for user in users:\n            submitted.append(BlockCompletion.objects.submit_completion(\n                user=user,\n                course_key=self._course_key,\n                block_key=block_key,\n                completion=completion\n            ))\n        return submitted"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef submit_completion(self, block_key, completion):\n        return BlockCompletion.objects.submit_completion(\n            user=self._user,\n            course_key=self._course_key,\n            block_key=block_key,\n            completion=completion\n        )", "response": "Submit a completion for the given block_key and return a new BlockCompletion object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the key to the last completed block in a course.", "response": "def get_key_to_last_completed_course_block(user, course_key):\n    \"\"\"\n    Returns the last block a \"user\" completed in a course (stated as \"course_key\").\n\n    raises UnavailableCompletionData when the user has not completed blocks in\n    the course.\n\n    raises UnavailableCompletionData when the visual progress waffle flag is\n    disabled.\n    \"\"\"\n\n    last_completed_block = BlockCompletion.get_latest_block_completed(user, course_key)\n\n    if last_completed_block is not None:\n        return last_completed_block.block_key\n\n    raise UnavailableCompletionData(course_key)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nthrowing an exception if the Cerberus response is not successful.", "response": "def throw_if_bad_response(response):\n    \"\"\"Throw an exception if the Cerberus response is not successful.\"\"\"\n    try:\n        response.raise_for_status()\n    except RequestException:\n        try:\n            msg = 'Response code: {}; response body:\\n{}'.format(response.status_code, json.dumps(response.json(), indent=2))\n            raise CerberusClientException(msg)\n        except ValueError:\n            msg = 'Response code: {}; response body:\\n{}'.format(response.status_code, response.text)\n            raise CerberusClientException(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a KeenClient instance using environment variables.", "response": "def _initialize_client_from_environment():\n    ''' Initialize a KeenClient instance using environment variables. '''\n    global _client, project_id, write_key, read_key, master_key, base_url\n\n    if _client is None:\n        # check environment for project ID and keys\n        project_id = project_id or os.environ.get(\"KEEN_PROJECT_ID\")\n        write_key = write_key or os.environ.get(\"KEEN_WRITE_KEY\")\n        read_key = read_key or os.environ.get(\"KEEN_READ_KEY\")\n        master_key = master_key or os.environ.get(\"KEEN_MASTER_KEY\")\n        base_url = base_url or os.environ.get(\"KEEN_BASE_URL\")\n\n        if not project_id:\n            raise InvalidEnvironmentError(\"Please set the KEEN_PROJECT_ID environment variable or set keen.project_id!\")\n\n        _client = KeenClient(project_id,\n                             write_key=write_key,\n                             read_key=read_key,\n                             master_key=master_key,\n                             base_url=base_url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding an event to the Keen", "response": "def add_event(event_collection, body, timestamp=None):\n    \"\"\" Adds an event.\n\n    Depending on the persistence strategy of the client,\n    this will either result in the event being uploaded to Keen\n    immediately or will result in saving the event to some local cache.\n\n    :param event_collection: the name of the collection to insert the\n    event to\n    :param body: dict, the body of the event to insert the event to\n    :param timestamp: datetime, optional, the timestamp of the event\n    \"\"\"\n    _initialize_client_from_environment()\n    _client.add_event(event_collection, body, timestamp=timestamp)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_image_beacon(event_collection, body, timestamp=None):\n    _initialize_client_from_environment()\n    return _client.generate_image_beacon(event_collection, body, timestamp=timestamp)", "response": "Generates an image beacon URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming a count query Counts the number of events that meet the given criteria. :param event_collection: string, the name of the collection to query :param timeframe: string or dict, the timeframe in which the events happened example: \"previous_7_days\" :param timezone: int, the timezone you'd like to use for the timeframe and interval in seconds :param interval: string, the time interval used for measuring data over time example: \"daily\" :param filters: array of dict, contains the filters you'd like to apply to the data example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}] :param group_by: string or array of strings, the name(s) of the properties you would like to group you results by. example: \"customer.id\" or [\"browser\",\"operating_system\"] :param order_by: dictionary or list of dictionary objects containing the property_name(s) to order by and the desired direction(s) of sorting. Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING} May not be used without a group_by specified. :param limit: positive integer limiting the displayed results of a query using order_by :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're willing to trade for increased query performance, in seconds", "response": "def count(event_collection, timeframe=None, timezone=None, interval=None, filters=None, group_by=None, order_by=None,\n          max_age=None, limit=None):\n    \"\"\" Performs a count query\n\n    Counts the number of events that meet the given criteria.\n\n    :param event_collection: string, the name of the collection to query\n    :param timeframe: string or dict, the timeframe in which the events\n    happened example: \"previous_7_days\"\n    :param timezone: int, the timezone you'd like to use for the timeframe\n    and interval in seconds\n    :param interval: string, the time interval used for measuring data over\n    time example: \"daily\"\n    :param filters: array of dict, contains the filters you'd like to apply to the data\n    example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n    :param group_by: string or array of strings, the name(s) of the properties you would\n    like to group you results by.  example: \"customer.id\" or [\"browser\",\"operating_system\"]\n    :param order_by: dictionary or list of dictionary objects containing the property_name(s)\n    to order by and the desired direction(s) of sorting.\n    Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING}\n    May not be used without a group_by specified.\n    :param limit: positive integer limiting the displayed results of a query using order_by\n    :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're\n    willing to trade for increased query performance, in seconds\n\n    \"\"\"\n    _initialize_client_from_environment()\n    return _client.count(event_collection=event_collection, timeframe=timeframe, timezone=timezone,\n                         interval=interval, filters=filters, group_by=group_by, order_by=order_by,\n                         max_age=max_age, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sum(event_collection, target_property, timeframe=None, timezone=None, interval=None, filters=None,\n        group_by=None, order_by=None, max_age=None, limit=None):\n    \"\"\" Performs a sum query\n\n    Adds the values of a target property for events that meet the given criteria.\n\n    :param event_collection: string, the name of the collection to query\n    :param target_property: string, the name of the event property you would like use\n    :param timeframe: string or dict, the timeframe in which the events\n    happened example: \"previous_7_days\"\n    :param timezone: int, the timezone you'd like to use for the timeframe\n    and interval in seconds\n    :param interval: string, the time interval used for measuring data over\n    time example: \"daily\"\n    :param filters: array of dict, contains the filters you'd like to apply to the data\n    example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n    :param group_by: string or array of strings, the name(s) of the properties you would\n    like to group you results by.  example: \"customer.id\" or [\"browser\",\"operating_system\"]\n    :param order_by: dictionary or list of dictionary objects containing the property_name(s)\n    to order by and the desired direction(s) of sorting.\n    Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING}\n    May not be used without a group_by specified.\n    :param limit: positive integer limiting the displayed results of a query using order_by\n    :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're\n    willing to trade for increased query performance, in seconds\n\n    \"\"\"\n    _initialize_client_from_environment()\n    return _client.sum(event_collection=event_collection, timeframe=timeframe, timezone=timezone,\n                       interval=interval, filters=filters, group_by=group_by, order_by=order_by,\n                       target_property=target_property, max_age=max_age, limit=limit)", "response": "This function performs a sum of the values of a target property for events in the given timeframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extraction(event_collection, timeframe=None, timezone=None, filters=None, latest=None, email=None,\n               property_names=None):\n    \"\"\" Performs a data extraction\n\n    Returns either a JSON object of events or a response\n     indicating an email will be sent to you with data.\n\n    :param event_collection: string, the name of the collection to query\n    :param timeframe: string or dict, the timeframe in which the events\n    happened example: \"previous_7_days\"\n    :param timezone: int, the timezone you'd like to use for the timeframe\n    and interval in seconds\n    :param filters: array of dict, contains the filters you'd like to apply to the data\n    example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n    :param latest: int, the number of most recent records you'd like to return\n    :param email: string, optional string containing an email address to email results to\n    :param property_names: string or list of strings, used to limit the properties returned\n\n    \"\"\"\n    _initialize_client_from_environment()\n    return _client.extraction(event_collection=event_collection, timeframe=timeframe, timezone=timezone,\n                              filters=filters, latest=latest, email=email, property_names=property_names)", "response": "This function performs a data extraction of a single object in a timeframe."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef multi_analysis(event_collection, analyses, timeframe=None, interval=None, timezone=None,\n                   filters=None, group_by=None, order_by=None, max_age=None, limit=None):\n    \"\"\" Performs a multi-analysis query\n\n    Returns a dictionary of analysis results.\n\n    :param event_collection: string, the name of the collection to query\n    :param analyses: dict, the types of analyses you'd like to run.  example:\n    {\"total money made\":{\"analysis_type\":\"sum\",\"target_property\":\"purchase.price\",\n    \"average price\":{\"analysis_type\":\"average\",\"target_property\":\"purchase.price\"}\n    :param timeframe: string or dict, the timeframe in which the events\n    happened example: \"previous_7_days\"\n    :param interval: string, the time interval used for measuring data over\n    time example: \"daily\"\n    :param timezone: int, the timezone you'd like to use for the timeframe\n    and interval in seconds\n    :param filters: array of dict, contains the filters you'd like to apply to the data\n    example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n    :param group_by: string or array of strings, the name(s) of the properties you would\n    like to group you results by.  example: \"customer.id\" or [\"browser\",\"operating_system\"]\n    :param order_by: dictionary or list of dictionary objects containing the property_name(s)\n    to order by and the desired direction(s) of sorting.\n    Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING}\n    May not be used without a group_by specified.\n    :param limit: positive integer limiting the displayed results of a query using order_by\n    :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're\n    willing to trade for increased query performance, in seconds\n\n    \"\"\"\n    _initialize_client_from_environment()\n    return _client.multi_analysis(event_collection=event_collection, timeframe=timeframe,\n                                  interval=interval, timezone=timezone, filters=filters,\n                                  group_by=group_by, order_by=order_by, analyses=analyses,\n                                  max_age=max_age, limit=limit)", "response": "Performs a multi - analysis on the object in the specified timeframe."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_access_key(name, is_active=True, permitted=[], options={}):\n    _initialize_client_from_environment()\n    return _client.create_access_key(name=name, is_active=is_active,\n                                     permitted=permitted, options=options)", "response": "Creates a new master key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_access_key_full(access_key_id, name, is_active, permitted, options):\n    _initialize_client_from_environment()\n    return _client.update_access_key_full(access_key_id, name, is_active, permitted, options)", "response": "Updates the name is_active permitted and options values for a given access key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap for opj_version library routine.", "response": "def version():\n    \"\"\"Wrapper for opj_version library routine.\"\"\"\n    try:\n        OPENJP2.opj_version.restype = ctypes.c_char_p\n    except:\n        return \"0.0.0\"\n\n    v = OPENJP2.opj_version()\n    return v.decode('utf-8') if sys.hexversion >= 0x03000000 else v"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_error(status):\n    global ERROR_MSG_LST\n    if status != 1:\n        if len(ERROR_MSG_LST) > 0:\n            # clear out the existing error message so that we don't pick up\n            # a bad one next time around.\n            msg = '\\n'.join(ERROR_MSG_LST)\n            ERROR_MSG_LST = []\n            raise OpenJPEGLibraryError(msg)\n        else:\n            raise OpenJPEGLibraryError(\"OpenJPEG function failure.\")", "response": "Check if an error occured in the OpenJPEG\n           ."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a J2K or JJP2 compress structure.", "response": "def create_compress(codec_format):\n    \"\"\"Creates a J2K/JP2 compress structure.\n\n    Wraps the openjp2 library function opj_create_compress.\n\n    Parameters\n    ----------\n    codec_format : int\n        Specifies codec to select.  Should be one of CODEC_J2K or CODEC_JP2.\n\n    Returns\n    -------\n    codec :  Reference to CODEC_TYPE instance.\n    \"\"\"\n    OPENJP2.opj_create_compress.restype = CODEC_TYPE\n    OPENJP2.opj_create_compress.argtypes = [CODEC_FORMAT_TYPE]\n\n    codec = OPENJP2.opj_create_compress(codec_format)\n    return codec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread an entire image and returns a tree of image data structures.", "response": "def decode(codec, stream, image):\n    \"\"\"Reads an entire image.\n\n    Wraps the openjp2 library function opj_decode.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        The JPEG2000 codec\n    stream : STREAM_TYPE_P\n        The stream to decode.\n    image : ImageType\n        Output image structure.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_decode fails.\n    \"\"\"\n    OPENJP2.opj_decode.argtypes = [CODEC_TYPE, STREAM_TYPE_P,\n                                   ctypes.POINTER(ImageType)]\n    OPENJP2.opj_decode.restype = check_error\n\n    OPENJP2.opj_decode(codec, stream, image)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef decode_tile_data(codec, tidx, data, data_size, stream):\n    OPENJP2.opj_decode_tile_data.argtypes = [CODEC_TYPE,\n                                             ctypes.c_uint32,\n                                             ctypes.POINTER(ctypes.c_uint8),\n                                             ctypes.c_uint32,\n                                             STREAM_TYPE_P]\n    OPENJP2.opj_decode_tile_data.restype = check_error\n\n    datap = data.ctypes.data_as(ctypes.POINTER(ctypes.c_uint8))\n    OPENJP2.opj_decode_tile_data(codec,\n                                 ctypes.c_uint32(tidx),\n                                 datap,\n                                 ctypes.c_uint32(data_size),\n                                 stream)", "response": "Reads the data of a single tile into a new set of pages."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_decompress(codec_format):\n    OPENJP2.opj_create_decompress.argtypes = [CODEC_FORMAT_TYPE]\n    OPENJP2.opj_create_decompress.restype = CODEC_TYPE\n\n    codec = OPENJP2.opj_create_decompress(codec_format)\n    return codec", "response": "Creates a J2K or JJP2 decompress structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef destroy_codec(codec):\n    OPENJP2.opj_destroy_codec.argtypes = [CODEC_TYPE]\n    OPENJP2.opj_destroy_codec.restype = ctypes.c_void_p\n    OPENJP2.opj_destroy_codec(codec)", "response": "Wraps the openjp2 library function opj_destroy_codec."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode(codec, stream):\n    OPENJP2.opj_encode.argtypes = [CODEC_TYPE, STREAM_TYPE_P]\n    OPENJP2.opj_encode.restype = check_error\n\n    OPENJP2.opj_encode(codec, stream)", "response": "Wraps openjp2 library routine opj_encode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the decoded tile from the codec", "response": "def get_decoded_tile(codec, stream, imagep, tile_index):\n    \"\"\"get the decoded tile from the codec\n\n    Wraps the openjp2 library function opj_get_decoded_tile.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        The jpeg2000 codec.\n    stream : STREAM_TYPE_P\n        The input stream.\n    image : ImageType\n        Output image structure.\n    tiler_index : int\n        Index of the tile which will be decoded.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_get_decoded_tile fails.\n    \"\"\"\n    OPENJP2.opj_get_decoded_tile.argtypes = [CODEC_TYPE,\n                                             STREAM_TYPE_P,\n                                             ctypes.POINTER(ImageType),\n                                             ctypes.c_uint32]\n    OPENJP2.opj_get_decoded_tile.restype = check_error\n\n    OPENJP2.opj_get_decoded_tile(codec, stream, imagep, tile_index)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef end_compress(codec, stream):\n    OPENJP2.opj_end_compress.argtypes = [CODEC_TYPE, STREAM_TYPE_P]\n    OPENJP2.opj_end_compress.restype = check_error\n    OPENJP2.opj_end_compress(codec, stream)", "response": "End of compressing the current image."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nends of decompressing the current image.", "response": "def end_decompress(codec, stream):\n    \"\"\"End of decompressing the current image.\n\n    Wraps the openjp2 library function opj_end_decompress.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        Compressor handle.\n    stream : STREAM_TYPE_P\n        Output stream buffer.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_end_decompress fails.\n    \"\"\"\n    OPENJP2.opj_end_decompress.argtypes = [CODEC_TYPE, STREAM_TYPE_P]\n    OPENJP2.opj_end_decompress.restype = check_error\n    OPENJP2.opj_end_decompress(codec, stream)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping for openjp2 library function opj_image_destroy.", "response": "def image_destroy(image):\n    \"\"\"Deallocate any resources associated with an image.\n\n    Wraps the openjp2 library function opj_image_destroy.\n\n    Parameters\n    ----------\n    image : ImageType pointer\n        Image resource to be disposed.\n    \"\"\"\n    OPENJP2.opj_image_destroy.argtypes = [ctypes.POINTER(ImageType)]\n    OPENJP2.opj_image_destroy.restype = ctypes.c_void_p\n\n    OPENJP2.opj_image_destroy(image)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping for openjp2 library function opj_image_create.", "response": "def image_create(comptparms, clrspc):\n    \"\"\"Creates a new image structure.\n\n    Wraps the openjp2 library function opj_image_create.\n\n    Parameters\n    ----------\n    cmptparms : comptparms_t\n        The component parameters.\n    clrspc : int\n        Specifies the color space.\n\n    Returns\n    -------\n    image : ImageType\n        Reference to ImageType instance.\n    \"\"\"\n    OPENJP2.opj_image_create.argtypes = [ctypes.c_uint32,\n                                         ctypes.POINTER(ImageComptParmType),\n                                         COLOR_SPACE_TYPE]\n    OPENJP2.opj_image_create.restype = ctypes.POINTER(ImageType)\n\n    image = OPENJP2.opj_image_create(len(comptparms),\n                                     comptparms,\n                                     clrspc)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for openjp2 library function opj_image_tile_create.", "response": "def image_tile_create(comptparms, clrspc):\n    \"\"\"Creates a new image structure.\n\n    Wraps the openjp2 library function opj_image_tile_create.\n\n    Parameters\n    ----------\n    cmptparms : comptparms_t\n        The component parameters.\n    clrspc : int\n        Specifies the color space.\n\n    Returns\n    -------\n    image : ImageType\n        Reference to ImageType instance.\n    \"\"\"\n    ARGTYPES = [ctypes.c_uint32,\n                ctypes.POINTER(ImageComptParmType),\n                COLOR_SPACE_TYPE]\n    OPENJP2.opj_image_tile_create.argtypes = ARGTYPES\n    OPENJP2.opj_image_tile_create.restype = ctypes.POINTER(ImageType)\n\n    image = OPENJP2.opj_image_tile_create(len(comptparms),\n                                          comptparms,\n                                          clrspc)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_header(stream, codec):\n    ARGTYPES = [STREAM_TYPE_P, CODEC_TYPE,\n                ctypes.POINTER(ctypes.POINTER(ImageType))]\n    OPENJP2.opj_read_header.argtypes = ARGTYPES\n    OPENJP2.opj_read_header.restype = check_error\n\n    imagep = ctypes.POINTER(ImageType)()\n    OPENJP2.opj_read_header(stream, codec, ctypes.byref(imagep))\n    return imagep", "response": "Decodes an image header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a tile header from the specified codec and stream.", "response": "def read_tile_header(codec, stream):\n    \"\"\"Reads a tile header.\n\n    Wraps the openjp2 library function opj_read_tile_header.\n\n    Parameters\n    ----------\n    codec : codec_t\n        The JPEG2000 codec to read.\n    stream : STREAM_TYPE_P\n        The JPEG2000 stream.\n\n    Returns\n    -------\n    tile_index : int\n        index of the tile being decoded\n    data_size : int\n        number of bytes for the decoded area\n    x0, y0 : int\n        upper left-most coordinate of tile\n    x1, y1 : int\n        lower right-most coordinate of tile\n    ncomps : int\n        number of components in the tile\n    go_on : bool\n        indicates that decoding should continue\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_read_tile_header fails.\n    \"\"\"\n    ARGTYPES = [CODEC_TYPE,\n                STREAM_TYPE_P,\n                ctypes.POINTER(ctypes.c_uint32),\n                ctypes.POINTER(ctypes.c_uint32),\n                ctypes.POINTER(ctypes.c_int32),\n                ctypes.POINTER(ctypes.c_int32),\n                ctypes.POINTER(ctypes.c_int32),\n                ctypes.POINTER(ctypes.c_int32),\n                ctypes.POINTER(ctypes.c_uint32),\n                ctypes.POINTER(BOOL_TYPE)]\n    OPENJP2.opj_read_tile_header.argtypes = ARGTYPES\n    OPENJP2.opj_read_tile_header.restype = check_error\n\n    tile_index = ctypes.c_uint32()\n    data_size = ctypes.c_uint32()\n    col0 = ctypes.c_int32()\n    row0 = ctypes.c_int32()\n    col1 = ctypes.c_int32()\n    row1 = ctypes.c_int32()\n    ncomps = ctypes.c_uint32()\n    go_on = BOOL_TYPE()\n    OPENJP2.opj_read_tile_header(codec,\n                                 stream,\n                                 ctypes.byref(tile_index),\n                                 ctypes.byref(data_size),\n                                 ctypes.byref(col0),\n                                 ctypes.byref(row0),\n                                 ctypes.byref(col1),\n                                 ctypes.byref(row1),\n                                 ctypes.byref(ncomps),\n                                 ctypes.byref(go_on))\n    go_on = bool(go_on.value)\n    return (tile_index.value,\n            data_size.value,\n            col0.value,\n            row0.value,\n            col1.value,\n            row1.value,\n            ncomps.value,\n            go_on)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps openjp2 library function opj_set_decode area.", "response": "def set_decode_area(codec, image, start_x=0, start_y=0, end_x=0, end_y=0):\n    \"\"\"Wraps openjp2 library function opj_set_decode area.\n\n    Sets the given area to be decoded.  This function should be called right\n    after read_header and before any tile header reading.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        Codec initialized by create_decompress function.\n    image : ImageType pointer\n        The decoded image previously set by read_header.\n    start_x, start_y : optional, int\n        The left and upper position of the rectangle to decode.\n    end_x, end_y : optional, int\n        The right and lower position of the rectangle to decode.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_set_decode_area fails.\n    \"\"\"\n    OPENJP2.opj_set_decode_area.argtypes = [CODEC_TYPE,\n                                            ctypes.POINTER(ImageType),\n                                            ctypes.c_int32,\n                                            ctypes.c_int32,\n                                            ctypes.c_int32,\n                                            ctypes.c_int32]\n    OPENJP2.opj_set_decode_area.restype = check_error\n\n    OPENJP2.opj_set_decode_area(codec, image,\n                                ctypes.c_int32(start_x),\n                                ctypes.c_int32(start_y),\n                                ctypes.c_int32(end_x),\n                                ctypes.c_int32(end_y))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_default_decoder_parameters():\n    ARGTYPES = [ctypes.POINTER(DecompressionParametersType)]\n    OPENJP2.opj_set_default_decoder_parameters.argtypes = ARGTYPES\n    OPENJP2.opj_set_default_decoder_parameters.restype = ctypes.c_void_p\n\n    dparams = DecompressionParametersType()\n    OPENJP2.opj_set_default_decoder_parameters(ctypes.byref(dparams))\n    return dparams", "response": "Wraps openjp2 library function opj_set_default_decoder_parameters. Opj_set_default_decoder_parameters returns a new object with decoding parameters set to default values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap openjp2 library function opj_set_default_encoder_parameters and returns the resulting value.", "response": "def set_default_encoder_parameters():\n    \"\"\"Wraps openjp2 library function opj_set_default_encoder_parameters.\n\n    Sets encoding parameters to default values.  That means\n\n        lossless\n        1 tile\n        size of precinct : 2^15 x 2^15 (means 1 precinct)\n        size of code-block : 64 x 64\n        number of resolutions: 6\n        no SOP marker in the codestream\n        no EPH marker in the codestream\n        no sub-sampling in x or y direction\n        no mode switch activated\n        progression order: LRCP\n        no index file\n        no ROI upshifted\n        no offset of the origin of the image\n        no offset of the origin of the tiles\n        reversible DWT 5-3\n\n    The signature for this function differs from its C library counterpart, as\n    the the C function pass-by-reference parameter becomes the Python return\n    value.\n\n    Returns\n    -------\n    cparameters : CompressionParametersType\n        Compression parameters.\n    \"\"\"\n    ARGTYPES = [ctypes.POINTER(CompressionParametersType)]\n    OPENJP2.opj_set_default_encoder_parameters.argtypes = ARGTYPES\n    OPENJP2.opj_set_default_encoder_parameters.restype = ctypes.c_void_p\n\n    cparams = CompressionParametersType()\n    OPENJP2.opj_set_default_encoder_parameters(ctypes.byref(cparams))\n    return cparams"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap openjp2 library routine opj_set_error_handler.", "response": "def set_error_handler(codec, handler, data=None):\n    \"\"\"Wraps openjp2 library function opj_set_error_handler.\n\n    Set the error handler use by openjpeg.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        Codec initialized by create_compress function.\n    handler : python function\n        The callback function to be used.\n    user_data : anything\n        User/client data.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_set_error_handler fails.\n    \"\"\"\n    OPENJP2.opj_set_error_handler.argtypes = [CODEC_TYPE,\n                                              ctypes.c_void_p,\n                                              ctypes.c_void_p]\n    OPENJP2.opj_set_error_handler.restype = check_error\n    OPENJP2.opj_set_error_handler(codec, handler, data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_info_handler(codec, handler, data=None):\n    OPENJP2.opj_set_info_handler.argtypes = [CODEC_TYPE,\n                                             ctypes.c_void_p,\n                                             ctypes.c_void_p]\n    OPENJP2.opj_set_info_handler.restype = check_error\n    OPENJP2.opj_set_info_handler(codec, handler, data)", "response": "Wraps openjp2 library routine opj_set_info_handler."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping openjp2 library routine opj_set_warning_handler.", "response": "def set_warning_handler(codec, handler, data=None):\n    \"\"\"Wraps openjp2 library function opj_set_warning_handler.\n\n    Set the warning handler use by openjpeg.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        Codec initialized by create_compress function.\n    handler : python function\n        The callback function to be used.\n    user_data : anything\n        User/client data.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_set_warning_handler fails.\n    \"\"\"\n    OPENJP2.opj_set_warning_handler.argtypes = [CODEC_TYPE,\n                                                ctypes.c_void_p,\n                                                ctypes.c_void_p]\n    OPENJP2.opj_set_warning_handler.restype = check_error\n\n    OPENJP2.opj_set_warning_handler(codec, handler, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap openjp2 library routine opj_setup_decoder.", "response": "def setup_decoder(codec, dparams):\n    \"\"\"Wraps openjp2 library function opj_setup_decoder.\n\n    Setup the decoder with decompression parameters.\n\n    Parameters\n    ----------\n    codec:  CODEC_TYPE\n        Codec initialized by create_compress function.\n    dparams:  DecompressionParametersType\n        Decompression parameters.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_setup_decoder fails.\n    \"\"\"\n    ARGTYPES = [CODEC_TYPE, ctypes.POINTER(DecompressionParametersType)]\n    OPENJP2.opj_setup_decoder.argtypes = ARGTYPES\n    OPENJP2.opj_setup_decoder.restype = check_error\n\n    OPENJP2.opj_setup_decoder(codec, ctypes.byref(dparams))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping openjp2 library routine opj_setup_encoder.", "response": "def setup_encoder(codec, cparams, image):\n    \"\"\"Wraps openjp2 library function opj_setup_encoder.\n\n    Setup the encoder parameters using the current image and using user\n    parameters.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        codec initialized by create_compress function\n    cparams : CompressionParametersType\n        compression parameters\n    image : ImageType\n        input-filled image\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_setup_encoder fails.\n    \"\"\"\n    ARGTYPES = [CODEC_TYPE,\n                ctypes.POINTER(CompressionParametersType),\n                ctypes.POINTER(ImageType)]\n    OPENJP2.opj_setup_encoder.argtypes = ARGTYPES\n    OPENJP2.opj_setup_encoder.restype = check_error\n    OPENJP2.opj_setup_encoder(codec, ctypes.byref(cparams), image)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps openjp2 library routine opj_start_compress.", "response": "def start_compress(codec, image, stream):\n    \"\"\"Wraps openjp2 library function opj_start_compress.\n\n    Start to compress the current image.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        Compressor handle.\n    image : pointer to ImageType\n        Input filled image.\n    stream : STREAM_TYPE_P\n        Input stream.\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_start_compress fails.\n    \"\"\"\n    OPENJP2.opj_start_compress.argtypes = [CODEC_TYPE,\n                                           ctypes.POINTER(ImageType),\n                                           STREAM_TYPE_P]\n    OPENJP2.opj_start_compress.restype = check_error\n\n    OPENJP2.opj_start_compress(codec, image, stream)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream_create_default_file_stream(fname, isa_read_stream):\n    ARGTYPES = [ctypes.c_char_p, ctypes.c_int32]\n    OPENJP2.opj_stream_create_default_file_stream.argtypes = ARGTYPES\n    OPENJP2.opj_stream_create_default_file_stream.restype = STREAM_TYPE_P\n    read_stream = 1 if isa_read_stream else 0\n    file_argument = ctypes.c_char_p(fname.encode())\n    stream = OPENJP2.opj_stream_create_default_file_stream(file_argument,\n                                                           read_stream)\n    return stream", "response": "Wraps openjp2 library function opj_stream_create_default_vile_stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps openjp2 library function opj_stream_destroy.", "response": "def stream_destroy(stream):\n    \"\"\"Wraps openjp2 library function opj_stream_destroy.\n\n    Destroys the stream created by create_stream.\n\n    Parameters\n    ----------\n    stream : STREAM_TYPE_P\n        The file stream.\n    \"\"\"\n    OPENJP2.opj_stream_destroy.argtypes = [STREAM_TYPE_P]\n    OPENJP2.opj_stream_destroy.restype = ctypes.c_void_p\n    OPENJP2.opj_stream_destroy(stream)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping openjp2 library function opj_write_tile.", "response": "def write_tile(codec, tile_index, data, data_size, stream):\n    \"\"\"Wraps openjp2 library function opj_write_tile.\n\n    Write a tile into an image.\n\n    Parameters\n    ----------\n    codec : CODEC_TYPE\n        The jpeg2000 codec\n    tile_index : int\n        The index of the tile to write, zero-indexing assumed\n    data : array\n        Image data arranged in usual C-order\n    data_size : int\n        Size of a tile in bytes\n    stream : STREAM_TYPE_P\n        The stream to write data to\n\n    Raises\n    ------\n    RuntimeError\n        If the OpenJPEG library routine opj_write_tile fails.\n    \"\"\"\n    OPENJP2.opj_write_tile.argtypes = [CODEC_TYPE,\n                                       ctypes.c_uint32,\n                                       ctypes.POINTER(ctypes.c_uint8),\n                                       ctypes.c_uint32,\n                                       STREAM_TYPE_P]\n    OPENJP2.opj_write_tile.restype = check_error\n\n    datap = data.ctypes.data_as(ctypes.POINTER(ctypes.c_uint8))\n    OPENJP2.opj_write_tile(codec,\n                           ctypes.c_uint32(int(tile_index)),\n                           datap,\n                           ctypes.c_uint32(int(data_size)),\n                           stream)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef paddingLength(str_len, blocksize=AES_blocksize):\n  ''' Function to calculate the length of the required padding\n  Input: (int) str_len   - String length of the text \n         (int) blocksize - block size of the algorithm    \n  Returns: (int) number of required pad bytes for string of size.'''\n  assert 0 < blocksize < 255, 'blocksize must be between 0 and 255'\n  assert str_len > 0 , 'string length should be non-negative'\n  \n  'If the last block is already full, append an extra block of padding'\n  pad_len = blocksize - (str_len % blocksize)\n  \n  return pad_len", "response": "Function to calculate the length of the required padding bytes for a given string of size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef removeCMSPadding(str, blocksize=AES_blocksize):\n  '''CMS padding: Remove padding with bytes containing the number of padding bytes '''\n  try:\n    pad_len = ord(str[-1]) # last byte contains number of padding bytes\n  except TypeError:\n    pad_len = str[-1]\n  assert pad_len <= blocksize, 'padding error' \n  assert pad_len <= len(str), 'padding error'\n    \n  return str[:-pad_len]", "response": "CMS padding : Remove padding with bytes containing the number of padding bytes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending padding to the end of a string according to ANSI X. 923", "response": "def appendBitPadding(str, blocksize=AES_blocksize):\n  '''Bit padding a.k.a. One and Zeroes Padding\n  A single set ('1') bit is added to the message and then as many reset ('0') bits as required (possibly none) are added.\n  Input: (str) str - String to be padded\n         (int) blocksize - block size of the algorithm\n  Return: Padded string according to ANSI X.923 standart\n\n  Used in when padding bit strings.\n  0x80 in binary is 10000000\n  0x00 in binary is 00000000\n\n  Defined in ANSI X.923 (based on NIST Special Publication 800-38A) and ISO/IEC 9797-1 as Padding Method 2.\n  Used in hash functions MD5 and SHA, described in RFC 1321 step 3.1.\n  '''\n  pad_len = paddingLength(len(str), blocksize) - 1\n   \n  padding = chr(0x80)+'\\0'*pad_len\n  \n  return str + padding"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove bit padding with 0x80 followed by zero bytes", "response": "def removeBitPadding(str, blocksize=AES_blocksize):\n    'Remove bit padding with 0x80 followed by zero (null) bytes'\n    pad_len = 0    \n    for char in str[::-1]: # str[::-1] reverses string\n        if char == '\\0':\n            pad_len += 1\n        else:\n            break\n    pad_len += 1\n    str = str[:-pad_len]\n   \n    return str"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove Padding with zeroes + last byte equal to the number of padding bytes", "response": "def removeZeroLenPadding(str, blocksize=AES_blocksize):\n  'Remove Padding with zeroes + last byte equal to the number of padding bytes'\n  try:\n    pad_len = ord(str[-1]) # last byte contains number of padding bytes\n  except TypeError:\n    pad_len = str[-1]\n  assert pad_len < blocksize, 'padding error' \n  assert pad_len < len(str), 'padding error'\n    \n  return str[:-pad_len]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npads with null bytes", "response": "def appendNullPadding(str, blocksize=AES_blocksize):\n  'Pad with null bytes'\n  pad_len = paddingLength(len(str), blocksize)\n\n  padding = '\\0'*pad_len\n  \n  return str + padding"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef removeNullPadding(str, blocksize=AES_blocksize):\n    'Remove padding with null bytes'\n    pad_len = 0    \n    for char in str[::-1]: # str[::-1] reverses string\n        if char == '\\0':\n            pad_len += 1\n        else:\n            break\n    str = str[:-pad_len]\n    return str", "response": "Remove padding with null bytes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef removeSpacePadding(str, blocksize=AES_blocksize):\n    'Remove padding with spaces' \n    pad_len = 0    \n    for char in str[::-1]: # str[::-1] reverses string\n        if char == ' ':\n            pad_len += 1\n        else:\n            break\n\n    str = str[:-pad_len]\n   \n    return str", "response": "Remove padding with spaces"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef appendRandomLenPadding(str, blocksize=AES_blocksize):\n  'ISO 10126 Padding (withdrawn, 2007): Pad with random bytes + last byte equal to the number of padding bytes'\n  pad_len = paddingLength(len(str), blocksize) - 1\n  \n  from os import urandom\n  \n  padding = urandom(pad_len)+chr(pad_len)\n  \n  return str + padding", "response": "ISO 10126 Padding ( withdrawn 2007 ) Pad with random bytes + last byte equal to the number of padding bytes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove random padding from the end of the string", "response": "def removeRandomLenPadding(str, blocksize=AES_blocksize):\n  'ISO 10126 Padding (withdrawn, 2007): Remove Padding with random bytes + last byte equal to the number of padding bytes'\n  pad_len = ord(str[-1]) # last byte contains number of padding bytes\n  assert pad_len < blocksize, 'padding error' \n  assert pad_len < len(str), 'padding error'\n    \n  return str[:-pad_len]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef appendPadding(str, blocksize=AES_blocksize, mode='CMS'):\n  ''' Pad (append padding to) string for use with symmetric encryption algorithm\n    Input: (string) str - String to be padded\n           (int)    blocksize - block size of the encryption algorithm\n           (string) mode - padding scheme one in (CMS, Bit, ZeroLen, Null, Space, Random)\n    Return:(string) Padded string according to chosen padding mode\n  '''\n  if mode not in (0,'CMS'):     \n    for k in MODES.keys():\n        if mode in k:\n            return globals()['append'+k[1]+'Padding'](str, blocksize) \n        else:\n            return appendCMSPadding(str, blocksize)\n  else:\n      return appendCMSPadding(str, blocksize)", "response": "Pads a string to be used with symmetric encryption algorithm\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving padding from string", "response": "def removePadding(str, blocksize=AES_blocksize, mode='CMS'):\n  ''' Remove padding from string \n  Input: (str) str - String to be padded\n         (int) blocksize - block size of the algorithm\n         (string) mode - padding scheme one in (CMS, Bit, ZeroLen, Null, Space, Random)\n  Return:(string) Decrypted string without padding \n    '''    \n  if mode not in (0,'CMS'):     \n    for k in MODES.keys():\n        if mode in k:\n            return globals()['append'+k[1]+'Padding'](str, blocksize)\n        else:\n            return removeCMSPadding(str, blocksize)\n  else:\n      return removeCMSPadding(str, blocksize)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _default_error_handler(msg, _):\n    msg = \"OpenJPEG library error:  {0}\".format(msg.decode('utf-8').rstrip())\n    opj2.set_error_message(msg)", "response": "Default error handler callback for libopenjp2."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _default_warning_handler(library_msg, _):\n    library_msg = library_msg.decode('utf-8').rstrip()\n    msg = \"OpenJPEG library warning:  {0}\".format(library_msg)\n    warnings.warn(msg, UserWarning)", "response": "Default warning handler callback."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self):\n        self.length = os.path.getsize(self.filename)\n\n        with open(self.filename, 'rb') as fptr:\n\n            # Make sure we have a JPEG2000 file.  It could be either JP2 or\n            # J2C.  Check for J2C first, single box in that case.\n            read_buffer = fptr.read(2)\n            signature, = struct.unpack('>H', read_buffer)\n            if signature == 0xff4f:\n                self._codec_format = opj2.CODEC_J2K\n                # That's it, we're done.  The codestream object is only\n                # produced upon explicit request.\n                return\n\n            self._codec_format = opj2.CODEC_JP2\n\n            # Should be JP2.\n            # First 4 bytes should be 12, the length of the 'jP  ' box.\n            # 2nd 4 bytes should be the box ID ('jP  ').\n            # 3rd 4 bytes should be the box signature (13, 10, 135, 10).\n            fptr.seek(0)\n            read_buffer = fptr.read(12)\n            values = struct.unpack('>I4s4B', read_buffer)\n            box_length = values[0]\n            box_id = values[1]\n            signature = values[2:]\n            if (((box_length != 12) or (box_id != b'jP  ') or\n                 (signature != (13, 10, 135, 10)))):\n                msg = '{filename} is not a JPEG 2000 file.'\n                msg = msg.format(filename=self.filename)\n                raise IOError(msg)\n\n            # Back up and start again, we know we have a superbox (box of\n            # boxes) here.\n            fptr.seek(0)\n            self.box = self.parse_superbox(fptr)\n            self._validate()", "response": "Parses the JPEG 2000 file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the JPEG 2000 outermost superbox.", "response": "def _validate(self):\n        \"\"\"Validate the JPEG 2000 outermost superbox.  These checks must be\n        done at a file level.\n        \"\"\"\n        # A JP2 file must contain certain boxes.  The 2nd box must be a file\n        # type box.\n        if not isinstance(self.box[1], FileTypeBox):\n            msg = \"{filename} does not contain a valid File Type box.\"\n            msg = msg.format(filename=self.filename)\n            raise IOError(msg)\n\n        # A jp2-branded file cannot contain an \"any ICC profile\n        ftyp = self.box[1]\n        if ftyp.brand == 'jp2 ':\n            jp2h = [box for box in self.box if box.box_id == 'jp2h'][0]\n            colrs = [box for box in jp2h.box if box.box_id == 'colr']\n            for colr in colrs:\n                if colr.method not in (core.ENUMERATED_COLORSPACE,\n                                       core.RESTRICTED_ICC_PROFILE):\n                    msg = (\"Color Specification box method must specify \"\n                           \"either an enumerated colorspace or a restricted \"\n                           \"ICC profile if the file type box brand is 'jp2 '.\")\n                    warnings.warn(msg, UserWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_cinema_params(self, cinema_mode, fps):\n        if re.match(\"1.5|2.0\", version.openjpeg_version) is not None:\n            msg = (\"Writing Cinema2K or Cinema4K files is not supported with \"\n                   \"OpenJPEG library versions less than 2.1.0.  The installed \"\n                   \"version of OpenJPEG is {version}.\")\n            msg = msg.format(version=version.openjpeg_version)\n            raise IOError(msg)\n\n        # Cinema modes imply MCT.\n        self._cparams.tcp_mct = 1\n\n        if cinema_mode == 'cinema2k':\n            if fps not in [24, 48]:\n                msg = 'Cinema2K frame rate must be either 24 or 48.'\n                raise IOError(msg)\n\n            if fps == 24:\n                self._cparams.rsiz = core.OPJ_PROFILE_CINEMA_2K\n                self._cparams.max_comp_size = core.OPJ_CINEMA_24_COMP\n                self._cparams.max_cs_size = core.OPJ_CINEMA_24_CS\n            else:\n                self._cparams.rsiz = core.OPJ_PROFILE_CINEMA_2K\n                self._cparams.max_comp_size = core.OPJ_CINEMA_48_COMP\n                self._cparams.max_cs_size = core.OPJ_CINEMA_48_CS\n\n        else:\n            # cinema4k\n            self._cparams.rsiz = core.OPJ_PROFILE_CINEMA_4K", "response": "Populate the internal structures for the cinema2k and cinema4k compression parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate_cparams(self, img_array, mct=None, cratios=None, psnr=None,\n                          cinema2k=None, cinema4k=None, irreversible=None,\n                          cbsize=None, eph=None, grid_offset=None, modesw=None,\n                          numres=None, prog=None, psizes=None, sop=None,\n                          subsam=None, tilesize=None, colorspace=None):\n        \"\"\"Directs processing of write method arguments.\n\n        Parameters\n        ----------\n        img_array : ndarray\n            Image data to be written to file.\n        kwargs : dictionary\n            Non-image keyword inputs provided to write method.\n        \"\"\"\n        other_args = (mct, cratios, psnr, irreversible, cbsize, eph,\n                      grid_offset, modesw, numres, prog, psizes, sop, subsam)\n        if (((cinema2k is not None or cinema4k is not None) and\n             (not all([arg is None for arg in other_args])))):\n            msg = (\"Cannot specify cinema2k/cinema4k along with any other \"\n                   \"options.\")\n            raise IOError(msg)\n\n        if cratios is not None and psnr is not None:\n            msg = \"Cannot specify cratios and psnr options together.\"\n            raise IOError(msg)\n\n        if version.openjpeg_version_tuple[0] == 1:\n            cparams = opj.set_default_encoder_parameters()\n        else:\n            cparams = opj2.set_default_encoder_parameters()\n\n        outfile = self.filename.encode()\n        num_pad_bytes = opj2.PATH_LEN - len(outfile)\n        outfile += b'0' * num_pad_bytes\n        cparams.outfile = outfile\n\n        if self.filename[-4:].endswith(('.jp2', '.JP2')):\n            cparams.codec_fmt = opj2.CODEC_JP2\n        else:\n            cparams.codec_fmt = opj2.CODEC_J2K\n\n        # Set defaults to lossless to begin.\n        cparams.tcp_rates[0] = 0\n        cparams.tcp_numlayers = 1\n        cparams.cp_disto_alloc = 1\n\n        cparams.irreversible = 1 if irreversible else 0\n\n        if cinema2k is not None:\n            self._cparams = cparams\n            self._set_cinema_params('cinema2k', cinema2k)\n            return\n\n        if cinema4k is not None:\n            self._cparams = cparams\n            self._set_cinema_params('cinema4k', cinema4k)\n            return\n\n        if cbsize is not None:\n            cparams.cblockw_init = cbsize[1]\n            cparams.cblockh_init = cbsize[0]\n\n        if cratios is not None:\n            cparams.tcp_numlayers = len(cratios)\n            for j, cratio in enumerate(cratios):\n                cparams.tcp_rates[j] = cratio\n            cparams.cp_disto_alloc = 1\n\n        cparams.csty |= 0x02 if sop else 0\n        cparams.csty |= 0x04 if eph else 0\n\n        if grid_offset is not None:\n            cparams.image_offset_x0 = grid_offset[1]\n            cparams.image_offset_y0 = grid_offset[0]\n\n        if modesw is not None:\n            for shift in range(6):\n                power_of_two = 1 << shift\n                if modesw & power_of_two:\n                    cparams.mode |= power_of_two\n\n        if numres is not None:\n            cparams.numresolution = numres\n\n        if prog is not None:\n            cparams.prog_order = core.PROGRESSION_ORDER[prog.upper()]\n\n        if psnr is not None:\n            cparams.tcp_numlayers = len(psnr)\n            for j, snr_layer in enumerate(psnr):\n                cparams.tcp_distoratio[j] = snr_layer\n            cparams.cp_fixed_quality = 1\n\n        if psizes is not None:\n            for j, (prch, prcw) in enumerate(psizes):\n                cparams.prcw_init[j] = prcw\n                cparams.prch_init[j] = prch\n            cparams.csty |= 0x01\n            cparams.res_spec = len(psizes)\n\n        if subsam is not None:\n            cparams.subsampling_dy = subsam[0]\n            cparams.subsampling_dx = subsam[1]\n\n        if tilesize is not None:\n            cparams.cp_tdx = tilesize[1]\n            cparams.cp_tdy = tilesize[0]\n            cparams.tile_size_on = opj2.TRUE\n\n        if mct is None:\n            # If the multi component transform was not specified, we infer\n            # that it should be used if the color space is RGB.\n            cparams.tcp_mct = 1 if self._colorspace == opj2.CLRSPC_SRGB else 0\n        else:\n            if self._colorspace == opj2.CLRSPC_GRAY:\n                msg = (\"Cannot specify usage of the multi component transform \"\n                       \"if the colorspace is gray.\")\n                raise IOError(msg)\n            cparams.tcp_mct = 1 if mct else 0\n\n        self._validate_compression_params(img_array, cparams, colorspace)\n\n        self._cparams = cparams", "response": "Populate the cparams object with the appropriate values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites image data to a JP2 JPX J2k file.", "response": "def _write(self, img_array, verbose=False, **kwargs):\n        \"\"\"Write image data to a JP2/JPX/J2k file.  Intended usage of the\n        various parameters follows that of OpenJPEG's opj_compress utility.\n\n        This method can only be used to create JPEG 2000 images that can fit\n        in memory.\n        \"\"\"\n        if re.match(\"0|1.[0-4]\", version.openjpeg_version) is not None:\n            msg = (\"You must have at least version 1.5 of OpenJPEG \"\n                   \"in order to write images.\")\n            raise RuntimeError(msg)\n\n        self._determine_colorspace(**kwargs)\n        self._populate_cparams(img_array, **kwargs)\n\n        if opj2.OPENJP2 is not None:\n            self._write_openjp2(img_array, verbose=verbose)\n        else:\n            self._write_openjpeg(img_array, verbose=verbose)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _write_openjpeg(self, img_array, verbose=False):\n        if img_array.ndim == 2:\n            # Force the image to be 3D.  Just makes things easier later on.\n            img_array = img_array.reshape(img_array.shape[0],\n                                          img_array.shape[1],\n                                          1)\n\n        self._populate_comptparms(img_array)\n\n        with ExitStack() as stack:\n            image = opj.image_create(self._comptparms, self._colorspace)\n            stack.callback(opj.image_destroy, image)\n\n            numrows, numcols, numlayers = img_array.shape\n\n            # set image offset and reference grid\n            image.contents.x0 = self._cparams.image_offset_x0\n            image.contents.y0 = self._cparams.image_offset_y0\n            image.contents.x1 = (image.contents.x0 +\n                                 (numcols - 1) * self._cparams.subsampling_dx +\n                                 1)\n            image.contents.y1 = (image.contents.y0 +\n                                 (numrows - 1) * self._cparams.subsampling_dy +\n                                 1)\n\n            # Stage the image data to the openjpeg data structure.\n            for k in range(0, numlayers):\n                layer = np.ascontiguousarray(img_array[:, :, k],\n                                             dtype=np.int32)\n                dest = image.contents.comps[k].data\n                src = layer.ctypes.data\n                ctypes.memmove(dest, src, layer.nbytes)\n\n            cinfo = opj.create_compress(self._cparams.codec_fmt)\n            stack.callback(opj.destroy_compress, cinfo)\n\n            # Setup the info, warning, and error handlers.\n            # Always use the warning and error handler.  Use of an info\n            # handler is optional.\n            event_mgr = opj.EventMgrType()\n            _info_handler = _INFO_CALLBACK if verbose else None\n            event_mgr.info_handler = _info_handler\n            event_mgr.warning_handler = ctypes.cast(_WARNING_CALLBACK,\n                                                    ctypes.c_void_p)\n            event_mgr.error_handler = ctypes.cast(_ERROR_CALLBACK,\n                                                  ctypes.c_void_p)\n\n            opj.setup_encoder(cinfo, ctypes.byref(self._cparams), image)\n\n            cio = opj.cio_open(cinfo)\n            stack.callback(opj.cio_close, cio)\n\n            if not opj.encode(cinfo, cio, image):\n                raise IOError(\"Encode error.\")\n\n            pos = opj.cio_tell(cio)\n\n            blob = ctypes.string_at(cio.contents.buffer, pos)\n            fptr = open(self.filename, 'wb')\n            stack.callback(fptr.close)\n            fptr.write(blob)\n\n        self.parse()", "response": "Write JPEG 2000 file using OpenJPEG 1. 5 interface."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_j2k_colorspace(self, cparams, colorspace):\n        if cparams.codec_fmt == opj2.CODEC_J2K and colorspace is not None:\n            msg = 'Do not specify a colorspace when writing a raw codestream.'\n            raise IOError(msg)", "response": "Validate that the colorspace is valid for J2K."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate code block size.", "response": "def _validate_codeblock_size(self, cparams):\n        \"\"\"\n        Code block dimensions must satisfy certain restrictions.\n\n        They must both be a power of 2 and the total area defined by the width\n        and height cannot be either too great or too small for the codec.\n        \"\"\"\n        if cparams.cblockw_init != 0 and cparams.cblockh_init != 0:\n            # These fields ARE zero if uninitialized.\n            width = cparams.cblockw_init\n            height = cparams.cblockh_init\n            if height * width > 4096 or height < 4 or width < 4:\n                msg = (\"The code block area is specified as \"\n                       \"{height} x {width} = {area} square pixels.  \"\n                       \"Code block area cannot exceed 4096 square pixels.  \"\n                       \"Code block height and width dimensions must be larger \"\n                       \"than 4 pixels.\")\n                msg = msg.format(height=height, width=width,\n                                 area=height * width)\n                raise IOError(msg)\n            if ((math.log(height, 2) != math.floor(math.log(height, 2)) or\n                 math.log(width, 2) != math.floor(math.log(width, 2)))):\n                msg = (\"Bad code block size ({height} x {width}).  \"\n                       \"The dimensions must be powers of 2.\")\n                msg = msg.format(height=height, width=width)\n                raise IOError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the precinct size of the current class.", "response": "def _validate_precinct_size(self, cparams):\n        \"\"\"\n        Precinct dimensions must satisfy certain restrictions if specified.\n\n        They must both be a power of 2 and must both be at least twice the\n        size of their codeblock size counterparts.\n        \"\"\"\n        code_block_specified = False\n        if cparams.cblockw_init != 0 and cparams.cblockh_init != 0:\n            code_block_specified = True\n\n        if cparams.res_spec != 0:\n            # precinct size was not specified if this field is zero.\n            for j in range(cparams.res_spec):\n                prch = cparams.prch_init[j]\n                prcw = cparams.prcw_init[j]\n                if j == 0 and code_block_specified:\n                    height, width = cparams.cblockh_init, cparams.cblockw_init\n                    if prch < height * 2 or prcw < width * 2:\n                        msg = (\"The highest resolution precinct size \"\n                               \"({prch} x {prcw}) must be at least twice that \"\n                               \"of the code block size \"\n                               \"({cbh} x {cbw}).\")\n                        msg = msg.format(prch=prch, prcw=prcw,\n                                         cbh=height, cbw=width)\n                        raise IOError(msg)\n                if ((math.log(prch, 2) != math.floor(math.log(prch, 2)) or\n                     math.log(prcw, 2) != math.floor(math.log(prcw, 2)))):\n                    msg = (\"Bad precinct size ({height} x {width}).  \"\n                           \"Precinct dimensions must be powers of 2.\")\n                    msg = msg.format(height=prch, width=prcw)\n                    raise IOError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_image_rank(self, img_array):\n        if img_array.ndim == 1 or img_array.ndim > 3:\n            msg = \"{0}D imagery is not allowed.\".format(img_array.ndim)\n            raise IOError(msg)", "response": "Images must be either 2D or 3D."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_image_datatype(self, img_array):\n        if img_array.dtype != np.uint8 and img_array.dtype != np.uint16:\n            msg = (\"Only uint8 and uint16 datatypes are currently supported \"\n                   \"when writing.\")\n            raise RuntimeError(msg)", "response": "Validate the image datatype."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_compression_params(self, img_array, cparams, colorspace):\n        self._validate_j2k_colorspace(cparams, colorspace)\n        self._validate_codeblock_size(cparams)\n        self._validate_precinct_size(cparams)\n        self._validate_image_rank(img_array)\n        self._validate_image_datatype(img_array)", "response": "Check that the compression parameters are valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _determine_colorspace(self, colorspace=None, **kwargs):\n        if colorspace is None:\n            # Must infer the colorspace from the image dimensions.\n            if len(self.shape) < 3:\n                # A single channel image is grayscale.\n                self._colorspace = opj2.CLRSPC_GRAY\n            elif self.shape[2] == 1 or self.shape[2] == 2:\n                # A single channel image or an image with two channels is going\n                # to be greyscale.\n                self._colorspace = opj2.CLRSPC_GRAY\n            else:\n                # Anything else must be RGB, right?\n                self._colorspace = opj2.CLRSPC_SRGB\n        else:\n            if colorspace.lower() not in ('rgb', 'grey', 'gray'):\n                msg = 'Invalid colorspace \"{0}\".'.format(colorspace)\n                raise IOError(msg)\n            elif colorspace.lower() == 'rgb' and self.shape[2] < 3:\n                msg = 'RGB colorspace requires at least 3 components.'\n                raise IOError(msg)\n\n            # Turn the colorspace from a string to the enumerated value that\n            # the library expects.\n            COLORSPACE_MAP = {'rgb': opj2.CLRSPC_SRGB,\n                              'gray': opj2.CLRSPC_GRAY,\n                              'grey': opj2.CLRSPC_GRAY,\n                              'ycc': opj2.CLRSPC_YCC}\n\n            self._colorspace = COLORSPACE_MAP[colorspace.lower()]", "response": "Determine the colorspace from the supplied inputs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_openjp2(self, img_array, verbose=False):\n        if img_array.ndim == 2:\n            # Force the image to be 3D.  Just makes things easier later on.\n            numrows, numcols = img_array.shape\n            img_array = img_array.reshape(numrows, numcols, 1)\n\n        self._populate_comptparms(img_array)\n\n        with ExitStack() as stack:\n            image = opj2.image_create(self._comptparms, self._colorspace)\n            stack.callback(opj2.image_destroy, image)\n\n            self._populate_image_struct(image, img_array)\n\n            codec = opj2.create_compress(self._cparams.codec_fmt)\n            stack.callback(opj2.destroy_codec, codec)\n\n            if self._verbose or verbose:\n                info_handler = _INFO_CALLBACK\n            else:\n                info_handler = None\n\n            opj2.set_info_handler(codec, info_handler)\n            opj2.set_warning_handler(codec, _WARNING_CALLBACK)\n            opj2.set_error_handler(codec, _ERROR_CALLBACK)\n\n            opj2.setup_encoder(codec, self._cparams, image)\n\n            strm = opj2.stream_create_default_file_stream(self.filename,\n                                                          False)\n            stack.callback(opj2.stream_destroy, strm)\n\n            opj2.start_compress(codec, image, strm)\n            opj2.encode(codec, strm)\n            opj2.end_compress(codec, strm)\n\n        # Refresh the metadata.\n        self.parse()", "response": "Write JPEG 2000 file using OpenJPEG 2. x interface."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nappends a JP2 box to the file in - place.", "response": "def append(self, box):\n        \"\"\"Append a JP2 box to the file in-place.\n\n        Parameters\n        ----------\n        box : Jp2Box\n            Instance of a JP2 box.  Only UUID and XML boxes can currently be\n            appended.\n        \"\"\"\n        if self._codec_format == opj2.CODEC_J2K:\n            msg = \"Only JP2 files can currently have boxes appended to them.\"\n            raise IOError(msg)\n\n        if not ((box.box_id == 'xml ') or\n                (box.box_id == 'uuid' and\n                 box.uuid == UUID('be7acfcb-97a9-42e8-9c71-999491e3afac'))):\n            msg = (\"Only XML boxes and XMP UUID boxes can currently be \"\n                   \"appended.\")\n            raise IOError(msg)\n\n        # Check the last box.  If the length field is zero, then rewrite\n        # the length field to reflect the true length of the box.\n        with open(self.filename, 'rb') as ifile:\n            offset = self.box[-1].offset\n            ifile.seek(offset)\n            read_buffer = ifile.read(4)\n            box_length, = struct.unpack('>I', read_buffer)\n            if box_length == 0:\n                # Reopen the file in write mode and rewrite the length field.\n                true_box_length = os.path.getsize(ifile.name) - offset\n                with open(self.filename, 'r+b') as ofile:\n                    ofile.seek(offset)\n                    write_buffer = struct.pack('>I', true_box_length)\n                    ofile.write(write_buffer)\n\n        # Can now safely append the box.\n        with open(self.filename, 'ab') as ofile:\n            box.write(ofile)\n\n        self.parse()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new JP2 file with the contents of the codestream.", "response": "def wrap(self, filename, boxes=None):\n        \"\"\"Create a new JP2/JPX file wrapped in a new set of JP2 boxes.\n\n        This method is primarily aimed at wrapping a raw codestream in a set of\n        of JP2 boxes (turning it into a JP2 file instead of just a raw\n        codestream), or rewrapping a codestream in a JP2 file in a new \"jacket\"\n        of JP2 boxes.\n\n        Parameters\n        ----------\n        filename : str\n            JP2 file to be created from a raw codestream.\n        boxes : list\n            JP2 box definitions to define the JP2 file format.  If not\n            provided, a default \"\"jacket\" is assumed, consisting of JP2\n            signature, file type, JP2 header, and contiguous codestream boxes.\n            A JPX file rewrapped without the boxes argument results in a JP2\n            file encompassing the first codestream.\n\n        Returns\n        -------\n        Jp2k\n            Newly wrapped Jp2k object.\n\n        Examples\n        --------\n        >>> import glymur, tempfile\n        >>> jfile = glymur.data.goodstuff()\n        >>> j2k = glymur.Jp2k(jfile)\n        >>> tfile = tempfile.NamedTemporaryFile(suffix='jp2')\n        >>> jp2 = j2k.wrap(tfile.name)\n        \"\"\"\n        if boxes is None:\n            boxes = self._get_default_jp2_boxes()\n\n        self._validate_jp2_box_sequence(boxes)\n\n        with open(filename, 'wb') as ofile:\n            for box in boxes:\n                if box.box_id != 'jp2c':\n                    box.write(ofile)\n                else:\n                    self._write_wrapped_codestream(ofile, box)\n            ofile.flush()\n\n        jp2 = Jp2k(filename)\n        return jp2"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the codestream of the codestream.", "response": "def _write_wrapped_codestream(self, ofile, box):\n        \"\"\"Write wrapped codestream.\"\"\"\n        # Codestreams require a bit more care.\n        # Am I a raw codestream?\n        if len(self.box) == 0:\n            # Yes, just write the codestream box header plus all\n            # of myself out to file.\n            ofile.write(struct.pack('>I', self.length + 8))\n            ofile.write(b'jp2c')\n            with open(self.filename, 'rb') as ifile:\n                ofile.write(ifile.read())\n            return\n\n        # OK, I'm a jp2/jpx file.  Need to find out where the raw codestream\n        # actually starts.\n        offset = box.offset\n        if offset == -1:\n            if self.box[1].brand == 'jpx ':\n                msg = (\"The codestream box must have its offset and length \"\n                       \"attributes fully specified if the file type brand is \"\n                       \"JPX.\")\n                raise IOError(msg)\n\n            # Find the first codestream in the file.\n            jp2c = [_box for _box in self.box if _box.box_id == 'jp2c']\n            offset = jp2c[0].offset\n\n        # Ready to write the codestream.\n        with open(self.filename, 'rb') as ifile:\n            ifile.seek(offset)\n\n            # Verify that the specified codestream is right.\n            read_buffer = ifile.read(8)\n            L, T = struct.unpack_from('>I4s', read_buffer, 0)\n            if T != b'jp2c':\n                msg = \"Unable to locate the specified codestream.\"\n                raise IOError(msg)\n            if L == 0:\n                # The length of the box is presumed to last until the end of\n                # the file.  Compute the effective length of the box.\n                L = os.path.getsize(ifile.name) - ifile.tell() + 8\n\n            elif L == 1:\n                # The length of the box is in the XL field, a 64-bit value.\n                read_buffer = ifile.read(8)\n                L, = struct.unpack('>Q', read_buffer)\n\n            ifile.seek(offset)\n            read_buffer = ifile.read(L)\n            ofile.write(read_buffer)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a default set of JP2 boxes.", "response": "def _get_default_jp2_boxes(self):\n        \"\"\"Create a default set of JP2 boxes.\"\"\"\n        # Try to create a reasonable default.\n        boxes = [JPEG2000SignatureBox(),\n                 FileTypeBox(),\n                 JP2HeaderBox(),\n                 ContiguousCodestreamBox()]\n        height = self.codestream.segment[1].ysiz\n        width = self.codestream.segment[1].xsiz\n        num_components = len(self.codestream.segment[1].xrsiz)\n        if num_components < 3:\n            colorspace = core.GREYSCALE\n        else:\n            if len(self.box) == 0:\n                # Best guess is SRGB\n                colorspace = core.SRGB\n            else:\n                # Take whatever the first jp2 header / color specification\n                # says.\n                jp2hs = [box for box in self.box if box.box_id == 'jp2h']\n                colorspace = jp2hs[0].box[1].colorspace\n\n        boxes[2].box = [ImageHeaderBox(height=height, width=width,\n                                       num_components=num_components),\n                        ColourSpecificationBox(colorspace=colorspace)]\n\n        return boxes"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _remove_ellipsis(self, index, numrows, numcols, numbands):\n        # Remove the first ellipsis we find.\n        rows = slice(0, numrows)\n        cols = slice(0, numcols)\n        bands = slice(0, numbands)\n        if index[0] is Ellipsis:\n            if len(index) == 2:\n                # jp2k[..., other_slice]\n                newindex = (rows, cols, index[1])\n            else:\n                # jp2k[..., cols, bands]\n                newindex = (rows, index[1], index[2])\n        elif index[1] is Ellipsis:\n            if len(index) == 2:\n                # jp2k[rows, ...]\n                newindex = (index[0], cols, bands)\n            else:\n                # jp2k[rows, ..., bands]\n                newindex = (index[0], cols, index[2])\n        else:\n            # Assume that we don't have 4D imagery, of course.\n            newindex = (index[0], index[1], bands)\n\n        return newindex", "response": "Remove the first ellipsis in the index so that it references the image."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read(self, **kwargs):\n        if re.match(\"0|1.[01234]\", version.openjpeg_version):\n            msg = (\"You must have a version of OpenJPEG at least as high as \"\n                   \"1.5.0 before you can read JPEG2000 images with glymur.  \"\n                   \"Your version is {version}\")\n            raise TypeError(msg.format(version=version.openjpeg_version))\n\n        if version.openjpeg_version_tuple[0] < 2:\n            img = self._read_openjpeg(**kwargs)\n        else:\n            img = self._read_openjp2(**kwargs)\n        return img", "response": "Read a JPEG 2000 image."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _subsampling_sanity_check(self):\n        dxs = np.array(self.codestream.segment[1].xrsiz)\n        dys = np.array(self.codestream.segment[1].yrsiz)\n        if np.any(dxs - dxs[0]) or np.any(dys - dys[0]):\n            msg = (\"The read_bands method should be used with the subsampling \"\n                   \"factors are different. \"\n                   \"\\n\\n{siz_segment}\")\n            msg = msg.format(siz_segment=str(self.codestream.segment[1]))\n            raise IOError(msg)", "response": "Check for differing subsample factors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a JPEG 2000 image using libopenjpeg.", "response": "def _read_openjpeg(self, rlevel=0, verbose=False, area=None):\n        \"\"\"Read a JPEG 2000 image using libopenjpeg.\n\n        Parameters\n        ----------\n        rlevel : int, optional\n            Factor by which to rlevel output resolution.  Use -1 to get the\n            lowest resolution thumbnail.\n        verbose : bool, optional\n            Print informational messages produced by the OpenJPEG library.\n        area : tuple, optional\n            Specifies decoding image area,\n            (first_row, first_col, last_row, last_col)\n\n        Returns\n        -------\n        ndarray\n            The image data.\n\n        Raises\n        ------\n        RuntimeError\n            If the image has differing subsample factors.\n        \"\"\"\n        self._subsampling_sanity_check()\n\n        self._populate_dparams(rlevel)\n\n        with ExitStack() as stack:\n            try:\n                self._dparams.decod_format = self._codec_format\n\n                dinfo = opj.create_decompress(self._dparams.decod_format)\n\n                event_mgr = opj.EventMgrType()\n                handler = ctypes.cast(_INFO_CALLBACK, ctypes.c_void_p)\n                event_mgr.info_handler = handler if self.verbose else None\n                event_mgr.warning_handler = ctypes.cast(_WARNING_CALLBACK,\n                                                        ctypes.c_void_p)\n                event_mgr.error_handler = ctypes.cast(_ERROR_CALLBACK,\n                                                      ctypes.c_void_p)\n                opj.set_event_mgr(dinfo, ctypes.byref(event_mgr))\n\n                opj.setup_decoder(dinfo, self._dparams)\n\n                with open(self.filename, 'rb') as fptr:\n                    src = fptr.read()\n                cio = opj.cio_open(dinfo, src)\n\n                raw_image = opj.decode(dinfo, cio)\n\n                stack.callback(opj.image_destroy, raw_image)\n                stack.callback(opj.destroy_decompress, dinfo)\n                stack.callback(opj.cio_close, cio)\n\n                image = self._extract_image(raw_image)\n\n            except ValueError:\n                opj2.check_error(0)\n\n        if area is not None:\n            x0, y0, x1, y1 = area\n            extent = 2 ** rlevel\n\n            area = [int(round(float(x) / extent + 2 ** -20)) for x in area]\n            rows = slice(area[0], area[2], None)\n            cols = slice(area[1], area[3], None)\n            image = image[rows, cols]\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a JPEG 2000 image using libopenjp2.", "response": "def _read_openjp2(self, rlevel=0, layer=None, area=None, tile=None,\n                      verbose=False):\n        \"\"\"Read a JPEG 2000 image using libopenjp2.\n\n        Parameters\n        ----------\n        layer : int, optional\n            Number of quality layer to decode.\n        rlevel : int, optional\n            Factor by which to rlevel output resolution.  Use -1 to get the\n            lowest resolution thumbnail.\n        area : tuple, optional\n            Specifies decoding image area,\n            (first_row, first_col, last_row, last_col)\n        tile : int, optional\n            Number of tile to decode.\n        verbose : bool, optional\n            Print informational messages produced by the OpenJPEG library.\n\n        Returns\n        -------\n        ndarray\n            The image data.\n\n        Raises\n        ------\n        RuntimeError\n            If the image has differing subsample factors.\n        \"\"\"\n        self.layer = layer\n        self._subsampling_sanity_check()\n        self._populate_dparams(rlevel, tile=tile, area=area)\n        image = self._read_openjp2_common()\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a JPEG 2000 image using libopenjp2.", "response": "def _read_openjp2_common(self):\n        \"\"\"\n        Read a JPEG 2000 image using libopenjp2.\n\n        Returns\n        -------\n        ndarray or lst\n            Either the image as an ndarray or a list of ndarrays, each item\n            corresponding to one band.\n        \"\"\"\n        with ExitStack() as stack:\n            filename = self.filename\n            stream = opj2.stream_create_default_file_stream(filename, True)\n            stack.callback(opj2.stream_destroy, stream)\n            codec = opj2.create_decompress(self._codec_format)\n            stack.callback(opj2.destroy_codec, codec)\n\n            opj2.set_error_handler(codec, _ERROR_CALLBACK)\n            opj2.set_warning_handler(codec, _WARNING_CALLBACK)\n\n            if self._verbose:\n                opj2.set_info_handler(codec, _INFO_CALLBACK)\n            else:\n                opj2.set_info_handler(codec, None)\n\n            opj2.setup_decoder(codec, self._dparams)\n            raw_image = opj2.read_header(stream, codec)\n            stack.callback(opj2.image_destroy, raw_image)\n\n            if self._dparams.nb_tile_to_decode:\n                opj2.get_decoded_tile(codec, stream, raw_image,\n                                      self._dparams.tile_index)\n            else:\n                opj2.set_decode_area(codec, raw_image,\n                                     self._dparams.DA_x0, self._dparams.DA_y0,\n                                     self._dparams.DA_x1, self._dparams.DA_y1)\n                opj2.decode(codec, stream, raw_image)\n\n            opj2.end_decompress(codec, stream)\n\n            image = self._extract_image(raw_image)\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the decompression structure with appropriate input parameters.", "response": "def _populate_dparams(self, rlevel, tile=None, area=None):\n        \"\"\"Populate decompression structure with appropriate input parameters.\n\n        Parameters\n        ----------\n        rlevel : int\n            Factor by which to rlevel output resolution.\n        area : tuple\n            Specifies decoding image area,\n            (first_row, first_col, last_row, last_col)\n        tile : int\n            Number of tile to decode.\n        \"\"\"\n        if opj2.OPENJP2 is not None:\n            dparam = opj2.set_default_decoder_parameters()\n        else:\n            dparam = opj.DecompressionParametersType()\n            opj.set_default_decoder_parameters(ctypes.byref(dparam))\n\n        infile = self.filename.encode()\n        nelts = opj2.PATH_LEN - len(infile)\n        infile += b'0' * nelts\n        dparam.infile = infile\n\n        # Return raw codestream components instead of \"interpolating\" the\n        # colormap?\n        dparam.flags |= 1 if self.ignore_pclr_cmap_cdef else 0\n\n        dparam.decod_format = self._codec_format\n        dparam.cp_layer = self._layer\n\n        # Must check the specified rlevel against the maximum.\n        if rlevel != 0:\n            # Must check the specified rlevel against the maximum.\n            max_rlevel = self.codestream.segment[2].num_res\n            if rlevel == -1:\n                # -1 is shorthand for the largest rlevel\n                rlevel = max_rlevel\n            elif rlevel < -1 or rlevel > max_rlevel:\n                msg = (\"rlevel must be in the range [-1, {max_rlevel}] \"\n                       \"for this image.\")\n                msg = msg.format(max_rlevel=max_rlevel)\n                raise IOError(msg)\n\n        dparam.cp_reduce = rlevel\n\n        if area is not None:\n            if area[0] < 0 or area[1] < 0 or area[2] <= 0 or area[3] <= 0:\n                msg = (\"The upper left corner coordinates must be nonnegative \"\n                       \"and the lower right corner coordinates must be \"\n                       \"positive.  The specified upper left and lower right \"\n                       \"coordinates are ({y0}, {x0}) and ({y1}, {x1}).\")\n                msg = msg.format(x0=area[1], y0=area[0],\n                                 x1=area[3], y1=area[2])\n                raise IOError(msg)\n            dparam.DA_y0 = area[0]\n            dparam.DA_x0 = area[1]\n            dparam.DA_y1 = area[2]\n            dparam.DA_x1 = area[3]\n\n        if tile is not None:\n            dparam.tile_index = tile\n            dparam.nb_tile_to_decode = 1\n\n        self._dparams = dparam"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a JPEG 2000 image.", "response": "def read_bands(self, rlevel=0, layer=None, area=None, tile=None,\n                   verbose=False, ignore_pclr_cmap_cdef=False):\n        \"\"\"Read a JPEG 2000 image.\n\n        The only time you should use this method is when the image has\n        different subsampling factors across components.  Otherwise you should\n        use the read method.\n\n        Parameters\n        ----------\n        layer : int, optional\n            Number of quality layer to decode.\n        rlevel : int, optional\n            Factor by which to rlevel output resolution.\n        area : tuple, optional\n            Specifies decoding image area,\n            (first_row, first_col, last_row, last_col)\n        tile : int, optional\n            Number of tile to decode.\n        ignore_pclr_cmap_cdef : bool\n            Whether or not to ignore the pclr, cmap, or cdef boxes during any\n            color transformation.  Defaults to False.\n        verbose : bool, optional\n            Print informational messages produced by the OpenJPEG library.\n\n        Returns\n        -------\n        list\n            List of the individual image components.\n\n        See also\n        --------\n        read : read JPEG 2000 image\n\n        Examples\n        --------\n        >>> import glymur\n        >>> jfile = glymur.data.nemo()\n        >>> jp = glymur.Jp2k(jfile)\n        >>> components_lst = jp.read_bands(rlevel=1)\n        \"\"\"\n        if version.openjpeg_version < '2.1.0':\n            msg = (\"You must have at least version 2.1.0 of OpenJPEG \"\n                   \"installed before using this method.  Your version of \"\n                   \"OpenJPEG is {version}.\")\n            msg = msg.format(version=version.openjpeg_version)\n            raise IOError(msg)\n\n        self.ignore_pclr_cmap_cdef = ignore_pclr_cmap_cdef\n        self.layer = layer\n        self._populate_dparams(rlevel, tile=tile, area=area)\n        lst = self._read_openjp2_common()\n        return lst"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _extract_image(self, raw_image):\n        ncomps = raw_image.contents.numcomps\n\n        # Make a pass thru the image, see if any of the band datatypes or\n        # dimensions differ.\n        dtypes, nrows, ncols = [], [], []\n        for k in range(raw_image.contents.numcomps):\n            component = raw_image.contents.comps[k]\n            dtypes.append(self._component2dtype(component))\n            nrows.append(component.h)\n            ncols.append(component.w)\n        is_cube = all(r == nrows[0] and c == ncols[0] and d == dtypes[0]\n                      for r, c, d in zip(nrows, ncols, dtypes))\n\n        if is_cube:\n            image = np.zeros((nrows[0], ncols[0], ncomps), dtypes[0])\n        else:\n            image = []\n\n        for k in range(raw_image.contents.numcomps):\n            component = raw_image.contents.comps[k]\n\n            self._validate_nonzero_image_size(nrows[k], ncols[k], k)\n\n            addr = ctypes.addressof(component.data.contents)\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                nelts = nrows[k] * ncols[k]\n                band = np.ctypeslib.as_array(\n                    (ctypes.c_int32 * nelts).from_address(addr))\n                if is_cube:\n                    image[:, :, k] = np.reshape(band.astype(dtypes[k]),\n                                                (nrows[k], ncols[k]))\n                else:\n                    image.append(np.reshape(band.astype(dtypes[k]),\n                                 (nrows[k], ncols[k])))\n\n        if is_cube and image.shape[2] == 1:\n            # The third dimension has just a single layer.  Make the image\n            # data 2D instead of 3D.\n            image.shape = image.shape[0:2]\n\n        return image", "response": "Extracts the unequally - sized components from the image structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _component2dtype(self, component):\n        if component.prec > 16:\n            msg = \"Unhandled precision: {0} bits.\".format(component.prec)\n            raise IOError(msg)\n\n        if component.sgnd:\n            dtype = np.int8 if component.prec <=8 else np.int16\n        else:\n            dtype = np.uint8 if component.prec <=8 else np.uint16\n\n        return dtype", "response": "Return the numpy datatype for an OpenJPEG component."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the codestream entry for the current file.", "response": "def get_codestream(self, header_only=True):\n        \"\"\"Retrieve codestream.\n\n        Parameters\n        ----------\n        header_only : bool, optional\n            If True, only marker segments in the main header are parsed.\n            Supplying False may impose a large performance penalty.\n\n        Returns\n        -------\n        Codestream\n            Object describing the codestream syntax.\n\n        Examples\n        --------\n        >>> import glymur\n        >>> jfile = glymur.data.nemo()\n        >>> jp2 = glymur.Jp2k(jfile)\n        >>> codestream = jp2.get_codestream()\n        >>> print(codestream.segment[1])\n        SIZ marker segment @ (3233, 47)\n            Profile:  no profile\n            Reference Grid Height, Width:  (1456 x 2592)\n            Vertical, Horizontal Reference Grid Offset:  (0 x 0)\n            Reference Tile Height, Width:  (1456 x 2592)\n            Vertical, Horizontal Reference Tile Offset:  (0 x 0)\n            Bitdepth:  (8, 8, 8)\n            Signed:  (False, False, False)\n            Vertical, Horizontal Subsampling:  ((1, 1), (1, 1), (1, 1))\n        \"\"\"\n        with open(self.filename, 'rb') as fptr:\n            if self._codec_format == opj2.CODEC_J2K:\n                codestream = Codestream(fptr, self.length,\n                                        header_only=header_only)\n            else:\n                box = [x for x in self.box if x.box_id == 'jp2c']\n                fptr.seek(box[0].offset)\n                read_buffer = fptr.read(8)\n                (box_length, _) = struct.unpack('>I4s', read_buffer)\n                if box_length == 0:\n                    # The length of the box is presumed to last until the end\n                    # of the file.  Compute the effective length of the box.\n                    box_length = os.path.getsize(fptr.name) - fptr.tell() + 8\n                elif box_length == 1:\n                    # Seek past the XL field.\n                    read_buffer = fptr.read(8)\n                    box_length, = struct.unpack('>Q', read_buffer)\n                codestream = Codestream(fptr, box_length - 8,\n                                        header_only=header_only)\n\n            return codestream"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the image structure needed for compression.", "response": "def _populate_image_struct(self, image, imgdata):\n        \"\"\"Populates image struct needed for compression.\n\n        Parameters\n        ----------\n        image : ImageType(ctypes.Structure)\n            Corresponds to image_t type in openjp2 headers.\n        img_array : ndarray\n            Image data to be written to file.\n        \"\"\"\n\n        numrows, numcols, num_comps = imgdata.shape\n        for k in range(num_comps):\n            self._validate_nonzero_image_size(numrows, numcols, k)\n\n        # set image offset and reference grid\n        image.contents.x0 = self._cparams.image_offset_x0\n        image.contents.y0 = self._cparams.image_offset_y0\n        image.contents.x1 = (image.contents.x0 +\n                             (numcols - 1) * self._cparams.subsampling_dx + 1)\n        image.contents.y1 = (image.contents.y0 +\n                             (numrows - 1) * self._cparams.subsampling_dy + 1)\n\n        # Stage the image data to the openjpeg data structure.\n        for k in range(0, num_comps):\n            if self._cparams.rsiz in (core.OPJ_PROFILE_CINEMA_2K,\n                                      core.OPJ_PROFILE_CINEMA_4K):\n                image.contents.comps[k].prec = 12\n                image.contents.comps[k].bpp = 12\n\n            layer = np.ascontiguousarray(imgdata[:, :, k], dtype=np.int32)\n            dest = image.contents.comps[k].data\n            src = layer.ctypes.data\n            ctypes.memmove(dest, src, layer.nbytes)\n\n        return image"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nraising IOError if the image has no area of zero.", "response": "def _validate_nonzero_image_size(self, nrows, ncols, component_index):\n        \"\"\"The image cannot have area of zero.\n        \"\"\"\n        if nrows == 0 or ncols == 0:\n            # Letting this situation continue would segfault openjpeg.\n            msg = \"Component {0} has dimensions {1} x {2}\"\n            msg = msg.format(component_index, nrows, ncols)\n            raise IOError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating the JP2 box sequence.", "response": "def _validate_jp2_box_sequence(self, boxes):\n        \"\"\"Run through series of tests for JP2 box legality.\n\n        This is non-exhaustive.\n        \"\"\"\n        JP2_IDS = ['colr', 'cdef', 'cmap', 'jp2c', 'ftyp', 'ihdr', 'jp2h',\n                   'jP  ', 'pclr', 'res ', 'resc', 'resd', 'xml ', 'ulst',\n                   'uinf', 'url ', 'uuid']\n\n        self._validate_signature_compatibility(boxes)\n        self._validate_jp2h(boxes)\n        self._validate_jp2c(boxes)\n        if boxes[1].brand == 'jpx ':\n            self._validate_jpx_box_sequence(boxes)\n        else:\n            # Validate the JP2 box IDs.\n            count = self._collect_box_count(boxes)\n            for box_id in count.keys():\n                if box_id not in JP2_IDS:\n                    msg = (\"The presence of a '{0}' box requires that the \"\n                           \"file type brand be set to 'jpx '.\")\n                    raise IOError(msg.format(box_id))\n\n            self._validate_jp2_colr(boxes)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating JP2 requirements on colour specification boxes.", "response": "def _validate_jp2_colr(self, boxes):\n        \"\"\"\n        Validate JP2 requirements on colour specification boxes.\n        \"\"\"\n        lst = [box for box in boxes if box.box_id == 'jp2h']\n        jp2h = lst[0]\n        for colr in [box for box in jp2h.box if box.box_id == 'colr']:\n            if colr.approximation != 0:\n                msg = (\"A JP2 colr box cannot have a non-zero approximation \"\n                       \"field.\")\n                raise IOError(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning through series of tests for JPX box legality.", "response": "def _validate_jpx_box_sequence(self, boxes):\n        \"\"\"Run through series of tests for JPX box legality.\"\"\"\n        self._validate_label(boxes)\n        self._validate_jpx_compatibility(boxes, boxes[1].compatibility_list)\n        self._validate_singletons(boxes)\n        self._validate_top_level(boxes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_signature_compatibility(self, boxes):\n        # Check for a bad sequence of boxes.\n        # 1st two boxes must be 'jP  ' and 'ftyp'\n        if boxes[0].box_id != 'jP  ' or boxes[1].box_id != 'ftyp':\n            msg = (\"The first box must be the signature box and the second \"\n                   \"must be the file type box.\")\n            raise IOError(msg)\n\n        # The compatibility list must contain at a minimum 'jp2 '.\n        if 'jp2 ' not in boxes[1].compatibility_list:\n            msg = \"The ftyp box must contain 'jp2 ' in the compatibility list.\"\n            raise IOError(msg)", "response": "Validate the file signature and compatibility status."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_jp2c(self, boxes):\n        # jp2c must be preceeded by jp2h\n        jp2h_lst = [idx for (idx, box) in enumerate(boxes)\n                    if box.box_id == 'jp2h']\n        jp2h_idx = jp2h_lst[0]\n        jp2c_lst = [idx for (idx, box) in enumerate(boxes)\n                    if box.box_id == 'jp2c']\n        if len(jp2c_lst) == 0:\n            msg = (\"A codestream box must be defined in the outermost \"\n                   \"list of boxes.\")\n            raise IOError(msg)\n\n        jp2c_idx = jp2c_lst[0]\n        if jp2h_idx >= jp2c_idx:\n            msg = \"The codestream box must be preceeded by a jp2 header box.\"\n            raise IOError(msg)", "response": "Validate the codestream box in relation to other boxes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate_jp2h(self, boxes):\n        self._check_jp2h_child_boxes(boxes, 'top-level')\n\n        jp2h_lst = [box for box in boxes if box.box_id == 'jp2h']\n        jp2h = jp2h_lst[0]\n\n        # 1st jp2 header box cannot be empty.\n        if len(jp2h.box) == 0:\n            msg = \"The JP2 header superbox cannot be empty.\"\n            raise IOError(msg)\n\n        # 1st jp2 header box must be ihdr\n        if jp2h.box[0].box_id != 'ihdr':\n            msg = (\"The first box in the jp2 header box must be the image \"\n                   \"header box.\")\n            raise IOError(msg)\n\n        # colr must be present in jp2 header box.\n        colr_lst = [j for (j, box) in enumerate(jp2h.box)\n                    if box.box_id == 'colr']\n        if len(colr_lst) == 0:\n            msg = \"The jp2 header box must contain a color definition box.\"\n            raise IOError(msg)\n        colr = jp2h.box[colr_lst[0]]\n\n        self._validate_channel_definition(jp2h, colr)", "response": "Validate the JP2 Header box."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_channel_definition(self, jp2h, colr):\n        cdef_lst = [j for (j, box) in enumerate(jp2h.box)\n                    if box.box_id == 'cdef']\n        if len(cdef_lst) > 1:\n            msg = (\"Only one channel definition box is allowed in the \"\n                   \"JP2 header.\")\n            raise IOError(msg)\n        elif len(cdef_lst) == 1:\n            cdef = jp2h.box[cdef_lst[0]]\n            if colr.colorspace == core.SRGB:\n                if any([chan + 1 not in cdef.association or\n                        cdef.channel_type[chan] != 0 for chan in [0, 1, 2]]):\n                    msg = (\"All color channels must be defined in the \"\n                           \"channel definition box.\")\n                    raise IOError(msg)\n            elif colr.colorspace == core.GREYSCALE:\n                if 0 not in cdef.channel_type:\n                    msg = (\"All color channels must be defined in the \"\n                           \"channel definition box.\")\n                    raise IOError(msg)", "response": "Validate the channel definition box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_jp2h_child_boxes(self, boxes, parent_box_name):\n        JP2H_CHILDREN = set(['bpcc', 'cdef', 'cmap', 'ihdr', 'pclr'])\n\n        box_ids = set([box.box_id for box in boxes])\n        intersection = box_ids.intersection(JP2H_CHILDREN)\n        if len(intersection) > 0 and parent_box_name not in ['jp2h', 'jpch']:\n            msg = \"A {0} box can only be nested in a JP2 header box.\"\n            raise IOError(msg.format(list(intersection)[0]))\n\n        # Recursively check any contained superboxes.\n        for box in boxes:\n            if hasattr(box, 'box'):\n                self._check_jp2h_child_boxes(box.box, box.box_id)", "response": "Checks that all child boxes of a JP2 header are in the JP2 header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncount the occurences of each box type.", "response": "def _collect_box_count(self, boxes):\n        \"\"\"Count the occurences of each box type.\"\"\"\n        count = Counter([box.box_id for box in boxes])\n\n        # Add the counts in the superboxes.\n        for box in boxes:\n            if hasattr(box, 'box'):\n                count.update(self._collect_box_count(box.box))\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_superbox_for_top_levels(self, boxes):\n        # We are only looking at the boxes contained in a superbox, so if any\n        # of the blacklisted boxes show up here, it's an error.\n        TOP_LEVEL_ONLY_BOXES = set(['dtbl'])\n        box_ids = set([box.box_id for box in boxes])\n        intersection = box_ids.intersection(TOP_LEVEL_ONLY_BOXES)\n        if len(intersection) > 0:\n            msg = \"A {0} box cannot be nested in a superbox.\"\n            raise IOError(msg.format(list(intersection)[0]))\n\n        # Recursively check any contained superboxes.\n        for box in boxes:\n            if hasattr(box, 'box'):\n                self._check_superbox_for_top_levels(box.box)", "response": "Check that all boxes are at the top level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_top_level(self, boxes):\n        # Add the counts in the superboxes.\n        for box in boxes:\n            if hasattr(box, 'box'):\n                self._check_superbox_for_top_levels(box.box)\n\n        count = self._collect_box_count(boxes)\n\n        # If there is one data reference box, then there must also be one ftbl.\n        if 'dtbl' in count and 'ftbl' not in count:\n            msg = ('The presence of a data reference box requires the '\n                   'presence of a fragment table box as well.')\n            raise IOError(msg)", "response": "Validate that all boxes are at the top level."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_singletons(self, boxes):\n        count = self._collect_box_count(boxes)\n        # Which boxes occur more than once?\n        multiples = [box_id for box_id, bcount in count.items() if bcount > 1]\n        if 'dtbl' in multiples:\n            raise IOError('There can only be one dtbl box in a file.')", "response": "Validate that there are no singletons in a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_jpx_compatibility(self, boxes, compatibility_list):\n        JPX_IDS = ['asoc', 'nlst']\n        jpx_cl = set(compatibility_list)\n        for box in boxes:\n            if box.box_id in JPX_IDS:\n                if len(set(['jpx ', 'jpxb']).intersection(jpx_cl)) == 0:\n                    msg = (\"A JPX box requires that either 'jpx ' or 'jpxb' \"\n                           \"be present in the ftype compatibility list.\")\n                    raise RuntimeError(msg)\n            if hasattr(box, 'box') != 0:\n                # Same set of checks on any child boxes.\n                self._validate_jpx_compatibility(box.box, compatibility_list)", "response": "Validate that all JPX boxes are present in the compatibility list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that all label boxes are inside association codestream headers or codestream compositing layer header boxes.", "response": "def _validate_label(self, boxes):\n        \"\"\"\n        Label boxes can only be inside association, codestream headers, or\n        compositing layer header boxes.\n        \"\"\"\n        for box in boxes:\n            if box.box_id != 'asoc':\n                if hasattr(box, 'box'):\n                    for boxi in box.box:\n                        if boxi.box_id == 'lbl ':\n                            msg = (\"A label box cannot be nested inside a \"\n                                   \"{0} box.\")\n                            msg = msg.format(box.box_id)\n                            raise IOError(msg)\n                    # Same set of checks on any child boxes.\n                    self._validate_label(box.box)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfulfill an HTTP request to Keen s API.", "response": "def fulfill(self, method, *args, **kwargs):\n\n        \"\"\" Fulfill an HTTP request to Keen's API. \"\"\"\n\n        return getattr(self.session, method)(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post_event(self, event):\n\n        url = \"{0}/{1}/projects/{2}/events/{3}\".format(self.base_url, self.api_version,\n                                                       self.project_id,\n                                                       event.event_collection)\n        headers = utilities.headers(self.write_key)\n        payload = event.to_json()\n        response = self.fulfill(HTTPMethods.POST, url, data=payload, headers=headers, timeout=self.post_timeout)\n        self._error_handling(response)", "response": "Post an event to the Keen IO API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_events(self, events):\n\n        \"\"\"\n        Posts a single event to the Keen IO API. The write key must be set first.\n\n        :param events: an Event to upload\n        \"\"\"\n\n        url = \"{0}/{1}/projects/{2}/events\".format(self.base_url, self.api_version,\n                                                   self.project_id)\n        headers = utilities.headers(self.write_key)\n        payload = json.dumps(events)\n        response = self.fulfill(HTTPMethods.POST, url, data=payload, headers=headers, timeout=self.post_timeout)\n        self._error_handling(response)\n        return self._get_response_json(response)", "response": "Post a single event to the Keen IO API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating that a given order_by is well - formed and returns True if either no order_by is present or if the order_by is well - formed or False if it is not well - formed.", "response": "def _order_by_is_valid_or_none(self, params):\n        \"\"\"\n        Validates that a given order_by has proper syntax.\n\n        :param params: Query params.\n        :return: Returns True if either no order_by is present, or if the order_by is well-formed.\n        \"\"\"\n        if not \"order_by\" in params or not params[\"order_by\"]:\n            return True\n\n        def _order_by_dict_is_not_well_formed(d):\n            if not isinstance(d, dict):\n                # Bad type.\n                return True\n            if \"property_name\" in d and d[\"property_name\"]:\n                if \"direction\" in d and not direction.is_valid_direction(d[\"direction\"]):\n                    # Bad direction provided.\n                    return True\n                for k in d:\n                    if k != \"property_name\" and k != \"direction\":\n                        # Unexpected key.\n                        return True\n                # Everything looks good!\n                return False\n            # Missing required key.\n            return True\n\n        # order_by is converted to a list before this point if it wasn't one before.\n        order_by_list = json.loads(params[\"order_by\"])\n\n        for order_by in order_by_list:\n            if _order_by_dict_is_not_well_formed(order_by):\n                return False\n        if not \"group_by\" in params or not params[\"group_by\"]:\n            # We must have group_by to have order_by make sense.\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _limit_is_valid_or_none(self, params):\n        if not \"limit\" in params or not params[\"limit\"]:\n            return True\n        if not isinstance(params[\"limit\"], int) or params[\"limit\"] < 1:\n            return False\n        if not \"order_by\" in params:\n            return False\n        return True", "response": "Validates that a given limit is present or is well - formed. Returns True if it is well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms a query using the Keen IO analysis API.", "response": "def query(self, analysis_type, params, all_keys=False):\n        \"\"\"\n        Performs a query using the Keen IO analysis API.  A read key must be set first.\n\n        \"\"\"\n        if not self._order_by_is_valid_or_none(params):\n            raise ValueError(\"order_by given is invalid or is missing required group_by.\")\n        if not self._limit_is_valid_or_none(params):\n            raise ValueError(\"limit given is invalid or is missing required order_by.\")\n\n        url = \"{0}/{1}/projects/{2}/queries/{3}\".format(self.base_url, self.api_version,\n                                                        self.project_id, analysis_type)\n\n        headers = utilities.headers(self.read_key)\n        payload = params\n        response = self.fulfill(HTTPMethods.GET, url, params=payload, headers=headers, timeout=self.get_timeout)\n        self._error_handling(response)\n\n        response = response.json()\n\n        if not all_keys:\n            response = response[\"result\"]\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes events from the Keen IO API.", "response": "def delete_events(self, event_collection, params):\n        \"\"\"\n        Deletes events via the Keen IO API. A master key must be set first.\n\n        :param event_collection: string, the event collection from which event are being deleted\n\n        \"\"\"\n\n        url = \"{0}/{1}/projects/{2}/events/{3}\".format(self.base_url,\n                                                       self.api_version,\n                                                       self.project_id,\n                                                       event_collection)\n        headers = utilities.headers(self.master_key)\n        response = self.fulfill(HTTPMethods.DELETE, url, params=params, headers=headers, timeout=self.post_timeout)\n\n        self._error_handling(response)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_collection(self, event_collection):\n\n        url = \"{0}/{1}/projects/{2}/events/{3}\".format(self.base_url, self.api_version,\n                                                       self.project_id, event_collection)\n        headers = utilities.headers(self.read_key)\n        response = self.fulfill(HTTPMethods.GET, url, headers=headers, timeout=self.get_timeout)\n        self._error_handling(response)\n\n        return response.json()", "response": "Retrieves information about a specific event collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding permissions to the existing list of permissions on this key.", "response": "def add_access_key_permissions(self, access_key_id, permissions):\n        \"\"\"\n        Adds to the existing list of permissions on this key with the contents of this list.\n        Will not remove any existing permissions or modify the remainder of the key.\n\n        :param access_key_id: the 'key' value of the access key to add permissions to\n        :param permissions: the new permissions to add to the existing list of permissions\n        \"\"\"\n        # Get current state via HTTPS.\n        current_access_key = self.get_access_key(access_key_id)\n\n        # Copy and only change the single parameter.\n        payload_dict = KeenApi._build_access_key_dict(current_access_key)\n\n        # Turn into sets to avoid duplicates.\n        old_permissions = set(payload_dict[\"permitted\"])\n        new_permissions = set(permissions)\n        combined_permissions = old_permissions.union(new_permissions)\n        payload_dict[\"permitted\"] = list(combined_permissions)\n\n        # Now just treat it like a full update.\n        return self.update_access_key_full(access_key_id, **payload_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a list of permissions from the existing list of permissions.", "response": "def remove_access_key_permissions(self, access_key_id, permissions):\n        \"\"\"\n        Removes a list of permissions from the existing list of permissions.\n        Will not remove all existing permissions unless all such permissions are included\n        in this list. Not to be confused with key revocation.\n\n        See also: revoke_access_key()\n\n        :param access_key_id: the 'key' value of the access key to remove some permissions from\n        :param permissions: the permissions you wish to remove from this access key\n        \"\"\"\n        # Get current state via HTTPS.\n        current_access_key = self.get_access_key(access_key_id)\n\n        # Copy and only change the single parameter.\n        payload_dict = KeenApi._build_access_key_dict(current_access_key)\n\n        # Turn into sets to avoid duplicates.\n        old_permissions = set(payload_dict[\"permitted\"])\n        removal_permissions = set(permissions)\n        reduced_permissions = old_permissions.difference(removal_permissions)\n        payload_dict[\"permitted\"] = list(reduced_permissions)\n\n        # Now just treat it like a full update.\n        return self.update_access_key_full(access_key_id, **payload_dict)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the name is_active permitted and options values of a given master key.", "response": "def update_access_key_full(self, access_key_id, name, is_active, permitted, options):\n        \"\"\"\n        Replaces the 'name', 'is_active', 'permitted', and 'options' values of a given key.\n        A master key must be set first.\n\n        :param access_key_id: the 'key' value of the access key for which the values will be replaced\n        :param name: the new name desired for this access key\n        :param is_active: whether the key should become enabled (True) or revoked (False)\n        :param permitted: the new list of permissions desired for this access key\n        :param options: the new dictionary of options for this access key\n        \"\"\"\n        url = \"{0}/{1}/projects/{2}/keys/{3}\".format(self.base_url, self.api_version,\n                                                     self.project_id, access_key_id)\n        headers = utilities.headers(self.master_key)\n        payload_dict = {\n            \"name\": name,\n            \"is_active\": is_active,\n            \"permitted\": permitted,\n            \"options\": options\n        }\n        payload = json.dumps(payload_dict)\n        response = self.fulfill(HTTPMethods.POST, url, data=payload, headers=headers, timeout=self.get_timeout)\n        self._error_handling(response)\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches all Cached Datasets for a Project.", "response": "def all(self):\n        \"\"\" Fetch all Cached Datasets for a Project. Read key must be set.\n        \"\"\"\n        return self._get_json(HTTPMethods.GET,\n                              self._cached_datasets_url,\n                              self._get_master_key())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfetch a single Cached Dataset for a Project.", "response": "def get(self, dataset_name):\n        \"\"\" Fetch a single Cached Dataset for a Project. Read key must be set.\n\n        :param dataset_name: Name of Cached Dataset (not `display_name`)\n        \"\"\"\n        url = \"{0}/{1}\".format(self._cached_datasets_url, dataset_name)\n        return self._get_json(HTTPMethods.GET, url, self._get_read_key())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create(self, dataset_name, query, index_by, display_name):\n        url = \"{0}/{1}\".format(self._cached_datasets_url, dataset_name)\n        payload = {\n            \"query\": query,\n            \"index_by\": index_by,\n            \"display_name\": display_name\n        }\n        return self._get_json(HTTPMethods.PUT, url, self._get_master_key(), json=payload)", "response": "Create a Cached Dataset for a Project."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef results(self, dataset_name, index_by, timeframe):\n        url = \"{0}/{1}/results\".format(self._cached_datasets_url, dataset_name)\n\n        index_by = index_by if isinstance(index_by, str) else json.dumps(index_by)\n        timeframe = timeframe if isinstance(timeframe, str) else json.dumps(timeframe)\n\n        query_params = {\n            \"index_by\": index_by,\n            \"timeframe\": timeframe\n        }\n\n        return self._get_json(\n            HTTPMethods.GET, url, self._get_read_key(), params=query_params\n        )", "response": "Retrieve results from a Cached Dataset. Read key must be set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, dataset_name):\n        url = \"{0}/{1}\".format(self._cached_datasets_url, dataset_name)\n        self._get_json(HTTPMethods.DELETE, url, self._get_master_key())\n        return True", "response": "Delete a Cached Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef glymurrc_fname():\n\n    # Current directory.\n    fname = os.path.join(os.getcwd(), 'glymurrc')\n    if os.path.exists(fname):\n        return fname\n\n    confdir = get_configdir()\n    if confdir is not None:\n        fname = os.path.join(confdir, 'glymurrc')\n        if os.path.exists(fname):\n            return fname\n\n    # didn't find a configuration file.\n    return None", "response": "Return the path to the configuration file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_library_handle(libname, path):\n\n    if path is None or path in ['None', 'none']:\n        # Either could not find a library via ctypes or\n        # user-configuration-file, or we could not find it in any of the\n        # default locations, or possibly the user intentionally does not want\n        # one of the libraries to load.\n        return None\n\n    try:\n        if os.name == \"nt\":\n            opj_lib = ctypes.windll.LoadLibrary(path)\n        else:\n            opj_lib = ctypes.CDLL(path)\n    except (TypeError, OSError):\n        msg = 'The {libname} library at {path} could not be loaded.'\n        msg = msg.format(path=path, libname=libname)\n        warnings.warn(msg, UserWarning)\n        opj_lib = None\n\n    return opj_lib", "response": "Load the library return the ctypes handle."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_config_file(libname):\n    filename = glymurrc_fname()\n    if filename is None:\n        # There's no library file path to return in this case.\n        return None\n\n    # Read the configuration file for the library location.\n    parser = ConfigParser()\n    parser.read(filename)\n    try:\n        path = parser.get('library', libname)\n    except (NoOptionError, NoSectionError):\n        path = None\n    return path", "response": "Reads the configuration file for the specified library."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to ascertain locations of openjp2 openjpeg libraries. Returns ------- tuple of library handles", "response": "def glymur_config():\n    \"\"\"\n    Try to ascertain locations of openjp2, openjpeg libraries.\n\n    Returns\n    -------\n    tuple\n        tuple of library handles\n    \"\"\"\n    handles = (load_openjpeg_library(x) for x in ['openjp2', 'openjpeg'])\n    handles = tuple(handles)\n\n    if all(handle is None for handle in handles):\n        msg = \"Neither the openjp2 nor the openjpeg library could be loaded.  \"\n        warnings.warn(msg)\n    return handles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_configdir():\n    if 'XDG_CONFIG_HOME' in os.environ:\n        return os.path.join(os.environ['XDG_CONFIG_HOME'], 'glymur')\n\n    if 'HOME' in os.environ and os.name != 'nt':\n        # HOME is set by WinPython to something unusual, so we don't\n        # necessarily want that.\n        return os.path.join(os.environ['HOME'], '.config', 'glymur')\n\n    # Last stand.  Should handle windows... others?\n    return os.path.join(os.path.expanduser('~'), 'glymur')", "response": "Return string representing the configuration directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_option(key, value):\n    if key not in _options.keys():\n        raise KeyError('{key} not valid.'.format(key=key))\n    _options[key] = value", "response": "Set the value of the specified option."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset one or more options to their default value.", "response": "def reset_option(key):\n    \"\"\"\n    Reset one or more options to their default value.\n\n    Pass \"all\" as argument to reset all options.\n\n    Available options:\n\n        parse.full_codestream\n        print.xml\n        print.codestream\n        print.short\n\n    Parameter\n    ---------\n    key : str\n        Name of a single option.\n    \"\"\"\n    global _options\n    if key == 'all':\n        _options = copy.deepcopy(_original_options)\n    else:\n        if key not in _options.keys():\n            raise KeyError('{key} not valid.'.format(key=key))\n        _options[key] = _original_options[key]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset printing options for JPEG 2000 boxes.", "response": "def set_printoptions(**kwargs):\n    \"\"\"Set printing options.\n\n    These options determine the way JPEG 2000 boxes are displayed.\n\n    Parameters\n    ----------\n    short : bool, optional\n        When True, only the box ID, offset, and length are displayed.  Useful\n        for displaying only the basic structure or skeleton of a JPEG 2000\n        file.\n    xml : bool, optional\n        When False, printing of the XML contents of any XML boxes or UUID XMP\n        boxes is suppressed.\n    codestream : bool, optional\n        When False, the codestream segments are not printed.  Otherwise the\n        segments are printed depending on how set_parseoptions has been used.\n\n    See also\n    --------\n    get_printoptions\n\n    Examples\n    --------\n    To put back the default options, you can use:\n\n    >>> import glymur\n    >>> glymur.set_printoptions(short=False, xml=True, codestream=True)\n    \"\"\"\n    warnings.warn('Use set_option instead of set_printoptions.',\n                  DeprecationWarning)\n    for key, value in kwargs.items():\n        if key not in ['short', 'xml', 'codestream']:\n            raise KeyError('\"{0}\" not a valid keyword parameter.'.format(key))\n        set_option('print.' + key, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the current print options.", "response": "def get_printoptions():\n    \"\"\"Return the current print options.\n\n    Returns\n    -------\n    dict\n        Dictionary of current print options with keys\n\n          - short : bool\n          - xml : bool\n          - codestream : bool\n\n        For a full description of these options, see `set_printoptions`.\n\n    See also\n    --------\n    set_printoptions\n    \"\"\"\n    warnings.warn('Use get_option instead of get_printoptions.',\n                  DeprecationWarning)\n    d = {}\n    for key in ['short', 'xml', 'codestream']:\n        d[key] = _options['print.' + key]\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a slash to the end of a string", "response": "def _add_slash(self, string=None):\n        \"\"\" if a string doesn't end in a '/' add one \"\"\"\n        if(not str.endswith(string, '/')):\n            return str.join('', [string, '/'])\n        return str(string)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the Cerberus token based on auth type", "response": "def _set_token(self):\n        \"\"\"Set the Cerberus token based on auth type\"\"\"\n        try:\n            self.token = os.environ['CERBERUS_TOKEN']\n            if self.verbose:\n                print(\"Overriding Cerberus token with environment variable.\", file=sys.stderr)\n            logger.info(\"Overriding Cerberus token with environment variable.\")\n            return\n        except:\n            pass\n        if self.username:\n            ua = UserAuth(self.cerberus_url, self.username, self.password)\n            self.token = ua.get_token()\n        else:\n            awsa = AWSAuth(self.cerberus_url, region=self.region, aws_session=self.aws_session, verbose=self.verbose)\n            self.token = awsa.get_token()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_roles(self):\n        roles_resp = get_with_retry(self.cerberus_url + '/v1/role',\n                                  headers=self.HEADERS)\n\n        throw_if_bad_response(roles_resp)\n        return roles_resp.json()", "response": "Get all the roles that can be granted to a safe deposit box."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_role(self, key):\n\n        json_resp = self.get_roles()\n        for item in json_resp:\n            if key in item[\"name\"]:\n                return item[\"id\"]\n        raise CerberusClientException(\"Key '%s' not found\" % key)", "response": "Return id of named role."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsimplifies version of get_roles that returns a dict of just name : id for the roles", "response": "def list_roles(self):\n        \"\"\"Simplified version of get_roles that returns a dict of just name: id for the roles\"\"\"\n\n        json_resp = self.get_roles()\n        temp_dict = {}\n        for item in json_resp:\n            temp_dict[item[\"name\"]] = item[\"id\"]\n        return temp_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of categories that a safe deposit box can belong to", "response": "def get_categories(self):\n        \"\"\" Return a list of categories that a safe deposit box can belong to\"\"\"\n        sdb_resp = get_with_retry(self.cerberus_url + '/v1/category',\n                                headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n        return sdb_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new safe deposit box.", "response": "def create_sdb(self, name, category_id, owner, description=\"\", user_group_permissions=None,\n                   iam_principal_permissions=None):\n        \"\"\"Create a safe deposit box.\n\n        You need to refresh your token before the iam role is granted permission to the new safe deposit box.\n        Keyword arguments:\n            name (string) -- name of the safe deposit box\n            category_id (string) -- category id that determines where to store the sdb. (ex: shared, applications)\n            owner (string) -- AD group that owns the safe deposit box\n            description (string) -- Description of the safe deposit box\n            user_group_permissions (list) -- list of dictionaries containing the key name and maybe role_id\n            iam_principal_permissions (list) -- list of dictionaries containing the key name iam_principal_arn\n            and role_id\n        \"\"\"\n\n        # Do some sanity checking\n        if user_group_permissions is None:\n            user_group_permissions = []\n        if iam_principal_permissions is None:\n            iam_principal_permissions = []\n        if list != type(user_group_permissions):\n            raise(TypeError('Expected list, but got ' + str(type(user_group_permissions))))\n        if list != type(iam_principal_permissions):\n            raise(TypeError('Expected list, but got ' + str(type(iam_principal_permissions))))\n        temp_data = {\n            \"name\": name,\n            \"description\": description,\n            \"category_id\": category_id,\n            \"owner\": owner,\n        }\n        if len(user_group_permissions) > 0:\n            temp_data[\"user_group_permissions\"] = user_group_permissions\n        if len(iam_principal_permissions) > 0:\n            temp_data[\"iam_principal_permissions\"] = iam_principal_permissions\n        data = json.encoder.JSONEncoder().encode(temp_data)\n        sdb_resp = post_with_retry(self.cerberus_url + '/v2/safe-deposit-box', data=str(data), headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n        return sdb_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes a safe deposit box specified by id", "response": "def delete_sdb(self, sdb_id):\n        \"\"\" Delete a safe deposit box specified by id\n\n        Keyword arguments:\n        sdb_id -- this is the id of the safe deposit box, not the path.\"\"\"\n        sdb_resp = delete_with_retry(self.cerberus_url + '/v2/safe-deposit-box/' + sdb_id,\n                                   headers=self.HEADERS)\n        throw_if_bad_response(sdb_resp)\n        return sdb_resp"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_sdb_path(self, sdb):\n        sdb_id = self.get_sdb_id(sdb)\n        sdb_resp = get_with_retry(\n            self.cerberus_url + '/v1/safe-deposit-box/' + sdb_id + '/',\n            headers=self.HEADERS\n        )\n\n        throw_if_bad_response(sdb_resp)\n\n        return sdb_resp.json()['path']", "response": "Return the path for a SDB"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the keys for a SDB which are need for the full secure data path", "response": "def get_sdb_keys(self, path):\n        \"\"\"Return the keys for a SDB, which are need for the full secure data path\"\"\"\n        list_resp = get_with_retry(\n            self.cerberus_url + '/v1/secret/' + path + '/?list=true',\n            headers=self.HEADERS\n        )\n\n        throw_if_bad_response(list_resp)\n\n        return list_resp.json()['data']['keys']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the ID for the given safe deposit box.", "response": "def get_sdb_id(self, sdb):\n        \"\"\" Return the ID for the given safe deposit box.\n\n        Keyword arguments:\n        sdb -- This is the name of the safe deposit box, not the path\"\"\"\n        json_resp = self.get_sdbs()\n        for r in json_resp:\n            if r['name'] == sdb:\n                return str(r['id'])\n\n        # If we haven't returned yet then we didn't find what we were\n        # looking for.\n        raise CerberusClientException(\"'%s' not found\" % sdb)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives the path return the ID for the given safe deposit box.", "response": "def get_sdb_id_by_path(self, sdb_path):\n        \"\"\" Given the path, return the ID for the given safe deposit box.\"\"\"\n        json_resp = self.get_sdbs()\n\n        # Deal with the supplied path possibly missing an ending slash\n        path = self._add_slash(sdb_path)\n\n        for r in json_resp:\n            if r['path'] == path:\n                return str(r['id'])\n\n        # If we haven't returned yet then we didn't find what we were\n        # looking for.\n        raise CerberusClientException(\"'%s' not found\" % sdb_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the details for a given safe deposit box id", "response": "def get_sdb_by_id(self, sdb_id):\n        \"\"\" Return the details for the given safe deposit box id\n\n        Keyword arguments:\n        sdb_id -- this is the id of the safe deposit box, not the path.\n        \"\"\"\n        sdb_resp = get_with_retry(self.cerberus_url + '/v2/safe-deposit-box/' + sdb_id,\n                                headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n\n        return sdb_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_sdb_secret_version_paths(self, sdb_id):\n        sdb_resp = get_with_retry(str.join('', [self.cerberus_url, '/v1/sdb-secret-version-paths/', sdb_id]),\n                                headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n\n        return sdb_resp.json()", "response": "This function returns the SDB secret version paths. This function takes the sdb_id and returns the JSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_sdbs(self):\n        sdb_raw = self.get_sdbs()\n        sdbs = []\n        for s in sdb_raw:\n            sdbs.append(s['name'])\n\n        return sdbs", "response": "Return a list of all sdbs by Name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates a safe deposit box.", "response": "def update_sdb(self, sdb_id, owner=None, description=None, user_group_permissions=None,\n                   iam_principal_permissions=None):\n        \"\"\"\n        Update a safe deposit box.\n\n        Keyword arguments:\n\n            owner (string) -- AD group that owns the safe deposit box\n            description (string) -- Description of the safe deposit box\n            user_group_permissions (list) -- list of dictionaries containing the key name and maybe role_id\n            iam_principal_permissions (list) -- list of dictionaries containing the key name iam_principal_arn\n            and role_id\n        \"\"\"\n        # Grab current data\n        old_data = self.get_sdb_by_id(sdb_id)\n\n        # Assemble information to update\n        temp_data = {}\n        keys = ('owner', 'description', 'iam_principal_permissions', 'user_group_permissions')\n        for k in keys:\n            if k in old_data:\n                temp_data[k] = old_data[k]\n        if owner is not None:\n            temp_data[\"owner\"] = owner\n        if description is not None:\n            temp_data[\"description\"] = description\n        if user_group_permissions is not None and len(user_group_permissions) > 0:\n            temp_data[\"user_group_permissions\"] = user_group_permissions\n        if iam_principal_permissions is not None and len(iam_principal_permissions) > 0:\n            temp_data[\"iam_principal_permissions\"] = iam_principal_permissions\n\n        data = json.encoder.JSONEncoder().encode(temp_data)\n        sdb_resp = put_with_retry(self.cerberus_url + '/v2/safe-deposit-box/' + sdb_id, data=str(data),\n                                headers=self.HEADERS)\n\n        throw_if_bad_response(sdb_resp)\n        return sdb_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes a file at the given secure data path", "response": "def delete_file(self, secure_data_path):\n        \"\"\"Delete a file at the given secure data path\"\"\"\n        secret_resp = delete_with_retry(self.cerberus_url + '/v1/secure-file/' + secure_data_path,\n                                      headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_file_metadata(self, secure_data_path, version=None):\n        if not version:\n            version = \"CURRENT\"\n\n        payload = {'versionId': str(version)}\n        secret_resp = head_with_retry(str.join('', [self.cerberus_url, '/v1/secure-file/', secure_data_path]),\n                                    params=payload, headers=self.HEADERS)\n\n        throw_if_bad_response(secret_resp)\n\n        return secret_resp.headers", "response": "Get just the metadata for a file"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the header metadata to pull out the filename and store it under the key filename.", "response": "def _parse_metadata_filename(self, metadata):\n        \"\"\"\n        Parse the header metadata to pull out the filename and then store it under the key 'filename'\n        \"\"\"\n        index = metadata['Content-Disposition'].index('=')+1\n        metadata['filename'] = metadata['Content-Disposition'][index:].replace('\"', '')\n        return metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a file and the metadata and header information around it.", "response": "def get_file(self, secure_data_path, version=None):\n        \"\"\"\n        Return a requests.structures.CaseInsensitiveDict object containing a file and the\n        metadata/header information around it.\n\n        The binary data of the file is under the key 'data'\n        \"\"\"\n\n        query = self._get_file(secure_data_path, version)\n        resp = query.headers.copy()\n        resp = self._parse_metadata_filename(resp)\n        resp['data'] = query.content\n\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the versions of a particular file in the safety deposit box", "response": "def get_file_versions(self, secure_data_path, limit=None, offset=None):\n        \"\"\"\n        Get versions of a particular file\n        This is just a shim to get_secret_versions\n\n        secure_data_path -- full path to the file in the safety deposit box\n        limit -- Default(100), limits how many records to be returned from the api at once.\n        offset -- Default(0), used for pagination.  Will request records from the given offset.\n        \"\"\"\n\n        return self.get_secret_versions(secure_data_path, limit, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_all_file_version_ids(self, secure_data_path, limit=None):\n\n        offset = 0\n        # Prime the versions dictionary so that all the logic can happen in the loop\n        versions = {'has_next': True, 'next_offset': 0}\n        while (versions['has_next']):\n            offset = versions['next_offset']\n            versions = self.get_file_versions(secure_data_path, limit, offset)\n            for summary in versions['secure_data_version_summaries']:\n                yield summary", "response": "Returns a generator that yields all the file version ids that are available for the file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_all_file_versions(self, secure_data_path, limit=None):\n        for secret in self._get_all_file_version_ids(secure_data_path, limit):\n            yield {'secret': self.get_file_data(secure_data_path, version=secret['id']),\n                   'version': secret}", "response": "Returns a generator that yields all the file versions of the file and its version info"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of files in the path.", "response": "def list_files(self, secure_data_path, limit=None, offset=None):\n        \"\"\"Return the list of files in the path.  May need to be paginated\"\"\"\n\n        # Make sure that limit and offset are in range.\n        # Set the normal defaults\n        if not limit or limit <= 0:\n            limit = 100\n\n        if not offset or offset < 0:\n            offset = 0\n\n        payload = {'limit': str(limit), 'offset': str(offset)}\n\n        # Because of the addition of versionId and the way URLs are constructed, secure_data_path should\n        #  always end in a '/'.\n        secure_data_path = self._add_slash(secure_data_path)\n        secret_resp = get_with_retry(self.cerberus_url + '/v1/secure-files/' + secure_data_path,\n                                   params=payload, headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_file(self, secure_data_path, filehandle, content_type=None):\n\n        # Parse out the filename from the path\n        filename = secure_data_path.rsplit('/', 1)\n\n        if content_type:\n            data = {'file-content': (filename, filehandle, content_type)}\n        else:\n            data = {'file-content': (filename, filehandle)}\n\n        headers = self.HEADERS.copy()\n        if 'Content-Type' in headers:\n            headers.__delitem__('Content-Type')\n\n        secret_resp = post_with_retry(self.cerberus_url + '/v1/secure-file/' + secure_data_path,\n                                    files=data, headers=headers)\n        throw_if_bad_response(secret_resp)\n        return secret_resp", "response": "This method uploads a file to a secure data path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_secret(self, secure_data_path, key, version=None):\n        warnings.warn(\n            \"get_secret is deprecated, use get_secrets_data instead\",\n            DeprecationWarning\n        )\n        secret_resp_json = self._get_secrets(secure_data_path, version)\n\n        if key in secret_resp_json['data']:\n            return secret_resp_json['data'][key]\n        else:\n            raise CerberusClientException(\"Key '%s' not found\" % key)", "response": "Get a secret from the Cerberus server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_secrets(self, secure_data_path, version=None):\n        if not version:\n            version = \"CURRENT\"\n        payload = {'versionId': str(version)}\n        secret_resp = get_with_retry(str.join('', [self.cerberus_url, '/v1/secret/', secure_data_path]),\n                                   params=payload, headers=self.HEADERS)\n\n        throw_if_bad_response(secret_resp)\n\n        return secret_resp.json()", "response": "Returns full json secrets based on the secure data path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_secrets(self, secure_data_path, version=None):\n        warnings.warn(\n            \"get_secrets is deprecated, use get_secrets_data instead\",\n            DeprecationWarning\n        )\n        return self._get_secrets(secure_data_path, version)", "response": "Deprecated method to get json secrets based on the secure data path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets versions of a particular secret key.", "response": "def get_secret_versions(self, secure_data_path, limit=None, offset=None):\n        \"\"\"\n        Get versions of a particular secret key\n\n        secure_data_path -- full path to the key in the safety deposit box\n        limit -- Default(100), limits how many records to be returned from the api at once.\n        offset -- Default(0), used for pagination.  Will request records from the given offset.\n        \"\"\"\n\n        # Make sure that limit and offset are in range.\n        # Set the normal defaults\n        if not limit or limit <= 0:\n            limit = 100\n\n        if not offset or offset < 0:\n            offset = 0\n\n        payload = {'limit': str(limit), 'offset': str(offset)}\n        secret_resp = get_with_retry(str.join('', [self.cerberus_url, '/v1/secret-versions/', secure_data_path]),\n                                   params=payload, headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_all_secret_version_ids(self, secure_data_path, limit=None):\n\n        offset = 0\n        # Prime the versions dictionary so that all the logic can happen in the loop\n        versions = {'has_next': True, 'next_offset': 0}\n        while (versions['has_next']):\n            offset = versions['next_offset']\n            versions = self.get_secret_versions(secure_data_path, limit, offset)\n            for summary in versions['secure_data_version_summaries']:\n                yield summary", "response": "Returns a generator that returns all the secret version ids that are available for the given secure data path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a generator that yields all the secrets and their versions", "response": "def _get_all_secret_versions(self, secure_data_path, limit=None):\n        \"\"\"\n        Convenience function that returns a generator yielding the contents of secrets and their version info\n        secure_data_path -- full path to the key in the safety deposit box\n        limit -- Default(100), limits how many records to be returned from the api at once.\n        \"\"\"\n        for secret in self._get_all_secret_version_ids(secure_data_path, limit):\n            yield {'secret': self.get_secrets_data(secure_data_path, version=secret['id']),\n                   'version': secret}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn json secrets based on the secure_data_path", "response": "def list_secrets(self, secure_data_path):\n        \"\"\"Return json secrets based on the secure_data_path, this will list keys in a folder\"\"\"\n\n        # Because of the addition of versionId and the way URLs are constructed, secure_data_path should\n        #  always end in a '/'.\n        secure_data_path = self._add_slash(secure_data_path)\n        secret_resp = get_with_retry(self.cerberus_url + '/v1/secret/' + secure_data_path + '?list=true',\n                                   headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite secret to a secure data path", "response": "def put_secret(self, secure_data_path, secret, merge=True):\n        \"\"\"Write secret(s) to a secure data path provided a dictionary of key/values\n\n        Keyword arguments:\n        secure_data_path -- full path in the safety deposit box that contains the key\n        secret -- A dictionary containing key/values to be written at the secure data path\n        merge -- Boolean that determines if the provided secret keys should be merged with\n            the values already present at the secure data path.  If False the keys will\n            completely overwrite what was stored at the secure data path. (default True)\n        \"\"\"\n        # json encode the input.  Cerberus is sensitive to double vs single quotes.\n        # an added bonus is that json encoding transforms python2 unicode strings\n        # into a compatible format.\n        data = json.encoder.JSONEncoder().encode(secret)\n        if merge:\n            data = self.secret_merge(secure_data_path, secret)\n        secret_resp = post_with_retry(self.cerberus_url + '/v1/secret/' + secure_data_path,\n                                    data=str(data), headers=self.HEADERS)\n        throw_if_bad_response(secret_resp)\n        return secret_resp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef secret_merge(self, secure_data_path, key):\n        get_resp = get_with_retry(self.cerberus_url + '/v1/secret/' + secure_data_path, headers=self.HEADERS)\n        temp_key = {}\n        # Ignore a return of 404 since it means the key might not exist\n        if get_resp.status_code == requests.codes.bad and get_resp.status_code not in [403, 404]:\n            throw_if_bad_response(get_resp)\n        elif get_resp.status_code in [403, 404]:\n            temp_key = {}\n        else:\n            temp_key = get_resp.json()['data']\n        # Allow key to be either a string describing a dict or a dict.\n        if type(key) == str:\n            temp_key.update(ast.literal_eval(key))\n        else:\n            temp_key.update(key)\n        # This is a bit of a hack to get around python 2 treating unicode strings\n        # differently.  Cerberus will throw a 400 if we try to post python 2 style\n        # unicode stings as the payload.\n        combined_key = json.encoder.JSONEncoder().encode(temp_key)\n        return combined_key", "response": "Compare key and values at secure_data_path and merges them."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_v4_signed_headers(self):\n        if self.aws_session is None:\n            boto_session = session.Session()\n            creds = boto_session.get_credentials()\n        else:\n            creds = self.aws_session.get_credentials()\n        if creds is None:\n            raise CerberusClientException(\"Unable to locate AWS credentials\")\n        readonly_credentials = creds.get_frozen_credentials()\n\n        # hardcode get-caller-identity request\n        data = OrderedDict((('Action','GetCallerIdentity'), ('Version', '2011-06-15')))\n        url = 'https://sts.{}.amazonaws.com/'.format(self.region)\n        request_object = awsrequest.AWSRequest(method='POST', url=url, data=data)\n\n        signer = auth.SigV4Auth(readonly_credentials, 'sts', self.region)\n        signer.add_auth(request_object)\n        return request_object.headers", "response": "Returns V4 signed get - caller - identity request headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_token(self):\n        signed_headers = self._get_v4_signed_headers()\n        for header in self.HEADERS:\n            signed_headers[header] = self.HEADERS[header]\n\n        resp = post_with_retry(self.cerberus_url + '/v2/auth/sts-identity', headers=signed_headers)\n        throw_if_bad_response(resp)\n\n        token = resp.json()['client_token']\n        iam_principal_arn = resp.json()['metadata']['aws_iam_principal_arn']\n        if self.verbose:\n            print('Successfully authenticated with Cerberus as {}'.format(iam_principal_arn), file=sys.stderr)\n        logger.info('Successfully authenticated with Cerberus as {}'.format(iam_principal_arn))\n\n        return token", "response": "Returns a client token from Cerberus"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tiff_header(read_buffer):\n    # First 8 should be (73, 73, 42, 8) or (77, 77, 42, 8)\n    data = struct.unpack('BB', read_buffer[0:2])\n    if data[0] == 73 and data[1] == 73:\n        # little endian\n        endian = '<'\n    elif data[0] == 77 and data[1] == 77:\n        # big endian\n        endian = '>'\n    else:\n        msg = (\"The byte order indication in the TIFF header ({byte_order}) \"\n               \"is invalid.  It should be either {little_endian} or \"\n               \"{big_endian}.\")\n        msg = msg.format(byte_order=read_buffer[6:8],\n                         little_endian=bytes([73, 73]),\n                         big_endian=bytes([77, 77]))\n        raise IOError(msg)\n\n    _, offset = struct.unpack(endian + 'HI', read_buffer[2:8])\n\n    # This is the 'Exif Image' portion.\n    exif = ExifImageIfd(endian, read_buffer, offset)\n    return exif.processed_ifd", "response": "Interpret the uuid raw data as a tiff header."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninterpret an Exif image tag data payload.", "response": "def parse_tag(self, dtype, count, offset_buf):\n        \"\"\"Interpret an Exif image tag data payload.\n        \"\"\"\n        try:\n            fmt = self.datatype2fmt[dtype][0] * count\n            payload_size = self.datatype2fmt[dtype][1] * count\n        except KeyError:\n            msg = 'Invalid TIFF tag datatype ({0}).'.format(dtype)\n            raise IOError(msg)\n\n        if payload_size <= 4:\n            # Interpret the payload from the 4 bytes in the tag entry.\n            target_buffer = offset_buf[:payload_size]\n        else:\n            # Interpret the payload at the offset specified by the 4 bytes in\n            # the tag entry.\n            offset, = struct.unpack(self.endian + 'I', offset_buf)\n            target_buffer = self.read_buffer[offset:offset + payload_size]\n\n        if dtype == 2:\n            # ASCII\n            payload = target_buffer.decode('utf-8').rstrip('\\x00')\n\n        else:\n            payload = struct.unpack(self.endian + fmt, target_buffer)\n            if dtype == 5 or dtype == 10:\n                # Rational or Signed Rational.  Construct the list of values.\n                rational_payload = []\n                for j in range(count):\n                    value = float(payload[j * 2]) / float(payload[j * 2 + 1])\n                    rational_payload.append(value)\n                payload = rational_payload\n            if count == 1:\n                # If just a single value, then return a scalar instead of a\n                # tuple.\n                payload = payload[0]\n\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap the tag name instead of tag number to the tag value.", "response": "def post_process(self, tagnum2name):\n        \"\"\"Map the tag name instead of tag number to the tag value.\n        \"\"\"\n        for tag, value in self.raw_ifd.items():\n            try:\n                tag_name = tagnum2name[tag]\n            except KeyError:\n                # Ok, we don't recognize this tag.  Just use the numeric id.\n                msg = 'Unrecognized Exif tag ({tag}).'.format(tag=tag)\n                warnings.warn(msg, UserWarning)\n                tag_name = tag\n            self.processed_ifd[tag_name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all(self):\n\n        response = self._get_json(HTTPMethods.GET, self.saved_query_url, self._get_master_key())\n\n        return response", "response": "Gets all saved queries for a project from the Keen IO API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a single saved query for a project from the Keen IO API given a query name.", "response": "def get(self, query_name):\n        \"\"\"\n        Gets a single saved query for a project from the Keen IO API given a\n        query name.\n        Master key must be set.\n        \"\"\"\n\n        url = \"{0}/{1}\".format(self.saved_query_url, query_name)\n        response = self._get_json(HTTPMethods.GET, url, self._get_master_key())\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a single saved query with a result object for a project given a query name.", "response": "def results(self, query_name):\n        \"\"\"\n        Gets a single saved query with a 'result' object for a project from the\n        Keen IO API given a query name.\n        Read or Master key must be set.\n        \"\"\"\n\n        url = \"{0}/{1}/result\".format(self.saved_query_url, query_name)\n        response = self._get_json(HTTPMethods.GET, url, self._get_read_key())\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Keen IO Saved Query.", "response": "def create(self, query_name, saved_query):\n        \"\"\"\n        Creates the saved query via a PUT request to Keen IO Saved Query endpoint.\n        Master key must be set.\n        \"\"\"\n        url = \"{0}/{1}\".format(self.saved_query_url, query_name)\n        payload = saved_query\n\n        # To support clients that may have already called dumps() to work around how this used to\n        # work, make sure it's not a str. Hopefully it's some sort of mapping. When we actually\n        # try to send the request, client code will get an InvalidJSONError if payload isn't\n        # a json-formatted string.\n        if not isinstance(payload, str):\n            payload = json.dumps(saved_query)\n\n        response = self._get_json(HTTPMethods.PUT, url, self._get_master_key(), data=payload)\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, query_name, saved_query_attributes):\n\n        query_name_attr_name = \"query_name\"\n        refresh_rate_attr_name = \"refresh_rate\"\n        query_attr_name = \"query\"\n        metadata_attr_name = \"metadata\"\n\n        old_saved_query = self.get(query_name)\n\n        # Create a new query def to send back. We cannot send values for attributes like 'urls',\n        # 'last_modified_date', 'run_information', etc.\n        new_saved_query = {\n            query_name_attr_name: old_saved_query[query_name_attr_name], # expected\n            refresh_rate_attr_name: old_saved_query[refresh_rate_attr_name], # expected\n            query_attr_name: {}\n        }\n\n        # If metadata was set, preserve it. The Explorer UI currently stores information here.\n        old_metadata = (old_saved_query[metadata_attr_name]\n                       if metadata_attr_name in old_saved_query\n                       else None)\n\n        if old_metadata:\n            new_saved_query[metadata_attr_name] = old_metadata\n\n        # Preserve any non-empty properties of the existing query. We get back values like None\n        # for 'group_by', 'interval' or 'timezone', but those aren't accepted values when updating.\n        old_query = old_saved_query[query_attr_name] # expected\n\n        # Shallow copy since we want the entire object heirarchy to start with.\n        for (key, value) in six.iteritems(old_query):\n            if value:\n                new_saved_query[query_attr_name][key] = value\n\n        # Now, recursively overwrite any attributes passed in.\n        SavedQueriesInterface._deep_update(new_saved_query, saved_query_attributes)\n\n        return self.create(query_name, new_saved_query)", "response": "Updates the attributes of the specified query_name with the values from the given dict of attributes to be updated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a saved query from a project with a query name.", "response": "def delete(self, query_name):\n        \"\"\"\n        Deletes a saved query from a project with a query name.\n        Master key must be set.\n        \"\"\"\n\n        url = \"{0}/{1}\".format(self.saved_query_url, query_name)\n        self._get_json(HTTPMethods.DELETE, url, self._get_master_key())\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npad an input string to a given block size.", "response": "def pad_aes256(s):\n    \"\"\"\n    Pads an input string to a given block size.\n    :param s: string\n    :returns: The padded string.\n    \"\"\"\n    if len(s) % AES.block_size == 0:\n        return s\n\n    return Padding.appendPadding(s, blocksize=AES.block_size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unpad_aes256(s):\n    if not s:\n        return s\n\n    try:\n        return Padding.removePadding(s, blocksize=AES.block_size)\n    except AssertionError:\n        # if there's an error while removing padding, just return s.\n        return s", "response": "Removes padding from an input string based on a given block size."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef old_pad(s):\n    if len(s) % OLD_BLOCK_SIZE == 0:\n        return s\n\n    return Padding.appendPadding(s, blocksize=OLD_BLOCK_SIZE)", "response": "Pads an input string to a given block size."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef old_unpad(s):\n    if not s:\n        return s\n\n    try:\n        return Padding.removePadding(s, blocksize=OLD_BLOCK_SIZE)\n    except AssertionError:\n        # if there's an error while removing padding, just return s.\n        return s", "response": "Removes padding from an input string based on a given block size."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef encode_aes256(key, plaintext):\n    if len(key) != 64:\n        raise TypeError(\"encode_aes256() expects a 256 bit key encoded as a 64 hex character string\")\n\n    # generate AES.block_size cryptographically secure random bytes for our IV (initial value)\n    iv = os.urandom(AES.block_size)\n    # set up an AES cipher object\n    cipher = AES.new(binascii.unhexlify(key.encode('ascii')), mode=AES.MODE_CBC, IV=iv)\n    # encrypt the plaintext after padding it\n    ciphertext = cipher.encrypt(ensure_bytes(pad_aes256(plaintext)))\n    # append the hexed IV and the hexed ciphertext\n    iv_plus_encrypted = binascii.hexlify(iv) + binascii.hexlify(ciphertext)\n    # return that\n    return iv_plus_encrypted", "response": "This function encodes some given plaintext with the given key and returns the ciphertext of the ciphertext."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef old_encode_aes(key, plaintext):\n    # generate 16 cryptographically secure random bytes for our IV (initial value)\n    iv = os.urandom(16)\n    # set up an AES cipher object\n    cipher = AES.new(ensure_bytes(old_pad(key)), mode=AES.MODE_CBC, IV=iv)\n    # encrypte the plaintext after padding it\n    ciphertext = cipher.encrypt(ensure_bytes(old_pad(plaintext)))\n    # append the hexed IV and the hexed ciphertext\n    iv_plus_encrypted = binascii.hexlify(iv) + binascii.hexlify(ciphertext)\n    # return that\n    return iv_plus_encrypted", "response": "This method encrypts some given plaintext with the given key and returns the ciphertext of the encrypted plaintext."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef old_decode_aes(key, iv_plus_encrypted):\n    # grab first 16 bytes (aka 32 characters of hex) - that's the IV\n    hexed_iv = iv_plus_encrypted[:32]\n    # grab everything else - that's the ciphertext (aka encrypted message)\n    hexed_ciphertext = iv_plus_encrypted[32:]\n    # unhex the iv and ciphertext\n    iv = binascii.unhexlify(hexed_iv)\n    ciphertext = binascii.unhexlify(hexed_ciphertext)\n    # set up the correct AES cipher object\n    cipher = AES.new(ensure_bytes(old_pad(key)), mode=AES.MODE_CBC, IV=iv)\n    # decrypt!\n    plaintext = cipher.decrypt(ciphertext)\n    # return the unpadded version of this\n    return old_unpad(plaintext)", "response": "Utility method to decode a payload consisting of the hexed IV + the hexed ciphertext using the given key."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns auth response which has client token unless MFA is required", "response": "def get_auth(self):\n        \"\"\"Returns auth response which has client token unless MFA is required\"\"\"\n        auth_resp = get_with_retry(self.cerberus_url + '/v2/auth/user',\n                                 auth=(self.username, self.password),\n                                 headers=self.HEADERS)\n\n        if auth_resp.status_code != 200:\n            throw_if_bad_response(auth_resp)\n\n        return auth_resp.json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_token(self):\n        auth_resp = self.get_auth()\n\n        if auth_resp['status'] == 'mfa_req':\n            token_resp = self.get_mfa(auth_resp)\n        else:\n            token_resp = auth_resp\n\n        token = token_resp['data']['client_token']['client_token']\n        return token", "response": "sets client token from Cerberus"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_mfa(self, auth_resp):\n        devices = auth_resp['data']['devices']\n        if len(devices) == 1:\n            # If there's only one option, don't show selection prompt\n            selection = \"0\"\n            x = 1\n        else:\n            print(\"Found the following MFA devices\")\n            x=0\n            for device in devices:\n                print(\"{0}: {1}\".format(x, device['name']))\n                x = x + 1\n\n            selection = input(\"Enter a selection: \")\n        if selection.isdigit():\n            selection_num=int(str(selection))\n        else:\n            raise CerberusClientException( str.join('', [\"Selection: '\", selection,\"' is not a number\"]))\n\n        if (selection_num >= x) or (selection_num < 0):\n            raise CerberusClientException(str.join('', [\"Selection: '\", str(selection_num), \"' is out of range\"]))\n\n        sec_code = input('Enter ' + auth_resp['data']['devices'][selection_num]['name'] + ' security code: ')\n\n        mfa_resp = post_with_retry(\n            self.cerberus_url + '/v2/auth/mfa_check',\n            json={'otp_token': sec_code,\n                  'device_id': auth_resp['data']['devices'][selection_num]['id'],\n                  'state_token': auth_resp['data']['state_token']},\n            headers=self.HEADERS\n        )\n\n        if mfa_resp.status_code != 200:\n            throw_if_bad_response(mfa_resp)\n\n        return mfa_resp.json()", "response": "Gets MFA code from user and returns response which includes the client token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_standard_flag(read_buffer, mask_length):\n    # The mask length tells us the format string to use when unpacking\n    # from the buffer read from file.\n    mask_format = {1: 'B', 2: 'H', 4: 'I'}[mask_length]\n\n    num_standard_flags, = struct.unpack_from('>H', read_buffer, offset=0)\n\n    # Read in standard flags and standard masks.  Each standard flag should\n    # be two bytes, but the standard mask flag is as long as specified by\n    # the mask length.\n    fmt = '>' + ('H' + mask_format) * num_standard_flags\n    data = struct.unpack_from(fmt, read_buffer, offset=2)\n\n    standard_flag = data[0:num_standard_flags * 2:2]\n    standard_mask = data[1:num_standard_flags * 2:2]\n\n    return standard_flag, standard_mask", "response": "Construct standard flag and standard mask data from the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs vendor features and vendor mask data from the buffer.", "response": "def _parse_vendor_features(read_buffer, mask_length):\n    \"\"\"Construct vendor features, vendor mask data from the file.\n\n    Specifically working on Reader Requirements box.\n\n    Parameters\n    ----------\n    fptr : file object\n        File object for JP2K file.\n    mask_length : int\n        Length of vendor mask flag\n    \"\"\"\n    # The mask length tells us the format string to use when unpacking\n    # from the buffer read from file.\n    mask_format = {1: 'B', 2: 'H', 4: 'I'}[mask_length]\n\n    num_vendor_features, = struct.unpack_from('>H', read_buffer)\n\n    # Each vendor feature consists of a 16-byte UUID plus a mask whose\n    # length is specified by, you guessed it, \"mask_length\".\n    entry_length = 16 + mask_length\n    vendor_feature = []\n    vendor_mask = []\n    for j in range(num_vendor_features):\n        uslice = slice(2 + j * entry_length, 2 + (j + 1) * entry_length)\n        ubuffer = read_buffer[uslice]\n        vendor_feature.append(UUID(bytes=ubuffer[0:16]))\n\n        vmask = struct.unpack('>' + mask_format, ubuffer[16:])\n        vendor_mask.append(vmask)\n\n    return vendor_feature, vendor_mask"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dispatch_validation_error(self, msg, writing=False):\n        if writing:\n            raise IOError(msg)\n        else:\n            warnings.warn(msg)", "response": "Dispatches a validation error depending on circumstance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _indent(self, textstr, indent_level=4):\n        if sys.hexversion >= 0x03030000:\n            return textwrap.indent(textstr, ' ' * indent_level)\n        else:\n            lst = [(' ' * indent_level + x) for x in textstr.split('\\n')]\n            return '\\n'.join(lst)", "response": "Indent a string.\n\n        Textwrap's indent method only exists for 3.3 or above.  In 2.7 we have\n        to fake it.\n\n        Parameters\n        ----------\n        textstring : str\n            String to be indented.\n        indent_level : str\n            Number of spaces of indentation to add.\n\n        Returns\n        -------\n        str\n            Possibly multi-line string indented by the specified amount."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting a superbox. Parameters ---------- fptr : file or file object Superbox (box of boxes) to be written to this file. box_id : bytes 4-byte sequence that identifies the superbox.", "response": "def _write_superbox(self, fptr, box_id):\n        \"\"\"Write a superbox.\n\n        Parameters\n        ----------\n        fptr : file or file object\n            Superbox (box of boxes) to be written to this file.\n        box_id : bytes\n            4-byte sequence that identifies the superbox.\n        \"\"\"\n        # Write the contained boxes, then come back and write the length.\n        orig_pos = fptr.tell()\n        fptr.write(struct.pack('>I4s', 0, box_id))\n        for box in self.box:\n            box.write(fptr)\n\n        end_pos = fptr.tell()\n        fptr.seek(orig_pos)\n        fptr.write(struct.pack('>I', end_pos - orig_pos))\n        fptr.seek(end_pos)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the current box.", "response": "def _parse_this_box(self, fptr, box_id, start, num_bytes):\n        \"\"\"Parse the current box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object, currently points to start of box payload, not the\n            start of the box.\n        box_id : str\n            4-letter identifier for the current box.\n        start, num_bytes : int\n            Byte offset and length of the current box.\n\n        Returns\n        -------\n        Jp2kBox\n            Object corresponding to the current box.\n        \"\"\"\n        try:\n            parser = _BOX_WITH_ID[box_id].parse\n\n        except KeyError:\n            # We don't recognize the box ID, so create an UnknownBox and be\n            # done with it.\n            msg = ('Unrecognized box ({box_id}) encountered at byte offset '\n                   '{offset}.')\n            msg = msg.format(box_id=box_id, offset=fptr.tell() - 8)\n            warnings.warn(msg, UserWarning)\n            box = UnknownBox(box_id, offset=start, length=num_bytes,\n                             longname='Unknown')\n\n            return box\n\n        try:\n            box = parser(fptr, start, num_bytes)\n        except ValueError as err:\n            msg = (\"Encountered an unrecoverable ValueError while parsing a \"\n                   \"{box_id} box at byte offset {offset}.  The original error \"\n                   \"message was \\\"{original_error_message}\\\".\")\n            msg = msg.format(box_id=_BOX_WITH_ID[box_id].longname,\n                             offset=start,\n                             original_error_message=str(err))\n            warnings.warn(msg, UserWarning)\n            box = UnknownBox(box_id.decode('utf-8'),\n                             length=num_bytes,\n                             offset=start, longname='Unknown')\n\n        return box"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_superbox(self, fptr):\n\n        superbox = []\n\n        start = fptr.tell()\n\n        while True:\n\n            # Are we at the end of the superbox?\n            if start >= self.offset + self.length:\n                break\n\n            read_buffer = fptr.read(8)\n            if len(read_buffer) < 8:\n                msg = \"Extra bytes at end of file ignored.\"\n                warnings.warn(msg, UserWarning)\n                return superbox\n\n            (box_length, box_id) = struct.unpack('>I4s', read_buffer)\n            if box_length == 0:\n                # The length of the box is presumed to last until the end of\n                # the file.  Compute the effective length of the box.\n                num_bytes = os.path.getsize(fptr.name) - fptr.tell() + 8\n\n            elif box_length == 1:\n                # The length of the box is in the XL field, a 64-bit value.\n                read_buffer = fptr.read(8)\n                num_bytes, = struct.unpack('>Q', read_buffer)\n\n            else:\n                # The box_length value really is the length of the box!\n                num_bytes = box_length\n\n            box = self._parse_this_box(fptr, box_id, start, num_bytes)\n\n            superbox.append(box)\n\n            # Position to the start of the next box.\n            if num_bytes > self.length:\n                # Length of the current box goes past the end of the\n                # enclosing superbox.\n                msg = '{0} box has incorrect box length ({1})'\n                msg = msg.format(box_id, num_bytes)\n                warnings.warn(msg)\n            elif fptr.tell() > start + num_bytes:\n                # The box must be invalid somehow, as the file pointer is\n                # positioned past the end of the box.\n                msg = ('{box_id} box may be invalid, the file pointer is '\n                       'positioned {num_bytes} bytes past the end of the box.')\n                msg = msg.format(box_id=box_id,\n                                 num_bytes=fptr.tell() - (start + num_bytes))\n                warnings.warn(msg, UserWarning)\n            fptr.seek(start + num_bytes)\n\n            start += num_bytes\n\n        return superbox", "response": "Parse a superbox (box consisting of nothing but other boxes.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        list\n            List of top-level boxes in the JPEG 2000 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that the box obeys the specifications.", "response": "def _validate(self, writing=False):\n        \"\"\"Verify that the box obeys the specifications.\"\"\"\n        if self.colorspace is not None and self.icc_profile is not None:\n            msg = (\"Colorspace and icc_profile cannot both be set when \"\n                   \"creating a ColourSpecificationBox.\")\n            self._dispatch_validation_error(msg, writing=writing)\n\n        if self.method not in _COLORSPACE_METHODS.keys():\n            msg = \"Invalid colorspace method value ({method}).\"\n            msg = msg.format(method=self.method)\n            if writing:\n                raise IOError(msg)\n            else:\n                warnings.warn(msg, UserWarning)\n\n        if self.approximation not in _APPROXIMATION_MEASURES.keys():\n            msg = \"Invalid colr approximation value ({approx}).\"\n            msg = msg.format(approx=self.approximation)\n            if not writing:\n                # Don't bother to check this for the case of writing=True\n                # because it's already handles in the wrapping code.\n                warnings.warn(msg, UserWarning)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _write_validate(self):\n        if self.colorspace is None:\n            msg = (\"Writing colr boxes without enumerated \"\n                   \"colorspaces is not supported at this time.\")\n            self._dispatch_validation_error(msg, writing=True)\n\n        if self.icc_profile is None:\n            if self.colorspace not in [SRGB, GREYSCALE, YCC]:\n                msg = (\"Colorspace should correspond to one of SRGB, \"\n                       \"GREYSCALE, or YCC.\")\n                self._dispatch_validation_error(msg, writing=True)\n\n        self._validate(writing=True)", "response": "In addition to constructor validation steps run validation steps\n        for writing."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, fptr):\n        self._write_validate()\n        length = 15 if self.icc_profile is None else 11 + len(self.icc_profile)\n        fptr.write(struct.pack('>I4s', length, b'colr'))\n\n        read_buffer = struct.pack('>BBBI',\n                                  self.method,\n                                  self.precedence,\n                                  self.approximation,\n                                  self.colorspace)\n        fptr.write(read_buffer)", "response": "Write an Colour Specification box to file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n\n        lst = struct.unpack_from('>BBB', read_buffer, offset=0)\n        method, precedence, approximation = lst\n\n        if method == 1:\n            # enumerated colour space\n            colorspace, = struct.unpack_from('>I', read_buffer, offset=3)\n            if colorspace not in _COLORSPACE_MAP_DISPLAY.keys():\n                msg = \"Unrecognized colorspace ({colorspace}).\"\n                msg = msg.format(colorspace=colorspace)\n                warnings.warn(msg, UserWarning)\n            icc_profile = None\n\n        else:\n            # ICC profile\n            colorspace = None\n            if (num_bytes - 3) < 128:\n                msg = (\"ICC profile header is corrupt, length is \"\n                       \"only {length} when it should be at least 128.\")\n                warnings.warn(msg.format(length=num_bytes - 3), UserWarning)\n                icc_profile = None\n            else:\n                profile = _ICCProfile(read_buffer[3:])\n                icc_profile = profile.header\n\n        return cls(method=method,\n                   precedence=precedence,\n                   approximation=approximation,\n                   colorspace=colorspace,\n                   icc_profile=icc_profile,\n                   length=length,\n                   offset=offset)", "response": "Parse the JPEG 2000 color specification box."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate(self, writing=False):\n        # channel type and association must be specified.\n        if not ((len(self.index) == len(self.channel_type)) and\n                (len(self.channel_type) == len(self.association))):\n            msg = (\"The length of the index ({index}), channel_type \"\n                   \"({channel_type}), and association ({association}) inputs \"\n                   \"must be the same.\")\n            msg = msg.format(index=len(self.index),\n                             channel_type=len(self.channel_type),\n                             association=len(self.association))\n            self._dispatch_validation_error(msg, writing=writing)\n\n        # channel types must be one of 0, 1, 2, 65535\n        if any(x not in [0, 1, 2, 65535] for x in self.channel_type):\n            msg = (\"channel_type specified as {channel_type}, but all values \"\n                   \"must be in the set of\\n\\n\"\n                   \"    0     - colour image data for associated color\\n\"\n                   \"    1     - opacity\\n\"\n                   \"    2     - premultiplied opacity\\n\"\n                   \"    65535 - unspecified\\n\")\n            msg = msg.format(channel_type=self.channel_type)\n            self._dispatch_validation_error(msg, writing=writing)", "response": "Verify that the box obeys the specifications."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, fptr):\n        self._validate(writing=True)\n        num_components = len(self.association)\n        fptr.write(struct.pack('>I4s', 8 + 2 + num_components * 6, b'cdef'))\n        fptr.write(struct.pack('>H', num_components))\n        for j in range(num_components):\n            fptr.write(struct.pack('>' + 'H' * 3,\n                                   self.index[j],\n                                   self.channel_type[j],\n                                   self.association[j]))", "response": "Write a channel definition box to file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n\n        # Read the number of components.\n        num_components, = struct.unpack_from('>H', read_buffer)\n\n        data = struct.unpack_from('>' + 'HHH' * num_components, read_buffer,\n                                  offset=2)\n        index = data[0:num_components * 6:3]\n        channel_type = data[1:num_components * 6:3]\n        association = data[2:num_components * 6:3]\n\n        return cls(index=tuple(index),\n                   channel_type=tuple(channel_type),\n                   association=tuple(association),\n                   length=length, offset=offset)", "response": "Parse the ComponentDefinitionBox\n            box."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _validate(self, writing=True):\n        if any([box.box_id != 'colr' for box in self.box]):\n            msg = (\"Colour group boxes can only contain colour specification \"\n                   \"boxes.\")\n            self._dispatch_validation_error(msg, writing=writing)", "response": "Verify that the box obeys the specifications."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a colour group box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a colour group box to file.\n        \"\"\"\n        self._validate(writing=True)\n        self._write_superbox(fptr, b'cgrp')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, fptr):\n        length = 8 + 4 * len(self.component_index)\n        write_buffer = struct.pack('>I4s', length, b'cmap')\n        fptr.write(write_buffer)\n\n        for j in range(len(self.component_index)):\n            write_buffer = struct.pack('>HBB',\n                                       self.component_index[j],\n                                       self.mapping_type[j],\n                                       self.palette_index[j])\n            fptr.write(write_buffer)", "response": "Write a Component Mapping box to file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the ComponentMappingBox box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse component mapping box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        ComponentMappingBox\n            Instance of the current component mapping box.\n        \"\"\"\n        num_bytes = offset + length - fptr.tell()\n        num_components = int(num_bytes / 4)\n\n        read_buffer = fptr.read(num_bytes)\n        data = struct.unpack('>' + 'HBB' * num_components, read_buffer)\n\n        component_index = data[0:num_bytes:3]\n        mapping_type = data[1:num_bytes:3]\n        palette_index = data[2:num_bytes:3]\n\n        return cls(component_index, mapping_type, palette_index,\n                   length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(cls, fptr, offset=0, length=0):\n        main_header_offset = fptr.tell()\n        if config.get_option('parse.full_codestream'):\n            codestream = Codestream(fptr, length, header_only=False)\n        else:\n            codestream = None\n        box = cls(codestream, main_header_offset=main_header_offset,\n                  length=length, offset=offset)\n        box._filename = fptr.name\n        box._length = length\n        return box", "response": "Parse a contiguous codestream box."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate(self, writing=False):\n        for box in self.DR:\n            if box.box_id != 'url ':\n                msg = ('Child boxes of a data reference box can only be data '\n                       'entry URL boxes.')\n                self._dispatch_validation_error(msg, writing=writing)", "response": "Verify that the data reference box obeys the specifications."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a Data Reference box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a Data Reference box to file.\n        \"\"\"\n        self._write_validate()\n\n        # Very similar to the way a superbox is written.\n        orig_pos = fptr.tell()\n        fptr.write(struct.pack('>I4s', 0, b'dtbl'))\n\n        # Write the number of data entry url boxes.\n        write_buffer = struct.pack('>H', len(self.DR))\n        fptr.write(write_buffer)\n\n        for box in self.DR:\n            box.write(fptr)\n\n        end_pos = fptr.tell()\n        fptr.seek(orig_pos)\n        fptr.write(struct.pack('>I', end_pos - orig_pos))\n        fptr.seek(end_pos)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the data reference box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse data reference box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        DataReferenceBox\n            Instance of the current data reference box.\n        \"\"\"\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n\n        # Read the number of data references\n        ndr, = struct.unpack_from('>H', read_buffer, offset=0)\n\n        # Need to keep track of where the next url box starts.\n        box_offset = 2\n\n        data_entry_url_box_list = []\n        for j in range(ndr):\n\n            # Create an in-memory binary stream for each URL box.\n            box_fptr = io.BytesIO(read_buffer[box_offset:])\n            box_buffer = box_fptr.read(8)\n            (box_length, box_id) = struct.unpack_from('>I4s', box_buffer,\n                                                      offset=0)\n            box = DataEntryURLBox.parse(box_fptr, 0, box_length)\n\n            # Need to adjust the box start to that of the \"real\" file.\n            box.offset = offset + 8 + box_offset\n            data_entry_url_box_list.append(box)\n\n            # Point to the next embedded URL box.\n            box_offset += box_length\n\n        return cls(data_entry_url_box_list, length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the box before writing to file.", "response": "def _validate(self, writing=False):\n        \"\"\"\n        Validate the box before writing to file.\n        \"\"\"\n        if self.brand not in ['jp2 ', 'jpx ']:\n            msg = (\"The file type brand was '{brand}'.  \"\n                   \"It should be either 'jp2 ' or 'jpx '.\")\n            msg = msg.format(brand=self.brand)\n            if writing:\n                raise IOError(msg)\n            else:\n                warnings.warn(msg, UserWarning)\n        for item in self.compatibility_list:\n            if item not in self._valid_cls:\n                msg = (\"The file type compatibility list {items} is \"\n                       \"not valid.  All items should be members of \"\n                       \"{valid_entries}.\")\n                msg = msg.format(items=self.compatibility_list,\n                                 valid_entries=self._valid_cls)\n                if writing:\n                    raise IOError(msg)\n                else:\n                    warnings.warn(msg, UserWarning)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a File Type box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a File Type box to file.\n        \"\"\"\n        self._validate(writing=True)\n        length = 16 + 4 * len(self.compatibility_list)\n        fptr.write(struct.pack('>I4s', length, b'ftyp'))\n        fptr.write(self.brand.encode())\n        fptr.write(struct.pack('>I', self.minor_version))\n\n        for item in self.compatibility_list:\n            fptr.write(item.encode())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        # Extract the brand, minor version.\n        (brand, minor_version) = struct.unpack_from('>4sI', read_buffer, 0)\n        if sys.hexversion >= 0x030000:\n            brand = brand.decode('utf-8')\n\n        # Extract the compatibility list.  Each entry has 4 bytes.\n        num_entries = int((length - 16) / 4)\n        compatibility_list = []\n        for j in range(int(num_entries)):\n            entry, = struct.unpack_from('>4s', read_buffer, 8 + j * 4)\n            if sys.hexversion >= 0x03000000:\n                try:\n                    entry = entry.decode('utf-8')\n                except UnicodeDecodeError:\n                    # The entry is invalid, but we've got code to catch this\n                    # later on.\n                    pass\n\n            compatibility_list.append(entry)\n\n        return cls(brand=brand, minor_version=minor_version,\n                   compatibility_list=compatibility_list,\n                   length=length, offset=offset)", "response": "Parse the JPEG 2000 file type box."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a fragment list box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a fragment list box to file.\n        \"\"\"\n        self._validate(writing=True)\n        num_items = len(self.fragment_offset)\n        length = 8 + 2 + num_items * 14\n        fptr.write(struct.pack('>I4s', length, b'flst'))\n        fptr.write(struct.pack('>H', num_items))\n        for j in range(num_items):\n            write_buffer = struct.pack('>QIH',\n                                       self.fragment_offset[j],\n                                       self.fragment_length[j],\n                                       self.data_reference[j])\n            fptr.write(write_buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        num_fragments, = struct.unpack_from('>H', read_buffer, offset=0)\n\n        lst = struct.unpack_from('>' + 'QIH' * num_fragments,\n                                 read_buffer,\n                                 offset=2)\n        frag_offset = lst[0::3]\n        frag_len = lst[1::3]\n        data_reference = lst[2::3]\n        return cls(frag_offset, frag_len, data_reference,\n                   length=length, offset=offset)", "response": "Parse the JPX free box."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate(self, writing=False):\n        box_ids = [box.box_id for box in self.box]\n        if len(box_ids) != 1 or box_ids[0] != 'flst':\n            msg = (\"Fragment table boxes must have a single fragment list \"\n                   \"box as a child box.\")\n            self._dispatch_validation_error(msg, writing=writing)", "response": "Self - validate the box before writing."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite a fragment table box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a fragment table box to file.\n        \"\"\"\n        self._validate(writing=True)\n        self._write_superbox(fptr, b'ftbl')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(cls, fptr, offset, length):\n        # Must seek to end of box.\n        nbytes = offset + length - fptr.tell()\n        fptr.read(nbytes)\n        return cls(length=length, offset=offset)", "response": "Parse a JPX free box."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, fptr):\n        fptr.write(struct.pack('>I4s', 22, b'ihdr'))\n\n        # signedness and bps are stored together in a single byte\n        bit_depth_signedness = 0x80 if self.signed else 0x00\n        bit_depth_signedness |= self.bits_per_component - 1\n        read_buffer = struct.pack('>IIHBBBB',\n                                  self.height,\n                                  self.width,\n                                  self.num_components,\n                                  bit_depth_signedness,\n                                  self.compression,\n                                  1 if self.colorspace_unknown else 0,\n                                  1 if self.ip_provided else 0)\n        fptr.write(read_buffer)", "response": "Write an Image Header box to file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, fptr, offset, length):\n        # Read the box information\n        read_buffer = fptr.read(14)\n        params = struct.unpack('>IIHBBBB', read_buffer)\n        height = params[0]\n        width = params[1]\n        num_components = params[2]\n        bits_per_component = (params[3] & 0x7f) + 1\n        signed = (params[3] & 0x80) > 1\n        compression = params[4]\n        colorspace_unknown = True if params[5] else False\n        ip_provided = True if params[6] else False\n\n        return cls(height, width, num_components=num_components,\n                   bits_per_component=bits_per_component,\n                   signed=signed,\n                   compression=compression,\n                   colorspace_unknown=colorspace_unknown,\n                   ip_provided=ip_provided,\n                   length=length, offset=offset)", "response": "Parse the JPEG 2000 image header box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(cls, fptr, offset, length):\n        nbytes = length - 8\n        data = fptr.read(nbytes)\n        bpc = tuple(((x & 0x7f) + 1) for x in bytearray(data))\n        signed = tuple(((x & 0x80) > 0) for x in bytearray(data))\n\n        return cls(bpc, signed, length=length, offset=offset)", "response": "Parse bits per component of the current log entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the JPEG 2000 header box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse JPEG 2000 header box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        JP2HeaderBox\n            Instance of the current JP2 header box.\n        \"\"\"\n        box = cls(length=length, offset=offset)\n\n        # The JP2 header box is a superbox, so go ahead and parse its child\n        # boxes.\n        box.box = box.parse_superbox(fptr)\n\n        return box"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, fptr):\n        fptr.write(struct.pack('>I4s', 12, b'jP  '))\n        fptr.write(struct.pack('>BBBB', *self.signature))", "response": "Write a JPEG 2000 Signature box to file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the JPEG 2000 signature box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse JPEG 2000 signature box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        JPEG2000SignatureBox\n            Instance of the current JPEG2000 signature box.\n        \"\"\"\n        read_buffer = fptr.read(4)\n        signature = struct.unpack('>BBBB', read_buffer)\n\n        return cls(signature=signature, length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nverify that the box obeys the specifications.", "response": "def _validate(self, writing=False):\n        \"\"\"Verify that the box obeys the specifications.\"\"\"\n        if ((len(self.bits_per_component) != len(self.signed)) or\n                (len(self.signed) != self.palette.shape[1])):\n            msg = (\"The length of the 'bits_per_component' and the 'signed' \"\n                   \"members must equal the number of columns of the palette.\")\n            self._dispatch_validation_error(msg, writing=writing)\n        bps = self.bits_per_component\n        if writing and not all(b == bps[0] for b in bps):\n            # We don't support writing palettes with bit depths that are\n            # different.\n            msg = \"Writing palettes with varying bit depths is not supported.\"\n            self._dispatch_validation_error(msg, writing=writing)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, fptr):\n        self._validate(writing=True)\n        bytes_per_row = sum(self.bits_per_component) / 8\n        bytes_per_palette = bytes_per_row * self.palette.shape[0]\n        box_length = 8 + 3 + self.palette.shape[1] + bytes_per_palette\n\n        # Write the usual (L, T) header.\n        write_buffer = struct.pack('>I4s', int(box_length), b'pclr')\n        fptr.write(write_buffer)\n\n        # NE, NPC\n        write_buffer = struct.pack('>HB', self.palette.shape[0],\n                                   self.palette.shape[1])\n        fptr.write(write_buffer)\n\n        # Bits Per Sample.  Signed components aren't supported.\n        bps_signed = [x - 1 for x in self.bits_per_component]\n        write_buffer = struct.pack('>' + 'B' * self.palette.shape[1],\n                                   *bps_signed)\n        fptr.write(write_buffer)\n\n        # C(i,j)\n        fptr.write(memoryview(self.palette))", "response": "Write a Palette box to file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the current NCBI - 2001 color palette box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse palette box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        PaletteBox\n            Instance of the current palette box.\n        \"\"\"\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        nrows, ncols = struct.unpack_from('>HB', read_buffer, offset=0)\n\n        bps_signed = struct.unpack_from('>' + 'B' * ncols, read_buffer,\n                                        offset=3)\n        bps = [((x & 0x7f) + 1) for x in bps_signed]\n        signed = [((x & 0x80) > 1) for x in bps_signed]\n\n        # Are any components signed or differently sized?  We don't handle\n        # that.\n        if any(signed) or len(set(bps)) != 1:\n            msg = (\"Palettes with signed components or differently sized \"\n                   \"components are not supported.\")\n            raise IOError(msg)\n\n        # The palette is unsigned and all components have the same width.\n        # This should cover all but a vanishingly small share of palettes.\n        b = bps[0]\n        dtype = np.uint8 if b <=8 else np.uint16 if b <= 16 else np.uint32\n\n        palette = np.frombuffer(read_buffer[3 + ncols:], dtype=dtype)\n        palette = np.reshape(palette, (nrows, ncols))\n\n        return cls(palette, bps, signed, length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        mask_length, = struct.unpack_from('>B', read_buffer, offset=0)\n\n        # Fully Understands Aspect Mask\n        # Decodes Completely Mask\n        fuam = dcm = standard_flag = standard_mask = []\n        vendor_feature = vendor_mask = []\n\n        # The mask length tells us the format string to use when unpacking\n        # from the buffer read from file.\n        try:\n            mask_format = {1: 'B', 2: 'H', 4: 'I', 8: 'Q'}[mask_length]\n            fuam, dcm = struct.unpack_from('>' + mask_format * 2, read_buffer,\n                                           offset=1)\n            std_flg_offset = 1 + 2 * mask_length\n            data = _parse_standard_flag(read_buffer[std_flg_offset:],\n                                        mask_length)\n            standard_flag, standard_mask = data\n\n            nflags = len(standard_flag)\n            vndr_offset = 1 + 2 * mask_length + 2 + (2 + mask_length) * nflags\n            data = _parse_vendor_features(read_buffer[vndr_offset:],\n                                          mask_length)\n            vendor_feature, vendor_mask = data\n\n        except KeyError:\n            msg = ('The ReaderRequirements box (rreq) has a mask length of '\n                   '{length} bytes, but only values of 1, 2, 4, or 8 are '\n                   'supported.  The box contents will not be interpreted.')\n            warnings.warn(msg.format(length=mask_length), UserWarning)\n\n        return cls(fuam, dcm, standard_flag, standard_mask,\n                   vendor_feature, vendor_mask,\n                   length=length, offset=offset)", "response": "Parse the reader requirements box."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(cls, fptr, offset, length):\n        read_buffer = fptr.read(10)\n        (rn1, rd1, rn2, rd2, re1, re2) = struct.unpack('>HHHHBB', read_buffer)\n        vres = rn1 / rd1 * math.pow(10, re1)\n        hres = rn2 / rd2 * math.pow(10, re2)\n\n        return cls(vres, hres, length=length, offset=offset)", "response": "Parse CaptureResolutionBox.\n            returns CaptureResolutionBox instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write(self, fptr):\n        length = 8 + len(self.label.encode())\n        fptr.write(struct.pack('>I4s', length, b'lbl '))\n        fptr.write(self.label.encode())", "response": "Write a Label box to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        label = read_buffer.decode('utf-8')\n        return cls(label, length=length, offset=offset)", "response": "Parse a LabelBox\n            box."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        raw_data = fptr.read(num_bytes)\n        num_associations = int(len(raw_data) / 4)\n        lst = struct.unpack('>' + 'I' * num_associations, raw_data)\n        return cls(lst, length=length, offset=offset)", "response": "Parse the n - tuple label list box."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, fptr):\n        fptr.write(struct.pack('>I4s',\n                               len(self.associations) * 4 + 8, b'nlst'))\n\n        fmt = '>' + 'I' * len(self.associations)\n        write_buffer = struct.pack(fmt, *self.associations)\n        fptr.write(write_buffer)", "response": "Write a NumberList box to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite an XML box to file.", "response": "def write(self, fptr):\n        \"\"\"\n        Write an XML box to file.\n        \"\"\"\n        read_buffer = ET.tostring(self.xml.getroot(), encoding='utf-8')\n        fptr.write(struct.pack('>I4s', len(read_buffer) + 8, b'xml '))\n        fptr.write(read_buffer)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n\n        if sys.hexversion < 0x03000000 and codecs.BOM_UTF8 in read_buffer:\n            # Python3 with utf-8 handles this just fine.  Actually so does\n            # Python2 right here since we decode using utf-8.  The real\n            # problem comes when __str__ is used on the XML box, and that\n            # is where Python2 falls short because of the ascii codec.\n            msg = ('A BOM (byte order marker) was detected and '\n                   'removed from the XML contents in the box starting at byte '\n                   'offset {offset:d}.')\n            msg = msg.format(offset=offset)\n            warnings.warn(msg, UserWarning)\n            read_buffer = read_buffer.replace(codecs.BOM_UTF8, b'')\n\n        try:\n            text = read_buffer.decode('utf-8')\n        except UnicodeDecodeError as err:\n            # Possibly bad string of bytes to begin with.\n            # Try to search for <?xml and go from there.\n            decl_start = read_buffer.find(b'<?xml')\n            if decl_start <= -1:\n                # Nope, that's not it.  All is lost.\n                msg = ('A problem was encountered while parsing an XML box:'\n                       '\\n\\n\\t\"{error}\"\\n\\nNo XML was retrieved.')\n                warnings.warn(msg.format(error=str(err)), UserWarning)\n                return XMLBox(xml=None, length=length, offset=offset)\n\n            text = read_buffer[decl_start:].decode('utf-8')\n\n            # Let the user know that the XML box was problematic.\n            msg = ('A UnicodeDecodeError was encountered parsing an XML box '\n                   'at byte position {offset:d} ({reason}), but the XML was '\n                   'still recovered.')\n            msg = msg.format(offset=offset, reason=err.reason)\n            warnings.warn(msg, UserWarning)\n\n        # Strip out any trailing nulls, as they can foul up XML parsing.\n        text = text.rstrip(chr(0))\n        bfptr = io.BytesIO(text.encode('utf-8'))\n\n        try:\n            xml = ET.parse(bfptr)\n        except ET.ParseError as err:\n            msg = ('A problem was encountered while parsing an XML box:'\n                   '\\n\\n\\t\"{reason}\"\\n\\nNo XML was retrieved.')\n            msg = msg.format(reason=str(err))\n            warnings.warn(msg, UserWarning)\n            xml = None\n\n        return cls(xml=xml, length=length, offset=offset)", "response": "Parse an XML box."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, fptr):\n        num_uuids = len(self.ulst)\n        length = 4 + 4 + 2 + num_uuids * 16\n        write_buffer = struct.pack('>I4sH', length, b'ulst', num_uuids)\n        fptr.write(write_buffer)\n\n        for j in range(num_uuids):\n            fptr.write(self.ulst[j].bytes)", "response": "Write a UUID list box to file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the UUIDListBox object.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse UUIDList box.\n\n        Parameters\n        ----------\n        f : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        UUIDListBox\n            Instance of the current UUID list box.\n        \"\"\"\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n\n        num_uuids, = struct.unpack_from('>H', read_buffer)\n\n        ulst = []\n        for j in range(num_uuids):\n            uuid_buffer = read_buffer[2 + j * 16:2 + (j + 1) * 16]\n            ulst.append(UUID(bytes=uuid_buffer))\n\n        return cls(ulst, length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite a data entry url box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a data entry url box to file.\n        \"\"\"\n        # Make sure it is written out as null-terminated.\n        url = self.url\n        if self.url[-1] != chr(0):\n            url = url + chr(0)\n        url = url.encode()\n\n        length = 8 + 1 + 3 + len(url)\n        write_buffer = struct.pack('>I4sBBBB',\n                                   length, b'url ',\n                                   self.version,\n                                   self.flag[0], self.flag[1], self.flag[2])\n        fptr.write(write_buffer)\n        fptr.write(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(cls, fptr, offset, length):\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        data = struct.unpack_from('>BBBB', read_buffer)\n        version = data[0]\n        flag = data[1:4]\n\n        url = read_buffer[4:].decode('utf-8').rstrip(chr(0))\n        return cls(version, flag, url, length=length, offset=offset)", "response": "Parse the data entry URL box."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print_geotiff(self):\n        if self.data is None:\n            return \"corrupt\"\n        in_mem_name = '/vsimem/geo.tif'\n        gdal.FileFromMemBuffer(in_mem_name, self.raw_data)\n        gtif = gdal.Open(in_mem_name)\n\n        # Report projection\n        proj_ref = gtif.GetProjectionRef()\n        sref = osr.SpatialReference()\n        sref.ImportFromWkt(proj_ref)\n        psz_pretty_wkt = sref.ExportToPrettyWkt(False)\n\n        # report geotransform\n        geo_transform = gtif.GetGeoTransform(can_return_null=True)\n        fmt = ('Origin = ({origin_x:.15f},{origin_y:.15f})\\n'\n               'Pixel Size = ({pixel_x:.15f},{pixel_y:.15f})')\n        geotransform_str = fmt.format(origin_x=geo_transform[0],\n                                      origin_y=geo_transform[3],\n                                      pixel_x=geo_transform[1],\n                                      pixel_y=geo_transform[5])\n\n        # setup projected to lat/long transform if appropriate\n        if proj_ref is not None and len(proj_ref) > 0:\n            hProj = osr.SpatialReference(proj_ref)\n            if hProj is not None:\n                hLatLong = hProj.CloneGeogCS()\n\n            if hLatLong is not None:\n                gdal.PushErrorHandler('CPLQuietErrorHandler')\n                hTransform = osr.CoordinateTransformation(hProj, hLatLong)\n                gdal.PopErrorHandler()\n                msg = 'Unable to load PROJ.4 library'\n\n        # report corners\n        uleft = self.GDALInfoReportCorner(gtif, hTransform, \"Upper Left\", 0, 0)\n        lleft = self.GDALInfoReportCorner(gtif, hTransform, \"Lower Left\",\n                                          0, gtif.RasterYSize)\n        uright = self.GDALInfoReportCorner(gtif, hTransform, \"Upper Right\",\n                                           gtif.RasterXSize, 0)\n        lright = self.GDALInfoReportCorner(gtif, hTransform, \"Lower Right\",\n                                           gtif.RasterXSize, gtif.RasterYSize)\n        center = self.GDALInfoReportCorner(gtif, hTransform, \"Center\",\n                                           gtif.RasterXSize / 2.0,\n                                           gtif.RasterYSize / 2.0)\n\n        gdal.Unlink(in_mem_name)\n\n        fmt = (\"Coordinate System =\\n\"\n               \"{coordinate_system}\\n\"\n               \"{geotransform}\\n\"\n               \"Corner Coordinates:\\n\"\n               \"{upper_left}\\n\"\n               \"{lower_left}\\n\"\n               \"{upper_right}\\n\"\n               \"{lower_right}\\n\"\n               \"{center}\")\n        msg = fmt.format(coordinate_system=self._indent(psz_pretty_wkt),\n                         geotransform=geotransform_str,\n                         upper_left=uleft, upper_right=uright,\n                         lower_left=lleft, lower_right=lright, center=center)\n        return msg", "response": "Print geotiff information.  Shamelessly ripped off from gdalinfo.py\n\n        Returns\n        -------\n        str\n            String representation of the degenerate geotiff."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a UUID box to file.", "response": "def write(self, fptr):\n        \"\"\"Write a UUID box to file.\n        \"\"\"\n        length = 4 + 4 + 16 + len(self.raw_data)\n        write_buffer = struct.pack('>I4s', length, b'uuid')\n        fptr.write(write_buffer)\n        fptr.write(self.uuid.bytes)\n        fptr.write(self.raw_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the UUID box.", "response": "def parse(cls, fptr, offset, length):\n        \"\"\"Parse UUID box.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n        offset : int\n            Start position of box in bytes.\n        length : int\n            Length of the box in bytes.\n\n        Returns\n        -------\n        UUIDBox\n            Instance of the current UUID box.\n        \"\"\"\n        num_bytes = offset + length - fptr.tell()\n        read_buffer = fptr.read(num_bytes)\n        the_uuid = UUID(bytes=read_buffer[0:16])\n        return cls(the_uuid, read_buffer[16:], length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_precinct_size(spcod):\n    spcod = np.frombuffer(spcod, dtype=np.uint8)\n    precinct_size = []\n    for item in spcod:\n        ep2 = (item & 0xF0) >> 4\n        ep1 = item & 0x0F\n        precinct_size.append((2 ** ep1, 2 ** ep2))\n    return tuple(precinct_size)", "response": "Compute precinct size from SPcod or SPcoc."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _context_string(context):\n    msg = 'Code block context:\\n            '\n    lines = ['Selective arithmetic coding bypass:  {0}',\n             'Reset context probabilities on coding pass boundaries:  {1}',\n             'Termination on each coding pass:  {2}',\n             'Vertically stripe causal context:  {3}',\n             'Predictable termination:  {4}',\n             'Segmentation symbols:  {5}']\n    msg += '\\n            '.join(lines)\n    msg = msg.format(((context & 0x01) > 0),\n                     ((context & 0x02) > 0),\n                     ((context & 0x04) > 0),\n                     ((context & 0x08) > 0),\n                     ((context & 0x10) > 0),\n                     ((context & 0x20) > 0))\n    return msg", "response": "Produce a string to represent the code block context"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_quantization(read_buffer, sqcd):\n    numbytes = len(read_buffer)\n\n    exponent = []\n    mantissa = []\n\n    if sqcd & 0x1f == 0:  # no quantization\n        data = struct.unpack('>' + 'B' * numbytes, read_buffer)\n        for j in range(len(data)):\n            exponent.append(data[j] >> 3)\n            mantissa.append(0)\n    else:\n        fmt = '>' + 'H' * int(numbytes / 2)\n        data = struct.unpack(fmt, read_buffer)\n        for j in range(len(data)):\n            exponent.append(data[j] >> 11)\n            mantissa.append(data[j] & 0x07ff)\n\n    return mantissa, exponent", "response": "Tease out the quantization values from the given buffer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _print_quantization_style(sqcc):\n\n    msg = '\\n    Quantization style:  '\n    if sqcc & 0x1f == 0:\n        msg += 'no quantization, '\n    elif sqcc & 0x1f == 1:\n        msg += 'scalar implicit, '\n    elif sqcc & 0x1f == 2:\n        msg += 'scalar explicit, '\n    return msg", "response": "Print the quantization style of the current resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_unrecognized_segment(self, fptr):\n        msg = (\"Unrecognized codestream marker 0x{marker_id:x} encountered at \"\n               \"byte offset {offset}.\")\n        msg = msg.format(marker_id=self._marker_id, offset=fptr.tell())\n        warnings.warn(msg, UserWarning)\n        cpos = fptr.tell()\n        read_buffer = fptr.read(2)\n        next_item, = struct.unpack('>H', read_buffer)\n        fptr.seek(cpos)\n        if ((next_item & 0xff00) >> 8) == 255:\n                # No segment associated with this marker, so reset\n                # to two bytes after it.\n            segment = Segment(id='0x{0:x}'.format(self._marker_id),\n                              offset=self._offset, length=0)\n        else:\n            segment = self._parse_reserved_segment(fptr)\n        return segment", "response": "Parse an unrecognized segment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the reserved segment.", "response": "def _parse_reserved_segment(self, fptr):\n        \"\"\"Parse valid marker segment, segment description is unknown.\n\n        Parameters\n        ----------\n        fptr : file object\n            The file to parse.\n\n        Returns\n        -------\n        Segment\n            The current segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n        if length > 0:\n            data = fptr.read(length - 2)\n        else:\n            data = None\n\n        segment = Segment(marker_id='0x{0:x}'.format(self._marker_id),\n                          offset=offset, length=length, data=data)\n        return segment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the tile part bit stream for SOP EPH marker segments.", "response": "def _parse_tile_part_bit_stream(self, fptr, sod_marker, tile_length):\n        \"\"\"Parse the tile part bit stream for SOP, EPH marker segments.\"\"\"\n        read_buffer = fptr.read(tile_length)\n        # The tile length could possibly be too large and extend past\n        # the end of file.  We need to be a bit resilient.\n        count = min(tile_length, len(read_buffer))\n        packet = np.frombuffer(read_buffer, dtype=np.uint8, count=count)\n\n        indices = np.where(packet == 0xff)\n        for idx in indices[0]:\n            try:\n                if packet[idx + 1] == 0x91 and (idx < (len(packet) - 5)):\n                    offset = sod_marker.offset + 2 + idx\n                    length = 4\n                    nsop = packet[(idx + 4):(idx + 6)].view('uint16')[0]\n                    if sys.byteorder == 'little':\n                        nsop = nsop.byteswap()\n                    segment = SOPsegment(nsop, length, offset)\n                    self.segment.append(segment)\n                elif packet[idx + 1] == 0x92:\n                    offset = sod_marker.offset + 2 + idx\n                    length = 0\n                    segment = EPHsegment(length, offset)\n                    self.segment.append(segment)\n            except IndexError:\n                continue"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_cme_segment(self, fptr):\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(4)\n        data = struct.unpack('>HH', read_buffer)\n        length = data[0]\n        rcme = data[1]\n        ccme = fptr.read(length - 4)\n\n        return CMEsegment(rcme, ccme, length, offset)", "response": "Parse the CME marker segment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the COC marker segment.", "response": "def _parse_coc_segment(self, fptr):\n        \"\"\"Parse the COC marker segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        COCSegment\n            The current COC segment.\n        \"\"\"\n        kwargs = {}\n        offset = fptr.tell() - 2\n        kwargs['offset'] = offset\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n        kwargs['length'] = length\n\n        fmt = '>B' if self._csiz <= 255 else '>H'\n        nbytes = 1 if self._csiz <= 255 else 2\n        read_buffer = fptr.read(nbytes)\n        ccoc, = struct.unpack(fmt, read_buffer)\n\n        read_buffer = fptr.read(1)\n        scoc, = struct.unpack('>B', read_buffer)\n\n        numbytes = offset + 2 + length - fptr.tell()\n        read_buffer = fptr.read(numbytes)\n        spcoc = np.frombuffer(read_buffer, dtype=np.uint8)\n        spcoc = spcoc\n\n        return COCsegment(ccoc, scoc, spcoc, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the COD segment.", "response": "def _parse_cod_segment(cls, fptr):\n        \"\"\"Parse the COD segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        CODSegment\n            The current COD segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        read_buffer = fptr.read(length - 2)\n\n        lst = struct.unpack_from('>BBHBBBBBB', read_buffer, offset=0)\n        scod, prog, nlayers, mct, nr, xcb, ycb, cstyle, xform = lst\n\n        if len(read_buffer) > 10:\n            precinct_size = _parse_precinct_size(read_buffer[10:])\n        else:\n            precinct_size = None\n\n        sop = (scod & 2) > 0\n        eph = (scod & 4) > 0\n\n        if sop or eph:\n            cls._parse_tpart_flag = True\n        else:\n            cls._parse_tpart_flag = False\n\n        pargs = (scod, prog, nlayers, mct, nr, xcb, ycb, cstyle, xform,\n                 precinct_size)\n\n        return CODsegment(*pargs, length=length, offset=offset)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the CRG marker segment.", "response": "def _parse_crg_segment(self, fptr):\n        \"\"\"Parse the CRG marker segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        CRGSegment\n            The current CRG segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        read_buffer = fptr.read(4 * self._csiz)\n        data = struct.unpack('>' + 'HH' * self._csiz, read_buffer)\n        xcrg = data[0::2]\n        ycrg = data[1::2]\n\n        return CRGsegment(xcrg, ycrg, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_eoc_segment(self, fptr):\n        offset = fptr.tell() - 2\n        length = 0\n\n        return EOCsegment(length, offset)", "response": "Parse the end - of - codestream marker segment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the PLT segment.", "response": "def _parse_plt_segment(self, fptr):\n        \"\"\"Parse the PLT segment.\n\n        The packet headers are not parsed, i.e. they remain uninterpreted raw\n        data buffers.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        PLTSegment\n            The current PLT segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(3)\n        length, zplt = struct.unpack('>HB', read_buffer)\n\n        numbytes = length - 3\n        read_buffer = fptr.read(numbytes)\n        iplt = np.frombuffer(read_buffer, dtype=np.uint8)\n\n        packet_len = []\n        plen = 0\n        for byte in iplt:\n            plen |= (byte & 0x7f)\n            if byte & 0x80:\n                # Continue by or-ing in the next byte.\n                plen <<= 7\n            else:\n                packet_len.append(plen)\n                plen = 0\n\n        iplt = packet_len\n\n        return PLTsegment(zplt, iplt, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_pod_segment(self, fptr):\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        n = ((length - 2) / 7) if self._csiz < 257 else ((length - 2) / 9)\n        n = int(n)\n        nbytes = n * 7 if self._csiz < 257 else n * 9\n        read_buffer = fptr.read(nbytes)\n        fmt = '>' + 'BBHBBB' * n if self._csiz < 257 else '>' + 'BHHBHB' * n\n        pod_params = struct.unpack(fmt, read_buffer)\n\n        return PODsegment(pod_params, length, offset)", "response": "Parse the POD segment."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_ppm_segment(self, fptr):\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(3)\n        length, zppm = struct.unpack('>HB', read_buffer)\n\n        numbytes = length - 3\n        read_buffer = fptr.read(numbytes)\n\n        return PPMsegment(zppm, read_buffer, length, offset)", "response": "Parse the PPM segment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_ppt_segment(self, fptr):\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(3)\n        length, zppt = struct.unpack('>HB', read_buffer)\n        length = length\n        zppt = zppt\n\n        numbytes = length - 3\n        ippt = fptr.read(numbytes)\n\n        return PPTsegment(zppt, ippt, length, offset)", "response": "Parse the PPT segment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the QCC segment.", "response": "def _parse_qcc_segment(cls, fptr):\n        \"\"\"Parse the QCC segment.\n\n        Parameters\n        ----------\n        fptr : file object\n            The file to parse.\n\n        Returns\n        -------\n        QCCSegment\n            The current QCC segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        read_buffer = fptr.read(length - 2)\n        fmt = '>HB' if cls._csiz > 256 else '>BB'\n        mantissa_exponent_offset = 3 if cls._csiz > 256 else 2\n        cqcc, sqcc = struct.unpack_from(fmt, read_buffer)\n        if cqcc >= cls._csiz:\n            msg = (\"Invalid QCC component number ({invalid_comp_no}), \"\n                   \"the actual number of components is only {valid_comp_no}.\")\n            msg = msg.format(invalid_comp_no=cqcc, valid_comp_no=cls._csiz)\n            warnings.warn(msg, UserWarning)\n\n        spqcc = read_buffer[mantissa_exponent_offset:]\n\n        return QCCsegment(cqcc, sqcc, spqcc, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the QCD segment.", "response": "def _parse_qcd_segment(self, fptr):\n        \"\"\"Parse the QCD segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        QCDSegment\n            The current QCD segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(3)\n        length, sqcd = struct.unpack('>HB', read_buffer)\n        spqcd = fptr.read(length - 3)\n\n        return QCDsegment(sqcd, spqcd, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_rgn_segment(cls, fptr):\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        nbytes = 3 if cls._csiz < 257 else 4\n        fmt = '>BBB' if cls._csiz < 257 else '>HBB'\n        read_buffer = fptr.read(nbytes)\n        data = struct.unpack(fmt, read_buffer)\n\n        length = length\n        crgn = data[0]\n        srgn = data[1]\n        sprgn = data[2]\n\n        return RGNsegment(crgn, srgn, sprgn, length, offset)", "response": "Parse the RGN segment."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the SIZ segment.", "response": "def _parse_siz_segment(cls, fptr):\n        \"\"\"Parse the SIZ segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        SIZSegment\n            The current SIZ segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        read_buffer = fptr.read(length - 2)\n        data = struct.unpack_from('>HIIIIIIIIH', read_buffer)\n\n        rsiz = data[0]\n        if rsiz not in _KNOWN_PROFILES:\n            msg = \"Invalid profile: (Rsiz={rsiz}).\".format(rsiz=rsiz)\n            warnings.warn(msg, UserWarning)\n\n        xysiz = (data[1], data[2])\n        xyosiz = (data[3], data[4])\n        xytsiz = (data[5], data[6])\n        xytosiz = (data[7], data[8])\n\n        # Csiz is the number of components\n        Csiz = data[9]\n\n        data = struct.unpack_from('>' + 'B' * (length - 36 - 2),\n                                  read_buffer, offset=36)\n\n        bitdepth = tuple(((x & 0x7f) + 1) for x in data[0::3])\n        signed = tuple(((x & 0x80) > 0) for x in data[0::3])\n        xrsiz = data[1::3]\n        yrsiz = data[2::3]\n\n        for j, subsampling in enumerate(zip(xrsiz, yrsiz)):\n            if 0 in subsampling:\n                msg = (\"Invalid subsampling value for component {comp}: \"\n                       \"dx={dx}, dy={dy}.\")\n                msg = msg.format(comp=j, dx=subsampling[0], dy=subsampling[1])\n                warnings.warn(msg, UserWarning)\n\n        try:\n            num_tiles_x = (xysiz[0] - xyosiz[0]) / (xytsiz[0] - xytosiz[0])\n            num_tiles_y = (xysiz[1] - xyosiz[1]) / (xytsiz[1] - xytosiz[1])\n        except ZeroDivisionError:\n            msg = (\"Invalid tile specification:  \"\n                   \"size of {num_tile_rows} x {num_tile_cols}, \"\n                   \"offset of {row_offset} x {col_offset}.\")\n            msg = msg.format(num_tile_rows=xytsiz[1],\n                             num_tile_cols=xytsiz[0],\n                             row_offset=xytosiz[1],\n                             col_offset=xytosiz[0])\n            warnings.warn(msg, UserWarning)\n        else:\n            numtiles = math.ceil(num_tiles_x) * math.ceil(num_tiles_y)\n            if numtiles > 65535:\n                msg = \"Invalid number of tiles: ({numtiles}).\"\n                msg = msg.format(numtiles=numtiles)\n                warnings.warn(msg, UserWarning)\n\n        kwargs = {\n            'rsiz': rsiz,\n            'xysiz': xysiz,\n            'xyosiz': xyosiz,\n            'xytsiz': xytsiz,\n            'xytosiz': xytosiz,\n            'Csiz': Csiz,\n            'bitdepth': bitdepth,\n            'signed': signed,\n            'xyrsiz': (xrsiz, yrsiz),\n            'length': length,\n            'offset': offset\n        }\n        segment = SIZsegment(**kwargs)\n\n        # Need to keep track of the number of components from SIZ for\n        # other segments.\n        cls._csiz = Csiz\n\n        return segment"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the current SOD segment.", "response": "def _parse_sod_segment(self, fptr):\n        \"\"\"Parse the SOD (start-of-data) segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        SODSegment\n            The current SOD segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n        length = 0\n\n        return SODsegment(length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the SOT segment.", "response": "def _parse_sot_segment(self, fptr):\n        \"\"\"Parse the SOT segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        SOTSegment\n            The current SOT segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(10)\n        data = struct.unpack('>HHIBB', read_buffer)\n\n        length = data[0]\n        isot = data[1]\n        psot = data[2]\n        tpsot = data[3]\n        tnsot = data[4]\n\n        segment = SOTsegment(isot, psot, tpsot, tnsot, length, offset)\n\n        # Need to keep easy access to tile offsets and lengths for when\n        # we encounter start-of-data marker segments.\n\n        self._tile_offset.append(segment.offset)\n        if segment.psot == 0:\n            tile_part_length = (self.offset + self.length -\n                                segment.offset - 2)\n        else:\n            tile_part_length = segment.psot\n        self._tile_length.append(tile_part_length)\n\n        return segment"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the TLM segment.", "response": "def _parse_tlm_segment(self, fptr):\n        \"\"\"Parse the TLM segment.\n\n        Parameters\n        ----------\n        fptr : file\n            Open file object.\n\n        Returns\n        -------\n        TLMSegment\n            The current TLM segment.\n        \"\"\"\n        offset = fptr.tell() - 2\n\n        read_buffer = fptr.read(2)\n        length, = struct.unpack('>H', read_buffer)\n\n        read_buffer = fptr.read(length - 2)\n        ztlm, stlm = struct.unpack_from('>BB', read_buffer)\n        ttlm_st = (stlm >> 4) & 0x3\n        ptlm_sp = (stlm >> 6) & 0x1\n\n        nbytes = length - 4\n        if ttlm_st == 0:\n            ntiles = nbytes / ((ptlm_sp + 1) * 2)\n        else:\n            ntiles = nbytes / (ttlm_st + (ptlm_sp + 1) * 2)\n\n        if ttlm_st == 0:\n            ttlm = None\n            fmt = ''\n        elif ttlm_st == 1:\n            fmt = 'B'\n        elif ttlm_st == 2:\n            fmt = 'H'\n\n        if ptlm_sp == 0:\n            fmt += 'H'\n        else:\n            fmt += 'I'\n\n        data = struct.unpack_from('>' + fmt * int(ntiles), read_buffer,\n                                  offset=2)\n        if ttlm_st == 0:\n            ttlm = None\n            ptlm = data\n        else:\n            ttlm = data[0::2]\n            ptlm = data[1::2]\n\n        return TLMsegment(ztlm, ttlm, ptlm, length, offset)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the reserved marker.", "response": "def _parse_reserved_marker(self, fptr):\n        \"\"\"Marker range between 0xff30 and 0xff39.\n        \"\"\"\n        the_id = '0x{0:x}'.format(self._marker_id)\n        segment = Segment(marker_id=the_id, offset=self._offset, length=0)\n        return segment"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_json(self):\n        event_as_dict = copy.deepcopy(self.event_body)\n        if self.timestamp:\n            if \"keen\" in event_as_dict:\n                event_as_dict[\"keen\"][\"timestamp\"] = self.timestamp.isoformat()\n            else:\n                event_as_dict[\"keen\"] = {\"timestamp\": self.timestamp.isoformat()}\n        return json.dumps(event_as_dict)", "response": "Serializes the event to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_event(self, event_collection, event_body, timestamp=None):\n        event = Event(self.project_id, event_collection, event_body,\n                      timestamp=timestamp)\n        self.persistence_strategy.persist(event)", "response": "Adds an event to the local cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating an image beacon URL.", "response": "def generate_image_beacon(self, event_collection, event_body, timestamp=None):\n        \"\"\" Generates an image beacon URL.\n\n        :param event_collection: the name of the collection to insert the\n        event to\n        :param event_body: dict, the body of the event to insert the event to\n        :param timestamp: datetime, optional, the timestamp of the event\n        \"\"\"\n        event = Event(self.project_id, event_collection, event_body,\n                      timestamp=timestamp)\n        event_json = event.to_json()\n        return \"{0}/{1}/projects/{2}/events/{3}?api_key={4}&data={5}\".format(\n            self.api.base_url, self.api.api_version, self.project_id, self._url_escape(event_collection),\n            self.api.write_key.decode(sys.getdefaultencoding()), self._base64_encode(event_json)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete events from the specified event collection.", "response": "def delete_events(self, event_collection, timeframe=None, timezone=None, filters=None):\n        \"\"\" Deletes events.\n\n        :param event_collection: string, the event collection from which event are being deleted\n        :param timeframe: string or dict, the timeframe in which the events happened\n        example: \"previous_7_days\"\n        :param timezone: int, the timezone you'd like to use for the timeframe\n        and interval in seconds\n        :param filters: array of dict, contains the filters you'd like to apply to the data\n        example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n\n        \"\"\"\n        params = self.get_params(timeframe=timeframe, timezone=timezone, filters=filters)\n        return self.api.delete_events(event_collection, params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_access_key(self, name, is_active=True, permitted=[], options={}):\n\n        return self.api.create_access_key(name=name, is_active=is_active,\n                                          permitted=permitted, options=options)", "response": "Creates a new master key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreplace the name is_active permitted and options values of a given access key.", "response": "def update_access_key_full(self, access_key_id, name, is_active, permitted, options):\n        \"\"\"\n        Replaces the 'name', 'is_active', 'permitted', and 'options' values of a given key.\n        A master key must be set first.\n\n        :param access_key_id: the 'key' value of the access key for which the values will be replaced\n        :param name: the new name desired for this access key\n        :param is_active: whether the key should become enabled (True) or revoked (False)\n        :param permitted: the new list of permissions desired for this access key\n        :param options: the new dictionary of options for this access key\n        \"\"\"\n        return self.api.update_access_key_full(access_key_id, name, is_active, permitted, options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a select unique query Returns an array of the unique values of a target property for events that meet the given criteria. :param event_collection: string, the name of the collection to query :param target_property: string, the name of the event property you would like use :param timeframe: string or dict, the timeframe in which the events happened example: \"previous_7_days\" :param timezone: int, the timezone you'd like to use for the timeframe and interval in seconds :param interval: string, the time interval used for measuring data over time example: \"daily\" :param filters: array of dict, contains the filters you'd like to apply to the data example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}] :param group_by: string or array of strings, the name(s) of the properties you would like to group your results by. example: \"customer.id\" or [\"browser\",\"operating_system\"] :param order_by: dictionary or list of dictionary objects containing the property_name(s) to order by and the desired direction(s) of sorting. Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING} May not be used without a group_by specified. :param limit: positive integer limiting the displayed results of a query using order_by :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're willing to trade for increased query performance, in seconds", "response": "def select_unique(self, event_collection, target_property, timeframe=None, timezone=None, interval=None,\n                      filters=None, group_by=None, order_by=None, max_age=None, limit=None):\n        \"\"\" Performs a select unique query\n\n        Returns an array of the unique values of a target property for events that meet the given criteria.\n\n        :param event_collection: string, the name of the collection to query\n        :param target_property: string, the name of the event property you would like use\n        :param timeframe: string or dict, the timeframe in which the events\n        happened example: \"previous_7_days\"\n        :param timezone: int, the timezone you'd like to use for the timeframe\n        and interval in seconds\n        :param interval: string, the time interval used for measuring data over\n        time example: \"daily\"\n        :param filters: array of dict, contains the filters you'd like to apply to the data\n        example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n        :param group_by: string or array of strings, the name(s) of the properties you would\n        like to group your results by.  example: \"customer.id\" or [\"browser\",\"operating_system\"]\n        :param order_by: dictionary or list of dictionary objects containing the property_name(s)\n        to order by and the desired direction(s) of sorting.\n        Example: {\"property_name\":\"result\", \"direction\":keen.direction.DESCENDING}\n        May not be used without a group_by specified.\n        :param limit: positive integer limiting the displayed results of a query using order_by\n        :param max_age: an integer, greater than 30 seconds, the maximum 'staleness' you're\n        willing to trade for increased query performance, in seconds\n\n        \"\"\"\n        params = self.get_params(event_collection=event_collection, timeframe=timeframe, timezone=timezone,\n                                 interval=interval, filters=filters, group_by=group_by, order_by=order_by,\n                                 target_property=target_property, max_age=max_age, limit=limit)\n        return self.api.query(\"select_unique\", params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform a data extraction Returns either a JSON object of events or a response indicating an email will be sent to you with data. :param event_collection: string, the name of the collection to query :param timeframe: string or dict, the timeframe in which the events happened example: \"previous_7_days\" :param timezone: int, the timezone you'd like to use for the timeframe and interval in seconds :param filters: array of dict, contains the filters you'd like to apply to the data example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}] :param latest: int, the number of most recent records you'd like to return :param email: string, optional string containing an email address to email results to :param property_names: string or list of strings, used to limit the properties returned", "response": "def extraction(self, event_collection, timeframe=None, timezone=None, filters=None, latest=None,\n                   email=None, property_names=None):\n        \"\"\" Performs a data extraction\n\n        Returns either a JSON object of events or a response\n         indicating an email will be sent to you with data.\n\n        :param event_collection: string, the name of the collection to query\n        :param timeframe: string or dict, the timeframe in which the events\n        happened example: \"previous_7_days\"\n        :param timezone: int, the timezone you'd like to use for the timeframe\n        and interval in seconds\n        :param filters: array of dict, contains the filters you'd like to apply to the data\n        example: [{\"property_name\":\"device\", \"operator\":\"eq\", \"property_value\":\"iPhone\"}]\n        :param latest: int, the number of most recent records you'd like to return\n        :param email: string, optional string containing an email address to email results to\n        :param property_names: string or list of strings, used to limit the properties returned\n\n        \"\"\"\n        params = self.get_params(event_collection=event_collection, timeframe=timeframe, timezone=timezone,\n                                 filters=filters, latest=latest, email=email, property_names=property_names)\n        return self.api.query(\"extraction\", params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef funnel(self, steps, timeframe=None, timezone=None, max_age=None, all_keys=False):\n        params = self.get_params(\n            steps=steps,\n            timeframe=timeframe,\n            timezone=timezone,\n            max_age=max_age,\n        )\n\n        return self.api.query(\"funnel\", params, all_keys=all_keys)", "response": "This method performs a Funnel query on the object that contains the results for each step of the funnel."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef version():\n    OPENJPEG.opj_version.restype = ctypes.c_char_p\n    library_version = OPENJPEG.opj_version()\n    if sys.hexversion >= 0x03000000:\n        return library_version.decode('utf-8')\n    else:\n        return library_version", "response": "Wrapper for opj_version library routine."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for openjpeg library function opj_cio_open.", "response": "def cio_open(cinfo, src=None):\n    \"\"\"Wrapper for openjpeg library function opj_cio_open.\"\"\"\n    argtypes = [ctypes.POINTER(CommonStructType), ctypes.c_char_p,\n                ctypes.c_int]\n    OPENJPEG.opj_cio_open.argtypes = argtypes\n    OPENJPEG.opj_cio_open.restype = ctypes.POINTER(CioType)\n\n    if src is None:\n        length = 0\n    else:\n        length = len(src)\n\n    cio = OPENJPEG.opj_cio_open(ctypes.cast(cinfo,\n                                            ctypes.POINTER(CommonStructType)),\n                                src,\n                                length)\n    return cio"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap openjpeg library function cio_close.", "response": "def cio_close(cio):\n    \"\"\"Wraps openjpeg library function cio_close.\n    \"\"\"\n    OPENJPEG.opj_cio_close.argtypes = [ctypes.POINTER(CioType)]\n    OPENJPEG.opj_cio_close(cio)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the current position in byte stream.", "response": "def cio_tell(cio):\n    \"\"\"Get position in byte stream.\"\"\"\n    OPENJPEG.cio_tell.argtypes = [ctypes.POINTER(CioType)]\n    OPENJPEG.cio_tell.restype = ctypes.c_int\n    pos = OPENJPEG.cio_tell(cio)\n    return pos"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap for openjpeg library function opj_create_compress.", "response": "def create_compress(fmt):\n    \"\"\"Wrapper for openjpeg library function opj_create_compress.\n\n    Creates a J2K/JPT/JP2 compression structure.\n    \"\"\"\n    OPENJPEG.opj_create_compress.argtypes = [ctypes.c_int]\n    OPENJPEG.opj_create_compress.restype = ctypes.POINTER(CompressionInfoType)\n    cinfo = OPENJPEG.opj_create_compress(fmt)\n    return cinfo"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap openjpeg library function opj_create_decompress.", "response": "def create_decompress(fmt):\n    \"\"\"Wraps openjpeg library function opj_create_decompress.\n    \"\"\"\n    OPENJPEG.opj_create_decompress.argtypes = [ctypes.c_int]\n    restype = ctypes.POINTER(DecompressionInfoType)\n    OPENJPEG.opj_create_decompress.restype = restype\n    dinfo = OPENJPEG.opj_create_decompress(fmt)\n    return dinfo"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping for openjpeg library function opj_decode.", "response": "def decode(dinfo, cio):\n    \"\"\"Wrapper for opj_decode.\n    \"\"\"\n    argtypes = [ctypes.POINTER(DecompressionInfoType), ctypes.POINTER(CioType)]\n    OPENJPEG.opj_decode.argtypes = argtypes\n    OPENJPEG.opj_decode.restype = ctypes.POINTER(ImageType)\n    image = OPENJPEG.opj_decode(dinfo, cio)\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef destroy_compress(cinfo):\n    argtypes = [ctypes.POINTER(CompressionInfoType)]\n    OPENJPEG.opj_destroy_compress.argtypes = argtypes\n    OPENJPEG.opj_destroy_compress(cinfo)", "response": "Wrapper for openjpeg library function opj_destroy_compress."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwraps for openjpeg library function opj_encode.", "response": "def encode(cinfo, cio, image):\n    \"\"\"Wrapper for openjpeg library function opj_encode.\n\n    Encodes an image into a JPEG-2000 codestream.\n\n    Parameters\n    ----------\n    cinfo : compression handle\n\n    cio : output buffer stream\n\n    image : image to encode\n    \"\"\"\n    argtypes = [ctypes.POINTER(CompressionInfoType),\n                ctypes.POINTER(CioType),\n                ctypes.POINTER(ImageType)]\n    OPENJPEG.opj_encode.argtypes = argtypes\n    OPENJPEG.opj_encode.restype = ctypes.c_int\n    status = OPENJPEG.opj_encode(cinfo, cio, image)\n    return status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef destroy_decompress(dinfo):\n    argtypes = [ctypes.POINTER(DecompressionInfoType)]\n    OPENJPEG.opj_destroy_decompress.argtypes = argtypes\n    OPENJPEG.opj_destroy_decompress(dinfo)", "response": "Wraps openjpeg library function opj_destroy_decompress."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_create(cmptparms, cspace):\n    lst = [ctypes.c_int, ctypes.POINTER(ImageComptParmType), ctypes.c_int]\n    OPENJPEG.opj_image_create.argtypes = lst\n    OPENJPEG.opj_image_create.restype = ctypes.POINTER(ImageType)\n\n    image = OPENJPEG.opj_image_create(len(cmptparms), cmptparms, cspace)\n    return(image)", "response": "Wrapper for openjpeg library function opj_image_create."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping openjpeg library function opj_image_destroy.", "response": "def image_destroy(image):\n    \"\"\"Wraps openjpeg library function opj_image_destroy.\"\"\"\n    OPENJPEG.opj_image_destroy.argtypes = [ctypes.POINTER(ImageType)]\n    OPENJPEG.opj_image_destroy(image)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_default_encoder_parameters():\n    cparams = CompressionParametersType()\n    argtypes = [ctypes.POINTER(CompressionParametersType)]\n    OPENJPEG.opj_set_default_encoder_parameters.argtypes = argtypes\n    OPENJPEG.opj_set_default_encoder_parameters(ctypes.byref(cparams))\n    return cparams", "response": "Wrapper for openjpeg library function opj_set_default_encoder_parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping for openjpeg library function opj_set_event_mgr.", "response": "def set_event_mgr(dinfo, event_mgr, context=None):\n    \"\"\"Wrapper for openjpeg library function opj_set_event_mgr.\n    \"\"\"\n    argtypes = [ctypes.POINTER(CommonStructType),\n                ctypes.POINTER(EventMgrType),\n                ctypes.c_void_p]\n    OPENJPEG.opj_set_event_mgr.argtypes = argtypes\n    OPENJPEG.opj_set_event_mgr(ctypes.cast(dinfo,\n                                           ctypes.POINTER(CommonStructType)),\n                               event_mgr, context)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping for openjpeg library function opj_setup_encoder.", "response": "def setup_encoder(cinfo, cparameters, image):\n    \"\"\"Wrapper for openjpeg library function opj_setup_decoder.\"\"\"\n    argtypes = [ctypes.POINTER(CompressionInfoType),\n                ctypes.POINTER(CompressionParametersType),\n                ctypes.POINTER(ImageType)]\n    OPENJPEG.opj_setup_encoder.argtypes = argtypes\n    OPENJPEG.opj_setup_encoder(cinfo, cparameters, image)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup_decoder(dinfo, dparams):\n    argtypes = [ctypes.POINTER(DecompressionInfoType),\n                ctypes.POINTER(DecompressionParametersType)]\n    OPENJPEG.opj_setup_decoder.argtypes = argtypes\n    OPENJPEG.opj_setup_decoder(dinfo, dparams)", "response": "Wrapper for openjpeg library function opj_setup_decoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_upper_triangular_matrix(matrix):\n\n    # Print column header\n    # Assumes first row contains all needed headers\n    first = sorted(matrix.keys())[0]\n    print('\\t', end=' ')\n    for i in matrix[first]:\n        print('{}\\t'.format(i), end=' ')\n    print()\n\n    indent_count = 0\n\n    for i in matrix:\n        # Print line header\n        print('{}\\t'.format(i), end=' ')\n\n        if indent_count:\n            print('\\t' * indent_count, end=' ')\n\n        for j in sorted(matrix[i]): # required because dict doesn't guarantee insertion order\n            print('{}\\t'.format(matrix[i][j]), end=' ')\n\n        print()\n\n        indent_count = indent_count + 1", "response": "Prints a CVRP upper triangular matrix."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_upper_triangular_matrix_as_complete(matrix):\n    for i in sorted(matrix.keys()):\n        for j in sorted(matrix.keys()):\n            a, b = i, j\n            if a > b:\n                a, b = b, a\n\n            print(matrix[a][b], end=' ')\n\n        print()", "response": "Prints a CVRP upper triangular matrix as a normal matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting a solution to stdout", "response": "def print_solution(solution):\n    \"\"\"Prints a solution\n    \n    Arguments\n    ---------\n    solution : BaseSolution\n\n    Example\n    -------\n    ::\n    \n        [8, 9, 10, 7]: 160\n        [5, 6]: 131\n        [3, 4, 2]: 154\n        Total cost: 445\n        \n    \"\"\"\n    total_cost = 0\n    for solution in solution.routes():\n        cost = solution.length()\n        total_cost = total_cost + cost\n        print('{}: {}'.format(solution, cost))\n        #print('xxx')\n    print('Total cost: {}'.format(total_cost))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates if total generation of a grid in a pkl file is what expected.", "response": "def validate_generation(session, nw):\n    '''Validate if total generation of a grid in a pkl file is what expected.\n    \n    Parameters\n    ----------\n    session : sqlalchemy.orm.session.Session\n        Database session\n    nw:\n        The network\n        \n    Returns\n    -------\n    DataFrame\n        compare_by_level\n    DataFrame\n        compare_by_type\n    '''\n    #config network intern variables\n    nw._config = nw.import_config()\n    nw._pf_config = nw.import_pf_config()\n    nw._static_data = nw.import_static_data()\n    nw._orm = nw.import_orm()\n\n    #rescue generation from input table\n    generation_input = nw.list_generators(session)\n\n    #make table of generators that are in the grid\n    gen_idx = 0\n    gen_dict = {}\n    for mv_district in nw.mv_grid_districts():\n        #search over MV grid\n        for node in mv_district.mv_grid.graph_nodes_sorted():\n            if isinstance(node, GeneratorDing0):\n                gen_idx+=1\n                subtype = node.subtype\n                if subtype == None:\n                    subtype = 'other'\n                type = node.type\n                if type == None:\n                    type = 'other'\n                gen_dict[gen_idx] = {\n                    'v_level':node.v_level,\n                    'type':type,\n                    'subtype':subtype,\n                    'GenCap':node.capacity,\n                }\n\n        #search over LV grids\n        for LA in mv_district.lv_load_areas():\n            for lv_district in LA.lv_grid_districts():\n                # generation capacity\n                for g in lv_district.lv_grid.generators():\n                    gen_idx+=1\n                    subtype = g.subtype\n                    if subtype == None:\n                        subtype = 'other'\n                    type = g.type\n                    if type == None:\n                        type = 'other'\n                    gen_dict[gen_idx] = {\n                        'v_level':g.v_level,\n                        'type':type,\n                        'subtype':subtype,\n                        'GenCap':g.capacity,\n                    }\n\n    generation_effective = pd.DataFrame.from_dict(gen_dict, orient='index')\n\n    #compare by voltage level\n    input_by_level = generation_input.groupby('v_level').sum()['GenCap'].apply(lambda x: np.round(x,3))\n    effective_by_level = generation_effective.groupby('v_level').sum()['GenCap'].apply(lambda x: np.round(x,3))\n\n    compare_by_level = pd.concat([input_by_level,effective_by_level,input_by_level==effective_by_level],axis=1)\n    compare_by_level.columns = ['table','ding0','equal?']\n\n    #compare by type/subtype\n    generation_input['type'] =generation_input['type']+'/'+generation_input['subtype']\n    generation_effective['type'] =generation_effective['type']+'/'+generation_effective['subtype']\n\n    input_by_type = generation_input.groupby('type').sum()['GenCap'].apply(lambda x: np.round(x,3))\n    effective_by_type = generation_effective.groupby('type').sum()['GenCap'].apply(lambda x: np.round(x,3))\n\n    compare_by_type = pd.concat([input_by_type,effective_by_type,input_by_type==effective_by_type],axis=1)\n    compare_by_type.columns = ['table','ding0','equal?']\n    compare_by_type.index.names = ['type/subtype']\n\n    return compare_by_level, compare_by_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_load_areas(session, nw):\n    '''Validate if total load of a grid in a pkl file is what expected from load areas\n    \n    Parameters\n    ----------\n    session : sqlalchemy.orm.session.Session\n        Database session\n    nw\n        The network\n        \n    Returns\n    -------\n    DataFrame\n        compare_by_la\n    Bool\n        True if data base IDs of LAs are the same as the IDs in the grid \n    '''\n    #config network intern variables\n    nw._config = nw.import_config()\n    nw._pf_config = nw.import_pf_config()\n    nw._static_data = nw.import_static_data()\n    nw._orm = nw.import_orm()\n\n    #rescue peak load from input table\n    load_input = nw.list_load_areas(session, nw.mv_grid_districts())\n    la_input = sorted(load_input.index)\n    load_input = load_input.sum(axis=0).apply(lambda x: np.round(x,3))\n    load_input.sort_index(inplace=True)\n\n    #search for LA in the grid\n    la_idx = 0\n    la_dict = {}\n    for mv_district in nw.mv_grid_districts():\n        for LA in mv_district.lv_load_areas():\n            la_idx +=1\n            la_dict[la_idx] = {\n                'id_db':LA.id_db,\n                'peak_load_residential':LA.peak_load_residential,\n                'peak_load_retail':LA.peak_load_retail,\n                'peak_load_industrial':LA.peak_load_industrial,\n                'peak_load_agricultural':LA.peak_load_agricultural,\n            }\n\n    #compare by LA\n    load_effective = pd.DataFrame.from_dict(la_dict,orient='index').set_index('id_db')\n    la_effective = sorted(load_effective.index)\n    load_effective = load_effective.sum(axis=0).apply(lambda x: np.round(x,3))\n    load_effective.sort_index(inplace=True)\n\n    compare_by_la = pd.concat([load_input,load_effective,load_input==load_effective],axis=1)\n    compare_by_la.columns = ['table','ding0','equal?']\n    compare_by_la.index.names = ['sector']\n\n    return compare_by_la, la_input==la_effective", "response": "Validate if total load of a grid in a pkl file is what expected from load areas in a network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate if total load of a grid in a pkl file is what expected from LV districts.", "response": "def validate_lv_districts(session, nw):\n    '''Validate if total load of a grid in a pkl file is what expected from LV districts\n\n    Parameters\n    ----------\n    session : sqlalchemy.orm.session.Session\n            Database session\n    nw: \n        The network\n\n    Returns\n    -------\n    DataFrame\n        compare_by_district\n    DataFrame\n        compare_by_loads\n    '''\n    # config network intern variables\n    nw._config = nw.import_config()\n    nw._pf_config = nw.import_pf_config()\n    nw._static_data = nw.import_static_data()\n    nw._orm = nw.import_orm()\n\n    # rescue peak load from input table\n    lv_ditricts = [dist.id_db for mv in nw.mv_grid_districts()\n                              for la in mv.lv_load_areas()\n                              for dist in la.lv_grid_districts()]\n    load_input = nw.list_lv_grid_districts(session, lv_ditricts)\n    load_input = load_input.sum(axis=0).apply(lambda x: np.round(x, 3))\n    load_input.sort_index(inplace=True)\n    load_input.index.names = ['id_db']\n    load_input['peak_load_retind']=load_input['peak_load_retail']+load_input['peak_load_industrial']\n\n    # search for lv_district in the grid\n    lv_dist_idx = 0\n    lv_dist_dict = {}\n    lv_load_idx = 0\n    lv_load_dict = {}\n    for mv_district in nw.mv_grid_districts():\n        for LA in mv_district.lv_load_areas():\n            for lv_district in LA.lv_grid_districts():\n                lv_dist_idx += 1\n                lv_dist_dict[lv_dist_idx] = {\n                    'id_db':lv_district.id_db,\n                    'peak_load_residential':lv_district.peak_load_residential,\n                    'peak_load_retail':lv_district.peak_load_retail,\n                    'peak_load_industrial':lv_district.peak_load_industrial,\n                    'peak_load_agricultural':lv_district.peak_load_agricultural,\n                    'peak_load_retind': lv_district.peak_load_industrial + lv_district.peak_load_retail,\n                }\n                for node in lv_district.lv_grid.graph_nodes_sorted():\n                    if isinstance(node,LVLoadDing0):\n                        lv_load_idx +=1\n                        peak_load_agricultural = 0\n                        peak_load_residential = 0\n                        peak_load_retail = 0\n                        peak_load_industrial = 0\n                        peak_load_retind = 0\n\n                        if 'agricultural' in node.consumption:\n                            tipo = 'agricultural'\n                            peak_load_agricultural = node.peak_load\n                        elif 'industrial' in node.consumption:\n                            if node.consumption['retail']==0:\n                                tipo = 'industrial'\n                                peak_load_industrial = node.peak_load\n                            elif node.consumption['industrial']==0:\n                                tipo = 'retail'\n                                peak_load_retail = node.peak_load\n                            else:\n                                tipo = 'ret_ind'\n                                peak_load_retind = node.peak_load\n                        elif 'residential' in node.consumption:\n                            tipo = 'residential'\n                            peak_load_residential = node.peak_load\n                        else:\n                            tipo = 'none'\n                            print(node.consumption)\n                        lv_load_dict[lv_load_idx] = {\n                            'id_db':node.id_db,\n                            'peak_load_residential':peak_load_residential,\n                            'peak_load_retail':peak_load_retail,\n                            'peak_load_industrial':peak_load_industrial,\n                            'peak_load_agricultural':peak_load_agricultural,\n                            'peak_load_retind':peak_load_retind,\n                        }\n\n        for node in mv_district.mv_grid.graph_nodes_sorted():\n            if isinstance(node,LVLoadAreaCentreDing0):\n                lv_load_idx +=1\n                lv_load_dict[lv_load_idx] = {\n                    'id_db': node.id_db,\n                    'peak_load_residential': node.lv_load_area.peak_load_residential,\n                    'peak_load_retail': node.lv_load_area.peak_load_retail,\n                    'peak_load_industrial': node.lv_load_area.peak_load_industrial,\n                    'peak_load_agricultural': node.lv_load_area.peak_load_agricultural,\n                    'peak_load_retind':0,\n                }\n\n    #compare by LV district\n    load_effective_lv_distr = pd.DataFrame.from_dict(lv_dist_dict,orient='index').set_index('id_db').sum(axis=0).apply(lambda x: np.round(x,3))\n    load_effective_lv_distr.sort_index(inplace=True)\n\n    compare_by_district = pd.concat([load_input,load_effective_lv_distr,load_input==load_effective_lv_distr],axis=1)\n    compare_by_district.columns = ['table','ding0','equal?']\n    compare_by_district.index.names = ['sector']\n\n    #compare by LV Loads\n    load_effective_lv_load = pd.DataFrame.from_dict(lv_load_dict,orient='index').set_index('id_db')\n    load_effective_lv_load = load_effective_lv_load.sum(axis=0).apply(lambda x: np.round(x,3))\n    load_effective_lv_load.sort_index(inplace=True)\n\n    load_effective_lv_load['peak_load_retind'] = load_effective_lv_load['peak_load_retail'] + \\\n                                                 load_effective_lv_load['peak_load_industrial'] + \\\n                                                 load_effective_lv_load['peak_load_retind']\n\n    compare_by_load = pd.concat([load_input,load_effective_lv_load,load_input==load_effective_lv_load],axis=1)\n    compare_by_load.columns = ['table','ding0','equal?']\n    compare_by_load.index.names = ['sector']\n\n    return compare_by_district, compare_by_load"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for over - loading of branches and transformers for MV or LV grid.", "response": "def check_load(grid, mode):\n    \"\"\" Checks for over-loading of branches and transformers for MV or LV grid.\n\n    Parameters\n    ----------\n    grid : GridDing0\n        Grid identifier.\n    mode : str\n        Kind of grid ('MV' or 'LV').\n\n    Returns\n    -------\n    :obj:`dict`\n        Dict of critical branches with max. relative overloading, and the \n        following format::\n        \n            {\n            branch_1: rel_overloading_1, \n            ..., \n            branch_n: rel_overloading_n\n            }\n        \n    :any:`list` of :obj:`GridDing0`\n        List of critical transformers with the following format::\n        \n        [trafo_1, ..., trafo_m]\n\n    Notes\n    -----\n        Lines'/cables' max. capacity (load case and feed-in case) are taken from [#]_.\n        \n\n    References\n    ----------\n    .. [#] dena VNS\n    \n    See Also\n    --------\n    ding0.flexopt.reinforce_measures.reinforce_branches_current :\n    ding0.flexopt.reinforce_measures.reinforce_branches_voltage :\n    \n    \"\"\"\n\n    crit_branches = {}\n    crit_stations = []\n\n    if mode == 'MV':\n        # load load factors (conditions) for cables, lines and trafos for load- and feedin case\n\n        # load_factor_mv_trans_lc_normal = float(cfg_ding0.get('assumptions',\n        #                                                      'load_factor_mv_trans_lc_normal'))\n        load_factor_mv_line_lc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_line_lc_normal'))\n        load_factor_mv_cable_lc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_cable_lc_normal'))\n        #load_factor_mv_trans_fc_normal = float(cfg_ding0.get('assumptions',\n        #                                                     'load_factor_mv_trans_fc_normal'))\n        load_factor_mv_line_fc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_line_fc_normal'))\n        load_factor_mv_cable_fc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_cable_fc_normal'))\n\n        mw2kw = 1e3\n        kw2mw = 1e-3\n\n        # STEP 1: check branches' loads\n        for branch in grid.graph_edges():\n            s_max_th = 3**0.5 * branch['branch'].type['U_n'] * branch['branch'].type['I_max_th']\n\n            if branch['branch'].kind == 'line':\n                s_max_th_lcfc = [s_max_th * load_factor_mv_line_lc_normal,\n                                 s_max_th * load_factor_mv_line_fc_normal]\n            elif branch['branch'].kind == 'cable':\n                s_max_th_lcfc = [s_max_th * load_factor_mv_cable_lc_normal,\n                                 s_max_th * load_factor_mv_cable_fc_normal]\n            else:\n                raise ValueError('Branch kind is invalid!')\n\n            # check loads only for non-aggregated Load Areas (aggregated ones are skipped raising except)\n            try:\n                # check if s_res exceeds allowed values for laod and feedin case\n                # CAUTION: The order of values is fix! (1. load case, 2. feedin case)\n                if any([s_res * mw2kw > _ for s_res, _ in zip(branch['branch'].s_res, s_max_th_lcfc)]):\n                    # save max. relative overloading\n                    crit_branches[branch] = max(branch['branch'].s_res) * mw2kw / s_max_th\n            except:\n                pass\n\n        # STEP 2: check HV-MV station's load\n\n        # NOTE: HV-MV station reinforcement is not required for status-quo\n        # scenario since HV-MV trafos already sufficient for load+generation\n        # case as done in MVStationDing0.choose_transformers()\n\n        # OLD snippet:\n        # cum_peak_load = grid.grid_district.peak_load\n        # cum_peak_generation = grid.station().peak_generation(mode='MVLV')\n        #\n        # # reinforcement necessary only if generation > load\n        # if cum_peak_generation > cum_peak_load:\n        #     grid.station().choose_transformers\n        #\n        # cum_trafo_capacity = sum((_.s_max_a for _ in grid.station().transformers()))\n        #\n        # max_trafo = max((_.s_max_a for _ in grid.station().transformers()))\n        #\n        # # determine number and size of required transformers\n        # kw2mw = 1e-3\n        # residual_apparent_power = cum_generation_sum * kw2mw - \\\n        #                           cum_trafo_capacity\n\n    elif mode == 'LV':\n        raise NotImplementedError\n\n    if crit_branches:\n        logger.info('==> {} branches have load issues.'.format(\n            len(crit_branches)))\n    if crit_stations:\n        logger.info('==> {} stations have load issues.'.format(\n            len(crit_stations)))\n\n    return crit_branches, crit_stations"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking for voltage stability issues at all nodes for MV or LV grid.", "response": "def check_voltage(grid, mode):\n    \"\"\" Checks for voltage stability issues at all nodes for MV or LV grid\n\n    Parameters\n    ----------\n    grid : GridDing0\n        Grid identifier.\n    mode : str\n        Kind of grid ('MV' or 'LV').\n\n    Returns\n    -------\n    :any:`list` of :any:`GridDing0`\n        List of critical nodes, sorted descending by voltage difference.\n\n    Notes\n    -----\n        The examination is done in two steps, according to [#]_ :\n        \n        1. It is checked #TODO: what?\n        \n        2. #TODO: what's next?\n        \n    References\n    ----------\n    .. [#] dena VNS\n    \"\"\"\n\n    crit_nodes = {}\n\n    if mode == 'MV':\n        # load max. voltage difference for load and feedin case\n        mv_max_v_level_lc_diff_normal = float(cfg_ding0.get('mv_routing_tech_constraints',\n                                                            'mv_max_v_level_lc_diff_normal'))\n        mv_max_v_level_fc_diff_normal = float(cfg_ding0.get('mv_routing_tech_constraints',\n                                                            'mv_max_v_level_fc_diff_normal'))\n\n        # check nodes' voltages\n        voltage_station = grid._station.voltage_res\n        for node in grid.graph_nodes_sorted():\n            try:\n                # compare node's voltage with max. allowed voltage difference for load and feedin case\n                if (abs(voltage_station[0] - node.voltage_res[0]) > mv_max_v_level_lc_diff_normal) or\\\n                   (abs(voltage_station[1] - node.voltage_res[1]) > mv_max_v_level_fc_diff_normal):\n\n                    crit_nodes[node] = {'node': node,\n                                        'v_diff': max([abs(v2-v1) for v1, v2 in zip(node.voltage_res, voltage_station)])}\n            except:\n                pass\n\n    elif mode == 'LV':\n        raise NotImplementedError\n\n    if crit_nodes:\n        logger.info('==> {} nodes have voltage issues.'.format(len(crit_nodes)))\n\n    return [_['node'] for _ in sorted(crit_nodes.values(), key=lambda _: _['v_diff'], reverse=True)]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_critical_line_loading(grid):\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n    lf_trafo_load = cfg_ding0.get('assumptions',\n                                  \"load_factor_lv_trans_lc_normal\")\n    lf_trafo_gen = cfg_ding0.get('assumptions',\n                                  \"load_factor_lv_trans_fc_normal\")\n\n    critical_branches = []\n    critical_stations = []\n\n    # Convert grid to a tree (is a directed graph)\n    # based on this tree, descendants of each node are accessible\n    station = grid._station\n\n    tree = nx.dfs_tree(grid._graph, station)\n\n    for node in tree.nodes():\n\n        # list of descendant nodes including the node itself\n        descendants = list(nx.descendants(tree, node))\n        descendants.append(node)\n\n        if isinstance(node, LVStationDing0):\n            # determine cumulative peak load at node and assign to branch\n            peak_load, peak_gen = peak_load_generation_at_node(descendants)\n\n            if grid.id_db == 61107:\n                if isinstance(node, LVStationDing0):\n                    print(node)\n            # get trafos cumulative apparent power\n            s_max_trafos = sum([_.s_max_a for _ in node._transformers])\n\n            # compare with load and generation connected to\n            if (((peak_load / cos_phi_load) > s_max_trafos * lf_trafo_load) or\n                    ((peak_gen / cos_phi_feedin) > s_max_trafos * lf_trafo_gen)):\n                critical_stations.append(\n                    {'station': node,\n                     's_max': [\n                         peak_load / cos_phi_load,\n                         peak_gen / cos_phi_feedin]})\n\n        else:\n            # preceeding node of node\n            predecessors = list(tree.predecessors(node))\n\n            # a non-meshed grid topology returns a list with only 1 item\n            predecessor = predecessors[0]\n\n            # get preceeding\n            branches = grid.graph_branches_from_node(node)\n            preceeding_branch = [branch for branch in branches\n                                 if branch[0] is predecessor][0]\n\n            # determine cumulative peak load at node and assign to branch\n            peak_load, peak_gen = peak_load_generation_at_node(descendants)\n\n            s_max_th = 3 ** 0.5 * preceeding_branch[1]['branch'].type['U_n'] * \\\n                       preceeding_branch[1]['branch'].type['I_max_th'] / 1e3\n\n            if (((peak_load / cos_phi_load) > s_max_th) or\n                    ((peak_gen / cos_phi_feedin) > s_max_th)):\n                critical_branches.append(\n                    {'branch': preceeding_branch[1]['branch'],\n                     's_max': [\n                         peak_load / cos_phi_load,\n                         peak_gen / cos_phi_feedin]})\n\n    return critical_branches, critical_stations", "response": "Returns a list of critical line loading for each LV station and branch."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the maximum occuring load and generation at a certain node", "response": "def peak_load_generation_at_node(nodes):\n    \"\"\"\n    Get maximum occuring load and generation at a certain node\n\n    Summarizes peak loads and nominal generation power of descendant nodes\n    of a branch\n\n    Parameters\n    ----------\n    nodes : :any:`list`\n        Any LV grid Ding0 node object that is part of the grid topology\n\n    Return\n    ------\n    :any:`float`\n        peak_load : Sum of peak loads of descendant nodes\n    :any:`float`\n        peak_generation : Sum of nominal power of generation at descendant nodes\n    \"\"\"\n\n    loads = [node.peak_load for node in nodes\n             if isinstance(node, LVLoadDing0)]\n    peak_load = sum(loads)\n\n    generation = [node.capacity for node in nodes\n             if isinstance(node, GeneratorDing0)]\n    peak_generation = sum(generation)\n\n    return peak_load, peak_generation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_critical_voltage_at_nodes(grid):\n\n    v_delta_tolerable_fc = cfg_ding0.get('assumptions',\n                                      'lv_max_v_level_fc_diff_normal')\n    v_delta_tolerable_lc = cfg_ding0.get('assumptions',\n                                      'lv_max_v_level_lc_diff_normal')\n\n    omega = 2 * math.pi * 50\n\n    crit_nodes = []\n\n    # get list of nodes of main branch in right order\n    tree = nx.dfs_tree(grid._graph, grid._station)\n\n    # list for nodes of main branch\n    main_branch = []\n\n    # list of stub cable distributors branching from main branch\n    grid_conn_points = []\n\n    # fill two above lists\n    for node in list(nx.descendants(tree, grid._station)):\n        successors = list(tree.successors(node))\n        if successors and all(isinstance(successor, LVCableDistributorDing0)\n               for successor in successors):\n            main_branch.append(node)\n        elif (isinstance(node, LVCableDistributorDing0) and\n            all(isinstance(successor, (GeneratorDing0, LVLoadDing0))\n               for successor in successors)):\n            grid_conn_points.append(node)\n\n    # voltage at substation bus bar\n    r_mv_grid, x_mv_grid = get_mv_impedance(grid)\n\n    r_trafo = sum([tr.r for tr in grid._station._transformers])\n    x_trafo = sum([tr.x for tr in grid._station._transformers])\n\n    v_delta_load_case_bus_bar, \\\n    v_delta_gen_case_bus_bar = get_voltage_at_bus_bar(grid, tree)\n\n    if (abs(v_delta_gen_case_bus_bar) > v_delta_tolerable_fc\n        or abs(v_delta_load_case_bus_bar) > v_delta_tolerable_lc):\n        crit_nodes.append({'node': grid._station,\n                           'v_diff': [v_delta_load_case_bus_bar,\n                                      v_delta_gen_case_bus_bar]})\n\n\n\n    # voltage at main route nodes\n    for first_node in [b for b in tree.successors(grid._station)\n                   if b in main_branch]:\n\n        # cumulative resistance/reactance at bus bar\n        r = r_mv_grid + r_trafo\n        x = x_mv_grid + x_trafo\n\n        # roughly estimate transverse voltage drop\n        stub_node = [_ for _ in list(tree.successors(first_node)) if\n                     _ not in main_branch][0]\n        v_delta_load_stub, v_delta_gen_stub = voltage_delta_stub(\n            grid,\n            tree,\n            first_node,\n            stub_node,\n            r,\n            x)\n\n        # cumulative voltage drop/increase at substation bus bar\n        v_delta_load_cum = v_delta_load_case_bus_bar\n        v_delta_gen_cum = v_delta_gen_case_bus_bar\n\n        # calculate voltage at first node of branch\n        voltage_delta_load, voltage_delta_gen, r, x = \\\n            get_voltage_delta_branch(grid, tree, first_node, r, x)\n\n        v_delta_load_cum += voltage_delta_load\n        v_delta_gen_cum += voltage_delta_gen\n\n        if (abs(v_delta_gen_cum) > (v_delta_tolerable_fc - v_delta_gen_stub)\n            or abs(v_delta_load_cum) > (v_delta_tolerable_lc - v_delta_load_stub)):\n            crit_nodes.append({'node': first_node,\n                               'v_diff': [v_delta_load_cum,\n                                          v_delta_gen_cum]})\n            crit_nodes.append({'node': stub_node,\n                               'v_diff': [\n                                   v_delta_load_cum + v_delta_load_stub,\n                                   v_delta_gen_cum + v_delta_gen_stub]})\n\n        # get next neighboring nodes down the tree\n        successor = [x for x in tree.successors(first_node)\n                      if x in main_branch]\n        if successor:\n            successor = successor[0] # simply unpack\n\n        # successively determine voltage levels for succeeding nodes\n        while successor:\n            voltage_delta_load, voltage_delta_gen, r, x = \\\n                get_voltage_delta_branch(grid, tree, successor, r, x)\n\n            v_delta_load_cum += voltage_delta_load\n            v_delta_gen_cum += voltage_delta_gen\n\n            # roughly estimate transverse voltage drop\n            stub_node = [_ for _ in tree.successors(successor) if\n                         _ not in main_branch][0]\n            v_delta_load_stub, v_delta_gen_stub = voltage_delta_stub(\n                grid,\n                tree,\n                successor,\n                stub_node,\n                r,\n                x)\n\n            if (abs(v_delta_gen_cum) > (v_delta_tolerable_fc - v_delta_gen_stub)\n                or abs(v_delta_load_cum) > (\n                            v_delta_tolerable_lc - v_delta_load_stub)):\n                crit_nodes.append({'node': successor,\n                                   'v_diff': [v_delta_load_cum,\n                                              v_delta_gen_cum]})\n                crit_nodes.append({'node': stub_node,\n                                   'v_diff': [\n                                       v_delta_load_cum + v_delta_load_stub,\n                                       v_delta_gen_cum + v_delta_gen_stub]})\n\n            successor = [_ for _ in tree.successors(successor)\n                         if _ in main_branch]\n            if successor:\n                successor = successor[0]\n\n    return crit_nodes", "response": "r Returns the critical voltage at each node of the LV grid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef voltage_delta_vde(v_nom, s_max, r, x, cos_phi):\n    delta_v = (s_max * (\n        r * cos_phi + x * math.sin(math.acos(cos_phi)))) / v_nom ** 2\n    return delta_v", "response": "Estimate the voltage drop or increase in radial grids of a given voltage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the total generation and peak load of neighboring house connected to main branch.", "response": "def get_house_conn_gen_load(graph, node):\n    \"\"\"\n    Get generation capacity/ peak load of neighboring house connected to main\n    branch\n\n    Parameters\n    ----------\n    graph : :networkx:`NetworkX Graph Obj< >`\n        Directed graph\n    node : graph node\n        Node of the main branch of LV grid\n\n    Returns\n    -------\n    :any:`list`\n        A list containing two items\n        \n        # peak load of connected house branch\n        # generation capacity of connected generators\n    \"\"\"\n    generation = 0\n    peak_load = 0\n\n    for cus_1 in graph.successors(node):\n        for cus_2 in graph.successors(cus_1):\n            if not isinstance(cus_2, list):\n                cus_2 = [cus_2]\n            generation += sum([gen.capacity for gen in cus_2\n                          if isinstance(gen, GeneratorDing0)])\n            peak_load += sum([load.peak_load for load in cus_2\n                          if isinstance(load, LVLoadDing0)])\n\n    return [peak_load, generation]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_voltage_delta_branch(grid, tree, node, r_preceeding, x_preceeding):\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n    v_nom = cfg_ding0.get('assumptions', 'lv_nominal_voltage')\n    omega = 2 * math.pi * 50\n\n    # add resitance/ reactance to preceeding\n    in_edge = [_ for _ in grid.graph_branches_from_node(node) if\n               _[0] in list(tree.predecessors(node))][0][1]\n    r = r_preceeding + (in_edge['branch'].type['R'] *\n                     in_edge['branch'].length)\n    x = x_preceeding + (in_edge['branch'].type['L'] / 1e3 * omega *\n                     in_edge['branch'].length)\n\n    # get apparent power for load and generation case\n    peak_load, gen_capacity = get_house_conn_gen_load(tree, node)\n    s_max_load = peak_load / cos_phi_load\n    s_max_feedin = gen_capacity / cos_phi_feedin\n\n    # determine voltage increase/ drop a node\n    voltage_delta_load = voltage_delta_vde(v_nom, s_max_load, r, x,\n                                           cos_phi_load)\n    voltage_delta_gen = voltage_delta_vde(v_nom, s_max_feedin, r, -x,\n                                          cos_phi_feedin)\n\n    return [voltage_delta_load, voltage_delta_gen, r, x]", "response": "Returns the voltage delta for a preceeding branch of node at a given voltage level."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the impedance of MV grid", "response": "def get_mv_impedance(grid):\n    \"\"\"\n    Determine MV grid impedance (resistance and reactance separately)\n\n    Parameters\n    ----------\n    grid : LVGridDing0\n\n    Returns\n    -------\n    :any:`list`\n        List containing resistance and reactance of MV grid\n    \"\"\"\n\n    omega = 2 * math.pi * 50\n\n    mv_grid = grid.grid_district.lv_load_area.mv_grid_district.mv_grid\n    edges = mv_grid.find_path(grid._station, mv_grid._station, type='edges')\n    r_mv_grid = sum([e[2]['branch'].type['R'] * e[2]['branch'].length / 1e3\n                     for e in edges])\n    x_mv_grid = sum([e[2]['branch'].type['L'] / 1e3 * omega * e[2][\n        'branch'].length / 1e3 for e in edges])\n\n    return [r_mv_grid, x_mv_grid]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the voltage delta for a stub branch.", "response": "def voltage_delta_stub(grid, tree, main_branch_node, stub_node, r_preceeding,\n                       x_preceedig):\n    \"\"\"\n    Determine voltage for stub branches\n\n    Parameters\n    ----------\n    grid : LVGridDing0\n        Ding0 grid object\n    tree : :networkx:`NetworkX Graph Obj< >`\n        Tree of grid topology\n    main_branch_node : graph node\n        Node of main branch that stub branch node in connected to\n    main_branch : dict\n        Nodes of main branch\n    r_preceeding : float\n        Resitance of preceeding grid\n    x_preceeding : float\n        Reactance of preceeding grid\n\n    Return\n    ------\n    :any:`float`\n        Delta voltage for node\n    \"\"\"\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n    v_nom = cfg_ding0.get('assumptions', 'lv_nominal_voltage')\n    omega = 2 * math.pi * 50\n\n    stub_branch = [_ for _ in grid.graph_branches_from_node(main_branch_node) if\n                   _[0] == stub_node][0][1]\n    r_stub = stub_branch['branch'].type['R'] * stub_branch[\n        'branch'].length / 1e3\n    x_stub = stub_branch['branch'].type['L'] / 1e3 * omega * \\\n             stub_branch['branch'].length / 1e3\n    s_max_gen = [_.capacity / cos_phi_feedin\n                 for _ in tree.successors(stub_node)\n                 if isinstance(_, GeneratorDing0)]\n    if s_max_gen:\n        s_max_gen = s_max_gen[0]\n        v_delta_stub_gen = voltage_delta_vde(v_nom, s_max_gen, r_stub + r_preceeding,\n                                             x_stub + x_preceedig, cos_phi_feedin)\n    else:\n        v_delta_stub_gen = 0\n\n    s_max_load = [_.peak_load / cos_phi_load\n                  for _ in tree.successors(stub_node)\n                  if isinstance(_, LVLoadDing0)]\n    if s_max_load:\n        s_max_load = s_max_load[0]\n        v_delta_stub_load = voltage_delta_vde(v_nom, s_max_load, r_stub + r_preceeding,\n                                              x_stub + x_preceedig, cos_phi_load)\n    else:\n        v_delta_stub_load = 0\n\n    return [v_delta_stub_load, v_delta_stub_gen]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_voltage_at_bus_bar(grid, tree):\n\n    # voltage at substation bus bar\n    r_mv_grid, x_mv_grid = get_mv_impedance(grid)\n\n    r_trafo = sum([tr.r for tr in grid._station._transformers])\n    x_trafo = sum([tr.x for tr in grid._station._transformers])\n\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n    v_nom = cfg_ding0.get('assumptions', 'lv_nominal_voltage')\n\n    # loads and generators connected to bus bar\n    bus_bar_load = sum(\n        [node.peak_load for node in tree.successors(grid._station)\n         if isinstance(node, LVLoadDing0)]) / cos_phi_load\n    bus_bar_generation = sum(\n        [node.capacity for node in tree.successors(grid._station)\n         if isinstance(node, GeneratorDing0)]) / cos_phi_feedin\n\n    v_delta_load_case_bus_bar = voltage_delta_vde(v_nom,\n                                                  bus_bar_load,\n                                                  (r_mv_grid + r_trafo),\n                                                  (x_mv_grid + x_trafo),\n                                                  cos_phi_load)\n    v_delta_gen_case_bus_bar = voltage_delta_vde(v_nom,\n                                                 bus_bar_generation,\n                                                 (r_mv_grid + r_trafo),\n                                                 -(x_mv_grid + x_trafo),\n                                                 cos_phi_feedin)\n\n    return v_delta_load_case_bus_bar, v_delta_gen_case_bus_bar", "response": "Returns voltage level at bus bar of MV - LV substation"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_lv_load_area(self, lv_load_area):\n        # TODO: check docstring\n        \"\"\"Adds a LV load_area to _lv_load_areas if not already existing\n        \n        Args\n        ----\n        lv_load_area: :shapely:`Shapely Polygon object<polygons>`\n            Descr\n        \"\"\"\n        self._lv_load_areas.append(lv_load_area)\n        if not isinstance(lv_load_area, MVCableDistributorDing0):\n            self.peak_load += lv_load_area.peak_load", "response": "Adds a LV load_area to _lv_load_areas if not already existing."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef can_add_lv_load_area(self, node):\n        # TODO: check docstring\n        \"\"\"Sums up peak load of LV stations \n        \n        That is, total peak load for satellite string\n        \n        Args\n        ----\n        node: GridDing0\n            Descr\n        \n        Returns\n        -------\n        bool\n            True if ????\n        \n        \"\"\"\n        # get power factor for loads\n        cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n\n        lv_load_area = node.lv_load_area\n        if lv_load_area not in self.lv_load_areas():  # and isinstance(lv_load_area, LVLoadAreaDing0):\n            path_length_to_root = lv_load_area.mv_grid_district.mv_grid.graph_path_length(self.root_node, node)\n            if ((path_length_to_root <= self.branch_length_max) and\n                (lv_load_area.peak_load + self.peak_load) / cos_phi_load <= self.peak_load_max):\n                return True\n            else:\n                return False", "response": "Check if a node can add LV load area."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies Or - Opt intra - route operator to a given solution.", "response": "def operator_oropt(self, graph, solution, op_diff_round_digits, anim=None):\n        # TODO: check docstring\n        \"\"\"Applies Or-Opt intra-route operator to solution\n        \n        Takes chains of nodes (length=3..1 consecutive nodes) from a given\n        route and calculates savings when inserted into another position on the\n        same route (all possible positions). Performes best move (max. saving)\n        and starts over again with new route until no improvement is found.\n        \n        Args\n        ----\n        graph: :networkx:`NetworkX Graph Obj< >`\n            A NetworkX graaph is used.\n        solution: BaseSolution\n            BaseSolution instance\n        op_diff_round_digits: float\n            Precision (floating point digits) for rounding route length differences.\n            \n            *Details*: In some cases when an exchange is performed on two routes with one node each,\n            the difference between the both solutions (before and after the exchange) is not zero.\n            This is due to internal rounding errors of float type. So the loop won't break\n            (alternating between these two solutions), we need an additional criterion to avoid\n            this behaviour: A threshold to handle values very close to zero as if they were zero\n            (for a more detailed description of the matter see http://floating-point-gui.de or\n            https://docs.python.org/3.5/tutorial/floatingpoint.html)\n        anim: AnimationDing0\n            AnimationDing0 object\n        \n        Returns\n        -------\n        LocalSearchSolution\n           A solution (LocalSearchSolution class)\n        \n        Notes\n        -----\n        Since Or-Opt is an intra-route operator, it has not to be checked if route can allocate (Route's method\n        can_allocate()) nodes during relocation regarding max. peak load/current because the line/cable type is the\n        same along the entire route. However, node order within a route has an impact on the voltage stability\n        so the check would be actually required. Due to large line capacity (load factor of lines/cables ~60 %)\n        the voltage stability issues are neglected.\n\n        (Inner) Loop variables:\n        \n        * s: length (count of consecutive nodes) of the chain that is moved. Values: 3..1\n        * i: node that precedes the chain before moving (position in the route `tour`, not node name)\n        * j: node that precedes the chain after moving (position in the route `tour`, not node name)\n        \n        Todo\n        ----\n        * insert literature reference for Or-algorithm here\n        * Remove ugly nested loops, convert to more efficient matrix operations\n        \"\"\"\n        no_ctr = 100\n        # shorter var names for loop\n        dm = graph._matrix\n        dn = graph._nodes\n        \n        for route in solution.routes():\n\n            # exclude routes with single high-demand nodes (Load Areas)\n            if len(route._nodes) == 1:\n                if solution._problem._is_aggregated[str(route._nodes[0])]:\n                    continue\n\n            n = len(route._nodes)+1\n\n            # create tour by adding depot at start and end\n            tour = [graph._depot] + route._nodes + [graph._depot]\n            \n            # Or-Opt: Search better solutions by checking possible chain moves\n            while True:\n                length = route.length()\n                length_best = length\n                \n                for s in range(3,0,-1):\n                    for i in range(1,n-s):\n                        length_diff = (length -\n                                       dm[dn[tour[i-1].name()]][dn[tour[i].name()]] -\n                                       dm[dn[tour[i+s-1].name()]][dn[tour[i+s].name()]] +\n                                       dm[dn[tour[i-1].name()]][dn[tour[i+s].name()]])\n                        for j in range(i+s+1,n+1):\n                            if j == n:\n                                j2 = 1\n                            else:\n                                j2 = j+1\n                            length_new = (length_diff +\n                                          dm[dn[tour[j-1].name()]][dn[tour[i].name()]] +\n                                          dm[dn[tour[i+s-1].name()]][dn[tour[j2-1].name()]] -\n                                          dm[dn[tour[j-1].name()]][dn[tour[j2-1].name()]])\n                            if length_new < length_best:\n                                length_best = length_new\n                                s_best, i_best, j_best = s, i, j\n                if length_best < length:\n                    tour = tour[0:i_best] + tour[i_best+s_best:j_best] + tour[i_best:i_best+s_best] + tour[j_best:n+1]\n\n                    if anim is not None:\n                        solution.draw_network(anim)\n\n                # no improvement found\n                if length_best == length:\n                    # replace old route by new (same arg for allocation and deallocation since node order is considered at allocation)\n                    solution._routes[solution._routes.index(route)].deallocate(tour[1:-1])\n                    solution._routes[solution._routes.index(route)].allocate(tour[1:-1])\n                    \n                    break\n        \n        #solution = LocalSearchSolution(solution, graph, new_routes)\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef operator_relocate(self, graph, solution, op_diff_round_digits, anim):\n        # shorter var names for loop\n        dm = graph._matrix\n        dn = graph._nodes        \n        \n        # Relocate: Search better solutions by checking possible node moves\n        while True:\n            length_diff_best = 0\n            \n            for route in solution.routes():\n\n                # exclude origin routes with single high-demand nodes (Load Areas)\n                if len(route._nodes) == 1:\n                    if solution._problem._is_aggregated[str(route._nodes[0])]:\n                        continue\n\n                # create tour by adding depot at start and end\n                tour = [graph._depot] + route._nodes + [graph._depot]\n                \n                for target_route in solution.routes():\n\n                    # exclude (origin+target) routes with single high-demand nodes (Load Areas)\n                    if len(target_route._nodes) == 1:\n                        if solution._problem._is_aggregated[str(target_route._nodes[0])]:\n                            continue\n\n                    target_tour = [graph._depot] + target_route._nodes + [graph._depot]\n                    \n                    if route == target_route:\n                        continue\n                    \n                    n = len(route._nodes)\n                    nt = len(target_route._nodes)+1\n\n                    for i in range(0,n):\n                        node = route._nodes[i]\n                        for j in range(0,nt):\n                            #target_node = target_route._nodes[j]\n                            \n                            if target_route.can_allocate([node]):\n                                length_diff = (-dm[dn[tour[i].name()]][dn[tour[i+1].name()]] -\n                                                dm[dn[tour[i+1].name()]][dn[tour[i+2].name()]] +\n                                                dm[dn[tour[i].name()]][dn[tour[i+2].name()]] +\n                                                dm[dn[target_tour[j].name()]][dn[tour[i+1].name()]] +\n                                                dm[dn[tour[i+1].name()]][dn[target_tour[j+1].name()]] -\n                                                dm[dn[target_tour[j].name()]][dn[target_tour[j+1].name()]])\n\n                                if length_diff < length_diff_best:\n                                    length_diff_best = length_diff\n                                    node_best, target_route_best, j_best = node, target_route, j\n                                        \n            if length_diff_best < 0:\n                # insert new node\n                target_route_best.insert([node_best], j_best)\n                # remove empty routes from solution\n                solution._routes = [route for route in solution._routes if route._nodes]\n\n                if anim is not None:\n                    solution.draw_network(anim)\n                \n                #print('Bessere Loesung gefunden:', node_best, target_node_best, target_route_best, length_diff_best)\n            \n            # no improvement found\n            if round(length_diff_best, op_diff_round_digits) == 0:\n                break\n\n            \n        return solution", "response": "Applies Relocate inter - route operator to solution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef operator_cross(self, graph, solution, op_diff_round_digits):\n        # TODO: check docstring\n        \"\"\"applies Cross inter-route operator to solution\n\n        Takes every node from every route and calculates savings when inserted\n        into all possible positions in other routes. Insertion is done at\n        position with max. saving and procedure starts over again with newly\n        created graph as input. Stops when no improvement is found.\n        \n        Args\n        ----\n        graph: :networkx:`NetworkX Graph Obj< >`\n            Descr\n        solution: BaseSolution\n            Descr\n        op_diff_round_digits: float\n            Precision (floating point digits) for rounding route length differences.\n            \n            *Details*: In some cases when an exchange is performed on two routes with one node each,\n            the difference between the both solutions (before and after the exchange) is not zero.\n            This is due to internal rounding errors of float type. So the loop won't break\n            (alternating between these two solutions), we need an additional criterion to avoid\n            this behaviour: A threshold to handle values very close to zero as if they were zero\n            (for a more detaisled description of the matter see http://floating-point-gui.de or\n            https://docs.python.org/3.5/tutorial/floatingpoint.html)\n        \n        Returns\n        -------\n        LocalSearchSolution\n           A solution (LocalSearchSolution class)\n\n        Todo\n        ----\n        * allow moves of a 2-node chain\n        * Remove ugly nested loops, convert to more efficient matrix operations\n        \"\"\"\n\n        # shorter var names for loop\n        dm = graph._matrix\n        dn = graph._nodes", "response": "This function applies Cross inter - route operator to solution."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsolving a given set of savings solution using local search.", "response": "def solve(self, graph, savings_solution, timeout, debug=False, anim=None):\n        \"\"\"Improve initial savings solution using local search\n\n        Parameters\n        ----------\n        graph: :networkx:`NetworkX Graph Obj< >`\n            Graph instance\n        savings_solution: SavingsSolution\n            initial solution of CVRP problem (instance of `SavingsSolution` class)\n        timeout: int\n            max processing time in seconds\n        debug: bool, defaults to False\n            If True, information is printed while routing\n        anim: AnimationDing0\n            AnimationDing0 object\n        \n        Returns\n        -------\n        LocalSearchSolution\n           A solution (LocalSearchSolution class)\n\n        \"\"\"\n        # TODO: If necessary, use timeout to set max processing time of local search\n\n        # load threshold for operator (see exchange or relocate operator's description for more information)\n        op_diff_round_digits = int(cfg_ding0.get('mv_routing', 'operator_diff_round_digits'))\n\n        solution = LocalSearchSolution(graph, savings_solution)\n\n        # FOR BENCHMARKING OF OPERATOR'S ORDER:\n        #self.benchmark_operator_order(graph, savings_solution, op_diff_round_digits)\n\n        for run in range(10):\n            start = time.time()\n            solution = self.operator_exchange(graph, solution, op_diff_round_digits, anim)\n            time1 = time.time()\n            if debug:\n                logger.debug('Elapsed time (exchange, run {1}): {0}, '\n                             'Solution\\'s length: {2}'.format(\n                    time1 - start, str(run), solution.length()))\n\n            solution = self.operator_relocate(graph, solution, op_diff_round_digits, anim)\n            time2 = time.time()\n            if debug:\n                logger.debug('Elapsed time (relocate, run {1}): {0}, '\n                             'Solution\\'s length: {2}'.format(\n                    time2 - time1, str(run), solution.length()))\n\n            solution = self.operator_oropt(graph, solution, op_diff_round_digits, anim)\n            time3 = time.time()\n            if debug:\n                logger.debug('Elapsed time (oropt, run {1}): {0}, '\n                             'Solution\\'s length: {2}'.format(\n                    time3 - time2, str(run), solution.length()))\n\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_branches(grid):\n\n    station = grid._station\n\n    tree = nx.dfs_tree(grid._graph, station)\n\n    # TODO: idea\n    # 1. build tree from lv_grid station as root -> diretions should point to\n    # descending leafs\n    # 2. for analysis of current issues get list of descendants with\n    # nx.descendants(tree, station). Sum peak load / gen capacity\n    # 3. Extract nodes belonging to main route of a branch by checking all\n    # successors if these are LVCalbleDistributors\n    # notes and hints:\n    # 1. associated edges can be accessed via grid._graph.in_edges(<node>)\n    # respectively grid._graph.out_edges(<node>)\n    # 2. when using nx.descendants(tree, station) make sure the order of nodes\n    # is maintained as this is important to properly assess voltage and over-\n    # loading issues\n\n    # first_cbl_dists = [x for x in grid._graph.neighbors(station)\n    #                    if isinstance(x, LVCableDistributorDing0)]\n\n\n    # if len(first_cbl_dists) > 0:\n    #     ancestors =  nx.ancestors(grid._graph, first_cbl_dists[0])\n    # else:\n    #     ancestors = None\n    # return ancestors\n    branch_heads = list(nx.neighbors(tree, station))\n\n    descendants = {branch_head: list(nx.descendants(tree, branch_head)) for\n                   branch_head in branch_heads}\n\n\n\n    return descendants", "response": "Get branches of sectoral loads"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_mv_grid_district(self, mv_grid_district):\n        # TODO: use setter method here (make attribute '_mv_grid_districts' private)\n        if mv_grid_district not in self.mv_grid_districts():\n            self._mv_grid_districts.append(mv_grid_district)", "response": "Adds a MV grid_district to _mv_grid_districts if not already existing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nletting DING0 run by shouting at this method (or just call it from NetworkDing0 instance). This method is a wrapper for the main functionality of DING0. Parameters ---------- session : sqlalchemy.orm.session.Session Database session mv_grid_districts_no : List of Integers List of MV grid_districts/stations to be imported (if empty, all grid_districts & stations are imported) debug : bool, defaults to False If True, information is printed during process export_figures : bool, defaults to False If True, figures are shown or exported (default path: ~/.ding0/) during run. Returns ------- msg : str Message of invalidity of a grid district Notes ----- The steps performed in this method are to be kept in the given order since there are hard dependencies between them. Short description of all steps performed: * STEP 1: Import MV Grid Districts and subjacent objects Imports MV Grid Districts, HV-MV stations, Load Areas, LV Grid Districts and MV-LV stations, instantiates and initiates objects. * STEP 2: Import generators Conventional and renewable generators of voltage levels 4..7 are imported and added to corresponding grid. * STEP 3: Parametrize grid Parameters of MV grid are set such as voltage level and cable/line types according to MV Grid District's characteristics. * STEP 4: Validate MV Grid Districts Tests MV grid districts for validity concerning imported data such as count of Load Areas. * STEP 5: Build LV grids Builds LV grids for every non-aggregated LA in every MV Grid District using model grids. * STEP 6: Build MV grids Builds MV grid by performing a routing on Load Area centres to build ring topology. * STEP 7: Connect MV and LV generators Generators are connected to grids, used approach depends on voltage level. * STEP 8: Set IDs for all branches in MV and LV grids While IDs of imported objects can be derived from dataset's ID, branches are created in steps 5+6 and need unique IDs (e.g. for PF calculation). * STEP 9: Relocate switch disconnectors in MV grid Switch disconnectors are set during routing process (step 6) according to the load distribution within a ring. After further modifications of the grid within step 6+7 they have to be relocated (note: switch disconnectors are called circuit breakers in DING0 for historical reasons). * STEP 10: Open all switch disconnectors in MV grid Under normal conditions, rings are operated in open state (half-rings). Furthermore, this is required to allow powerflow for MV grid. * STEP 11: Do power flow analysis of MV grid The technically working MV grid created in step 6 was extended by satellite loads and generators. It is finally tested again using powerflow calculation. * STEP 12: Reinforce MV grid MV grid is eventually reinforced persuant to results from step 11. STEP 13: Close all switch disconnectors in MV grid The rings are finally closed to hold a complete graph (if the SDs are open, the edges adjacent to a SD will not be exported!)", "response": "def run_ding0(self, session, mv_grid_districts_no=None, debug=False, export_figures=False):\n        \"\"\" Let DING0 run by shouting at this method (or just call\n            it from NetworkDing0 instance). This method is a wrapper\n            for the main functionality of DING0.\n\n        Parameters\n        ----------\n        session : sqlalchemy.orm.session.Session\n            Database session\n        mv_grid_districts_no : List of Integers\n            List of MV grid_districts/stations to be imported (if empty,\n            all grid_districts & stations are imported)\n        debug : bool, defaults to False\n            If True, information is printed during process\n        export_figures : bool, defaults to False\n            If True, figures are shown or exported (default path: ~/.ding0/) during run.\n\n        Returns\n        -------\n        msg : str\n            Message of invalidity of a grid district\n\n        Notes\n        -----\n        The steps performed in this method are to be kept in the given order\n        since there are hard dependencies between them. Short description of\n        all steps performed:\n        \n        * STEP 1: Import MV Grid Districts and subjacent objects\n        \n            Imports MV Grid Districts, HV-MV stations, Load Areas, LV Grid Districts\n            and MV-LV stations, instantiates and initiates objects.\n            \n        * STEP 2: Import generators\n        \n            Conventional and renewable generators of voltage levels 4..7 are imported\n            and added to corresponding grid.\n        \n        * STEP 3: Parametrize grid\n        \n            Parameters of MV grid are set such as voltage level and cable/line types\n            according to MV Grid District's characteristics.\n        \n        * STEP 4: Validate MV Grid Districts\n        \n            Tests MV grid districts for validity concerning imported data such as\n            count of Load Areas.\n        \n        * STEP 5: Build LV grids\n        \n            Builds LV grids for every non-aggregated LA in every MV Grid District\n            using model grids.\n        \n        * STEP 6: Build MV grids\n        \n            Builds MV grid by performing a routing on Load Area centres to build\n            ring topology.\n        \n        * STEP 7: Connect MV and LV generators\n        \n            Generators are connected to grids, used approach depends on voltage\n            level.\n        \n        * STEP 8: Set IDs for all branches in MV and LV grids\n        \n            While IDs of imported objects can be derived from dataset's ID, branches\n            are created in steps 5+6 and need unique IDs (e.g. for PF calculation).\n        \n        * STEP 9: Relocate switch disconnectors in MV grid\n        \n            Switch disconnectors are set during routing process (step 6) according\n            to the load distribution within a ring. After further modifications of\n            the grid within step 6+7 they have to be relocated (note: switch\n            disconnectors are called circuit breakers in DING0 for historical reasons).\n        \n        * STEP 10: Open all switch disconnectors in MV grid\n        \n            Under normal conditions, rings are operated in open state (half-rings).\n            Furthermore, this is required to allow powerflow for MV grid.\n        \n        * STEP 11: Do power flow analysis of MV grid\n        \n            The technically working MV grid created in step 6 was extended by satellite\n            loads and generators. It is finally tested again using powerflow calculation.\n        \n        * STEP 12: Reinforce MV grid\n        \n            MV grid is eventually reinforced persuant to results from step 11.\n\n        STEP 13: Close all switch disconnectors in MV grid\n            The rings are finally closed to hold a complete graph (if the SDs are open,\n            the edges adjacent to a SD will not be exported!)\n        \"\"\"\n        if debug:\n            start = time.time()\n\n        # STEP 1: Import MV Grid Districts and subjacent objects\n        self.import_mv_grid_districts(session,\n                                      mv_grid_districts_no=mv_grid_districts_no)\n\n        # STEP 2: Import generators\n        self.import_generators(session, debug=debug)\n\n        # STEP 3: Parametrize MV grid\n        self.mv_parametrize_grid(debug=debug)\n\n        # STEP 4: Validate MV Grid Districts\n        msg = self.validate_grid_districts()\n\n        # STEP 5: Build LV grids\n        self.build_lv_grids()\n\n        # STEP 6: Build MV grids\n        self.mv_routing(debug=False)\n        if export_figures:\n            grid = self._mv_grid_districts[0].mv_grid\n            plot_mv_topology(grid, subtitle='Routing completed', filename='1_routing_completed.png')\n\n        # STEP 7: Connect MV and LV generators\n        self.connect_generators(debug=False)\n        if export_figures:\n            plot_mv_topology(grid, subtitle='Generators connected', filename='2_generators_connected.png')\n\n        # STEP 8: Set IDs for all branches in MV and LV grids\n        self.set_branch_ids()\n\n        # STEP 9: Relocate switch disconnectors in MV grid\n        self.set_circuit_breakers(debug=debug)\n        if export_figures:\n            plot_mv_topology(grid, subtitle='Circuit breakers relocated', filename='3_circuit_breakers_relocated.png')\n    \n        # STEP 10: Open all switch disconnectors in MV grid\n        self.control_circuit_breakers(mode='open')\n    \n        # STEP 11: Do power flow analysis of MV grid\n        self.run_powerflow(session, method='onthefly', export_pypsa=False, debug=debug)\n        if export_figures:\n            plot_mv_topology(grid, subtitle='PF result (load case)',\n                             filename='4_PF_result_load.png',\n                             line_color='loading', node_color='voltage', testcase='load')\n            plot_mv_topology(grid, subtitle='PF result (feedin case)',\n                             filename='5_PF_result_feedin.png',\n                             line_color='loading', node_color='voltage', testcase='feedin')\n    \n        # STEP 12: Reinforce MV grid\n        self.reinforce_grid()\n\n        # STEP 13: Close all switch disconnectors in MV grid\n        self.control_circuit_breakers(mode='close')\n\n        if export_figures:\n            plot_mv_topology(grid, subtitle='Final grid PF result (load case)',\n                             filename='6_final_grid_PF_result_load.png',\n                             line_color='loading', node_color='voltage', testcase='load')\n            plot_mv_topology(grid, subtitle='Final grid PF result (feedin case)',\n                             filename='7_final_grid_PF_result_feedin.png',\n                             line_color='loading', node_color='voltage', testcase='feedin')\n\n        if debug:\n            logger.info('Elapsed time for {0} MV Grid Districts (seconds): {1}'.format(\n                str(len(mv_grid_districts_no)), time.time() - start))\n\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_mvgd_lvla_lvgd_obj_from_id(self):\n\n        mv_grid_districts_dict = {}\n        lv_load_areas_dict = {}\n        lv_grid_districts_dict = {}\n        lv_stations_dict = {}\n\n        for mv_grid_district in self.mv_grid_districts():\n            mv_grid_districts_dict[mv_grid_district.id_db] = mv_grid_district\n            for lv_load_area in mv_grid_district.lv_load_areas():\n                lv_load_areas_dict[lv_load_area.id_db] = lv_load_area\n                for lv_grid_district in lv_load_area.lv_grid_districts():\n                    lv_grid_districts_dict[lv_grid_district.id_db] = lv_grid_district\n                    lv_stations_dict[lv_grid_district.lv_grid.station().id_db] = lv_grid_district.lv_grid.station()\n\n        return mv_grid_districts_dict, lv_load_areas_dict, lv_grid_districts_dict, lv_stations_dict", "response": "Build dict with mapping from MVLoadAreaDing0 id to LVStationDing0 object and LVLoadAreaDing0 id to LVStationDing0 object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_mv_grid_district(self, poly_id, subst_id, grid_district_geo_data,\n                        station_geo_data):\n        \"\"\"Initiates single MV grid_district including station and grid\n\n        Parameters\n        ----------\n        poly_id: int\n            ID of grid_district according to database table. Also used as ID for created grid #TODO: check type\n        subst_id: int\n            ID of station according to database table #TODO: check type\n        grid_district_geo_data: :shapely:`Shapely Polygon object<polygons>`\n            Polygon of grid district\n        station_geo_data: :shapely:`Shapely Point object<points>`\n            Point of station\n            \n        Returns\n        -------\n        :shapely:`Shapely Polygon object<polygons>`\n            Description of return #TODO: check\n\n        \"\"\"\n\n        mv_station = MVStationDing0(id_db=subst_id, geo_data=station_geo_data)\n\n        mv_grid = MVGridDing0(network=self,\n                              id_db=poly_id,\n                              station=mv_station)\n        mv_grid_district = MVGridDistrictDing0(id_db=poly_id,\n                                               mv_grid=mv_grid,\n                                               geo_data=grid_district_geo_data)\n        mv_grid.grid_district = mv_grid_district\n        mv_station.grid = mv_grid\n\n        self.add_mv_grid_district(mv_grid_district)\n\n        return mv_grid_district", "response": "Builds MV grid district and station for a single MV grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds and associates lv_grid_districts and lv_stations from LV load area and LV stations.", "response": "def build_lv_grid_district(self,\n                               lv_load_area,\n                               lv_grid_districts,\n                               lv_stations):\n        \"\"\"Instantiates and associates lv_grid_district incl grid and station.\n        \n        The instantiation creates more or less empty objects including relevant\n        data for transformer choice and grid creation\n\n        Parameters\n        ----------\n        lv_load_area: :shapely:`Shapely Polygon object<polygons>`\n            load_area object\n        lv_grid_districts: :pandas:`pandas.DataFrame<dataframe>`\n            Table containing lv_grid_districts of according load_area \n        lv_stations : :pandas:`pandas.DataFrame<dataframe>`\n            Table containing lv_stations of according load_area\n        \"\"\"\n\n        # There's no LVGD for current LA\n        # -> TEMP WORKAROUND: Create single LVGD from LA, replace unknown valuess by zero\n        # TODO: Fix #155 (see also: data_processing #68)\n        if len(lv_grid_districts) == 0:\n            # raise ValueError(\n            #     'Load Area {} has no LVGD - please re-open #155'.format(\n            #         repr(lv_load_area)))\n            geom = wkt_dumps(lv_load_area.geo_area)\n\n            lv_grid_districts = \\\n                lv_grid_districts.append(\n                    pd.DataFrame(\n                        {'la_id': [lv_load_area.id_db],\n                         'geom': [geom],\n                         'population': [0],\n\n                         'peak_load_residential': [lv_load_area.peak_load_residential],\n                         'peak_load_retail': [lv_load_area.peak_load_retail],\n                         'peak_load_industrial': [lv_load_area.peak_load_industrial],\n                         'peak_load_agricultural': [lv_load_area.peak_load_agricultural],\n\n                         'sector_count_residential': [0],\n                         'sector_count_retail': [0],\n                         'sector_count_industrial': [0],\n                         'sector_count_agricultural': [0],\n\n                         'sector_consumption_residential': [0],\n                         'sector_consumption_retail': [0],\n                         'sector_consumption_industrial': [0],\n                         'sector_consumption_agricultural': [0]\n                         },\n                        index=[lv_load_area.id_db]\n                    )\n                )\n\n        lv_nominal_voltage = cfg_ding0.get('assumptions', 'lv_nominal_voltage')\n\n        # Associate lv_grid_district to load_area\n        for id, row in lv_grid_districts.iterrows():\n            lv_grid_district = LVGridDistrictDing0(\n                id_db=id,\n                lv_load_area=lv_load_area,\n                geo_data=wkt_loads(row['geom']),\n                population=0 if isnan(row['population']) else int(row['population']),\n                peak_load_residential=row['peak_load_residential'],\n                peak_load_retail=row['peak_load_retail'],\n                peak_load_industrial=row['peak_load_industrial'],\n                peak_load_agricultural=row['peak_load_agricultural'],\n                peak_load=(row['peak_load_residential'] +\n                               row['peak_load_retail'] +\n                               row['peak_load_industrial'] +\n                               row['peak_load_agricultural']),\n                sector_count_residential=int(row['sector_count_residential']),\n                sector_count_retail=int(row['sector_count_retail']),\n                sector_count_industrial=int(row['sector_count_industrial']),\n                sector_count_agricultural=int(row['sector_count_agricultural']),\n                sector_consumption_residential=row[\n                    'sector_consumption_residential'],\n                sector_consumption_retail=row['sector_consumption_retail'],\n                sector_consumption_industrial=row[\n                    'sector_consumption_industrial'],\n                sector_consumption_agricultural=row[\n                    'sector_consumption_agricultural'])\n\n            # be aware, lv_grid takes grid district's geom!\n            lv_grid = LVGridDing0(network=self,\n                                  grid_district=lv_grid_district,\n                                  id_db=id,\n                                  geo_data=wkt_loads(row['geom']),\n                                  v_level=lv_nominal_voltage)\n\n            # create LV station\n            lv_station = LVStationDing0(\n                id_db=id,\n                grid=lv_grid,\n                lv_load_area=lv_load_area,\n                geo_data=wkt_loads(lv_stations.loc[id, 'geom'])\n                            if id in lv_stations.index.values\n                            else lv_load_area.geo_centre,\n                peak_load=lv_grid_district.peak_load)\n\n            # assign created objects\n            # note: creation of LV grid is done separately,\n            # see NetworkDing0.build_lv_grids()\n            lv_grid.add_station(lv_station)\n            lv_grid_district.lv_grid = lv_grid\n            lv_load_area.add_lv_grid_district(lv_grid_district)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef import_mv_grid_districts(self, session, mv_grid_districts_no=None):\n\n        # check arguments\n        if not all(isinstance(_, int) for _ in mv_grid_districts_no):\n            raise TypeError('`mv_grid_districts` has to be a list of integers.')\n\n        # get srid settings from config\n        try:\n            srid = str(int(cfg_ding0.get('geo', 'srid')))\n        except OSError:\n            logger.exception('cannot open config file.')\n\n        # build SQL query\n        grid_districts = session.query(self.orm['orm_mv_grid_districts'].subst_id,\n                                       func.ST_AsText(func.ST_Transform(\n                                           self.orm['orm_mv_grid_districts'].geom, srid)). \\\n                                       label('poly_geom'),\n                                       func.ST_AsText(func.ST_Transform(\n                                           self.orm['orm_mv_stations'].point, srid)). \\\n                                       label('subs_geom')).\\\n            join(self.orm['orm_mv_stations'], self.orm['orm_mv_grid_districts'].subst_id ==\n                 self.orm['orm_mv_stations'].subst_id).\\\n            filter(self.orm['orm_mv_grid_districts'].subst_id.in_(mv_grid_districts_no)). \\\n            filter(self.orm['version_condition_mvgd']). \\\n            filter(self.orm['version_condition_mv_stations']). \\\n            distinct()\n\n        # read MV data from db\n        mv_data = pd.read_sql_query(grid_districts.statement,\n                                    session.bind,\n                                    index_col='subst_id')\n\n        # iterate over grid_district/station datasets and initiate objects\n        for poly_id, row in mv_data.iterrows():\n            subst_id = poly_id\n            region_geo_data = wkt_loads(row['poly_geom'])\n\n            # transform `region_geo_data` to epsg 3035\n            # to achieve correct area calculation of mv_grid_district\n            station_geo_data = wkt_loads(row['subs_geom'])\n            # projection = partial(\n            #     pyproj.transform,\n            #     pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            #     pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n            #\n            # region_geo_data = transform(projection, region_geo_data)\n\n            mv_grid_district = self.build_mv_grid_district(poly_id,\n                                             subst_id,\n                                             region_geo_data,\n                                             station_geo_data)\n\n            # import all lv_stations within mv_grid_district\n            lv_stations = self.import_lv_stations(session)\n\n            # import all lv_grid_districts within mv_grid_district\n            lv_grid_districts = self.import_lv_grid_districts(session, lv_stations)\n\n            # import load areas\n            self.import_lv_load_areas(session,\n                                      mv_grid_district,\n                                      lv_grid_districts,\n                                      lv_stations)\n\n            # add sum of peak loads of underlying lv grid_districts to mv_grid_district\n            mv_grid_district.add_peak_demand()\n\n        logger.info('=====> MV Grid Districts imported')", "response": "Imports MV Grid Districts HV - MV stations Load Areas LV Grid Districts and LV grid districts from database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport load areas from database for a single MV grid district and LV stations.", "response": "def import_lv_load_areas(self, session, mv_grid_district, lv_grid_districts,\n                             lv_stations):\n        \"\"\"Imports load_areas (load areas) from database for a single MV grid_district\n\n        Parameters\n        ----------\n        session : sqlalchemy.orm.session.Session\n            Database session\n        mv_grid_district : MV grid_district/station (instance of MVGridDistrictDing0 class) for\n            which the import of load areas is performed\n        lv_grid_districts: DataFrame\n            LV grid districts within this mv_grid_district\n        lv_stations: :pandas:`pandas.DataFrame<dataframe>`\n            LV stations within this mv_grid_district\n        \"\"\"\n\n        # get ding0s' standard CRS (SRID)\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n        # SET SRID 3035 to achieve correct area calculation of lv_grid_district\n        #srid = '3035'\n\n        # threshold: load area peak load, if peak load < threshold => disregard\n        # load area\n        lv_loads_threshold = cfg_ding0.get('mv_routing', 'load_area_threshold')\n\n        gw2kw = 10 ** 6  # load in database is in GW -> scale to kW\n\n        # build SQL query\n        lv_load_areas_sqla = session.query(\n            self.orm['orm_lv_load_areas'].id.label('id_db'),\n            self.orm['orm_lv_load_areas'].zensus_sum,\n            self.orm['orm_lv_load_areas'].zensus_count.label('zensus_cnt'),\n            self.orm['orm_lv_load_areas'].ioer_sum,\n            self.orm['orm_lv_load_areas'].ioer_count.label('ioer_cnt'),\n            self.orm['orm_lv_load_areas'].area_ha.label('area'),\n            self.orm['orm_lv_load_areas'].sector_area_residential,\n            self.orm['orm_lv_load_areas'].sector_area_retail,\n            self.orm['orm_lv_load_areas'].sector_area_industrial,\n            self.orm['orm_lv_load_areas'].sector_area_agricultural,\n            self.orm['orm_lv_load_areas'].sector_share_residential,\n            self.orm['orm_lv_load_areas'].sector_share_retail,\n            self.orm['orm_lv_load_areas'].sector_share_industrial,\n            self.orm['orm_lv_load_areas'].sector_share_agricultural,\n            self.orm['orm_lv_load_areas'].sector_count_residential,\n            self.orm['orm_lv_load_areas'].sector_count_retail,\n            self.orm['orm_lv_load_areas'].sector_count_industrial,\n            self.orm['orm_lv_load_areas'].sector_count_agricultural,\n            self.orm['orm_lv_load_areas'].nuts.label('nuts_code'),\n            func.ST_AsText(func.ST_Transform(self.orm['orm_lv_load_areas'].geom, srid)).\\\n                label('geo_area'),\n            func.ST_AsText(func.ST_Transform(self.orm['orm_lv_load_areas'].geom_centre, srid)).\\\n                label('geo_centre'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_residential * gw2kw).\\\n                label('peak_load_residential'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_retail * gw2kw).\\\n                label('peak_load_retail'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_industrial * gw2kw).\\\n                label('peak_load_industrial'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_agricultural * gw2kw).\\\n                label('peak_load_agricultural'),\n            ((self.orm['orm_lv_load_areas'].sector_peakload_residential\n              + self.orm['orm_lv_load_areas'].sector_peakload_retail\n              + self.orm['orm_lv_load_areas'].sector_peakload_industrial\n              + self.orm['orm_lv_load_areas'].sector_peakload_agricultural)\n             * gw2kw).label('peak_load')). \\\n            filter(self.orm['orm_lv_load_areas'].subst_id == mv_grid_district. \\\n                   mv_grid._station.id_db).\\\n            filter(((self.orm['orm_lv_load_areas'].sector_peakload_residential  # only pick load areas with peak load > lv_loads_threshold\n                     + self.orm['orm_lv_load_areas'].sector_peakload_retail\n                     + self.orm['orm_lv_load_areas'].sector_peakload_industrial\n                     + self.orm['orm_lv_load_areas'].sector_peakload_agricultural)\n                       * gw2kw) > lv_loads_threshold). \\\n            filter(self.orm['version_condition_la'])\n\n        # read data from db\n        lv_load_areas = pd.read_sql_query(lv_load_areas_sqla.statement,\n                                          session.bind,\n                                          index_col='id_db')\n\n        # create load_area objects from rows and add them to graph\n        for id_db, row in lv_load_areas.iterrows():\n\n            # create LV load_area object\n            lv_load_area = LVLoadAreaDing0(id_db=id_db,\n                                           db_data=row,\n                                           mv_grid_district=mv_grid_district,\n                                           peak_load=row['peak_load'])\n\n            # sub-selection of lv_grid_districts/lv_stations within one\n            # specific load area\n            lv_grid_districts_per_load_area = lv_grid_districts.\\\n                loc[lv_grid_districts['la_id'] == id_db]\n            lv_stations_per_load_area = lv_stations.\\\n                loc[lv_stations['la_id'] == id_db]\n\n            self.build_lv_grid_district(lv_load_area,\n                                        lv_grid_districts_per_load_area,\n                                        lv_stations_per_load_area)\n\n            # create new centre object for Load Area\n            lv_load_area_centre = LVLoadAreaCentreDing0(id_db=id_db,\n                                                        geo_data=wkt_loads(row['geo_centre']),\n                                                        lv_load_area=lv_load_area,\n                                                        grid=mv_grid_district.mv_grid)\n            # links the centre object to Load Area\n            lv_load_area.lv_load_area_centre = lv_load_area_centre\n\n            # add Load Area to MV grid district (and add centre object to MV gris district's graph)\n            mv_grid_district.add_lv_load_area(lv_load_area)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_lv_grid_districts(self, session, lv_stations):\n\n        # get ding0s' standard CRS (SRID)\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n        # SET SRID 3035 to achieve correct area calculation of lv_grid_district\n        # srid = '3035'\n\n        gw2kw = 10 ** 6  # load in database is in GW -> scale to kW\n\n        # 1. filter grid districts of relevant load area\n        lv_grid_districs_sqla = session.query(\n            self.orm['orm_lv_grid_district'].mvlv_subst_id,\n            self.orm['orm_lv_grid_district'].la_id,\n            self.orm['orm_lv_grid_district'].zensus_sum.label('population'),\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_residential * gw2kw).\n                label('peak_load_residential'),\n            (self.orm['orm_lv_grid_district'].sector_peakload_retail * gw2kw).\n                label('peak_load_retail'),\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_industrial * gw2kw).\n                label('peak_load_industrial'),\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_agricultural * gw2kw).\n                label('peak_load_agricultural'),\n            ((self.orm['orm_lv_grid_district'].sector_peakload_residential\n              + self.orm['orm_lv_grid_district'].sector_peakload_retail\n              + self.orm['orm_lv_grid_district'].sector_peakload_industrial\n              + self.orm['orm_lv_grid_district'].sector_peakload_agricultural)\n             * gw2kw).label('peak_load'),\n            func.ST_AsText(func.ST_Transform(\n                self.orm['orm_lv_grid_district'].geom, srid)).label('geom'),\n            self.orm['orm_lv_grid_district'].sector_count_residential,\n            self.orm['orm_lv_grid_district'].sector_count_retail,\n            self.orm['orm_lv_grid_district'].sector_count_industrial,\n            self.orm['orm_lv_grid_district'].sector_count_agricultural,\n            (self.orm[\n                 'orm_lv_grid_district'].sector_consumption_residential * gw2kw). \\\n                label('sector_consumption_residential'),\n            (self.orm['orm_lv_grid_district'].sector_consumption_retail * gw2kw). \\\n                label('sector_consumption_retail'),\n            (self.orm[\n                'orm_lv_grid_district'].sector_consumption_industrial * gw2kw). \\\n                label('sector_consumption_industrial'),\n            (self.orm[\n                'orm_lv_grid_district'].sector_consumption_agricultural * gw2kw). \\\n                label('sector_consumption_agricultural'),\n            self.orm['orm_lv_grid_district'].mvlv_subst_id). \\\n            filter(self.orm['orm_lv_grid_district'].mvlv_subst_id.in_(\n            lv_stations.index.tolist())). \\\n            filter(self.orm['version_condition_lvgd'])\n\n        # read data from db\n        lv_grid_districts = pd.read_sql_query(lv_grid_districs_sqla.statement,\n                                              session.bind,\n                                              index_col='mvlv_subst_id')\n\n        lv_grid_districts[\n            ['sector_count_residential',\n             'sector_count_retail',\n             'sector_count_industrial',\n             'sector_count_agricultural']] = lv_grid_districts[\n            ['sector_count_residential',\n             'sector_count_retail',\n             'sector_count_industrial',\n             'sector_count_agricultural']].fillna(0)\n\n        return lv_grid_districts", "response": "Imports all LV grid districts within given load area."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_lv_stations(self, session):\n\n        # get ding0s' standard CRS (SRID)\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        # get list of mv grid districts\n        mv_grid_districts = list(self.get_mvgd_lvla_lvgd_obj_from_id()[0])\n\n        lv_stations_sqla = session.query(self.orm['orm_lv_stations'].mvlv_subst_id,\n                                         self.orm['orm_lv_stations'].la_id,\n                                         func.ST_AsText(func.ST_Transform(\n                                           self.orm['orm_lv_stations'].geom, srid)). \\\n                                         label('geom')).\\\n            filter(self.orm['orm_lv_stations'].subst_id.in_(mv_grid_districts)). \\\n            filter(self.orm['version_condition_mvlvst'])\n\n        # read data from db\n        lv_grid_stations = pd.read_sql_query(lv_stations_sqla.statement,\n                                             session.bind,\n                                             index_col='mvlv_subst_id')\n        return lv_grid_stations", "response": "Import LV stations within the given load area."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting renewable and conventional generators and returns a new tuple of the new generators and conventional generators.", "response": "def import_generators(self, session, debug=False):\n        \"\"\"\n        Imports renewable (res) and conventional (conv) generators\n\n        Args:\n            session : sqlalchemy.orm.session.Session\n                Database session\n            debug: If True, information is printed during process\n        Notes:\n            Connection of generators is done later on in NetworkDing0's method connect_generators()\n        \"\"\"\n\n        def import_res_generators():\n            \"\"\"Imports renewable (res) generators\"\"\"\n\n            # build query\n            generators_sqla = session.query(\n                self.orm['orm_re_generators'].columns.id,\n                self.orm['orm_re_generators'].columns.subst_id,\n                self.orm['orm_re_generators'].columns.la_id,\n                self.orm['orm_re_generators'].columns.mvlv_subst_id,\n                self.orm['orm_re_generators'].columns.electrical_capacity,\n                self.orm['orm_re_generators'].columns.generation_type,\n                self.orm['orm_re_generators'].columns.generation_subtype,\n                self.orm['orm_re_generators'].columns.voltage_level,\n                self.orm['orm_re_generators'].columns.w_id,\n                func.ST_AsText(func.ST_Transform(\n                    self.orm['orm_re_generators'].columns.rea_geom_new, srid)).label('geom_new'),\n                func.ST_AsText(func.ST_Transform(\n                    self.orm['orm_re_generators'].columns.geom, srid)).label('geom')\n            ). \\\n                filter(\n                self.orm['orm_re_generators'].columns.subst_id.in_(list(mv_grid_districts_dict))). \\\n                filter(self.orm['orm_re_generators'].columns.voltage_level.in_([4, 5, 6, 7])). \\\n                filter(self.orm['version_condition_re'])\n\n            # read data from db\n            generators = pd.read_sql_query(generators_sqla.statement,\n                                           session.bind,\n                                           index_col='id')\n            # define generators with unknown subtype as 'unknown'\n            generators.loc[generators[\n                               'generation_subtype'].isnull(),\n                           'generation_subtype'] = 'unknown'\n\n            for id_db, row in generators.iterrows():\n\n                # treat generators' geom:\n                # use geom_new (relocated genos from data processing)\n                # otherwise use original geom from EnergyMap\n                if row['geom_new']:\n                    geo_data = wkt_loads(row['geom_new'])\n                elif not row['geom_new']:\n                    geo_data = wkt_loads(row['geom'])\n                    logger.warning(\n                        'Generator {} has no geom_new entry,'\n                        'EnergyMap\\'s geom entry will be used.'.format(\n                        id_db))\n                # if no geom is available at all, skip generator\n                elif not row['geom']:\n                    #geo_data =\n                    logger.error('Generator {} has no geom entry either'\n                                 'and will be skipped.'.format(id_db))\n                    continue\n\n                # look up MV grid\n                mv_grid_district_id = row['subst_id']\n                mv_grid = mv_grid_districts_dict[mv_grid_district_id].mv_grid\n\n                # create generator object\n                if row['generation_type'] in ['solar', 'wind']:\n                    generator = GeneratorFluctuatingDing0(\n                        id_db=id_db,\n                        mv_grid=mv_grid,\n                        capacity=row['electrical_capacity'],\n                        type=row['generation_type'],\n                        subtype=row['generation_subtype'],\n                        v_level=int(row['voltage_level']),\n                        weather_cell_id=row['w_id'])\n                else:\n                    generator = GeneratorDing0(\n                        id_db=id_db,\n                        mv_grid=mv_grid,\n                        capacity=row['electrical_capacity'],\n                        type=row['generation_type'],\n                        subtype=row['generation_subtype'],\n                        v_level=int(row['voltage_level']))\n\n                # MV generators\n                if generator.v_level in [4, 5]:\n                    generator.geo_data = geo_data\n                    mv_grid.add_generator(generator)\n\n                # LV generators\n                elif generator.v_level in [6, 7]:\n\n                    # look up MV-LV substation id\n                    mvlv_subst_id = row['mvlv_subst_id']\n\n                    # if there's a LVGD id\n                    if mvlv_subst_id and not isnan(mvlv_subst_id):\n                        # assume that given LA exists\n                        try:\n                            # get LVGD\n                            lv_station = lv_stations_dict[mvlv_subst_id]\n                            lv_grid_district = lv_station.grid.grid_district\n                            generator.lv_grid = lv_station.grid\n\n                            # set geom (use original from db)\n                            generator.geo_data = geo_data\n\n                        # if LA/LVGD does not exist, choose random LVGD and move generator to station of LVGD\n                        # this occurs due to exclusion of LA with peak load < 1kW\n                        except:\n                            lv_grid_district = random.choice(list(lv_grid_districts_dict.values()))\n\n                            generator.lv_grid = lv_grid_district.lv_grid\n                            generator.geo_data = lv_grid_district.lv_grid.station().geo_data\n\n                            logger.warning('Generator {} cannot be assigned to '\n                                           'non-existent LV Grid District and was '\n                                           'allocated to a random LV Grid District ({}).'.format(\n                                            repr(generator), repr(lv_grid_district)))\n                            pass\n\n                    else:\n                        lv_grid_district = random.choice(list(lv_grid_districts_dict.values()))\n\n                        generator.lv_grid = lv_grid_district.lv_grid\n                        generator.geo_data = lv_grid_district.lv_grid.station().geo_data\n\n                        logger.warning('Generator {} has no la_id and was '\n                                       'assigned to a random LV Grid District ({}).'.format(\n                                        repr(generator), repr(lv_grid_district)))\n\n                    generator.lv_load_area = lv_grid_district.lv_load_area\n                    lv_grid_district.lv_grid.add_generator(generator)\n\n        def import_conv_generators():\n            \"\"\"Imports conventional (conv) generators\"\"\"\n\n            # build query\n            generators_sqla = session.query(\n                self.orm['orm_conv_generators'].columns.id,\n                self.orm['orm_conv_generators'].columns.subst_id,\n                self.orm['orm_conv_generators'].columns.name,\n                self.orm['orm_conv_generators'].columns.capacity,\n                self.orm['orm_conv_generators'].columns.fuel,\n                self.orm['orm_conv_generators'].columns.voltage_level,\n                func.ST_AsText(func.ST_Transform(\n                    self.orm['orm_conv_generators'].columns.geom, srid)).label('geom')). \\\n                filter(\n                self.orm['orm_conv_generators'].columns.subst_id.in_(list(mv_grid_districts_dict))). \\\n                filter(self.orm['orm_conv_generators'].columns.voltage_level.in_([4, 5, 6])). \\\n                filter(self.orm['version_condition_conv'])\n\n            # read data from db\n            generators = pd.read_sql_query(generators_sqla.statement,\n                                           session.bind,\n                                           index_col='id')\n\n            for id_db, row in generators.iterrows():\n\n                # look up MV grid\n                mv_grid_district_id = row['subst_id']\n                mv_grid = mv_grid_districts_dict[mv_grid_district_id].mv_grid\n\n                # create generator object\n                generator = GeneratorDing0(id_db=id_db,\n                                           name=row['name'],\n                                           geo_data=wkt_loads(row['geom']),\n                                           mv_grid=mv_grid,\n                                           capacity=row['capacity'],\n                                           type=row['fuel'],\n                                           subtype='unknown',\n                                           v_level=int(row['voltage_level']))\n\n                # add generators to graph\n                if generator.v_level in [4, 5]:\n                    mv_grid.add_generator(generator)\n                # there's only one conv. geno with v_level=6 -> connect to MV grid\n                elif generator.v_level in [6]:\n                    generator.v_level = 5\n                    mv_grid.add_generator(generator)\n\n        # get ding0s' standard CRS (SRID)\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        # get predefined random seed and initialize random generator\n        seed = int(cfg_ding0.get('random', 'seed'))\n        random.seed(a=seed)\n\n        # build dicts to map MV grid district and Load Area ids to related objects\n        mv_grid_districts_dict,\\\n        lv_load_areas_dict,\\\n        lv_grid_districts_dict,\\\n        lv_stations_dict = self.get_mvgd_lvla_lvgd_obj_from_id()\n\n        # import renewable generators\n        import_res_generators()\n\n        # import conventional generators\n        import_conv_generators()\n\n        logger.info('=====> Generators imported')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads parameters from config files Returns ------- cfg_dict int-----------", "response": "def import_config(self):\n        \"\"\" Loads parameters from config files\n\n        Returns\n        -------\n        int\n            config object #TODO check type\n        \"\"\"\n\n        # load parameters from configs\n        cfg_ding0.load_config('config_db_tables.cfg')\n        cfg_ding0.load_config('config_calc.cfg')\n        cfg_ding0.load_config('config_files.cfg')\n        cfg_ding0.load_config('config_misc.cfg')\n        \n        cfg_dict = cfg_ding0.cfg._sections\n\n        return cfg_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new PFConfigDing0 object and imports config from file", "response": "def import_pf_config(self):\n        \"\"\" Creates power flow config class and imports config from file\n\n        Returns\n        -------\n        PFConfigDing0\n            PFConfigDing0 object\n        \"\"\"\n\n        scenario = cfg_ding0.get(\"powerflow\", \"test_grid_stability_scenario\")\n        start_hour = int(cfg_ding0.get(\"powerflow\", \"start_hour\"))\n        end_hour = int(cfg_ding0.get(\"powerflow\", \"end_hour\"))\n        start_time = datetime(1970, 1, 1, 00, 00, 0)\n\n        resolution = cfg_ding0.get(\"powerflow\", \"resolution\")\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        return PFConfigDing0(scenarios=[scenario],\n                             timestep_start=start_time,\n                             timesteps_count=end_hour-start_hour,\n                             srid=srid,\n                             resolution=resolution)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_static_data(self):\n\n        package_path = ding0.__path__[0]\n\n        static_data = {}\n\n        equipment_mv_parameters_trafos = cfg_ding0.get('equipment',\n                                                       'equipment_mv_parameters_trafos')\n        static_data['MV_trafos'] = pd.read_csv(os.path.join(package_path, 'data',\n                                   equipment_mv_parameters_trafos),\n                                   comment='#',\n                                   delimiter=',',\n                                   decimal='.',\n                                   converters={'S_nom': lambda x: int(x)})\n\n        # import equipment\n        equipment_mv_parameters_lines = cfg_ding0.get('equipment',\n                                                      'equipment_mv_parameters_lines')\n        static_data['MV_overhead_lines'] = pd.read_csv(os.path.join(package_path, 'data',\n                                           equipment_mv_parameters_lines),\n                                           comment='#',\n                                           converters={'I_max_th': lambda x: int(x),\n                                                       'U_n': lambda x: int(x),\n                                                       'reinforce_only': lambda x: int(x)})\n\n        equipment_mv_parameters_cables = cfg_ding0.get('equipment',\n                                                       'equipment_mv_parameters_cables')\n        static_data['MV_cables'] = pd.read_csv(os.path.join(package_path, 'data',\n                                   equipment_mv_parameters_cables),\n                                   comment='#',\n                                   converters={'I_max_th': lambda x: int(x),\n                                               'U_n': lambda x: int(x),\n                                               'reinforce_only': lambda x: int(x)})\n\n        equipment_lv_parameters_cables = cfg_ding0.get('equipment',\n                                                       'equipment_lv_parameters_cables')\n        static_data['LV_cables'] = pd.read_csv(os.path.join(package_path, 'data',\n                                   equipment_lv_parameters_cables),\n                                   comment='#',\n                                   index_col='name',\n                                   converters={'I_max_th': lambda x: int(x), 'U_n': lambda x: int(x)})\n\n        equipment_lv_parameters_trafos = cfg_ding0.get('equipment',\n                                                       'equipment_lv_parameters_trafos')\n        static_data['LV_trafos'] = pd.read_csv(os.path.join(package_path, 'data',\n                                   equipment_lv_parameters_trafos),\n                                   comment='#',\n                                   delimiter=',',\n                                   decimal='.',\n                                   converters={'S_nom': lambda x: int(x)})\n\n        # import LV model grids\n        model_grids_lv_string_properties = cfg_ding0.get('model_grids',\n                                                         'model_grids_lv_string_properties')\n        static_data['LV_model_grids_strings'] = pd.read_csv(os.path.join(package_path, 'data',\n                                                model_grids_lv_string_properties),\n                                                comment='#',\n                                                delimiter=';',\n                                                decimal=',',\n                                                index_col='string_id',\n                                                converters={'string_id': lambda x: int(x),\n                                                            'type': lambda x: int(x),\n                                                            'Kerber Original': lambda x: int(x),\n                                                            'count house branch': lambda x: int(x),\n                                                            'distance house branch': lambda x: int(x),\n                                                            'cable width': lambda x: int(x),\n                                                            'string length': lambda x: int(x),\n                                                            'length house branch A': lambda x: int(x),\n                                                            'length house branch B': lambda x: int(x),\n                                                            'cable width A': lambda x: int(x),\n                                                            'cable width B': lambda x: int(x)})\n\n        model_grids_lv_apartment_string = cfg_ding0.get('model_grids',\n                                                        'model_grids_lv_apartment_string')\n        converters_ids = {}\n        for id in range(1,47):  # create int() converter for columns 1..46\n            converters_ids[str(id)] = lambda x: int(x)\n        static_data['LV_model_grids_strings_per_grid'] = pd.read_csv(os.path.join(package_path, 'data',\n                                                         model_grids_lv_apartment_string),\n                                                         comment='#',\n                                                         delimiter=';',\n                                                         decimal=',',\n                                                         index_col='apartment_count',\n                                                         converters=dict({'apartment_count': lambda x: int(x)},\n                                                                         **converters_ids))\n\n        return static_data", "response": "Imports static data into NetworkDing0 such as equipment."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports the ORM classes for oedb access depending on input in config", "response": "def import_orm(self):\n        #TODO: check docstring\n        \"\"\" Import ORM classes for oedb access depending on input in config in\n        self.config which is loaded from 'config_db_tables.cfg'\n        \n        Returns\n        -------\n        int\n            Descr #TODO check type\n        \"\"\"\n\n        orm = {}\n\n        data_source = self.config['input_data_source']['input_data']\n        mv_grid_districts_name = self.config[data_source]['mv_grid_districts']\n        mv_stations_name = self.config[data_source]['mv_stations']\n        lv_load_areas_name = self.config[data_source]['lv_load_areas']\n        lv_grid_district_name = self.config[data_source]['lv_grid_district']\n        lv_stations_name = self.config[data_source]['lv_stations']\n        conv_generators_name = self.config[data_source]['conv_generators']\n        re_generators_name = self.config[data_source]['re_generators']\n\n        from egoio.db_tables import model_draft as orm_model_draft, \\\n            supply as orm_supply, \\\n            demand as orm_demand, \\\n            grid as orm_grid\n\n        if data_source == 'model_draft':\n            orm['orm_mv_grid_districts'] = orm_model_draft.__getattribute__(mv_grid_districts_name)\n            orm['orm_mv_stations'] = orm_model_draft.__getattribute__(mv_stations_name)\n            orm['orm_lv_load_areas'] = orm_model_draft.__getattribute__(lv_load_areas_name)\n            orm['orm_lv_grid_district'] = orm_model_draft.__getattribute__(lv_grid_district_name)\n            orm['orm_lv_stations'] = orm_model_draft.__getattribute__(lv_stations_name)\n            orm['orm_conv_generators'] = orm_model_draft.__getattribute__(conv_generators_name)\n            orm['orm_re_generators'] = orm_model_draft.__getattribute__(re_generators_name)\n            orm['version_condition_mvgd'] = 1 == 1\n            orm['version_condition_mv_stations'] = 1 == 1\n            orm['version_condition_la'] = 1 == 1\n            orm['version_condition_lvgd'] = 1 == 1\n            orm['version_condition_mvlvst'] = 1 == 1\n            orm['version_condition_re'] = 1 == 1\n            orm['version_condition_conv'] = 1 == 1\n        elif data_source == 'versioned':\n            orm['orm_mv_grid_districts'] = orm_grid.__getattribute__(mv_grid_districts_name)\n            orm['orm_mv_stations'] = orm_grid.__getattribute__(mv_stations_name)\n            orm['orm_lv_load_areas'] = orm_demand.__getattribute__(lv_load_areas_name)\n            orm['orm_lv_grid_district'] = orm_grid.__getattribute__(lv_grid_district_name)\n            orm['orm_lv_stations'] = orm_grid.__getattribute__(lv_stations_name)\n            orm['orm_conv_generators'] = orm_supply.__getattribute__(conv_generators_name)\n            orm['orm_re_generators'] = orm_supply.__getattribute__(re_generators_name)\n            orm['data_version'] = self.config[data_source]['version']\n            orm['version_condition_mvgd'] =\\\n                orm['orm_mv_grid_districts'].version == orm['data_version']\n            orm['version_condition_mv_stations'] = \\\n                orm['orm_mv_stations'].version == orm['data_version']\n            orm['version_condition_la'] =\\\n                orm['orm_lv_load_areas'].version == orm['data_version']\n            orm['version_condition_lvgd'] =\\\n                orm['orm_lv_grid_district'].version == orm['data_version']\n            orm['version_condition_mvlvst'] =\\\n                orm['orm_lv_stations'].version == orm['data_version']\n            orm['version_condition_re'] =\\\n                orm['orm_re_generators'].columns.version == orm['data_version']\n            orm['version_condition_conv'] =\\\n                orm['orm_conv_generators'].columns.version == orm['data_version']\n        else:\n            logger.error(\"Invalid data source {} provided. Please re-check the file \"\n                         \"`config_db_tables.cfg`\".format(data_source))\n            raise NameError(\"{} is no valid data source!\".format(data_source))\n\n        return orm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntesting MV grid districts for validity concerning imported data.", "response": "def validate_grid_districts(self):\n        #TODO: check docstring\n        \"\"\" Tests MV grid districts for validity concerning imported data such as:\n            \n            i) Uno            \n            ii) Dos\n        \n        Invalid MV grid districts are subsequently deleted from Network.\n        \"\"\"\n\n        msg_invalidity = []\n        invalid_mv_grid_districts = []\n\n        for grid_district in self.mv_grid_districts():\n\n            # there's only one node (MV station) => grid is empty\n            if len(grid_district.mv_grid._graph.nodes()) == 1:\n                invalid_mv_grid_districts.append(grid_district)\n                msg_invalidity.append('MV Grid District {} seems to be empty ' \\\n                                      'and ' \\\n                                      'was removed'.format(grid_district))\n            # there're only aggregated load areas\n            elif all([lvla.is_aggregated for lvla in\n                      grid_district.lv_load_areas()]):\n                invalid_mv_grid_districts.append(grid_district)\n                msg_invalidity.append(\"MV Grid District {} contains only \" \\\n                                 \"aggregated Load Areas and was removed\" \\\n                                 \"\".format(grid_district))\n\n        for grid_district in invalid_mv_grid_districts:\n            self._mv_grid_districts.remove(grid_district)\n\n        logger.warning(\"\\n\".join(msg_invalidity))\n        logger.info('=====> MV Grids validated')\n        return msg_invalidity"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport MV grids to database for visualization purposes.", "response": "def export_mv_grid(self, session, mv_grid_districts):\n        \"\"\" Exports MV grids to database for visualization purposes\n\n        Parameters\n        ----------\n        session : sqlalchemy.orm.session.Session\n            Database session\n        mv_grid_districts : List of MV grid_districts (instances of MVGridDistrictDing0 class)\n            whose MV grids are exported.\n\n        Returns\n        -------\n        int\n            Description #TODO\n        \"\"\"\n\n        # check arguments\n        if not all(isinstance(_, int) for _ in mv_grid_districts):\n            raise TypeError('`mv_grid_districts` has to be a list of integers.')\n\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        # delete all existing datasets\n        # db_int.sqla_mv_grid_viz.__table__.create(conn) # create if not exist\n        # change_owner_to(conn,\n        #                 db_int.sqla_mv_grid_viz.__table_args__['schema'],\n        #                 db_int.sqla_mv_grid_viz.__tablename__,\n        #                 'oeuser')\n        session.query(db_int.sqla_mv_grid_viz).delete()\n        session.commit()\n\n        # build data array from MV grids (nodes and branches)\n        for grid_district in self.mv_grid_districts():\n\n            grid_id = grid_district.mv_grid.id_db\n\n            # init arrays for nodes\n            mv_stations = []\n            mv_cable_distributors = []\n            mv_circuit_breakers = []\n            lv_load_area_centres = []\n            lv_stations = []\n            mv_generators = []\n            lines = []\n\n            # get nodes from grid's graph and append to corresponding array\n            for node in grid_district.mv_grid._graph.nodes():\n                if isinstance(node, LVLoadAreaCentreDing0):\n                    lv_load_area_centres.append((node.geo_data.x, node.geo_data.y))\n                elif isinstance(node, MVCableDistributorDing0):\n                    mv_cable_distributors.append((node.geo_data.x, node.geo_data.y))\n                elif isinstance(node, MVStationDing0):\n                    mv_stations.append((node.geo_data.x, node.geo_data.y))\n                elif isinstance(node, CircuitBreakerDing0):\n                    mv_circuit_breakers.append((node.geo_data.x, node.geo_data.y))\n                elif isinstance(node, GeneratorDing0):\n                    mv_generators.append((node.geo_data.x, node.geo_data.y))\n\n            # create shapely obj from stations and convert to\n            # geoalchemy2.types.WKBElement\n            # set to None if no objects found (otherwise SQLAlchemy will throw an error).\n            if lv_load_area_centres:\n                lv_load_area_centres_wkb = from_shape(MultiPoint(lv_load_area_centres), srid=srid)\n            else:\n                lv_load_area_centres_wkb = None\n\n            if mv_cable_distributors:\n                mv_cable_distributors_wkb = from_shape(MultiPoint(mv_cable_distributors), srid=srid)\n            else:\n                mv_cable_distributors_wkb = None\n\n            if mv_circuit_breakers:\n                mv_circuit_breakers_wkb = from_shape(MultiPoint(mv_circuit_breakers), srid=srid)\n            else:\n                mv_circuit_breakers_wkb = None\n\n            if mv_stations:\n                mv_stations_wkb = from_shape(Point(mv_stations), srid=srid)\n            else:\n                mv_stations_wkb = None\n\n            if mv_generators:\n                mv_generators_wkb = from_shape(MultiPoint(mv_generators), srid=srid)\n            else:\n                mv_generators_wkb = None\n\n            # get edges (lines) from grid's graph and append to corresponding array\n            for branch in grid_district.mv_grid.graph_edges():\n                line = branch['adj_nodes']\n                lines.append(((line[0].geo_data.x,\n                               line[0].geo_data.y),\n                              (line[1].geo_data.x,\n                               line[1].geo_data.y)))\n\n            # create shapely obj from lines and convert to\n            # geoalchemy2.types.WKBElement\n            mv_lines_wkb = from_shape(MultiLineString(lines), srid=srid)\n\n            # get nodes from lv grid districts and append to corresponding array\n            for lv_load_area in grid_district.lv_load_areas():\n                for lv_grid_district in lv_load_area.lv_grid_districts():\n                    station = lv_grid_district.lv_grid.station()\n                    if station not in grid_district.mv_grid.graph_isolated_nodes():\n                        lv_stations.append((station.geo_data.x, station.geo_data.y))\n            lv_stations_wkb = from_shape(MultiPoint(lv_stations), srid=srid)\n\n            # add dataset to session\n            dataset = db_int.sqla_mv_grid_viz(\n                grid_id=grid_id,\n                geom_mv_station=mv_stations_wkb,\n                geom_mv_cable_dists=mv_cable_distributors_wkb,\n                geom_mv_circuit_breakers=mv_circuit_breakers_wkb,\n                geom_lv_load_area_centres=lv_load_area_centres_wkb,\n                geom_lv_stations=lv_stations_wkb,\n                geom_mv_generators=mv_generators_wkb,\n                geom_mv_lines=mv_lines_wkb)\n            session.add(dataset)\n\n        # commit changes to db\n        session.commit()\n\n        # logger.info('=====> MV Grids exported')\n        logger.info('MV Grids exported')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef export_mv_grid_new(self, session, mv_grid_districts):\n\n        # check arguments\n        if not all(isinstance(_, int) for _ in mv_grid_districts):\n            raise TypeError('`mv_grid_districts` has to be a list of integers.')\n\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        # delete all existing datasets\n        # db_int.sqla_mv_grid_viz_branches.__table__.create(conn) # create if not exist\n        # change_owner_to(conn,\n        #                 db_int.sqla_mv_grid_viz_branches.__table_args__['schema'],\n        #                 db_int.sqla_mv_grid_viz_branches.__tablename__,\n        #                 'oeuser')\n        # db_int.sqla_mv_grid_viz_nodes.__table__.create(conn) # create if not exist\n        # change_owner_to(conn,\n        #                 db_int.sqla_mv_grid_viz_nodes.__table_args__['schema'],\n        #                 db_int.sqla_mv_grid_viz_nodes.__tablename__,\n        #                 'oeuser')\n        session.query(db_int.sqla_mv_grid_viz_branches).delete()\n        session.query(db_int.sqla_mv_grid_viz_nodes).delete()\n        session.commit()\n\n        # build data array from MV grids (nodes and branches)\n        for grid_district in self.mv_grid_districts():\n\n            # get nodes from grid's graph and create datasets\n            for node in grid_district.mv_grid._graph.nodes():\n                if hasattr(node, 'voltage_res'):\n                    node_name = '_'.join(['MV',\n                                          str(grid_district.mv_grid.id_db),\n                                          repr(node)])\n\n                    node_dataset = db_int.sqla_mv_grid_viz_nodes(\n                        node_id=node_name,\n                        grid_id=grid_district.mv_grid.id_db,\n                        v_nom=grid_district.mv_grid.v_level,\n                        geom=from_shape(Point(node.geo_data), srid=srid),\n                        v_res0=node.voltage_res[0],\n                        v_res1=node.voltage_res[1]\n                    )\n                    session.add(node_dataset)\n                # LA centres of agg. LA\n                elif isinstance(node, LVLoadAreaCentreDing0):\n                    if node.lv_load_area.is_aggregated:\n                        node_name = '_'.join(['MV',\n                                              str(grid_district.mv_grid.id_db),\n                                              repr(node)])\n\n                        node_dataset = db_int.sqla_mv_grid_viz_nodes(\n                            node_id=node_name,\n                            grid_id=grid_district.mv_grid.id_db,\n                            v_nom=grid_district.mv_grid.v_level,\n                            geom=from_shape(Point(node.geo_data), srid=srid),\n                            v_res0=0,\n                            v_res1=0\n                        )\n                        session.add(node_dataset)\n\n            # get branches (lines) from grid's graph and create datasets\n            for branch in grid_district.mv_grid.graph_edges():\n                if hasattr(branch['branch'], 's_res'):\n                    branch_name = '_'.join(['MV',\n                                            str(grid_district.mv_grid.id_db),\n                                            'lin',\n                                            str(branch['branch'].id_db)])\n\n                    branch_dataset = db_int.sqla_mv_grid_viz_branches(\n                        branch_id=branch_name,\n                        grid_id=grid_district.mv_grid.id_db,\n                        type_name=branch['branch'].type['name'],\n                        type_kind=branch['branch'].kind,\n                        type_v_nom=branch['branch'].type['U_n'],\n                        type_s_nom=3**0.5 * branch['branch'].type['I_max_th'] * branch['branch'].type['U_n'],\n                        length=branch['branch'].length / 1e3,\n                        geom=from_shape(LineString([branch['adj_nodes'][0].geo_data,\n                                                    branch['adj_nodes'][1].geo_data]),\n                                        srid=srid),\n                        s_res0=branch['branch'].s_res[0],\n                        s_res1=branch['branch'].s_res[1]\n                    )\n                    session.add(branch_dataset)\n                else:\n                    branch_name = '_'.join(['MV',\n                                            str(grid_district.mv_grid.id_db),\n                                            'lin',\n                                            str(branch['branch'].id_db)])\n\n                    branch_dataset = db_int.sqla_mv_grid_viz_branches(\n                        branch_id=branch_name,\n                        grid_id=grid_district.mv_grid.id_db,\n                        type_name=branch['branch'].type['name'],\n                        type_kind=branch['branch'].kind,\n                        type_v_nom=branch['branch'].type['U_n'],\n                        type_s_nom=3**0.5 * branch['branch'].type['I_max_th'] * branch['branch'].type['U_n'],\n                        length=branch['branch'].length / 1e3,\n                        geom=from_shape(LineString([branch['adj_nodes'][0].geo_data,\n                                                    branch['adj_nodes'][1].geo_data]),\n                                        srid=srid),\n                        s_res0=0,\n                        s_res1=0\n                    )\n                    session.add(branch_dataset)\n\n\n        # commit changes to db\n        session.commit()\n\n        logger.info('=====> MV Grids exported (NEW)')", "response": "Exports MV grids to database for visualization purposes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport grid data to dataframe for statistical analysis.", "response": "def to_dataframe(self):\n        \"\"\"Export grid data to dataframes for statistical analysis.\n\n        The export to dataframe is similar to db tables exported by `export_mv_grid_new`.\n\n        Returns\n        -------\n        :pandas:`pandas.DataFrame<dataframe>`\n            Pandas Data Frame\n            \n        See Also\n        --------\n        ding0.core.NetworkDing0.export_mv_grid_new :\n        \n        \"\"\"\n\n        node_cols = ['node_id', 'grid_id', 'v_nom', 'geom', 'v_res0', 'v_res1',\n                     'peak_load', 'generation_capacity', 'type']\n        edges_cols = ['branch_id', 'grid_id', 'type_name', 'type_kind',\n                      'type_v_nom', 'type_s_nom', 'length', 'geom', 's_res0',\n                      's_res1']\n\n        nodes_df = pd.DataFrame(columns=node_cols)\n        edges_df = pd.DataFrame(columns=edges_cols)\n\n        srid = str(int(self.config['geo']['srid']))\n\n        for grid_district in self.mv_grid_districts():\n\n            # get nodes from grid's graph and create datasets\n            for node in grid_district.mv_grid.graph_nodes_sorted():\n                node_name = '_'.join(['MV',\n                                      str(grid_district.mv_grid.id_db),\n                                      repr(node)])\n                geom = from_shape(Point(node.geo_data), srid=srid)\n                if isinstance(node, LVStationDing0):\n                    peak_load = node.peak_load\n                    generation_capacity = node.peak_generation\n                    if hasattr(node, 'voltage_res'):\n                        type = 'LV Station'\n                    else:\n                        type = 'LV station (aggregated)'\n                elif isinstance(node, GeneratorDing0):\n                    peak_load = 0\n                    generation_capacity = node.capacity\n                    type = node.type\n                elif isinstance(node, MVCableDistributorDing0):\n                    peak_load = 0\n                    generation_capacity = 0\n                    type = 'Cable distributor'\n                elif isinstance(node, LVLoadAreaCentreDing0):\n                    peak_load = 0\n                    generation_capacity = 0\n                    type = 'Load area center of aggregated load area'\n                elif isinstance(node, CircuitBreakerDing0):\n                    peak_load = 0\n                    generation_capacity = 0\n                    type = 'Switch Disconnector'\n                    #round coordinates of circuit breaker\n                    geosjson = mapping(node.geo_data)\n                    decimal_places = 4 #with 4, its around 10m error N-S or E-W\n                    geosjson['coordinates'] = np.round(np.array(geosjson['coordinates']), decimal_places)\n                    geom = from_shape(Point(shape(geosjson)), srid=srid)\n                else:\n                    peak_load = 0\n                    generation_capacity = 0\n                    type = 'Unknown'\n\n                # add res voltages from nodes which were part of PF only\n                if hasattr(node, 'voltage_res'):\n                    v_res0 = node.voltage_res[0]\n                    v_res1 = node.voltage_res[1]\n                else:\n                    v_res0 = v_res1 = 0\n\n                nodes_df = nodes_df.append(pd.Series(\n                    {'node_id': node_name,\n                     'grid_id': grid_district.mv_grid.id_db,\n                     'v_nom': grid_district.mv_grid.v_level,\n                     'geom': geom,\n                     'peak_load': peak_load,\n                     'generation_capacity': generation_capacity,\n                     'v_res0': v_res0,\n                     'v_res1': v_res1,\n                     'type': type,\n                     'rings': len(grid_district.mv_grid._rings)\n                    }), ignore_index=True)\n\n            # get branches (lines) from grid's graph and create datasets\n            for branch in grid_district.mv_grid.graph_edges():\n                if hasattr(branch['branch'], 's_res'):\n                    branch_name = '_'.join(['MV',\n                                            str(\n                                                grid_district.mv_grid.id_db),\n                                            'lin',\n                                            str(branch[\n                                                    'branch'].id_db)])\n\n                    edges_df = edges_df.append(pd.Series(\n                        {'branch_id': branch_name,\n                        'grid_id': grid_district.mv_grid.id_db,\n                        'type_name': branch['branch'].type['name'],\n                        'type_kind': branch['branch'].kind,\n                        'type_v_nom': branch['branch'].type['U_n'],\n                        'type_s_nom': 3 ** 0.5 * branch['branch'].type[\n                            'I_max_th'] * branch['branch'].type['U_n'],\n                        'length': branch['branch'].length / 1e3,\n                        'geom': from_shape(\n                            LineString([branch['adj_nodes'][0].geo_data,\n                                        branch['adj_nodes'][\n                                            1].geo_data]),\n                            srid=srid),\n                        's_res0': branch['branch'].s_res[0],\n                        's_res1': branch['branch'].s_res[1]}), ignore_index=True)\n\n        return nodes_df, edges_df"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms routing on all MV grids in MV grids.", "response": "def mv_routing(self, debug=False, animation=False):\n        \"\"\" Performs routing on all MV grids.\n\n        Parameters\n        ----------\n        debug: bool, default to False\n            If True, information is printed while routing\n        animation: bool, default to False\n            If True, images of route modification steps are exported during routing process. A new animation object is created.\n                \n        See Also\n        --------\n        ding0.core.network.grids.MVGridDing0.routing : for details on MVGridDing0 objects routing\n        ding0.tools.animation.AnimationDing0 : for details on animation function.\n        \"\"\"\n\n        if animation:\n            anim = AnimationDing0()\n        else:\n            anim = None\n\n        for grid_district in self.mv_grid_districts():\n            grid_district.mv_grid.routing(debug=debug, anim=anim)\n\n        logger.info('=====> MV Routing (Routing, Connection of Satellites & '\n                    'Stations) performed')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild LV grids for every non - aggregated LA in every MV grid district using model grids.", "response": "def build_lv_grids(self):\n        \"\"\" Builds LV grids for every non-aggregated LA in every MV grid\n        district using model grids.\n        \"\"\"\n\n        for mv_grid_district in self.mv_grid_districts():\n            for load_area in mv_grid_district.lv_load_areas():\n                if not load_area.is_aggregated:\n                    for lv_grid_district in load_area.lv_grid_districts():\n\n                        lv_grid_district.lv_grid.build_grid()\n                else:\n                    logger.info(\n                        '{} is of type aggregated. No grid is created.'.format(repr(load_area)))\n\n        logger.info('=====> LV model grids created')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connect_generators(self, debug=False):\n\n        for mv_grid_district in self.mv_grid_districts():\n            mv_grid_district.mv_grid.connect_generators(debug=debug)\n\n            # get predefined random seed and initialize random generator\n            seed = int(cfg_ding0.get('random', 'seed'))\n            random.seed(a=seed)\n\n            for load_area in mv_grid_district.lv_load_areas():\n                if not load_area.is_aggregated:\n                    for lv_grid_district in load_area.lv_grid_districts():\n\n                        lv_grid_district.lv_grid.connect_generators(debug=debug)\n                        if debug:\n                            lv_grid_district.lv_grid.graph_draw(mode='LV')\n                else:\n                    logger.info(\n                        '{} is of type aggregated. LV generators are not connected to LV grids.'.format(repr(load_area)))\n\n        logger.info('=====> Generators connected')", "response": "Connect generators to grids for every MV and LV grid district."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform Parametrization of all MV grids in the MV grid districts.", "response": "def mv_parametrize_grid(self, debug=False):\n        \"\"\" Performs Parametrization of grid equipment of all MV grids.\n         \n        Parameters\n        ----------\n        debug: bool, defaults to False\n            If True, information is printed during process.\n        \n        See Also\n        --------\n        ding0.core.network.grids.MVGridDing0.parametrize_grid\n        \"\"\"\n\n        for grid_district in self.mv_grid_districts():\n            grid_district.mv_grid.parametrize_grid(debug=debug)\n\n        logger.info('=====> MV Grids parametrized')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting branch ids for all MV and LV grids in this MV grid.", "response": "def set_branch_ids(self):\n        \"\"\" Performs generation and setting of ids of branches for all MV and underlying LV grids.            \n            \n        See Also\n        --------\n        ding0.core.network.grids.MVGridDing0.set_branch_ids\n        \"\"\"\n\n        for grid_district in self.mv_grid_districts():\n            grid_district.mv_grid.set_branch_ids()\n\n        logger.info('=====> Branch IDs set')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_circuit_breakers(self, debug=False):\n\n        for grid_district in self.mv_grid_districts():\n            grid_district.mv_grid.set_circuit_breakers(debug=debug)\n\n        logger.info('=====> MV Circuit Breakers relocated')", "response": "Calculates the optimal position of the existing circuit breakers and relocates them within the MV grids."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nopening or closes all circuit breakers of all MV grids.", "response": "def control_circuit_breakers(self, mode=None):\n        \"\"\" Opens or closes all circuit breakers of all MV grids.\n\n        Args\n        ----\n        mode: str\n            Set mode='open' to open, mode='close' to close\n        \"\"\"\n\n        for grid_district in self.mv_grid_districts():\n            if mode == 'open':\n                grid_district.mv_grid.open_circuit_breakers()\n            elif mode == 'close':\n                grid_district.mv_grid.close_circuit_breakers()\n            else:\n                raise ValueError('\\'mode\\' is invalid.')\n\n        if mode == 'open':\n            logger.info('=====> MV Circuit Breakers opened')\n        elif mode == 'close':\n            logger.info('=====> MV Circuit Breakers closed')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_powerflow(self, session, method='onthefly', export_pypsa=False, debug=False):\n\n        if method == 'db':\n            # Empty tables\n            pypsa_io.delete_powerflow_tables(session)\n\n            for grid_district in self.mv_grid_districts():\n                if export_pypsa:\n                    export_pypsa_dir = repr(grid_district.mv_grid)\n                else:\n                    export_pypsa_dir = None\n                grid_district.mv_grid.run_powerflow(session, method='db',\n                                                    export_pypsa_dir=export_pypsa_dir,\n                                                    debug=debug)\n\n        elif method == 'onthefly':\n            for grid_district in self.mv_grid_districts():\n                if export_pypsa:\n                    export_pypsa_dir = repr(grid_district.mv_grid)\n                else:\n                    export_pypsa_dir = None\n                grid_district.mv_grid.run_powerflow(session,\n                                                    method='onthefly',\n                                                    export_pypsa_dir=export_pypsa_dir,\n                                                    debug=debug)", "response": "Runs power flow calculation for all MV grids in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms grid reinforcement measures for all MV and LV grids in this instance.", "response": "def reinforce_grid(self):\n        \"\"\" Performs grid reinforcement measures for all MV and LV grids\n        Args:\n\n        Returns:\n\n        \"\"\"\n        # TODO: Finish method and enable LV case\n\n        for grid_district in self.mv_grid_districts():\n\n            # reinforce MV grid\n            grid_district.mv_grid.reinforce_grid()\n\n            # reinforce LV grids\n            for lv_load_area in grid_district.lv_load_areas():\n                if not lv_load_area.is_aggregated:\n                    for lv_grid_district in lv_load_area.lv_grid_districts():\n                        lv_grid_district.lv_grid.reinforce_grid()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef metadata(self, run_id=None):\n\n        # Get latest version and/or git commit hash\n        try:\n            version = subprocess.check_output(\n                [\"git\", \"describe\", \"--tags\", \"--always\"]).decode('utf8')\n        except:\n            version = None\n\n        # Collect names of database table used to run Ding0 and data version\n        if self.config['input_data_source']['input_data'] == 'versioned':\n            data_version = self.config['versioned']['version']\n            database_tables = self.config['versioned']\n        elif self.config['input_data_source']['input_data'] == 'model_draft':\n            data_version = 'model_draft'\n            database_tables = self.config['model_draft']\n        else:\n            data_version = 'unknown'\n            database_tables = 'unknown'\n\n        # Collect assumptions\n        assumptions = {}\n        assumptions.update(self.config['assumptions'])\n        assumptions.update(self.config['mv_connect'])\n        assumptions.update(self.config['mv_routing'])\n        assumptions.update(self.config['mv_routing_tech_constraints'])\n\n        # Determine run_id if not set\n        if not run_id:\n            run_id = datetime.now().strftime(\"%Y%m%d%H%M%S\")\n\n        # Set instance attribute run_id\n        if not self._run_id:\n            self._run_id = run_id\n\n        # Assing data to dict\n        metadata = dict(\n            version=version,\n            mv_grid_districts=[int(_.id_db) for _ in self._mv_grid_districts],\n            database_tables=database_tables,\n            data_version=data_version,\n            assumptions=assumptions,\n            run_id=self._run_id\n        )\n\n        return metadata", "response": "Provide metadata on a Ding0 run."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all generators in the database.", "response": "def list_generators(self, session):\n        \"\"\"\n        List renewable (res) and conventional (conv) generators\n\n        Args\n        ----\n        session : sqlalchemy.orm.session.Session\n            Database session\n            \n        Returns\n        -------\n        DataFrame\n        \"\"\"\n        srid = str(int(cfg_ding0.get('geo', 'srid')))\n\n        # build dicts to map MV grid district and Load Area ids to related objects\n        mv_grid_districts_dict,\\\n        lv_load_areas_dict,\\\n        lv_grid_districts_dict,\\\n        lv_stations_dict = self.get_mvgd_lvla_lvgd_obj_from_id()\n\n        # import renewable generators\n        # build query\n        generators_sqla = session.query(\n                self.orm['orm_re_generators'].columns.id,\n                self.orm['orm_re_generators'].columns.subst_id,\n                self.orm['orm_re_generators'].columns.la_id,\n                self.orm['orm_re_generators'].columns.mvlv_subst_id,\n                self.orm['orm_re_generators'].columns.electrical_capacity,\n                self.orm['orm_re_generators'].columns.generation_type,\n                self.orm['orm_re_generators'].columns.generation_subtype,\n                self.orm['orm_re_generators'].columns.voltage_level,\n                func.ST_AsText(func.ST_Transform(\n                   self.orm['orm_re_generators'].columns.rea_geom_new, srid)).label('geom_new'),\n                func.ST_AsText(func.ST_Transform(\n                   self.orm['orm_re_generators'].columns.geom, srid)).label('geom')\n                ).filter(\n                self.orm['orm_re_generators'].columns.subst_id.in_(list(mv_grid_districts_dict))). \\\n                filter(self.orm['orm_re_generators'].columns.voltage_level.in_([4, 5, 6, 7])). \\\n                filter(self.orm['version_condition_re'])\n\n        # read data from db\n        generators_res = pd.read_sql_query(generators_sqla.statement,\n                                        session.bind,\n                                        index_col='id')\n\n        generators_res.columns = ['GenCap' if c=='electrical_capacity' else\n                                  'type' if c=='generation_type' else\n                                  'subtype' if c=='generation_subtype' else\n                                  'v_level' if c=='voltage_level' else\n                                  c for c in generators_res.columns]\n        ###########################\n        # Imports conventional (conv) generators\n        # build query\n        generators_sqla = session.query(\n                self.orm['orm_conv_generators'].columns.id,\n                self.orm['orm_conv_generators'].columns.subst_id,\n                self.orm['orm_conv_generators'].columns.name,\n                self.orm['orm_conv_generators'].columns.capacity,\n                self.orm['orm_conv_generators'].columns.fuel,\n                self.orm['orm_conv_generators'].columns.voltage_level,\n                func.ST_AsText(func.ST_Transform(\n                   self.orm['orm_conv_generators'].columns.geom, srid)).label('geom')\n                ).filter(\n                self.orm['orm_conv_generators'].columns.subst_id.in_(list(mv_grid_districts_dict))). \\\n                filter(self.orm['orm_conv_generators'].columns.voltage_level.in_([4, 5, 6])). \\\n                filter(self.orm['version_condition_conv'])\n\n        # read data from db\n        generators_conv = pd.read_sql_query(generators_sqla.statement,\n                                           session.bind,\n                                           index_col='id')\n\n        generators_conv.columns = ['GenCap' if c=='capacity' else\n                                   'type' if c=='fuel' else\n                                   'v_level' if c=='voltage_level' else\n                                   c for c in generators_conv.columns]\n        ###########################\n        generators = pd.concat([generators_conv, generators_res], axis=0)\n        generators = generators.fillna('other')\n        return generators"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist load areas peak load from database for a single MV grid_district", "response": "def list_load_areas(self, session, mv_districts):\n        \"\"\"list load_areas (load areas) peak load from database for a single MV grid_district\n\n        Parameters\n        ----------\n        session : sqlalchemy.orm.session.Session\n            Database session\n        mv_districts: \n            List of MV districts\n        \"\"\"\n\n        # threshold: load area peak load, if peak load < threshold => disregard\n        # load area\n        lv_loads_threshold = cfg_ding0.get('mv_routing', 'load_area_threshold')\n        #lv_loads_threshold = 0\n\n        gw2kw = 10 ** 6  # load in database is in GW -> scale to kW\n\n        #filter list for only desired MV districts\n        stations_list = [d.mv_grid._station.id_db for d in mv_districts]\n\n        # build SQL query\n        lv_load_areas_sqla = session.query(\n            self.orm['orm_lv_load_areas'].id.label('id_db'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_residential * gw2kw).\\\n                label('peak_load_residential'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_retail * gw2kw).\\\n                label('peak_load_retail'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_industrial * gw2kw).\\\n                label('peak_load_industrial'),\n            (self.orm['orm_lv_load_areas'].sector_peakload_agricultural * gw2kw).\\\n                label('peak_load_agricultural'),\n            #self.orm['orm_lv_load_areas'].subst_id\n            ). \\\n            filter(self.orm['orm_lv_load_areas'].subst_id.in_(stations_list)).\\\n            filter(((self.orm['orm_lv_load_areas'].sector_peakload_residential  # only pick load areas with peak load > lv_loads_threshold\n                     + self.orm['orm_lv_load_areas'].sector_peakload_retail\n                     + self.orm['orm_lv_load_areas'].sector_peakload_industrial\n                     + self.orm['orm_lv_load_areas'].sector_peakload_agricultural)\n                       * gw2kw) > lv_loads_threshold). \\\n            filter(self.orm['version_condition_la'])\n\n        # read data from db\n        lv_load_areas = pd.read_sql_query(lv_load_areas_sqla.statement,\n                                          session.bind,\n                                          index_col='id_db')\n\n        return lv_load_areas"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimports all LV grid districts within given LV stations.", "response": "def list_lv_grid_districts(self, session, lv_stations):\n        \"\"\"Imports all lv grid districts within given load area\n\n        Parameters\n        ----------\n        session : sqlalchemy.orm.session.Session\n            Database session\n        lv_stations:\n            List required LV_stations==LV districts.\n\n        Returns\n        -------\n        pandas Dataframe\n            Table of lv_grid_districts\n        \"\"\"\n        gw2kw = 10 ** 6  # load in database is in GW -> scale to kW\n\n        # 1. filter grid districts of relevant load area\n        lv_grid_districs_sqla = session.query(\n            self.orm['orm_lv_grid_district'].mvlv_subst_id,\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_residential * gw2kw).\n                label('peak_load_residential'),\n            (self.orm['orm_lv_grid_district'].sector_peakload_retail * gw2kw).\n                label('peak_load_retail'),\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_industrial * gw2kw).\n                label('peak_load_industrial'),\n            (self.orm[\n                 'orm_lv_grid_district'].sector_peakload_agricultural * gw2kw).\n                label('peak_load_agricultural'),\n            ). \\\n            filter(self.orm['orm_lv_grid_district'].mvlv_subst_id.in_(\n            lv_stations)). \\\n            filter(self.orm['version_condition_lvgd'])\n\n        # read data from db\n        lv_grid_districts = pd.read_sql_query(lv_grid_districs_sqla.statement,\n                                              session.bind,\n                                              index_col='mvlv_subst_id')\n\n        return lv_grid_districts"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_dir(dirpath):\n\n    if not os.path.isdir(dirpath):\n        os.mkdir(dirpath)\n\n        print(\"We create a directory for you and your Ding0 data: {}\".format(\n            dirpath))", "response": "Create a directory and report about it"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_home_dir():\n    ding0_dir = str(cfg_ding0.get('config',\n                                  'config_dir'))\n    return os.path.join(os.path.expanduser('~'), ding0_dir)", "response": "Returns default home directory of Ding0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup_logger(log_dir=None, loglevel=logging.DEBUG):\n\n    create_home_dir()\n    create_dir(os.path.join(get_default_home_dir(), 'log'))\n\n    if log_dir is None:\n        log_dir = os.path.join(get_default_home_dir(), 'log')\n\n    logger = logging.getLogger('ding0') # use filename as name in log\n    logger.setLevel(loglevel)\n\n    # create a file handler\n    handler = logging.FileHandler(os.path.join(log_dir, 'ding0.log'))\n    handler.setLevel(logging.DEBUG)\n    formatter = logging.Formatter(\n        '%(asctime)s-%(funcName)s-%(message)s (%(levelname)s)')\n    handler.setFormatter(formatter)\n\n    # create a stream handler (print to prompt)\n    stream = logging.StreamHandler()\n    stream.setLevel(logging.INFO)\n    stream_formatter = logging.Formatter(\n        '%(message)s (%(levelname)s)')\n    stream.setFormatter(stream_formatter)\n\n    # add the handlers to the logger\n    logger.addHandler(handler)\n    logger.addHandler(stream)\n\n    logger.info('########## New run of Ding0 issued #############')\n\n    return logger", "response": "Create a logger object and return it"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a deep copy of the instance", "response": "def clone(self):\n        \"\"\"Returns a deep copy of self\n\n        Function clones:\n        \n        * routes\n        * allocation\n        * nodes\n\n        Returns\n        -------\n        SavingsSolution\n            A clone (deepcopy) of the instance itself\n        \"\"\"\n\n        new_solution = self.__class__(self._problem)\n\n        # Clone routes\n        for index, r in enumerate(self._routes):\n            new_route = new_solution._routes[index] = models.Route(self._problem)\n            for node in r.nodes():\n                # Insert new node on new route\n                new_node = new_solution._nodes[node.name()]\n                new_route.allocate([new_node])\n\n        # remove empty routes from new solution\n        new_solution._routes = [route for route in new_solution._routes if route._nodes]\n        \n        return new_solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_complete(self):\n        allocated = all(\n            [node.route_allocation() is not None for node in list(self._nodes.values()) if node.name() != self._problem.depot().name()]\n        )\n\n        valid_routes = len(self._routes) == 1 #workaround: try to use only one route (otherwise process will stop if no of vehicles is reached)\n\n        return allocated and valid_routes", "response": "Returns True if this solution is a complete solution i. e. all nodes are allocated and all routes are valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process(self, pair):\n        # TODO: check docstring\n        \"\"\"Processes a pair of nodes into the current solution\n\n        MUST CREATE A NEW INSTANCE, NOT CHANGE ANY INSTANCE ATTRIBUTES\n\n        Returns a new instance (deep copy) of self object\n        \n        Args\n        ----\n        pair : type\n            description\n            \n        Returns\n        -------\n        type\n            Description (Copy of self?)\n        \"\"\"\n        a, b = pair\n\n        new_solution = self.clone()\n\n        i, j = new_solution.get_pair((a, b))\n\n        route_i = i.route_allocation()\n        route_j = j.route_allocation()\n\n        inserted = False\n\n        if ((route_i is not None and route_j is not None) and (route_i != route_j)):\n            if route_i._nodes.index(i) == 0 and route_j._nodes.index(j) == len(route_j._nodes) - 1:\n                if route_j.can_allocate(route_i._nodes):\n                    route_j.allocate(route_i._nodes)\n\n                    if i.route_allocation() != j.route_allocation():\n                        raise Exception('wtf')\n\n                    inserted = True\n            elif route_j._nodes.index(j) == 0 and route_i._nodes.index(i) == len(route_i._nodes) - 1:\n                if route_i.can_allocate(route_j._nodes):\n                    route_i.allocate(route_j._nodes)\n\n                    if i.route_allocation() != j.route_allocation():\n                        raise Exception('wtf j')\n\n                    inserted = True\n\n        new_solution._routes = [route for route in new_solution._routes if route._nodes]\n\n        return new_solution, inserted", "response": "Processes a pair of nodes into the current solution"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef can_process(self, pairs):\n        i, j = pairs\n\n        # Neither points are in a route\n        if i.route_allocation() is None or j.route_allocation() is None:\n            return True\n\n        if self._allocated == len(list(self._problem.nodes())) - 1: # All nodes in a route\n            return False\n\n        return False", "response": "Returns True if this solution can process pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_savings_list(self, graph):\n\n        savings_list = {}\n\n        for i, j in graph.edges():\n            # t = (i, j)\n            if repr(i) < repr(j):\n                t = (i, j)\n            else:\n                t = (j, i)\n\n            if i == graph.depot() or j == graph.depot():\n                continue\n\n            savings_list[t] = graph.distance(graph.depot(), i) + graph.distance(graph.depot(), j) - graph.distance(i, j)\n\n        sorted_savings_list = sorted(list(savings_list.items()), key=operator.itemgetter(1), reverse=True)\n\n        return [nodes for nodes, saving in sorted_savings_list]", "response": "Compute Clarke and Wright savings list\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef solve(self, graph, timeout, debug=False, anim=None):\n\n        savings_list = self.compute_savings_list(graph)\n\n        solution = SavingsSolution(graph)\n\n        start = time.time()\n\n        for i, j in savings_list[:]:\n            if solution.is_complete():\n                break\n\n            if solution.can_process((i, j)):\n                solution, inserted = solution.process((i, j))\n\n                if inserted:\n                    savings_list.remove((i, j))\n\n                    if anim:\n                        solution.draw_network(anim)\n\n            if time.time() - start > timeout:\n                break\n\n        return solution", "response": "Solves the CVRP problem using Clarke and Wright Savings methods."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_to_dir(network, export_dir):\n\n    package_path = ding0.__path__[0]\n\n    network.export_to_csv_folder(os.path.join(package_path,\n                                              'output',\n                                              'debug',\n                                              'grid',\n                                              export_dir))", "response": "Exports PyPSA network as CSV files to directory export_dir"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of dataframes containing the given list of nodes.", "response": "def nodes_to_dict_of_dataframes(grid, nodes, lv_transformer=True):\n    \"\"\"\n    Creates dictionary of dataframes containing grid\n\n    Parameters\n    ----------\n    grid: ding0.Network\n    nodes: list of ding0 grid components objects\n        Nodes of the grid graph\n    lv_transformer: bool, True\n        Toggle transformer representation in power flow analysis\n\n    Returns:\n    components: dict of pandas.DataFrame\n        DataFrames contain components attributes. Dict is keyed by components\n        type\n    components_data: dict of pandas.DataFrame\n        DataFrame containing components time-varying data\n    \"\"\"\n\n    generator_instances = [MVStationDing0, GeneratorDing0]\n    # TODO: MVStationDing0 has a slack generator\n\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n    srid = int(cfg_ding0.get('geo', 'srid'))\n\n    load_in_generation_case = cfg_ding0.get('assumptions',\n                                            'load_in_generation_case')\n    generation_in_load_case = cfg_ding0.get('assumptions',\n                                            'generation_in_load_case')\n\n    Q_factor_load = tan(acos(cos_phi_load))\n    Q_factor_generation = tan(acos(cos_phi_feedin))\n\n    voltage_set_slack = cfg_ding0.get(\"mv_routing_tech_constraints\",\n                                      \"mv_station_v_level_operation\")\n\n    kw2mw = 1e-3\n\n    # define dictionaries\n    buses = {'bus_id': [], 'v_nom': [], 'geom': [], 'grid_id': []}\n    bus_v_mag_set = {'bus_id': [], 'temp_id': [], 'v_mag_pu_set': [],\n                     'grid_id': []}\n    generator = {'generator_id': [], 'bus': [], 'control': [], 'grid_id': [],\n                 'p_nom': []}\n    generator_pq_set = {'generator_id': [], 'temp_id': [], 'p_set': [],\n                        'grid_id': [], 'q_set': []}\n    load = {'load_id': [], 'bus': [], 'grid_id': []}\n    load_pq_set = {'load_id': [], 'temp_id': [], 'p_set': [],\n                   'grid_id': [], 'q_set': []}\n\n    # # TODO: consider other implications of `lv_transformer is True`\n    # if lv_transformer is True:\n    #     bus_instances.append(Transformer)\n\n    # # TODO: only for debugging, remove afterwards\n    # import csv\n    # nodeslist = sorted([node.__repr__() for node in nodes\n    #                     if node not in grid.graph_isolated_nodes()])\n    # with open('/home/guido/ding0_debug/nodes_via_dataframe.csv', 'w', newline='') as csvfile:\n    #     writer = csv.writer(csvfile, delimiter='\\n')\n    #     writer.writerow(nodeslist)\n\n    for node in nodes:\n        if node not in grid.graph_isolated_nodes():\n            # buses only\n            if isinstance(node, MVCableDistributorDing0):\n                buses['bus_id'].append(node.pypsa_id)\n                buses['v_nom'].append(grid.v_level)\n                buses['geom'].append(from_shape(node.geo_data, srid=srid))\n                buses['grid_id'].append(grid.id_db)\n\n                bus_v_mag_set['bus_id'].append(node.pypsa_id)\n                bus_v_mag_set['temp_id'].append(1)\n                bus_v_mag_set['v_mag_pu_set'].append([1, 1])\n                bus_v_mag_set['grid_id'].append(grid.id_db)\n\n            # bus + generator\n            elif isinstance(node, tuple(generator_instances)):\n                # slack generator\n                if isinstance(node, MVStationDing0):\n                    logger.info('Only MV side bus of MVStation will be added.')\n                    generator['generator_id'].append(\n                        '_'.join(['MV', str(grid.id_db), 'slack']))\n                    generator['control'].append('Slack')\n                    generator['p_nom'].append(0)\n                    bus_v_mag_set['v_mag_pu_set'].append(\n                        [voltage_set_slack, voltage_set_slack])\n\n                # other generators\n                if isinstance(node, GeneratorDing0):\n                    generator['generator_id'].append('_'.join(\n                        ['MV', str(grid.id_db), 'gen', str(node.id_db)]))\n                    generator['control'].append('PQ')\n                    generator['p_nom'].append(node.capacity * node.capacity_factor)\n\n                    generator_pq_set['generator_id'].append('_'.join(\n                        ['MV', str(grid.id_db), 'gen', str(node.id_db)]))\n                    generator_pq_set['temp_id'].append(1)\n                    generator_pq_set['p_set'].append(\n                        [node.capacity * node.capacity_factor * kw2mw * generation_in_load_case,\n                         node.capacity * node.capacity_factor * kw2mw])\n                    generator_pq_set['q_set'].append(\n                        [node.capacity * node.capacity_factor * kw2mw * Q_factor_generation * generation_in_load_case,\n                         node.capacity * node.capacity_factor * kw2mw * Q_factor_generation])\n                    generator_pq_set['grid_id'].append(grid.id_db)\n                    bus_v_mag_set['v_mag_pu_set'].append([1, 1])\n\n                buses['bus_id'].append(node.pypsa_id)\n                buses['v_nom'].append(grid.v_level)\n                buses['geom'].append(from_shape(node.geo_data, srid=srid))\n                buses['grid_id'].append(grid.id_db)\n\n                bus_v_mag_set['bus_id'].append(node.pypsa_id)\n                bus_v_mag_set['temp_id'].append(1)\n                bus_v_mag_set['grid_id'].append(grid.id_db)\n\n                generator['grid_id'].append(grid.id_db)\n                generator['bus'].append(node.pypsa_id)\n\n\n            # aggregated load at hv/mv substation\n            elif isinstance(node, LVLoadAreaCentreDing0):\n                load['load_id'].append(node.pypsa_id)\n                load['bus'].append('_'.join(['HV', str(grid.id_db), 'trd']))\n                load['grid_id'].append(grid.id_db)\n\n                load_pq_set['load_id'].append(node.pypsa_id)\n                load_pq_set['temp_id'].append(1)\n                load_pq_set['p_set'].append(\n                    [node.lv_load_area.peak_load * kw2mw,\n                     node.lv_load_area.peak_load * kw2mw * load_in_generation_case])\n                load_pq_set['q_set'].append(\n                    [node.lv_load_area.peak_load * kw2mw * Q_factor_load,\n                     node.lv_load_area.peak_load * kw2mw * Q_factor_load * load_in_generation_case])\n                load_pq_set['grid_id'].append(grid.id_db)\n\n                # generator representing generation capacity of aggregate LA\n                # analogously to load, generation is connected directly to\n                # HV-MV substation\n                generator['generator_id'].append('_'.join(\n                    ['MV', str(grid.id_db), 'lcg', str(node.id_db)]))\n                generator['control'].append('PQ')\n                generator['p_nom'].append(node.lv_load_area.peak_generation)\n                generator['grid_id'].append(grid.id_db)\n                generator['bus'].append('_'.join(['HV', str(grid.id_db), 'trd']))\n\n                generator_pq_set['generator_id'].append('_'.join(\n                    ['MV', str(grid.id_db), 'lcg', str(node.id_db)]))\n                generator_pq_set['temp_id'].append(1)\n                generator_pq_set['p_set'].append(\n                    [node.lv_load_area.peak_generation * kw2mw * generation_in_load_case,\n                     node.lv_load_area.peak_generation * kw2mw])\n                generator_pq_set['q_set'].append(\n                    [node.lv_load_area.peak_generation * kw2mw * Q_factor_generation * generation_in_load_case,\n                     node.lv_load_area.peak_generation * kw2mw * Q_factor_generation])\n                generator_pq_set['grid_id'].append(grid.id_db)\n\n            # bus + aggregate load of lv grids (at mv/ls substation)\n            elif isinstance(node, LVStationDing0):\n                # Aggregated load representing load in LV grid\n                load['load_id'].append(\n                    '_'.join(['MV', str(grid.id_db), 'loa', str(node.id_db)]))\n                load['bus'].append(node.pypsa_id)\n                load['grid_id'].append(grid.id_db)\n\n                load_pq_set['load_id'].append(\n                    '_'.join(['MV', str(grid.id_db), 'loa', str(node.id_db)]))\n                load_pq_set['temp_id'].append(1)\n                load_pq_set['p_set'].append(\n                    [node.peak_load * kw2mw,\n                     node.peak_load * kw2mw * load_in_generation_case])\n                load_pq_set['q_set'].append(\n                    [node.peak_load * kw2mw * Q_factor_load,\n                     node.peak_load * kw2mw * Q_factor_load * load_in_generation_case])\n                load_pq_set['grid_id'].append(grid.id_db)\n\n                # bus at primary MV-LV transformer side\n                buses['bus_id'].append(node.pypsa_id)\n                buses['v_nom'].append(grid.v_level)\n                buses['geom'].append(from_shape(node.geo_data, srid=srid))\n                buses['grid_id'].append(grid.id_db)\n\n                bus_v_mag_set['bus_id'].append(node.pypsa_id)\n                bus_v_mag_set['temp_id'].append(1)\n                bus_v_mag_set['v_mag_pu_set'].append([1, 1])\n                bus_v_mag_set['grid_id'].append(grid.id_db)\n\n                # generator representing generation capacity in LV grid\n                generator['generator_id'].append('_'.join(\n                    ['MV', str(grid.id_db), 'gen', str(node.id_db)]))\n                generator['control'].append('PQ')\n                generator['p_nom'].append(node.peak_generation)\n                generator['grid_id'].append(grid.id_db)\n                generator['bus'].append(node.pypsa_id)\n\n                generator_pq_set['generator_id'].append('_'.join(\n                    ['MV', str(grid.id_db), 'gen', str(node.id_db)]))\n                generator_pq_set['temp_id'].append(1)\n                generator_pq_set['p_set'].append(\n                    [node.peak_generation * kw2mw * generation_in_load_case,\n                     node.peak_generation * kw2mw])\n                generator_pq_set['q_set'].append(\n                    [node.peak_generation * kw2mw * Q_factor_generation * generation_in_load_case,\n                     node.peak_generation * kw2mw * Q_factor_generation])\n                generator_pq_set['grid_id'].append(grid.id_db)\n\n            elif isinstance(node, CircuitBreakerDing0):\n                # TODO: remove this elif-case if CircuitBreaker are removed from graph\n                continue\n            else:\n                raise TypeError(\"Node of type\", node, \"cannot be handled here\")\n        else:\n            if not isinstance(node, CircuitBreakerDing0):\n                add_info =  \"LA is aggr. {0}\".format(\n                    node.lv_load_area.is_aggregated)\n            else:\n                add_info = \"\"\n            logger.warning(\"Node {0} is not connected to the graph and will \" \\\n                  \"be omitted in power flow analysis. {1}\".format(\n                node, add_info))\n\n    components = {'Bus': DataFrame(buses).set_index('bus_id'),\n                  'Generator': DataFrame(generator).set_index('generator_id'),\n                  'Load': DataFrame(load).set_index('load_id')}\n\n    components_data = {'Bus': DataFrame(bus_v_mag_set).set_index('bus_id'),\n                       'Generator': DataFrame(generator_pq_set).set_index(\n                           'generator_id'),\n                       'Load': DataFrame(load_pq_set).set_index('load_id')}\n\n    # with open('/home/guido/ding0_debug/number_of_nodes_buses.csv', 'a') as csvfile:\n    #     csvfile.write(','.join(['\\n', str(len(nodes)), str(len(grid.graph_isolated_nodes())), str(len(components['Bus']))]))\n\n    return components, components_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting edges to dict of dataframes.", "response": "def edges_to_dict_of_dataframes(grid, edges):\n    \"\"\"\n    Export edges to DataFrame\n\n    Parameters\n    ----------\n    grid: ding0.Network\n    edges: list\n        Edges of Ding0.Network graph\n\n    Returns\n    -------\n    edges_dict: dict\n    \"\"\"\n    omega = 2 * pi * 50\n    srid = int(cfg_ding0.get('geo', 'srid'))\n\n    lines = {'line_id': [], 'bus0': [], 'bus1': [], 'x': [], 'r': [],\n             's_nom': [], 'length': [], 'cables': [], 'geom': [],\n             'grid_id': []}\n\n    # iterate over edges and add them one by one\n    for edge in edges:\n\n        line_name = '_'.join(['MV',\n                              str(grid.id_db),\n                              'lin',\n                              str(edge['branch'].id_db)])\n\n        # TODO: find the real cause for being L, C, I_th_max type of Series\n        if (isinstance(edge['branch'].type['L'], Series) or\n                isinstance(edge['branch'].type['C'], Series)):\n            x = omega * edge['branch'].type['L'].values[0] * 1e-3\n        else:\n\n            x = omega * edge['branch'].type['L'] * 1e-3\n\n        if isinstance(edge['branch'].type['R'], Series):\n            r = edge['branch'].type['R'].values[0]\n        else:\n            r = edge['branch'].type['R']\n\n        if (isinstance(edge['branch'].type['I_max_th'], Series) or\n                isinstance(edge['branch'].type['U_n'], Series)):\n            s_nom = sqrt(3) * edge['branch'].type['I_max_th'].values[0] * \\\n                    edge['branch'].type['U_n'].values[0]\n        else:\n            s_nom = sqrt(3) * edge['branch'].type['I_max_th'] * \\\n                    edge['branch'].type['U_n']\n\n        # get lengths of line\n        l = edge['branch'].length / 1e3\n\n        lines['line_id'].append(line_name)\n        lines['bus0'].append(edge['adj_nodes'][0].pypsa_id)\n        lines['bus1'].append(edge['adj_nodes'][1].pypsa_id)\n        lines['x'].append(x * l)\n        lines['r'].append(r * l)\n        lines['s_nom'].append(s_nom)\n        lines['length'].append(l)\n        lines['cables'].append(3)\n        lines['geom'].append(from_shape(\n            LineString([edge['adj_nodes'][0].geo_data,\n                        edge['adj_nodes'][1].geo_data]),\n            srid=srid))\n        lines['grid_id'].append(grid.id_db)\n\n    return {'Line': DataFrame(lines).set_index('line_id')}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun powerflow test for a single set of components.", "response": "def run_powerflow_onthefly(components, components_data, grid, export_pypsa_dir=None, debug=False):\n    \"\"\"\n    Run powerflow to test grid stability\n\n    Two cases are defined to be tested here:\n     i) load case\n     ii) feed-in case\n\n    Parameters\n    ----------\n    components: dict of pandas.DataFrame\n    components_data: dict of pandas.DataFrame\n    export_pypsa_dir: str\n        Sub-directory in output/debug/grid/ where csv Files of PyPSA network are exported to.\n        Export is omitted if argument is empty.\n    \"\"\"\n\n    scenario = cfg_ding0.get(\"powerflow\", \"test_grid_stability_scenario\")\n    start_hour = cfg_ding0.get(\"powerflow\", \"start_hour\")\n    end_hour = cfg_ding0.get(\"powerflow\", \"end_hour\")\n\n    # choose temp_id\n    temp_id_set = 1\n    timesteps = 2\n    start_time = datetime(1970, 1, 1, 00, 00, 0)\n    resolution = 'H'\n\n    # inspect grid data for integrity\n    if debug:\n        data_integrity(components, components_data)\n\n    # define investigated time range\n    timerange = DatetimeIndex(freq=resolution,\n                              periods=timesteps,\n                              start=start_time)\n\n    # TODO: Instead of hard coding PF config, values from class PFConfigDing0 can be used here.\n\n    # create PyPSA powerflow problem\n    network, snapshots = create_powerflow_problem(timerange, components)\n\n    # import pq-sets\n    for key in ['Load', 'Generator']:\n        for attr in ['p_set', 'q_set']:\n            # catch MV grid districts without generators\n            if not components_data[key].empty:\n                series = transform_timeseries4pypsa(components_data[key][\n                                                        attr].to_frame(),\n                                                    timerange,\n                                                    column=attr)\n                import_series_from_dataframe(network,\n                                             series,\n                                             key,\n                                             attr)\n    series = transform_timeseries4pypsa(components_data['Bus']\n                                        ['v_mag_pu_set'].to_frame(),\n                                        timerange,\n                                        column='v_mag_pu_set')\n\n    import_series_from_dataframe(network,\n                                 series,\n                                 'Bus',\n                                 'v_mag_pu_set')\n\n    # add coordinates to network nodes and make ready for map plotting\n    # network = add_coordinates(network)\n\n    # start powerflow calculations\n    network.pf(snapshots)\n\n    # # make a line loading plot\n    # # TODO: make this optional\n    # plot_line_loading(network, timestep=0,\n    #                   filename='Line_loading_load_case.png')\n    # plot_line_loading(network, timestep=1,\n    #                   filename='Line_loading_feed-in_case.png')\n\n    # process results\n    bus_data, line_data = process_pf_results(network)\n\n    # assign results data to graph\n    assign_bus_results(grid, bus_data)\n    assign_line_results(grid, line_data)\n\n    # export network if directory is specified\n    if export_pypsa_dir:\n        export_to_dir(network, export_dir=export_pypsa_dir)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks grid data for integrity", "response": "def data_integrity(components, components_data):\n    \"\"\"\n    Check grid data for integrity\n\n    Parameters\n    ----------\n    components: dict\n        Grid components\n    components_data: dict\n        Grid component data (such as p,q and v set points)\n\n    Returns\n    -------\n    \"\"\"\n\n    data_check = {}\n\n    for comp in ['Bus', 'Load']:  # list(components_data.keys()):\n        data_check[comp] = {}\n        data_check[comp]['length_diff'] = len(components[comp]) - len(\n            components_data[comp])\n\n    # print short report to user and exit program if not integer\n    for comp in list(data_check.keys()):\n        if data_check[comp]['length_diff'] != 0:\n            logger.exception(\"{comp} data is invalid. You supplied {no_comp} {comp} \"\n                  \"objects and {no_data} datasets. Check you grid data \"\n                  \"and try again\".format(comp=comp,\n                                         no_comp=len(components[comp]),\n                                         no_data=len(components_data[comp])))\n            sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nassigning voltage levels obtained from PF to graph.", "response": "def assign_bus_results(grid, bus_data):\n    \"\"\"\n    Write results obtained from PF to graph\n\n    Parameters\n    ----------\n    grid: ding0.network\n    bus_data: pandas.DataFrame\n        DataFrame containing voltage levels obtained from PF analysis\n    \"\"\"\n\n    # iterate of nodes and assign voltage obtained from power flow analysis\n    for node in grid._graph.nodes():\n        # check if node is connected to graph\n        if (node not in grid.graph_isolated_nodes()\n            and not isinstance(node,\n                               LVLoadAreaCentreDing0)):\n            if isinstance(node, LVStationDing0):\n                node.voltage_res = bus_data.loc[node.pypsa_id, 'v_mag_pu']\n            elif isinstance(node, (LVStationDing0, LVLoadAreaCentreDing0)):\n                if node.lv_load_area.is_aggregated:\n                    node.voltage_res = bus_data.loc[node.pypsa_id, 'v_mag_pu']\n            elif not isinstance(node, CircuitBreakerDing0):\n                node.voltage_res = bus_data.loc[node.pypsa_id, 'v_mag_pu']\n            else:\n                logger.warning(\"Object {} has been skipped while importing \"\n                               \"results!\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assign_line_results(grid, line_data):\n\n    package_path = ding0.__path__[0]\n\n    edges = [edge for edge in grid.graph_edges()\n             if (edge['adj_nodes'][0] in grid._graph.nodes() and not isinstance(\n            edge['adj_nodes'][0], LVLoadAreaCentreDing0))\n             and (\n             edge['adj_nodes'][1] in grid._graph.nodes() and not isinstance(\n                 edge['adj_nodes'][1], LVLoadAreaCentreDing0))]\n\n    decimal_places = 6\n    for edge in edges:\n        s_res = [\n            round(sqrt(\n                max(abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                    'branch'].id_db), 'p0'][0]),\n                    abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                        'branch'].id_db), 'p1'][0])) ** 2 +\n                max(abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                    'branch'].id_db), 'q0'][0]),\n                    abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                        'branch'].id_db), 'q1'][0])) ** 2),decimal_places),\n            round(sqrt(\n                max(abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                    'branch'].id_db), 'p0'][1]),\n                    abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                        'branch'].id_db), 'p1'][1])) ** 2 +\n                max(abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                    'branch'].id_db), 'q0'][1]),\n                    abs(line_data.loc[\"MV_{0}_lin_{1}\".format(grid.id_db, edge[\n                        'branch'].id_db), 'q1'][1])) ** 2),decimal_places)]\n\n        edge['branch'].s_res = s_res", "response": "Assigns line results obtained from PF to graph\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_pypsa_network(time_range_lim):\n    network = Network()\n    network.set_snapshots(time_range_lim)\n    snapshots = network.snapshots\n\n    return network, snapshots", "response": "Instantiate PyPSA network and set snapshots"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_timeseries4pypsa(timeseries, timerange, column=None):\n    timeseries.index = [str(i) for i in timeseries.index]\n\n    if column is None:\n        pypsa_timeseries = timeseries.apply(\n            Series).transpose().set_index(timerange)\n    else:\n        pypsa_timeseries = timeseries[column].apply(\n            Series).transpose().set_index(timerange)\n\n    return pypsa_timeseries", "response": "Transform pq - set timeseries to PyPSA compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates PyPSA powerflow problem object and fill with data", "response": "def create_powerflow_problem(timerange, components):\n    \"\"\"\n    Create PyPSA network object and fill with data\n    Parameters\n    ----------\n    timerange: Pandas DatetimeIndex\n        Time range to be analyzed by PF\n    components: dict\n    Returns\n    -------\n    network: PyPSA powerflow problem object\n    \"\"\"\n\n    # initialize powerflow problem\n    network, snapshots = init_pypsa_network(timerange)\n\n    # add components to network\n    for component in components.keys():\n        network.import_components_from_dataframe(components[component],\n                                                 component)\n\n    return network, snapshots"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parallel_run(districts_list, n_of_processes, n_of_districts, run_id,\n                 base_path=None):\n    '''Organize parallel runs of ding0.\n\n    The function take all districts in a list and divide them into\n    n_of_processes parallel processes. For each process, the assigned districts\n    are given to the function process_runs() with the argument n_of_districts\n\n    Parameters\n    ----------\n    districts_list: list of int\n        List with all districts to be run.\n    n_of_processes: int\n        Number of processes to run in parallel\n    n_of_districts: int\n        Number of districts to be run in each cluster given as argument to\n        process_stats()\n    run_id: str\n        Identifier for a run of Ding0. For example it is used to create a\n        subdirectory of os.path.join(`base_path`, 'results')\n    base_path : str\n        Base path for ding0 data (input, results and logs).\n        Default is `None` which sets it to :code:`~/.ding0` (may deviate on\n        windows systems).\n        Specify your own but keep in mind that it a required a particular\n        structure of subdirectories.\n\n    See Also\n    --------\n    ding0_runs\n\n    '''\n\n    # define base path\n    if base_path is None:\n        base_path = BASEPATH\n\n    if not os.path.exists(os.path.join(base_path, run_id)):\n        os.makedirs(os.path.join(base_path, run_id))\n\n    start = time.time()\n    #######################################################################\n    # Define an output queue\n    output_info = mp.Queue()\n    #######################################################################\n    # Setup a list of processes that we want to run\n    max_dist = len(districts_list)\n    threat_long = floor(max_dist / n_of_processes)\n\n    if threat_long == 0:\n        threat_long = 1\n\n    threats = [districts_list[x:x + threat_long] for x in range(0, len(districts_list), threat_long)]\n\n    processes = []\n    for th in threats:\n        mv_districts = th\n        processes.append(mp.Process(target=process_runs,\n                                    args=(mv_districts, n_of_districts,\n                                          output_info, run_id, base_path)))\n    #######################################################################\n    # Run processes\n    for p in processes:\n        p.start()\n    # Resque output_info from processes\n    output = [output_info.get() for p in processes]\n    output = list(itertools.chain.from_iterable(output))\n    # Exit the completed processes\n    for p in processes:\n        p.join()\n\n    #######################################################################\n    print('Elapsed time for', str(max_dist),\n          'MV grid districts (seconds): {}'.format(time.time() - start))\n\n    return output", "response": "This function processes the districts in a list and runs them in parallel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_runs(mv_districts, n_of_districts, output_info, run_id, base_path):\n    '''Runs a process organized by parallel_run()\n\n    The function take all districts mv_districts and divide them into clusters\n    of n_of_districts each. For each cluster, ding0 is run and the resulting\n    network is saved as a pickle\n\n    Parameters\n    ----------\n    mv_districts: list of int\n        List with all districts to be run.\n    n_of_districts: int\n        Number of districts in a cluster\n    output_info:\n        Info about how the run went\n    run_id: str\n        Identifier for a run of Ding0. For example it is used to create a\n        subdirectory of os.path.join(`base_path`, 'results')\n    base_path : str\n        Base path for ding0 data (input, results and logs).\n        Default is `None` which sets it to :code:`~/.ding0` (may deviate on\n        windows systems).\n        Specify your own but keep in mind that it a required a particular\n        structure of subdirectories.\n\n    See Also\n    --------\n    parallel_run\n\n    '''\n    #######################################################################\n    # database connection/ session\n    engine = db.connection(section='oedb')\n    session = sessionmaker(bind=engine)()\n\n    #############################\n    clusters = [mv_districts[x:x + n_of_districts] for x in range(0, len(mv_districts), n_of_districts)]\n    output_clusters= []\n\n    for cl in clusters:\n        print('\\n########################################')\n        print('  Running ding0 for district', cl)\n        print('########################################')\n\n        nw_name = 'ding0_grids_' + str(cl[0])\n        if not cl[0] == cl[-1]:\n            nw_name = nw_name+'_to_'+str(cl[-1])\n        nw = NetworkDing0(name=nw_name)\n        try:\n            msg = nw.run_ding0(session=session, mv_grid_districts_no=cl)\n            if msg:\n                status = 'run error'\n            else:\n                msg = ''\n                status = 'OK'\n                results.save_nd_to_pickle(nw, os.path.join(base_path, run_id))\n            output_clusters.append((nw_name,status,msg, nw.metadata))\n        except Exception as e:\n            output_clusters.append((nw_name,  'corrupt dist', e, nw.metadata))\n            continue\n\n    output_info.put(output_clusters)", "response": "Runs a process organized by parallel_run"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef process_metadata(meta):\n    mvgds = []\n\n    metadata = meta[0]\n\n    for mvgd in meta:\n        if isinstance(mvgd['mv_grid_districts'], list):\n            mvgds.extend(mvgd['mv_grid_districts'])\n        else:\n            mvgds.append(mvgd['mv_grid_districts'])\n\n    metadata['mv_grid_districts'] = mvgds\n\n    return metadata", "response": "Process metadata of run on multiple MV grid districts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reinforce_grid(grid, mode):\n    #TODO: finish docstring\n    \"\"\" Evaluates grid reinforcement needs and performs measures\n\n    Grid reinforcement according to methods described in [VNSRP]_ supplemented\n    by [DENA]_.\n    \n    Parameters\n    ----------\n    grid: GridDing0\n        Grid instance\n    mode: str\n        Choose of: 'MV' or 'LV'\n\n    Notes\n    -----\n    Currently only MV branch reinforcement is implemented. HV-MV stations are not\n    reinforced since not required for status-quo scenario.\n\n    References\n    ----------\n    .. [DENA] Deutsche Energie-Agentur GmbH (dena), \"dena-Verteilnetzstudie. Ausbau- und Innovationsbedarf der\n            Stromverteilnetze in Deutschland bis 2030.\", 2012\n    .. [VNSRP] Ackermann, T., Untsch, S., Koch, M., & Rothfuchs, H. (2014).\n            Verteilnetzstudie Rheinland-Pfalz. Hg. v. Ministerium f\u00fcr\n            Wirtschaft, Klimaschutz, Energie und Landesplanung Rheinland-Pfalz\n            (MWKEL). energynautics GmbH.\n\n    \"\"\"\n\n    # kind of grid to be evaluated (MV or LV)\n    if mode == 'MV':\n        crit_branches, crit_stations = check_load(grid, mode)\n\n        # STEP 1: reinforce branches\n\n        # do reinforcement\n        reinforce_branches_current(grid, crit_branches)\n\n        # if branches or stations have been reinforced: run PF again to check for voltage issues\n        if crit_branches or crit_stations:\n            grid.network.run_powerflow(conn=None, method='onthefly')\n\n        crit_nodes = check_voltage(grid, mode)\n        crit_nodes_count_prev_step = len(crit_nodes)\n\n        # as long as there are voltage issues, do reinforcement\n        while crit_nodes:\n            # determine all branches on the way from HV-MV substation to crit. nodes\n            crit_branches_v = grid.find_and_union_paths(grid.station(), crit_nodes)\n\n            # do reinforcement\n            reinforce_branches_voltage(grid, crit_branches_v)\n\n            # run PF\n            grid.network.run_powerflow(session=None, method='onthefly')\n\n            crit_nodes = check_voltage(grid, mode)\n\n            # if there are critical nodes left but no larger cable available, stop reinforcement\n            if len(crit_nodes) == crit_nodes_count_prev_step:\n                logger.warning('==> There are {0} branches that cannot be '\n                               'reinforced (no appropriate cable '\n                               'available).'.format(\n                    len(grid.find_and_union_paths(grid.station(),\n                                                        crit_nodes))))\n                break\n\n            crit_nodes_count_prev_step = len(crit_nodes)\n\n        if not crit_nodes:\n            logger.info('==> All voltage issues in {mode} grid could be '\n                        'solved using reinforcement.'.format(mode=mode))\n\n        # STEP 2: reinforce HV-MV station\n        # NOTE: HV-MV station reinforcement is not required for status-quo\n        # scenario since HV-MV trafos already sufficient for load+generation\n        # case as done in MVStationDing0.choose_transformers()\n\n    elif mode == 'LV':\n        # get overloaded branches\n        # overloading issues\n        critical_branches, critical_stations = get_critical_line_loading(grid)\n\n\n        # reinforce overloaded lines by increasing size\n        unresolved = reinforce_lv_branches_overloading(grid, critical_branches)\n        logger.info(\n            \"Out of {crit_branches} with overloading {unresolved} remain \"\n            \"with unresolved issues due to line overloading. \"\n            \"LV grid: {grid}\".format(\n                crit_branches=len(critical_branches),\n                unresolved=len(unresolved),\n                grid=grid))\n\n        # reinforce substations\n        extend_substation(grid, critical_stations, mode)\n\n        # get node with over-voltage\n        crit_nodes = get_critical_voltage_at_nodes(grid) #over-voltage issues\n\n        crit_nodes_count_prev_step = len(crit_nodes)\n\n        logger.info('{cnt_crit_branches} in {grid} have voltage issues'.format(\n            cnt_crit_branches=crit_nodes_count_prev_step,\n            grid=grid))\n\n        # as long as there are voltage issues, do reinforcement\n        while crit_nodes:\n            # determine all branches on the way from HV-MV substation to crit. nodes\n            crit_branches_v = grid.find_and_union_paths(\n                grid.station(),\n                [_['node'] for _ in crit_nodes])\n\n            # do reinforcement\n            reinforce_branches_voltage(grid, crit_branches_v, mode)\n\n            # get node with over-voltage\n            crit_nodes = get_critical_voltage_at_nodes(grid)\n\n            # if there are critical nodes left but no larger cable available, stop reinforcement\n            if len(crit_nodes) == crit_nodes_count_prev_step:\n                logger.warning('==> There are {0} branches that cannot be '\n                               'reinforced (no appropriate cable '\n                               'available).'.format(\n                    len(crit_branches_v)))\n                break\n\n            crit_nodes_count_prev_step = len(crit_nodes)\n\n        if not crit_nodes:\n            logger.info('==> All voltage issues in {mode} grid could be '\n                        'solved using reinforcement.'.format(mode=mode))\n\n        # reinforcement of LV stations on voltage issues\n        crit_stations_voltage = [_ for _ in crit_nodes\n                        if isinstance(_['node'], LVStationDing0)]\n        if crit_stations_voltage:\n            extend_substation_voltage(crit_stations_voltage, grid_level='LV')", "response": "Evaluate and perform reinforcement of a single grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef example_stats(filename):\n\n    nd = results.load_nd_from_pickle(filename=filename)\n\n    nodes_df, edges_df = nd.to_dataframe()\n\n    # get statistical numbers about grid\n    stats = results.calculate_mvgd_stats(nd)\n\n    # plot distribution of load/generation of subjacent LV grids\n    stations = nodes_df[nodes_df['type'] == 'LV Station']\n    f, axarr = plt.subplots(2, sharex=True)\n    f.suptitle(\"Peak load (top) / peak generation capacity (bottom) at LV \"\n               \"substations in kW\")\n    stations['peak_load'].hist(bins=20, alpha=0.5, ax=axarr[0])\n    axarr[0].set_title(\"Peak load in kW\")\n    stations['generation_capacity'].hist(bins=20, alpha=0.5, ax=axarr[1])\n    axarr[1].set_title(\"Peak generation capacity in kW\")\n    plt.show()\n\n    # Introduction of report\n    print(\"You are analyzing MV grid district {mvgd}\\n\".format(\n        mvgd=int(stats.index.values)))\n\n    # print all the calculated stats\n    # this isn't a particularly beautiful format but it is\n    # information rich\n    with option_context('display.max_rows', None,\n                        'display.max_columns', None,\n                        'display.max_colwidth', -1):\n        print(stats.T)", "response": "This function generates a simple example of how to calculate statistics about a MV grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compare_graphs(graph1, mode, graph2=None):\n\n    # get path\n    package_path = ding0.__path__[0]\n    file = path.join(package_path, 'output', 'debug', 'graph1.gpickle')\n\n    if mode == 'write':\n        try:\n            nx.write_gpickle(graph1, file)\n            print('=====> DEBUG: Graph written to', file)\n        except:\n            raise FileNotFoundError('Could not write to file', file)\n\n    elif mode == 'compare':\n        if graph2 is None:\n            try:\n                graph2 = nx.read_gpickle(file)\n                print('=====> DEBUG: Graph read from', file)\n            except:\n                raise FileNotFoundError('File not found:', file)\n\n        # get data\n        nodes1 = sorted(graph1.nodes(), key=lambda _: repr(_))\n        nodes2 = sorted(graph2.nodes(), key=lambda _: repr(_))\n        edges1 = sorted(graph1.edges(), key=lambda _: repr(_))\n        edges2 = sorted(graph2.edges(), key=lambda _: repr(_))\n\n        graphs_are_isomorphic = True\n\n        # check nodes\n        if len(nodes1) > len(nodes2):\n            print('Node count in graph 1 > node count in graph 2')\n            print('Difference:', [node for node in nodes1 if repr(node) not in repr(nodes2)])\n            graphs_are_isomorphic = False\n        elif len(nodes2) > len(nodes1):\n            print('Node count in graph 2 > node count in graph 1')\n            print('Difference:', [node for node in nodes2 if repr(node) not in repr(nodes1)])\n            graphs_are_isomorphic = False\n\n        # check edges\n        if len(edges1) > len(edges2):\n            print('Edge count in graph 1 > edge count in graph 2')\n            print('Difference:', [edge for edge in edges1 if (repr(edge) not in repr(edges2)) and\n                                  (repr(tuple(reversed(edge))) not in repr(edges2))])\n            graphs_are_isomorphic = False\n        elif len(edges2) > len(edges1):\n            print('Edge count in graph 2 > edge count in graph 1')\n            print('Difference:', [edge for edge in edges2 if (repr(edge) not in repr(edges1)) and\n                                  (repr(tuple(reversed(edge))) not in repr(edges1))])\n            graphs_are_isomorphic = False\n        elif (len(edges1) == len(edges1)) and (len([edge for edge in edges1 if (repr(edge) not in repr(edges2)) and\n            (repr(tuple(reversed(edge))) not in repr(edges2))]) > 0):\n            print('Edge count in graph 1 = edge count in graph 2')\n            print('Difference:', [edge for edge in edges2 if (repr(edge) not in repr(edges1)) and\n                                  (repr(tuple(reversed(edge))) not in repr(edges1))])\n            graphs_are_isomorphic = False\n\n        if graphs_are_isomorphic:\n            print('=====> DEBUG: Graphs are isomorphic')\n        else:\n            print('=====> DEBUG: Graphs are NOT isomorphic')\n\n    else:\n        raise ValueError('Invalid value for mode, use mode=\\'write\\' or \\'compare\\'')\n\n    exit(0)", "response": "Compare two graphs with saved one which is loaded via networkx gpickle and return the new one which is loaded via networkx gpickle\n ArcGIS."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate cumulative peak generation of generators connected to underlying grids.", "response": "def peak_generation(self, mode):\n        \"\"\"Calculates cumulative peak generation of generators connected to underlying grids\n        \n        This is done instantaneously using bottom-up approach.\n\n        Parameters\n        ----------\n        mode: str\n            determines which generators are included::\n\n            'MV':   Only generation capacities of MV level are considered.\n            \n            'MVLV': Generation capacities of MV and LV are considered\n                    (= cumulative generation capacities in entire MVGD).\n\n        Returns\n        -------\n        float\n            Cumulative peak generation\n        \"\"\"\n\n        if mode == 'MV':\n            return sum([_.capacity for _ in self.grid.generators()])\n\n        elif mode == 'MVLV':\n            # calc MV geno capacities\n            cum_mv_peak_generation = sum([_.capacity for _ in self.grid.generators()])\n            # calc LV geno capacities\n            cum_lv_peak_generation = 0\n            for load_area in self.grid.grid_district.lv_load_areas():\n                cum_lv_peak_generation += load_area.peak_generation\n\n            return cum_mv_peak_generation + cum_lv_peak_generation\n\n        else:\n            raise ValueError('parameter \\'mode\\' is invalid!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting operation voltage level", "response": "def set_operation_voltage_level(self):\n        \"\"\"Set operation voltage level\n    \n        \"\"\"\n\n        mv_station_v_level_operation = float(cfg_ding0.get('mv_routing_tech_constraints',\n                                                           'mv_station_v_level_operation'))\n\n        self.v_level_operation = mv_station_v_level_operation * self.grid.v_level"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nselect appropriate transformers for the HV - MV substation.", "response": "def select_transformers(self):\n        \"\"\" Selects appropriate transformers for the HV-MV substation.\n\n        The transformers are chosen according to max. of load case and feedin-case\n        considering load factors.\n        The HV-MV transformer with the next higher available nominal apparent power is\n        chosen. If one trafo is not sufficient, multiple trafos are used. Additionally,\n        in a second step an redundant trafo is installed with max. capacity of the\n        selected trafos of the first step according to general planning principles for\n        MV distribution grids (n-1).\n\n        Parameters\n        ----------\n        transformers : dict\n            Contains technical information of p hv/mv transformers\n        **kwargs : dict\n            Should contain a value behind the key 'peak_load'\n\n\n        Notes\n        -----\n        Parametrization of transformers bases on [#]_.\n        \n        Potential hv-mv-transformers are chosen according to [#]_.\n        \n\n        References\n        ----------\n        .. [#] Deutsche Energie-Agentur GmbH (dena), \"dena-Verteilnetzstudie.\n            Ausbau- und Innovationsbedarf der Stromverteilnetze in Deutschland\n            bis 2030.\", 2012\n        .. [#] X. Tao, \"Automatisierte Grundsatzplanung von\n            Mittelspannungsnetzen\", Dissertation, 2006\n\n        \"\"\"\n\n        # get power factor for loads and generators\n        cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n        cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n\n        # get trafo load factors\n        load_factor_mv_trans_lc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_trans_lc_normal'))\n        load_factor_mv_trans_fc_normal = float(cfg_ding0.get('assumptions',\n                                                             'load_factor_mv_trans_fc_normal'))\n\n        # get equipment parameters of MV transformers\n        trafo_parameters = self.grid.network.static_data['MV_trafos']\n\n        # get peak load and peak generation\n        cum_peak_load = self.peak_load / cos_phi_load\n        cum_peak_generation = self.peak_generation(mode='MVLV') / cos_phi_feedin\n\n        # check if load or generation is greater respecting corresponding load factor\n        if (cum_peak_load / load_factor_mv_trans_lc_normal) > \\\n           (cum_peak_generation / load_factor_mv_trans_fc_normal):\n            # use peak load and load factor from load case\n            load_factor_mv_trans = load_factor_mv_trans_lc_normal\n            residual_apparent_power = cum_peak_load\n        else:\n            # use peak generation and load factor for feedin case\n            load_factor_mv_trans = load_factor_mv_trans_fc_normal\n            residual_apparent_power = cum_peak_generation\n\n        # determine number and size of required transformers\n\n        # get max. trafo\n        transformer_max = trafo_parameters.iloc[trafo_parameters['S_nom'].idxmax()]\n\n        while residual_apparent_power > 0:\n            if residual_apparent_power > load_factor_mv_trans * transformer_max['S_nom']:\n                transformer = transformer_max\n            else:\n                # choose trafo\n                transformer = trafo_parameters.iloc[\n                    trafo_parameters[trafo_parameters['S_nom'] * load_factor_mv_trans >\n                                     residual_apparent_power]['S_nom'].idxmin()]\n\n            # add transformer on determined size with according parameters\n            self.add_transformer(TransformerDing0(**{'grid': self.grid,\n                                                     'v_level': self.grid.v_level,\n                                                     's_max_longterm': transformer['S_nom']}))\n            # calc residual load\n            residual_apparent_power -= (load_factor_mv_trans *\n                                        transformer['S_nom'])\n\n        # if no transformer was selected (no load in grid district), use smallest one\n        if len(self._transformers) == 0:\n            transformer = trafo_parameters.iloc[trafo_parameters['S_nom'].idxmin()]\n\n            self.add_transformer(\n                TransformerDing0(grid=self.grid,\n                                 v_level=self.grid.v_level,\n                                 s_max_longterm=transformer['S_nom']))\n\n        # add redundant transformer of the size of the largest transformer\n        s_max_max = max((o.s_max_a for o in self._transformers))\n        self.add_transformer(TransformerDing0(**{'grid': self.grid,\n                                                 'v_level': self.grid.v_level,\n                                                 's_max_longterm': s_max_max}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pypsa_id(self):\n        #TODO: docstring\n        \"\"\" Description    \n        \"\"\"\n        return '_'.join(['MV', str(\n            self.grid.grid_district.lv_load_area.mv_grid_district.mv_grid.\\\n                id_db), 'tru', str(self.id_db)])", "response": "Return pypsa id for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates base path dir and subdirectories for raw and processed results", "response": "def create_results_dirs(base_path):\n    \"\"\"Create base path dir and subdirectories\n\n    Parameters\n    ----------\n    base_path : str\n        The base path has subdirectories for raw and processed results\n    \"\"\"\n\n    if not os.path.exists(base_path):\n        print(\"Creating directory {} for results data.\".format(base_path))\n        os.mkdir(base_path)\n    if not os.path.exists(os.path.join(base_path, 'results')):\n        os.mkdir(os.path.join(base_path, 'results'))\n    if not os.path.exists(os.path.join(base_path, 'plots')):\n        os.mkdir(os.path.join(base_path, 'plots'))\n    if not os.path.exists(os.path.join(base_path, 'info')):\n        os.mkdir(os.path.join(base_path, 'info'))\n    if not os.path.exists(os.path.join(base_path, 'log')):\n        os.mkdir(os.path.join(base_path, 'log'))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_multiple_grid_districts(mv_grid_districts, run_id, failsafe=False,\n                                base_path=None):\n    \"\"\"\n    Perform ding0 run on given grid districts\n\n    Parameters\n    ----------\n    mv_grid_districs : list\n        Integers describing grid districts\n    run_id: str\n        Identifier for a run of Ding0. For example it is used to create a\n        subdirectory of os.path.join(`base_path`, 'results')\n    failsafe : bool\n        Setting to True enables failsafe mode where corrupt grid districts\n        (mostly due to data issues) are reported and skipped. Report is to be\n         found in the log dir under :code:`~/.ding0` . Default is False.\n    base_path : str\n        Base path for ding0 data (input, results and logs).\n        Default is `None` which sets it to :code:`~/.ding0` (may deviate on\n        windows systems).\n        Specify your own but keep in mind that it a required a particular\n        structure of subdirectories.\n\n    Returns\n    -------\n    msg : str\n        Traceback of error computing corrupt MV grid district\n        .. TODO: this is only true if try-except environment is moved into this\n            fundion and traceback return is implemented\n\n    Notes\n    -----\n    Consider that a large amount of MV grid districts may take hours or up to\n    days to compute. A computational run for a single grid district may consume\n    around 30 secs.\n    \"\"\"\n    start = time.time()\n\n    # define base path\n    if base_path is None:\n        base_path = BASEPATH\n\n    # database connection/ session\n    engine = db.connection(section='oedb')\n    session = sessionmaker(bind=engine)()\n\n    corrupt_grid_districts = pd.DataFrame(columns=['id', 'message'])\n\n    for mvgd in mv_grid_districts:\n        # instantiate ding0  network object\n        nd = NetworkDing0(name='network', run_id=run_id)\n\n        if not os.path.exists(os.path.join(base_path, \"grids\")):\n            os.mkdir(os.path.join(base_path, \"grids\"))\n\n        if not failsafe:\n            # run DING0 on selected MV Grid District\n            msg = nd.run_ding0(session=session,\n                               mv_grid_districts_no=[mvgd])\n\n            # save results\n            results.save_nd_to_pickle(nd, os.path.join(base_path, \"grids\"))\n        else:\n            # try to perform ding0 run on grid district\n            try:\n                msg = nd.run_ding0(session=session,\n                                   mv_grid_districts_no=[mvgd])\n                # if not successful, put grid district to report\n                if msg:\n                    corrupt_grid_districts = corrupt_grid_districts.append(\n                        pd.Series({'id': mvgd,\n                                   'message': msg[0]}),\n                        ignore_index=True)\n                # if successful, save results\n                else:\n                    results.save_nd_to_pickle(nd, os.path.join(base_path,\n                                                               \"grids\"))\n            except Exception as e:\n                corrupt_grid_districts = corrupt_grid_districts.append(\n                    pd.Series({'id': mvgd,\n                               'message': e}),\n                    ignore_index=True)\n\n                continue\n\n        # Merge metadata of multiple runs\n        if 'metadata' not in locals():\n            metadata = nd.metadata\n\n        else:\n            if isinstance(mvgd, list):\n                metadata['mv_grid_districts'].extend(mvgd)\n            else:\n                metadata['mv_grid_districts'].append(mvgd)\n\n    # Save metadata to disk\n    with open(os.path.join(base_path, \"grids\", 'Ding0_{}.meta'.format(run_id)),\n              'w') as f:\n        json.dump(metadata, f)\n\n    # report on unsuccessful runs\n    corrupt_grid_districts.to_csv(\n        os.path.join(\n            base_path,\n            \"grids\",\n            'corrupt_mv_grid_districts.txt'),\n        index=False,\n        float_format='%.0f')\n\n    print('Elapsed time for', str(len(mv_grid_districts)),\n          'MV grid districts (seconds): {}'.format(time.time() - start))\n\n    return msg", "response": "This function runs ding0 on a list of MV grid districts and returns a single MV grid district."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_mv_topology(grid, subtitle='', filename=None, testcase='load',\n                     line_color=None, node_color='type',\n                     limits_cb_lines=None, limits_cb_nodes=None,\n                     background_map=True):\n    \"\"\" Draws MV grid graph using networkx\n\n    Parameters\n    ----------\n    grid : :obj:`MVGridDing0`\n        MV grid to plot.\n    subtitle : str\n        Extend plot's title by this string.\n    filename : str\n        If provided, the figure will be saved and not displayed (default path: ~/.ding0/).\n        A prefix is added to the file name.\n    testcase : str\n        Defines which case is to be used. Refer to config_calc.cfg to see further\n        assumptions for the cases. Possible options are:\n        \n        * 'load' (default)\n          Heavy-load-flow case\n        * 'feedin'\n          Feedin-case\n    line_color : str\n        Defines whereby to choose line colors. Possible options are:\n\n        * 'loading'\n          Line color is set according to loading of the line in heavy load case.\n          You can use parameter `limits_cb_lines` to adjust the color range.\n        * None (default)\n          Lines are plotted in black. Is also the fallback option in case of\n          wrong input.\n    node_color : str\n        Defines whereby to choose node colors. Possible options are:\n\n        * 'type' (default)\n          Node color as well as size is set according to type of node\n          (generator, MV station, etc.). Is also the fallback option in case of\n          wrong input.\n        * 'voltage'\n          Node color is set according to voltage deviation from 1 p.u..\n          You can use parameter `limits_cb_nodes` to adjust the color range.\n    limits_cb_lines : :obj:`tuple`\n        Tuple with limits for colorbar of line color. First entry is the\n        minimum and second entry the maximum value. E.g. pass (0, 1) to\n        adjust the colorbar to 0..100% loading.\n        Default: None (min and max loading are used).\n    limits_cb_nodes : :obj:`tuple`\n        Tuple with limits for colorbar of nodes. First entry is the\n        minimum and second entry the maximum value. E.g. pass (0.9, 1) to\n        adjust the colorbar to 90%..100% voltage.\n        Default: None (min and max loading are used).\n    background_map : bool, optional\n        If True, a background map is plotted (default: stamen toner light).\n        The additional package `contextily` is needed for this functionality.\n        Default: True\n\n    Notes\n    -----\n    WGS84 pseudo mercator (epsg:3857) is used as coordinate reference system (CRS).\n    Therefore, the drawn graph representation may be falsified!\n    \"\"\"\n\n    def set_nodes_style_and_position(nodes):\n\n        # TODO: MOVE settings to config\n        # node types (name of classes)\n        node_types = ['MVStationDing0',\n                      'LVStationDing0',\n                      'LVLoadAreaCentreDing0',\n                      'MVCableDistributorDing0',\n                      'GeneratorDing0',\n                      'GeneratorFluctuatingDing0',\n                      'CircuitBreakerDing0',\n                      'n/a']\n        \n        # node styles\n        colors_dict = {'MVStationDing0': '#f2ae00',\n                       'LVStationDing0': 'grey',\n                       'LVLoadAreaCentreDing0': '#fffc3d',\n                       'MVCableDistributorDing0': '#000000',\n                       'GeneratorDing0': '#00b023',\n                       'GeneratorFluctuatingDing0': '#0078b0',\n                       'CircuitBreakerDing0': '#c20000',\n                       'n/a': 'orange'}\n        sizes_dict = {'MVStationDing0': 120,\n                      'LVStationDing0': 7,\n                      'LVLoadAreaCentreDing0': 30,\n                      'MVCableDistributorDing0': 5,\n                      'GeneratorDing0': 50,\n                      'GeneratorFluctuatingDing0': 50,\n                      'CircuitBreakerDing0': 50,\n                      'n/a': 5}\n        zindex_by_type = {'MVStationDing0': 16,\n                          'LVStationDing0': 12,\n                          'LVLoadAreaCentreDing0': 11,\n                          'MVCableDistributorDing0': 13,\n                          'GeneratorDing0': 14,\n                          'GeneratorFluctuatingDing0': 14,\n                          'CircuitBreakerDing0': 15,\n                          'n/a': 10}\n\n        # dict of node class names: list of nodes\n        nodes_by_type = {_: [] for _ in node_types}\n        # dict of node class names: list of node-individual color\n        node_colors_by_type = {_: [] for _ in node_types}\n        # dict of node class names: list of node-individual size\n        node_sizes_by_type = {_: [] for _ in node_types}\n        node_sizes_by_type['all'] = []\n        # dict of nodes:node-individual positions\n        nodes_pos = {}\n\n        for n in nodes:\n            if type(n).__name__ in node_types:\n                nodes_by_type[type(n).__name__].append(n)\n                node_colors_by_type[type(n).__name__].append(colors_dict[type(n).__name__])\n                node_sizes_by_type[type(n).__name__].append(sizes_dict[type(n).__name__])\n                node_sizes_by_type['all'].append(sizes_dict[type(n).__name__])\n            else:\n                nodes_by_type['n/a'].append(n)\n                node_colors_by_type['n/a'].append(colors_dict['n/a'])\n                node_sizes_by_type['n/a'].append(sizes_dict['n/a'])\n                node_sizes_by_type['all'].append(sizes_dict['n/a'])\n            nodes_pos[n] = (n.geo_data.x, n.geo_data.y)\n\n        return node_types, nodes_by_type, node_colors_by_type,\\\n               node_sizes_by_type, zindex_by_type, nodes_pos\n\n    def reproject_nodes(nodes_pos, model_proj='4326'):\n        inProj = Proj(init='epsg:{srid}'.format(srid=model_proj))\n        outProj = Proj(init='epsg:3857')\n        nodes_pos2 = {}\n        for k, v in nodes_pos.items():\n            x2, y2 = transform(inProj, outProj,\n                               v[0],\n                               v[1])\n            nodes_pos2[k] = (x2, y2)\n        return nodes_pos2\n\n    def plot_background_map(ax):\n        url = ctx.sources.ST_TONER_LITE\n        xmin, xmax, ymin, ymax = ax.axis()\n        basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax,\n                                         zoom=12, url=url)\n        ax.imshow(basemap, extent=extent, interpolation='bilinear', zorder=0)\n        ax.axis((xmin, xmax, ymin, ymax))\n\n    def plot_region_data(ax):\n        # get geoms of MV grid district, load areas and LV grid districts\n        mv_grid_district = gpd.GeoDataFrame({'geometry': grid.grid_district.geo_data},\n                                            crs={'init': 'epsg:{srid}'.format(srid=model_proj)})\n        load_areas = gpd.GeoDataFrame({'geometry': [la.geo_area for la in grid.grid_district.lv_load_areas()]},\n                                      crs={'init': 'epsg:{srid}'.format(srid=model_proj)})\n        lv_grid_districts = gpd.GeoDataFrame({'geometry': [lvgd.geo_data\n                                                           for la in grid.grid_district.lv_load_areas()\n                                                           for lvgd in la.lv_grid_districts()]},\n                                             crs={'init': 'epsg:{srid}'.format(srid=model_proj)})\n\n        # reproject to WGS84 pseudo mercator\n        mv_grid_district = mv_grid_district.to_crs(epsg=3857)\n        load_areas = load_areas.to_crs(epsg=3857)\n        lv_grid_districts = lv_grid_districts.to_crs(epsg=3857)\n\n        # plot\n        mv_grid_district.plot(ax=ax, color='#ffffff', alpha=0.2, edgecolor='k', linewidth=2, zorder=2)\n        load_areas.plot(ax=ax, color='#fffea3', alpha=0.1, edgecolor='k', linewidth=0.5, zorder=3)\n        lv_grid_districts.plot(ax=ax, color='#ffffff', alpha=0.05, edgecolor='k', linewidth=0.5, zorder=4)\n\n    if not isinstance(grid, MVGridDing0):\n        logger.warning('Sorry, but plotting is currently only available for MV grids but you did not pass an'\n                       'instance of MVGridDing0. Plotting is skipped.')\n        return\n\n    g = grid._graph\n    model_proj = grid.network.config['geo']['srid']\n\n    if testcase == 'feedin':\n        case_idx = 1\n    else:\n        case_idx = 0\n\n    nodes_types, nodes_by_type, node_colors_by_type, node_sizes_by_type, zindex_by_type, nodes_pos =\\\n        set_nodes_style_and_position(g.nodes())\n\n    # reproject to WGS84 pseudo mercator\n    nodes_pos = reproject_nodes(nodes_pos, model_proj=model_proj)\n\n    plt.figure(figsize=(9, 6))\n    ax = plt.gca()\n\n    if line_color == 'loading':\n        edges_color = []\n        for n1, n2 in g.edges():\n            edge = g.adj[n1][n2]\n            if hasattr(edge['branch'], 's_res'):\n                edges_color.append(edge['branch'].s_res[case_idx] * 1e3 /\n                                   (3 ** 0.5 * edge['branch'].type['U_n'] * edge['branch'].type['I_max_th']))\n            else:\n                edges_color.append(0)\n        edges_cmap = plt.get_cmap('jet')\n        #edges_cmap.set_over('#952eff')\n    else:\n        edges_color = ['black'] * len(list(grid.graph_edges()))\n        edges_cmap = None\n\n    # plot nodes by voltage\n    if node_color == 'voltage':\n        voltage_station = grid._station.voltage_res[case_idx]\n        nodes_color = []\n        for n in g.nodes():\n            if hasattr(n, 'voltage_res'):\n                nodes_color.append(n.voltage_res[case_idx])\n            else:\n                nodes_color.append(voltage_station)\n\n        if testcase == 'feedin':\n            nodes_cmap = plt.get_cmap('Reds')\n            nodes_vmax = voltage_station + float(grid.network.config\n                                                 ['mv_routing_tech_constraints']\n                                                 ['mv_max_v_level_fc_diff_normal'])\n            nodes_vmin = voltage_station\n        else:\n            nodes_cmap = plt.get_cmap('Reds_r')\n            nodes_vmin = voltage_station - float(grid.network.config\n                                                 ['mv_routing_tech_constraints']\n                                                 ['mv_max_v_level_lc_diff_normal'])\n            nodes_vmax = voltage_station\n\n        nodes = nx.draw_networkx_nodes(g,\n                                       pos=nodes_pos,\n                                       node_color=nodes_color,\n                                       # node_shape='o', # TODO: Add additional symbols here\n                                       cmap=nodes_cmap,\n                                       vmin=nodes_vmin,\n                                       vmax=nodes_vmax,\n                                       node_size=node_sizes_by_type['all'],\n                                       linewidths=0.25,\n                                       ax=ax)\n        nodes.set_zorder(10)\n        nodes.set_edgecolor('k')\n\n        # colorbar nodes\n        if limits_cb_nodes is None:\n            limits_cb_nodes = (math.floor(min(nodes_color)*100)/100,\n                               math.ceil(max(nodes_color)*100)/100)\n        v_range = np.linspace(limits_cb_nodes[0], limits_cb_nodes[1], 101)\n        cb_voltage = plt.colorbar(nodes, boundaries=v_range,\n                                  ticks=v_range[0:101:10],\n                                  fraction=0.04, pad=0.1)\n        cb_voltage.set_clim(vmin=limits_cb_nodes[0],\n                            vmax=limits_cb_nodes[1])\n        cb_voltage.set_label('Node voltage deviation in %', size='smaller')\n        cb_voltage.ax.tick_params(labelsize='smaller')\n\n    # plot nodes by type\n    else:\n        for node_type in nodes_types:\n            if len(nodes_by_type[node_type]) != 0:\n                nodes = nx.draw_networkx_nodes(g,\n                                               nodelist=nodes_by_type[node_type],\n                                               pos=nodes_pos,\n                                               # node_shape='o', # TODO: Add additional symbols here\n                                               node_color=node_colors_by_type[node_type],\n                                               cmap=None,\n                                               vmin=None,\n                                               vmax=None,\n                                               node_size=node_sizes_by_type[node_type],\n                                               linewidths=0.25,\n                                               label=node_type,\n                                               ax=ax)\n                nodes.set_zorder(zindex_by_type[node_type])\n                nodes.set_edgecolor('k')\n\n    edges = nx.draw_networkx_edges(g,\n                                   pos=nodes_pos,\n                                   edge_color=edges_color,\n                                   edge_cmap=edges_cmap,\n                                   edge_vmin=0,\n                                   edge_vmax=1,\n                                   #width=1,\n                                   ax=ax)\n    edges.set_zorder(5)\n\n    if line_color == 'loading':\n        # colorbar edges\n        if limits_cb_lines is None:\n            limits_cb_lines = (math.floor(min(edges_color)*100)/100,\n                               math.ceil(max(edges_color)*100)/100)\n        loading_range = np.linspace(limits_cb_lines[0], limits_cb_lines[1], 101)\n        cb_loading = plt.colorbar(edges, boundaries=loading_range,\n                                  ticks=loading_range[0:101:10],\n                                  fraction=0.04, pad=0.04)\n        cb_loading.set_clim(vmin=limits_cb_lines[0],\n                            vmax=limits_cb_lines[1])\n        cb_loading.set_label('Line loading in % of nominal capacity', size='smaller')\n        cb_loading.ax.tick_params(labelsize='smaller')\n\n    if use_ctx and background_map:\n        plot_background_map(ax=ax)\n    if use_gpd:\n        plot_region_data(ax=ax)\n\n    plt.legend(fontsize=7)\n    plt.title('MV Grid District {id} - {st}'.format(id=grid.id_db,\n                                                    st=subtitle))\n\n    # hide axes labels (coords)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    if filename is None:\n        plt.tight_layout()\n        plt.show()\n    else:\n        path = os.path.join(get_default_home_dir(), 'ding0_grid_{id}_{filename}'.format(id=str(grid.id_db),\n                                                                                        filename=filename))\n        plt.savefig(path, dpi=300, bbox_inches='tight')\n        plt.close()\n        logger.info('==> Figure saved to {path}'.format(path=path))", "response": "Plots MV grid graph using networkx"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads config file specified by filename.", "response": "def load_config(filename):\n    \"\"\" Read config file specified by `filename`\n    \n    Parameters\n    ----------\n    filename : str\n        Description of filename\n    \"\"\"\n    package_path = ding0.__path__[0]\n    FILE = path.join(package_path, 'config', filename)\n\n    try:\n        cfg.read(FILE)\n        global _loaded\n        _loaded = True\n    except:\n        logger.exception(\"configfile not found.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(section, key, value):\n\n    if not _loaded:\n        init()\n\n    if not cfg.has_section(section):\n        cfg.add_section(section)\n\n    cfg.set(section, key, value)\n\n    with open(FILE, 'w') as configfile:\n        cfg.write(configfile)", "response": "Sets a value to a section key - pair."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_depot_section(f):\n    depots = []\n\n    for line in f:\n        line = strip(line)\n        if line == '-1' or line == 'EOF': # End of section\n            break\n        else:\n            depots.append(line)\n\n    if len(depots) != 1:\n        raise ParseException('One and only one depot is supported')\n\n    return int(depots[0])", "response": "Parse TSPLIB DEPOT_SECTION data part from file descriptor f\n    \n    Returns a list of depots"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses TSPLIB NODE_COORD_SECTION or DEMAND_SECTION from file descript f Returns a dict containing the node as key", "response": "def _parse_nodes_section(f, current_section, nodes):\n    \"\"\"Parse TSPLIB NODE_COORD_SECTION or DEMAND_SECTION from file descript f\n\n    Returns a dict containing the node as key\n    \"\"\"\n    section = {}\n    dimensions = None\n\n    if current_section == 'NODE_COORD_SECTION':\n        dimensions = 3 # i: (i, j)\n    elif current_section == 'DEMAND_SECTION':\n        dimensions = 2 # i: q\n    else:\n        raise ParseException('Invalid section {}'.format(current_section))\n\n    n = 0\n    for line in f:\n        line = strip(line)\n\n        # Check dimensions\n        definitions = re.split(r'\\s*', line)\n        if len(definitions) != dimensions:\n            raise ParseException('Invalid dimensions from section {}. Expected: {}'.format(current_section, dimensions))\n\n        node = int(definitions[0])\n        values = [int(v) for v in definitions[1:]]\n\n        if len(values) == 1:\n            values = values[0]\n\n        section[node] = values\n\n        n = n + 1\n        if n == nodes:\n            break\n\n    # Assert all nodes were read\n    if n != nodes:\n        raise ParseException('Missing {} nodes definition from section {}'.format(nodes - n, current_section))\n\n    return section"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses TSPLIB EDGE_WEIGHT_SECTION from file f", "response": "def _parse_edge_weight(f, nodes):\n    \"\"\"Parse TSPLIB EDGE_WEIGHT_SECTION from file f\n\n    Supports only FULL_MATRIX for now\n    \"\"\"\n    matrix = []\n\n    n = 0\n\n    for line in f:\n        line = strip(line)\n\n        regex = re.compile(r'\\s+')\n\n        row = regex.split(line)\n\n        matrix.append(row)\n\n        n = n + 1\n\n        if n == nodes:\n            break\n\n    if n != nodes:\n        raise ParseException('Missing {} nodes definition from section EDGE_WEIGHT_SECTION'.format(nodes - n))\n\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the Eclidian distance between two points a and b.", "response": "def calculate_euc_distance(a, b):\n    \"\"\"Calculates Eclidian distances from two points a and b\n    \n    Args\n    ----\n    a : (:obj:`float`, :obj:`float`)\n        Two-dimension tuple (x1,y1)\n    b : (:obj:`float`, :obj:`float`)\n        Two-dimension tuple (x2,y2)\n\n    Returns\n    -------\n    float\n        the distance.\n    \"\"\"\n    x1, y1 = a\n    x2, y2 = b\n\n    return int(round(math.sqrt(((x1 - x2) ** 2) + (((y1 - y2) ** 2)))))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npost - process specs after pure parsing", "response": "def _post_process_specs(specs):\n    \"\"\"Post-process specs after pure parsing\n\n    Casts any number expected values into integers\n    \n    Args\n    ----\n    specs :\n    \n    \n    Notes\n    -----\n    Modifies the specs object\n    \"\"\"\n    integer_specs = ['DIMENSION', 'CAPACITY']\n\n    for s in integer_specs:\n        specs[s] = int(specs[s])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_node_matrix_from_coord_section(specs):\n    distances = specs['NODE_COORD_SECTION']\n\n    specs['MATRIX'] = {}\n\n    for i in distances:\n        origin = tuple(distances[i])\n\n        specs['MATRIX'][i] = {}\n\n        for j in specs['NODE_COORD_SECTION']:\n            destination = tuple(distances[j])\n\n            distance = calculate_euc_distance(origin, destination)\n\n            #\n            # Upper triangular matrix\n            # if i > j, ij = 0\n            #\n            #if i > j:\n            #    continue\n\n            specs['MATRIX'][i][j] = distance", "response": "Transformed parsed data from NODE_COORD_SECTION into an upper triangular matrix where each element in the matrix is a tuple of the origin and destination nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform parsed data from EDGE_WEIGHT_SECTION into an upper triangular matrix 'MATRIX' key added to specs", "response": "def _create_node_matrix_from_full_matrix(specs):\n    \"\"\"Transform parsed data from EDGE_WEIGHT_SECTION into an upper triangular matrix\n\n    'MATRIX' key added to `specs`\n    \"\"\"\n    old_matrix = specs['EDGE_WEIGHT_SECTION']\n    nodes = specs['DIMENSION']\n\n    specs['MATRIX'] = {}\n\n    for i in range(nodes):\n        specs['MATRIX'][i + 1] = {}\n\n        for j in range(nodes):\n            if i > j:\n                continue\n\n            specs['MATRIX'][i + 1][j + 1] = int(old_matrix[i][j])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_tsplib(f):\n    line = ''\n\n    specs = {}\n\n    used_specs = ['NAME', 'COMMENT', 'DIMENSION', 'CAPACITY', 'TYPE', 'EDGE_WEIGHT_TYPE']\n    used_data = ['DEMAND_SECTION', 'DEPOT_SECTION']\n\n    # Parse specs part\n    for line in f:\n        line = strip(line)\n\n        # Arbitrary sort, so we test everything out\n        s = None\n        for s in used_specs:\n            if line.startswith(s):\n                specs[s] = line.split('{} :'.format(s))[-1].strip() # get value data part\n                break\n\n        if s == 'EDGE_WEIGHT_TYPE' and s in specs and specs[s] == 'EXPLICIT':\n            used_specs.append('EDGE_WEIGHT_FORMAT')\n\n        # All specs read\n        if len(specs) == len(used_specs):\n            break\n\n    if len(specs) != len(used_specs):\n        missing_specs = set(used_specs).symmetric_difference(set(specs))\n        raise ParseException('Error parsing TSPLIB data: specs {} missing'.format(missing_specs))\n\n    print(specs)\n\n    if specs['EDGE_WEIGHT_TYPE'] == 'EUC_2D':\n        used_data.append('NODE_COORD_SECTION')\n    elif specs['EDGE_WEIGHT_FORMAT'] == 'FULL_MATRIX':\n        used_data.append('EDGE_WEIGHT_SECTION')\n    else:\n        raise ParseException('EDGE_WEIGHT_TYPE or EDGE_WEIGHT_FORMAT not supported')\n\n    _post_process_specs(specs)\n\n    # Parse data part\n    for line in f:\n        line = strip(line)\n\n        for d in used_data:\n            if line.startswith(d):\n                if d == 'DEPOT_SECTION':\n                    specs[d] = _parse_depot_section(f)\n                elif d in ['NODE_COORD_SECTION', 'DEMAND_SECTION']:\n                    specs[d] = _parse_nodes_section(f, d, specs['DIMENSION'])\n                elif d == 'EDGE_WEIGHT_SECTION':\n                    specs[d] = _parse_edge_weight(f, specs['DIMENSION'])\n\n        if len(specs) == len(used_specs) + len(used_data):\n            break\n\n    if len(specs) != len(used_specs) + len(used_data):\n        missing_specs = set(specs).symmetric_difference(set(used_specs).union(set(used_data)))\n        raise ParseException('Error parsing TSPLIB data: specs {} missing'.format(missing_specs))\n\n    _post_process_data(specs)\n\n    return specs", "response": "Parses a TSPLIB file descriptor and returns a dict containing the problem definition"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_file(filename):\n    sanitized_filename = sanitize(filename)\n\n    f = open(sanitized_filename)\n\n    specs = None\n\n    try:\n        specs = _parse_tsplib(f)\n    except ParseException:\n        raise\n    finally: # 'finally' is executed even when we re-raise exceptions\n        f.close()\n\n    if specs['TYPE'] != 'CVRP':\n        raise Exception('Not a CVRP TSPLIB problem. Found: {}'.format(specs['TYPE']))\n    \n    #additional params for graph/network (temporary)\n    specs['VOLTAGE'] = 20000\n    specs['CABLETYPE'] = 1\n\n    #return (Graph(specs), specs)\n    return Graph(specs)", "response": "Reads a TSPLIB file and returns the problem data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_circuit_breaker(self, circ_breaker):\n        if circ_breaker not in self._circuit_breakers and isinstance(circ_breaker, CircuitBreakerDing0):\n            self._circuit_breakers.append(circ_breaker)\n            self.graph_add_node(circ_breaker)", "response": "Adds a circuit breaker to the internal list of circuit breakers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd MV station to the MV object.", "response": "def add_station(self, mv_station, force=False):\n        \"\"\"Adds MV station if not already existing\n\n        Args\n        ----\n        mv_station: MVStationDing0\n            Description #TODO\n        force: bool\n            If True, MV Station is set even though it's not empty (override)\n        \"\"\"\n        if not isinstance(mv_station, MVStationDing0):\n            raise Exception('Given MV station is not a MVStationDing0 object.')\n        if self._station is None:\n            self._station = mv_station\n            self.graph_add_node(mv_station)\n        else:\n            if force:\n                self._station = mv_station\n            else:\n                raise Exception('MV Station already set, use argument `force=True` to override.')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a MV load to _loads and grid graph if not already existing", "response": "def add_load(self, lv_load):\n        \"\"\"Adds a MV load to _loads and grid graph if not already existing\n        \n        Args\n        ----\n        lv_load : float\n            Desription #TODO\n        \"\"\"\n        if lv_load not in self._loads and isinstance(lv_load,\n                                                     MVLoadDing0):\n            self._loads.append(lv_load)\n            self.graph_add_node(lv_load)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_cable_distributor(self, cable_dist):\n        if cable_dist not in self.cable_distributors() and isinstance(cable_dist,\n                                                                      MVCableDistributorDing0):\n            # add to array and graph\n            self._cable_distributors.append(cable_dist)\n            self.graph_add_node(cable_dist)", "response": "Adds a cable distributor to the internal list of _cable_distributors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves a cable distributor from _cable_distributors and _graph.", "response": "def remove_cable_distributor(self, cable_dist):\n        \"\"\"Removes a cable distributor from _cable_distributors if existing\"\"\"\n        if cable_dist in self.cable_distributors() and isinstance(cable_dist,\n                                                                  MVCableDistributorDing0):\n            # remove from array and graph\n            self._cable_distributors.remove(cable_dist)\n            if self._graph.has_node(cable_dist):\n                self._graph.remove_node(cable_dist)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a ring to the list of rings", "response": "def add_ring(self, ring):\n        \"\"\"Adds a ring to _rings if not already existing\"\"\"\n        if ring not in self._rings and isinstance(ring, RingDing0):\n            self._rings.append(ring)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a generator for iterating over rings and satellite nodes.", "response": "def rings_nodes(self, include_root_node=False, include_satellites=False):\n        \"\"\" Returns a generator for iterating over rings (=routes of MVGrid's graph)\n\n        Args\n        ----\n        include_root_node: bool, defaults to False\n            If True, the root node is included in the list of ring nodes.\n        include_satellites: bool, defaults to False\n            If True, the satellite nodes (nodes that diverge from ring nodes) is included in the list of ring nodes.\n            \n        Yields\n        ------\n        :any:`list` of :obj:`GridDing0`\n            List with nodes of each ring of _graph in- or excluding root node (HV/MV station) (arg `include_root_node`),\n            format::\n             \n            [ ring_m_node_1, ..., ring_m_node_n ]\n            \n        Notes\n        -----\n            Circuit breakers must be closed to find rings, this is done automatically.\n        \"\"\"\n        for circ_breaker in self.circuit_breakers():\n            if circ_breaker.status is 'open':\n                circ_breaker.close()\n                logger.info('Circuit breakers were closed in order to find MV '\n                            'rings')\n\n        for ring in nx.cycle_basis(self._graph, root=self._station):\n            if not include_root_node:\n                ring.remove(self._station)\n\n            if include_satellites:\n                ring_nodes = ring\n                satellites = []\n                for ring_node in ring:\n                    # determine all branches diverging from each ring node\n                    satellites.append(\n                        self.graph_nodes_from_subtree(\n                            ring_node,\n                            include_root_node=include_root_node\n                        )\n                    )\n                # return ring and satellite nodes (flatted list of lists)\n                yield ring + [_ for sublist in satellites for _ in sublist]\n            else:\n                yield ring"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a generator for iterating over each ring and edges and nodes in the ring.", "response": "def rings_full_data(self):\n        \"\"\" Returns a generator for iterating over each ring\n\n        Yields\n        ------\n            For each ring, tuple composed by ring ID, list of edges, list of nodes\n        Notes\n        -----\n            Circuit breakers must be closed to find rings, this is done automatically.\n        \"\"\"\n        #close circuit breakers\n        for circ_breaker in self.circuit_breakers():\n            if not circ_breaker.status == 'closed':\n                circ_breaker.close()\n                logger.info('Circuit breakers were closed in order to find MV '\n                            'rings')\n        #find True rings (cycles from station through breaker and back to station)\n        for ring_nodes in nx.cycle_basis(self._graph, root=self._station):\n            edges_ring = []\n            for node in ring_nodes:\n                for edge in self.graph_branches_from_node(node):\n                    nodes_in_the_branch = self.graph_nodes_from_branch(edge[1]['branch'])\n                    if (nodes_in_the_branch[0] in ring_nodes and\n                        nodes_in_the_branch[1] in ring_nodes\n                        ):\n                        if not edge[1]['branch'] in edges_ring:\n                            edges_ring.append(edge[1]['branch'])\n            yield (edges_ring[0].ring,edges_ring,ring_nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef graph_nodes_from_subtree(self, node_source, include_root_node=False):\n        if node_source in self._graph.nodes():\n\n            # get all nodes that are member of a ring\n            node_ring = []\n            for ring in self.rings_nodes(include_root_node=include_root_node):\n                if node_source in ring:\n                    node_ring = ring\n                    break\n\n            # result set\n            nodes_subtree = set()\n\n            # get nodes from subtree\n            if node_source in node_ring:\n                for path in nx.shortest_path(self._graph, node_source).values():\n                    if len(path)>1:\n                        if (path[1] not in node_ring) and (path[1] is not self.station()):\n                            nodes_subtree.update(path[1:len(path)])\n            else:\n                raise ValueError(node_source, 'is not member of ring.')\n\n        else:\n            raise ValueError(node_source, 'is not member of graph.')\n\n        return list(nodes_subtree)", "response": "Returns a list of nodes that are connected to node_source and are not part of the \n        ring of node_source."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_branch_ids(self):\n\n        # MV grid:\n        ctr = 1\n        for branch in self.graph_edges():\n            branch['branch'].id_db = self.grid_district.id_db * 10**4 + ctr\n            ctr += 1\n\n        # LV grid:\n        for lv_load_area in self.grid_district.lv_load_areas():\n            for lv_grid_district in lv_load_area.lv_grid_districts():\n                ctr = 1\n                for branch in lv_grid_district.lv_grid.graph_edges():\n                    branch['branch'].id_db = lv_grid_district.id_db * 10**7 + ctr\n                    ctr += 1", "response": "Generates and sets ids of branches for MV and LV grids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the routing on load area centres and creates MV grid with the grid topology.", "response": "def routing(self, debug=False, anim=None):\n        \"\"\" Performs routing on Load Area centres to build MV grid with ring topology.\n\n        Args\n        ----\n        debug: bool, defaults to False\n            If True, information is printed while routing\n        anim: type, defaults to None\n            Descr #TODO\n        \"\"\"\n\n        # do the routing\n        self._graph = mv_routing.solve(graph=self._graph,\n                                       debug=debug,\n                                       anim=anim)\n        logger.info('==> MV Routing for {} done'.format(repr(self)))\n\n        # connect satellites (step 1, with restrictions like max. string length, max peak load per string)\n        self._graph = mv_connect.mv_connect_satellites(mv_grid=self,\n                                                       graph=self._graph,\n                                                       mode='normal',\n                                                       debug=debug)\n        logger.info('==> MV Sat1 for {} done'.format(repr(self)))\n\n        # connect satellites to closest line/station on a MV ring that have not been connected in step 1\n        self._graph = mv_connect.mv_connect_satellites(mv_grid=self,\n                                                       graph=self._graph,\n                                                       mode='isolated',\n                                                       debug=debug)\n        logger.info('==> MV Sat2 for {} done'.format(repr(self)))\n\n        # connect stations\n        self._graph = mv_connect.mv_connect_stations(mv_grid_district=self.grid_district,\n                                                     graph=self._graph,\n                                                     debug=debug)\n        logger.info('==> MV Stations for {} done'.format(repr(self)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connect_generators(self, debug=False):\n\n        self._graph = mv_connect.mv_connect_generators(self.grid_district, self._graph, debug)", "response": "Connects MV generators to grid"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms the parametrization of the grid equipment for this MV station and the current grid.", "response": "def parametrize_grid(self, debug=False):\n        \"\"\" Performs Parametrization of grid equipment:\n        \n            i) Sets voltage level of MV grid, \n            ii) Operation voltage level and transformer of HV/MV station, \n            iii) Default branch types (normal, aggregated, settlement)\n\n        Args\n        ----\n        debug: bool, defaults to False\n            If True, information is printed during process.\n            \n        Notes\n        -----\n        It is assumed that only cables are used within settlements.\n        \"\"\"\n        # TODO: Add more detailed description\n\n        # set grid's voltage level\n        self.set_voltage_level()\n\n        # set MV station's voltage level\n        self._station.set_operation_voltage_level()\n\n        # set default branch types (normal, aggregated areas and within settlements)\n        self.default_branch_type,\\\n        self.default_branch_type_aggregated,\\\n        self.default_branch_type_settle = self.set_default_branch_type(debug)\n\n        # set default branch kinds\n        self.default_branch_kind_aggregated = self.default_branch_kind\n        self.default_branch_kind_settle = 'cable'\n\n        # choose appropriate transformers for each HV/MV sub-station\n        self._station.select_transformers()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_voltage_level(self, mode='distance'):\n\n        if mode == 'load_density':\n\n            # get power factor for loads\n            cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n\n            # get load density\n            load_density_threshold = float(cfg_ding0.get('assumptions',\n                                                         'load_density_threshold'))\n\n            # transform MVGD's area to epsg 3035\n            # to achieve correct area calculation\n            projection = partial(\n                pyproj.transform,\n                pyproj.Proj(init='epsg:4326'),  # source coordinate system\n                pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n\n            # calculate load density\n            kw2mw = 1e-3\n            sqm2sqkm = 1e6\n            load_density = ((self.grid_district.peak_load * kw2mw / cos_phi_load) /\n                            (transform(projection, self.grid_district.geo_data).area / sqm2sqkm)) # unit MVA/km^2\n\n            # identify voltage level\n            if load_density < load_density_threshold:\n                self.v_level = 20\n            elif load_density >= load_density_threshold:\n                self.v_level = 10\n            else:\n                raise ValueError('load_density is invalid!')\n\n        elif mode == 'distance':\n\n            # get threshold for 20/10kV disambiguation\n            voltage_per_km_threshold = float(cfg_ding0.get('assumptions',\n                                                           'voltage_per_km_threshold'))\n\n            # initial distance\n            dist_max = 0\n            import time\n            start = time.time()\n            for node in self.graph_nodes_sorted():\n                if isinstance(node, LVLoadAreaCentreDing0):\n                    # calc distance from MV-LV station to LA centre\n                    dist_node = calc_geo_dist_vincenty(self.station(), node) / 1e3\n                    if dist_node > dist_max:\n                        dist_max = dist_node\n\n            # max. occurring distance to a Load Area exceeds threshold => grid operates at 20kV\n            if dist_max >= voltage_per_km_threshold:\n                self.v_level = 20\n            # not: grid operates at 10kV\n            else:\n                self.v_level = 10\n\n        else:\n            raise ValueError('parameter \\'mode\\' is invalid!')", "response": "Sets voltage level of MV grid according to load density and distance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the default branch type according to grid district s peak load and standard equipment.", "response": "def set_default_branch_type(self, debug=False):\n        \"\"\" Determines default branch type according to grid district's peak load and standard equipment.\n\n        Args\n        ----\n        debug: bool, defaults to False\n            If True, information is printed during process\n\n        Returns\n        -------\n        :pandas:`pandas.Series<series>`   \n            default branch type: pandas Series object. If no appropriate type is found, return largest possible one.\n        :pandas:`pandas.Series<series>`    \n            default branch type max: pandas Series object. Largest available line/cable type\n\n        Notes\n        -----\n        Parameter values for cables and lines are taken from [#]_, [#]_ and [#]_.\n\n        Lines are chosen to have 60 % load relative to their nominal capacity according to [#]_.\n\n        Decision on usage of overhead lines vs. cables is determined by load density of the considered region. Urban\n        areas usually are equipped with underground cables whereas rural areas often have overhead lines as MV\n        distribution system [#]_.\n\n        References\n        ----------\n        .. [#] Klaus Heuck et al., \"Elektrische Energieversorgung\", Vieweg+Teubner, Wiesbaden, 2007\n        .. [#] Ren\u00e9 Flosdorff et al., \"Elektrische Energieverteilung\", Vieweg+Teubner, 2005\n        .. [#] S\u00fcdkabel GmbH, \"Einadrige VPE-isolierte Mittelspannungskabel\",\n            http://www.suedkabel.de/cms/upload/pdf/Garnituren/Einadrige_VPE-isolierte_Mittelspannungskabel.pdf, 2017\n        .. [#] Deutsche Energie-Agentur GmbH (dena), \"dena-Verteilnetzstudie. Ausbau- und Innovationsbedarf der\n            Stromverteilnetze in Deutschland bis 2030.\", 2012\n        .. [#] Tao, X., \"Automatisierte Grundsatzplanung von\n            Mittelspannungsnetzen\", Dissertation, RWTH Aachen, 2007\n        \"\"\"\n\n        # decide whether cable or line is used (initially for entire grid) and set grid's attribute\n        if self.v_level == 20:\n            self.default_branch_kind = 'line'\n        elif self.v_level == 10:\n            self.default_branch_kind = 'cable'\n\n        # get power factor for loads\n        cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n\n        # get max. count of half rings per MV grid district\n        mv_half_ring_count_max = int(cfg_ding0.get('mv_routing_tech_constraints',\n                                                   'mv_half_ring_count_max'))\n        #mv_half_ring_count_max=20\n\n        # load cable/line assumptions, file_names and parameter\n        if self.default_branch_kind == 'line':\n            load_factor_normal = float(cfg_ding0.get('assumptions',\n                                                     'load_factor_mv_line_lc_normal'))\n            branch_parameters = self.network.static_data['MV_overhead_lines']\n\n            # load cables as well to use it within settlements\n            branch_parameters_settle = self.network.static_data['MV_cables']\n            # select types with appropriate voltage level\n            branch_parameters_settle = branch_parameters_settle[branch_parameters_settle['U_n'] == self.v_level]\n\n        elif self.default_branch_kind == 'cable':\n            load_factor_normal = float(cfg_ding0.get('assumptions',\n                                                     'load_factor_mv_cable_lc_normal'))\n            branch_parameters = self.network.static_data['MV_cables']\n        else:\n            raise ValueError('Grid\\'s default_branch_kind is invalid, could not set branch parameters.')\n\n        # select appropriate branch params according to voltage level, sorted ascending by max. current\n        # use <240mm2 only (ca. 420A) for initial rings and for disambiguation of agg. LA\n        branch_parameters = branch_parameters[branch_parameters['U_n'] == self.v_level]\n        branch_parameters = branch_parameters[branch_parameters['reinforce_only'] == 0].sort_values('I_max_th')\n\n        # get largest line/cable type\n        branch_type_max = branch_parameters.loc[branch_parameters['I_max_th'].idxmax()]\n\n        # set aggregation flag using largest available line/cable\n        self.set_nodes_aggregation_flag(branch_type_max['I_max_th'] * load_factor_normal)\n\n        # calc peak current sum (= \"virtual\" current) of whole grid (I = S / sqrt(3) / U) excluding load areas of type\n        # satellite and aggregated\n        peak_current_sum = ((self.grid_district.peak_load -\n                             self.grid_district.peak_load_satellites -\n                             self.grid_district.peak_load_aggregated) /\n                            cos_phi_load /\n                            (3**0.5) / self.v_level)  # units: kVA / kV = A\n\n        branch_type_settle = branch_type_settle_max = None\n\n        # search the smallest possible line/cable for MV grid district in equipment datasets for all load areas\n        # excluding those of type satellite and aggregated\n        for idx, row in branch_parameters.iterrows():\n            # calc number of required rings using peak current sum of grid district,\n            # load factor and max. current of line/cable\n            half_ring_count = round(peak_current_sum / (row['I_max_th'] * load_factor_normal))\n\n            if debug:\n                logger.debug('=== Selection of default branch type in {} ==='.format(self))\n                logger.debug('Peak load= {} kVA'.format(self.grid_district.peak_load))\n                logger.debug('Peak current={}'.format(peak_current_sum))\n                logger.debug('I_max_th={}'.format(row['I_max_th']))\n                logger.debug('Half ring count={}'.format(half_ring_count))\n\n            # if count of half rings is below or equal max. allowed count, use current branch type as default\n            if half_ring_count <= mv_half_ring_count_max:\n                if self.default_branch_kind == 'line':\n\n                    # take only cables that can handle at least the current of the line\n                    branch_parameters_settle_filter = branch_parameters_settle[\\\n                                                      branch_parameters_settle['I_max_th'] - row['I_max_th'] > 0]\n\n                    # get cable type with similar (but greater) I_max_th\n                    # note: only grids with lines as default branch kind get cables in settlements\n                    # (not required in grids with cables as default branch kind)\n                    branch_type_settle = branch_parameters_settle_filter.loc[\\\n                                         branch_parameters_settle_filter['I_max_th'].idxmin()]\n\n                return row, branch_type_max, branch_type_settle\n\n        # no equipment was found, return largest available line/cable\n\n        if debug:\n            logger.debug('No appropriate line/cable type could be found for '\n                         '{}, declare some load areas as aggregated.'.format(self))\n\n        if self.default_branch_kind == 'line':\n            branch_type_settle_max = branch_parameters_settle.loc[branch_parameters_settle['I_max_th'].idxmax()]\n\n        return branch_type_max, branch_type_max, branch_type_settle_max"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the load areas to be aggregated if they are too high demand.", "response": "def set_nodes_aggregation_flag(self, peak_current_branch_max):\n        \"\"\" Set Load Areas with too high demand to aggregated type.\n\n        Args\n        ----\n        peak_current_branch_max: float\n            Max. allowed current for line/cable\n\n        \"\"\"\n\n        for lv_load_area in self.grid_district.lv_load_areas():\n            peak_current_node = (lv_load_area.peak_load / (3**0.5) / self.v_level)  # units: kVA / kV = A\n            if peak_current_node > peak_current_branch_max:\n                lv_load_area.is_aggregated = True\n\n        # add peak demand for all Load Areas of aggregation type\n        self.grid_district.add_aggregated_peak_demand()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexport MVGridDing0 grid to PyPSA database tables.", "response": "def export_to_pypsa(self, session, method='onthefly'):\n        \"\"\"Exports MVGridDing0 grid to PyPSA database tables\n\n        Peculiarities of MV grids are implemented here. Derive general export\n        method from this and adapt to needs of LVGridDing0\n\n        Parameters\n        ----------\n        session: :sqlalchemy:`SQLAlchemy session object<orm/session_basics.html>`\n            Description\n        method: str\n            Specify export method::\n            \n            'db': grid data will be exported to database\n            'onthefly': grid data will be passed to PyPSA directly (default)\n\n        Notes\n        -----\n        It has to be proven that this method works for LV grids as well!\n\n        Ding0 treats two stationary case of powerflow:\n        1) Full load: We assume no generation and loads to be set to peak load\n        2) Generation worst case:\n        \"\"\"\n\n        # definitions for temp_resolution table\n        temp_id = 1\n        timesteps = 2\n        start_time = datetime(1970, 1, 1, 00, 00, 0)\n        resolution = 'H'\n\n        nodes = self._graph.nodes()\n\n        edges = [edge for edge in list(self.graph_edges())\n                 if (edge['adj_nodes'][0] in nodes and not isinstance(\n                edge['adj_nodes'][0], LVLoadAreaCentreDing0))\n                 and (edge['adj_nodes'][1] in nodes and not isinstance(\n                edge['adj_nodes'][1], LVLoadAreaCentreDing0))]\n\n        if method == 'db':\n\n            # Export node objects: Busses, Loads, Generators\n            pypsa_io.export_nodes(self,\n                                  session,\n                                  nodes,\n                                  temp_id,\n                                  lv_transformer=False)\n\n            # Export edges\n            pypsa_io.export_edges(self, session, edges)\n\n            # Create table about temporal coverage of PF analysis\n            pypsa_io.create_temp_resolution_table(session,\n                                                  timesteps=timesteps,\n                                                  resolution=resolution,\n                                                  start_time=start_time)\n        elif method == 'onthefly':\n\n            nodes_dict, components_data = pypsa_io.nodes_to_dict_of_dataframes(\n                self,\n                nodes,\n                lv_transformer=False)\n            edges_dict = pypsa_io.edges_to_dict_of_dataframes(self, edges)\n            components = tools.merge_two_dicts(nodes_dict, edges_dict)\n\n            return components, components_data\n        else:\n            raise ValueError('Sorry, this export method does not exist!')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_powerflow(self, session, export_pypsa_dir=None,  method='onthefly', debug=False):\n\n        if method == 'db':\n            raise NotImplementedError(\"Please use 'onthefly'.\")\n\n        elif method == 'onthefly':\n            components, components_data = self.export_to_pypsa(session, method)\n            pypsa_io.run_powerflow_onthefly(components,\n                                            components_data,\n                                            self,\n                                            export_pypsa_dir=export_pypsa_dir,\n                                            debug=debug)", "response": "This method runs the power flow for all MV grids in the current database and returns the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_powerflow_results(self, session):\n\n        # bus data\n        pypsa_io.import_pfa_bus_results(session, self)\n\n        # line data\n        pypsa_io.import_pfa_line_results(session, self)", "response": "Assign results from power flow analysis to edges and nodes\n Arcs and set them as attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a LV station to _station and grid graph if not already existing.", "response": "def add_station(self, lv_station):\n        \"\"\"Adds a LV station to _station and grid graph if not already existing\"\"\"\n        if not isinstance(lv_station, LVStationDing0):\n            raise Exception('Given LV station is not a LVStationDing0 object.')\n        if self._station is None:\n            self._station = lv_station\n            self.graph_add_node(lv_station)\n            self.grid_district.lv_load_area.mv_grid_district.mv_grid.graph_add_node(lv_station)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loads_sector(self, sector='res'):\n        \n        for load in self._loads:\n            if (sector == 'res') and (load.string_id is not None):\n                yield load\n            elif (sector == 'ria') and (load.string_id is None):\n                yield load", "response": "Returns a generator for iterating over loads of the specified sector."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_load(self, lv_load):\n        if lv_load not in self._loads and isinstance(lv_load,\n                                                     LVLoadDing0):\n            self._loads.append(lv_load)\n            self.graph_add_node(lv_load)", "response": "Adds a LV load to _loads and grid graph if not already existing"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_cable_dist(self, lv_cable_dist):\n        if lv_cable_dist not in self._cable_distributors and isinstance(lv_cable_dist,\n                                                                        LVCableDistributorDing0):\n            self._cable_distributors.append(lv_cable_dist)\n            self.graph_add_node(lv_cable_dist)", "response": "Adds a LV cable_dist to the _cable_distributors and the grid graph if not already existing."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build_grid(self):\n\n        # add required transformers\n        build_grid.transformer(self)\n\n        # add branches of sectors retail/industrial and agricultural\n        build_grid.build_ret_ind_agr_branches(self.grid_district)\n\n        # add branches of sector residential\n        build_grid.build_residential_branches(self.grid_district)", "response": "Create LV grid graph\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconnecting LV generators to grid", "response": "def connect_generators(self, debug=False):\n        \"\"\" Connects LV generators (graph nodes) to grid (graph)\n\n        Args\n        ----\n        debug: bool, defaults to False\n             If True, information is printed during process\n        \"\"\"\n\n        self._graph = lv_connect.lv_connect_generators(self.grid_district, self._graph, debug)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a deep copy of self", "response": "def clone(self):\n        \"\"\"Returns a deep copy of self\n\n        Function clones:\n        \n        * allocation\n        * nodes\n        \n        Returns\n        -------\n        type\n            Deep copy of self\n        \"\"\"\n\n        new_route = self.__class__(self._problem)\n        for node in self.nodes():\n            # Insere new node on new route\n            new_node = node.__class__(node._name, node._demand)\n            new_route.allocate([new_node])\n\n        return new_route"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef length(self):\n        cost = 0\n        depot = self._problem.depot()\n\n        last = depot\n        for i in self._nodes:\n            a, b = last, i\n            if a.name() > b.name():\n                a, b = b, a\n\n            cost = cost + self._problem.distance(a, b)\n            last = i\n\n        cost = cost + self._problem.distance(depot, last)\n\n        return cost", "response": "Returns the route length"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef length_from_nodelist(self, nodelist):\n        cost = 0\n\n        for n1, n2 in zip(nodelist[0:len(nodelist) - 1], nodelist[1:len(nodelist)]):\n            cost += self._problem.distance(n1, n2)\n\n        return cost", "response": "Returns the length of the route from the first to the last node in nodelist"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef can_allocate(self, nodes, pos=None):\n        # TODO: check docstring\n        \"\"\"Returns True if this route can allocate nodes in `nodes` list\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n        pos : type, defaults to None\n            Desc\n            \n        Returns\n        -------\n        bool\n            True if this route can allocate nodes in `nodes` list\n        \"\"\"\n\n        # clone route and nodes\n        new_route = self.clone()\n        new_nodes = [node.clone() for node in nodes]\n        if pos is None:\n            pos = len(self._nodes)\n        new_route._nodes = new_route._nodes[:pos] + new_nodes + new_route._nodes[pos:]\n        new_route._demand = sum([node.demand() for node in new_route._nodes])\n\n        if new_route.tech_constraints_satisfied():\n            return True\n\n        return False", "response": "Returns True if this route can allocate nodes in nodes list\necords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nallocate all nodes from nodes list in this route.", "response": "def allocate(self, nodes, append=True):\n        # TODO: check docstring\n        \"\"\"Allocates all nodes from `nodes` list in this route\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n        append : bool, defaults to True\n            Desc\n        \n        \"\"\"\n\n        nodes_demand = 0\n        for node in [node for node in nodes]:\n            if node._allocation:\n                node._allocation.deallocate([node])\n\n            node._allocation = self\n            nodes_demand = nodes_demand + node.demand()\n            if append:\n                self._nodes.append(node)\n            else:\n                self._nodes.insert(0, node)\n\n        self._demand = self._demand + nodes_demand"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef deallocate(self, nodes):\n        # TODO: check docstring\n        \"\"\"Deallocates all nodes from `nodes` list from this route\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n        \"\"\"\n\n        nodes_demand = 0\n        for node in nodes:\n            self._nodes.remove(node)\n            node._allocation = None\n            nodes_demand = nodes_demand + node.demand()\n\n        self._demand = self._demand - nodes_demand\n\n        if self._demand < 0:\n            raise Exception('Trying to deallocate more than previously allocated')", "response": "Deallocates all nodes from nodes list from this route\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert all nodes from nodes list into this route at position pos.", "response": "def insert(self, nodes, pos):\n        # TODO: check docstring\n        \"\"\"Inserts all nodes from `nodes` list into this route at position `pos`\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n        pos : type\n            Desc\n        \n        \"\"\"\n        \n        node_list = []\n        nodes_demand = 0\n        for node in [node for node in nodes]:\n            if node._allocation:\n                node._allocation.deallocate([node])\n            node_list.append(node)\n            node._allocation = self\n            nodes_demand = nodes_demand + node.demand()\n\n        self._nodes = self._nodes[:pos] + node_list + self._nodes[pos:]\n        self._demand += nodes_demand"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_interior(self, node):\n        # TODO: check docstring\n        \"\"\"Returns True if node is interior to the route, i.e., not adjascent to depot\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n            \n        Returns\n        -------\n        bool\n            True if node is interior to the route\n        \n        \"\"\"\n        return self._nodes.index(node) != 0 and self._nodes.index(node) != len(self._nodes) - 1", "response": "Returns True if node is interior to the route False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the node is the last node in the route ecords.", "response": "def last(self, node):\n        # TODO: check docstring\n        \"\"\"Returns True if node is the last node in the route\n        \n        Parameters\n        ----------\n        nodes : type\n            Desc\n            \n        Returns\n        -------\n        bool\n            True if node is the last node in the route\n        \"\"\"\n        return self._nodes.index(node) == len(self._nodes) - 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the optimal position of a circuit breaker on a route.", "response": "def calc_circuit_breaker_position(self, debug=False):\n        \"\"\" Calculates the optimal position of a circuit breaker on route.\n        \n        Parameters\n        ----------\n        debug: bool, defaults to False\n            If True, prints process information.\n        \n        Returns\n        -------\n        int\n            position of circuit breaker on route (index of last node on 1st half-ring preceding the circuit breaker)\n\n        Notes\n        -----\n        According to planning principles of MV grids, a MV ring is run as two strings (half-rings) separated by a\n        circuit breaker which is open at normal operation.\n        Assuming a ring (route which is connected to the root node at either sides), the optimal position of a circuit\n        breaker is defined as the position (virtual cable) between two nodes where the conveyed current is minimal on\n        the route. Instead of the peak current, the peak load is used here (assuming a constant voltage).\n\n        The circuit breakers are used here for checking tech. constraints only and will be re-located after connection\n        of satellites and stations in ding0.grid.mv_grid.tools.set_circuit_breakers\n\n        References\n        ----------\n        \n        See Also\n        --------\n        ding0.grid.mv_grid.tools.set_circuit_breakers\n\n        \"\"\"\n        # TODO: add references (Tao)\n\n        # set init value\n        demand_diff_min = 10e6\n\n        # check possible positions in route\n        for ctr in range(len(self._nodes)):\n            # split route and calc demand difference\n            route_demand_part1 = sum([node.demand() for node in self._nodes[0:ctr]])\n            route_demand_part2 = sum([node.demand() for node in self._nodes[ctr:len(self._nodes)]])\n            demand_diff = abs(route_demand_part1 - route_demand_part2)\n\n            if demand_diff < demand_diff_min:\n                demand_diff_min = demand_diff\n                position = ctr\n\n        if debug:\n            logger.debug('sum 1={}'.format(\n                sum([node.demand() for node in self._nodes[0:position]])))\n            logger.debug('sum 2={}'.format(sum([node.demand() for node in\n                                                self._nodes[\n                                                position:len(self._nodes)]])))\n            logger.debug(\n                'Position of circuit breaker: {0}-{1} (sumdiff={2})'.format(\n                    self._nodes[position - 1], self._nodes[position],\n                    demand_diff_min))\n\n        return position"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck route validity according to technical constraints (voltage and current rating) It considers constraints as * current rating of cable/line * voltage stability at all nodes Notes ----- The validation is done for every tested MV grid configuration during CVRP algorithm. The current rating is checked using load factors from [#]_. Due to the high amount of steps the voltage rating cannot be checked using load flow calculation. Therefore we use a simple method which determines the voltage change between two consecutive nodes according to [#]_. Furthermore it is checked if new route has got more nodes than allowed (typ. 2*10 according to [#]_). References ---------- .. [#] Deutsche Energie-Agentur GmbH (dena), \"dena-Verteilnetzstudie. Ausbau- und Innovationsbedarf der Stromverteilnetze in Deutschland bis 2030.\", 2012 .. [#] M. Sakulin, W. Hipp, \"Netzaspekte von dezentralen Erzeugungseinheiten, Studie im Auftrag der E-Control GmbH\", TU Graz, 2004 .. [#] Klaus Heuck et al., \"Elektrische Energieversorgung\", Vieweg+Teubner, Wiesbaden, 2007 .. [#] FGH e.V.: \"Technischer Bericht 302: Ein Werkzeug zur Optimierung der St\u00f6rungsbeseitigung f\u00fcr Planung und Betrieb von Mittelspannungsnetzen\", Tech. rep., 2008", "response": "def tech_constraints_satisfied(self):\n        \"\"\" Check route validity according to technical constraints (voltage and current rating)\n        \n        It considers constraints as\n        \n        * current rating of cable/line\n        * voltage stability at all nodes\n\n        Notes\n        -----\n            The validation is done for every tested MV grid configuration during CVRP algorithm. The current rating is\n            checked using load factors from [#]_. Due to the high amount of steps the voltage rating cannot be checked\n            using load flow calculation. Therefore we use a simple method which determines the voltage change between\n            two consecutive nodes according to [#]_.\n            Furthermore it is checked if new route has got more nodes than allowed (typ. 2*10 according to [#]_).\n\n        References\n        ----------\n            \n        .. [#] Deutsche Energie-Agentur GmbH (dena), \"dena-Verteilnetzstudie. Ausbau- und Innovationsbedarf der\n            Stromverteilnetze in Deutschland bis 2030.\", 2012\n        .. [#] M. Sakulin, W. Hipp, \"Netzaspekte von dezentralen Erzeugungseinheiten,\n            Studie im Auftrag der E-Control GmbH\", TU Graz, 2004\n        .. [#] Klaus Heuck et al., \"Elektrische Energieversorgung\", Vieweg+Teubner, Wiesbaden, 2007\n        .. [#] FGH e.V.: \"Technischer Bericht 302: Ein Werkzeug zur Optimierung der St\u00f6rungsbeseitigung\n            f\u00fcr Planung und Betrieb von Mittelspannungsnetzen\", Tech. rep., 2008\n        \"\"\"\n\n        # load parameters\n        load_area_count_per_ring = float(cfg_ding0.get('mv_routing',\n                                                       'load_area_count_per_ring'))\n\n        max_half_ring_length = float(cfg_ding0.get('mv_routing',\n                                                   'max_half_ring_length'))\n\n        if self._problem._branch_kind == 'line':\n            load_factor_normal = float(cfg_ding0.get('assumptions',\n                                                     'load_factor_mv_line_lc_normal'))\n            load_factor_malfunc = float(cfg_ding0.get('assumptions',\n                                                      'load_factor_mv_line_lc_malfunc'))\n        elif self._problem._branch_kind == 'cable':\n            load_factor_normal = float(cfg_ding0.get('assumptions',\n                                                     'load_factor_mv_cable_lc_normal'))\n            load_factor_malfunc = float(cfg_ding0.get('assumptions',\n                                                      'load_factor_mv_cable_lc_malfunc'))\n        else:\n            raise ValueError('Grid\\'s _branch_kind is invalid, could not use branch parameters.')\n\n        mv_max_v_level_lc_diff_normal = float(cfg_ding0.get('mv_routing_tech_constraints',\n                                                            'mv_max_v_level_lc_diff_normal'))\n        mv_max_v_level_lc_diff_malfunc = float(cfg_ding0.get('mv_routing_tech_constraints',\n                                                             'mv_max_v_level_lc_diff_malfunc'))\n        cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n\n\n        # step 0: check if route has got more nodes than allowed\n        if len(self._nodes) > load_area_count_per_ring:\n            return False\n\n        # step 1: calc circuit breaker position\n        position = self.calc_circuit_breaker_position()\n\n        # step 2: calc required values for checking current & voltage\n        # get nodes of half-rings\n        nodes_hring1 = [self._problem._depot] + self._nodes[0:position]\n        nodes_hring2 = list(reversed(self._nodes[position:len(self._nodes)] + [self._problem._depot]))\n        # get all nodes of full ring for both directions\n        nodes_ring1 = [self._problem._depot] + self._nodes\n        nodes_ring2 = list(reversed(self._nodes + [self._problem._depot]))\n        # factor to calc reactive from active power\n        Q_factor = tan(acos(cos_phi_load))\n        # line/cable params per km\n        r = self._problem._branch_type['R']  # unit for r: ohm/km\n        x = self._problem._branch_type['L'] * 2*pi * 50 / 1e3  # unit for x: ohm/km\n\n        # step 3: check if total lengths of half-rings exceed max. allowed distance\n        if (self.length_from_nodelist(nodes_hring1) > max_half_ring_length or\n            self.length_from_nodelist(nodes_hring2) > max_half_ring_length):\n            return False\n\n        # step 4a: check if current rating of default cable/line is violated\n        # (for every of the 2 half-rings using load factor for normal operation)\n        demand_hring_1 = sum([node.demand() for node in self._nodes[0:position]])\n        demand_hring_2 = sum([node.demand() for node in self._nodes[position:len(self._nodes)]])\n        peak_current_sum_hring1 = demand_hring_1 / (3**0.5) / self._problem._v_level  # units: kVA / kV = A\n        peak_current_sum_hring2 = demand_hring_2 / (3**0.5) / self._problem._v_level  # units: kVA / kV = A\n\n        if (peak_current_sum_hring1 > (self._problem._branch_type['I_max_th'] * load_factor_normal) or\n            peak_current_sum_hring2 > (self._problem._branch_type['I_max_th'] * load_factor_normal)):\n            return False\n\n        # step 4b: check if current rating of default cable/line is violated\n        # (for full ring using load factor for malfunction operation)\n        peak_current_sum_ring = self._demand / (3**0.5) / self._problem._v_level  # units: kVA / kV = A\n        if peak_current_sum_ring > (self._problem._branch_type['I_max_th'] * load_factor_malfunc):\n            return False\n\n        # step 5a: check voltage stability at all nodes\n        # (for every of the 2 half-rings using max. voltage difference for normal operation)\n\n        # get operation voltage level from station\n        v_level_hring1 =\\\n            v_level_hring2 =\\\n            v_level_ring_dir1 =\\\n            v_level_ring_dir2 =\\\n            v_level_op =\\\n            self._problem._v_level * 1e3\n\n        # set initial r and x\n        r_hring1 =\\\n            r_hring2 =\\\n            x_hring1 =\\\n            x_hring2 =\\\n            r_ring_dir1 =\\\n            r_ring_dir2 =\\\n            x_ring_dir1 =\\\n            x_ring_dir2 = 0\n\n        for n1, n2 in zip(nodes_hring1[0:len(nodes_hring1)-1], nodes_hring1[1:len(nodes_hring1)]):\n            r_hring1 += self._problem.distance(n1, n2) * r\n            x_hring1 += self._problem.distance(n1, n2) * x\n            v_level_hring1 -= n2.demand() * 1e3 * (r_hring1 + x_hring1*Q_factor) / v_level_op\n            if (v_level_op - v_level_hring1) > (v_level_op * mv_max_v_level_lc_diff_normal):\n                return False\n\n        for n1, n2 in zip(nodes_hring2[0:len(nodes_hring2)-1], nodes_hring2[1:len(nodes_hring2)]):\n            r_hring2 += self._problem.distance(n1, n2) * r\n            x_hring2 += self._problem.distance(n1, n2) * x\n            v_level_hring2 -= n2.demand() * 1e3 * (r_hring2 + x_hring2 * Q_factor) / v_level_op\n            if (v_level_op - v_level_hring2) > (v_level_op * mv_max_v_level_lc_diff_normal):\n                return False\n\n        # step 5b: check voltage stability at all nodes\n        # (for full ring calculating both directions simultaneously using max. voltage diff. for malfunction operation)\n        for (n1, n2), (n3, n4) in zip(zip(nodes_ring1[0:len(nodes_ring1)-1], nodes_ring1[1:len(nodes_ring1)]),\n                                      zip(nodes_ring2[0:len(nodes_ring2)-1], nodes_ring2[1:len(nodes_ring2)])):\n            r_ring_dir1 += self._problem.distance(n1, n2) * r\n            r_ring_dir2 += self._problem.distance(n3, n4) * r\n            x_ring_dir1 += self._problem.distance(n1, n2) * x\n            x_ring_dir2 += self._problem.distance(n3, n4) * x\n            v_level_ring_dir1 -= (n2.demand() * 1e3 * (r_ring_dir1 + x_ring_dir1 * Q_factor) / v_level_op)\n            v_level_ring_dir2 -= (n4.demand() * 1e3 * (r_ring_dir2 + x_ring_dir2 * Q_factor) / v_level_op)\n            if ((v_level_op - v_level_ring_dir1) > (v_level_op * mv_max_v_level_lc_diff_malfunc) or\n                (v_level_op - v_level_ring_dir2) > (v_level_op * mv_max_v_level_lc_diff_malfunc)):\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a deep copy of self", "response": "def clone(self):\n        # TODO: check docstring\n        \"\"\"Returns a deep copy of self\n        \n        Function clones:\n        \n        * allocation\n        * nodes\n        \n        Returns\n        -------\n        type\n            Deep copy of self\n        \"\"\"\n\n        new_node = self.__class__(self._name, self._demand)\n\n        return new_node"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator for iterating over edges.", "response": "def edges(self):\n        # TODO: check docstring\n        \"\"\"Returns a generator for iterating over edges\n        \n        Yields\n        ------\n        type\n            Generator for iterating over edges.\n            \n        \"\"\"\n        for i in sorted(self._matrix.keys(), key=lambda x:x.name()):\n            for j in sorted(self._matrix[i].keys(), key=lambda x:x.name()):\n                if i != j:\n                    yield (i, j)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the distance between node i and node j.", "response": "def distance(self, i, j):\n        # TODO: check docstring\n        \"\"\"Returns the distance between node i and node j\n        \n        Parameters\n        ----------\n        i : type\n            Descr\n        j : type\n            Desc\n            \n        Returns\n        -------\n        float\n            Distance between node i and node j.\n        \"\"\"\n\n        a, b = i, j\n\n        if a.name() > b.name():\n            a, b = b, a\n        \n        return self._matrix[self._nodes[a.name()]][self._nodes[b.name()]]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_transformers(grid, s_max=None):\n\n    load_factor_lv_trans_lc_normal = cfg_ding0.get('assumptions',\n                                                   'load_factor_lv_trans_lc_normal')\n    load_factor_lv_trans_fc_normal = cfg_ding0.get('assumptions',\n                                                   'load_factor_lv_trans_fc_normal')\n\n    cos_phi_load = cfg_ding0.get('assumptions',\n                                 'cos_phi_load')\n    cos_phi_gen = cfg_ding0.get('assumptions',\n                                'cos_phi_gen')\n\n    # get equipment parameters of LV transformers\n    trafo_parameters = grid.network.static_data['LV_trafos']\n\n    # determine s_max from grid object if not provided via arguments\n    if s_max is None:\n        # get maximum from peak load and peak generation\n        s_max_load = grid.grid_district.peak_load / cos_phi_load\n        s_max_gen = grid.station().peak_generation / cos_phi_gen\n\n        # check if load or generation is greater respecting corresponding load factor\n        if s_max_load > s_max_gen:\n            # use peak load and load factor from load case\n            load_factor_lv_trans = load_factor_lv_trans_lc_normal\n            s_max = s_max_load\n        else:\n            # use peak generation and load factor for feedin case\n            load_factor_lv_trans = load_factor_lv_trans_fc_normal\n            s_max = s_max_gen\n    else:\n        if s_max['case'] == 'load':\n            load_factor_lv_trans = load_factor_lv_trans_lc_normal\n        elif s_max['case'] == 'gen':\n            load_factor_lv_trans = load_factor_lv_trans_fc_normal\n        else:\n            logger.error('No proper \\'case\\' provided for argument s_max')\n            raise ValueError('Please provide proper \\'case\\' for argument '\n                             '`s_max`.')\n        s_max = s_max['s_max']\n\n    # get max. trafo\n    transformer_max = trafo_parameters.iloc[trafo_parameters['S_nom'].idxmax()]\n\n    # peak load is smaller than max. available trafo\n    if s_max < (transformer_max['S_nom'] * load_factor_lv_trans ):\n        # choose trafo\n        transformer = trafo_parameters.iloc[\n            trafo_parameters[\n                trafo_parameters['S_nom'] * load_factor_lv_trans > s_max][\n                'S_nom'].idxmin()]\n        transformer_cnt = 1\n    # peak load is greater than max. available trafo -> use multiple trafos\n    else:\n        transformer_cnt = 2\n        # increase no. of trafos until peak load can be supplied\n        while not any(trafo_parameters['S_nom'] * load_factor_lv_trans > (\n                    s_max / transformer_cnt)):\n            transformer_cnt += 1\n        transformer = trafo_parameters.iloc[\n            trafo_parameters[\n                trafo_parameters['S_nom'] * load_factor_lv_trans\n                > (s_max / transformer_cnt)]['S_nom'].idxmin()]\n\n    return transformer, transformer_cnt", "response": "Selects LV transformers according to peak load of LV grid district."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transformer(grid):\n\n    # choose size and amount of transformers\n    transformer, transformer_cnt = select_transformers(grid)\n\n    # create transformers and add them to station of LVGD\n    for t in range(0, transformer_cnt):\n        lv_transformer = TransformerDing0(\n            grid=grid,\n            id_db=id,\n            v_level=0.4,\n            s_max_longterm=transformer['S_nom'],\n            r=transformer['R'],\n            x=transformer['X'])\n\n        # add each transformer to its station\n        grid._station.add_transformer(lv_transformer)", "response": "Choose transformers and add them to LV grid s station"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect a typified grid for retail and industrial and agricultural grids.", "response": "def select_grid_model_ria(lvgd, sector):\n    \"\"\"Select a typified grid for retail/industrial and agricultural\n\n    Parameters\n    ----------\n    lvgd : ding0.core.structure.regions.LVGridDistrictDing0\n        Low-voltage grid district object\n    sector : str\n        Either 'retail/industrial' or 'agricultural'. Depending on choice\n        different parameters to grid topology apply\n\n    Returns\n    -------\n    :obj:`dict`\n        Parameters that describe branch lines of a sector\n    \"\"\"\n\n    cable_lf = cfg_ding0.get('assumptions',\n                             'load_factor_lv_cable_lc_normal')\n\n    cos_phi_load = cfg_ding0.get('assumptions',\n                                 'cos_phi_load')\n\n    max_lv_branch_line_load = cfg_ding0.get('assumptions',\n                                            'max_lv_branch_line')\n\n    # make a distinction between sectors\n    if sector == 'retail/industrial':\n        max_branch_length = cfg_ding0.get(\n            \"assumptions\",\n            \"branch_line_length_retail_industrial\")\n        peak_load = lvgd.peak_load_retail + \\\n                    lvgd.peak_load_industrial\n        count_sector_areas = lvgd.sector_count_retail + \\\n                             lvgd.sector_count_industrial\n    elif sector == 'agricultural':\n        max_branch_length = cfg_ding0.get(\n            \"assumptions\",\n            \"branch_line_length_agricultural\")\n        peak_load = lvgd.peak_load_agricultural\n        count_sector_areas = lvgd.sector_count_agricultural\n    else:\n        raise ValueError('Sector {} does not exist!'.format(sector))\n\n    # determine size of a single load\n    single_peak_load = peak_load / count_sector_areas\n\n    # if this single load exceeds threshold of 300 kVA it is splitted\n    while single_peak_load > (max_lv_branch_line_load * (cable_lf * cos_phi_load)):\n        single_peak_load = single_peak_load / 2\n        count_sector_areas = count_sector_areas * 2\n\n    grid_model = {}\n\n    # determine parameters of branches and loads connected to the branch\n    # line\n    if 0 < single_peak_load:\n        grid_model['max_loads_per_branch'] = math.floor(\n            (max_lv_branch_line_load * (cable_lf * cos_phi_load)) / single_peak_load)\n        grid_model['single_peak_load'] = single_peak_load\n        grid_model['full_branches'] = math.floor(\n            count_sector_areas / grid_model['max_loads_per_branch'])\n        grid_model['remaining_loads'] = count_sector_areas - (\n            grid_model['full_branches'] * grid_model['max_loads_per_branch']\n        )\n        grid_model['load_distance'] = max_branch_length / (\n            grid_model['max_loads_per_branch'] + 1)\n        grid_model['load_distance_remaining'] = max_branch_length / (\n            grid_model['remaining_loads'] + 1)\n    else:\n        if count_sector_areas > 0:\n            logger.warning(\n                'LVGD {lvgd} has in sector {sector} no load but area count'\n                'is {count}. This is maybe related to #153'.format(\n                    lvgd=lvgd,\n                    sector=sector,\n                    count=count_sector_areas))\n            grid_model = None\n\n    # add consumption to grid_model for assigning it to the load object\n    # consumption is given per sector and per individual load\n    if sector == 'retail/industrial':\n        grid_model['consumption'] = {\n            'retail': lvgd.sector_consumption_retail / (\n                grid_model['full_branches'] *\n                grid_model['max_loads_per_branch'] +\n                grid_model['remaining_loads']),\n            'industrial': lvgd.sector_consumption_industrial / (\n                grid_model['full_branches'] *\n                grid_model['max_loads_per_branch'] +\n                grid_model['remaining_loads'])}\n    elif sector == 'agricultural':\n        grid_model['consumption'] = {\n            'agricultural': lvgd.sector_consumption_agricultural / (\n                grid_model['full_branches'] *\n                grid_model['max_loads_per_branch'] +\n                grid_model['remaining_loads'])}\n\n    return grid_model"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a graph of LV grids for LV grids retail and industrial and agricultural branches for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids for LV grids.", "response": "def build_lv_graph_ria(lvgd, grid_model_params):\n    \"\"\"Build graph for LV grid of sectors retail/industrial and agricultural\n\n    Based on structural description of LV grid topology for sectors\n    retail/industrial and agricultural (RIA) branches for these sectors are\n    created and attached to the LV grid's MV-LV substation bus bar.\n\n    LV loads of the sectors retail/industrial and agricultural are located\n    in separat branches for each sector (in case of large load multiple of\n    these).\n    These loads are distributed across the branches by an equidistant\n    distribution.\n\n    This function accepts the dict `grid_model_params` with particular\n    structure\n\n    >>> grid_model_params = {\n    >>> ... 'agricultural': {\n    >>> ...     'max_loads_per_branch': 2\n    >>> ...     'single_peak_load': 140,\n    >>> ...     'full_branches': 2,\n    >>> ...     'remaining_loads': 1,\n    >>> ...     'load_distance': 800/3,\n    >>> ...     'load_distance_remaining': 400}}\n\n    Parameters\n    ----------\n    lvgd : LVGridDistrictDing0\n        Low-voltage grid district object\n    grid_model_params : dict\n        Dict of structural information of sectoral LV grid branchwith particular \n        structure, e.g.::\n\n            grid_model_params = {\n                'agricultural': {\n                    'max_loads_per_branch': 2\n                    'single_peak_load': 140,\n                    'full_branches': 2,\n                    'remaining_loads': 1,\n                    'load_distance': 800/3,\n                    'load_distance_remaining': 400\n                }\n            }\n\n    Notes\n    -----\n    We assume a distance from the load to the branch it is connected to of\n    30 m. This assumption is defined in the config files.\n    \"\"\"\n\n    def lv_graph_attach_branch():\n        \"\"\"Attach a single branch including its equipment (cable dist, loads\n        and line segments) to graph of `lv_grid`\n        \"\"\"\n\n        # determine maximum current occuring due to peak load\n        # of this load load_no\n        I_max_load = val['single_peak_load'] / (3 ** 0.5 * 0.4) / cos_phi_load\n\n        # determine suitable cable for this current\n        suitable_cables_stub = lvgd.lv_grid.network.static_data['LV_cables'][\n            (lvgd.lv_grid.network.static_data['LV_cables'][\n                'I_max_th'] * cable_lf) > I_max_load]\n        cable_type_stub = suitable_cables_stub.loc[\n            suitable_cables_stub['I_max_th'].idxmin(), :\n            ]\n\n        # cable distributor to divert from main branch\n        lv_cable_dist = LVCableDistributorDing0(\n            grid=lvgd.lv_grid,\n            branch_no=branch_no,\n            load_no=load_no)\n        # add lv_cable_dist to graph\n        lvgd.lv_grid.add_cable_dist(lv_cable_dist)\n\n        # cable distributor within building (to connect load+geno)\n        lv_cable_dist_building = LVCableDistributorDing0(\n            grid=lvgd.lv_grid,\n            branch_no=branch_no,\n            load_no=load_no,\n            in_building=True)\n        # add lv_cable_dist_building to graph\n        lvgd.lv_grid.add_cable_dist(lv_cable_dist_building)\n\n        # create an instance of Ding0 LV load\n        lv_load = LVLoadDing0(grid=lvgd.lv_grid,\n                              branch_no=branch_no,\n                              load_no=load_no,\n                              peak_load=val['single_peak_load'],\n                              consumption=val['consumption'])\n\n        # add lv_load to graph\n        lvgd.lv_grid.add_load(lv_load)\n\n        # create branch line segment between either (a) station\n        # and cable distributor or (b) between neighboring cable\n        # distributors\n        if load_no == 1:\n            # case a: cable dist <-> station\n            lvgd.lv_grid._graph.add_edge(\n                lvgd.lv_grid.station(),\n                lv_cable_dist,\n                branch=BranchDing0(\n                    length=val['load_distance'],\n                    kind='cable',\n                    type=cable_type,\n                    id_db='branch_{sector}{branch}_{load}'.format(\n                        branch=branch_no,\n                        load=load_no,\n                        sector=sector_short)\n                ))\n        else:\n            # case b: cable dist <-> cable dist\n            lvgd.lv_grid._graph.add_edge(\n                lvgd.lv_grid._cable_distributors[-4],\n                lv_cable_dist,\n                branch=BranchDing0(\n                    length=val['load_distance'],\n                    kind='cable',\n                    type=cable_type,\n                    id_db='branch_{sector}{branch}_{load}'.format(\n                        branch=branch_no,\n                        load=load_no,\n                        sector=sector_short)))\n\n        # create branch stub that connects the load to the\n        # lv_cable_dist located in the branch line\n        lvgd.lv_grid._graph.add_edge(\n            lv_cable_dist,\n            lv_cable_dist_building,\n            branch=BranchDing0(\n                length=cfg_ding0.get(\n                    'assumptions',\n                    'lv_ria_branch_connection_distance'),\n                kind='cable',\n                type=cable_type_stub,\n                id_db='stub_{sector}{branch}_{load}'.format(\n                    branch=branch_no,\n                    load=load_no,\n                    sector=sector_short))\n        )\n\n        lvgd.lv_grid._graph.add_edge(\n            lv_cable_dist_building,\n            lv_load,\n            branch=BranchDing0(\n                length=1,\n                kind='cable',\n                type=cable_type_stub,\n                id_db='stub_{sector}{branch}_{load}'.format(\n                    branch=branch_no,\n                    load=load_no,\n                    sector=sector_short))\n        )\n\n    cable_lf = cfg_ding0.get('assumptions',\n                             'load_factor_lv_cable_lc_normal')\n    cos_phi_load = cfg_ding0.get('assumptions',\n                                 'cos_phi_load')\n\n    # iterate over branches for sectors retail/industrial and agricultural\n    for sector, val in grid_model_params.items():\n        if sector == 'retail/industrial':\n            sector_short = 'RETIND'\n        elif sector == 'agricultural':\n            sector_short = 'AGR'\n        else:\n            sector_short = ''\n        if val is not None:\n            for branch_no in list(range(1, val['full_branches'] + 1)):\n\n                # determine maximum current occuring due to peak load of branch\n                I_max_branch = (val['max_loads_per_branch'] *\n                                val['single_peak_load']) / (3 ** 0.5 * 0.4) / (\n                    cos_phi_load)\n\n                # determine suitable cable for this current\n                suitable_cables = lvgd.lv_grid.network.static_data['LV_cables'][\n                    (lvgd.lv_grid.network.static_data['LV_cables'][\n                        'I_max_th'] * cable_lf) > I_max_branch]\n                cable_type = suitable_cables.loc[\n                    suitable_cables['I_max_th'].idxmin(), :\n                ]\n\n                # create Ding0 grid objects and add to graph\n                for load_no in list(range(1, val['max_loads_per_branch'] + 1)):\n                    # create a LV grid string and attached to station\n                    lv_graph_attach_branch()\n\n            # add remaining branch\n            if val['remaining_loads'] > 0:\n                if 'branch_no' not in locals():\n                    branch_no = 0\n                # determine maximum current occuring due to peak load of branch\n                I_max_branch = (val['max_loads_per_branch'] *\n                                val['single_peak_load']) / (3 ** 0.5 * 0.4) / (\n                    cos_phi_load)\n\n                # determine suitable cable for this current\n                suitable_cables = lvgd.lv_grid.network.static_data['LV_cables'][\n                    (lvgd.lv_grid.network.static_data['LV_cables'][\n                        'I_max_th'] * cable_lf) > I_max_branch]\n                cable_type = suitable_cables.loc[\n                    suitable_cables['I_max_th'].idxmin(), :\n                ]\n\n                branch_no += 1\n\n                for load_no in list(range(1, val['remaining_loads'] + 1)):\n                    # create a LV grid string and attach to station\n                    lv_graph_attach_branch()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select_grid_model_residential(lvgd):\n\n    # Load properties of LV typified model grids\n    string_properties = lvgd.lv_grid.network.static_data['LV_model_grids_strings']\n    # Load relational table of apartment count and strings of model grid\n    apartment_string = lvgd.lv_grid.network.static_data[\n        'LV_model_grids_strings_per_grid']\n\n    # load assumtions\n    apartment_house_branch_ratio = cfg_ding0.get(\"assumptions\",\n                                                 \"apartment_house_branch_ratio\")\n    population_per_apartment = cfg_ding0.get(\"assumptions\",\n                                             \"population_per_apartment\")\n\n    # calc count of apartments to select string types\n    apartments = round(lvgd.population / population_per_apartment)\n\n    if apartments > 196:\n        apartments = 196\n\n    # select set of strings that represent one type of model grid\n    strings = apartment_string.loc[apartments]\n    selected_strings = [int(s) for s in strings[strings >= 1].index.tolist()]\n\n    # slice dataframe of string parameters\n    selected_strings_df = string_properties.loc[selected_strings]\n\n    # add number of occurences of each branch to df\n    occurence_selector = [str(i) for i in selected_strings]\n    selected_strings_df['occurence'] = strings.loc[occurence_selector].tolist()\n\n    return selected_strings_df", "response": "Selects typified model grid based on population and returns DataFrame of model grids."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the nxGraph based on the LV grid district and the selected string dataframe.", "response": "def build_lv_graph_residential(lvgd, selected_string_df):\n    \"\"\"Builds nxGraph based on the LV grid model\n\n    Parameters\n    ----------\n    lvgd : LVGridDistrictDing0\n        Low-voltage grid district object\n    selected_string_df: :pandas:`pandas.DataFrame<dataframe>`\n        Table of strings of the selected grid model\n\n    Notes\n    -----\n    To understand what is happening in this method a few data table columns\n    are explained here\n\n    * `count house branch`: number of houses connected to a string\n    * `distance house branch`: distance on a string between two house branches\n    * `string length`: total length of a string\n    * `length house branch A|B`: cable from string to connection point of a house\n\n    A|B in general brings some variation in to the typified model grid and\n    refer to different length of house branches and different cable types\n    respectively different cable widths.\n    \"\"\"\n\n    houses_connected = (\n        selected_string_df['occurence'] * selected_string_df[\n            'count house branch']).sum()\n\n    average_load = lvgd.peak_load_residential / \\\n                   houses_connected\n\n    average_consumption = lvgd.sector_consumption_residential / \\\n                   houses_connected\n\n    hh_branch = 0\n\n    # iterate over each type of branch\n    for i, row in selected_string_df.iterrows():\n\n        # get overall count of branches to set unique branch_no\n        branch_count_sum = len(list(\n            lvgd.lv_grid._graph.neighbors(lvgd.lv_grid.station())))\n\n        # iterate over it's occurences\n        for branch_no in range(1, int(row['occurence']) + 1):\n\n            hh_branch += 1\n            # iterate over house branches\n            for house_branch in range(1, row['count house branch'] + 1):\n                if house_branch % 2 == 0:\n                    variant = 'B'\n                else:\n                    variant = 'A'\n\n                # cable distributor to divert from main branch\n                lv_cable_dist = LVCableDistributorDing0(\n                    grid=lvgd.lv_grid,\n                    string_id=i,\n                    branch_no=branch_no + branch_count_sum,\n                    load_no=house_branch)\n                # add lv_cable_dist to graph\n                lvgd.lv_grid.add_cable_dist(lv_cable_dist)\n\n                # cable distributor within building (to connect load+geno)\n                lv_cable_dist_building = LVCableDistributorDing0(\n                    grid=lvgd.lv_grid,\n                    string_id=i,\n                    branch_no=branch_no + branch_count_sum,\n                    load_no=house_branch,\n                    in_building=True)\n                # add lv_cable_dist_building to graph\n                lvgd.lv_grid.add_cable_dist(lv_cable_dist_building)\n\n                lv_load = LVLoadDing0(grid=lvgd.lv_grid,\n                                      string_id=i,\n                                      branch_no=branch_no + branch_count_sum,\n                                      load_no=house_branch,\n                                      peak_load=average_load,\n                                      consumption={\n                                          'residential': average_consumption})\n\n                # add lv_load to graph\n                lvgd.lv_grid.add_load(lv_load)\n\n                cable_name = row['cable type'] + \\\n                             ' 4x1x{}'.format(row['cable width'])\n                cable_type = lvgd.lv_grid.network.static_data[\n                    'LV_cables'].loc[cable_name]\n\n                # connect current lv_cable_dist to station\n                if house_branch == 1:\n                    # edge connect first house branch in branch with the station\n                    lvgd.lv_grid._graph.add_edge(\n                        lvgd.lv_grid.station(),\n                        lv_cable_dist,\n                        branch=BranchDing0(\n                            length=row['distance house branch'],\n                            kind='cable',\n                            type=cable_type,\n                            id_db='branch_{sector}{branch}_{load}'.format(\n                                branch=hh_branch,\n                                load=house_branch,\n                                sector='HH')\n                        ))\n                # connect current lv_cable_dist to last one\n                else:\n                    lvgd.lv_grid._graph.add_edge(\n                        lvgd.lv_grid._cable_distributors[-4],\n                        lv_cable_dist,\n                        branch=BranchDing0(\n                            length=row['distance house branch'],\n                            kind='cable',\n                            type=lvgd.lv_grid.network.static_data[\n                                'LV_cables'].loc[cable_name],\n                            id_db='branch_{sector}{branch}_{load}'.format(\n                                branch=hh_branch,\n                                load=house_branch,\n                                sector='HH')))\n\n                # connect house to cable distributor\n                house_cable_name = row['cable type {}'.format(variant)] + \\\n                                   ' 4x1x{}'.format(\n                                       row['cable width {}'.format(variant)])\n                lvgd.lv_grid._graph.add_edge(\n                    lv_cable_dist,\n                    lv_cable_dist_building,\n                    branch=BranchDing0(\n                        length=row['length house branch {}'.format(\n                            variant)],\n                        kind='cable',\n                        type=lvgd.lv_grid.network.static_data['LV_cables']. \\\n                            loc[house_cable_name],\n                        id_db='branch_{sector}{branch}_{load}'.format(\n                            branch=hh_branch,\n                            load=house_branch,\n                            sector='HH'))\n                )\n\n                lvgd.lv_grid._graph.add_edge(\n                    lv_cable_dist_building,\n                    lv_load,\n                    branch=BranchDing0(\n                        length=1,\n                        kind='cable',\n                        type=lvgd.lv_grid.network.static_data['LV_cables']. \\\n                            loc[house_cable_name],\n                        id_db='branch_{sector}{branch}_{load}'.format(\n                            branch=hh_branch,\n                            load=house_branch,\n                            sector='HH'))\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the residential sector branches for lv grid district and LV grid districts.", "response": "def build_residential_branches(lvgd):\n    \"\"\"Based on population and identified peak load data, the according grid\n    topology for residential sector is determined and attached to the grid graph\n\n    Parameters\n    ----------\n    lvgd : LVGridDistrictDing0\n        Low-voltage grid district object\n    \"\"\"\n\n    # Choice of typified lv model grid depends on population within lv\n    # grid district. If no population is given, lv grid is omitted and\n    # load is represented by lv station's peak load\n    if lvgd.population > 0 \\\n            and lvgd.peak_load_residential > 0:\n        model_grid = select_grid_model_residential(lvgd)\n\n        build_lv_graph_residential(lvgd, model_grid)\n\n    # no residential load but population\n    elif lvgd.population > 0 \\\n            and lvgd.peak_load_residential == 0:\n        logger.warning(\n            '{} has population but no residential load. '\n            'No grid is created.'.format(\n                repr(lvgd)))\n\n    # residential load but no population\n    elif lvgd.population == 0 \\\n            and lvgd.peak_load_residential > 0:\n        logger.warning(\n            '{} has no population but residential load. '\n            'No grid is created and thus this load is '\n            'missing in overall balance!'.format(\n                repr(lvgd)))\n\n    else:\n        logger.info(\n            '{} has got no residential load. '\n            'No grid is created.'.format(\n                repr(lvgd)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lv_grid_generators_bus_bar(nd):\n\n    lv_stats = {}\n\n    for la in nd._mv_grid_districts[0].lv_load_areas():\n        for lvgd in la.lv_grid_districts():\n            station_neighbors = list(lvgd.lv_grid._graph[\n                                         lvgd.lv_grid._station].keys())\n\n            # check if nodes of a statio are members of list generators\n            station_generators = [x for x in station_neighbors\n                                  if x in lvgd.lv_grid.generators()]\n\n            lv_stats[repr(lvgd.lv_grid._station)] = station_generators\n\n    return lv_stats", "response": "Calculate statistics about generators at bus bar in LV grids"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_nd_to_pickle(nd, path='', filename=None):\n\n    abs_path = os.path.abspath(path)\n\n    if len(nd._mv_grid_districts) > 1:\n        name_extension = '_{number}-{number2}'.format(\n            number=nd._mv_grid_districts[0].id_db,\n            number2=nd._mv_grid_districts[-1].id_db)\n    else:\n        name_extension = '_{number}'.format(number=nd._mv_grid_districts[0].id_db)\n\n    if filename is None:\n        filename = \"ding0_grids_{ext}.pkl\".format(\n            ext=name_extension)\n\n    # delete attributes of `nd` in order to make pickling work\n    # del nd._config\n    del nd._orm\n\n    pickle.dump(nd, open(os.path.join(abs_path, filename), \"wb\"))", "response": "Use pickle to save the whole nd - object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload nd - object from pickle file.", "response": "def load_nd_from_pickle(filename=None, path=''):\n    \"\"\"\n    Use pickle to save the whole nd-object to disc\n\n    Parameters\n    ----------\n    filename : str\n        Filename of nd pickle\n    path : str\n        Absolute or relative path where pickle should be saved. Default is ''\n        which means pickle is save to PWD\n\n    Returns\n    -------\n    nd : NetworkDing0\n        Ding0 grid container object\n    \"\"\"\n\n    abs_path = os.path.abspath(path)\n\n    if filename is None:\n        raise NotImplementedError\n\n    return pickle.load(open(os.path.join(abs_path, filename), \"rb\"))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_cable_length(stats, plotpath):\n\n    # cable and line kilometer distribution\n    f, axarr = plt.subplots(2, 2, sharex=True)\n    stats.hist(column=['Length of MV overhead lines'], bins=5, alpha=0.5, ax=axarr[0, 0])\n    stats.hist(column=['Length of MV underground cables'], bins=5, alpha=0.5, ax=axarr[0, 1])\n    stats.hist(column=['Length of LV overhead lines'], bins=5, alpha=0.5, ax=axarr[1, 0])\n    stats.hist(column=['Length of LV underground cables'], bins=5, alpha=0.5, ax=axarr[1, 1])\n\n    plt.savefig(os.path.join(plotpath,\n                             'Histogram_cable_line_length.pdf'))", "response": "Plots the length of each cable in MV grid district\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots of generation over load", "response": "def plot_generation_over_load(stats, plotpath):\n    \"\"\"\n    Plot of generation over load\n    \"\"\"\n\n    # Generation capacity vs. peak load\n    sns.set_context(\"paper\", font_scale=1.1)\n    sns.set_style(\"ticks\")\n\n    # reformat to MW\n\n    gen_cap_indexes = [\"Gen. Cap. of MV at v_level 4\",\n                       \"Gen. Cap. of MV at v_level 5\",\n                       \"Gen. Cap. of LV at v_level 6\",\n                       \"Gen. Cap. of LV at v_level 7\"]\n    peak_load_index = [\"LA Total LV Peak Load total\"]\n    stats['generation_capacity'] = stats[gen_cap_indexes].sum(axis=1) / 1e3\n    stats['peak_load'] = stats[peak_load_index] / 1e3\n\n    sns.lmplot('generation_capacity', 'peak_load',\n               data=stats,\n               fit_reg=False,\n               # hue='v_nom',\n               # hue='Voltage level',\n               scatter_kws={\"marker\": \"D\",\n                            \"s\": 100},\n               aspect=2)\n    plt.title('Peak load vs. generation capacity')\n    plt.xlabel('Generation capacity in MW')\n    plt.ylabel('Peak load in MW')\n\n    plt.savefig(os.path.join(plotpath,\n                             'Scatter_generation_load.pdf'))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concat_nd_pickles(self, mv_grid_districts):\n\n    pickle_name = cfg_ding0.get('output', 'nd_pickle')\n    # self.nd = self.read_pickles_from_files(pickle_name)\n\n    # TODO: instead of passing a list of mvgd's, pass list of filenames plus optionally a basth_path\n    for mvgd in mv_grid_districts[1:]:\n\n        filename = os.path.join(\n            self.base_path,\n            'results', pickle_name.format(mvgd))\n        if os.path.isfile(filename):\n            mvgd_pickle = pickle.load(open(filename, 'rb'))\n            if mvgd_pickle._mv_grid_districts:\n                mvgd.add_mv_grid_district(mvgd_pickle._mv_grid_districts[0])\n\n    # save to concatenated pickle\n    pickle.dump(mvgd,\n                open(os.path.join(\n                    self.base_path,\n                    'results',\n                    \"ding0_grids_{0}-{1}.pkl\".format(\n                        mv_grid_districts[0],\n                        mv_grid_districts[-1])),\n                    \"wb\"))\n\n    # save stats (edges and nodes data) to csv\n    nodes, edges = mvgd.to_dataframe()\n    nodes.to_csv(os.path.join(\n        self.base_path,\n        'results', 'mvgd_nodes_stats_{0}-{1}.csv'.format(\n            mv_grid_districts[0], mv_grid_districts[-1])),\n        index=False)\n    edges.to_csv(os.path.join(\n        self.base_path,\n        'results', 'mvgd_edges_stats_{0}-{1}.csv'.format(\n            mv_grid_districts[0], mv_grid_districts[-1])),\n        index=False)", "response": "Read multiple pickles and save to file"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate statistics about the LV grids in a single network and returns a pandas. DataFrame containing several statistical numbers about the LV grids in the same network.", "response": "def calculate_lvgd_stats(nw):\n    \"\"\"\n    LV Statistics for an arbitrary network\n\n    Parameters\n    ----------\n    nw: :any:`list` of NetworkDing0\n        The MV grid(s) to be studied\n\n    Returns\n    -------\n    lvgd_stats : pandas.DataFrame\n        Dataframe containing several statistical numbers about the LVGD\n    \"\"\"\n    ##############################\n    #  ETRS (equidistant) to WGS84 (conformal) projection\n    proj = partial(\n        pyproj.transform,\n        # pyproj.Proj(init='epsg:3035'),  # source coordinate system\n        #  pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n        pyproj.Proj(init='epsg:4326'),  # source coordinate system\n        pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n    ##############################\n    # close circuit breakers\n    nw.control_circuit_breakers(mode='close')\n    ##############################\n    lv_dist_idx = 0\n    lv_dist_dict = {}\n    lv_gen_idx = 0\n    lv_gen_dict = {}\n    lv_load_idx = 0\n    lv_load_dict = {}\n    branch_idx = 0\n    branches_dict = {}\n    trafos_idx = 0\n    trafos_dict = {}\n    for mv_district in nw.mv_grid_districts():\n        for LA in mv_district.lv_load_areas():\n            for lv_district in LA.lv_grid_districts():\n                lv_dist_idx += 1\n                branches_from_station = len(lv_district.lv_grid.graph_branches_from_node(lv_district.lv_grid.station()))\n                lv_dist_dict[lv_dist_idx] = {\n                    'MV_grid_id': mv_district.mv_grid.id_db,\n                    'LV_grid_id': lv_district.lv_grid.id_db,\n                    'Load Area ID': LA.id_db,\n                    'Population': lv_district.population,\n                    'Peak Load Residential': lv_district.peak_load_residential,\n                    'Peak Load Retail': lv_district.peak_load_retail,\n                    'Peak Load Industrial': lv_district.peak_load_industrial,\n                    'Peak Load Agricultural': lv_district.peak_load_agricultural,\n                    'N\u00b0 of Sector Residential': lv_district.sector_count_residential,\n                    'N\u00b0 of Sector Retail': lv_district.sector_count_retail,\n                    'N\u00b0 of Sector Industrial': lv_district.sector_count_industrial,\n                    'N\u00b0 of Sector Agricultural': lv_district.sector_count_agricultural,\n                    'Accum. Consumption Residential': lv_district.sector_consumption_residential,\n                    'Accum. Consumption Retail': lv_district.sector_consumption_retail,\n                    'Accum. Consumption Industrial': lv_district.sector_consumption_industrial,\n                    'Accum. Consumption Agricultural': lv_district.sector_consumption_agricultural,\n                    'N\u00b0 of branches from LV Station': branches_from_station,\n                    'Load Area is Aggregated': LA.is_aggregated,\n                    'Load Area is Satellite': LA.is_satellite,\n                }\n                # generation capacity\n                for g in lv_district.lv_grid.generators():\n                    lv_gen_idx += 1\n                    subtype = g.subtype\n                    if subtype == None:\n                        subtype = 'other'\n                    type = g.type\n                    if type == None:\n                        type = 'other'\n                    lv_gen_dict[lv_gen_idx] = {\n                        'LV_grid_id': lv_district.lv_grid.id_db,\n                        'v_level': g.v_level,\n                        'subtype': type + '/' + subtype,\n                        'GenCap': g.capacity,\n                    }\n                # nodes bzw. LV loads\n                for node in lv_district.lv_grid.graph_nodes_sorted():\n                    if isinstance(node, LVLoadDing0):\n                        lv_load_idx += 1\n                        if 'agricultural' in node.consumption:\n                            tipo = 'agricultural'\n                        elif 'industrial' in node.consumption:\n                            tipo = 'ind_ret'\n                        elif 'residential' in node.consumption:\n                            tipo = 'residential'\n                        else:\n                            tipo = 'none'\n                        lv_load_dict[lv_load_idx] = {\n                            'LV_grid_id': lv_district.lv_grid.id_db,\n                            'load_type': tipo,\n                        }\n                # branches\n                for branch in lv_district.lv_grid.graph_edges():\n                    branch_idx += 1\n                    branches_dict[branch_idx] = {\n                        'LV_grid_id': lv_district.lv_grid.id_db,\n                        'length': branch['branch'].length / 1e3,\n                        'type_name': branch['branch'].type.to_frame().columns[0],\n                        'type_kind': branch['branch'].kind,\n                    }\n                # Transformers\n                for trafo in lv_district.lv_grid.station().transformers():\n                    trafos_idx += 1\n                    trafos_dict[trafos_idx] = {\n                        'LV_grid_id': lv_district.lv_grid.id_db,\n                        's_max_a': trafo.s_max_a,\n                    }\n\n                # geographic\n                district_geo = transform(proj, lv_district.geo_data)\n                lv_dist_dict[lv_dist_idx].update({'Area': district_geo.area})\n\n    lvgd_stats = pd.DataFrame.from_dict(lv_dist_dict, orient='index').set_index('LV_grid_id')\n    # generate partial dataframes\n    gen_df = pd.DataFrame.from_dict(lv_gen_dict, orient='index')\n    load_df = pd.DataFrame.from_dict(lv_load_dict, orient='index')\n    branch_df = pd.DataFrame.from_dict(branches_dict, orient='index')\n    trafos_df = pd.DataFrame.from_dict(trafos_dict, orient='index')\n\n    # resque desired data\n    if not gen_df.empty:\n        # generation by voltage level\n        lv_generation = gen_df.groupby(['LV_grid_id', 'v_level'])['GenCap'].sum().to_frame().unstack(level=-1)\n        lv_generation.columns = ['Gen. Cap. v_level ' + str(_[1]) if isinstance(_, tuple) else str(_) for _ in\n                                 lv_generation.columns]\n        lvgd_stats = pd.concat([lvgd_stats, lv_generation], axis=1)\n        # generation by type/subtype\n        lv_generation = gen_df.groupby(['LV_grid_id', 'subtype'])['GenCap'].sum().to_frame().unstack(level=-1)\n        lv_generation.columns = ['Gen. Cap. type ' + str(_[1]) if isinstance(_, tuple) else str(_) for _ in\n                                 lv_generation.columns]\n        lvgd_stats = pd.concat([lvgd_stats, lv_generation], axis=1)\n    if not load_df.empty:\n        # number of residential loads\n        lv_loads = load_df[load_df['load_type'] == 'residential'].groupby(['LV_grid_id'])[\n            'load_type'].count().to_frame()  # .unstack(level=-1)\n        lv_loads.columns = ['N\u00b0 of loads residential']\n        lvgd_stats = pd.concat([lvgd_stats, lv_loads], axis=1)\n        # number of agricultural loads\n        lv_loads = load_df[load_df['load_type'] == 'agricultural'].groupby(['LV_grid_id'])[\n            'load_type'].count().to_frame()  # .unstack(level=-1)\n        lv_loads.columns = ['N\u00b0 of loads agricultural']\n        lvgd_stats = pd.concat([lvgd_stats, lv_loads], axis=1)\n        # number of mixed industrial / retail loads\n        lv_loads = load_df[load_df['load_type'] == 'ind_ret'].groupby(['LV_grid_id'])[\n            'load_type'].count().to_frame()  # .unstack(level=-1)\n        lv_loads.columns = ['N\u00b0 of loads mixed industrial/retail']\n        lvgd_stats = pd.concat([lvgd_stats, lv_loads], axis=1)\n\n    if not branch_df.empty:\n        # branches by type name\n        lv_branches = branch_df.groupby(['LV_grid_id', 'type_name'])['length'].sum().to_frame().unstack(level=-1)\n        lv_branches.columns = ['Length Type ' + _[1] if isinstance(_, tuple) else _ for _ in lv_branches.columns]\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n        # branches by kind\n        lv_branches = branch_df[branch_df['type_kind'] == 'line'].groupby(['LV_grid_id'])['length'].sum().to_frame()\n        lv_branches.columns = ['Length of overhead lines']\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n        lv_branches = branch_df[branch_df['type_kind'] == 'cable'].groupby(['LV_grid_id'])['length'].sum().to_frame()\n        lv_branches.columns = ['Length of underground cables']\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n        # N\u00b0of branches\n        lv_branches = branch_df.groupby(['LV_grid_id', 'type_name'])['length'].count().to_frame().unstack(level=-1)\n        lv_branches.columns = ['N\u00b0 of branches Type ' + _[1] if isinstance(_, tuple) else _ for _ in\n                               lv_branches.columns]\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n        lv_branches = branch_df[branch_df['type_kind'] == 'line'].groupby(['LV_grid_id'])['length'].count().to_frame()\n        lv_branches.columns = ['N\u00b0 of branches overhead lines']\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n        lv_branches = branch_df[branch_df['type_kind'] == 'cable'].groupby(['LV_grid_id'])['length'].count().to_frame()\n        lv_branches.columns = ['N\u00b0 of branches underground cables']\n        lvgd_stats = pd.concat([lvgd_stats, lv_branches], axis=1)\n    if not trafos_df.empty:\n        # N of trafos\n        lv_trafos = trafos_df.groupby(['LV_grid_id'])['s_max_a'].count().to_frame()\n        lv_trafos.columns = ['N\u00b0 of MV/LV Trafos']\n        lvgd_stats = pd.concat([lvgd_stats, lv_trafos], axis=1)\n        # Capacity of trafos\n        lv_trafos = trafos_df.groupby(['LV_grid_id'])['s_max_a'].sum().to_frame()\n        lv_trafos.columns = ['Accumulated s_max_a in MVLV trafos']\n        lvgd_stats = pd.concat([lvgd_stats, lv_trafos], axis=1)\n\n    lvgd_stats = lvgd_stats.fillna(0)\n    lvgd_stats = lvgd_stats[sorted(lvgd_stats.columns.tolist())]\n    return lvgd_stats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calculate_mvgd_stats(nw):\n    ##############################\n\n    omega = 2 * pi * 50\n\n    # close circuit breakers\n    nw.control_circuit_breakers(mode='close')\n    ##############################\n    # Collect info from nw into dataframes\n    # define dictionaries for collection\n    trafos_dict = {}\n    generators_dict = {}\n    branches_dict = {}\n    ring_dict = {}\n    LA_dict = {}\n    other_nodes_dict = {}\n    lv_branches_dict = {}\n    # initiate indexes\n    trafos_idx = 0\n    gen_idx = 0\n    branch_idx = 0\n    ring_idx = 0\n    LA_idx = 0\n    lv_branches_idx = 0\n\n    # loop over MV grid districts\n    for district in nw.mv_grid_districts():\n\n        # node of MV station\n        root = district.mv_grid.station()\n\n        ###################################\n        # get impedance of path to each terminal node\n        # and get thermal capacity of first segment of path to each terminal node\n\n        # store properties of terminal nodes in dictionaries\n        # properties are e.g. impedance of path, length of path, thermal limit of first segment of path\n        mv_impedances = {}\n        mvlv_impedances = {}\n\n        mv_path_lengths = {}\n        mvlv_path_lengths = {}\n\n        mv_thermal_limits = {}  # I_max of first segment on MV for each MV path\n        lv_thermal_limits = {}  # I_max of first segment on LV for each LV path\n        mvlv_thermal_limits = {}  # I_max of first segment on MV for each MVLV path\n\n        n_outgoing_LV = 0\n        n_stations_LV = 0\n\n        n_outgoing_MV = 0\n\n        G = district.mv_grid._graph\n\n        for node in G.nodes():\n            if isinstance(node, MVStationDing0):\n                n_outgoing_MV += len(list(G.neighbors(node)))\n                continue\n            mv_impedance = 0\n            mv_path_length = 0\n            if not isinstance(node, MVCableDistributorDing0) and not isinstance(node, CircuitBreakerDing0):\n                if not nx.has_path(G, root, node):\n                    continue\n                    #print(node, node.lv_load_area.is_aggregated) # only debug\n                else:\n                    path = nx.shortest_path(G, root, node)\n                    for i in range(len(path) - 1):\n                        mv_impedance += np.sqrt(\n                            (G.adj[path[i]][path[i + 1]]['branch'].type[\n                                 'L'] * 1e-3 * omega * \\\n                             G.adj[path[i]][path[i + 1]][\n                                 'branch'].length) ** 2. + \\\n                            (G.adj[path[i]][path[i + 1]]['branch'].type[\n                                 'R'] * \\\n                             G.adj[path[i]][path[i + 1]][\n                                 'branch'].length) ** 2.)\n                        mv_path_length += G.adj[path[i]][path[i + 1]][\n                            'branch'].length\n\n                    mv_impedances[node] = mv_impedance\n                    mv_path_lengths[node] = mv_path_length\n                    mv_thermal_limit = G.adj[path[0]][path[1]]['branch'].type['I_max_th']\n                    mv_thermal_limits[node] = mv_thermal_limit\n\n                    if isinstance(node, LVStationDing0):\n                        # add impedance of transformers in LV station\n                        lvstation_impedance = 0.\n                        for trafo in node.transformers():\n                            lvstation_impedance += 1. / np.hypot(trafo.r,trafo.x)  # transformers operating in parallel\n                        if lvstation_impedance > 0.:  # avoid dividing by zero\n                            lvstation_impedance = 1. / lvstation_impedance\n                        else:\n                            lvstation_impedance = 0.\n                        # identify LV nodes belonging to LV station\n                        for lv_LA in district.lv_load_areas():\n                            for lv_dist in lv_LA.lv_grid_districts():\n                                if lv_dist.lv_grid._station == node:\n                                    G_lv = lv_dist.lv_grid._graph\n                                    # loop over all LV terminal nodes belonging to LV station\n                                    for lv_node in G_lv.nodes():\n                                        if isinstance(lv_node, GeneratorDing0) or isinstance(lv_node, LVLoadDing0):\n                                            path = nx.shortest_path(G_lv, node, lv_node)\n                                            lv_impedance = lvstation_impedance\n                                            lv_path_length = 0.\n                                            for i in range(len(path)-1):\n                                                lv_impedance += np.sqrt((G_lv.adj[path[i]][path[i+1]]['branch'].type['L'] * 1e-3 * omega * \\\n                                                                          G_lv.adj[path[i]][path[i+1]]['branch'].length)**2. + \\\n                                                                         (G_lv.adj[path[i]][path[i+1]]['branch'].type['R'] * \\\n                                                                          G_lv.adj[path[i]][path[i+1]]['branch'].length)**2.)\n                                                lv_path_length += G_lv.adj[path[i]][path[i+1]]['branch'].length\n                                            lv_thermal_limit = G_lv.adj[path[0]][path[1]]['branch'].type['I_max_th']\n\n                                            mvlv_impedances[lv_node] = mv_impedance + lv_impedance\n                                            mvlv_path_lengths[lv_node] = mv_path_length + lv_path_length\n                                            lv_thermal_limits[lv_node] = lv_thermal_limit\n                                            mvlv_thermal_limits[lv_node] = mv_thermal_limit\n\n                                        elif isinstance(lv_node, LVStationDing0):\n                                            n_outgoing_LV += len(list(G_lv.neighbors(lv_node)))\n                                            n_stations_LV += 1\n\n        # compute mean values by looping over terminal nodes\n        sum_impedances = 0.\n        sum_thermal_limits = 0.\n        sum_path_lengths = 0.\n        n_terminal_nodes_MV = 0\n\n        # terminal nodes on MV\n        for terminal_node in mv_impedances.keys():  # neglect LVStations here because already part of MVLV paths below\n            if not isinstance(terminal_node, LVStationDing0) and not isinstance(terminal_node, MVStationDing0):\n                sum_impedances += mv_impedances[terminal_node]\n                sum_thermal_limits += mv_thermal_limits[terminal_node]\n                sum_path_lengths += mv_path_lengths[terminal_node]\n                n_terminal_nodes_MV += 1\n\n        sum_thermal_limits_LV = 0.\n        n_terminal_nodes_LV = 0\n\n        # terminal nodes on LV\n        for terminal_node in mvlv_impedances.keys():\n            sum_impedances += mvlv_impedances[terminal_node]\n            sum_thermal_limits += mvlv_thermal_limits[terminal_node]\n            sum_thermal_limits_LV += lv_thermal_limits[terminal_node]\n            sum_path_lengths += mvlv_path_lengths[terminal_node]\n            n_terminal_nodes_LV += 1\n\n        n_terminal_nodes = n_terminal_nodes_MV + n_terminal_nodes_LV\n\n        if n_terminal_nodes < 1:\n            mean_impedance = np.nan\n            mean_thermal_limit = np.nan\n            mean_path_length = np.nan\n        else:\n            mean_impedance = sum_impedances / n_terminal_nodes\n            mean_thermal_limit = sum_thermal_limits / n_terminal_nodes\n            mean_path_length = sum_path_lengths / n_terminal_nodes\n        if n_terminal_nodes_LV < 1:\n            mean_thermal_limit_LV = np.nan\n        else:\n            mean_thermal_limit_LV = sum_thermal_limits_LV / n_terminal_nodes_LV\n        number_outgoing_LV = n_outgoing_LV  # / n_stations_LV\n        number_outgoing_MV = n_outgoing_MV\n\n        ###################################\n        # compute path lengths (written by Miguel)\n        max_mv_path = 0\n        max_mvlv_path = 0\n\n        # rings\n        nodes_in_rings = []\n        branches_in_rings = []\n        for ring in district.mv_grid.rings_full_data():\n            ring_idx += 1\n\n            # generation cap\n            ring_gen = 0\n            for node in ring[2]:\n                nodes_in_rings.append(node)\n                if isinstance(node, GeneratorDing0):\n                    ring_gen += node.capacity\n            # length\n            ring_length = 0\n            for branch in ring[1]:\n                branches_in_rings.append(branch)\n                ring_length += branch.length / 1e3\n\n            ring_dict[ring_idx] = {\n                'grid_id': district.mv_grid.id_db,\n                'ring_length': ring_length,\n                'ring_capacity': ring_gen,\n            }\n\n        # transformers in main station\n        for trafo in district.mv_grid.station().transformers():\n            trafos_idx += 1\n            trafos_dict[trafos_idx] = {\n                'grid_id': district.mv_grid.id_db,\n                's_max_a': trafo.s_max_a}\n\n        # Generators and other MV special nodes\n        cd_count = 0\n        LVs_count = 0\n        cb_count = 0\n        lv_trafo_count = 0\n        lv_trafo_cap = 0\n\n        for node in district.mv_grid._graph.nodes():\n            mv_path_length = 0\n            mvlv_path_length = 0\n\n            if isinstance(node, GeneratorDing0):\n                gen_idx += 1\n                isolation = not node in nodes_in_rings\n                subtype = node.subtype\n                if subtype == None:\n                    subtype = 'other'\n                generators_dict[gen_idx] = {\n                    'grid_id': district.mv_grid.id_db,\n                    'type': node.type,\n                    'sub_type': node.type + '/' + subtype,\n                    'gen_cap': node.capacity,\n                    'v_level': node.v_level,\n                    'isolation': isolation,\n                }\n                mv_path_length = district.mv_grid.graph_path_length(\n                    node_source=root,\n                    node_target=node)\n\n            elif isinstance(node, MVCableDistributorDing0):\n                cd_count += 1\n            elif isinstance(node, LVStationDing0):\n                LVs_count += 1\n                lv_trafo_count += len([trafo for trafo in node.transformers()])\n                lv_trafo_cap += np.sum([trafo.s_max_a for trafo in node.transformers()])\n\n                if not node.lv_load_area.is_aggregated:\n                    mv_path_length = district.mv_grid.graph_path_length(\n                        node_source=root,\n                        node_target=node)\n                    max_lv_path = 0\n                    for lv_LA in district.lv_load_areas():\n                        for lv_dist in lv_LA.lv_grid_districts():\n                            if lv_dist.lv_grid._station == node:\n                                for lv_node in lv_dist.lv_grid._graph.nodes():\n                                    lv_path_length = lv_dist.lv_grid.graph_path_length(\n                                        node_source=node,\n                                        node_target=lv_node)\n                                    max_lv_path = max(max_lv_path, lv_path_length)\n                    mvlv_path_length = mv_path_length + max_lv_path\n\n            elif isinstance(node, CircuitBreakerDing0):\n                cb_count += 1\n\n            max_mv_path = max(max_mv_path, mv_path_length / 1000)\n            max_mvlv_path = max(max_mvlv_path, mvlv_path_length / 1000)\n\n        other_nodes_dict[district.mv_grid.id_db] = {\n            'CD_count': cd_count,\n            'LV_count': LVs_count,\n            'CB_count': cb_count,\n            'MVLV_trafo_count': lv_trafo_count,\n            'MVLV_trafo_cap': lv_trafo_cap,\n            'max_mv_path': max_mv_path,\n            'max_mvlv_path': max_mvlv_path,\n            'mean_impedance': mean_impedance,\n            'mean_thermal_limit': mean_thermal_limit,\n            'mean_thermal_limit_LV': mean_thermal_limit_LV,\n            'mean_path_length': mean_path_length / 1.e3,\n            'number_outgoing_LV': number_outgoing_LV,\n            'number_outgoing_MV': number_outgoing_MV\n        }\n\n        # branches\n        for branch in district.mv_grid.graph_edges():\n            branch_idx += 1\n            br_in_ring = branch['branch'] in branches_in_rings\n            branches_dict[branch_idx] = {\n                'grid_id': district.mv_grid.id_db,\n                'length': branch['branch'].length / 1e3,\n                'type_name': branch['branch'].type['name'],\n                'type_kind': branch['branch'].kind,\n                'in_ring': br_in_ring,\n            }\n        # Load Areas\n        for LA in district.lv_load_areas():\n            LA_idx += 1\n            LA_dict[LA_idx] = {\n                'grid_id': district.mv_grid.id_db,\n                'is_agg': LA.is_aggregated,\n                'is_sat': LA.is_satellite,\n                # 'peak_gen':LA.peak_generation,\n            }\n            LA_pop = 0\n            residential_peak_load = 0\n            retail_peak_load = 0\n            industrial_peak_load = 0\n            agricultural_peak_load = 0\n            lv_gen_level_6 = 0\n            lv_gen_level_7 = 0\n            for lv_district in LA.lv_grid_districts():\n                LA_pop = + lv_district.population\n                residential_peak_load += lv_district.peak_load_residential\n                retail_peak_load += lv_district.peak_load_retail\n                industrial_peak_load += lv_district.peak_load_industrial\n                agricultural_peak_load += lv_district.peak_load_agricultural\n\n                # generation capacity\n                for g in lv_district.lv_grid.generators():\n                    if g.v_level == 6:\n                        lv_gen_level_6 += g.capacity\n                    elif g.v_level == 7:\n                        lv_gen_level_7 += g.capacity\n\n                # branches lengths\n                for br in lv_district.lv_grid.graph_edges():\n                    lv_branches_idx += 1\n                    lv_branches_dict[lv_branches_idx] = {\n                        'grid_id': district.mv_grid.id_db,\n                        'length': br['branch'].length / 1e3,\n                        'type_name': br['branch'].type.to_frame().columns[0],  # why is it different as for MV grids?\n                        'type_kind': br['branch'].kind,\n                    }\n\n            LA_dict[LA_idx].update({\n                'population': LA_pop,\n                'residential_peak_load': residential_peak_load,\n                'retail_peak_load': retail_peak_load,\n                'industrial_peak_load': industrial_peak_load,\n                'agricultural_peak_load': agricultural_peak_load,\n                'total_peak_load': residential_peak_load + retail_peak_load + \\\n                                   industrial_peak_load + agricultural_peak_load,\n                'lv_generation': lv_gen_level_6 + lv_gen_level_7,\n                'lv_gens_lvl_6': lv_gen_level_6,\n                'lv_gens_lvl_7': lv_gen_level_7,\n            })\n\n        # geographic\n        #  ETRS (equidistant) to WGS84 (conformal) projection\n        proj = partial(\n            pyproj.transform,\n            # pyproj.Proj(init='epsg:3035'),  # source coordinate system\n            # pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n            pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n        district_geo = transform(proj, district.geo_data)\n        other_nodes_dict[district.mv_grid.id_db].update({'Dist_area': district_geo.area})\n\n    mvgd_stats = pd.DataFrame.from_dict({}, orient='index')\n    ###################################\n    # built dataframes from dictionaries\n    trafos_df = pd.DataFrame.from_dict(trafos_dict, orient='index')\n    generators_df = pd.DataFrame.from_dict(generators_dict, orient='index')\n    other_nodes_df = pd.DataFrame.from_dict(other_nodes_dict, orient='index')\n    branches_df = pd.DataFrame.from_dict(branches_dict, orient='index')\n    lv_branches_df = pd.DataFrame.from_dict(lv_branches_dict, orient='index')\n    ring_df = pd.DataFrame.from_dict(ring_dict, orient='index')\n    LA_df = pd.DataFrame.from_dict(LA_dict, orient='index')\n\n    ###################################\n    # Aggregated data HV/MV Trafos\n    if not trafos_df.empty:\n        mvgd_stats = pd.concat([mvgd_stats, trafos_df.groupby('grid_id').count()['s_max_a']], axis=1)\n        mvgd_stats = pd.concat([mvgd_stats, trafos_df.groupby('grid_id').sum()[['s_max_a']]], axis=1)\n        mvgd_stats.columns = ['N\u00b0 of HV/MV Trafos', 'Trafos HV/MV Acc s_max_a']\n\n    ###################################\n    # Aggregated data Generators\n    if not generators_df.empty:\n        # MV generation per sub_type\n        mv_generation = generators_df.groupby(['grid_id', 'sub_type'])['gen_cap'].sum().to_frame().unstack(level=-1)\n        mv_generation.columns = ['Gen. Cap. of MV ' + _[1] if isinstance(_, tuple) else _\n                                 for _ in mv_generation.columns]\n        mvgd_stats = pd.concat([mvgd_stats, mv_generation], axis=1)\n\n        # MV generation at V levels\n        mv_generation = generators_df.groupby(\n            ['grid_id', 'v_level'])['gen_cap'].sum().to_frame().unstack(level=-1)\n        mv_generation.columns = ['Gen. Cap. of MV at v_level ' + str(_[1])\n                                 if isinstance(_, tuple) else _\n                                 for _ in mv_generation.columns]\n        mvgd_stats = pd.concat([mvgd_stats, mv_generation], axis=1)\n        # Isolated generators\n        mv_generation = generators_df[generators_df['isolation']].groupby(\n            ['grid_id'])['gen_cap'].count().to_frame()  # .unstack(level=-1)\n        mv_generation.columns = ['N\u00b0 of isolated MV Generators']\n        mvgd_stats = pd.concat([mvgd_stats, mv_generation], axis=1)\n\n    ###################################\n    # Aggregated data of other nodes\n    if not other_nodes_df.empty:\n        # print(other_nodes_df['CD_count'].to_frame())\n        mvgd_stats['N\u00b0 of Cable Distr'] = other_nodes_df['CD_count'].to_frame().astype(int)\n        mvgd_stats['N\u00b0 of LV Stations'] = other_nodes_df['LV_count'].to_frame().astype(int)\n        mvgd_stats['N\u00b0 of Circuit Breakers'] = other_nodes_df['CB_count'].to_frame().astype(int)\n        mvgd_stats['District Area'] = other_nodes_df['Dist_area'].to_frame()\n        mvgd_stats['N\u00b0 of MV/LV Trafos'] = other_nodes_df['MVLV_trafo_count'].to_frame().astype(int)\n        mvgd_stats['Trafos MV/LV Acc s_max_a'] = other_nodes_df['MVLV_trafo_cap'].to_frame()\n        mvgd_stats['Length of MV max path'] = other_nodes_df['max_mv_path'].to_frame()\n        mvgd_stats['Length of MVLV max path'] = other_nodes_df['max_mvlv_path'].to_frame()\n        mvgd_stats['Impedance Z of path to terminal node (mean value)'] = \\\n            other_nodes_df['mean_impedance'].to_frame()\n        mvgd_stats['I_max of first segment of path from MV station to terminal node (mean value)'] = \\\n            other_nodes_df['mean_thermal_limit'].to_frame()\n        mvgd_stats['I_max of first segment of path from LV station to terminal node (mean value)'] = \\\n            other_nodes_df['mean_thermal_limit_LV'].to_frame()\n        mvgd_stats['Length of path from MV station to terminal node (mean value)'] = \\\n            other_nodes_df['mean_path_length'].to_frame()\n        mvgd_stats['Number of lines and cables going out from LV stations'] = \\\n            other_nodes_df['number_outgoing_LV'].to_frame()\n        mvgd_stats['Number of lines and cables going out from MV stations'] = \\\n            other_nodes_df['number_outgoing_MV'].to_frame()\n\n    ###################################\n    # Aggregated data of MV Branches\n    if not branches_df.empty:\n        # km of underground cable\n        branches_data = branches_df[branches_df['type_kind'] == 'cable'].groupby(\n            ['grid_id'])['length'].sum().to_frame()\n        branches_data.columns = ['Length of MV underground cables']\n        mvgd_stats = pd.concat([mvgd_stats, branches_data], axis=1)\n\n        # km of overhead lines\n        branches_data = branches_df[branches_df['type_kind'] == 'line'].groupby(\n            ['grid_id'])['length'].sum().to_frame()\n        branches_data.columns = ['Length of MV overhead lines']\n        mvgd_stats = pd.concat([mvgd_stats, branches_data], axis=1)\n\n        # km of different wire types\n        branches_data = branches_df.groupby(\n            ['grid_id', 'type_name'])['length'].sum().to_frame().unstack(level=-1)\n        branches_data.columns = ['Length of MV type ' + _[1] if isinstance(_, tuple) else _\n                                 for _ in branches_data.columns]\n        mvgd_stats = pd.concat([mvgd_stats, branches_data], axis=1)\n\n        # branches not in ring\n        total_br = branches_df.groupby(['grid_id'])['length'].count().to_frame()\n        ring_br = branches_df[branches_df['in_ring']].groupby(\n            ['grid_id'])['length'].count().to_frame()\n        branches_data = total_br - ring_br\n        total_br.columns = ['N\u00b0 of MV branches']\n        mvgd_stats = pd.concat([mvgd_stats, total_br], axis=1)\n        branches_data.columns = ['N\u00b0 of MV branches not in a ring']\n        mvgd_stats = pd.concat([mvgd_stats, branches_data], axis=1)\n\n    ###################################\n    # Aggregated data of LV Branches\n    if not lv_branches_df.empty:\n        # km of underground cable\n        lv_branches_data = lv_branches_df[lv_branches_df['type_kind'] == 'cable'].groupby(\n            ['grid_id'])['length'].sum().to_frame()\n        lv_branches_data.columns = ['Length of LV underground cables']\n        mvgd_stats = pd.concat([mvgd_stats, lv_branches_data], axis=1)\n\n        # km of overhead lines\n        lv_branches_data = lv_branches_df[lv_branches_df['type_kind'] == 'line'].groupby(\n            ['grid_id'])['length'].sum().to_frame()\n        lv_branches_data.columns = ['Length of LV overhead lines']\n        mvgd_stats = pd.concat([mvgd_stats, lv_branches_data], axis=1)\n\n        # km of different wire types\n        lv_branches_data = lv_branches_df.groupby(\n            ['grid_id', 'type_name'])['length'].sum().to_frame().unstack(level=-1)\n        lv_branches_data.columns = ['Length of LV type ' + _[1] if isinstance(_, tuple) else _\n                                    for _ in lv_branches_data.columns]\n        mvgd_stats = pd.concat([mvgd_stats, lv_branches_data], axis=1)\n\n        # n\u00b0 of branches\n        total_lv_br = lv_branches_df.groupby(['grid_id'])['length'].count().to_frame()\n        total_lv_br.columns = ['N\u00b0 of LV branches']\n        mvgd_stats = pd.concat([mvgd_stats, total_lv_br], axis=1)\n\n    ###################################\n    # Aggregated data of Rings\n    if not ring_df.empty:\n        # N\u00b0 of rings\n        ring_data = ring_df.groupby(['grid_id'])['grid_id'].count().to_frame()\n        ring_data.columns = ['N\u00b0 of MV Rings']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n\n        # min,max,mean km of all rings\n        ring_data = ring_df.groupby(['grid_id'])['ring_length'].min().to_frame()\n        ring_data.columns = ['Length of MV Ring min']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n        ring_data = ring_df.groupby(['grid_id'])['ring_length'].max().to_frame()\n        ring_data.columns = ['Length of MV Ring max']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n        ring_data = ring_df.groupby(['grid_id'])['ring_length'].mean().to_frame()\n        ring_data.columns = ['Length of MV Ring mean']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n\n        # km of all rings\n        ring_data = ring_df.groupby(['grid_id'])['ring_length'].sum().to_frame()\n        ring_data.columns = ['Length of MV Rings total']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n\n        # km of non-ring\n        non_ring_data = branches_df.groupby(['grid_id'])['length'].sum().to_frame()\n        non_ring_data.columns = ['Length of MV Rings total']\n        ring_data = non_ring_data - ring_data\n        ring_data.columns = ['Length of MV Non-Rings total']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data.round(1).abs()], axis=1)\n\n        # rings generation capacity\n        ring_data = ring_df.groupby(['grid_id'])['ring_capacity'].sum().to_frame()\n        ring_data.columns = ['Gen. Cap. Connected to MV Rings']\n        mvgd_stats = pd.concat([mvgd_stats, ring_data], axis=1)\n    ###################################\n    # Aggregated data of Load Areas\n    if not LA_df.empty:\n        LA_data = LA_df.groupby(['grid_id'])['population'].count().to_frame()\n        LA_data.columns = ['N\u00b0 of Load Areas']\n\n        mvgd_stats = pd.concat([mvgd_stats, LA_data], axis=1)\n\n        LA_data = LA_df.groupby(['grid_id'])['population',\n                                             'residential_peak_load',\n                                             'retail_peak_load',\n                                             'industrial_peak_load',\n                                             'agricultural_peak_load',\n                                             'total_peak_load',\n                                             'lv_generation',\n                                             'lv_gens_lvl_6',\n                                             'lv_gens_lvl_7'\n        ].sum()\n        LA_data.columns = ['LA Total Population',\n                           'LA Total LV Peak Load Residential',\n                           'LA Total LV Peak Load Retail',\n                           'LA Total LV Peak Load Industrial',\n                           'LA Total LV Peak Load Agricultural',\n                           'LA Total LV Peak Load total',\n                           'LA Total LV Gen. Cap.',\n                           'Gen. Cap. of LV at v_level 6',\n                           'Gen. Cap. of LV at v_level 7',\n                           ]\n        mvgd_stats = pd.concat([mvgd_stats, LA_data], axis=1)\n\n    ###################################\n    # Aggregated data of Aggregated Load Areas\n    if not LA_df.empty:\n        agg_LA_data = LA_df[LA_df['is_agg']].groupby(\n            ['grid_id'])['population'].count().to_frame()\n        agg_LA_data.columns = ['N\u00b0 of Load Areas - Aggregated']\n        mvgd_stats = pd.concat([mvgd_stats, agg_LA_data], axis=1)\n\n        sat_LA_data = LA_df[LA_df['is_sat']].groupby(\n            ['grid_id'])['population'].count().to_frame()\n        sat_LA_data.columns = ['N\u00b0 of Load Areas - Satellite']\n        mvgd_stats = pd.concat([mvgd_stats, sat_LA_data], axis=1)\n\n        agg_LA_data = LA_df[LA_df['is_agg']].groupby(['grid_id'])['population',\n                                                                  'lv_generation',\n                                                                  'total_peak_load'].sum()\n        agg_LA_data.columns = ['LA Aggregated Population',\n                               'LA Aggregated LV Gen. Cap.', 'LA Aggregated LV Peak Load total'\n                               ]\n        mvgd_stats = pd.concat([mvgd_stats, agg_LA_data], axis=1)\n\n    ###################################\n    mvgd_stats = mvgd_stats.fillna(0)\n    mvgd_stats = mvgd_stats[sorted(mvgd_stats.columns.tolist())]\n    return mvgd_stats", "response": "Calculates statistics about the MV grids in a single network and stores them in a pandas. DataFrame containing several statistical numbers about the MV grids in the MV grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_mvgd_voltage_current_stats(nw):\n    ##############################\n    # close circuit breakers\n    nw.control_circuit_breakers(mode='close')\n    ##############################\n    nodes_idx = 0\n    nodes_dict = {}\n    branches_idx = 0\n    branches_dict = {}\n    for district in nw.mv_grid_districts():\n        # nodes voltage\n        for node in district.mv_grid.graph_nodes_sorted():\n            nodes_idx += 1\n            if hasattr(node, 'voltage_res'):\n                Vres0 = node.voltage_res[0]\n                Vres1 = node.voltage_res[1]\n            else:\n                Vres0 = 'Not available'\n                Vres1 = 'Not available'\n            nodes_dict[nodes_idx] = {'MV_grid_id': district.mv_grid.id_db,\n                                     'node id': node.__repr__(),\n                                     'V_res_0': Vres0,\n                                     'V_res_1': Vres1,\n                                     'V nominal': district.mv_grid.v_level}\n        # branches currents\n        for branch in district.mv_grid.graph_edges():\n            branches_idx += 1\n            if hasattr(branch['branch'], 's_res'):\n                s_res0 = branch['branch'].s_res[0]\n                s_res1 = branch['branch'].s_res[1]\n            else:\n                s_res0 = 'Not available'\n                s_res1 = 'Not available'\n\n            branches_dict[branches_idx] = {\n                'MV_grid_id': district.mv_grid.id_db,\n                'branch id': branch['branch'].__repr__(),  # .id_db\n                's_res_0': s_res0,\n                's_res_1': s_res1,\n                # 'length': branch['branch'].length / 1e3,\n            }\n    nodes_df = pd.DataFrame.from_dict(nodes_dict, orient='index')\n    branches_df = pd.DataFrame.from_dict(branches_dict, orient='index')\n\n    if not nodes_df.empty:\n        nodes_df = nodes_df.set_index('node id')\n        nodes_df = nodes_df.fillna(0)\n        nodes_df = nodes_df[sorted(nodes_df.columns.tolist())]\n        nodes_df.sort_index(inplace=True)\n\n    if not branches_df.empty:\n        branches_df = branches_df.set_index('branch id')\n        branches_df = branches_df.fillna(0)\n        branches_df = branches_df[sorted(branches_df.columns.tolist())]\n        branches_df.sort_index(inplace=True)\n\n    return (nodes_df, branches_df)", "response": "Calculates the MV Voltage and Current Statistics for a given network and returns a pandas. DataFrame containing the voltage statistics for each node in the MVGD and every edge in the MVGD."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calculate_lvgd_voltage_current_stats(nw):\n    ##############################\n    # close circuit breakers\n    nw.control_circuit_breakers(mode='close')\n    ##############################\n    nodes_idx = 0\n    nodes_dict = {}\n    branches_idx = 0\n    branches_dict = {}\n    for mv_district in nw.mv_grid_districts():\n        for LA in mv_district.lv_load_areas():\n            if not LA.is_aggregated:\n                for lv_district in LA.lv_grid_districts():\n                    # nodes voltage\n                    crit_nodes = get_critical_voltage_at_nodes(lv_district.lv_grid)\n                    for node in crit_nodes:\n                        nodes_idx += 1\n                        nodes_dict[nodes_idx] = {\n                            'MV_grid_id': mv_district.mv_grid.id_db,\n                            'LV_grid_id': lv_district.lv_grid.id_db,\n                            'LA_id': LA.id_db,\n                            'node id': node['node'].__repr__(),\n                            'v_diff_0': node['v_diff'][0],\n                            'v_diff_1': node['v_diff'][1],\n                            's_max_0': 'NA',\n                            's_max_1': 'NA',\n                            'V nominal': lv_district.lv_grid.v_level,\n                        }\n                    # branches currents\n                    critical_branches, critical_stations = get_critical_line_loading(lv_district.lv_grid)\n                    for branch in critical_branches:\n                        branches_idx += 1\n                        branches_dict[branches_idx] = {\n                            'MV_grid_id': mv_district.mv_grid.id_db,\n                            'LV_grid_id': lv_district.lv_grid.id_db,\n                            'LA_id': LA.id_db,\n                            'branch id': branch['branch'].__repr__(),\n                            's_max_0': branch['s_max'][0],\n                            's_max_1': branch['s_max'][1],\n                        }\n                    # stations\n                    for node in critical_stations:\n                        nodes_idx += 1\n                        nodes_dict[nodes_idx] = {\n                            'MV_grid_id': mv_district.mv_grid.id_db,\n                            'LV_grid_id': lv_district.lv_grid.id_db,\n                            'LA_id': LA.id_db,\n                            'node id': node['station'].__repr__(),\n                            's_max_0': node['s_max'][0],\n                            's_max_1': node['s_max'][1],\n                            'v_diff_0': 'NA',\n                            'v_diff_1': 'NA',\n                        }\n    nodes_df = pd.DataFrame.from_dict(nodes_dict, orient='index')\n    branches_df = pd.DataFrame.from_dict(branches_dict, orient='index')\n\n    if not nodes_df.empty:\n        nodes_df = nodes_df.set_index('node id')\n        nodes_df = nodes_df.fillna(0)\n        nodes_df = nodes_df[sorted(nodes_df.columns.tolist())]\n        nodes_df.sort_index(inplace=True)\n\n    if not branches_df.empty:\n        branches_df = branches_df.set_index('branch id')\n        branches_df = branches_df.fillna(0)\n        branches_df = branches_df[sorted(branches_df.columns.tolist())]\n        branches_df.sort_index(inplace=True)\n\n    return nodes_df, branches_df", "response": "Calculates LV Voltage and Current Statistics for a given network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a MV grid with ding0 over the selected MV Grid Districts and saves the result in pickle file.", "response": "def init_mv_grid(mv_grid_districts=[3545], filename='ding0_tests_grids_1.pkl'):\n    '''Runs ding0 over the districtis selected in mv_grid_districts\n\n    It also writes the result in filename. If filename = False,\n    then the network is not saved.\n\n    Parameters\n    ----------\n    mv_grid_districts: :any:`list` of :obj:`int`\n        Districts IDs: Defaults to [3545]\n    filename: str\n        Defaults to 'ding0_tests_grids_1.pkl'\n        If filename=False, then the network is not saved\n\n    Returns\n    -------\n    NetworkDing0\n        The created MV network.\n\n    '''\n    print('\\n########################################')\n    print('  Running ding0 for district', mv_grid_districts)\n    # database connection/ session\n    engine = db.connection(section='oedb')\n    session = sessionmaker(bind=engine)()\n\n    # instantiate new ding0 network object\n    nd = NetworkDing0(name='network')\n\n    # run DINGO on selected MV Grid District\n    nd.run_ding0(session=session, mv_grid_districts_no=mv_grid_districts)\n\n    # export grid to file (pickle)\n    if filename:\n        print('\\n########################################')\n        print('  Saving result in ', filename)\n        save_nd_to_pickle(nd, filename=filename)\n\n    print('\\n########################################')\n    return nd"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_stats(mv_districts,\n                  n_of_districts,\n                  source,\n                  mode,\n                  critical,\n                  filename,\n                  output):\n    '''Generates stats dataframes for districts in mv_districts.\n\n    If source=='ding0', then runned districts are saved to a pickle named\n    filename+str(n_of_districts[0])+'_to_'+str(n_of_districts[-1])+'.pkl'\n\n    Parameters\n    ----------\n    districts_list: list of int\n        List with all districts to be run.\n    n_of_districts: int\n        Number of districts to be run in each cluster\n    source: str\n        If 'pkl', pickle files are read.\n        If 'ding0', ding0 is run over the districts.\n    mode: str\n        If 'MV', medium voltage stats are calculated.\n        If 'LV', low voltage stats are calculated.\n        If empty, medium and low voltage stats are calculated.\n    critical: bool\n        If True, critical nodes and branches are returned\n    filename: str\n        filename prefix for saving pickles\n    output:\n        outer variable where the output is stored as a tuple of 6 lists::\n\n        * mv_stats: MV stats DataFrames.\n          If mode=='LV', then DataFrame is empty.\n\n        * lv_stats: LV stats DataFrames.\n          If mode=='MV', then DataFrame is empty.\n\n        * mv_crit_nodes: MV critical nodes stats DataFrames.\n          If mode=='LV', then DataFrame is empty.\n          If critical==False, then DataFrame is empty.\n\n        * mv_crit_edges: MV critical edges stats DataFrames.\n          If mode=='LV', then DataFrame is empty.\n          If critical==False, then DataFrame is empty.\n\n        * lv_crit_nodes: LV critical nodes stats DataFrames.\n          If mode=='MV', then DataFrame is empty.\n          If critical==False, then DataFrame is empty.\n\n        * lv_crit_edges: LV critical edges stats DataFrames.\n          If mode=='MV', then DataFrame is empty.\n          If critical==False, then DataFrame is empty.\n    '''\n    #######################################################################\n    # decide what exactly to do with MV LV\n    if mode == 'MV':\n        calc_mv = True\n        calc_lv = False\n    elif mode == 'LV':\n        calc_mv = False\n        calc_lv = True\n    else:\n        calc_mv = True\n        calc_lv = True\n    #######################################################################\n    clusters = [mv_districts[x:x + n_of_districts] for x in range(0, len(mv_districts), n_of_districts)]\n\n    mv_stats      = []\n    lv_stats      = []\n    mv_crit_nodes = []\n    mv_crit_edges = []\n    lv_crit_nodes = []\n    lv_crit_edges = []\n    #######################################################################\n    for cl in clusters:\n        nw_name = filename + str(cl[0])\n        if not cl[0] == cl[-1]:\n            nw_name = nw_name + '_to_' + str(cl[-1])\n\n        nw = NetworkDing0(name=nw_name)\n        if source == 'pkl':\n            print('\\n########################################')\n            print('  Reading data from pickle district', cl)\n            print('########################################')\n            try:\n                nw = load_nd_from_pickle(nw_name + '.pkl')\n            except Exception:\n                continue\n        else:\n            # database connection/ session\n            engine = db.connection(section='oedb')\n            session = sessionmaker(bind=engine)()\n\n            print('\\n########################################')\n            print('  Running ding0 for district', cl)\n            print('########################################')\n            try:\n                nw.run_ding0(session=session, mv_grid_districts_no=cl)\n                try:\n                    save_nd_to_pickle(nw, filename=nw_name + '.pkl')\n                except Exception:\n                    continue\n            except Exception:\n                continue\n\n            # Close database connection\n        if calc_mv:\n            stats = calculate_mvgd_stats(nw)\n            mv_stats.append(stats)\n        if calc_lv:\n            stats = calculate_lvgd_stats(nw)\n            lv_stats.append(stats)\n        if critical and calc_mv:\n            stats = calculate_mvgd_voltage_current_stats(nw)\n            mv_crit_nodes.append(stats[0])\n            mv_crit_edges.append(stats[1])\n        if critical and calc_lv:\n            stats = calculate_lvgd_voltage_current_stats(nw)\n            lv_crit_nodes.append(stats[0])\n            lv_crit_edges.append(stats[1])\n    #######################################################################\n    salida = (mv_stats, lv_stats, mv_crit_nodes, mv_crit_edges, lv_crit_nodes, lv_crit_edges)\n    output.put(salida)", "response": "Generates stats dataframes for the given set of districts."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parallel_running_stats(districts_list,\n                           n_of_processes,\n                           n_of_districts=1,\n                           source='pkl',\n                           mode='',\n                           critical=False,\n                           save_csv=False,\n                           save_path=''):\n    '''Organize parallel runs of ding0 to calculate stats\n\n    The function take all districts in a list and divide them into\n    n_of_processes parallel processes. For each process, the assigned districts\n    are given to the function process_runs() with arguments n_of_districts,\n    source, mode, and critical\n\n    Parameters\n    ----------\n    districts_list: list of int\n        List with all districts to be run.\n    n_of_processes: int\n        Number of processes to run in parallel\n    n_of_districts: int\n        Number of districts to be run in each cluster given as argument to\n        process_stats()\n    source: str\n        If 'pkl', pickle files are read. Otherwise, ding0 is run over the districts.\n    mode: str\n        If 'MV', medium voltage stats are calculated.\n        If 'LV', low voltage stats are calculated.\n        If empty, medium and low voltage stats are calculated.\n    critical: bool\n        If True, critical nodes and branches are returned\n    path: str\n        path to save the pkl and csv files\n\n    Returns\n    -------\n    DataFrame\n        mv_stats: MV stats in a DataFrame.\n        If mode=='LV', then DataFrame is empty.\n    DataFrame\n        lv_stats: LV stats in a DataFrame.\n        If mode=='MV', then DataFrame is empty.\n    DataFrame\n        mv_crit_nodes: MV critical nodes stats in a DataFrame.\n        If mode=='LV', then DataFrame is empty.\n        If critical==False, then DataFrame is empty.\n    DataFrame\n        mv_crit_edges: MV critical edges stats in a DataFrame.\n        If mode=='LV', then DataFrame is empty.\n        If critical==False, then DataFrame is empty.\n    DataFrame\n        lv_crit_nodes: LV critical nodes stats in a DataFrame.\n        If mode=='MV', then DataFrame is empty.\n        If critical==False, then DataFrame is empty.\n    DataFrame\n        lv_crit_edges: LV critical edges stats in a DataFrame.\n        If mode=='MV', then DataFrame is empty.\n        If critical==False, then DataFrame is empty.\n\n    See Also\n    --------\n    process_stats\n    '''\n    start = time.time()\n\n    nw_name = os.path.join(save_path, 'ding0_grids__')  # name of files prefix\n\n    #######################################################################\n    # Define an output queue\n    output_stats = mp.Queue()\n    #######################################################################\n    # Setup a list of processes that we want to run\n    max_dist = len(districts_list)\n    threat_long = floor(max_dist / n_of_processes)\n\n    if threat_long == 0:\n        threat_long = 1\n\n    threats = [districts_list[x:x + threat_long] for x in\n               range(0, len(districts_list), threat_long)]\n\n    processes = []\n    for districts in threats:\n        args = (districts, n_of_districts, source, mode, critical, nw_name, output_stats)\n        processes.append(mp.Process(target=process_stats, args=args))\n    #######################################################################\n    # Run processes\n    for p in processes:\n        p.start()\n    # Resque output_stats from processes\n    output = [output_stats.get() for p in processes]\n    # Exit the completed processes\n    for p in processes:\n        p.join()\n    #######################################################################\n    # create outputs\n    # Name of files\n    if save_csv:\n        nw_name = nw_name + str(districts_list[0])\n        if not districts_list[0] == districts_list[-1]:\n            nw_name = nw_name + '_to_' + str(districts_list[-1])\n\n    # concatenate all dataframes\n    try:\n        mv_stats = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][0]],\n            axis=0)\n    except:\n        mv_stats = pd.DataFrame.from_dict({})\n    try:\n        lv_stats = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][1]],\n            axis=0)\n    except:\n        lv_stats = pd.DataFrame.from_dict({})\n    try:\n        mv_crit_nodes = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][2]],\n            axis=0)\n    except:\n        mv_crit_nodes = pd.DataFrame.from_dict({})\n    try:\n        mv_crit_edges = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][3]],\n            axis=0)\n    except:\n        mv_crit_edges = pd.DataFrame.from_dict({})\n    try:\n        lv_crit_nodes = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][4]],\n            axis=0)\n    except:\n        lv_crit_nodes = pd.DataFrame.from_dict({})\n    try:\n        lv_crit_edges = pd.concat(\n            [df for p in range(0, len(processes)) for df in output[p][5]],\n            axis=0)\n    except:\n        lv_crit_edges = pd.DataFrame.from_dict({})\n\n    # format concatenated Dataframes\n    if not mv_stats.empty:\n        mv_stats = mv_stats.fillna(0)\n        mv_stats = mv_stats[sorted(mv_stats.columns.tolist())]\n        mv_stats.sort_index(inplace=True)\n        if save_csv:\n            mv_stats.to_csv(nw_name + '_mv_stats.csv')\n\n    if not lv_stats.empty:\n        lv_stats = lv_stats.fillna(0)\n        lv_stats = lv_stats[sorted(lv_stats.columns.tolist())]\n        lv_stats.sort_index(inplace=True)\n        if save_csv:\n            lv_stats.to_csv(nw_name + '_lv_stats.csv')\n\n    if not mv_crit_nodes.empty:\n        mv_crit_nodes = mv_crit_nodes.fillna(0)\n        mv_crit_nodes = mv_crit_nodes[sorted(mv_crit_nodes.columns.tolist())]\n        mv_crit_nodes.sort_index(inplace=True)\n        if save_csv:\n            mv_crit_nodes.to_csv(nw_name + '_mv_crit_nodes.csv')\n\n    if not mv_crit_edges.empty:\n        mv_crit_edges = mv_crit_edges.fillna(0)\n        mv_crit_edges = mv_crit_edges[sorted(mv_crit_edges.columns.tolist())]\n        mv_crit_edges.sort_index(inplace=True)\n        if save_csv:\n            mv_crit_edges.to_csv(nw_name + '_mv_crit_edges.csv')\n\n    if not lv_crit_nodes.empty:\n        lv_crit_nodes = lv_crit_nodes.fillna(0)\n        lv_crit_nodes = lv_crit_nodes[sorted(lv_crit_nodes.columns.tolist())]\n        lv_crit_nodes.sort_index(inplace=True)\n        if save_csv:\n            lv_crit_nodes.to_csv(nw_name + '_lv_crit_nodes.csv')\n\n    if not lv_crit_edges.empty:\n        lv_crit_edges = lv_crit_edges.fillna(0)\n        lv_crit_edges = lv_crit_edges[sorted(lv_crit_edges.columns.tolist())]\n        lv_crit_edges.sort_index(inplace=True)\n        if save_csv:\n            lv_crit_edges.to_csv(nw_name + '_lv_crit_edges.csv')\n\n    #######################################################################\n    print('\\n########################################')\n    print('  Elapsed time for', str(max_dist),\n          'MV grid districts (seconds): {}'.format(time.time() - start))\n    print('\\n########################################')\n    #######################################################################\n    return mv_stats, lv_stats, mv_crit_nodes, mv_crit_edges, lv_crit_nodes, lv_crit_edges", "response": "This function calculates parallel stats for the given list of districts in a single process."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_network(nw, mode=''):\n\n    # close circuit breakers\n    nw.control_circuit_breakers(mode='close')\n    # srid\n    srid = str(int(nw.config['geo']['srid']))\n    ##############################\n    # check what to do\n    lv_info = True\n    mv_info = True\n    if mode == 'LV':\n        mv_info = False\n    if mode == 'MV':\n        lv_info = False\n    ##############################\n    # from datetime import datetime\n    run_id = nw.metadata['run_id']  # datetime.now().strftime(\"%Y%m%d%H%M%S\")\n    ##############################\n    #############################\n    # go through the grid collecting info\n    lvgrid_idx = 0\n    lv_grid_dict = {}\n    lvloads_idx = 0\n    lv_loads_dict = {}\n    mvgrid_idx = 0\n    mv_grid_dict = {}\n    mvloads_idx = 0\n    mv_loads_dict = {}\n    mvgen_idx = 0\n    mv_gen_dict = {}\n    mvcb_idx = 0\n    mvcb_dict = {}\n    mvcd_idx = 0\n    mv_cd_dict = {}\n    mvstations_idx = 0\n    hvmv_stations_dict = {}\n    mvtrafos_idx = 0\n    hvmv_trafos_dict = {}\n    lvgen_idx = 0\n    lv_gen_dict = {}\n    lvcd_idx = 0\n    lv_cd_dict = {}\n    lvstations_idx = 0\n    mvlv_stations_dict = {}\n    lvtrafos_idx = 0\n    mvlv_trafos_dict = {}\n    areacenter_idx = 0\n    areacenter_dict = {}\n    lines_idx = 0\n    lines_dict = {}\n    LVMVmapping_idx = 0\n    mvlv_mapping_dict = {}\n\n    def aggregate_generators(gen, aggr):\n        \"\"\"Aggregate generation capacity per voltage level\n        Parameters\n        ----------\n        gen: ding0.core.GeneratorDing0\n            Ding0 Generator object\n        aggr: dict\n            Aggregated generation capacity. For structure see\n            `_determine_aggregated_nodes()`.\n        Returns\n        -------\n        \"\"\"\n\n        if gen.v_level not in aggr['generation']:\n            aggr['generation'][gen.v_level] = {}\n        if gen.type not in aggr['generation'][gen.v_level]:\n            aggr['generation'][gen.v_level][gen.type] = {}\n        if gen.subtype not in aggr['generation'][gen.v_level][gen.type]:\n            aggr['generation'][gen.v_level][gen.type].update(\n                {gen.subtype: {'ids': [gen.id_db],\n                               'capacity': gen.capacity}})\n        else:\n            aggr['generation'][gen.v_level][gen.type][gen.subtype][\n                'ids'].append(gen.id_db)\n            aggr['generation'][gen.v_level][gen.type][gen.subtype][\n                'capacity'] += gen.capacity\n\n        return aggr\n\n    def aggregate_loads(la_center, aggr):\n        \"\"\"Aggregate consumption in load area per sector\n        Parameters\n        ----------\n        la_center: LVLoadAreaCentreDing0\n            Load area center object from Ding0\n        Returns\n        -------\n        \"\"\"\n        for s in ['retail', 'industrial', 'agricultural', 'residential']:\n            if s not in aggr['load']:\n                aggr['load'][s] = {}\n\n            for t in ['nominal','peak']:\n                if t not in aggr['load'][s]:\n                    aggr['load'][s][t] = 0\n\n        aggr['load']['retail']['nominal'] += sum(\n            [_.sector_consumption_retail\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['industrial']['nominal'] += sum(\n            [_.sector_consumption_industrial\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['agricultural']['nominal'] += sum(\n            [_.sector_consumption_agricultural\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['residential']['nominal'] += sum(\n            [_.sector_consumption_residential\n             for _ in la_center.lv_load_area._lv_grid_districts])\n\n        aggr['load']['retail']['peak'] += sum(\n            [_.peak_load_retail\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['industrial']['peak'] += sum(\n            [_.peak_load_industrial\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['agricultural']['peak'] += sum(\n            [_.peak_load_agricultural\n             for _ in la_center.lv_load_area._lv_grid_districts])\n        aggr['load']['residential']['peak'] += sum(\n            [_.peak_load_residential\n             for _ in la_center.lv_load_area._lv_grid_districts])\n\n        return aggr\n\n    for mv_district in nw.mv_grid_districts():\n\n        mv_grid_id = mv_district.mv_grid.id_db\n        mv_grid_id_db = '_'.join(\n            [str(mv_district.mv_grid.__class__.__name__), 'MV', str(mv_grid_id), str(mv_district.mv_grid.id_db)])\n\n        if mv_info:\n            lv_grid_id = 0\n\n            # MV-grid\n            # ToDo: geom <- Polygon\n            mvgrid_idx += 1\n            mv_grid_dict[mvgrid_idx] = {\n                'MV_grid_id': mv_district.mv_grid.id_db,\n                'id_db': '_'.join([str(mv_district.mv_grid.__class__.__name__), 'MV', str(mv_grid_id),\n                                   str(mv_district.mv_grid.id_db)]),\n                # 'network': mv_district.mv_grid.network,\n                'geom': wkt_dumps(mv_district.geo_data),\n                'population':  # None,\n                    sum([_.zensus_sum\n                         for _ in\n                         mv_district._lv_load_areas  # ding0_grid.grid_district._lv_load_areas\n                         if not np.isnan(_.zensus_sum)]),\n                'voltage_nom': mv_district.mv_grid.v_level,  # in kV\n                'run_id': run_id\n            }\n\n            # id_db: Classname_MV/LV_mvgridid/lvgridid_id\n            # excemptions: class LVStations: LVStationDing0_MV_mvgridid_id(=lvgridid)\n\n            # MVGrid\n            for node in mv_district.mv_grid.graph_nodes_sorted():\n                geom = wkt_dumps(node.geo_data)\n                # geom = from_shape(Point(node.geo_data), srid=srid)\n                db_id = node.id_db\n\n                # LVStation\n                if isinstance(node, LVStationDing0):\n                    if not node.lv_load_area.is_aggregated:\n                        lvstations_idx += 1\n                        mvlv_stations_dict[lvstations_idx] = {\n                            'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                            'LV_grid_id_db': '_'.join(['LVGridDing0', 'LV', str(node.id_db), str(node.id_db)]),\n                            'geom': geom,\n                            'run_id': run_id,\n                        }\n\n                        # LV-MV mapping\n                        LVMVmapping_idx += 1\n                        mvlv_mapping_dict[LVMVmapping_idx] = {\n                            'MV_grid_id': mv_grid_id,\n                            'MV_grid_id_db': mv_grid_id_db,\n                            'LV_grid_id': node.id_db,\n                            'LV_grid_id_db': '_'.join(['LVGridDing0', 'LV', str(node.id_db), str(node.id_db)]),\n                            'run_id': run_id,\n                        }\n\n                        # Trafos LV\n                        for t in node.transformers():\n                            lvtrafos_idx += 1\n                            mvlv_trafos_dict[lvtrafos_idx] = {\n                                'id_db': '_'.join([str(t.__class__.__name__), 'LV', str(mv_grid_id), str(node.id_db)]),\n                                'geom': geom,\n                                'LV_grid_id_db': '_'.join(['LVGridDing0', 'LV', str(node.id_db), str(node.id_db)]),\n                                'voltage_op': t.v_level,\n                                'S_nom': t.s_max_a,\n                                'X': t.x,\n                                'R': t.r,\n                                'run_id': run_id,\n                            }\n\n                # MVStation\n                elif isinstance(node, MVStationDing0):\n                    mvstations_idx += 1\n                    hvmv_stations_dict[mvstations_idx] = {\n                        'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                        'MV_grid_id_db': mv_grid_id_db,\n                        'geom': geom,\n                        'run_id': run_id,\n                    }\n\n                    # Trafos MV\n                    for t in node.transformers():\n                        mvtrafos_idx += 1\n                        hvmv_trafos_dict[mvtrafos_idx] = {\n                            'id_db': '_'.join([str(t.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                            'geom': geom,\n                            'MV_grid_id_db': mv_grid_id_db,\n                            'voltage_op': t.v_level,\n                            'S_nom': t.s_max_a,\n                            'X': t.x,\n                            'R': t.r,\n                            'run_id': run_id,\n                        }\n\n                # MVGenerator\n                elif isinstance(node, GeneratorDing0):\n                    if node.subtype == None:\n                        subtype = 'other'\n                    else:\n                        subtype = node.subtype\n                    type = node.type\n                    mvgen_idx += 1\n                    mv_gen_dict[mvgen_idx] = {\n                        'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                        'MV_grid_id_db': mv_grid_id_db,\n                        'geom': geom,\n                        'type': type,\n                        'subtype': subtype,\n                        'v_level': node.v_level,\n                        'nominal_capacity': node.capacity,\n                        'run_id': run_id,\n                        'is_aggregated': False,\n                    }\n\n                # MVBranchTees\n                elif isinstance(node, MVCableDistributorDing0):\n                    mvcd_idx += 1\n                    mv_cd_dict[mvcd_idx] = {\n                        'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                        'MV_grid_id_db': mv_grid_id_db,\n                        'geom': geom,\n                        'run_id': run_id,\n                    }\n\n                # LoadAreaCentre\n                elif isinstance(node, LVLoadAreaCentreDing0):\n\n                    # type = 'Load area center of aggregated load area'\n\n                    areacenter_idx += 1\n                    aggr_lines = 0\n\n                    aggr = {'generation': {}, 'load': {}, 'aggregates': []}\n\n                    # Determine aggregated generation in LV grid\n                    for lvgd in node.lv_load_area._lv_grid_districts:\n\n                        for aggr_gen in lvgd.lv_grid.generators():\n                            aggr = aggregate_generators(aggr_gen, aggr)\n\n                            if aggr_gen.subtype == None:\n                                subtype = 'other'\n                            else:\n                                subtype = aggr_gen.subtype\n                            type = aggr_gen.type\n\n                    # Determine aggregated load in MV grid\n                    # -> Implement once loads in Ding0 MV grids exist\n\n                    # Determine aggregated load in LV grid\n                    aggr = aggregate_loads(node, aggr)\n\n                    # Collect metadata of aggregated load areas\n                    aggr['aggregates'] = {\n                        'population': node.lv_load_area.zensus_sum,\n                        'geom': node.lv_load_area.geo_area}\n                    aggr_line_type = nw._static_data['MV_cables'].iloc[\n                        nw._static_data['MV_cables']['I_max_th'].idxmax()]\n                    geom = wkt_dumps(node.lv_load_area.geo_area)\n\n                    for aggr_node in aggr:\n                        if aggr_node == 'generation':\n                            mvgenaggr_idx = 0\n\n                            for v_level in aggr['generation']:\n                                for type in aggr['generation'][v_level]:\n                                    for subtype in aggr['generation'][v_level][type]:\n                                        mvgen_idx += 1\n                                        mvgenaggr_idx += 1\n                                        mv_gen_dict[mvgen_idx] = {\n                                            'id_db': '_'.join(\n                                                [str(aggr_gen.__class__.__name__), 'MV', str(mv_grid_id),\n                                                 str(aggr_gen.id_db), str(mvgenaggr_idx)]),  # , str(mvgen_idx)\n                                            'MV_grid_id_db': mv_grid_id_db,\n                                            'geom': geom,#from_shape(Point(mv_district.mv_grid.station().geo_data), srid=srid),#lv_load_area.geo_area,#geom, #?? Polygon # hvmv_stations_dict[mvstations_idx]['geom'], #\n                                            'type': type,\n                                            'subtype': subtype,\n                                            'v_level': v_level,\n                                            'nominal_capacity': aggr['generation'][v_level][type][subtype]['capacity'],\n                                            'is_aggregated': True,\n                                            'run_id': run_id,\n                                        }\n\n                                        lines_idx += 1\n                                        aggr_lines += 1\n                                        lines_dict[lines_idx] = {\n                                            # ToDo: Rename edge_name\n                                            'edge_name': '_'.join(\n                                                [str(mv_grid_id), 'aggr', str(node.lv_load_area.id_db),\n                                                 str(aggr_lines)]),\n                                            # , 'vlevel', str(v_level), 'subtype', str(subtype)]),#}'.format(v_level=v_level, subtype=subtype),\n                                            'grid_id_db': mv_grid_id_db,\n                                            # ToDo: read type_name from aggr_line_type\n                                            'type_name': 'NA2XS2Y 3x1x500 RM/35',  # aggr_line_type.name,\n                                            'type_kind': 'cable',  # branch['branch'].kind,\n                                            'length': 1,\n                                            'U_n': aggr_line_type.U_n,\n                                            'I_max_th': aggr_line_type.I_max_th,\n                                            'R': aggr_line_type.R,\n                                            'L': aggr_line_type.L,\n                                            'C': aggr_line_type.C,\n                                            'node1': '_'.join(\n                                                [str(aggr_gen.__class__.__name__), 'MV', str(mv_grid_id),\n                                                 str(aggr_gen.id_db), str(mvgenaggr_idx)]),\n                                            'node2': '_'.join([\n                                                'MVStationDing0', 'MV', str(mv_grid_id), str(mv_grid_id)]),\n                                            'run_id': run_id,\n                                        }\n\n                        elif aggr_node == 'load':\n                            for type in aggr['load']:\n                                mvloads_idx += 1\n                                mv_loads_dict[mvloads_idx] = {\n                                    'id_db': '_'.join(\n                                        ['AggregatedLoad', 'MV', str(mv_grid_id), str(mvloads_idx)]),\n                                    'MV_grid_id_db': mv_grid_id_db,\n                                    'geom': geom,\n                                    # from_shape(Point(mv_district.mv_grid.station().geo_data), srid=srid),\n                                    'consumption_{}'.format(type): aggr['load'][type]['nominal'],\n                                    'is_aggregated': True,\n                                    'run_id': run_id,\n                                }\n\n                                lines_idx += 1\n                                aggr_lines += 1\n                                lines_dict[lines_idx] = {\n                                    # ToDo: Rename edge_name\n                                    'edge_name': '_'.join(\n                                        [str(mv_grid_id), 'aggr', str(node.lv_load_area.id_db), str(aggr_lines)]),\n                                    # 'edge_name': '_'.join(\n                                    #    ['line_aggr_load', str(node.lv_load_area), 'vlevel', str(v_level),\n                                    #     'subtype', str(subtype)]),  # }'.format(v_level=v_level, subtype=subtype),\n                                    'grid_id_db': mv_grid_id_db,\n                                    # ToDo: read type_name from aggr_line_type\n                                    'type_name': 'NA2XS2Y 3x1x500 RM/35',  # aggr_line_type.name,\n                                    'type_kind': 'cable',  # branch['branch'].kind,\n                                    # 'type': aggr_line_type,\n                                    'length': 1e-3,  # in km\n                                    'U_n': aggr_line_type.U_n,\n                                    'I_max_th': aggr_line_type.I_max_th,\n                                    'R': aggr_line_type.R,\n                                    'L': aggr_line_type.L,\n                                    'C': aggr_line_type.C,\n                                    'node1': '_'.join(\n                                        ['AggregatedLoad', 'MV', str(mv_grid_id), str(mvloads_idx)]),\n                                    'node2': '_'.join([\n                                        'MVStationDing0', 'MV', str(mv_grid_id), str(mv_grid_id)]),\n                                    'run_id': run_id,\n                                }\n\n                    # areacenter_dict[areacenter_idx] = {\n                    #    'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),#node.id_db,\n                    #    'MV_grid_id':node.grid,\n                    #    'geom':node.geo_data,\n                    #    'lv_load_area': node.lv_load_area,\n                    #    'run_id': run_id,#\n\n                    # }\n\n                # DisconnectingPoints\n                elif isinstance(node, CircuitBreakerDing0):\n                    mvcb_idx += 1\n                    mvcb_dict[mvcb_idx] = {\n                        'id_db': '_'.join([str(node.__class__.__name__), 'MV', str(mv_grid_id), str(node.id_db)]),\n                        'MV_grid_id': mv_grid_id,\n                        'MV_grid_id_db': mv_grid_id_db,\n                        'geom': geom,\n                        'status': node.status,\n                        'run_id': run_id,\n                    }\n                else:\n                    type = 'Unknown'\n\n            # MVedges\n            for branch in mv_district.mv_grid.graph_edges():\n                # geom = wkt_dumps(node.geo_data)\n                geom = from_shape(LineString([branch['adj_nodes'][0].geo_data, branch['adj_nodes'][1].geo_data]),\n                                  srid=srid)\n                if not any([isinstance(branch['adj_nodes'][0], LVLoadAreaCentreDing0),\n                            isinstance(branch['adj_nodes'][1], LVLoadAreaCentreDing0)]):\n                    lines_idx += 1\n                    lines_dict[lines_idx] = {\n                        'edge_name': branch['branch'].id_db,\n                        'grid_id_db': mv_grid_id_db,\n                        'type_name': branch['branch'].type['name'],\n                        'type_kind': branch['branch'].kind,\n                        'length': branch['branch'].length / 1e3,\n                        'U_n': branch['branch'].type['U_n'],\n                        'I_max_th': branch['branch'].type['I_max_th'],\n                        'R': branch['branch'].type['R'],\n                        'L': branch['branch'].type['L'],\n                        'C': branch['branch'].type['C'],\n                        'node1': '_'.join([str(branch['adj_nodes'][0].__class__.__name__), 'MV', str(mv_grid_id),\n                                           str(branch['adj_nodes'][0].id_db)]),\n                        'node2': '_'.join([str(branch['adj_nodes'][1].__class__.__name__), 'MV', str(mv_grid_id),\n                                           str(branch['adj_nodes'][1].id_db)]),\n                        'run_id': run_id,\n                    }\n\n        if lv_info:\n            for LA in mv_district.lv_load_areas():\n                for lv_district in LA.lv_grid_districts():\n                    if not lv_district.lv_grid.grid_district.lv_load_area.is_aggregated:\n\n                        # ding0_grid.grid_district._lv_load_areas._lv_grid_districts    _.lv_grid\n                        # LV-grid\n                        # ToDo: geom <- Polygon\n                        lvgrid_idx += 1\n                        lv_grid_dict[lvgrid_idx] = {\n                            'LV_grid_id': lv_district.lv_grid.id_db,\n                            'id_db': '_'.join(\n                                [str(lv_district.lv_grid.__class__.__name__), 'LV', str(lv_district.lv_grid.id_db),\n                                 str(lv_district.lv_grid.id_db)]),\n                            'geom': wkt_dumps(lv_district.geo_data),\n                            'population': lv_district.population,\n                            'voltage_nom': lv_district.lv_grid.v_level / 1e3,\n                            'run_id': run_id\n                        }\n\n                        lv_grid_id = lv_district.lv_grid.id_db\n                        lv_grid_id_db = '_'.join(\n                            [str(lv_district.lv_grid.__class__.__name__), 'LV', str(lv_district.lv_grid.id_db),\n                             str(lv_district.lv_grid.id_db)])\n\n                        # geom = from_shape(Point(lv_district.lv_grid.station().geo_data), srid=srid)\n                        # geom = wkt_dumps(lv_district.geo_data)# lv_grid.station() #ding0_lv_grid.grid_district.geo_data\n                        for node in lv_district.lv_grid.graph_nodes_sorted():\n                            # geom = wkt_dumps(node.geo_data)\n\n                            # LVGenerator\n                            if isinstance(node, GeneratorDing0):\n                                if node.subtype == None:\n                                    subtype = 'other'\n                                else:\n                                    subtype = node.subtype\n                                type = node.type\n                                lvgen_idx += 1\n                                lv_gen_dict[lvgen_idx] = {\n                                    'id_db': '_'.join(\n                                        [str(node.__class__.__name__), 'LV', str(lv_grid_id), str(node.id_db)]),\n                                    'LV_grid_id_db': lv_grid_id_db,\n                                    'geom': wkt_dumps(node.geo_data),\n                                    'type': type,\n                                    'subtype': subtype,\n                                    'v_level': node.v_level,\n                                    'nominal_capacity': node.capacity,\n                                    'run_id': run_id,\n                                }\n\n                            # LVcd\n                            elif isinstance(node, LVCableDistributorDing0):\n                                lvcd_idx += 1\n                                lv_cd_dict[lvcd_idx] = {\n                                    'id_db': '_'.join(\n                                        [str(node.__class__.__name__), 'LV', str(lv_grid_id), str(node.id_db)]),\n                                    'LV_grid_id_db': lv_grid_id_db,\n                                    'geom': None,\n                                    # wkt_dumps(lv_district.geo_data),#wkt_dumps(node.geo_data), Todo: why no geo_data?\n                                    'run_id': run_id,\n                                }\n\n                            # LVload\n                            elif isinstance(node, LVLoadDing0):\n                                consumption_dict = {}\n                                for k in ['residential', 'retail', 'agricultural', 'industrial']:\n                                    if k in node.consumption.keys():\n                                        consumption_dict[k] = node.consumption[k]\n                                    else:\n                                        consumption_dict[k] = None\n                                lvloads_idx += 1\n                                lv_loads_dict[lvloads_idx] = {\n                                    'id_db': '_'.join(\n                                        [str(node.__class__.__name__), 'LV', str(lv_grid_id), str(node.id_db)]),\n                                    'LV_grid_id_db': lv_grid_id_db,\n                                    'geom': None,#wkt_dumps(lv_district.geo_data),#wkt_dumps(node.geo_data), Todo: why no geo_data?\n                                    # 'consumption': json.dumps(node.consumption),\n                                    'consumption_residential': consumption_dict['residential'],\n                                    'consumption_retail': consumption_dict['retail'],\n                                    'consumption_agricultural': consumption_dict['agricultural'],\n                                    'consumption_industrial': consumption_dict['industrial'],\n                                    'run_id': run_id,\n                                }\n                                del consumption_dict\n\n                            else:\n                                type = 'Unknown'\n\n                        # LVedges\n                        for branch in lv_district.lv_grid.graph_edges():\n    #                        geom = from_shape(\n    #                            LineString([branch['adj_nodes'][0].geo_data, branch['adj_nodes'][1].geo_data]), srid=srid)\n                            if not any([isinstance(branch['adj_nodes'][0], LVLoadAreaCentreDing0),\n                                        isinstance(branch['adj_nodes'][1], LVLoadAreaCentreDing0)]):\n                                lines_idx += 1\n                                lines_dict[lines_idx] = {\n                                    'edge_name': branch['branch'].id_db,\n                                    'grid_id_db': lv_grid_id_db,\n                                    'type_name': branch['branch'].type.to_frame().columns[0],\n                                    'type_kind': branch['branch'].kind,\n                                    'length': branch['branch'].length / 1e3,  # length in km\n                                    'U_n': branch['branch'].type['U_n'] / 1e3,  # U_n in kV\n                                    'I_max_th': branch['branch'].type['I_max_th'],\n                                    'R': branch['branch'].type['R'],\n                                    'L': branch['branch'].type['L'],\n                                    'C': branch['branch'].type['C'],\n                                    'node1': '_'.join(\n                                        [str(branch['adj_nodes'][0].__class__.__name__), 'LV', str(lv_grid_id),\n                                         str(branch['adj_nodes'][0].id_db)])\n                                    if not isinstance(branch['adj_nodes'][0], LVStationDing0) else '_'.join(\n                                        [str(branch['adj_nodes'][0].__class__.__name__), 'MV', str(mv_grid_id),\n                                         str(branch['adj_nodes'][0].id_db)]),\n                                    'node2': '_'.join(\n                                        [str(branch['adj_nodes'][1].__class__.__name__), 'LV', str(lv_grid_id),\n                                         str(branch['adj_nodes'][1].id_db)])\n                                    if not isinstance(branch['adj_nodes'][1], LVStationDing0) else '_'.join(\n                                        [str(branch['adj_nodes'][1].__class__.__name__), 'MV', str(mv_grid_id),\n                                         str(branch['adj_nodes'][1].id_db)]),\n                                    'run_id': run_id,\n                                }\n\n    lv_grid       = pd.DataFrame.from_dict(lv_grid_dict, orient='index')\n    lv_gen        = pd.DataFrame.from_dict(lv_gen_dict, orient='index')\n    lv_cd         = pd.DataFrame.from_dict(lv_cd_dict, orient='index')\n    mvlv_stations = pd.DataFrame.from_dict(mvlv_stations_dict, orient='index')\n    mvlv_trafos   = pd.DataFrame.from_dict(mvlv_trafos_dict, orient='index')\n    lv_loads      = pd.DataFrame.from_dict(lv_loads_dict, orient='index')\n    mv_grid       = pd.DataFrame.from_dict(mv_grid_dict, orient='index')\n    mv_gen        = pd.DataFrame.from_dict(mv_gen_dict, orient='index')\n    # mv_cb         = pd.DataFrame.from_dict(mvcb_dict, orient='index')\n    mv_cd         = pd.DataFrame.from_dict(mv_cd_dict, orient='index')\n    hvmv_stations = pd.DataFrame.from_dict(hvmv_stations_dict, orient='index')\n    # mv_areacenter= pd.DataFrame.from_dict(areacenter_dict, orient='index')\n    hvmv_trafos   = pd.DataFrame.from_dict(hvmv_trafos_dict, orient='index')\n    mv_loads      = pd.DataFrame.from_dict(mv_loads_dict, orient='index')\n    lines         = pd.DataFrame.from_dict(lines_dict, orient='index')\n    mvlv_mapping  = pd.DataFrame.from_dict(mvlv_mapping_dict, orient='index')\n\n    lines = lines[sorted(lines.columns.tolist())]\n\n    return run_id, lv_grid, lv_gen, lv_cd, mvlv_stations, mvlv_trafos, lv_loads, mv_grid, mv_gen, mv_cd, \\\n           hvmv_stations, hvmv_trafos, mv_loads, lines, mvlv_mapping", "response": "Exports all nodes and lines of a network nw as DataFrames\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ding0_exemplary_plots(stats, base_path=BASEPATH):\n\n    # make some plot\n    plotpath = os.path.join(base_path, 'plots')\n    results.plot_cable_length(stats, plotpath)\n    plt.show()\n    results.plot_generation_over_load(stats, plotpath)\n    plt.show()", "response": "A function that performs the actual Ding0 exemplary plotting of the grid districts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nd_load_and_stats(filenames, base_path=BASEPATH):\n\n    # load Ding0 data\n    nds = []\n    for filename in filenames:\n        try:\n            nd_load = results.load_nd_from_pickle(filename=\n                                                  os.path.join(base_path,\n                                                               'grids',\n                                                               filename))\n\n            nds.append(nd_load)\n        except:\n            print(\"File {mvgd} not found. It was maybe excluded by Ding0 or \"\n                  \"just forgotten to generate by you...\".format(mvgd=filename))\n\n    nd = nds[0]\n\n    for n in nds[1:]:\n        nd.add_mv_grid_district(n._mv_grid_districts[0])\n\n    # get statistical numbers about grid\n    stats = results.calculate_mvgd_stats(nd)\n\n    return stats", "response": "Loads multiple files from disk and generates stats for each MV grid district."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreinforces cables that are not in the current level.", "response": "def reinforce_branches_current(grid, crit_branches):\n    #TODO: finish docstring\n    \"\"\" Reinforce MV or LV grid by installing a new branch/line type\n    \n    Parameters\n    ----------\n    grid : GridDing0\n        Grid identifier.\n    crit_branches : dict\n        Dict of critical branches with max. relative overloading.\n        \n    Notes\n    -----\n    The branch type to be installed is determined per branch using the rel. overloading. According to [#]_ \n    only cables are installed.\n    \n    References\n    ----------\n    .. [#] Ackermann et al. (RP VNS)    \n    \n    See Also\n    --------\n    ding0.flexopt.check_tech_constraints.check_load :\n    ding0.flexopt.reinforce_measures.reinforce_branches_voltage :\n    \"\"\"\n    # load cable data, file_names and parameter\n    branch_parameters = grid.network.static_data['MV_cables']\n    branch_parameters = branch_parameters[branch_parameters['U_n'] == grid.v_level].sort_values('I_max_th')\n\n    branch_ctr = 0\n\n    for branch, rel_overload in crit_branches.items():\n        try:\n            type = branch_parameters.loc[\n                branch_parameters[\n                    branch_parameters['I_max_th'] >= branch['branch']\n                    .type['I_max_th'] * rel_overload\n                ].loc[\n                    :, 'I_max_th'\n                ].idxmin(), :\n            ]\n            branch['branch'].type = type\n            branch_ctr += 1\n        except:\n            logger.warning('Branch {} could not be reinforced (current '\n                           'issues) as there is no appropriate cable type '\n                           'available. Original type is retained.'.format(\n                branch))\n            pass\n\n    if branch_ctr:\n        logger.info('==> {} branches were reinforced.'.format(str(branch_ctr)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreinforces cables that are at least voltage for a given grid level.", "response": "def reinforce_branches_voltage(grid, crit_branches, grid_level='MV'):\n    #TODO: finish docstring\n    \"\"\" Reinforce MV or LV grid by installing a new branch/line type\n\n    Parameters\n    ----------\n    grid : GridDing0\n        Grid identifier.\n    crit_branches : :any:`list` of :obj:`int`\n        List of critical branches. #TODO: check if a list or a dictionary\n    grid_level : str\n        Specifying either 'MV' for medium-voltage grid or 'LV' for\n        low-voltage grid level.\n        \n    Notes\n    -----\n    The branch type to be installed is determined per branch - the next larger cable available is used.\n    According to Ackermann only cables are installed.\n        \n    See Also\n    --------\n    ding0.flexopt.check_tech_constraints.check_load :\n    ding0.flexopt.reinforce_measures.reinforce_branches_voltage :\n    \"\"\"\n\n    # load cable data, file_names and parameter\n    branch_parameters = grid.network.static_data['{gridlevel}_cables'.format(\n        gridlevel=grid_level)]\n    branch_parameters = branch_parameters[branch_parameters['U_n'] == grid.v_level].sort_values('I_max_th')\n\n    branch_ctr = 0\n\n    for branch in crit_branches:\n        try:\n            type = branch_parameters.loc[\n                branch_parameters.loc[\n                       branch_parameters['I_max_th'] > branch.type['I_max_th']\n                ].loc[\n                    :, 'I_max_th'\n                ].idxmin(), :\n            ]\n            branch.type = type\n            branch_ctr += 1\n        except:\n            logger.warning('Branch {} could not be reinforced (voltage '\n                           'issues) as there is no appropriate cable type '\n                           'available. Original type is retained.'.format(\n                branch))\n            pass\n\n\n    if branch_ctr:\n        logger.info('==> {} branches were reinforced.'.format(str(branch_ctr)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extend_substation(grid, critical_stations, grid_level):\n    load_factor_lv_trans_lc_normal = cfg_ding0.get(\n        'assumptions',\n        'load_factor_lv_trans_lc_normal')\n    load_factor_lv_trans_fc_normal = cfg_ding0.get(\n        'assumptions',\n        'load_factor_lv_trans_fc_normal')\n\n    trafo_params = grid.network._static_data['{grid_level}_trafos'.format(\n        grid_level=grid_level)]\n    trafo_s_max_max = max(trafo_params['S_nom'])\n\n\n    for station in critical_stations:\n        # determine if load or generation case and apply load factor\n        if station['s_max'][0] > station['s_max'][1]:\n            case = 'load'\n            lf_lv_trans_normal = load_factor_lv_trans_lc_normal\n        else:\n            case = 'gen'\n            lf_lv_trans_normal = load_factor_lv_trans_fc_normal\n\n\n        # cumulative maximum power of transformers installed\n        s_max_trafos = sum([_.s_max_a\n                            for _ in station['station']._transformers])\n\n        # determine missing trafo power to solve overloading issue\n        s_trafo_missing = max(station['s_max']) - (\n            s_max_trafos * lf_lv_trans_normal)\n\n        # list of trafos with rated apparent power below `trafo_s_max_max`\n        extendable_trafos = [_ for _ in station['station']._transformers\n                             if _.s_max_a < trafo_s_max_max]\n\n        # try to extend power of existing trafos\n        while (s_trafo_missing > 0) and extendable_trafos:\n            # only work with first of potentially multiple trafos\n            trafo = extendable_trafos[0]\n            trafo_s_max_a_before = trafo.s_max_a\n\n            # extend power of first trafo to next higher size available\n            extend_trafo_power(extendable_trafos, trafo_params)\n\n            # diminish missing trafo power by extended trafo power and update\n            # extendable trafos list\n            s_trafo_missing -= ((trafo.s_max_a * lf_lv_trans_normal) -\n                                trafo_s_max_a_before)\n            extendable_trafos = [_ for _ in station['station']._transformers\n                                 if _.s_max_a < trafo_s_max_max]\n\n        # build new trafos inside station until\n        if s_trafo_missing > 0:\n            trafo_type, trafo_cnt = select_transformers(grid, s_max={\n                's_max': s_trafo_missing,\n                'case': case\n            })\n            \n            # create transformers and add them to station of LVGD\n            for t in range(0, trafo_cnt):\n                lv_transformer = TransformerDing0(\n                    grid=grid,\n                    id_db=id,\n                    v_level=0.4,\n                    s_max_longterm=trafo_type['S_nom'],\n                    r=trafo_type['R'],\n                    x=trafo_type['X'])\n\n                # add each transformer to its station\n                grid._station.add_transformer(lv_transformer)\n\n    logger.info(\"{stations_cnt} have been reinforced due to overloading \"\n                \"issues.\".format(stations_cnt=len(critical_stations)))", "response": "Extend existing substation by exchanging existing trafo and installing parallel one if necessary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extend_substation_voltage(crit_stations, grid_level='LV'):\n    grid = crit_stations[0]['node'].grid\n    trafo_params = grid.network._static_data['{grid_level}_trafos'.format(\n        grid_level=grid_level)]\n    trafo_s_max_max = max(trafo_params['S_nom'])\n    trafo_min_size = trafo_params.loc[trafo_params['S_nom'].idxmin(), :]\n\n    v_diff_max_fc = cfg_ding0.get('assumptions', 'lv_max_v_level_fc_diff_normal')\n    v_diff_max_lc = cfg_ding0.get('assumptions', 'lv_max_v_level_lc_diff_normal')\n\n    tree = nx.dfs_tree(grid._graph, grid._station)\n\n    for station in crit_stations:\n        v_delta = max(station['v_diff'])\n\n        # get list of nodes of main branch in right order\n        extendable_trafos = [_ for _ in station['node']._transformers\n                             if _.s_max_a < trafo_s_max_max]\n\n        v_delta_initially_lc = v_delta[0]\n        v_delta_initially_fc = v_delta[1]\n\n        new_transformers_cnt = 0\n\n        # extend existing trafo power while voltage issues exist and larger trafos\n        # are available\n        while (v_delta[0] > v_diff_max_lc) or (v_delta[1] > v_diff_max_fc):\n            if extendable_trafos:\n                # extend power of first trafo to next higher size available\n                extend_trafo_power(extendable_trafos, trafo_params)\n            elif new_transformers_cnt < 2:\n                # build a new transformer\n                lv_transformer = TransformerDing0(\n                    grid=grid,\n                    id_db=id,\n                    v_level=0.4,\n                    s_max_longterm=trafo_min_size['S_nom'],\n                    r=trafo_min_size['R'],\n                    x=trafo_min_size['X'])\n\n                # add each transformer to its station\n                grid._station.add_transformer(lv_transformer)\n\n                new_transformers_cnt += 1\n\n            # update break criteria\n            v_delta = get_voltage_at_bus_bar(grid, tree)\n            extendable_trafos = [_ for _ in station['node']._transformers\n                                 if _.s_max_a < trafo_s_max_max]\n\n            if (v_delta[0] == v_delta_initially_lc) or (\n                v_delta[1] == v_delta_initially_fc):\n                logger.warning(\"Extension of {station} has no effect on \"\n                               \"voltage delta at bus bar. Transformation power \"\n                               \"extension is halted.\".format(\n                    station=station['node']))\n                break", "response": "Extend substation if voltage issues at MV - LV substation occur."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reinforce_lv_branches_overloading(grid, crit_branches):\n    unsolved_branches = []\n\n    cable_lf = cfg_ding0.get('assumptions',\n                             'load_factor_lv_cable_lc_normal')\n\n    cables = grid.network.static_data['LV_cables']\n\n    # resolve overloading issues for each branch segment\n    for branch in crit_branches:\n        I_max_branch_load = branch['s_max'][0]\n        I_max_branch_gen = branch['s_max'][1]\n        I_max_branch = max([I_max_branch_load, I_max_branch_gen])\n\n        suitable_cables = cables[(cables['I_max_th'] * cable_lf)\n                          > I_max_branch]\n\n        if not suitable_cables.empty:\n            cable_type = suitable_cables.loc[suitable_cables['I_max_th'].idxmin(), :]\n            branch['branch'].type = cable_type\n            crit_branches.remove(branch)\n        else:\n            cable_type_max = cables.loc[cables['I_max_th'].idxmax(), :]\n            unsolved_branches.append(branch)\n            branch['branch'].type = cable_type_max\n            logger.error(\"No suitable cable type could be found for {branch} \"\n                          \"with I_th_max = {current}. \"\n                          \"Cable of type {cable} is chosen during \"\n                          \"reinforcement.\".format(\n                branch=branch['branch'],\n                cable=cable_type_max.name,\n                current=I_max_branch\n            ))\n\n    return unsolved_branches", "response": "Reinforce LV branches with line overloading issues."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextend power of first trafo in list of extendable trafos with rated power below maximum size available.", "response": "def extend_trafo_power(extendable_trafos, trafo_params):\n    \"\"\"\n    Extend power of first trafo in list of extendable trafos\n\n    Parameters\n    ----------\n    extendable_trafos : :any:`list`\n        Trafos with rated power below maximum size available trafo\n    trafo_params : :pandas:`pandas.DataFrame<dataframe>`\n        Transformer parameters\n    \"\"\"\n    trafo = extendable_trafos[0]\n    trafo_s_max_a_before = trafo.s_max_a\n    trafo_nearest_larger = trafo_params.loc[\n        trafo_params.loc[\n            trafo_params['S_nom'] > trafo_s_max_a_before\n        ].loc[\n            :, 'S_nom'\n        ].idxmin(), :\n    ]\n    trafo.s_max_a = trafo_nearest_larger['S_nom']\n    trafo.r = trafo_nearest_larger['R']\n    trafo.x = trafo_nearest_larger['X']"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a generator to the _generators and grid graph if not already existing", "response": "def add_generator(self, generator):\n        \"\"\"Adds a generator to _generators and grid graph if not already existing\n        \n        Parameters\n        ----------\n        generator : GridDing0\n            Description #TODO\n        \n        \"\"\"\n        if generator not in self._generators and isinstance(generator,\n                                                            GeneratorDing0):\n            self._generators.append(generator)\n            self.graph_add_node(generator)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a station or cable distributor object to the grid graph if it does not already exist.", "response": "def graph_add_node(self, node_object):\n        \"\"\"Adds a station or cable distributor object to grid graph if not already existing\n        \n        Parameters\n        ----------\n        node_object : GridDing0\n            Description #TODO\n        \n        \"\"\"\n        if ((node_object not in self._graph.nodes()) and\n            (isinstance(node_object, (StationDing0,\n                                      CableDistributorDing0,\n                                      LVLoadAreaCentreDing0,\n                                      CircuitBreakerDing0,\n                                      GeneratorDing0)))):\n            self._graph.add_node(node_object)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graph_draw(self, mode):\n\n        g = self._graph\n\n        if mode == 'MV':\n            # get draw params from nodes and edges (coordinates, colors, demands, etc.)\n            nodes_pos = {}; demands = {}; demands_pos = {}\n            nodes_color = []\n            for node in g.nodes():\n                if isinstance(node, (StationDing0,\n                                     LVLoadAreaCentreDing0,\n                                     CableDistributorDing0,\n                                     GeneratorDing0,\n                                     CircuitBreakerDing0)):\n                    nodes_pos[node] = (node.geo_data.x, node.geo_data.y)\n                    # TODO: MOVE draw/color settings to config\n                if node == self.station():\n                    nodes_color.append((1, 0.5, 0.5))\n                else:\n                    #demands[node] = 'd=' + '{:.3f}'.format(node.grid.region.peak_load_sum)\n                    #demands_pos[node] = tuple([a+b for a, b in zip(nodes_pos[node], [0.003]*len(nodes_pos[node]))])\n                    nodes_color.append((0.5, 0.5, 1))\n\n            edges_color = []\n            for edge in self.graph_edges():\n                if edge['branch'].critical:\n                    edges_color.append((1, 0, 0))\n                else:\n                    edges_color.append((0, 0, 0))\n\n            plt.figure()\n            nx.draw_networkx(g, nodes_pos, node_color=nodes_color, edge_color=edges_color, font_size=8)\n            #nx.draw_networkx_labels(g, demands_pos, labels=demands, font_size=8)\n            plt.show()\n\n        elif mode == 'LV':\n            nodes_pos = {}\n            nodes_color = []\n\n            for node in g.nodes():\n                # get neighbors of station (=first node of each branch)\n                station_neighbors = sorted(\n                    g.neighbors(self.station()), key=lambda _: repr(_))\n\n                # set x-offset according to count of branches\n                if len(station_neighbors) % 2 == 0:\n                    x_pos_start = -(len(station_neighbors) // 2 - 0.5)\n                else:\n                    x_pos_start = -(len(station_neighbors) // 2)\n\n                # set positions\n                if isinstance(node, CableDistributorDing0):\n                    if node.in_building:\n                        nodes_pos[node] = (x_pos_start + node.branch_no - 1 + 0.25, -node.load_no - 2)\n                        nodes_color.append((0.5, 0.5, 0.5))\n                    else:\n                        nodes_pos[node] = (x_pos_start + node.branch_no - 1, -node.load_no - 2)\n                        nodes_color.append((0.5, 0.5, 0.5))\n\n                elif isinstance(node, LoadDing0):\n                    nodes_pos[node] = (x_pos_start + node.branch_no - 1 + 0.5, -node.load_no - 2 - 0.25)\n                    nodes_color.append((0.5, 0.5, 1))\n                elif isinstance(node, GeneratorDing0):\n                    # get neighbor of geno\n                    neighbor = list(g.neighbors(node))[0]\n\n                    # neighbor is cable distributor of building\n                    if isinstance(neighbor, CableDistributorDing0):\n                        nodes_pos[node] = (x_pos_start + neighbor.branch_no - 1 + 0.5, -neighbor.load_no - 2 + 0.25)\n                    else:\n                        nodes_pos[node] = (1,1)\n\n                    nodes_color.append((0.5, 1, 0.5))\n                elif isinstance(node, StationDing0):\n                    nodes_pos[node] = (0, 0)\n                    nodes_color.append((1, 0.5, 0.5))\n\n            plt.figure()\n            nx.draw_networkx(g, nodes_pos, node_color=nodes_color, font_size=8, node_size=100)\n            plt.show()", "response": "Draws the grid graph using networkx"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graph_nodes_sorted(self):\n        return sorted(self._graph.nodes(), key=lambda _: repr(_))", "response": "Returns an ordered list of graph s nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of nodes that are connected by branch", "response": "def graph_nodes_from_branch(self, branch):\n        \"\"\" Returns nodes that are connected by `branch`\n\n        Args\n        ----\n        branch: BranchDing0\n            Description #TODO\n                \n        Returns\n        -------\n        (:obj:`GridDing0`, :obj:`GridDing0`)\n            2-tuple of nodes (Ding0 objects) #TODO:Check\n        \"\"\"\n        edges = nx.get_edge_attributes(self._graph, 'branch')\n        nodes = list(edges.keys())[list(edges.values()).index(branch)]\n        return nodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of branches that are connected to a node.", "response": "def graph_branches_from_node(self, node):\n        \"\"\" Returns branches that are connected to `node`\n\n        Args\n        ----\n        node: GridDing0\n            Ding0 object (member of graph)\n        \n        Returns\n        -------\n        :any:`list`\n            List of tuples (node in :obj:`GridDing0`, branch in :obj:`BranchDing0`) ::\n            \n                (node , branch_0 ),\n                ...,\n                (node , branch_N ),\n                \n        \"\"\"\n        # TODO: This method can be replaced and speed up by using NetworkX' neighbors()\n\n        branches = []\n        branches_dict = self._graph.adj[node]\n        for branch in branches_dict.items():\n            branches.append(branch)\n        return sorted(branches, key=lambda _: repr(_))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graph_edges(self):\n\n        # get edges with attributes\n        edges = nx.get_edge_attributes(self._graph, 'branch').items()\n\n        # sort them according to connected nodes\n        edges_sorted = sorted(list(edges), key=lambda _: (''.join(sorted([repr(_[0][0]),repr(_[0][1])]))))\n\n        for edge in edges_sorted:\n            yield {'adj_nodes': edge[0], 'branch': edge[1]}", "response": "Returns a generator for iterating over edges of a graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the shortest path from node_source to node_target in _graph using networkx shortest path algorithm.", "response": "def find_path(self, node_source, node_target, type='nodes'):\n        \"\"\"Determines shortest path\n\n        Determines the shortest path from `node_source` to\n        `node_target` in _graph using networkx' shortest path\n        algorithm.\n\n        Args\n        ----\n        node_source: GridDing0\n            source node, member of _graph\n        node_target: GridDing0\n            target node, member of _graph\n        type : str\n            Specify if nodes or edges should be returned. Default\n            is `nodes`\n\n        Returns\n        -------\n        :any:`list` of :obj:`GridDing0`\n            path: shortest path from `node_source` to `node_target` (list of nodes in _graph)\n\n        Notes\n        -----\n        WARNING: The shortest path is calculated using the count of hops, not the actual line lengths!\n        As long as the circuit breakers are open, this works fine since there's only one path. But if\n        they are closed, there are 2 possible paths. The result is a path which have min. count of hops\n        but might have a longer total path length than the second sone.\n        See networkx' function shortest_path() function for details on how the path is calculated.\n        \"\"\"\n        if (node_source in self._graph.nodes()) and (node_target in self._graph.nodes()):\n            path = nx.shortest_path(self._graph, node_source, node_target)\n        else:\n            raise Exception('At least one of the nodes is not a member of graph.')\n        if type == 'nodes':\n            return path\n        elif type == 'edges':\n            return [_ for _ in self._graph.edges(nbunch=path, data=True)\n                    if (_[0] in path and _[1] in path)]\n        else:\n            raise ValueError('Please specify type as nodes or edges')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_and_union_paths(self, node_source, nodes_target):\n        branches = set()\n        for node_target in nodes_target:\n            path = self.find_path(node_source, node_target)\n            node_pairs = list(zip(path[0:len(path) - 1], path[1:len(path)]))\n            for n1, n2 in node_pairs:\n                branches.add(self._graph.adj[n1][n2]['branch'])\n\n        return list(branches)", "response": "Determines shortest paths from node_source to all nodes in node_target in _graph using find_path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the absolute distance between node_source and node_target in meters using find_path and branches length attribute.", "response": "def graph_path_length(self, node_source, node_target):\n        \"\"\" Calculates the absolute distance between `node_source` and `node_target` in meters using find_path() and branches' length attribute.\n\n        Args\n        ----\n        node_source: GridDing0\n            source node, member of _graph\n        node_target: GridDing0\n            target node, member of _graph\n\n        Returns\n        -------\n        float\n            path length in m\n        \"\"\"\n\n        length = 0\n        path = self.find_path(node_source, node_target)\n        node_pairs = list(zip(path[0:len(path)-1], path[1:len(path)]))\n\n        for n1, n2 in node_pairs:\n            length += self._graph.adj[n1][n2]['branch'].length\n\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef graph_isolated_nodes(self):\n        return sorted(nx.isolates(self._graph), key=lambda x: repr(x))", "response": "Returns a list of isolated nodes in the graph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a transformer to the list of transformers.", "response": "def add_transformer(self, transformer):\n        \"\"\"Adds a transformer to _transformers if not already existing\n        \n        Args\n        ----\n        transformer : StationDing0\n            Description #TODO\n        \"\"\"\n        if transformer not in self.transformers() and isinstance(transformer, TransformerDing0):\n            self._transformers.append(transformer)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields all branches in the ring.", "response": "def branches(self):\n        \"\"\" #TODO: description\n        \"\"\"\n        for branch in self._grid.graph_edges():\n            if branch['branch'].ring == self:\n                yield branch"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lv_load_areas(self):\n        for lv_load_area in self._grid._graph.nodes():\n            if isinstance(lv_load_area, LVLoadAreaDing0):\n                if lv_load_area.ring == self:\n                    yield lv_load_area", "response": "Iterate over all LV load areas in the grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nopens a Circuit Breaker", "response": "def open(self):\n        \"\"\" Open a Circuit Breaker #TODO Check\n        \"\"\"\n        self.branch_nodes = self.grid.graph_nodes_from_branch(self.branch)\n        self.grid._graph.remove_edge(self.branch_nodes[0], self.branch_nodes[1])\n        self.status = 'open'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosing a Circuit Breaker", "response": "def close(self):\n        \"\"\" Close a Circuit Breaker #TODO Check\n        \"\"\"\n        self.grid._graph.add_edge(self.branch_nodes[0], self.branch_nodes[1], branch=self.branch)\n        self.status = 'closed'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a generator for iterating over load_areas", "response": "def lv_load_areas(self):\n        \"\"\"Returns a generator for iterating over load_areas\n        \n        Yields\n        ------\n        int\n            generator for iterating over load_areas\n        \"\"\"\n        for load_area in sorted(self._lv_load_areas, key=lambda _: repr(_)):\n            yield load_area"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_lv_load_area(self, lv_load_area):\n        if lv_load_area not in self.lv_load_areas() and isinstance(lv_load_area, LVLoadAreaDing0):\n            self._lv_load_areas.append(lv_load_area)\n            self.mv_grid.graph_add_node(lv_load_area.lv_load_area_centre)", "response": "Adds a LV Load Area to MV grid s _graph if not already existing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_lv_load_area_group(self, lv_load_area_group):\n        if lv_load_area_group not in self.lv_load_area_groups():\n            self._lv_load_area_groups.append(lv_load_area_group)", "response": "Adds a LV load_area to _lv_load_areas if not already existing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsummarize peak loads of underlying load areas in kVA.", "response": "def add_peak_demand(self):\n        \"\"\"Summarizes peak loads of underlying load_areas in kVA.\n        \n        (peak load sum and peak load of satellites)\n        \"\"\"\n        peak_load = peak_load_satellites = 0\n        for lv_load_area in self.lv_load_areas():\n            peak_load += lv_load_area.peak_load\n            if lv_load_area.is_satellite:\n                peak_load_satellites += lv_load_area.peak_load\n        self.peak_load = peak_load\n        self.peak_load_satellites = peak_load_satellites"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_aggregated_peak_demand(self):\n        peak_load_aggregated = 0\n        for lv_load_area in self.lv_load_areas():\n            if lv_load_area.is_aggregated:\n                peak_load_aggregated += lv_load_area.peak_load\n        self.peak_load_aggregated = peak_load_aggregated", "response": "Summarizes peak loads of underlying aggregated load_areas"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lv_grid_districts(self):\n        for lv_grid_district in sorted(self._lv_grid_districts, key=lambda _: repr(_)):\n            yield lv_grid_district", "response": "Returns a generator for iterating over LV grid districts\n        \n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_lv_grid_district(self, lv_grid_district):\n        # TODO: check docstring\n        \"\"\"Adds a LV grid district to _lv_grid_districts if not already existing\n        \n        Args\n        ----\n        lv_grid_district: :shapely:`Shapely Polygon object<polygons>`\n            Descr\n        \"\"\"\n\n        if lv_grid_district not in self._lv_grid_districts and \\\n                isinstance(lv_grid_district, LVGridDistrictDing0):\n            self._lv_grid_districts.append(lv_grid_district)", "response": "Adds a LV grid district to the list of LV grid districts."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef peak_generation(self):\n        cum_peak_generation = 0\n\n        for lv_grid_district in self._lv_grid_districts:\n            cum_peak_generation += lv_grid_district.lv_grid.station().peak_generation\n\n        return cum_peak_generation", "response": "Cumulative peak generation of generators connected to LV grids of \n        underlying LVGDs\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget pair description Parameters ---------- pair : :any:`list` of nodes Descr Returns ------- type Descr", "response": "def get_pair(self, pair):\n        \"\"\"get pair description\n        \n        Parameters\n        ----------\n        pair : :any:`list` of nodes\n            Descr\n            \n        Returns\n        -------\n        type\n            Descr\n        \"\"\"\n        i, j = pair\n        return (self._nodes[i.name()], self._nodes[j.name()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if this solution is complete.", "response": "def is_complete(self):\n        \"\"\"Returns True if this is a complete solution, i.e, all nodes are allocated\n        \n        Returns\n        -------\n        bool\n            True if all nodes are llocated.\n        \"\"\"\n        return all(\n            [node.route_allocation() is not None for node in list(self._nodes.values()) if node != self._problem.depot()]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a deep copy of self", "response": "def clone(self):\n        \"\"\"Returns a deep copy of self\n\n        Function clones:\n        \n        * route\n        * allocation\n        * nodes\n        \n        Returns\n        -------\n        type\n            Deep copy of self\n        \"\"\"\n\n        new_solution = self.__class__(self._problem, len(self._routes))\n\n        # Clone routes\n        for index, r in enumerate(self._routes):\n            new_route = new_solution._routes[index]\n            for node in r.nodes():\n                # Insere new node on new route\n                new_node = new_solution._nodes[node]\n                new_route.allocate([new_node])\n\n        return new_solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef length(self):\n        length = 0\n        for r in self._routes:\n            length = length + r.length()\n\n        return length", "response": "Returns the solution length or cost."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the solution s network using networkx.", "response": "def draw_network(self, anim):\n        \"\"\"Draws solution's graph using networkx\n        \n        Parameters\n        ----------\n        AnimationDing0\n            AnimationDing0 object\n            \n        \"\"\"\n\n        g = nx.Graph()\n        ntemp = []\n        nodes_pos = {}\n        demands = {}\n        demands_pos = {}\n        for no, node in self._nodes.items():\n            g.add_node(node)\n            ntemp.append(node)\n            coord = self._problem._coord[no]\n            nodes_pos[node] = tuple(coord)\n            demands[node] = 'd=' + str(node.demand())\n            demands_pos[node] = tuple([a+b for a, b in zip(coord, [2.5]*len(coord))])\n\n        depot = self._nodes[self._problem._depot._name]\n        for r in self.routes():\n            n1 = r._nodes[0:len(r._nodes)-1]\n            n2 = r._nodes[1:len(r._nodes)]\n            e = list(zip(n1, n2))\n            e.append((depot, r._nodes[0]))\n            e.append((r._nodes[-1], depot))\n            g.add_edges_from(e)\n\n        plt.figure()\n        ax = plt.gca()\n        ax.get_xaxis().set_visible(False)\n        ax.get_yaxis().set_visible(False)\n\n        if anim is not None:\n            nx.draw_networkx(g, nodes_pos, with_labels=False, node_size=50)\n            plt.savefig(anim.file_path +\n                        anim.file_prefix +\n                        (4 - len(str(anim.counter))) * '0' +\n                        str(anim.counter) + '.png',\n                        dpi=150,\n                        bbox_inches='tight')\n            anim.counter += 1\n            plt.close()\n        else:\n            nx.draw_networkx(g, nodes_pos)\n            nx.draw_networkx_labels(g, demands_pos, labels=demands)\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the geographical branches in a given polygon.", "response": "def calc_geo_branches_in_polygon(mv_grid, polygon, mode, proj):\n    \"\"\" Calculate geographical branches in polygon.\n\n    For a given `mv_grid` all branches (edges in the graph of the grid) are\n    tested if they are in the given `polygon`. You can choose different modes\n    and projections for this operation.\n\n    Parameters\n    ----------\n    mv_grid : MVGridDing0\n        MV Grid object. Edges contained in `mv_grid.graph_edges()` are taken\n        for the test.\n    polygon : :shapely:`Shapely Point object<points>`\n        Polygon that contains edges.\n    mode : str\n        Choose between 'intersects' or 'contains'.\n    proj : int\n        EPSG code to specify projection\n\n    Returns\n    -------\n    :any:`list` of :any:`BranchDing0` objects\n        List of branches\n\n    \"\"\"\n\n    branches = []\n    polygon_shp = transform(proj, polygon)\n    for branch in mv_grid.graph_edges():\n        nodes = branch['adj_nodes']\n        branch_shp = transform(proj, LineString([nodes[0].geo_data, nodes[1].geo_data]))\n\n        # check if branches intersect with polygon if mode = 'intersects'\n        if mode == 'intersects':\n            if polygon_shp.intersects(branch_shp):\n                branches.append(branch)\n        # check if polygon contains branches if mode = 'contains'\n        elif mode == 'contains':\n            if polygon_shp.contains(branch_shp):\n                branches.append(branch)\n        # error\n        else:\n            raise ValueError('Mode is invalid!')\n    return branches"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate branches in nodes associated graph that are at least partly within buffer of radius from node.", "response": "def calc_geo_branches_in_buffer(node, mv_grid, radius, radius_inc, proj):\n    \"\"\" Determines branches in nodes' associated graph that are at least partly\n    within buffer of `radius` from `node`.\n    \n    If there are no nodes, the buffer is successively extended by `radius_inc`\n    until nodes are found.\n\n    Parameters\n    ----------\n    node : LVStationDing0, GeneratorDing0, or CableDistributorDing0\n        origin node (e.g. LVStationDing0 object) with associated shapely object\n        (attribute `geo_data`) in any CRS (e.g. WGS84)\n    radius : float\n        buffer radius in m\n    radius_inc : float\n        radius increment in m\n    proj : int\n        pyproj projection object: nodes' CRS to equidistant CRS\n        (e.g. WGS84 -> ETRS)\n\n    Returns\n    -------\n    :any:`list` of :networkx:`NetworkX Graph Obj< >`\n        List of branches (NetworkX branch objects)\n\n    \"\"\"\n\n    branches = []\n\n    while not branches:\n        node_shp = transform(proj, node.geo_data)\n        buffer_zone_shp = node_shp.buffer(radius)\n        for branch in mv_grid.graph_edges():\n            nodes = branch['adj_nodes']\n            branch_shp = transform(proj, LineString([nodes[0].geo_data, nodes[1].geo_data]))\n            if buffer_zone_shp.intersects(branch_shp):\n                branches.append(branch)\n        radius += radius_inc\n\n    return branches"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_geo_dist_vincenty(node_source, node_target):\n\n    branch_detour_factor = cfg_ding0.get('assumptions', 'branch_detour_factor')\n\n    # notice: vincenty takes (lat,lon)\n    branch_length = branch_detour_factor * vincenty((node_source.geo_data.y, node_source.geo_data.x),\n                                                    (node_target.geo_data.y, node_target.geo_data.x)).m\n\n    # ========= BUG: LINE LENGTH=0 WHEN CONNECTING GENERATORS ===========\n    # When importing generators, the geom_new field is used as position. If it is empty, EnergyMap's geom\n    # is used and so there are a couple of generators at the same position => length of interconnecting\n    # line is 0. See issue #76\n    if branch_length == 0:\n        branch_length = 1\n        logger.warning('Geo distance is zero, check objects\\' positions. '\n                       'Distance is set to 1m')\n    # ===================================================================\n\n    return branch_length", "response": "Calculates the geodesic distance between node_source and node_target."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calc_geo_dist_matrix_vincenty(nodes_pos):\n\n    branch_detour_factor = cfg_ding0.get('assumptions', 'branch_detour_factor')\n\n    matrix = {}\n\n    for i in nodes_pos:\n        pos_origin = tuple(nodes_pos[i])\n\n        matrix[i] = {}\n\n        for j in nodes_pos:\n            pos_dest = tuple(nodes_pos[j])\n            # notice: vincenty takes (lat,lon), thus the (x,y)/(lon,lat) tuple is reversed\n            distance = branch_detour_factor * vincenty(tuple(reversed(pos_origin)), tuple(reversed(pos_dest))).km\n            matrix[i][j] = distance\n\n    return matrix", "response": "Calculates the geodesic distance between all nodes in nodes_pos incorporating the vincenty function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef calc_geo_centre_point(node_source, node_target):\n\n    proj_source = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n\n    # ETRS (equidistant) to WGS84 (conformal) projection\n    proj_target = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:3035'),  # source coordinate system\n            pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n\n    branch_shp = transform(proj_source, LineString([node_source.geo_data, node_target.geo_data]))\n\n    distance = vincenty((node_source.geo_data.y, node_source.geo_data.x),\n                        (node_target.geo_data.y, node_target.geo_data.x)).m\n\n    centre_point_shp = transform(proj_target, branch_shp.interpolate(distance/2))\n\n    return centre_point_shp", "response": "Calculates the geodesic distance between node_source and node_target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch all branches for nearest possible connection objects.", "response": "def find_nearest_conn_objects(node_shp, branches, proj, conn_dist_weight, debug, branches_only=False):\n    \"\"\"Searches all `branches` for the nearest possible connection object per branch.\n    \n    Picks out 1 object out of 3 possible objects: \n    \n    * 2 branch-adjacent stations and\n    * 1 potentially created cable distributor on the line (perpendicular projection)).\n    \n    The resulting stack (list) is sorted ascending by distance from node.\n\n    Args\n    ----\n    node_shp: :shapely:`Shapely Point object<points>`\n        Shapely Point object of node\n    branches: BranchDing0\n        BranchDing0 objects of MV region\n    proj: :pyproj:`pyproj Proj object< >`\n        nodes' CRS to equidistant CRS (e.g. WGS84 -> ETRS)\n    conn_dist_weight: float\n        length weighting to prefer stations instead of direct line connection.\n    debug: bool\n        If True, information is printed during process\n    branches_only: bool, defaults to False\n        If True, only branch objects are considered as connection objects\n\n    Returns\n    -------\n    :any:`list` \n        List of connection objects. \n        Each object is represented by dict with Ding0 object,\n        shapely object, and distance to node.\n    \n    See Also\n    --------\n    mv_connect_satellites : for details on `conn_dist_weight` param\n    \"\"\"\n\n    # threshold which is used to determine if 2 objects are on the same position (see below for details on usage)\n    conn_diff_tolerance = cfg_ding0.get('mv_routing', 'conn_diff_tolerance')\n\n    conn_objects_min_stack = []\n\n    for branch in branches:\n        stations = branch['adj_nodes']\n\n        # create shapely objects for 2 stations and line between them, transform to equidistant CRS\n        station1_shp = transform(proj, stations[0].geo_data)\n        station2_shp = transform(proj, stations[1].geo_data)\n        line_shp = LineString([station1_shp, station2_shp])\n\n        # create dict with DING0 objects (line & 2 adjacent stations), shapely objects and distances\n        if not branches_only:\n            conn_objects = {'s1': {'obj': stations[0],\n                                   'shp': station1_shp,\n                                   'dist': node_shp.distance(station1_shp) * conn_dist_weight * 0.999},\n                            's2': {'obj': stations[1],\n                                   'shp': station2_shp,\n                                   'dist': node_shp.distance(station2_shp) * conn_dist_weight * 0.999},\n                            'b': {'obj': branch,\n                                  'shp': line_shp,\n                                  'dist': node_shp.distance(line_shp)}}\n\n            # Remove branch from the dict of possible conn. objects if it is too close to a node.\n            # Without this solution, the target object is not unique for different runs (and so\n            # were the topology)\n            if (\n                    abs(conn_objects['s1']['dist'] - conn_objects['b']['dist']) < conn_diff_tolerance\n                 or abs(conn_objects['s2']['dist'] - conn_objects['b']['dist']) < conn_diff_tolerance\n               ):\n                del conn_objects['b']\n\n            # remove MV station as possible connection point\n            if isinstance(conn_objects['s1']['obj'], MVStationDing0):\n                del conn_objects['s1']\n            elif isinstance(conn_objects['s2']['obj'], MVStationDing0):\n                del conn_objects['s2']\n\n        else:\n            conn_objects = {'b': {'obj': branch,\n                                  'shp': line_shp,\n                                  'dist': node_shp.distance(line_shp)}}\n\n\n\n        # find nearest connection point on given triple dict (2 branch-adjacent stations + cable dist. on line)\n        conn_objects_min = min(conn_objects.values(), key=lambda v: v['dist'])\n        #if not branches_only:\n        #    conn_objects_min_stack.append(conn_objects_min)\n        #elif isinstance(conn_objects_min['shp'], LineString):\n        #    conn_objects_min_stack.append(conn_objects_min)\n        conn_objects_min_stack.append(conn_objects_min)\n\n    # sort all objects by distance from node\n    conn_objects_min_stack = [_ for _ in sorted(conn_objects_min_stack, key=lambda x: x['dist'])]\n\n    if debug:\n        logger.debug('Stack length: {}'.format(len(conn_objects_min_stack)))\n\n    return conn_objects_min_stack"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfunctioning searches through the list of possible target connection objects in conn_objects_min_stack and tries to connect to one of them.", "response": "def find_connection_point(node, node_shp, graph, proj, conn_objects_min_stack, conn_dist_ring_mod, debug):\n    \"\"\" Goes through the possible target connection objects in `conn_objects_min_stack` (from nearest to most far\n        object) and tries to connect `node` to one of them.\n\n    Function searches from nearest to most far object.\n    \n    Args\n    ----\n    node: LVLoadAreaCentreDing0, i.e.\n        Origin node - Ding0 graph object (e.g. LVLoadAreaCentreDing0)\n    node_shp: :shapely:`Shapely Point object<points>`\n        Shapely Point object of node\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes\n    proj: :pyproj:`pyproj Proj object< >`\n        equidistant CRS to conformal CRS (e.g. ETRS -> WGS84)\n    conn_objects_min_stack: list\n        List of connection objects. \n        \n        Each object is represented by dict with Ding0 object, shapely object,\n        and distance to node, sorted ascending by distance.\n    conn_dist_ring_mod: type\n        Max. distance when nodes are included into route instead of creating a \n        new line.\n    debug: bool\n        If True, information is printed during process\n\n    See Also\n    --------\n    ding0.grid.mv_grid.mv_connect : for details on the `conn_dist_ring_mod` parameter.\n    \"\"\"\n\n    node_connected = False\n\n    # go through the stack (from nearest to most far connection target object)\n    for dist_min_obj in conn_objects_min_stack:\n\n        nodes_are_members_of_ring = False\n\n        # target object is branch\n        if isinstance(dist_min_obj['shp'], LineString):\n            # rename for readability\n            node1 = dist_min_obj['obj']['adj_nodes'][0]\n            node2 = dist_min_obj['obj']['adj_nodes'][1]\n\n            lv_load_area_group = get_lv_load_area_group_from_node_pair(node1, node2)\n\n            # check if target branch belongs to a main ring\n            nodes_are_members_of_ring = any(node1 in ring and node2 in ring for ring in node.grid.rings_nodes())\n            branch_ring = dist_min_obj['obj']['branch'].ring\n\n        # target object is node\n        else:\n            if isinstance(dist_min_obj['obj'], MVCableDistributorDing0):\n                lv_load_area_group = dist_min_obj['obj'].lv_load_area_group\n            else:\n                lv_load_area_group = dist_min_obj['obj'].lv_load_area.lv_load_area_group\n\n        # target object doesn't belong to a satellite string (is not a member of a Load Area group)\n        if not lv_load_area_group:\n\n            # connect node\n            target_obj_result = connect_node(node,\n                                             node_shp,\n                                             node.lv_load_area.mv_grid_district.mv_grid,\n                                             dist_min_obj,\n                                             proj,\n                                             graph,\n                                             conn_dist_ring_mod,\n                                             debug)\n\n            # if node was connected via branch (target line not re-routed and not member of aggregated load area):\n            # create new LV load_area group for current node\n            if (target_obj_result is not None) and (target_obj_result != 're-routed'):\n                lv_load_area_group = LoadAreaGroupDing0(mv_grid_district=node.lv_load_area.mv_grid_district,\n                                                        root_node=target_obj_result)\n                lv_load_area_group.add_lv_load_area(lv_load_area=node.lv_load_area)\n                node.lv_load_area.lv_load_area_group = lv_load_area_group\n                node.lv_load_area.mv_grid_district.add_lv_load_area_group(lv_load_area_group)\n\n                if debug:\n                    logger.debug('New LV load_area group {} created!'.format(\n                        lv_load_area_group))\n\n                # node connected, stop connection for current node\n                node_connected = True\n                break\n\n            # node was inserted into line (target line was re-routed)\n            elif target_obj_result == 're-routed':\n\n                # if main ring was re-routed to include node => node is not a satellite anymore\n                if nodes_are_members_of_ring:\n                    node.lv_load_area.is_satellite = False\n                    node.lv_load_area.ring = branch_ring\n\n                # node connected, stop connection for current node\n                node_connected = True\n                break\n\n        # target object is member of a Load Area group\n        else:\n\n            # connect node\n            target_obj_result = connect_node(node,\n                                             node_shp,\n                                             node.lv_load_area.mv_grid_district.mv_grid,\n                                             dist_min_obj,\n                                             proj,\n                                             graph,\n                                             conn_dist_ring_mod,\n                                             debug)\n\n            # if node was connected via branch (target line not re-routed and not member of aggregated load area):\n            # create new LV load_area group for current node\n            if (target_obj_result is not None) and (target_obj_result != 're-routed'):\n                # node can join LV load_area group\n                if lv_load_area_group.can_add_lv_load_area(node=node):\n\n                    # add node to LV load_area group\n                    lv_load_area_group.add_lv_load_area(lv_load_area=node.lv_load_area)\n                    node.lv_load_area.lv_load_area_group = lv_load_area_group\n\n                    if isinstance(target_obj_result, MVCableDistributorDing0):\n                        lv_load_area_group.add_lv_load_area(lv_load_area=target_obj_result)\n                        target_obj_result.lv_load_area_group = lv_load_area_group\n\n                    if debug:\n                        logger.debug('LV load_area group {} joined!'.format(\n                            lv_load_area_group))\n\n                    # node connected, stop connection for current node\n                    node_connected = True\n                    break\n\n                # cannot join LV load_area group\n                else:\n                    if debug:\n                        logger.debug('Node {0} could not be added to '\n                                     'load_area group {1}'.format(\n                            node, lv_load_area_group))\n\n                    # rollback changes in graph\n                    disconnect_node(node, target_obj_result, graph, debug)\n\n                    # continue with next possible connection point\n                    continue\n\n            # node was inserted into line (target line was re-routed)\n            elif target_obj_result == 're-routed':\n                # add node to LV load_area group\n                lv_load_area_group.add_lv_load_area(lv_load_area=node.lv_load_area)\n                node.lv_load_area.lv_load_area_group = lv_load_area_group\n\n                # if main ring was re-routed to include node => node is not a satellite anymore\n                if nodes_are_members_of_ring:\n                    node.lv_load_area.is_satellite = False\n                    node.lv_load_area.ring = branch_ring\n\n                # node inserted into existing route, stop connection for current node\n                node_connected = True\n                break\n\n            # else: node could not be connected because target object belongs to load area of aggregated type\n\n    if not node_connected and debug:\n        logger.debug(\n            'Node {} could not be connected, try to increase the parameter '\n            '`load_area_sat_buffer_radius` in config file `config_calc.cfg` '\n            'to gain more possible connection points.'.format(node))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconnect a node to a MV grid.", "response": "def connect_node(node, node_shp, mv_grid, target_obj, proj, graph, conn_dist_ring_mod, debug):\n    \"\"\" Connects `node` to `target_obj`.\n\n    Args\n    ----\n    node: LVLoadAreaCentreDing0, i.e.\n        Origin node - Ding0 graph object (e.g. LVLoadAreaCentreDing0)\n    node_shp: :shapely:`Shapely Point object<points>`\n        Shapely Point object of origin node\n    target_obj: type\n        object that node shall be connected to\n    proj: :pyproj:`pyproj Proj object< >`\n        equidistant CRS to conformal CRS (e.g. ETRS -> WGS84)\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes and newly created branches\n    conn_dist_ring_mod: float\n        Max. distance when nodes are included into route instead of creating a \n        new line.\n    debug: bool\n        If True, information is printed during process.\n\n    Returns\n    -------\n    :obj:`LVLoadAreaCentreDing0`\n        object that node was connected to.\n        \n        (instance of :obj:`LVLoadAreaCentreDing0` or :obj:`MVCableDistributorDing0`.\n        \n        If node is included into line instead of creating a new line (see arg\n        `conn_dist_ring_mod`), `target_obj_result` is None.\n                           \n    See Also\n    --------\n    ding0.grid.mv_grid.mv_connect : for details on the `conn_dist_ring_mod` parameter.\n    \"\"\"\n\n    target_obj_result = None\n\n    # MV line is nearest connection point\n    if isinstance(target_obj['shp'], LineString):\n\n        adj_node1 = target_obj['obj']['adj_nodes'][0]\n        adj_node2 = target_obj['obj']['adj_nodes'][1]\n\n        # find nearest point on MV line\n        conn_point_shp = target_obj['shp'].interpolate(target_obj['shp'].project(node_shp))\n        conn_point_shp = transform(proj, conn_point_shp)\n\n        # target MV line does currently not connect a load area of type aggregated\n        if not target_obj['obj']['branch'].connects_aggregated:\n\n            # Node is close to line\n            # -> insert node into route (change existing route)\n            if (target_obj['dist'] < conn_dist_ring_mod):\n                # backup kind and type of branch\n                branch_type = graph.adj[adj_node1][adj_node2]['branch'].type\n                branch_kind = graph.adj[adj_node1][adj_node2]['branch'].kind\n                branch_ring = graph.adj[adj_node1][adj_node2]['branch'].ring\n\n                # check if there's a circuit breaker on current branch,\n                # if yes set new position between first node (adj_node1) and newly inserted node\n                circ_breaker = graph.adj[adj_node1][adj_node2]['branch'].circuit_breaker\n                if circ_breaker is not None:\n                    circ_breaker.geo_data = calc_geo_centre_point(adj_node1, node)\n\n                # split old ring main route into 2 segments (delete old branch and create 2 new ones\n                # along node)\n                graph.remove_edge(adj_node1, adj_node2)\n\n                branch_length = calc_geo_dist_vincenty(adj_node1, node)\n                branch = BranchDing0(length=branch_length,\n                                     circuit_breaker=circ_breaker,\n                                     kind=branch_kind,\n                                     type=branch_type,\n                                     ring=branch_ring)\n                if circ_breaker is not None:\n                    circ_breaker.branch = branch\n                graph.add_edge(adj_node1, node, branch=branch)\n\n                branch_length = calc_geo_dist_vincenty(adj_node2, node)\n                graph.add_edge(adj_node2, node, branch=BranchDing0(length=branch_length,\n                                                                   kind=branch_kind,\n                                                                   type=branch_type,\n                                                                   ring=branch_ring))\n\n                target_obj_result = 're-routed'\n\n                if debug:\n                    logger.debug('Ring main route modified to include '\n                                 'node {}'.format(node))\n\n            # Node is too far away from route\n            # => keep main route and create new line from node to (cable distributor on) route.\n            else:\n\n                # create cable distributor and add it to grid\n                cable_dist = MVCableDistributorDing0(geo_data=conn_point_shp,\n                                                     grid=mv_grid)\n                mv_grid.add_cable_distributor(cable_dist)\n\n                # check if there's a circuit breaker on current branch,\n                # if yes set new position between first node (adj_node1) and newly created cable distributor\n                circ_breaker = graph.adj[adj_node1][adj_node2]['branch'].circuit_breaker\n                if circ_breaker is not None:\n                    circ_breaker.geo_data = calc_geo_centre_point(adj_node1, cable_dist)\n\n                # split old branch into 2 segments (delete old branch and create 2 new ones along cable_dist)\n                # ===========================================================================================\n\n                # backup kind and type of branch\n                branch_kind = graph.adj[adj_node1][adj_node2]['branch'].kind\n                branch_type = graph.adj[adj_node1][adj_node2]['branch'].type\n                branch_ring = graph.adj[adj_node1][adj_node2]['branch'].ring\n\n                graph.remove_edge(adj_node1, adj_node2)\n\n                branch_length = calc_geo_dist_vincenty(adj_node1, cable_dist)\n                branch = BranchDing0(length=branch_length,\n                                     circuit_breaker=circ_breaker,\n                                     kind=branch_kind,\n                                     type=branch_type,\n                                     ring=branch_ring)\n                if circ_breaker is not None:\n                    circ_breaker.branch = branch\n                graph.add_edge(adj_node1, cable_dist, branch=branch)\n\n                branch_length = calc_geo_dist_vincenty(adj_node2, cable_dist)\n                graph.add_edge(adj_node2, cable_dist, branch=BranchDing0(length=branch_length,\n                                                                         kind=branch_kind,\n                                                                         type=branch_type,\n                                                                         ring=branch_ring))\n\n                # add new branch for satellite (station to cable distributor)\n                # ===========================================================\n\n                # get default branch kind and type from grid to use it for new branch\n                branch_kind = mv_grid.default_branch_kind\n                branch_type = mv_grid.default_branch_type\n\n                branch_length = calc_geo_dist_vincenty(node, cable_dist)\n                graph.add_edge(node, cable_dist, branch=BranchDing0(length=branch_length,\n                                                                    kind=branch_kind,\n                                                                    type=branch_type,\n                                                                    ring=branch_ring))\n                target_obj_result = cable_dist\n\n                # debug info\n                if debug:\n                    logger.debug('Nearest connection point for object {0} '\n                                 'is branch {1} (distance={2} m)'.format(\n                        node, target_obj['obj']['adj_nodes'], target_obj['dist']))\n\n    # node ist nearest connection point\n    else:\n\n        # what kind of node is to be connected? (which type is node of?)\n        #   LVLoadAreaCentreDing0: Connect to LVLoadAreaCentreDing0 only\n        #   LVStationDing0: Connect to LVLoadAreaCentreDing0, LVStationDing0 or MVCableDistributorDing0\n        #   GeneratorDing0: Connect to LVLoadAreaCentreDing0, LVStationDing0, MVCableDistributorDing0 or GeneratorDing0\n        if isinstance(node, LVLoadAreaCentreDing0):\n            valid_conn_objects = LVLoadAreaCentreDing0\n        elif isinstance(node, LVStationDing0):\n            valid_conn_objects = (LVLoadAreaCentreDing0, LVStationDing0, MVCableDistributorDing0)\n        elif isinstance(node, GeneratorDing0):\n            valid_conn_objects = (LVLoadAreaCentreDing0, LVStationDing0, MVCableDistributorDing0, GeneratorDing0)\n        else:\n            raise ValueError('Oops, the node you are trying to connect is not a valid connection object')\n\n        # if target is Load Area centre or LV station, check if it belongs to a load area of type aggregated\n        # (=> connection not allowed)\n        if isinstance(target_obj['obj'], (LVLoadAreaCentreDing0, LVStationDing0)):\n            target_is_aggregated = target_obj['obj'].lv_load_area.is_aggregated\n        else:\n            target_is_aggregated = False\n\n        # target node is not a load area of type aggregated\n        if isinstance(target_obj['obj'], valid_conn_objects) and not target_is_aggregated:\n\n            # get default branch kind and type from grid to use it for new branch\n            branch_kind = mv_grid.default_branch_kind\n            branch_type = mv_grid.default_branch_type\n\n            # get branch ring obj\n            branch_ring = mv_grid.get_ring_from_node(target_obj['obj'])\n\n            # add new branch for satellite (station to station)\n            branch_length = calc_geo_dist_vincenty(node, target_obj['obj'])\n            graph.add_edge(node, target_obj['obj'], branch=BranchDing0(length=branch_length,\n                                                                       kind=branch_kind,\n                                                                       type=branch_type,\n                                                                       ring=branch_ring))\n            target_obj_result = target_obj['obj']\n\n            # debug info\n            if debug:\n                logger.debug('Nearest connection point for object {0} is station {1} '\n                      '(distance={2} m)'.format(\n                    node, target_obj['obj'], target_obj['dist']))\n\n    return target_obj_result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disconnect_node(node, target_obj_result, graph, debug):\n\n    # backup kind and type of branch\n    branch_kind = graph.adj[node][target_obj_result]['branch'].kind\n    branch_type = graph.adj[node][target_obj_result]['branch'].type\n    branch_ring = graph.adj[node][target_obj_result]['branch'].ring\n\n    graph.remove_edge(node, target_obj_result)\n\n    if isinstance(target_obj_result, MVCableDistributorDing0):\n\n        neighbor_nodes = list(graph.neighbors(target_obj_result))\n\n        if len(neighbor_nodes) == 2:\n            node.grid.remove_cable_distributor(target_obj_result)\n\n            branch_length = calc_geo_dist_vincenty(neighbor_nodes[0], neighbor_nodes[1])\n            graph.add_edge(neighbor_nodes[0], neighbor_nodes[1], branch=BranchDing0(length=branch_length,\n                                                                                    kind=branch_kind,\n                                                                                    type=branch_type,\n                                                                                    ring=branch_ring))\n\n    if debug:\n        logger.debug('disconnect edge {0}-{1}'.format(node, target_obj_result))", "response": "Disconnects node from target_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset unparametrized branches to default branch type", "response": "def parametrize_lines(mv_grid):\n    \"\"\" Set unparametrized branches to default branch type\n    \n    Args\n    ----\n    mv_grid: MVGridDing0\n        MV grid instance\n\n    Notes\n    -----\n    During the connection process of satellites, new branches are created - \n    these have to be parametrized.\n    \"\"\"\n\n    for branch in mv_grid.graph_edges():\n        if branch['branch'].kind is None:\n            branch['branch'].kind = mv_grid.default_branch_kind\n        if branch['branch'].type is None:\n            branch['branch'].type = mv_grid.default_branch_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects satellites to MV grid", "response": "def mv_connect_satellites(mv_grid, graph, mode='normal', debug=False):\n    \"\"\" Connect satellites (small Load Areas) to MV grid\n\n    Args\n    ----\n    mv_grid: MVGridDing0\n        MV grid instance\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes\n    mode: str, defaults to 'normal'\n        Specify mode how satellite `LVLoadAreaCentreDing0` are connected to the\n        grid. Mode normal (default) considers for restrictions like max.\n        string length, max peak load per string.\n        The mode 'isolated' disregards any connection restrictions and connects\n        the node `LVLoadAreaCentreDing0` to the next connection point.\n        \n    debug: bool, defaults to False\n         If True, information is printed during process\n\n    Notes\n    -----\n    conn_dist_weight: The satellites can be connected to line (new terminal is\n    created) or to one station where the line ends, depending on the distance\n    from satellite to the objects. This threshold is a length weighting to\n    prefer stations instead of direct line connection to respect grid planning\n    principles.\n\n    Example: The distance from satellite to line is 1km, to station1 1.2km, to\n    station2 2km. With conn_dist_threshold=0.75, the 'virtual' distance to\n    station1 would be 1.2km * 0.75 = 0.9km, so this conn. point would be\n    preferred.\n\n    Returns\n    -------\n    :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes and newly created branches\n    \"\"\"\n\n    # conn_dist_weight: The satellites can be connected to line (new terminal is created) or to one station where the\n    # line ends, depending on the distance from satellite to the objects. This threshold is a length weighting to prefer\n    # stations instead of direct line connection to respect grid planning principles.\n    # Example: The distance from satellite to line is 1km, to station1 1.2km, to station2 2km.\n    # With conn_dist_threshold=0.75, the 'virtual' distance to station1 would be 1.2km * 0.75 = 0.9km, so this conn.\n    # point would be preferred.\n    conn_dist_weight = cfg_ding0.get('mv_connect', 'load_area_sat_conn_dist_weight')\n\n    # conn_dist_ring_mod: Allow re-routing of ring main route if node is closer than this threshold (in m) to ring.\n    conn_dist_ring_mod = cfg_ding0.get('mv_connect', 'load_area_sat_conn_dist_ring_mod')\n\n    load_area_sat_buffer_radius = cfg_ding0.get('mv_connect', 'load_area_sat_buffer_radius')\n    load_area_sat_buffer_radius_inc = cfg_ding0.get('mv_connect', 'load_area_sat_buffer_radius_inc')\n\n    start = time.time()\n\n    # WGS84 (conformal) to ETRS (equidistant) projection\n    proj1 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n\n    # ETRS (equidistant) to WGS84 (conformal) projection\n    proj2 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:3035'),  # source coordinate system\n            pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n\n    # check all nodes\n    if mode == 'normal':\n        #nodes = sorted(graph.nodes(), key=lambda x: repr(x))\n        nodes = mv_grid.graph_isolated_nodes()\n    elif mode == 'isolated':\n        nodes = mv_grid.graph_isolated_nodes()\n    else:\n        raise ValueError('\\'mode\\' is invalid.')\n\n    for node in nodes:\n\n        # node is Load Area centre\n        if isinstance(node, LVLoadAreaCentreDing0):\n\n            # satellites only\n            if node.lv_load_area.is_satellite:\n\n                node_shp = transform(proj1, node.geo_data)\n\n                if mode == 'normal':\n                    # get branches within a the predefined radius `load_area_sat_buffer_radius`\n                    branches = calc_geo_branches_in_buffer(node,\n                                                           mv_grid,\n                                                           load_area_sat_buffer_radius,\n                                                           load_area_sat_buffer_radius_inc, proj1)\n                elif mode == 'isolated':\n                    # get nodes of all MV rings\n                    nodes = set()\n                    [nodes.update(ring_nodes) for ring_nodes in list(mv_grid.rings_nodes(include_root_node=True))]\n                    nodes = list(nodes)\n                    # get branches of these nodes\n                    branches = []\n                    [branches.append(mv_grid.graph_branches_from_node(node_branches)) for node_branches in nodes]\n                    # reformat branches\n                    branches = [_ for _ in list(mv_grid.graph_edges())\n                                if (_['adj_nodes'][0] in nodes and _['adj_nodes'][1] in nodes)]\n\n                # calc distance between node and grid's lines -> find nearest line\n                conn_objects_min_stack = find_nearest_conn_objects(node_shp, branches, proj1,\n                                                                   conn_dist_weight, debug,\n                                                                   branches_only=False)\n\n                # iterate over object stack\n                find_connection_point(node, node_shp, graph, proj2, conn_objects_min_stack,\n                                      conn_dist_ring_mod, debug)\n\n    # parametrize newly created branches\n    parametrize_lines(mv_grid)\n\n    if debug:\n        logger.debug('Elapsed time (mv_connect): {}'.format(time.time() - start))\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mv_connect_stations(mv_grid_district, graph, debug=False):\n\n    # WGS84 (conformal) to ETRS (equidistant) projection\n    proj1 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n\n    # ETRS (equidistant) to WGS84 (conformal) projection\n    proj2 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:3035'),  # source coordinate system\n            pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n\n    conn_dist_weight = cfg_ding0.get('mv_connect', 'load_area_sat_conn_dist_weight')\n    conn_dist_ring_mod = cfg_ding0.get('mv_connect', 'load_area_stat_conn_dist_ring_mod')\n\n    for lv_load_area in mv_grid_district.lv_load_areas():\n\n        # exclude aggregated Load Areas and choose only load areas that were connected to grid before\n        if not lv_load_area.is_aggregated and \\\n           lv_load_area.lv_load_area_centre not in mv_grid_district.mv_grid.graph_isolated_nodes():\n\n            lv_load_area_centre = lv_load_area.lv_load_area_centre\n\n            # there's only one station: Replace Load Area centre by station in graph\n            if lv_load_area.lv_grid_districts_count() == 1:\n                # get station\n                lv_station = list(lv_load_area.lv_grid_districts())[0].lv_grid.station()\n\n                # get branches that are connected to Load Area centre\n                branches = mv_grid_district.mv_grid.graph_branches_from_node(lv_load_area_centre)\n\n                # connect LV station, delete Load Area centre\n                for node, branch in branches:\n                    # backup kind and type of branch\n                    branch_kind = branch['branch'].kind\n                    branch_type = branch['branch'].type\n                    branch_ring = branch['branch'].ring\n\n                    # respect circuit breaker if existent\n                    circ_breaker = branch['branch'].circuit_breaker\n                    if circ_breaker is not None:\n                        branch['branch'].circuit_breaker.geo_data = calc_geo_centre_point(lv_station, node)\n\n                    # delete old branch to Load Area centre and create a new one to LV station\n                    graph.remove_edge(lv_load_area_centre, node)\n\n                    branch_length = calc_geo_dist_vincenty(lv_station, node)\n                    branch = BranchDing0(length=branch_length,\n                                         circuit_breaker=circ_breaker,\n                                         kind=branch_kind,\n                                         type=branch_type,\n                                         ring=branch_ring)\n                    if circ_breaker is not None:\n                        circ_breaker.branch = branch\n                    graph.add_edge(lv_station, node, branch=branch)\n\n                # delete Load Area centre from graph\n                graph.remove_node(lv_load_area_centre)\n\n            # there're more than one station: Do normal connection process (as in satellites)\n            else:\n                # connect LV stations of all grid districts\n                # =========================================\n                for lv_grid_district in lv_load_area.lv_grid_districts():\n                    # get branches that are partly or fully located in load area\n                    branches = calc_geo_branches_in_polygon(mv_grid_district.mv_grid,\n                                                            lv_load_area.geo_area,\n                                                            mode='intersects',\n                                                            proj=proj1)\n\n                    # filter branches that belong to satellites (load area groups) if Load Area is not a satellite\n                    # itself\n                    if not lv_load_area.is_satellite:\n                        branches_valid = []\n                        for branch in branches:\n                            node1 = branch['adj_nodes'][0]\n                            node2 = branch['adj_nodes'][1]\n                            lv_load_area_group = get_lv_load_area_group_from_node_pair(node1, node2)\n\n                            # delete branch as possible conn. target if it belongs to a group (=satellite) or\n                            # if it belongs to a ring different from the ring of the current LVLA\n                            if (lv_load_area_group is None) and\\\n                               (branch['branch'].ring is lv_load_area.ring):\n                                branches_valid.append(branch)\n                        branches = branches_valid\n\n                    # find possible connection objects\n                    lv_station = lv_grid_district.lv_grid.station()\n                    lv_station_shp = transform(proj1, lv_station.geo_data)\n                    conn_objects_min_stack = find_nearest_conn_objects(lv_station_shp, branches, proj1,\n                                                                       conn_dist_weight, debug,\n                                                                       branches_only=False)\n\n                    # connect!\n                    connect_node(lv_station,\n                                 lv_station_shp,\n                                 mv_grid_district.mv_grid,\n                                 conn_objects_min_stack[0],\n                                 proj2,\n                                 graph,\n                                 conn_dist_ring_mod,\n                                 debug)\n\n                # Replace Load Area centre by cable distributor\n                # ================================================\n                # create cable distributor and add it to grid\n                cable_dist = MVCableDistributorDing0(geo_data=lv_load_area_centre.geo_data,\n                                                     grid=mv_grid_district.mv_grid)\n                mv_grid_district.mv_grid.add_cable_distributor(cable_dist)\n\n                # get branches that are connected to Load Area centre\n                branches = mv_grid_district.mv_grid.graph_branches_from_node(lv_load_area_centre)\n\n                # connect LV station, delete Load Area centre\n                for node, branch in branches:\n                    # backup kind and type of branch\n                    branch_kind = branch['branch'].kind\n                    branch_type = branch['branch'].type\n                    branch_ring = branch['branch'].ring\n\n                    # respect circuit breaker if existent\n                    circ_breaker = branch['branch'].circuit_breaker\n                    if circ_breaker is not None:\n                        branch['branch'].circuit_breaker.geo_data = calc_geo_centre_point(cable_dist, node)\n\n                    # delete old branch to Load Area centre and create a new one to LV station\n                    graph.remove_edge(lv_load_area_centre, node)\n\n                    branch_length = calc_geo_dist_vincenty(cable_dist, node)\n                    branch = BranchDing0(length=branch_length,\n                                         circuit_breaker=circ_breaker,\n                                         kind=branch_kind,\n                                         type=branch_type,\n                                         ring=branch_ring)\n                    if circ_breaker is not None:\n                        circ_breaker.branch = branch\n                    graph.add_edge(cable_dist, node, branch=branch)\n\n                # delete Load Area centre from graph\n                graph.remove_node(lv_load_area_centre)\n\n            # Replace all overhead lines by cables\n            # ====================================\n            # if grid's default type is overhead line\n            if mv_grid_district.mv_grid.default_branch_kind == 'line':\n                # get all branches in load area\n                branches = calc_geo_branches_in_polygon(mv_grid_district.mv_grid,\n                                                        lv_load_area.geo_area,\n                                                        mode='contains',\n                                                        proj=proj1)\n                # set type\n                for branch in branches:\n                    branch['branch'].kind = mv_grid_district.mv_grid.default_branch_kind_settle\n                    branch['branch'].type = mv_grid_district.mv_grid.default_branch_type_settle\n\n    return graph", "response": "Connect LV stations to MV grid"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mv_connect_generators(mv_grid_district, graph, debug=False):\n\n    generator_buffer_radius = cfg_ding0.get('mv_connect', 'generator_buffer_radius')\n    generator_buffer_radius_inc = cfg_ding0.get('mv_connect', 'generator_buffer_radius_inc')\n\n    # WGS84 (conformal) to ETRS (equidistant) projection\n    proj1 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:4326'),  # source coordinate system\n            pyproj.Proj(init='epsg:3035'))  # destination coordinate system\n\n    # ETRS (equidistant) to WGS84 (conformal) projection\n    proj2 = partial(\n            pyproj.transform,\n            pyproj.Proj(init='epsg:3035'),  # source coordinate system\n            pyproj.Proj(init='epsg:4326'))  # destination coordinate system\n\n    for generator in sorted(mv_grid_district.mv_grid.generators(), key=lambda x: repr(x)):\n\n        # ===== voltage level 4: generator has to be connected to MV station =====\n        if generator.v_level == 4:\n            mv_station = mv_grid_district.mv_grid.station()\n\n            branch_length = calc_geo_dist_vincenty(generator, mv_station)\n\n            # TODO: set branch type to something reasonable (to be calculated)\n            branch_kind = mv_grid_district.mv_grid.default_branch_kind\n            branch_type = mv_grid_district.mv_grid.default_branch_type\n\n            branch = BranchDing0(length=branch_length,\n                                 kind=branch_kind,\n                                 type=branch_type,\n                                 ring=None)\n            graph.add_edge(generator, mv_station, branch=branch)\n\n            if debug:\n                logger.debug('Generator {0} was connected to {1}'.format(\n                    generator, mv_station))\n\n        # ===== voltage level 5: generator has to be connected to MV grid (next-neighbor) =====\n        elif generator.v_level == 5:\n            generator_shp = transform(proj1, generator.geo_data)\n\n            # get branches within a the predefined radius `generator_buffer_radius`\n            branches = calc_geo_branches_in_buffer(generator,\n                                                   mv_grid_district.mv_grid,\n                                                   generator_buffer_radius,\n                                                   generator_buffer_radius_inc, proj1)\n\n            # calc distance between generator and grid's lines -> find nearest line\n            conn_objects_min_stack = find_nearest_conn_objects(generator_shp,\n                                                               branches,\n                                                               proj1,\n                                                               conn_dist_weight=1,\n                                                               debug=debug,\n                                                               branches_only=False)\n\n            # connect!\n            # go through the stack (from nearest to most far connection target object)\n            generator_connected = False\n            for dist_min_obj in conn_objects_min_stack:\n                # Note 1: conn_dist_ring_mod=0 to avoid re-routing of existent lines\n                # Note 2: In connect_node(), the default cable/line type of grid is used. This is reasonable since\n                #         the max. allowed power of the smallest possible cable/line type (3.64 MVA for overhead\n                #         line of type 48-AL1/8-ST1A) exceeds the max. allowed power of a generator (4.5 MVA (dena))\n                #         (if connected separately!)\n                target_obj_result = connect_node(generator,\n                                                 generator_shp,\n                                                 mv_grid_district.mv_grid,\n                                                 dist_min_obj,\n                                                 proj2,\n                                                 graph,\n                                                 conn_dist_ring_mod=0,\n                                                 debug=debug)\n\n                if target_obj_result is not None:\n                    if debug:\n                        logger.debug(\n                            'Generator {0} was connected to {1}'.format(\n                                generator, target_obj_result))\n                    generator_connected = True\n                    break\n\n            if not generator_connected and debug:\n                logger.debug(\n                    'Generator {0} could not be connected, try to '\n                    'increase the parameter `generator_buffer_radius` in '\n                    'config file `config_calc.cfg` to gain more possible '\n                    'connection points.'.format(generator))\n\n    return graph", "response": "Connect MV generators to MV grid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ding0_graph_to_routing_specs(graph):\n\n    # get power factor for loads\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n\n    specs = {}\n    nodes_demands = {}\n    nodes_pos = {}\n    nodes_agg = {}\n\n    # check if there are only load areas of type aggregated and satellite\n    # -> treat satellites as normal load areas (allow for routing)\n    satellites_only = True\n    for node in graph.nodes():\n        if isinstance(node, LVLoadAreaCentreDing0):\n            if not node.lv_load_area.is_satellite and not node.lv_load_area.is_aggregated:\n                satellites_only = False\n\n    for node in graph.nodes():\n        # station is LV station\n        if isinstance(node, LVLoadAreaCentreDing0):\n            # only major stations are connected via MV ring\n            # (satellites in case of there're only satellites in grid district)\n            if not node.lv_load_area.is_satellite or satellites_only:\n                # get demand and position of node\n                # convert node's demand to int for performance purposes and to avoid that node\n                # allocation with subsequent deallocation results in demand<0 due to rounding errors.\n                nodes_demands[str(node)] = int(node.lv_load_area.peak_load / cos_phi_load)\n                nodes_pos[str(node)] = (node.geo_data.x, node.geo_data.y)\n                # get aggregation flag\n                if node.lv_load_area.is_aggregated:\n                    nodes_agg[str(node)] = True\n                else:\n                    nodes_agg[str(node)] = False\n\n        # station is MV station\n        elif isinstance(node, MVStationDing0):\n            nodes_demands[str(node)] = 0\n            nodes_pos[str(node)] = (node.geo_data.x, node.geo_data.y)\n            specs['DEPOT'] = str(node)\n            specs['BRANCH_KIND'] = node.grid.default_branch_kind\n            specs['BRANCH_TYPE'] = node.grid.default_branch_type\n            specs['V_LEVEL'] = node.grid.v_level\n\n    specs['NODE_COORD_SECTION'] = nodes_pos\n    specs['DEMAND'] = nodes_demands\n    specs['MATRIX'] = calc_geo_dist_matrix_vincenty(nodes_pos)\n    specs['IS_AGGREGATED'] = nodes_agg\n\n    return specs", "response": "Build data dictionary from nodes for routing"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting solution from routing into ding0 graph.", "response": "def routing_solution_to_ding0_graph(graph, solution):\n    \"\"\" Insert `solution` from routing into `graph`\n\n    Args\n    ----\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes\n    solution: BaseSolution\n        Instance of `BaseSolution` or child class (e.g. `LocalSearchSolution`) (=solution from routing)\n\n    Returns\n    -------\n    :networkx:`NetworkX Graph Obj< >` \n        NetworkX graph object with nodes and edges\n    \"\"\"\n    # TODO: Bisherige Herangehensweise (diese Funktion): Branches werden nach Routing erstellt um die Funktionsf\u00e4higkeit\n    # TODO: des Routing-Tools auch f\u00fcr die TestCases zu erhalten. Es wird ggf. notwendig, diese direkt im Routing vorzunehmen.\n\n    # build node dict (name: obj) from graph nodes to map node names on node objects\n    node_list = {str(n): n for n in graph.nodes()}\n\n    # add edges from solution to graph\n    try:\n        depot = solution._nodes[solution._problem._depot.name()]\n        depot_node = node_list[depot.name()]\n        for r in solution.routes():\n            circ_breaker_pos = None\n\n            # if route has only one node and is not aggregated, it wouldn't be possible to add two lines from and to\n            # this node (undirected graph of NetworkX). So, as workaround, an additional MV cable distributor is added\n            # at nodes' position (resulting route: HV/MV_subst --- node --- cable_dist --- HV/MV_subst.\n            if len(r._nodes) == 1:\n                if not solution._problem._is_aggregated[r._nodes[0]._name]:\n                    # create new cable dist\n                    cable_dist = MVCableDistributorDing0(geo_data=node_list[r._nodes[0]._name].geo_data,\n                                                         grid=depot_node.grid)\n                    depot_node.grid.add_cable_distributor(cable_dist)\n\n                    # create new node (as dummy) an allocate to route r\n                    r.allocate([Node(name=repr(cable_dist), demand=0)])\n\n                    # add it to node list and allocated-list manually\n                    node_list[str(cable_dist)] = cable_dist\n                    solution._problem._is_aggregated[str(cable_dist)] = False\n\n                    # set circ breaker pos manually\n                    circ_breaker_pos = 1\n\n            # build edge list\n            n1 = r._nodes[0:len(r._nodes)-1]\n            n2 = r._nodes[1:len(r._nodes)]\n            edges = list(zip(n1, n2))\n            edges.append((depot, r._nodes[0]))\n            edges.append((r._nodes[-1], depot))\n\n            # create MV Branch object for every edge in `edges`\n            mv_branches = [BranchDing0() for _ in edges]\n            edges_with_branches = list(zip(edges, mv_branches))\n\n            # recalculate circuit breaker positions for final solution, create it and set associated branch.\n            # if circ. breaker position is not set manually (routes with more than one load area, see above)\n            if not circ_breaker_pos:\n                circ_breaker_pos = r.calc_circuit_breaker_position()\n\n            node1 = node_list[edges[circ_breaker_pos - 1][0].name()]\n            node2 = node_list[edges[circ_breaker_pos - 1][1].name()]\n\n            # ALTERNATIVE TO METHOD ABOVE: DO NOT CREATE 2 BRANCHES (NO RING) -> LA IS CONNECTED AS SATELLITE\n            # IF THIS IS COMMENTED-IN, THE IF-BLOCK IN LINE 87 HAS TO BE COMMENTED-OUT\n            # See issue #114\n            # ===============================\n            # do not add circuit breaker for routes which are aggregated load areas or\n            # routes that contain only one load area\n            # if not (node1 == depot_node and solution._problem._is_aggregated[edges[circ_breaker_pos - 1][1].name()] or\n            #         node2 == depot_node and solution._problem._is_aggregated[edges[circ_breaker_pos - 1][0].name()] or\n            #         len(r._nodes) == 1):\n            # ===============================\n\n            # do not add circuit breaker for routes which are aggregated load areas\n            if not (node1 == depot_node and solution._problem._is_aggregated[edges[circ_breaker_pos - 1][1].name()] or\n                    node2 == depot_node and solution._problem._is_aggregated[edges[circ_breaker_pos - 1][0].name()]):\n                branch = mv_branches[circ_breaker_pos - 1]\n                circ_breaker = CircuitBreakerDing0(grid=depot_node.grid, branch=branch,\n                                                   geo_data=calc_geo_centre_point(node1, node2))\n                branch.circuit_breaker = circ_breaker\n\n            # create new ring object for route\n            ring = RingDing0(grid=depot_node.grid)\n\n            # translate solution's node names to graph node objects using dict created before\n            # note: branch object is assigned to edge using an attribute ('branch' is used here), it can be accessed\n            # using the method `graph_edges()` of class `GridDing0`\n            edges_graph = []\n            for ((n1, n2), b) in edges_with_branches:\n                # get node objects\n                node1 = node_list[n1.name()]\n                node2 = node_list[n2.name()]\n\n                # set branch's ring attribute\n                b.ring = ring\n                # set LVLA's ring attribute\n                if isinstance(node1, LVLoadAreaCentreDing0):\n                    node1.lv_load_area.ring = ring\n\n                # set branch length\n                b.length = calc_geo_dist_vincenty(node1, node2)\n\n                # set branch kind and type\n                # 1) default\n                b.kind = depot_node.grid.default_branch_kind\n                b.type = depot_node.grid.default_branch_type\n                # 2) aggregated load area types\n                if node1 == depot_node and solution._problem._is_aggregated[n2.name()]:\n                    b.connects_aggregated = True\n                    b.kind = depot_node.grid.default_branch_kind_aggregated\n                    b.type = depot_node.grid.default_branch_type_aggregated\n                elif node2 == depot_node and solution._problem._is_aggregated[n1.name()]:\n                    b.connects_aggregated = True\n                    b.kind = depot_node.grid.default_branch_kind_aggregated\n                    b.type = depot_node.grid.default_branch_type_aggregated\n\n                # append to branch list\n                edges_graph.append((node1, node2, dict(branch=b)))\n\n            # add branches to graph\n            graph.add_edges_from(edges_graph)\n\n    except:\n        logger.exception(\n            'unexpected error while converting routing solution to DING0 graph (NetworkX).')\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndoing MV routing for given nodes in graph and return a new object with the result.", "response": "def solve(graph, debug=False, anim=None):\n    # TODO: check docstring\n    \"\"\" Do MV routing for given nodes in `graph`.\n    \n    Translate data from node objects to appropriate format before.\n\n    Args\n    ----\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes\n    debug: bool, defaults to False\n        If True, information is printed while routing\n    anim: AnimationDing0\n        AnimationDing0 object\n\n    Returns\n    -------\n    :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes and edges\n        \n    See Also\n    --------\n    ding0.tools.animation.AnimationDing0 : for a more detailed description on anim parameter.\n    \"\"\"\n\n    # TODO: Implement debug mode (pass to solver) to get more information while routing (print routes, draw network, ..)\n\n    # translate DING0 graph to routing specs\n    specs = ding0_graph_to_routing_specs(graph)\n\n    # create routing graph using specs\n    RoutingGraph = Graph(specs)\n\n    timeout = 30000\n\n    # create solver objects\n    savings_solver = savings.ClarkeWrightSolver()\n    local_search_solver = local_search.LocalSearchSolver()\n\n    start = time.time()\n\n    # create initial solution using Clarke and Wright Savings methods\n    savings_solution = savings_solver.solve(RoutingGraph, timeout, debug, anim)\n\n    # OLD, MAY BE USED LATER - Guido, please don't declare a variable later=now() :) :\n    #if not savings_solution.is_complete():\n    #    print('=== Solution is not a complete solution! ===')\n\n    if debug:\n        logger.debug('ClarkeWrightSolver solution:')\n        util.print_solution(savings_solution)\n        logger.debug('Elapsed time (seconds): {}'.format(time.time() - start))\n        #savings_solution.draw_network()\n\n    # improve initial solution using local search\n    local_search_solution = local_search_solver.solve(RoutingGraph, savings_solution, timeout, debug, anim)\n    # this line is for debug plotting purposes:\n    #local_search_solution = savings_solution\n\n    if debug:\n        logger.debug('Local Search solution:')\n        util.print_solution(local_search_solution)\n        logger.debug('Elapsed time (seconds): {}'.format(time.time() - start))\n        #local_search_solution.draw_network()\n\n    return routing_solution_to_ding0_graph(graph, local_search_solution)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cable_type(nom_power, nom_voltage, avail_cables):\n\n    I_max_load = nom_power / (3 ** 0.5 * nom_voltage)\n\n    # determine suitable cable for this current\n    suitable_cables = avail_cables[avail_cables['I_max_th'] > I_max_load]\n    if not suitable_cables.empty:\n        cable_type = suitable_cables.loc[suitable_cables['I_max_th'].idxmin(), :]\n    else:\n        cable_type = avail_cables.loc[avail_cables['I_max_th'].idxmax(), :]\n\n    return cable_type", "response": "Determine the type of a cable based on maximum occurring current based on maximum occurring current"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lv_connect_generators(lv_grid_district, graph, debug=False):\n\n    \"\"\" Connect LV generators to LV grid\n    \n    Args\n    ----\n    lv_grid_district: LVGridDistrictDing0\n        LVGridDistrictDing0 object for which the connection process has to be done\n    graph: :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes\n    debug: bool, defaults to False\n        If True, information is printed during process\n\n    Returns\n    -------\n    :networkx:`NetworkX Graph Obj< >`\n        NetworkX graph object with nodes and newly created branches\n    \"\"\"\n\n    cable_lf = cfg_ding0.get('assumptions',\n                             'load_factor_lv_cable_fc_normal')\n    cos_phi_gen = cfg_ding0.get('assumptions',\n                                'cos_phi_gen')\n\n    # generate random list (without replacement => unique elements)\n    # of loads (residential) to connect genos (P <= 30kW) to.\n    lv_loads_res = sorted(lv_grid_district.lv_grid.loads_sector(sector='res'),\n                          key=lambda _: repr(_))\n    if len(lv_loads_res) > 0:\n        lv_loads_res_rnd = set(random.sample(lv_loads_res,\n                                             len(lv_loads_res)))\n    else:\n        lv_loads_res_rnd = None\n\n    # generate random list (without replacement => unique elements)\n    # of loads (retail, industrial, agricultural) to connect genos\n    # (30kW <= P <= 100kW) to.\n    lv_loads_ria = sorted(lv_grid_district.lv_grid.loads_sector(sector='ria'),\n                          key=lambda _: repr(_))\n    if len(lv_loads_ria) > 0:\n        lv_loads_ria_rnd = set(random.sample(lv_loads_ria,\n                                             len(lv_loads_ria)))\n    else:\n        lv_loads_ria_rnd = None\n\n    for generator in sorted(lv_grid_district.lv_grid.generators(), key=lambda x: repr(x)):\n\n        # generator is of v_level 6 -> connect to LV station\n        if generator.v_level == 6:\n            lv_station = lv_grid_district.lv_grid.station()\n\n            branch_length = calc_geo_dist_vincenty(generator, lv_station)\n            branch_type = cable_type(\n                generator.capacity / (cable_lf * cos_phi_gen),\n                0.4,\n                lv_grid_district.lv_grid.network.static_data['LV_cables'])\n\n            branch = BranchDing0(length=branch_length,\n                                 kind='cable',\n                                 type=branch_type)\n\n            graph.add_edge(generator, lv_station, branch=branch)\n\n        # generator is of v_level 7 -> assign geno to load\n        elif generator.v_level == 7:\n\n            # connect genos with P <= 30kW to residential loads, if available\n            if (generator.capacity <= 30) and (lv_loads_res_rnd is not None):\n                if len(lv_loads_res_rnd) > 0:\n                    lv_load = lv_loads_res_rnd.pop()\n                # if random load list is empty, create new one\n                else:\n                    lv_loads_res_rnd = set(random.sample(lv_loads_res,\n                                                     len(lv_loads_res))\n                                       )\n                    lv_load = lv_loads_res_rnd.pop()\n\n                # get cable distributor of building\n                lv_conn_target = list(graph.neighbors(lv_load))[0]\n\n            # connect genos with 30kW <= P <= 100kW to residential loads\n            # to retail, industrial, agricultural loads, if available\n            elif (generator.capacity > 30) and (lv_loads_ria_rnd is not None):\n                if len(lv_loads_ria_rnd) > 0:\n                    lv_load = lv_loads_ria_rnd.pop()\n                # if random load list is empty, create new one\n                else:\n                    lv_loads_ria_rnd = set(random.sample(lv_loads_ria,\n                                                         len(lv_loads_ria))\n                                           )\n                    lv_load = lv_loads_ria_rnd.pop()\n\n                # get cable distributor of building\n                lv_conn_target = list(graph.neighbors(lv_load))[0]\n\n            # fallback: connect to station\n            else:\n                lv_conn_target = lv_grid_district.lv_grid.station()\n\n                logger.warning(\n                    'No valid conn. target found for {}.'\n                    'Connected to {}.'.format(\n                        repr(generator),\n                        repr(lv_conn_target)\n                    ))\n\n            # determine appropriate type of cable\n            branch_type = cable_type(\n                generator.capacity / (cable_lf * cos_phi_gen),\n                0.4,\n                lv_grid_district.lv_grid.network.static_data['LV_cables'])\n\n            # connect to cable dist. of building\n            branch = BranchDing0(length=1,\n                                 kind='cable',\n                                 type=branch_type)\n\n            graph.add_edge(generator, lv_conn_target, branch=branch)\n\n    return graph", "response": "Connect LV generators to LV grid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_circuit_breakers(mv_grid, mode='load', debug=False):\n\n    # get power factor for loads and generators\n    cos_phi_load = cfg_ding0.get('assumptions', 'cos_phi_load')\n    cos_phi_feedin = cfg_ding0.get('assumptions', 'cos_phi_gen')\n\n    # iterate over all rings and circuit breakers\n    for ring, circ_breaker in zip(mv_grid.rings_nodes(include_root_node=False), mv_grid.circuit_breakers()):\n\n        nodes_peak_load = []\n        nodes_peak_generation = []\n\n        # iterate over all nodes of ring\n        for node in ring:\n\n            # node is LV station -> get peak load and peak generation\n            if isinstance(node, LVStationDing0):\n                nodes_peak_load.append(node.peak_load / cos_phi_load)\n                nodes_peak_generation.append(node.peak_generation / cos_phi_feedin)\n\n            # node is cable distributor -> get all connected nodes of subtree using graph_nodes_from_subtree()\n            elif isinstance(node, CableDistributorDing0):\n                nodes_subtree = mv_grid.graph_nodes_from_subtree(node)\n                nodes_subtree_peak_load = 0\n                nodes_subtree_peak_generation = 0\n\n                for node_subtree in nodes_subtree:\n\n                    # node is LV station -> get peak load and peak generation\n                    if isinstance(node_subtree, LVStationDing0):\n                        nodes_subtree_peak_load += node_subtree.peak_load / \\\n                                                   cos_phi_load\n                        nodes_subtree_peak_generation += node_subtree.peak_generation / \\\n                                                         cos_phi_feedin\n\n                    # node is LV station -> get peak load and peak generation\n                    if isinstance(node_subtree, GeneratorDing0):\n                        nodes_subtree_peak_generation += node_subtree.capacity / \\\n                                                         cos_phi_feedin\n\n                nodes_peak_load.append(nodes_subtree_peak_load)\n                nodes_peak_generation.append(nodes_subtree_peak_generation)\n\n            else:\n                raise ValueError('Ring node has got invalid type.')\n\n\n        if mode == 'load':\n            node_peak_data = nodes_peak_load\n        elif mode == 'loadgen':\n            # is ring dominated by load or generation?\n            # (check if there's more load than generation in ring or vice versa)\n            if sum(nodes_peak_load) > sum(nodes_peak_generation):\n                node_peak_data = nodes_peak_load\n            else:\n                node_peak_data = nodes_peak_generation\n        else:\n            raise ValueError('parameter \\'mode\\' is invalid!')\n\n        # calc optimal circuit breaker position\n\n        # set init value\n        diff_min = 10e6\n\n        # check where difference of demand/generation in two half-rings is minimal\n        for ctr in range(len(node_peak_data)):\n            # split route and calc demand difference\n            route_data_part1 = sum(node_peak_data[0:ctr])\n            route_data_part2 = sum(node_peak_data[ctr:len(node_peak_data)])\n            diff = abs(route_data_part1 - route_data_part2)\n\n            # equality has to be respected, otherwise comparison stops when demand/generation=0\n            if diff <= diff_min:\n                diff_min = diff\n                position = ctr\n            else:\n                break\n\n        # relocate circuit breaker\n        node1 = ring[position-1]\n        node2 = ring[position]\n        circ_breaker.branch = mv_grid._graph.adj[node1][node2]['branch']\n        circ_breaker.branch_nodes = (node1, node2)\n        circ_breaker.branch.circuit_breaker = circ_breaker\n        circ_breaker.geo_data = calc_geo_centre_point(node1, node2)\n\n        if debug:\n            logger.debug('Ring: {}'.format(ring))\n            logger.debug('Circuit breaker {0} was relocated to edge {1}-{2} '\n                  '(position on route={3})'.format(\n                    circ_breaker, node1, node2, position)\n                )\n            logger.debug('Peak load sum: {}'.format(sum(nodes_peak_load)))\n            logger.debug('Peak loads: {}'.format(nodes_peak_load))", "response": "This function calculates the optimal circuit breaker position for a MV ring."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles the duplications in the strings files.", "response": "def handle_duplications(file_path):\n    \"\"\" Omits the duplications in the strings files.\n        Keys that appear more than once, will be joined to one appearance and the omit will be documented.\n\n    Args:\n        file_path (str): The path to the strings file.\n\n    \"\"\"\n    logging.info('Handling duplications for \"%s\"', file_path)\n    f = open_strings_file(file_path, \"r+\")\n    header_comment_key_value_tuples = extract_header_comment_key_value_tuples_from_file(f)\n    file_elements = []\n    section_file_elements = []\n    keys_to_objects = {}\n    duplicates_found = []\n    for header_comment, comments, key, value in header_comment_key_value_tuples:\n        if len(header_comment) > 0:\n            # New section - Appending the last section entries, sorted by comment\n            for elem in sorted(section_file_elements, key=lambda x: x.comments[0]):\n                file_elements.append(elem)\n            section_file_elements = []\n            file_elements.append(Comment(header_comment))\n\n        if key in keys_to_objects:\n            keys_to_objects[key].add_comments(comments)\n            duplicates_found.append(key)\n        else:\n            loc_obj = LocalizationEntry(comments, key, value)\n            keys_to_objects[key] = loc_obj\n            section_file_elements.append(loc_obj)\n\n    # Adding last section\n    for elem in sorted(section_file_elements, key=lambda x: x.comments[0]):\n        file_elements.append(elem)\n\n    f.seek(0)\n\n    for element in file_elements:\n        f.write(unicode(element))\n        f.write(u\"\\n\")\n\n    f.truncate()\n    f.close()\n\n    logging.info(\"Omitted %d duplicates (%s)\" % (len(duplicates_found), \",\".join(duplicates_found)))\n    logging.info('Finished handling duplications for \"%s\"', file_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting the heatmap of the cohort abundance and count data for several samples.", "response": "def plot_heatmap(\n        self,\n        rank=\"auto\",\n        normalize=\"auto\",\n        top_n=\"auto\",\n        threshold=\"auto\",\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        tooltip=None,\n        return_chart=False,\n        linkage=\"average\",\n        haxis=None,\n        metric=\"euclidean\",\n        legend=\"auto\",\n        label=None,\n    ):\n        \"\"\"Plot heatmap of taxa abundance/count data for several samples.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        normalize : 'auto' or `bool`, optional\n            Convert read counts to relative abundances such that each sample sums to 1.0. Setting\n            'auto' will choose automatically based on the data.\n        return_chart : `bool`, optional\n            When True, return an `altair.Chart` object instead of displaying the resulting plot in\n            the current notebook.\n        haxis : `string`, optional\n            The metadata field (or tuple containing multiple categorical fields) used to group\n            samples together. Each group of samples will be clustered independently.\n        metric : {'braycurtis', 'manhattan', 'jaccard', 'unifrac', 'unweighted_unifrac}, optional\n            Function to use when calculating the distance between two samples.\n        linkage : {'average', 'single', 'complete', 'weighted', 'centroid', 'median'}\n            The type of linkage to use when clustering axes.\n        top_n : `int`, optional\n            Display the top N most abundant taxa in the entire cohort of samples.\n        threshold : `float`\n            Display only taxa that are more abundant that this threshold in one or more samples.\n        title : `string`, optional\n            Text label at the top of the plot.\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n        tooltip : `string` or `list`, optional\n            A string or list containing strings representing metadata fields. When a point in the\n            plot is hovered over, the value of the metadata associated with that sample will be\n            displayed in a modal.\n        legend: `string`, optional\n            Title for color scale. Defaults to the field used to generate the plot, e.g.\n            readcount_w_children or abundance.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Plot a heatmap of the relative abundances of the top 10 most abundant families.\n\n        >>> plot_heatmap(rank='family', top_n=10)\n        \"\"\"\n        if rank is None:\n            raise OneCodexException(\"Please specify a rank or 'auto' to choose automatically\")\n\n        if not (threshold or top_n):\n            raise OneCodexException(\"Please specify at least one of: threshold, top_n\")\n\n        if len(self._results) < 2:\n            raise OneCodexException(\n                \"`plot_heatmap` requires 2 or more valid classification results.\"\n            )\n\n        if top_n == \"auto\" and threshold == \"auto\":\n            top_n = 10\n            threshold = None\n        elif top_n == \"auto\" and threshold != \"auto\":\n            top_n = None\n        elif top_n != \"auto\" and threshold == \"auto\":\n            threshold = None\n\n        if legend == \"auto\":\n            legend = self._field\n\n        df = self.to_df(\n            rank=rank, normalize=normalize, top_n=top_n, threshold=threshold, table_format=\"long\"\n        )\n\n        if tooltip:\n            if not isinstance(tooltip, list):\n                tooltip = [tooltip]\n        else:\n            tooltip = []\n\n        if haxis:\n            tooltip.append(haxis)\n\n        tooltip.insert(0, \"Label\")\n\n        magic_metadata, magic_fields = self._metadata_fetch(tooltip, label=label)\n\n        # add columns for prettier display\n        df[\"Label\"] = magic_metadata[\"Label\"][df[\"classification_id\"]].tolist()\n        df[\"tax_name\"] = [\"{} ({})\".format(self.taxonomy[\"name\"][t], t) for t in df[\"tax_id\"]]\n\n        # and for metadata\n        for f in tooltip:\n            df[magic_fields[f]] = magic_metadata[magic_fields[f]][df[\"classification_id\"]].tolist()\n\n        # if we've already been normalized, we must cluster samples by euclidean distance. beta\n        # diversity measures won't work with normalized distances.\n        if self._guess_normalized():\n            if metric != \"euclidean\":\n                raise OneCodexException(\n                    \"Results are normalized. Please re-run with metric=euclidean\"\n                )\n\n            df_sample_cluster = self.to_df(\n                rank=rank, normalize=normalize, top_n=top_n, threshold=threshold\n            )\n            df_taxa_cluster = df_sample_cluster\n        else:\n            df_sample_cluster = self.to_df(\n                rank=rank, normalize=False, top_n=top_n, threshold=threshold\n            )\n\n            df_taxa_cluster = self.to_df(\n                rank=rank, normalize=normalize, top_n=top_n, threshold=threshold\n            )\n\n        if haxis is None:\n            # cluster only once\n            sample_cluster = df_sample_cluster.ocx._cluster_by_sample(\n                rank=rank, metric=metric, linkage=linkage\n            )\n            taxa_cluster = df_taxa_cluster.ocx._cluster_by_taxa(linkage=linkage)\n\n            labels_in_order = magic_metadata[\"Label\"][sample_cluster[\"ids_in_order\"]].tolist()\n        else:\n            if not (\n                pd.api.types.is_bool_dtype(df[magic_fields[haxis]])\n                or pd.api.types.is_categorical_dtype(df[magic_fields[haxis]])  # noqa\n                or pd.api.types.is_object_dtype(df[magic_fields[haxis]])  # noqa\n            ):  # noqa\n                raise OneCodexException(\"Metadata field on horizontal axis can not be numerical\")\n\n            # taxa clustered only once\n            taxa_cluster = df_taxa_cluster.ocx._cluster_by_taxa(linkage=linkage)\n\n            # cluster samples for every group of metadata\n            groups = magic_metadata[magic_fields[haxis]].unique()\n            cluster_by_group = {}\n\n            labels_in_order = []\n\n            plot_data = {\"x\": [], \"y\": [], \"o\": [], \"b\": []}\n            label_data = {\"x\": [], \"y\": [], \"label\": []}\n\n            for idx, group in enumerate(groups):\n                # if value of metadata field is 'null', we have to use pd.isnull, can't use 'is None'\n                if pd.isnull(group):\n                    c_ids_in_group = magic_metadata.index[\n                        pd.isnull(magic_metadata[magic_fields[haxis]])\n                    ]\n                else:\n                    c_ids_in_group = magic_metadata.index[\n                        magic_metadata[magic_fields[haxis]] == group\n                    ]\n\n                if len(c_ids_in_group) == 0:\n                    continue\n\n                sample_slice = df_sample_cluster.loc[c_ids_in_group]\n\n                if len(c_ids_in_group) < 3:\n                    # clustering not possible in this case\n                    cluster_by_group[group] = {\"ids_in_order\": c_ids_in_group}\n                else:\n                    cluster_by_group[group] = sample_slice.ocx._cluster_by_sample(\n                        rank=rank, metric=metric, linkage=linkage\n                    )\n\n                plot_data[\"x\"].append(len(labels_in_order) + 0.25)\n\n                labels_in_order.extend(\n                    magic_metadata[\"Label\"][cluster_by_group[group][\"ids_in_order\"]].tolist()\n                )\n\n                plot_data[\"x\"].append(len(labels_in_order) - 0.25)\n                plot_data[\"y\"].extend([0, 0])\n                plot_data[\"o\"].extend([0, 1])\n                plot_data[\"b\"].extend([idx, idx])\n\n                label_data[\"x\"].append(sum(plot_data[\"x\"][-2:]) / 2)\n                label_data[\"y\"].append(1)\n                label_data[\"label\"].append(str(group))\n\n            label_bars = (\n                alt.Chart(\n                    pd.DataFrame(plot_data), width=15 * len(df_sample_cluster.index), height=10\n                )\n                .mark_line(point=False, opacity=0.5)\n                .encode(\n                    x=alt.X(\n                        \"x\",\n                        axis=None,\n                        scale=alt.Scale(\n                            domain=[0, len(df_sample_cluster.index)], zero=True, nice=False\n                        ),\n                    ),\n                    y=alt.Y(\"y\", axis=None),\n                    order=\"o\",\n                    color=alt.Color(\n                        \"b:N\",\n                        scale=alt.Scale(domain=list(range(idx + 1)), range=[\"black\"] * (idx + 1)),\n                        legend=None,\n                    ),\n                )\n            )\n\n            label_text = (\n                alt.Chart(\n                    pd.DataFrame(label_data), width=15 * len(df_sample_cluster.index), height=10\n                )\n                .mark_text(align=\"center\", baseline=\"middle\")\n                .encode(\n                    x=alt.X(\n                        \"x\",\n                        axis=None,\n                        scale=alt.Scale(\n                            domain=[0, len(df_sample_cluster.index)], zero=True, nice=False\n                        ),\n                    ),\n                    y=alt.Y(\n                        \"y\", axis=alt.Axis(title=haxis, ticks=False, domain=False, labels=False)\n                    ),\n                    text=\"label\",\n                )\n            )\n\n            top_label = alt.layer(label_text, label_bars)\n\n        # should ultimately be Label, tax_name, readcount_w_children, then custom fields\n        tooltip_for_altair = [magic_fields[f] for f in tooltip]\n        tooltip_for_altair.insert(1, \"tax_name\")\n        tooltip_for_altair.insert(2, \"{}:Q\".format(self._field))\n\n        alt_kwargs = dict(\n            x=alt.X(\"Label:N\", axis=alt.Axis(title=xlabel), sort=labels_in_order),\n            y=alt.Y(\n                \"tax_name:N\", axis=alt.Axis(title=ylabel), sort=taxa_cluster[\"labels_in_order\"]\n            ),\n            color=alt.Color(\"{}:Q\".format(self._field), legend=alt.Legend(title=legend)),\n            tooltip=tooltip_for_altair,\n            href=\"url:N\",\n            url=\"https://app.onecodex.com/classification/\" + alt.datum.classification_id,\n        )\n\n        chart = (\n            alt.Chart(\n                df,\n                width=15 * len(df[\"classification_id\"].unique()),\n                height=15 * len(df[\"tax_id\"].unique()),\n            )\n            .transform_calculate(url=alt_kwargs.pop(\"url\"))\n            .mark_rect()\n            .encode(**alt_kwargs)\n        )\n\n        if title:\n            chart = chart.properties(title=title)\n\n        if haxis:\n            if return_chart:\n                return top_label & chart\n            else:\n                (top_label & chart).display()\n        else:\n            if return_chart:\n                return chart\n            else:\n                chart.interactive().display()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _copy_resources(self):\n\n        for resource in self._client._resources:\n            # set the name param, the keys now have / in them\n            potion_resource = self._client._resources[resource]\n\n            try:\n                oc_cls = _model_lookup[resource]\n                oc_cls._api = self\n                oc_cls._resource = potion_resource\n                setattr(self, oc_cls.__name__, oc_cls)\n            except KeyError:  # Ignore resources we don't explicitly model\n                pass", "response": "Copy all of the resources over to the toplevel client"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprompting user for username and password gets API key from server", "response": "def login_uname_pwd(server, api_key=None):\n    \"\"\"\n    Prompts user for username and password, gets API key from server\n    if not provided.\n    \"\"\"\n    username = click.prompt(\"Please enter your One Codex (email)\")\n    if api_key is not None:\n        return username, api_key\n\n    password = click.prompt(\"Please enter your password (typing will be hidden)\", hide_input=True)\n\n    # now get the API key\n    api_key = fetch_api_key_from_uname(username, password, server)\n    return username, api_key"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlogs-in to One Codex server.", "response": "def _login(server, creds_file=None, api_key=None, silent=False):\n    \"\"\"\n    Login main function\n    \"\"\"\n    # fetch_api_key and check_version expect server to end in /\n    if server[-1] != \"/\":\n        server = server + \"/\"\n\n    # creds file path setup\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    # check if the creds file exists and is readable\n    if not os.path.exists(creds_file):\n        if silent:\n            return None\n\n        creds = {}\n    elif not os.access(creds_file, os.R_OK):\n        click.echo(\"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True)\n        sys.exit(1)\n    else:\n        # it is, so let's read it!\n        with open(creds_file, \"r\") as fp:\n            try:\n                creds = json.load(fp)\n            except ValueError:\n                click.echo(\n                    \"Your ~/.onecodex credentials file appears to be corrupted. \"  # noqa\n                    \"Please delete it and re-authorize.\",\n                    err=True,\n                )\n                sys.exit(1)\n\n        # check for updates if logged in more than one day ago\n        last_update = creds.get(\"updated_at\") or creds.get(\"saved_at\")\n        last_update = last_update if last_update else datetime.datetime.now().strftime(DATE_FORMAT)\n        diff = datetime.datetime.now() - datetime.datetime.strptime(last_update, DATE_FORMAT)\n\n        if diff.days >= 1:\n            # if creds_file is old, check for updates\n            upgrade_required, msg = check_version(__version__, server)\n            creds[\"updated_at\"] = datetime.datetime.now().strftime(DATE_FORMAT)\n\n            try:\n                json.dump(creds, open(creds_file, \"w\"))\n            except Exception as e:\n                if e.errno == errno.EACCES:\n                    click.echo(\n                        \"Please check the permissions on {}\".format(collapse_user(creds_file)),\n                        err=True,\n                    )\n                    sys.exit(1)\n                else:\n                    raise\n\n            if upgrade_required:\n                click.echo(\"\\nWARNING: {}\\n\".format(msg), err=True)\n\n        # finally, give the user back what they want (whether silent or not)\n        if silent:\n            return creds.get(\"api_key\", None)\n\n        click.echo(\n            \"Credentials file already exists ({}). Logout first.\".format(collapse_user(creds_file)),\n            err=True,\n        )\n        return creds.get(\"email\", None)\n\n    # creds_file was not found and we're not silent, so prompt user to login\n    email, api_key = login_uname_pwd(server, api_key=api_key)\n\n    if api_key is None:\n        click.echo(\n            \"We could not verify your credentials. Either you mistyped your email \"\n            \"or password, or your account does not currently have API access. \"\n            \"Please contact help@onecodex.com if you continue to experience problems.\"\n        )\n        sys.exit(1)\n\n    creds.update(\n        {\n            \"api_key\": api_key,\n            \"saved_at\": datetime.datetime.now().strftime(DATE_FORMAT),\n            \"updated_at\": None,\n            \"email\": email,\n        }\n    )\n\n    try:\n        json.dump(creds, open(creds_file, \"w\"))\n    except Exception as e:\n        if e.errno == errno.EACCES:\n            click.echo(\"Please check the permissions on {}\".format(creds_file), err=True)\n            sys.exit(1)\n        else:\n            raise\n\n    click.echo(\"Your ~/.onecodex credentials file was successfully created.\", err=True)\n\n    return email"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving the creds file if it exists.", "response": "def _remove_creds(creds_file=None):\n    \"\"\"\n    Remove ~/.onecodex file, returning True if successul or False if the file didn't exist\n    \"\"\"\n    if creds_file is None:\n        creds_file = os.path.expanduser(\"~/.onecodex\")\n\n    try:\n        os.remove(creds_file)\n    except Exception as e:\n        if e.errno == errno.ENOENT:\n            return False\n        elif e.errno == errno.EACCES:\n            click.echo(\n                \"Please check the permissions on {}\".format(collapse_user(creds_file)), err=True\n            )\n            sys.exit(1)\n        else:\n            raise\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login_required(fn):\n\n    @wraps(fn)\n    def login_wrapper(ctx, *args, **kwargs):\n        base_url = os.environ.get(\"ONE_CODEX_API_BASE\", \"https://app.onecodex.com\")\n\n        api_kwargs = {\"telemetry\": ctx.obj[\"TELEMETRY\"]}\n\n        api_key_prior_login = ctx.obj.get(\"API_KEY\")\n        bearer_token_env = os.environ.get(\"ONE_CODEX_BEARER_TOKEN\")\n        api_key_env = os.environ.get(\"ONE_CODEX_API_KEY\")\n        api_key_creds_file = _login(base_url, silent=True)\n\n        if api_key_prior_login is not None:\n            api_kwargs[\"api_key\"] = api_key_prior_login\n        elif bearer_token_env is not None:\n            api_kwargs[\"bearer_token\"] = bearer_token_env\n        elif api_key_env is not None:\n            api_kwargs[\"api_key\"] = api_key_env\n        elif api_key_creds_file is not None:\n            api_kwargs[\"api_key\"] = api_key_creds_file\n        else:\n            click.echo(\n                \"The command you specified requires authentication. Please login first.\\n\", err=True\n            )\n            ctx.exit()\n\n        ctx.obj[\"API\"] = Api(**api_kwargs)\n\n        return fn(ctx, *args, **kwargs)\n\n    return login_wrapper", "response": "Decorator for CLI commands that require login before proceeding."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the complete results table for the classification.", "response": "def results(self, json=True):\n        \"\"\"\n        Returns the complete results table for the classification.\n\n        Parameters\n        ----------\n        json : bool, optional\n            Return result as JSON? Default True.\n\n        Returns\n        -------\n        table : dict | DataFrame\n            Return a JSON object with the classification results or a Pandas DataFrame\n            if json=False.\n        \"\"\"\n        if json is True:\n            return self._results()\n        else:\n            return self._table()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef abundances(self, ids=None):\n        # TODO: Consider removing this method... since it's kind of trivial\n        #       May want to replace with something that actually gets genome-size adjusted\n        #       abundances from the results table\n        if ids is None:\n            # get the data frame\n            return self.table()\n\n        else:\n            res = self.table()\n            return res[res[\"tax_id\"].isin(ids)]", "response": "Query the results table to get the abundance data for all or some tax ids\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave changes on this Samples object back to One Codex server along with any changes on its metadata.", "response": "def save(self):\n        \"\"\"\n        Persist changes on this Samples object back to the One Codex server along with any changes\n        on its metadata (if it has any).\n        \"\"\"\n        super(Samples, self).save()\n        if self.metadata is not None:\n            self.metadata.save()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload(\n        cls, files, metadata=None, tags=None, project=None, coerce_ascii=False, progressbar=None\n    ):\n        \"\"\"Uploads a series of files to the One Codex server.\n\n        Parameters\n        ----------\n        files : `string` or `tuple`\n            A single path to a file on the system, or a tuple containing a pairs of paths. Tuple\n            values  will be interleaved as paired-end reads and both files should contain the same\n            number of records. Paths to single files will be uploaded as-is.\n        metadata : `dict`, optional\n        tags : `list`, optional\n        project : `string`, optional\n            UUID of project to associate this sample with.\n        coerce_ascii : `bool`, optional\n            If true, rename unicode filenames to ASCII and issue warning.\n        progressbar : `click.progressbar`, optional\n            If passed, display a progress bar using Click.\n\n        Returns\n        -------\n        A `Samples` object upon successful upload. None if the upload failed.\n        \"\"\"\n        res = cls._resource\n        if not isinstance(files, string_types) and not isinstance(files, tuple):\n            raise OneCodexException(\n                \"Please pass a string or tuple or forward and reverse filepaths.\"\n            )\n\n        if not isinstance(project, Projects) and project is not None:\n            project_search = Projects.get(project)\n            if not project_search:\n                project_search = Projects.where(name=project)\n            if not project_search:\n                try:\n                    project_search = Projects.where(project_name=project)\n                except HTTPError:\n                    project_search = None\n            if not project_search:\n                raise OneCodexException(\"{} is not a valid project UUID\".format(project))\n\n            if isinstance(project_search, list):\n                project = project_search[0]\n\n        sample_id = upload_sequence(\n            files,\n            res._client.session,\n            res,\n            metadata=metadata,\n            tags=tags,\n            project=project,\n            coerce_ascii=coerce_ascii,\n            progressbar=progressbar,\n        )\n\n        return cls.get(sample_id)", "response": "Uploads a series of files to One Codex server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_comments(self, comments):\n        for comment in comments:\n            if comment not in self.comments and len(comment) > 0:\n                self.comments.append(comment)\n            if len(self.comments[0]) == 0:\n                self.comments.pop(0)", "response": "Adds comments to the localization entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging the new translations with the old ones.", "response": "def merge_translations(localization_bundle_path):\n    \"\"\" Merges the new translation with the old one.\n\n    The translated files are saved as '.translated' file, and are merged with old translated file.\n\n    Args:\n        localization_bundle_path (str): The path to the localization bundle.\n\n    \"\"\"\n    logging.info(\"Merging translations\")\n    for lang_dir in os.listdir(localization_bundle_path):\n        if lang_dir == DEFAULT_LANGUAGE_DIRECTORY_NAME:\n            continue\n        for translated_path in glob.glob(os.path.join(localization_bundle_path, lang_dir, \"*\" + TRANSLATED_SUFFIX)):\n            strings_path = translated_path[:-1 * len(TRANSLATED_SUFFIX)]\n            localizable_path = os.path.join(localization_bundle_path,\n                                            DEFAULT_LANGUAGE_DIRECTORY_NAME,\n                                            os.path.basename(strings_path))\n\n            localization_merge_back(localizable_path, strings_path, translated_path, strings_path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_bargraph(\n        self,\n        rank=\"auto\",\n        normalize=\"auto\",\n        top_n=\"auto\",\n        threshold=\"auto\",\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        tooltip=None,\n        return_chart=False,\n        haxis=None,\n        legend=\"auto\",\n        label=None,\n    ):\n        \"\"\"Plot a bargraph of relative abundance of taxa for multiple samples.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        normalize : 'auto' or `bool`, optional\n            Convert read counts to relative abundances such that each sample sums to 1.0. Setting\n            'auto' will choose automatically based on the data.\n        return_chart : `bool`, optional\n            When True, return an `altair.Chart` object instead of displaying the resulting plot in\n            the current notebook.\n        top_n : `int`, optional\n            Display the top N most abundant taxa in the entire cohort of samples.\n        threshold : `float`\n            Display only taxa that are more abundant that this threshold in one or more samples.\n        title : `string`, optional\n            Text label at the top of the plot.\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n        tooltip : `string` or `list`, optional\n            A string or list containing strings representing metadata fields. When a point in the\n            plot is hovered over, the value of the metadata associated with that sample will be\n            displayed in a modal.\n        haxis : `string`, optional\n            The metadata field (or tuple containing multiple categorical fields) used to group\n            samples together.\n        legend: `string`, optional\n            Title for color scale. Defaults to the field used to generate the plot, e.g.\n            readcount_w_children or abundance.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Plot a bargraph of the top 10 most abundant genera\n\n        >>> plot_bargraph(rank='genus', top_n=10)\n        \"\"\"\n        if rank is None:\n            raise OneCodexException(\"Please specify a rank or 'auto' to choose automatically\")\n\n        if not (threshold or top_n):\n            raise OneCodexException(\"Please specify at least one of: threshold, top_n\")\n\n        if top_n == \"auto\" and threshold == \"auto\":\n            top_n = 10\n            threshold = None\n        elif top_n == \"auto\" and threshold != \"auto\":\n            top_n = None\n        elif top_n != \"auto\" and threshold == \"auto\":\n            threshold = None\n\n        if legend == \"auto\":\n            legend = self._field\n\n        df = self.to_df(\n            rank=rank, normalize=normalize, top_n=top_n, threshold=threshold, table_format=\"long\"\n        )\n\n        if tooltip:\n            if not isinstance(tooltip, list):\n                tooltip = [tooltip]\n        else:\n            tooltip = []\n\n        if haxis:\n            tooltip.append(haxis)\n\n        tooltip.insert(0, \"Label\")\n\n        # takes metadata columns and returns a dataframe with just those columns\n        # renames columns in the case where columns are taxids\n        magic_metadata, magic_fields = self._metadata_fetch(tooltip, label=label)\n\n        # add sort order to long-format df\n        if haxis:\n            sort_order = magic_metadata.sort_values(magic_fields[haxis]).index.tolist()\n\n            for sort_num, sort_class_id in enumerate(sort_order):\n                magic_metadata.loc[sort_class_id, \"sort_order\"] = sort_num\n\n            df[\"sort_order\"] = magic_metadata[\"sort_order\"][df[\"classification_id\"]].tolist()\n\n            sort_order = alt.EncodingSortField(field=\"sort_order\", op=\"mean\")\n        else:\n            sort_order = None\n\n        # transfer metadata from wide-format df (magic_metadata) to long-format df\n        for f in tooltip:\n            df[magic_fields[f]] = magic_metadata[magic_fields[f]][df[\"classification_id\"]].tolist()\n\n        # add taxa names\n        df[\"tax_name\"] = [\n            \"{} ({})\".format(self.taxonomy[\"name\"][t], t) if t in self.taxonomy[\"name\"] else t\n            for t in df[\"tax_id\"]\n        ]\n\n        #\n        # TODO: how to sort bars in bargraph\n        # - abundance (mean across all samples)\n        # - parent taxon (this will require that we make a few assumptions\n        # about taxonomic ranks but as all taxonomic data will be coming from\n        # OCX this should be okay)\n        #\n\n        ylabel = self._field if ylabel is None else ylabel\n        xlabel = \"\" if xlabel is None else xlabel\n\n        # should ultimately be Label, tax_name, readcount_w_children, then custom fields\n        tooltip_for_altair = [magic_fields[f] for f in tooltip]\n        tooltip_for_altair.insert(1, \"tax_name\")\n        tooltip_for_altair.insert(2, \"{}:Q\".format(self._field))\n\n        # generate dataframes to plot, one per facet\n        dfs_to_plot = []\n\n        if haxis:\n            # if using facets, first facet is just the vertical axis\n            blank_df = df.iloc[:1].copy()\n            blank_df[self._field] = 0\n\n            dfs_to_plot.append(blank_df)\n\n            for md_val in magic_metadata[magic_fields[haxis]].unique():\n                plot_df = df.where(df[magic_fields[haxis]] == md_val).dropna()\n\n                # preserve booleans\n                if magic_metadata[magic_fields[haxis]].dtype == \"bool\":\n                    plot_df[magic_fields[haxis]] = plot_df[magic_fields[haxis]].astype(bool)\n\n                dfs_to_plot.append(plot_df)\n        else:\n            dfs_to_plot.append(df)\n\n        charts = []\n\n        for plot_num, plot_df in enumerate(dfs_to_plot):\n            chart = (\n                alt.Chart(plot_df)\n                .mark_bar()\n                .encode(\n                    x=alt.X(\"Label\", axis=alt.Axis(title=xlabel), sort=sort_order),\n                    y=alt.Y(\n                        self._field,\n                        axis=alt.Axis(title=ylabel),\n                        scale=alt.Scale(domain=[0, 1], zero=True, nice=False),\n                    ),\n                    color=alt.Color(\"tax_name\", legend=alt.Legend(title=legend)),\n                    tooltip=tooltip_for_altair,\n                    href=\"url:N\",\n                )\n            )\n\n            if haxis:\n                if plot_num == 0:\n                    # first plot (blank_df) has vert axis but no horiz axis\n                    chart.encoding.x.axis = None\n                elif plot_num > 0:\n                    # strip vertical axis from subsequent facets\n                    chart.encoding.y.axis = None\n\n                    # facet's title set to value of metadata in this group\n                    chart.title = str(plot_df[magic_fields[haxis]].tolist()[0])\n\n            charts.append(chart)\n\n        # add all the facets together\n        final_chart = charts[0]\n\n        if len(charts) > 1:\n            for chart in charts[1:]:\n                final_chart |= chart\n\n        # add title to chart\n        # (cannot specify None or False for no title)\n        final_chart = final_chart.properties(title=title) if title else final_chart\n        return final_chart if return_chart else final_chart.display()", "response": "Plot a bargraph of relative abundances of the cohort of samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef raise_api_error(resp, state=None):\n    # TODO: Refactor into an Exception class\n    error_code = resp.status_code\n\n    if error_code == 402:\n        error_message = (\n            \"Please add a payment method to upload more samples. If you continue to \"\n            \"experience problems, contact us at help@onecodex.com for assistance.\"\n        )\n    elif error_code == 403:\n        error_message = \"Please login to your One Codex account or pass the appropriate API key.\"\n    else:\n        try:\n            error_json = resp.json()\n        except ValueError:\n            error_json = {}\n\n        if \"msg\" in error_json:\n            error_message = error_json[\"msg\"].rstrip(\".\")\n        elif \"message\" in error_json:\n            error_message = error_json[\"message\"].rstrip(\".\")\n        else:\n            error_message = None\n\n        if state == \"init\" and not error_message:\n            error_message = (\n                \"Could not initialize upload. Are you logged in? If this problem \"\n                \"continues, please contact help@onecodex.com for assistance.\"\n            )\n        elif state == \"upload\" and not error_message:\n            error_message = (\n                \"File could not be uploaded. If this problem continues, please contact \"\n                \"help@onecodex.com for assistance.\"\n            )\n        elif state == \"callback\" and not error_message:\n            error_message = (\n                \"Callback could not be completed. If this problem continues, please \"\n                \"contact help@onecodex.com for assistance.\"\n            )\n\n    if error_message is None:\n        error_message = \"Upload failed. Please contact help@onecodex.com for assistance.\"\n\n    raise UploadException(error_message)", "response": "Raise an exception with a pretty message in various states of upload."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_pca(\n        self,\n        rank=\"auto\",\n        normalize=\"auto\",\n        org_vectors=0,\n        org_vectors_scale=None,\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        color=None,\n        size=None,\n        tooltip=None,\n        return_chart=False,\n        label=None,\n    ):\n        \"\"\"Perform principal component analysis and plot first two axes.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        normalize : 'auto' or `bool`, optional\n            Convert read counts to relative abundances such that each sample sums to 1.0. Setting\n            'auto' will choose automatically based on the data.\n        org_vectors : `int`, optional\n            Plot this many of the top-contributing eigenvectors from the PCA results.\n        org_vectors_scale : `float`, optional\n            Multiply the length of the lines representing the eigenvectors by this constant.\n        title : `string`, optional\n            Text label at the top of the plot.\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n        size : `string` or `tuple`, optional\n            A string or a tuple containing strings representing metadata fields. The size of points\n            in the resulting plot will change based on the metadata associated with each sample.\n        color : `string` or `tuple`, optional\n            A string or a tuple containing strings representing metadata fields. The color of points\n            in the resulting plot will change based on the metadata associated with each sample.\n        tooltip : `string` or `list`, optional\n            A string or list containing strings representing metadata fields. When a point in the\n            plot is hovered over, the value of the metadata associated with that sample will be\n            displayed in a modal.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Perform PCA on relative abundances at the species-level and color the resulting points by\n        'geo_loc_name', a metadata field representing the geographical origin of each sample.\n\n        >>> plot_pca(rank='species', normalize=True, color='geo_loc_name')\n\n        Change the size of each point in the plot based on the abundance of Bacteroides.\n\n        >>> plot_pca(size='Bacteroides')\n\n        Display the abundances of Bacteroides, Prevotella, and Bifidobacterium in each sample when\n        hovering over points in the plot.\n\n        >>> plot_pca(tooltip=['Bacteroides', 'Prevotella', 'Bifidobacterium'])\n        \"\"\"\n        if rank is None:\n            raise OneCodexException(\"Please specify a rank or 'auto' to choose automatically\")\n\n        if len(self._results) < 2:\n            raise OneCodexException(\"`plot_pca` requires 2 or more valid classification results.\")\n\n        df = self.to_df(rank=rank, normalize=normalize)\n\n        if len(df.columns) < 2:\n            raise OneCodexException(\"Too few taxa in results. Need at least 2 for PCA.\")\n\n        if tooltip:\n            if not isinstance(tooltip, list):\n                tooltip = [tooltip]\n        else:\n            tooltip = []\n\n        tooltip.insert(0, \"Label\")\n\n        if color and color not in tooltip:\n            tooltip.insert(1, color)\n\n        if size and size not in tooltip:\n            tooltip.insert(2, size)\n\n        magic_metadata, magic_fields = self._metadata_fetch(tooltip, label=label)\n\n        pca = PCA()\n        pca_vals = pca.fit(df.values).transform(df.values)\n        pca_vals = pd.DataFrame(pca_vals, index=df.index)\n        pca_vals.rename(columns=lambda x: \"PC{}\".format(x + 1), inplace=True)\n\n        # label the axes\n        if xlabel is None:\n            xlabel = \"PC1 ({}%)\".format(round(pca.explained_variance_ratio_[0] * 100, 2))\n        if ylabel is None:\n            ylabel = \"PC2 ({}%)\".format(round(pca.explained_variance_ratio_[1] * 100, 2))\n\n        # don't send all the data to vega, just what we're plotting\n        plot_data = pd.concat(\n            [pca_vals.loc[:, (\"PC1\", \"PC2\")], magic_metadata], axis=1\n        ).reset_index()\n\n        alt_kwargs = dict(\n            x=alt.X(\"PC1\", axis=alt.Axis(title=xlabel)),\n            y=alt.Y(\"PC2\", axis=alt.Axis(title=ylabel)),\n            tooltip=[magic_fields[t] for t in tooltip],\n            href=\"url:N\",\n            url=\"https://app.onecodex.com/classification/\" + alt.datum.classification_id,\n        )\n\n        # only add these parameters if they are in use\n        if color:\n            alt_kwargs[\"color\"] = magic_fields[color]\n        if size:\n            alt_kwargs[\"size\"] = magic_fields[size]\n\n        chart = (\n            alt.Chart(plot_data)\n            .transform_calculate(url=alt_kwargs.pop(\"url\"))\n            .mark_circle()\n            .encode(**alt_kwargs)\n        )\n\n        if title:\n            chart = chart.properties(title=title)\n\n        # plot the organism eigenvectors that contribute the most\n        if org_vectors > 0:\n            plot_data = {\n                \"x\": [],\n                \"y\": [],\n                \"o\": [],  # order these points should be connected in\n                \"Eigenvectors\": [],\n            }\n\n            magnitudes = np.sqrt(pca.components_[0] ** 2 + pca.components_[1] ** 2)\n            magnitudes.sort()\n            cutoff = magnitudes[-1 * org_vectors]\n\n            if org_vectors_scale is None:\n                org_vectors_scale = 0.8 * np.max(pca_vals.abs().values)\n\n            for tax_id, var1, var2 in zip(\n                df.columns.values, pca.components_[0, :], pca.components_[1, :]\n            ):\n                if np.sqrt(var1 ** 2 + var2 ** 2) >= cutoff:\n                    plot_data[\"x\"].extend([0, var1 * float(org_vectors_scale)])\n                    plot_data[\"y\"].extend([0, var2 * float(org_vectors_scale)])\n                    plot_data[\"o\"].extend([0, 1])\n                    plot_data[\"Eigenvectors\"].extend([self.taxonomy[\"name\"][tax_id]] * 2)\n\n                    org_vectors -= 1\n\n                    if org_vectors == 0:\n                        break\n\n            plot_data = pd.DataFrame(plot_data)\n\n            vector_chart = (\n                alt.Chart(plot_data)\n                .mark_line(point=False)\n                .encode(\n                    x=alt.X(\"x\", axis=None),\n                    y=alt.Y(\"y\", axis=None),\n                    order=\"o\",\n                    color=\"Eigenvectors\",\n                )\n            )\n\n            chart += vector_chart\n\n        if return_chart:\n            return chart\n        else:\n            chart.interactive().display()", "response": "Plot the principal component of the metadata for a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pretty_print_error(err_json):\n    # Special case validation errors\n    if len(err_json) == 1 and \"validationOf\" in err_json[0]:\n        required_fields = \", \".join(err_json[0][\"validationOf\"][\"required\"])\n        return \"Validation error. Requires properties: {}.\".format(required_fields)\n\n    # General error handling\n    msg = \"; \".join(err.get(\"message\", \"\") for err in err_json)\n\n    # Fallback\n    if not msg:\n        msg = \"Bad request.\"\n    return msg", "response": "Pretty print Flask - Potion error messages for the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _to_json(self, include_references=True):\n        if include_references:\n            return json.dumps(self._resource._properties, cls=PotionJSONEncoder)\n        else:\n            return json.dumps(\n                {\n                    k: v\n                    for k, v in self._resource._properties.items()\n                    if not isinstance(v, Resource) and not k.startswith(\"$\")\n                },\n                cls=PotionJSONEncoder,\n            )", "response": "Convert the model to JSON using the PotionJSONEncode and automatically generates the resource as needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all(cls, sort=None, limit=None):\n        return cls.where(sort=sort, limit=limit)", "response": "Returns all objects of this type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef where(cls, *filters, **keyword_filters):\n        check_bind(cls)\n\n        # do this here to avoid passing this on to potion\n        filter_func = keyword_filters.pop(\"filter\", None)\n\n        public = False\n        if any(x[\"rel\"] == \"instances_public\" for x in cls._resource._schema[\"links\"]):\n            public = keyword_filters.pop(\"public\", False)\n\n        instances_route = keyword_filters.pop(\n            \"_instances\", \"instances\" if not public else \"instances_public\"\n        )\n\n        schema = next(l for l in cls._resource._schema[\"links\"] if l[\"rel\"] == instances_route)\n        sort_schema = schema[\"schema\"][\"properties\"][\"sort\"][\"properties\"]\n        where_schema = schema[\"schema\"][\"properties\"][\"where\"][\"properties\"]\n\n        sort = generate_potion_sort_clause(keyword_filters.pop(\"sort\", None), sort_schema)\n        limit = keyword_filters.pop(\"limit\", None if not public else 1000)\n        where = {}\n\n        # we're filtering by fancy objects (like SQLAlchemy's filter)\n        if len(filters) > 0:\n            if len(filters) == 1 and isinstance(filters[0], dict):\n                where = filters[0]\n            elif all(isinstance(f, six.string_types) for f in filters):\n                # if it's a list of strings, treat it as an multiple \"get\" request\n                where = {\"$uri\": {\"$in\": [cls._convert_id_to_uri(f) for f in filters]}}\n            else:\n                # we're doing some more advanced filtering\n                raise NotImplementedError(\"Advanced filtering hasn't been implemented yet\")\n\n        # we're filtering by keyword arguments (like SQLAlchemy's filter_by)\n        if len(keyword_filters) > 0:\n            for k, v in generate_potion_keyword_where(keyword_filters, where_schema, cls).items():\n                if k in where:\n                    raise AttributeError(\"Multiple definitions for same field {}\".format(k))\n                where[k] = v\n\n        # the potion-client method returns an iterator (which lazily fetchs the records\n        # using `per_page` instances per request) so for limiting we only want to fetch the first\n        # n (and not instantiate all the available which is what would happen if we just sliced)\n        cursor = getattr(cls._resource, instances_route)(\n            where=where, sort=sort, per_page=DEFAULT_PAGE_SIZE\n        )\n        if limit is not None:\n            cursor = itertools.islice(cursor, limit)\n\n        # finally, apply local filtering function on objects before returning\n        wrapped = [cls(_resource=r) for r in cursor]\n\n        if filter_func:\n            if callable(filter_func):\n                wrapped = [obj for obj in wrapped if filter_func(obj) is True]\n            else:\n                raise OneCodexException(\n                    \"Expected callable for filter, got: {}\".format(type(filter_func).__name__)\n                )\n\n        return wrapped", "response": "Returns a list of objects matching the given filters."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves one specific object from the server by its UUID.", "response": "def get(cls, uuid):\n        \"\"\"Retrieve one specific object from the server by its UUID (unique 16-character id). UUIDs\n        can be found in the web browser's address bar while viewing analyses and other objects.\n\n        Parameters\n        ----------\n        uuid : string\n            UUID of the object to retrieve.\n\n        Returns\n        -------\n        OneCodexBase | None\n            The object with that UUID or None if no object could be found.\n\n        Examples\n        --------\n        >>> api.Samples.get('xxxxxxxxxxxxxxxx')\n        <Sample xxxxxxxxxxxxxxxx>\n        \"\"\"\n        check_bind(cls)\n\n        # we're just retrieving one object from its uuid\n        try:\n            resource = cls._resource.fetch(uuid)\n            if isinstance(resource, list):\n                # TODO: Investigate why potion .fetch()\n                #       method is occassionally returning a list here...\n                if len(resource) == 1:\n                    resource = resource[0]\n                else:\n                    raise TypeError(\"Potion-Client error in fetching resource\")\n        except HTTPError as e:\n            # 404 error means this doesn't exist\n            if e.response.status_code == 404:\n                return None\n            else:\n                raise e\n        return cls(_resource=resource)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete this object from One Codex server.", "response": "def delete(self):\n        \"\"\"Delete this object from the One Codex server.\"\"\"\n        check_bind(self)\n        if self.id is None:\n            raise ServerError(\"{} object does not exist yet\".format(self.__class__.name))\n        elif not self.__class__._has_schema_method(\"destroy\"):\n            raise MethodNotSupported(\"{} do not support deletion.\".format(self.__class__.__name__))\n\n        try:\n            self._resource.delete()\n        except HTTPError as e:\n            if e.response.status_code == 403:\n                raise PermissionDenied(\"\")  # FIXME: is this right?\n            else:\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the filename used to represent a set of paired - end files.", "response": "def interleaved_filename(file_path):\n    \"\"\"Return filename used to represent a set of paired-end files. Assumes Illumina-style naming\n    conventions where each file has _R1_ or _R2_ in its name.\"\"\"\n    if not isinstance(file_path, tuple):\n        raise OneCodexException(\"Cannot get the interleaved filename without a tuple.\")\n    if re.match(\".*[._][Rr][12][_.].*\", file_path[0]):\n        return re.sub(\"[._][Rr][12]\", \"\", file_path[0])\n    else:\n        warnings.warn(\"Paired-end filenames do not match--are you sure they are correct?\")\n        return file_path[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _file_size(file_path, uncompressed=False):\n    _, ext = os.path.splitext(file_path)\n\n    if uncompressed:\n        if ext in {\".gz\", \".gzip\"}:\n            with gzip.GzipFile(file_path, mode=\"rb\") as fp:\n                try:\n                    fp.seek(0, os.SEEK_END)\n                    return fp.tell()\n                except ValueError:\n                    # on python2, cannot seek from end and must instead read to end\n                    fp.seek(0)\n                    while len(fp.read(8192)) != 0:\n                        pass\n                    return fp.tell()\n        elif ext in {\".bz\", \".bz2\", \".bzip\", \".bzip2\"}:\n            with bz2.BZ2File(file_path, mode=\"rb\") as fp:\n                fp.seek(0, os.SEEK_END)\n                return fp.tell()\n\n    return os.path.getsize(file_path)", "response": "Return size of a single file compressed or uncompressed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn information about the file path or paths if paired.", "response": "def _file_stats(file_path, enforce_fastx=True):\n    \"\"\"Return information about the file path (or paths, if paired), prior to upload.\n\n    Parameters\n    ----------\n    file_path : `string` or `tuple`\n        System path to the file(s) to be uploaded\n\n    Returns\n    -------\n    `string`\n        Filename, minus compressed extension (.gz or .bz2). If paired, use first path to generate\n        the filename that will be used to represent both paths in the pair.\n    `integer`\n        If paired, the uncompressed file size of both files in the path. If single, the raw file\n        size whether compressed or not. Pairs are always uploaded uncompressed, whereas single files\n        are uploaded in whatever format they're in. One Codex will uncompress and re-compress as\n        appropriate.\n    {'fasta', 'fastq'}\n        The format of the file being uploaded, guessed only by its extension. If paired, this\n        determines how many lines to pull from each file during interleaving.\n    \"\"\"\n    if isinstance(file_path, tuple):\n        assert len(file_path) == 2\n        file_size = sum(_file_size(f, uncompressed=True) for f in file_path)\n        file_path = interleaved_filename(file_path)\n        paired = True\n    else:\n        file_size = _file_size(file_path, uncompressed=False)\n        paired = False\n\n    new_filename, ext = os.path.splitext(os.path.basename(file_path))\n\n    if ext in {\".gz\", \".gzip\", \".bz\", \".bz2\", \".bzip\"}:\n        compressed = ext\n        new_filename, ext = os.path.splitext(new_filename)\n    else:\n        compressed = \"\"\n\n    # strip compressed extension if paired-end, since we're going to upload uncompressed\n    if paired and compressed:\n        final_filename = new_filename + ext\n    else:\n        final_filename = new_filename + ext + compressed\n\n    if enforce_fastx:\n        if ext in {\".fa\", \".fna\", \".fasta\"}:\n            file_format = \"fasta\"\n        elif ext in {\".fq\", \".fastq\"}:\n            file_format = \"fastq\"\n        else:\n            raise UploadException(\n                \"{}: extension must be one of .fa, .fna, .fasta, .fq, .fastq\".format(final_filename)\n            )\n    else:\n        file_format = None\n\n    if file_size == 0:\n        raise UploadException(\"{}: empty files can not be uploaded\".format(final_filename))\n\n    return final_filename, file_size, file_format"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _call_init_upload(file_name, file_size, metadata, tags, project, samples_resource):\n    upload_args = {\n        \"filename\": file_name,\n        \"size\": file_size,\n        \"upload_type\": \"standard\",  # this is multipart form data\n    }\n\n    if metadata:\n        # format metadata keys as snake case\n        new_metadata = {}\n\n        for md_key, md_val in metadata.items():\n            new_metadata[snake_case(md_key)] = md_val\n\n        upload_args[\"metadata\"] = new_metadata\n\n    if tags:\n        upload_args[\"tags\"] = tags\n\n    if project:\n        upload_args[\"project\"] = getattr(project, \"id\", project)\n\n    try:\n        upload_info = samples_resource.init_upload(upload_args)\n    except requests.exceptions.HTTPError as e:\n        raise_api_error(e.response, state=\"init\")\n    except requests.exceptions.ConnectionError:\n        raise_connectivity_error(file_name)\n\n    return upload_info", "response": "Call init_upload at One Codex API and return data used to upload the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _make_retry_fields(file_name, metadata, tags, project):\n    upload_args = {\"filename\": file_name}\n\n    if metadata:\n        # format metadata keys as snake case\n        new_metadata = {}\n\n        for md_key, md_val in metadata.items():\n            new_metadata[snake_case(md_key)] = md_val\n\n        upload_args[\"metadata\"] = new_metadata\n\n    if tags:\n        upload_args[\"tags\"] = tags\n\n    if project:\n        upload_args[\"project\"] = getattr(project, \"id\", project)\n\n    return upload_args", "response": "Generate fields to send to init_multipart_upload in the case that a Sample upload via fastx - proxy fails."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuploads a sequence file or pair of files to One Codex server.", "response": "def upload_sequence(\n    files,\n    session,\n    samples_resource,\n    metadata=None,\n    tags=None,\n    project=None,\n    coerce_ascii=False,\n    progressbar=None,\n):\n    \"\"\"Uploads a sequence file (or pair of files) to the One Codex server via either our proxy or directly to S3.\n\n    Parameters\n    ----------\n    files : `list`\n        A list of paths to files on the system, or tuples containing pairs of paths. Tuples will be\n        interleaved as paired-end reads and both files should contain the same number of records.\n        Paths to single files will be uploaded as-is.\n    session : `requests.Session`\n        Connection to One Codex API.\n    samples_resource : `onecodex.models.Samples`\n        Wrapped potion-client object exposing `init_upload` and `confirm_upload` methods.\n    metadata : `dict`, optional\n    tags : `list`, optional\n    project : `string`, optional\n        UUID of project to associate this sample with.\n    coerce_ascii : `bool`, optional\n        If true, rename unicode filenames to ASCII and issue warning.\n    progressbar : `click.progressbar`, optional\n        If passed, display a progress bar using Click.\n\n    Returns\n    -------\n    A Sample object for the completed upload.\n    \"\"\"\n    filename, file_size, file_format = _file_stats(files)\n\n    # if filename cannot be represented as ascii, raise and suggest renaming\n    try:\n        # python2\n        ascii_fname = unidecode(unicode(filename))\n    except NameError:\n        ascii_fname = unidecode(filename)\n\n    if filename != ascii_fname:\n        if coerce_ascii:\n            # TODO: Consider warnings.warn here instead\n            logging.warn(\n                \"Renaming {} to {}, must be ASCII\\n\".format(filename.encode(\"utf-8\"), ascii_fname)\n            )\n            filename = ascii_fname\n        else:\n            raise OneCodexException(\"Filenames must be ascii. Try using --coerce-ascii\")\n\n    # disable progressbar while keeping context manager\n    if not progressbar:\n        progressbar = FakeProgressBar()\n\n    # file_path is the path to the file on this disk. file_name is what we'll call the file in the\n    # mainline database. file_size is the sum of both files in a pair, or the size of an unpaired\n    # file. if paired, file_size is the uncompressed size. if unpaired, file_size is the actual\n    # size on disk. unpaired files are uploaded as-is. paired files are decompressed, interleaved,\n    # and uploaded as uncompressed data.\n    with progressbar as bar:\n        if isinstance(files, tuple):\n            fobj = FASTXInterleave(files, file_size, file_format, bar)\n        else:\n            fobj = FilePassthru(files, file_size, bar)\n\n        # must call init_upload in this loop in order to get a sample uuid we can call\n        # cancel_upload on later if user hits ctrl+c\n        fields = _call_init_upload(filename, file_size, metadata, tags, project, samples_resource)\n\n        def cancel_atexit():\n            bar.canceled = True\n            bar.update(1)\n            logging.info(\"Canceled upload for sample: {}\".format(fields[\"sample_id\"]))\n            samples_resource.cancel_upload({\"sample_id\": fields[\"sample_id\"]})\n\n        atexit_register(cancel_atexit)\n\n        # if the upload via init_upload fails, upload_sequence_fileobj will call\n        # init_multipart_upload, which accepts metadata to be integrated into a newly-created\n        # Sample model. if the s3 intermediate route is used, two Sample models will ultimately\n        # exist on mainline: the failed fastx-proxy upload and the successful s3 intermediate.\n        retry_fields = _make_retry_fields(filename, metadata, tags, project)\n\n        try:\n            sample_id = upload_sequence_fileobj(\n                fobj, filename, fields, retry_fields, session, samples_resource\n            )\n            atexit_unregister(cancel_atexit)\n            return sample_id\n        except KeyboardInterrupt:\n            cancel_atexit()\n            atexit_unregister(cancel_atexit)\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuploads a single file - like object to One Codex and returns a tuple of the HTTP response and HTTP status code.", "response": "def _direct_upload(file_obj, file_name, fields, session, samples_resource):\n    \"\"\"Uploads a single file-like object via our validating proxy. Maintains compatibility with direct upload\n    to a user's S3 bucket as well in case we disable our validating proxy.\n\n    Parameters\n    ----------\n    file_obj : `FASTXInterleave`, `FilePassthru`, or a file-like object\n        A wrapper around a pair of fastx files (`FASTXInterleave`) or a single fastx file. In the\n        case of paired files, they will be interleaved and uploaded uncompressed. In the case of a\n        single file, it will simply be passed through (`FilePassthru`) to One Codex, compressed\n        or otherwise. If a file-like object is given, its mime-type will be sent as 'text/plain'.\n    file_name : `string`\n        The file_name you wish to associate this fastx file with at One Codex.\n    fields : `dict`\n        Additional data fields to include as JSON in the POST. Must include 'sample_id' and\n        'upload_url' at a minimum.\n    samples_resource : `onecodex.models.Samples`\n        Wrapped potion-client object exposing `init_upload` and `confirm_upload` routes to mainline.\n\n    Raises\n    ------\n    RetryableUploadException\n        In cases where the proxy is temporarily down or we experience connectivity issues\n\n    UploadException\n        In other cases where the proxy determines the upload is invalid and should *not* be retried.\n    \"\"\"\n\n    # need an OrderedDict to preserve field order for S3, required for Python 2.7\n    multipart_fields = OrderedDict()\n\n    for k, v in fields[\"additional_fields\"].items():\n        multipart_fields[str(k)] = str(v)\n\n    # this attribute is only in FASTXInterleave and FilePassthru\n    mime_type = getattr(file_obj, \"mime_type\", \"text/plain\")\n    multipart_fields[\"file\"] = (file_name, file_obj, mime_type)\n    encoder = MultipartEncoder(multipart_fields)\n    upload_request = None\n\n    try:\n        upload_request = session.post(\n            fields[\"upload_url\"],\n            data=encoder,\n            headers={\"Content-Type\": encoder.content_type},\n            auth={},\n        )\n    except requests.exceptions.ConnectionError:\n        pass\n\n    # If we expect a status *always* try to check it,\n    # waiting up to 4 hours for buffering to complete (~30-50GB file gzipped)\n    if \"status_url\" in fields[\"additional_fields\"]:\n        now = time.time()\n        while time.time() < (now + 60 * 60 * 4):\n            try:\n                resp = session.post(\n                    fields[\"additional_fields\"][\"status_url\"],\n                    json={\"sample_id\": fields[\"sample_id\"]},\n                )\n                resp.raise_for_status()\n            except (ValueError, requests.exceptions.RequestException) as e:\n                logging.debug(\"Retrying due to error: {}\".format(e))\n                raise RetryableUploadException(\n                    \"Unexpected failure of direct upload proxy. Retrying...\"\n                )\n\n            if resp.json() and resp.json().get(\"complete\", True) is False:\n                logging.debug(\"Blocking on waiting for proxy to complete (in progress)...\")\n                time.sleep(30)\n            else:\n                break\n\n        # Return is successfully processed\n        if resp.json().get(\"code\") in [200, 201]:\n            file_obj.close()\n            return\n        elif resp.json().get(\"code\") == 500:\n            logging.debug(\"Retrying due to 500 from proxy...\")\n            raise RetryableUploadException(\"Unexpected issue with direct upload proxy. Retrying...\")\n        else:\n            raise_api_error(resp, state=\"upload\")\n\n    # Direct to S3 case\n    else:\n        file_obj.close()\n        if upload_request.status_code not in [200, 201]:\n            raise RetryableUploadException(\"Unknown connectivity issue with proxy upload.\")\n\n        # Issue a callback -- this only happens in the direct-to-S3 case\n        try:\n            if not fields[\"additional_fields\"].get(\"callback_url\"):\n                samples_resource.confirm_upload(\n                    {\"sample_id\": fields[\"sample_id\"], \"upload_type\": \"standard\"}\n                )\n        except requests.exceptions.HTTPError as e:\n            raise_api_error(e.response, state=\"callback\")\n        except requests.exceptions.ConnectionError:\n            raise_connectivity_error()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_sequence_fileobj(file_obj, file_name, fields, retry_fields, session, samples_resource):\n\n    # First attempt to upload via our validating proxy\n    try:\n        _direct_upload(file_obj, file_name, fields, session, samples_resource)\n        sample_id = fields[\"sample_id\"]\n    except RetryableUploadException:\n        # upload failed--retry direct upload to S3 intermediate\n        logging.error(\"{}: Connectivity issue, trying direct upload...\".format(file_name))\n        file_obj.seek(0)  # reset file_obj back to start\n\n        try:\n            retry_fields = samples_resource.init_multipart_upload(retry_fields)\n        except requests.exceptions.HTTPError as e:\n            raise_api_error(e.response, state=\"init\")\n        except requests.exceptions.ConnectionError:\n            raise_connectivity_error(file_name)\n\n        s3_upload = _s3_intermediate_upload(\n            file_obj,\n            file_name,\n            retry_fields,\n            session,\n            samples_resource._client._root_url + retry_fields[\"callback_url\"],  # full callback url\n        )\n        sample_id = s3_upload.get(\"sample_id\", \"<UUID not yet assigned>\")\n\n    logging.info(\"{}: finished as sample {}\".format(file_name, sample_id))\n    return sample_id", "response": "Uploads a single file - like object to One Codex server."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload multiple files to One Codex server via an intermediate bucket.", "response": "def upload_document(file_path, session, documents_resource, progressbar=None):\n    \"\"\"Uploads multiple document files to the One Codex server directly to S3 via an intermediate\n    bucket.\n\n    Parameters\n    ----------\n    file_path : `str`\n        A path to a file on the system.\n    session : `requests.Session`\n        Connection to One Codex API.\n    documents_resource : `onecodex.models.Documents`\n        Wrapped potion-client object exposing `init_upload` and `confirm_upload` methods.\n    progressbar : `click.progressbar`, optional\n        If passed, display a progress bar using Click.\n\n    Raises\n    ------\n    UploadException\n        In the case of a fatal exception during an upload.\n\n    Returns\n    -------\n    A `str` document ID for the newly uploaded file.\n    \"\"\"\n    if not isinstance(file_path, six.string_types):\n        raise ValueError(\n            \"Expected file_path to be a string, got {}\".format(type(file_path).__name__)\n        )\n\n    file_name, file_size, _ = _file_stats(file_path, enforce_fastx=False)\n\n    # disable progressbar while keeping context manager\n    if not progressbar:\n        progressbar = FakeProgressBar()\n\n    with progressbar as bar:\n        fobj = FilePassthru(file_path, file_size, bar)\n        document_id = upload_document_fileobj(fobj, file_name, session, documents_resource)\n        bar.finish()\n        return document_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a single file - like object to One Codex server directly to S3.", "response": "def upload_document_fileobj(file_obj, file_name, session, documents_resource, log=None):\n    \"\"\"Uploads a single file-like object to the One Codex server directly to S3.\n\n    Parameters\n    ----------\n    file_obj : `FilePassthru`, or a file-like object\n        If a file-like object is given, its mime-type will be sent as 'text/plain'. Otherwise,\n        `FilePassthru` will send a compressed type if the file is gzip'd or bzip'd.\n    file_name : `string`\n        The file_name you wish to associate this file with at One Codex.\n    fields : `dict`\n        Additional data fields to include as JSON in the POST.\n    session : `requests.Session`\n        Connection to One Codex API.\n    documents_resource : `onecodex.models.Documents`\n        Wrapped potion-client object exposing `init_upload` and `confirm_upload` routes to mainline.\n\n    Notes\n    -----\n    In contrast to `upload_sample_fileobj`, this method will /only/ upload to an S3 intermediate\n    bucket--not via our direct proxy or directly to a user's S3 bucket with a signed request.\n\n    Raises\n    ------\n    UploadException\n        In the case of a fatal exception during an upload.\n\n    Returns\n    -------\n    `string` containing sample UUID of newly uploaded file.\n    \"\"\"\n    try:\n        fields = documents_resource.init_multipart_upload()\n    except requests.exceptions.HTTPError as e:\n        raise_api_error(e.response, state=\"init\")\n    except requests.exceptions.ConnectionError:\n        raise_connectivity_error(file_name)\n\n    s3_upload = _s3_intermediate_upload(\n        file_obj,\n        file_name,\n        fields,\n        session,\n        documents_resource._client._root_url + fields[\"callback_url\"],  # full callback url\n    )\n\n    document_id = s3_upload.get(\"document_id\", \"<UUID not yet assigned>\")\n\n    logging.info(\"{}: finished as document {}\".format(file_name, document_id))\n    return document_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload a single file - like object to an intermediate S3 bucket.", "response": "def _s3_intermediate_upload(file_obj, file_name, fields, session, callback_url):\n    \"\"\"Uploads a single file-like object to an intermediate S3 bucket which One Codex can pull from\n    after receiving a callback.\n\n    Parameters\n    ----------\n    file_obj : `FASTXInterleave`, `FilePassthru`, or a file-like object\n        A wrapper around a pair of fastx files (`FASTXInterleave`) or a single fastx file. In the\n        case of paired files, they will be interleaved and uploaded uncompressed. In the case of a\n        single file, it will simply be passed through (`FilePassthru`) to One Codex, compressed\n        or otherwise. If a file-like object is given, its mime-type will be sent as 'text/plain'.\n    file_name : `string`\n        The file_name you wish to associate this fastx file with at One Codex.\n    fields : `dict`\n        Additional data fields to include as JSON in the POST.\n    callback_url : `string`\n        API callback at One Codex which will trigger a pull from this S3 bucket.\n\n    Raises\n    ------\n    UploadException\n        In the case of a fatal exception during an upload. Note we rely on boto3 to handle its own retry logic.\n\n    Returns\n    -------\n    `dict` : JSON results from internal confirm import callback URL\n    \"\"\"\n    import boto3\n    from boto3.s3.transfer import TransferConfig\n    from boto3.exceptions import S3UploadFailedError\n\n    # actually do the upload\n    client = boto3.client(\n        \"s3\",\n        aws_access_key_id=fields[\"upload_aws_access_key_id\"],\n        aws_secret_access_key=fields[\"upload_aws_secret_access_key\"],\n    )\n\n    # if boto uses threads, ctrl+c won't work\n    config = TransferConfig(use_threads=False)\n\n    # let boto3 update our progressbar rather than our FASTX wrappers, if applicable\n    boto_kwargs = {}\n\n    if hasattr(file_obj, \"progressbar\"):\n        boto_kwargs[\"Callback\"] = file_obj.progressbar.update\n        file_obj.progressbar = None\n\n    try:\n        client.upload_fileobj(\n            file_obj,\n            fields[\"s3_bucket\"],\n            fields[\"file_id\"],\n            ExtraArgs={\"ServerSideEncryption\": \"AES256\"},\n            Config=config,\n            **boto_kwargs\n        )\n    except S3UploadFailedError:\n        raise_connectivity_error(file_name)\n\n    # issue a callback\n    try:\n        resp = session.post(\n            callback_url,\n            json={\n                \"s3_path\": \"s3://{}/{}\".format(fields[\"s3_bucket\"], fields[\"file_id\"]),\n                \"filename\": file_name,\n                \"import_as_document\": fields.get(\"import_as_document\", False),\n            },\n        )\n    except requests.exceptions.ConnectionError:\n        raise_connectivity_error(file_name)\n\n    if resp.status_code != 200:\n        raise_connectivity_error(file_name)\n\n    try:\n        return resp.json()\n    except ValueError:\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling by the file object to seek to the specified location.", "response": "def seek(self, loc):\n        \"\"\"Called if upload fails and must be retried.\"\"\"\n        assert loc == 0\n\n        # rewind progress bar\n        if self.progressbar:\n            self.progressbar.update(-self._tell)\n\n        self._fp_left.seek(loc)\n        self._fp_right.seek(loc)\n        self._tell = loc\n        self._buf = Buffer()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by the file object to seek to a new location.", "response": "def seek(self, loc):\n        \"\"\"Called if upload fails and must be retried.\"\"\"\n        assert loc == 0\n\n        # rewind progress bar\n        if self.progressbar:\n            self.progressbar.update(-self._fp.tell())\n\n        self._fp.seek(loc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef merge_strings_files(old_strings_file, new_strings_file):\n    old_localizable_dict = generate_localization_key_to_entry_dictionary_from_file(old_strings_file)\n    output_file_elements = []\n\n    f = open_strings_file(new_strings_file, \"r+\")\n\n    for header_comment, comments, key, value in extract_header_comment_key_value_tuples_from_file(f):\n        if len(header_comment) > 0:\n            output_file_elements.append(Comment(header_comment))\n\n        localize_value = value\n        if key in old_localizable_dict:\n            localize_value = old_localizable_dict[key].value\n\n        output_file_elements.append(LocalizationEntry(comments, key, localize_value))\n\n    f.close()\n\n    write_file_elements_to_strings_file(old_strings_file, output_file_elements)", "response": "Merges the old strings file with the new one."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring the argument parser for the Taxonomy command line interface.", "response": "def configure_parser(self, parser):\n        \"\"\"\n        Adds the necessary supported arguments to the argument parser.\n\n        Args:\n            parser (argparse.ArgumentParser): The parser to add arguments to.\n        \"\"\"\n        parser.add_argument(\"--log_path\", default=\"\", help=\"The log file path\")\n        parser.add_argument(\"--verbose\", help=\"Increase logging verbosity\", action=\"store_true\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_with_standalone_parser(self):\n        parser = argparse.ArgumentParser(description=self.description())\n        self.configure_parser(parser)\n        self.run(parser.parse_args())", "response": "Will run the operation as standalone with a new ArgumentParser"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalculating the diversity between two communities.", "response": "def beta_diversity(self, metric=\"braycurtis\", rank=\"auto\"):\n        \"\"\"Calculate the diversity between two communities.\n\n        Parameters\n        ----------\n        metric : {'jaccard', 'braycurtis', 'cityblock'}\n            The distance metric to calculate.\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n\n        Returns\n        -------\n        skbio.stats.distance.DistanceMatrix, a distance matrix.\n        \"\"\"\n        if metric not in (\"jaccard\", \"braycurtis\", \"cityblock\"):\n            raise OneCodexException(\n                \"For beta diversity, metric must be one of: jaccard, braycurtis, cityblock\"\n            )\n\n        # needs read counts, not relative abundances\n        if self._guess_normalized():\n            raise OneCodexException(\"Beta diversity requires unnormalized read counts.\")\n\n        df = self.to_df(rank=rank, normalize=False)\n\n        counts = []\n        for c_id in df.index:\n            counts.append(df.loc[c_id].tolist())\n\n        return skbio.diversity.beta_diversity(metric, counts, df.index.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fetch_api_key_from_uname(username, password, server_url):\n    # TODO: Hit programmatic endpoint to fetch JWT key, not API key\n    with requests.Session() as session:\n        # get the login page normally\n        text = session.get(server_url + \"login\").text\n\n        # retrieve the CSRF token out of it\n        csrf = re.search('type=\"hidden\" value=\"([^\"]+)\"', text).group(1)\n\n        # and resubmit using the username/password *and* the CSRF\n        login_data = {\n            \"email\": username,\n            \"password\": password,\n            \"csrf_token\": csrf,\n            \"next\": \"/api/get_token\",\n        }\n        page = session.post(server_url + \"login\", data=login_data)\n        try:\n            key = page.json()[\"key\"]\n        except (ValueError, KeyError):  # ValueError includes simplejson.decoder.JSONDecodeError\n            key = None\n    return key", "response": "Fetch an API key from One Codex webpage given the username and password."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the current CLI version is supported by One Codex backend.", "response": "def check_version(version, server):\n    \"\"\"Check if the current CLI version is supported by the One Codex backend.\n\n    Parameters\n    ----------\n    version : `string`\n        Current version of the One Codex client library\n    server : `string`\n        Complete URL to One Codex server, e.g., https://app.onecodex.com\n\n    Returns\n    -------\n    `tuple` containing two values:\n        - True if the user *must* upgrade their software, otherwise False\n        - An error message if the user should upgrade, otherwise None.\n    \"\"\"\n\n    def version_inadequate(client_version, server_version):\n        \"\"\"Simple, fast check for version inequality.\n\n        Could use python package `semver` if we need more precise checks in\n        edge cases, but this generally works for now.\n        \"\"\"\n        client_version = tuple([int(x) for x in client_version.split(\"-\")[0].split(\".\")])\n        server_version = tuple([int(x) for x in server_version.split(\".\")])\n        return client_version < server_version\n\n    # this will probably live on /api/v0 forever for compat with older CLI versions\n    data = requests.post(server + \"api/v0/check_for_cli_update\", data={\"version\": version})\n\n    if data.status_code != 200:\n        return False, \"Error connecting to server\"\n\n    data = data.json()\n    latest_version = data[\"latest_version\"]\n\n    if version_inadequate(version, latest_version):\n        return (\n            True,\n            (\n                \"Please upgrade your client to the latest version (v{}) using the command \"\n                \"`pip install --upgrade onecodex`\".format(latest_version)\n            ),\n        )\n\n    return False, None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef valid_api_key(ctx, param, value):\n    if value is not None and len(value) != 32:\n        raise click.BadParameter(\n            \"API Key must be 32 characters long, not {}\".format(str(len(value)))\n        )\n    else:\n        return value", "response": "Ensures an API has valid length (this is a click callback)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pprint(j, no_pretty):\n    if not no_pretty:\n        click.echo(\n            json.dumps(j, cls=PotionJSONEncoder, sort_keys=True, indent=4, separators=(\",\", \": \"))\n        )\n    else:\n        click.echo(j)", "response": "Prints as formatted JSON\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the current system is missing an SSLContext object", "response": "def is_insecure_platform():\n    \"\"\"\n    Checks if the current system is missing an SSLContext object\n    \"\"\"\n    v = sys.version_info\n    if v.major == 3:\n        return False  # Python 2 issue\n\n    if v.major == 2 and v.minor == 7 and v.micro >= 9:\n        return False  # >= 2.7.9 includes the new SSL updates\n\n    try:\n        import OpenSSL  # noqa\n        import ndg  # noqa\n        import pyasn1  # noqa\n    except ImportError:\n        pass\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a nice message if SSL object is not available.", "response": "def warn_if_insecure_platform():\n    \"\"\"\n    Produces a nice message if SSLContext object is not available.\n    Also returns True -> platform is insecure\n                 False -> platform is secure\n    \"\"\"\n    m = (\n        \"\\n\"\n        \"######################################################################################\\n\"  # noqa\n        \"#                                                                                    #\\n\"  # noqa\n        \"#  Your version of Python appears to be out of date and lack important security      #\\n\"  # noqa\n        \"#  features. Please update to Python >= 2.7.9 or `pip install requests[security]`.   #\\n\"  # noqa\n        \"#                                                                                    #\\n\"  # noqa\n        \"#  InsecurePlatformWarning: A true SSLContext object is not available. This          #\\n\"  # noqa\n        \"#  prevents urllib3 from configuring SSL appropriately and may cause certain         #\\n\"  # noqa\n        \"#  SSL connections to fail. For more information, see                                #\\n\"  # noqa\n        \"#  https://urllib3.readthedocs.org/en/latest/security.html#insecureplatformwarning.  #\\n\"  # noqa\n        \"#                                                                                    #\\n\"  # noqa\n        \"######################################################################################\\n\"\n    )  # noqa\n    if is_insecure_platform():\n        click.echo(m, err=True)\n        return True\n    else:\n        cli_log.debug(\"Python SSLContext passed\")\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmanage the chunked downloading of a file given an url", "response": "def download_file_helper(url, input_path):\n    \"\"\"\n    Manages the chunked downloading of a file given an url\n    \"\"\"\n    r = requests.get(url, stream=True)\n    if r.status_code != 200:\n        cli_log.error(\"Failed to download file: %s\" % r.json()[\"message\"])\n    local_full_path = get_download_dest(input_path, r.url)\n    original_filename = os.path.split(local_full_path)[-1]\n    with open(local_full_path, \"wb\") as f:\n        click.echo(\"Downloading {}\".format(original_filename), err=True)\n        for chunk in r.iter_content(chunk_size=1024):\n            if chunk:  # filter out keep-alive new chunks\n                f.write(chunk)\n                f.flush()\n    pprint(\"Successfully downloaded %s to %s\" % (original_filename, local_full_path), True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_for_allowed_file(f):\n    for ext in SUPPORTED_EXTENSIONS:\n        if f.endswith(ext):\n            return True\n    log.error(\"Failed upload: Not an allowed file extension: %s\", f)\n    raise SystemExit", "response": "Checks a file extension against a list of seq file extensions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a path back to ~ from expanduser", "response": "def collapse_user(fp):\n    \"\"\"\n    Converts a path back to ~/ from expanduser()\n    \"\"\"\n    home_dir = os.path.expanduser(\"~\")\n    abs_path = os.path.abspath(fp)\n    return abs_path.replace(home_dir, \"~\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tree_build(self):\n        from skbio.tree import TreeNode\n\n        # build all the nodes\n        nodes = {}\n\n        for tax_id in self.taxonomy.index:\n            node = TreeNode(name=tax_id, length=1)\n            node.tax_name = self.taxonomy[\"name\"][tax_id]\n            node.rank = self.taxonomy[\"rank\"][tax_id]\n            node.parent_tax_id = self.taxonomy[\"parent_tax_id\"][tax_id]\n\n            nodes[tax_id] = node\n\n        # generate all the links\n        for tax_id in self.taxonomy.index:\n            try:\n                parent = nodes[nodes[tax_id].parent_tax_id]\n            except KeyError:\n                if tax_id != \"1\":\n                    warnings.warn(\n                        \"tax_id={} has parent_tax_id={} which is not in tree\"\n                        \"\".format(tax_id, nodes[tax_id].parent_tax_id)\n                    )\n\n                continue\n\n            parent.append(nodes[tax_id])\n\n        return nodes[\"1\"]", "response": "Build a tree from the taxonomy data present in this ClassificationsDataFrame or\n        SampleCollection."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprunes a tree back to contain only the tax_ids in the list and their parents.", "response": "def tree_prune_tax_ids(self, tree, tax_ids):\n        \"\"\"Prunes a tree back to contain only the tax_ids in the list and their parents.\n\n        Parameters\n        ----------\n        tree : `skbio.tree.TreeNode`\n            The root node of the tree to perform this operation on.\n        tax_ids : `list`\n            A `list` of taxonomic IDs to keep in the tree.\n\n        Returns\n        -------\n        `skbio.tree.TreeNode`, the root of a tree containing the given taxonomic IDs and their\n        parents, leading back to the root node.\n        \"\"\"\n        tax_ids_to_keep = []\n\n        for tax_id in tax_ids:\n            tax_ids_to_keep.append(tax_id)\n            tax_ids_to_keep.extend([x.name for x in tree.find(tax_id).ancestors()])\n\n        tree = tree.copy()\n        tree.remove_deleted(lambda n: n.name not in tax_ids_to_keep)\n\n        return tree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tree_prune_rank(self, tree, rank=\"species\"):\n        if rank is None:\n            return tree.copy()\n\n        tree = tree.copy()\n\n        for node in tree.postorder():\n            if node.rank == rank:\n                node._above_rank = True\n            elif any([getattr(n, \"_above_rank\", False) for n in node.children]):\n                node._above_rank = True\n            else:\n                node._above_rank = False\n\n        tree.remove_deleted(lambda n: not getattr(n, \"_above_rank\", False))\n\n        return tree", "response": "Takes a TreeNode tree and prunes off any tips not at the specified rank and backwards up\n        until all of the tips are at the specified rank and all of the tips are at the specified level. Returns a copy of the tree with all the genus - level nodes and higher."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_file_elements_to_strings_file(file_path, file_elements):\n    f = open_strings_file(file_path, \"w\")\n    for element in file_elements:\n        f.write(unicode(element))\n        f.write(u\"\\n\")\n\n    f.close()", "response": "Writes the elements to the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup_logging(args=None):\n    logging_level = logging.WARNING\n    if args is not None and args.verbose:\n        logging_level = logging.INFO\n    config = {\"level\": logging_level, \"format\": \"jtlocalize:%(message)s\"}\n\n    if args is not None and args.log_path != \"\":\n        config[\"filename\"] = args.log_path\n\n    logging.basicConfig(**config)", "response": "Setup logging module.\n\n    Args:\n        args (optional): The arguments returned by the argparse module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __generate_localization_dictionary_from_file(file_path, localization_entry_attribute_name_for_key):\n    localization_dictionary = {}\n    f = open_strings_file(file_path, \"r+\")\n    header_comment_key_value_tuples = extract_header_comment_key_value_tuples_from_file(f)\n\n    if len(header_comment_key_value_tuples) == 0:\n        logging.warning(\"Couldn't find any strings in file '%s'. Check encoding and format.\" % file_path)\n\n    for header_comment, comments, key, value in header_comment_key_value_tuples:\n        localization_entry = LocalizationEntry(comments, key, value)\n        localization_dictionary[\n            localization_entry.__getattribute__(localization_entry_attribute_name_for_key)] = localization_entry\n    f.close()\n    return localization_dictionary", "response": "Generates a dictionary mapping between keys defined by the given attribute name and localization entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the header comment key - value tuples from a file.", "response": "def extract_header_comment_key_value_tuples_from_file(file_descriptor):\n    \"\"\" Extracts tuples representing comments and localization entries from strings file.\n\n    Args:\n        file_descriptor (file): The file to read the tuples from\n\n    Returns:\n        list : List of tuples representing the headers and localization entries.\n\n    \"\"\"\n    file_data = file_descriptor.read()\n    findall_result = re.findall(HEADER_COMMENT_KEY_VALUE_TUPLES_REGEX, file_data, re.MULTILINE | re.DOTALL)\n\n    returned_list = []\n    for header_comment, _ignored, raw_comments, key, value in findall_result:\n        comments = re.findall(\"/\\* (.*?) \\*/\", raw_comments)\n        if len(comments) == 0:\n            comments = [u\"\"]\n        returned_list.append((header_comment, comments, key, value))\n\n    return returned_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract all string pairs matching the JTL pattern from given text file.", "response": "def extract_jtl_string_pairs_from_text_file(results_dict, file_path):\n    \"\"\" Extracts all string pairs matching the JTL pattern from given text file.\n\n    This can be used as an \"extract_func\" argument in the extract_string_pairs_in_directory method.\n\n    Args:\n        results_dict (dict): The dict to add the the string pairs to.\n        file_path (str): The path of the file from which to extract the string pairs.\n\n    \"\"\"\n    result_pairs = re.findall(JTL_REGEX, open(file_path).read())\n    for result_key, result_comment in result_pairs:\n        results_dict[result_key] = result_comment\n    return results_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextracting all string pairs from the files in the directory_path.", "response": "def extract_string_pairs_in_directory(directory_path, extract_func, filter_func):\n    \"\"\" Retrieves all string pairs in the directory\n\n    Args:\n        directory_path (str): The path of the directory containing the file to extract string pairs from.\n        extract_func (function): Function for extracting the localization keys and comments from the files.\n            The extract function receives 2 parameters:\n            - dict that the keys (a key in the dict) and comments (a value in the dict) are added to.\n            - str representing file path\n\n        filter_func (function): Function for filtering files in the directory.\n            The filter function receives the file name and returns a bool representing the filter result.\n            True if the file name passed the filter, False otherwise.\n\n    Returns:\n        dict: A mapping between string pairs first value (probably the key), and the second value (probably the comment).\n    \"\"\"\n    result = {}\n    for root, dirnames, filenames in os.walk(directory_path):\n        for file_name in filenames:\n            if filter_func(file_name):\n                file_path = os.path.join(root, file_name)\n                try:\n                    extract_func(result, file_path)\n                except Exception as e:\n                    print \"Error in file \" + file_name\n                    print e\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites a localization entry to the file", "response": "def write_entry_to_file(file_descriptor, entry_comment, entry_key):\n    \"\"\" Writes a localization entry to the file\n\n    Args:\n        file_descriptor (file, instance): The file to write the entry to.\n        entry_comment (str): The entry's comment.\n        entry_key (str): The entry's key.\n    \"\"\"\n    escaped_key = re.sub(r'([^\\\\])\"', '\\\\1\\\\\"', entry_key)\n    file_descriptor.write(u'/* %s */\\n' % entry_comment)\n    file_descriptor.write(u'\"%s\" = \"%s\";\\n' % (escaped_key, escaped_key))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending a dictionary of localization keys and comments to a file.", "response": "def append_dictionary_to_file(localization_key_to_comment, file_path, section_name):\n    \"\"\" Appends dictionary of localization keys and comments to a file\n\n    Args:\n        localization_key_to_comment (dict): A mapping between localization keys and comments.\n        file_path (str): The path of the file to append to.\n        section_name (str): The name of the section.\n\n    \"\"\"\n    output_file = open_strings_file(file_path, \"a\")\n    write_section_header_to_file(output_file, section_name)\n    for entry_key, entry_comment in sorted(localization_key_to_comment.iteritems(), key=operator.itemgetter(1)):\n        output_file.write(u'\\n')\n        write_entry_to_file(output_file, entry_comment, entry_key)\n    output_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_dict_to_new_file(file_name, localization_key_to_comment):\n    output_file_descriptor = open_strings_file(file_name, \"w\")\n    for entry_key, entry_comment in sorted(localization_key_to_comment.iteritems(), key=operator.itemgetter(1)):\n        write_entry_to_file(output_file_descriptor, entry_comment, entry_key)\n        output_file_descriptor.write(u'\\n')\n    output_file_descriptor.close()", "response": "Writes a dictionary of localization keys and comments to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_files(base_dir, extensions, exclude_dirs=list()):\n    result = []\n    for root, dir_names, file_names in os.walk(base_dir):\n        for filename in file_names:\n            candidate = os.path.join(root, filename)\n            if should_include_file_in_search(candidate, extensions, exclude_dirs):\n                result.append(candidate)\n    return result", "response": "Find all files matching the given extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetermines if a file should be included in a page.", "response": "def should_include_file_in_search(file_name, extensions, exclude_dirs):\n    \"\"\" Whether or not a filename matches a search criteria according to arguments.\n\n    Args:\n        file_name (str): A file path to check.\n        extensions (list): A list of file extensions file should match.\n        exclude_dirs (list): A list of directories to exclude from search.\n\n    Returns:\n        A boolean of whether or not file matches search criteria.\n\n    \"\"\"\n    return (exclude_dirs is None or not any(file_name.startswith(d) for d in exclude_dirs)) and \\\n        any(file_name.endswith(e) for e in extensions)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_auto_rank(self, rank):\n\n        if rank == \"auto\":\n            # if we're an accessor for a ClassificationsDataFrame, use its _rank property\n            if self.__class__.__name__ == \"OneCodexAccessor\":\n                return self._rank\n\n            if self._field == \"abundance\":\n                return \"species\"\n            else:\n                return \"genus\"\n        else:\n            return rank", "response": "Tries to figure out what rank we should use for analyses"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _guess_normalized(self):\n        return (\n            getattr(self, \"_normalized\", False)\n            or getattr(self, \"_field\", None) == \"abundance\"\n            or bool((self._results.sum(axis=1).round(4) == 1.0).all())\n        )", "response": "Returns true if the collated counts in self. _results appear to be normalized."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a list of metadata fields some of which can contain taxon names or taxon IDs and returns a DataFrame with transformed data that can be used for plotting.", "response": "def _metadata_fetch(self, metadata_fields, label=None):\n        \"\"\"Takes a list of metadata fields, some of which can contain taxon names or taxon IDs, and\n        returns a DataFrame with transformed data that can be used for plotting.\n\n        Parameters\n        ----------\n        metadata_fields : `list` of `string`\n            A list of metadata fields, taxon names, or taxon IDs to fetch and transform for display.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n            If this argument is not given, and \"Label\" is in `metadata_fields`, \"Label\" will be set\n            to the filename associated with an analysis.\n\n        Notes\n        -----\n        Taxon names and IDs are transformed into the relative abundances of those taxa within their\n        own rank. For example, 'Bacteroides' will return the relative abundances of 'Bacteroides'\n        among all taxa of rank genus. Taxon IDs are stored as strings in `ClassificationsDataFrame`\n        and are coerced to strings if integers are given.\n\n        Metadata fields are returned as is, from the `self.metadata` DataFrame. If multiple metadata\n        fields are specified in a tuple, their values are joined as strings separated by underscore.\n        Multiple metadata fields in a tuple must both be categorical. That is, a numerical field and\n        boolean can not be joined, or the result would be something like '87.4_True'.\n\n        The 'Label' field name is transformed to '_display_name'. This lets us label points in plots\n        by the name generated for each sample in `SampleCollection._collate_metadata`.\n\n        Returns\n        -------\n        `pandas.DataFrame`\n            Columns are renamed (if applicable) metadata fields and rows are `Classifications.id`.\n            Elements are transformed values. Not all metadata fields will have been renamed, but will\n            be present in the below `dict` nonetheless.\n        `dict`\n            Keys are metadata fields and values are renamed metadata fields. This can be used to map\n            metadata fields which were passed to this function, to prettier names. For example, if\n            'bacteroid' is passed, it will be matched with the Bacteroides genus and renamed to\n            'Bacteroides (816)', which includes its taxon ID.\n        \"\"\"\n        import pandas as pd\n\n        help_metadata = \", \".join(self.metadata.keys())\n        magic_metadata = pd.DataFrame({\"classification_id\": self._results.index}).set_index(\n            \"classification_id\"\n        )\n\n        # if user passed label kwarg but didn't put \"Label\" in the fields, assume the user wants\n        # that field added\n        if label is not None and \"Label\" not in metadata_fields:\n            metadata_fields.append(\"Label\")\n\n        # if we magically rename fields, keep track\n        magic_fields = {}\n\n        for f in set([f for f in metadata_fields if f]):\n            if isinstance(f, tuple):\n                # joined categorical metadata\n                for field in f:\n                    if field not in self.metadata:\n                        raise OneCodexException(\n                            \"Field {} not found. Choose from: {}\".format(field, help_metadata)\n                        )\n\n                    if not (\n                        pd.api.types.is_bool_dtype(self.metadata[field])\n                        or pd.api.types.is_categorical_dtype(self.metadata[field])  # noqa\n                        or pd.api.types.is_object_dtype(self.metadata[field])  # noqa\n                    ):\n                        raise OneCodexException(\n                            \"When specifying multiple metadata fields, all must be categorical\"\n                        )\n\n                # concatenate the columns together with underscores\n                composite_field = \"_\".join(f)\n                magic_metadata[composite_field] = \"\"\n                magic_metadata[composite_field] = (\n                    magic_metadata[composite_field]\n                    .str.cat([self.metadata[field].astype(str) for field in f], sep=\"_\")\n                    .str.lstrip(\"_\")\n                )\n                magic_fields[f] = composite_field\n            else:\n                str_f = str(f)\n\n                if str_f == \"Label\":\n                    magic_metadata[str_f] = self.metadata[\"filename\"]\n                    magic_fields[f] = str_f\n\n                    if isinstance(label, six.string_types):\n                        if label in self.metadata.columns:\n                            magic_metadata[str_f] = self.metadata[label]\n                        else:\n                            raise OneCodexException(\n                                \"Label field {} not found. Choose from: {}\".format(\n                                    label, help_metadata\n                                )\n                            )\n                    elif callable(label):\n                        for classification_id, metadata in self.metadata.to_dict(\n                            orient=\"index\"\n                        ).items():\n                            c_id_label = label(metadata)\n\n                            if not isinstance(c_id_label, six.string_types):\n                                raise OneCodexException(\n                                    \"Expected string from label function, got: {}\".format(\n                                        type(c_id_label).__name__\n                                    )\n                                )\n\n                            magic_metadata.loc[classification_id, \"Label\"] = c_id_label\n                    elif label is not None:\n                        raise OneCodexException(\n                            \"Expected string or callable for label, got: {}\".format(\n                                type(label).__name__\n                            )\n                        )\n                elif str_f in self.metadata:\n                    # exactly matches existing metadata field\n                    magic_metadata[f] = self.metadata[str_f]\n                    magic_fields[f] = str_f\n                elif str_f in self._results.keys():\n                    # is a tax_id\n                    tax_name = self.taxonomy[\"name\"][str_f]\n\n                    # report within-rank abundance\n                    df = self.to_df(rank=self.taxonomy[\"rank\"][str_f])\n\n                    renamed_field = \"{} ({})\".format(tax_name, str_f)\n                    magic_metadata[renamed_field] = df[str_f]\n                    magic_fields[f] = renamed_field\n                else:\n                    # try to match it up with a taxon name\n                    hits = []\n\n                    # don't both searching if the query is really short\n                    if len(str_f) > 4:\n                        for tax_id, tax_name in zip(self.taxonomy.index, self.taxonomy[\"name\"]):\n                            # if it's an exact match, use that and skip the rest\n                            if str_f.lower() == tax_name.lower():\n                                hits = [(tax_id, tax_name)]\n                                break\n                            # otherwise, keep trying to match\n                            elif str_f.lower() in tax_name.lower():\n                                hits.append((tax_id, tax_name))\n\n                        # take the hit with the lowest tax_id\n                        hits = sorted(hits, key=lambda x: int(x[0]))\n\n                    if hits:\n                        # report within-rank abundance\n                        df = self.to_df(rank=self.taxonomy[\"rank\"][hits[0][0]])\n\n                        renamed_field = \"{} ({})\".format(hits[0][1], hits[0][0])\n                        magic_metadata[renamed_field] = df[hits[0][0]]\n                        magic_fields[f] = renamed_field\n                    else:\n                        # matched nothing\n                        raise OneCodexException(\n                            \"Field or taxon {} not found. Choose from: {}\".format(\n                                str_f, help_metadata\n                            )\n                        )\n\n        return magic_metadata, magic_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes the ClassificationsDataFrame associated with these samples or SampleCollection and returns a ClassificationsDataFrame that contains the most abundant taxa and counts.", "response": "def to_df(\n        self,\n        rank=\"auto\",\n        top_n=None,\n        threshold=None,\n        remove_zeros=True,\n        normalize=\"auto\",\n        table_format=\"wide\",\n    ):\n        \"\"\"Takes the ClassificationsDataFrame associated with these samples, or SampleCollection,\n        does some filtering, and returns a ClassificationsDataFrame copy.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        top_n : `integer`, optional\n            Return only the top N most abundant taxa.\n        threshold : `float`, optional\n            Return only taxa more abundant than this threshold in one or more samples.\n        remove_zeros : `bool`, optional\n            Do not return taxa that have zero abundance in every sample.\n        normalize : {'auto', True, False}\n            Convert read counts to relative abundances (each sample sums to 1.0).\n        table_format : {'long', 'wide'}\n            If wide, rows are classifications, cols are taxa, elements are counts. If long, rows are\n            observations with three cols each: classification_id, tax_id, and count.\n\n        Returns\n        -------\n        `ClassificationsDataFrame`\n        \"\"\"\n        from onecodex.dataframes import ClassificationsDataFrame\n\n        rank = self._get_auto_rank(rank)\n        df = self._results.copy()\n\n        # subset by taxa\n        if rank:\n            if rank == \"kingdom\":\n                warnings.warn(\n                    \"Did you mean to specify rank=kingdom? Use rank=superkingdom to see Bacteria, \"\n                    \"Archaea and Eukaryota.\"\n                )\n\n            tax_ids_to_keep = []\n\n            for tax_id in df.keys():\n                if self.taxonomy[\"rank\"][tax_id] == rank:\n                    tax_ids_to_keep.append(tax_id)\n\n            if len(tax_ids_to_keep) == 0:\n                raise OneCodexException(\"No taxa kept--is rank ({}) correct?\".format(rank))\n\n            df = df.loc[:, tax_ids_to_keep]\n\n        # normalize\n        if normalize is False and self._guess_normalized():\n            raise OneCodexException(\"Data has already been normalized and this can not be undone.\")\n\n        if normalize is True or (\n            normalize == \"auto\" and rank is not None and self._field != \"abundance\"\n        ):\n            df = df.div(df.sum(axis=1), axis=0)\n\n        # remove columns (tax_ids) with no values that are > 0\n        if remove_zeros:\n            df = df.loc[:, (df != 0).any(axis=0)]\n\n        # restrict to taxa appearing in one or more samples at the given threshold\n        if threshold:\n            df = df.loc[:, df.max() >= threshold]\n\n        # restrict to N most abundant taxa\n        if top_n:\n            idx = df.sum(axis=0).sort_values(ascending=False).head(top_n).index\n            df = df.loc[:, idx]\n\n        # additional data to copy into the ClassificationsDataFrame\n        ocx_data = {\n            \"ocx_metadata\": self.metadata.copy(),\n            \"ocx_rank\": rank,\n            \"ocx_field\": self._field,\n            \"ocx_taxonomy\": self.taxonomy.copy(),\n            \"ocx_normalized\": normalize,\n        }\n\n        # generate long-format table\n        if table_format == \"long\":\n            long_df = {\"classification_id\": [], \"tax_id\": [], self._field: []}\n\n            for t_id in df:\n                for c_id, count in df[t_id].iteritems():\n                    long_df[\"classification_id\"].append(c_id)\n                    long_df[\"tax_id\"].append(t_id)\n                    long_df[self._field].append(count)\n\n            results_df = ClassificationsDataFrame(long_df, **ocx_data)\n        elif table_format == \"wide\":\n            results_df = ClassificationsDataFrame(df, **ocx_data)\n        else:\n            raise OneCodexException(\"table_format must be one of: long, wide\")\n\n        return results_df"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_notebook_node(self, nb, resources=None, **kw):\n        nb = copy.deepcopy(nb)\n\n        # setup our dictionary that's accessible from within jinja templates\n        if resources is None:\n            resources = {\"metadata\": {}}\n        elif \"metadata\" not in resources:\n            resources[\"metadata\"] = {}\n\n        # iterate over cells in the notebook and transform data as necessary\n        do_not_insert_date = False\n\n        for cell in nb.cells:\n            if cell[\"cell_type\"] == \"code\":\n                for out in cell[\"outputs\"]:\n                    # base64 encode SVGs otherwise Weasyprint can't render them. delete other\n                    # types of output in jupyter-vega cells (e.g., image/png, or javascript)\n                    if out.get(\"metadata\") and out[\"metadata\"].get(\"jupyter-vega\"):\n                        for mimetype in out.get(\"data\", []):\n                            if mimetype == \"image/svg+xml\":\n                                img = b64encode(\n                                    bytes(out[\"data\"][\"image/svg+xml\"], encoding=\"UTF-8\")\n                                ).decode()\n                                img = '<img src=\"data:image/svg+xml;charset=utf-8;base64,%s\">' % (\n                                    img,\n                                )\n                                out[\"data\"] = {\"image/svg+xml\": img}\n                                break\n                        else:\n                            out[\"data\"] = {}\n                    # transfer text/css blocks to HTML <head> tag\n                    elif out.get(\"metadata\") and out[\"metadata\"].get(\"onecodex\") == \"head.style\":\n                        for mimetype in out.get(\"data\", []):\n                            if mimetype == \"text/css\":\n                                style_block = '<style type=\"text/css\">{}</style>'.format(\n                                    out[\"data\"][\"text/css\"]\n                                )\n                                head_block = (\n                                    resources[\"metadata\"].get(\"head_block\", \"\") + style_block\n                                )\n                                resources[\"metadata\"][\"head_block\"] = head_block\n                                break\n\n                        # we don't want this to be output as text, so clear it\n                        out[\"data\"] = {\"text/plain\": \"\"}\n                    # if there's a custom date specified, don't insert it\n                    elif out.get(\"metadata\") and out[\"metadata\"].get(\"onecodex\") == \"customdate\":\n                        do_not_insert_date = True\n\n        # add one codex logo unless told not to\n        if not os.environ.get(\"ONE_CODEX_REPORT_NO_LOGO\", False):\n            img = b64encode(\n                bytes(open(os.path.join(ASSETS_PATH, \"one_codex_logo.png\"), \"rb\").read())\n            ).decode()\n            img = \"data:image/png;charset=utf-8;base64,%s\" % (img,)\n            logo_html = report.set_logo(img, position=\"right\")._repr_mimebundle_()[\"text/html\"]\n            head_block = resources[\"metadata\"].get(\"head_block\", \"\") + logo_html\n            resources[\"metadata\"][\"head_block\"] = head_block\n\n        # add today's date unless told not to (i.e. a custom date was specified)\n        if not do_not_insert_date:\n            date_div = report.set_date()._repr_mimebundle_()[0][\"text/html\"]\n            head_block = resources[\"metadata\"].get(\"head_block\", \"\") + date_div\n            resources[\"metadata\"][\"head_block\"] = head_block\n\n        # embed the default CSS\n        css = open(os.path.join(ASSETS_PATH, CSS_TEMPLATE_FILE), \"r\").read()\n        css = '<style type=\"text/css\">{}</style>'.format(css)\n        head_block = resources[\"metadata\"].get(\"head_block\", \"\") + css\n        resources[\"metadata\"][\"head_block\"] = head_block\n\n        # tag this report for traceability, if run from notebook service. these will be transferred\n        # to PDF metadata if the HTML output of this function is used as input for PDF generation\n        meta_tags = [(\"dcterms.created\", datetime.datetime.now(pytz.utc).isoformat())]\n\n        user_uuid = os.environ.get(\"ONE_CODEX_USER_UUID\")\n        if user_uuid is not None:\n            meta_tags.append((\"author\", \"one_codex_user_uuid_{}\".format(user_uuid)))\n\n        nb_uuid = os.environ.get(\"ONE_CODEX_NOTEBOOK_UUID\")\n        if nb_uuid is not None:\n            meta_tags.append((\"author\", \"one_codex_notebook_uuid_{}\".format(nb_uuid)))\n\n        meta_html = \"\"\n\n        for meta_name, meta_val in meta_tags:\n            meta_html += '<meta name=\"{}\" content=\"{}\" />\\n'.format(meta_name, meta_val)\n\n        head_block = resources[\"metadata\"].get(\"head_block\", \"\") + meta_html\n        resources[\"metadata\"][\"head_block\"] = head_block\n\n        output, resources = super(OneCodexHTMLExporter, self).from_notebook_node(\n            nb, resources=resources, **kw\n        )\n\n        return output, resources", "response": "Uses nbconvert s HTMLExporter to generate HTML from a notebook node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake output of OneCodexHTMLExporter and runs Weasyprint to get a PDF.", "response": "def from_notebook_node(self, nb, resources=None, **kw):\n        \"\"\"Takes output of OneCodexHTMLExporter and runs Weasyprint to get a PDF.\"\"\"\n        from weasyprint import HTML, CSS\n\n        nb = copy.deepcopy(nb)\n\n        output, resources = super(OneCodexPDFExporter, self).from_notebook_node(\n            nb, resources=resources, **kw\n        )\n        buf = BytesIO()\n        HTML(string=output).write_pdf(\n            buf, stylesheets=[CSS(os.path.join(ASSETS_PATH, CSS_TEMPLATE_FILE))]\n        )\n        buf.seek(0)\n        return buf.read(), resources"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake PDF output from PDFExporter and uploads to One Codex Documents portal.", "response": "def from_notebook_node(self, nb, resources=None, **kw):\n        \"\"\"Takes PDF output from PDFExporter and uploads to One Codex Documents portal.\"\"\"\n        output, resources = super(OneCodexDocumentExporter, self).from_notebook_node(\n            nb, resources=resources, **kw\n        )\n\n        from onecodex import Api\n        from onecodex.lib.upload import upload_document_fileobj\n\n        ocx = Api()\n\n        default_filename = \"Analysis Report - {dt:%B} {dt.day}, {dt:%Y}\".format(\n            dt=datetime.datetime.now()\n        )\n\n        file_name = resources[\"metadata\"].get(\"one_codex_doc_portal_filename\", default_filename)\n\n        try:\n            document_id = upload_document_fileobj(\n                BytesIO(output), file_name, ocx._client.session, ocx.Documents._resource\n            )\n        except UploadException as exc:\n            resp = json.dumps({\"status\": 500, \"message\": str(exc)})\n            return resp, resources\n        except Exception:\n            resp = json.dumps(\n                {\n                    \"status\": 500,\n                    \"message\": \"Upload failed. Please contact help@onecodex.com for assistance.\",\n                }\n            )\n            return resp, resources\n\n        resp = json.dumps({\"status\": 200, \"document_id\": document_id})\n        return resp, resources"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a tree structure of strings for the current language of the current language.", "response": "def generate_strings(project_base_dir, localization_bundle_path, tmp_directory, exclude_dirs, include_strings_file,\n                     special_ui_components_prefix):\n    \"\"\"\n    Calls the builtin 'genstrings' command with JTLocalizedString as the string to search for,\n    and adds strings extracted from UI elements internationalized with 'JTL' + removes duplications.\n    \"\"\"\n\n    localization_directory = os.path.join(localization_bundle_path, DEFAULT_LANGUAGE_DIRECTORY_NAME)\n    if not os.path.exists(localization_directory):\n        os.makedirs(localization_directory)\n\n    localization_file = os.path.join(localization_directory, LOCALIZATION_FILENAME)\n\n    # Creating the same directory tree structure in the tmp directory\n    tmp_localization_directory = os.path.join(tmp_directory, DEFAULT_LANGUAGE_DIRECTORY_NAME)\n    tmp_localization_file = os.path.join(tmp_localization_directory, LOCALIZATION_FILENAME)\n\n    if os.path.isdir(tmp_localization_directory):\n        shutil.rmtree(tmp_localization_directory)\n    os.mkdir(tmp_localization_directory)\n\n    logging.info(\"Running genstrings\")\n\n    source_files = extract_source_files(project_base_dir, exclude_dirs)\n\n    genstrings_cmd = 'genstrings -s JTLocalizedString -o %s %s' % (tmp_localization_directory, \" \".join(\n            ['\"%s\"' % (source_file,) for source_file in source_files]))\n\n    genstrings_process = subprocess.Popen(genstrings_cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE,\n                                          stdin=subprocess.PIPE, shell=True)\n\n    genstrings_out, genstrings_err = genstrings_process.communicate()\n\n    remove_empty_comments_from_file(tmp_localization_file)\n    add_genstrings_comments_to_file(tmp_localization_file, genstrings_err)\n\n    genstrings_rc = genstrings_process.returncode\n    if genstrings_rc != 0:\n        logging.fatal(\"genstrings returned %d, aborting run!\", genstrings_rc)\n        sys.exit(genstrings_rc)\n\n    create_localized_strings_from_ib_files(project_base_dir, exclude_dirs, tmp_localization_file,\n                                           special_ui_components_prefix)\n\n    if include_strings_file:\n        target = open_strings_file(tmp_localization_file, \"a\")\n        source = open_strings_file(include_strings_file, \"r\")\n        target.write(source.read())\n        source.close()\n        target.close()\n\n    handle_duplications(tmp_localization_file)\n\n    if os.path.isfile(localization_file):\n        logging.info(\"Merging old localizable with new one...\")\n        merge_strings_files(localization_file, tmp_localization_file)\n    else:\n        logging.info(\"No Localizable yet, moving the created file...\")\n        shutil.move(tmp_localization_file, localization_file)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a new SampleCollection containing only samples meeting the filter criteria.", "response": "def filter(self, filter_func):\n        \"\"\"Return a new SampleCollection containing only samples meeting the filter criteria.\n\n        Will pass any kwargs (e.g., field or skip_missing) used when instantiating the current class\n        on to the new SampleCollection that is returned.\n\n        Parameters\n        ----------\n        filter_func : `callable`\n            A function that will be evaluated on every object in the collection. The function must\n            return a `bool`. If True, the object will be kept. If False, it will be removed from the\n            SampleCollection that is returned.\n\n        Returns\n        -------\n        `onecodex.models.SampleCollection` containing only objects `filter_func` returned True on.\n\n        Examples\n        --------\n        Generate a new collection of Samples that have a specific filename extension:\n\n            new_collection = samples.filter(lambda s: s.filename.endswith('.fastq.gz'))\n        \"\"\"\n        if callable(filter_func):\n            return self.__class__([obj for obj in self if filter_func(obj) is True], **self._kwargs)\n        else:\n            raise OneCodexException(\n                \"Expected callable for filter, got: {}\".format(type(filter_func).__name__)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _classification_fetch(self, skip_missing=None):\n        skip_missing = skip_missing if skip_missing else self._kwargs[\"skip_missing\"]\n\n        new_classifications = []\n\n        for a in self._res_list:\n            if a.__class__.__name__ == \"Samples\":\n                c = a.primary_classification\n            elif a.__class__.__name__ == \"Classifications\":\n                c = a\n            else:\n                raise OneCodexException(\n                    \"Objects in SampleCollection must be one of: Classifications, Samples\"\n                )\n\n            if skip_missing and not c.success:\n                warnings.warn(\"Classification {} not successful. Skipping.\".format(c.id))\n                continue\n\n            new_classifications.append(c)\n\n        self._cached[\"classifications\"] = new_classifications", "response": "Turns a list of objects associated with a classification result into a list of Classifications objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn a list of objects associated with a classification result into a DataFrame of metadata. Returns None if no metadata is found.", "response": "def _collate_metadata(self):\n        \"\"\"Turns a list of objects associated with a classification result into a DataFrame of\n        metadata.\n\n        Returns\n        -------\n        None, but stores a result in self._cached.\n        \"\"\"\n        import pandas as pd\n\n        DEFAULT_FIELDS = None\n        metadata = []\n\n        for c in self._classifications:\n            m = c.sample.metadata\n\n            if DEFAULT_FIELDS is None:\n                DEFAULT_FIELDS = list(m._resource._schema[\"properties\"].keys())\n                DEFAULT_FIELDS.remove(\"$uri\")\n                DEFAULT_FIELDS.remove(\"sample\")\n\n            metadatum = {f: getattr(m, f) for f in DEFAULT_FIELDS}\n            metadatum[\"classification_id\"] = c.id\n            metadatum[\"sample_id\"] = m.sample.id\n            metadatum[\"metadata_id\"] = m.id\n            metadatum[\"created_at\"] = m.sample.created_at\n            metadatum[\"filename\"] = c.sample.filename\n\n            metadatum.update(m.custom)\n            metadata.append(metadatum)\n\n        if metadata:\n            metadata = pd.DataFrame(metadata).set_index(\"classification_id\")\n        else:\n            metadata = pd.DataFrame(\n                columns=[\"classification_id\", \"sample_id\", \"metadata_id\", \"created_at\"]\n            )\n\n        self._cached[\"metadata\"] = metadata"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the results of a classification result.", "response": "def _collate_results(self, field=None):\n        \"\"\"For a list of objects associated with a classification result, return the results as a\n        DataFrame and dict of taxa info.\n\n        Parameters\n        ----------\n        field : {'readcount_w_children', 'readcount', 'abundance'}\n            Which field to use for the abundance/count of a particular taxon in a sample.\n\n            - 'readcount_w_children': total reads of this taxon and all its descendants\n            - 'readcount': total reads of this taxon\n            - 'abundance': genome size-normalized relative abundances, from shotgun sequencing\n\n        Returns\n        -------\n        None, but stores a result in self._cached.\n        \"\"\"\n        import pandas as pd\n\n        field = field if field else self._kwargs[\"field\"]\n\n        if field not in (\"auto\", \"abundance\", \"readcount\", \"readcount_w_children\"):\n            raise OneCodexException(\"Specified field ({}) not valid.\".format(field))\n\n        # we'll fill these dicts that eventually turn into DataFrames\n        df = {\"classification_id\": [c.id for c in self._classifications]}\n\n        tax_info = {\"tax_id\": [], \"name\": [], \"rank\": [], \"parent_tax_id\": []}\n\n        if field == \"auto\":\n            field = \"readcount_w_children\"\n\n        self._cached[\"field\"] = field\n\n        for c_idx, c in enumerate(self._classifications):\n            # pulling results from mainline is the slowest part of the function\n            result = c.results()[\"table\"]\n\n            # d contains info about a taxon in result, including name, id, counts, rank, etc.\n            for d in result:\n                d_tax_id = d[\"tax_id\"]\n\n                if d_tax_id not in tax_info[\"tax_id\"]:\n                    for k in (\"tax_id\", \"name\", \"rank\", \"parent_tax_id\"):\n                        tax_info[k].append(d[k])\n\n                    # first time we've seen this taxon, so make a vector for it\n                    df[d_tax_id] = [0] * len(self._classifications)\n\n                df[d_tax_id][c_idx] = d[field]\n\n        # format as a Pandas DataFrame\n        df = pd.DataFrame(df).set_index(\"classification_id\").fillna(0)\n\n        df.columns.name = \"tax_id\"\n\n        tax_info = pd.DataFrame(tax_info).set_index(\"tax_id\")\n\n        self._cached[\"results\"] = df\n        self._cached[\"taxonomy\"] = tax_info"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_otu(self, biom_id=None):\n        otu_format = \"Biological Observation Matrix 1.0.0\"\n\n        # Note: This is exact format URL is required by https://github.com/biocore/biom-format\n        otu_url = \"http://biom-format.org\"\n\n        otu = OrderedDict(\n            {\n                \"id\": biom_id,\n                \"format\": otu_format,\n                \"format_url\": otu_url,\n                \"type\": \"OTU table\",\n                \"generated_by\": \"One Codex API V1\",\n                \"date\": datetime.now().isoformat(),\n                \"rows\": [],\n                \"columns\": [],\n                \"matrix_type\": \"sparse\",\n                \"matrix_element_type\": \"int\",\n            }\n        )\n\n        rows = defaultdict(dict)\n\n        tax_ids_to_names = {}\n        for classification in self._classifications:\n            col_id = len(otu[\"columns\"])  # 0 index\n\n            # Re-encoding the JSON is a bit of a hack, but\n            # we need a ._to_dict() method that properly\n            # resolves references and don't have one at the moment\n            columns_entry = {\n                \"id\": str(classification.id),\n                \"sample_id\": str(classification.sample.id),\n                \"sample_filename\": classification.sample.filename,\n                \"metadata\": json.loads(\n                    classification.sample.metadata._to_json(include_references=False)\n                ),\n            }\n\n            otu[\"columns\"].append(columns_entry)\n            sample_df = classification.table()\n\n            for row in sample_df.iterrows():\n                tax_id = row[1][\"tax_id\"]\n                tax_ids_to_names[tax_id] = row[1][\"name\"]\n                rows[tax_id][col_id] = int(row[1][\"readcount\"])\n\n        num_rows = len(rows)\n        num_cols = len(otu[\"columns\"])\n\n        otu[\"shape\"] = [num_rows, num_cols]\n        otu[\"data\"] = []\n\n        for present_taxa in sorted(rows):\n            # add the row entry\n            row_id = len(otu[\"rows\"])\n            otu[\"rows\"].append(\n                {\"id\": present_taxa, \"metadata\": {\"taxonomy\": tax_ids_to_names[present_taxa]}}\n            )\n\n            for sample_with_hit in rows[present_taxa]:\n                counts = rows[present_taxa][sample_with_hit]\n                otu[\"data\"].append([row_id, sample_with_hit, counts])\n\n        return otu", "response": "Converts a list of objects associated with a classification result into a Python OrderedDict that can be dumped to JSON."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef localization_merge_back(updated_localizable_file, old_translated_file, new_translated_file, merged_translated_file):\n    output_file_elements = []\n    old_translated_file_dict = generate_localization_key_to_entry_dictionary_from_file(old_translated_file)\n    new_translated_file_dict = generate_localization_key_to_entry_dictionary_from_file(new_translated_file)\n\n    f = open_strings_file(updated_localizable_file, \"r\")\n\n    for header_comment, comments, key, value in extract_header_comment_key_value_tuples_from_file(f):\n        translation_value = None\n        if len(header_comment) > 0:\n            output_file_elements.append(Comment(header_comment))\n\n        if value in new_translated_file_dict:\n            translation_value = new_translated_file_dict[value].value\n        elif key in old_translated_file_dict:\n            translation_value = old_translated_file_dict[key].value\n        elif key in new_translated_file_dict:\n            translation_value = new_translated_file_dict[key].value\n\n        if translation_value is not None:\n            output_file_elements.append(LocalizationEntry(comments, key, translation_value))\n\n    f.close()\n\n    write_file_elements_to_strings_file(merged_translated_file, output_file_elements)", "response": "This function generates a new file that contains the updated localization strings file and the new ones."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef boxplot(df, category, quantity, category_type=\"N\", title=None, xlabel=None, ylabel=None):\n    # must be one of Nominal, Ordinal, Time per altair\n    if category_type not in (\"N\", \"O\", \"T\"):\n        raise OneCodexException(\"If specifying category_type, must be N, O, or T\")\n\n    # adapted from https://altair-viz.github.io/gallery/boxplot_max_min.html\n    lower_box = \"q1({}):Q\".format(quantity)\n    lower_whisker = \"min({}):Q\".format(quantity)\n    upper_box = \"q3({}):Q\".format(quantity)\n    upper_whisker = \"max({}):Q\".format(quantity)\n\n    if category_type == \"T\":\n        x_format = \"hoursminutes({}):{}\".format(category, category_type)\n    else:\n        x_format = \"{}:{}\".format(category, category_type)\n\n    lower_plot = (\n        alt.Chart(df)\n        .mark_rule()\n        .encode(y=alt.Y(lower_whisker, axis=alt.Axis(title=ylabel)), y2=lower_box, x=x_format)\n    )\n\n    middle_plot = alt.Chart(df).mark_bar(size=35).encode(y=lower_box, y2=upper_box, x=x_format)\n\n    upper_plot = alt.Chart(df).mark_rule().encode(y=upper_whisker, y2=upper_box, x=x_format)\n\n    middle_tick = (\n        alt.Chart(df)\n        .mark_tick(color=\"black\", size=35)\n        .encode(\n            y=\"median({}):Q\".format(quantity),\n            x=alt.X(x_format, axis=alt.Axis(title=xlabel), scale=alt.Scale(rangeStep=45)),\n            tooltip=\"median({}):Q\".format(quantity),\n        )\n    )\n\n    chart = lower_plot + middle_plot + upper_plot + middle_tick\n\n    if title:\n        chart = chart.properties(title=title)\n\n    return chart", "response": "Plot a simple boxplot using Altair."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a simple square dendrogram using Altair.", "response": "def dendrogram(tree):\n    \"\"\"Plot a simple square dendrogram using Altair.\n\n    Parameters\n    ----------\n    tree : `dict` returned by `scipy.cluster.hierarchy.dendrogram`\n    Contains, at a minimum, 'icoord', 'dcoord', and 'leaves' keys. Scipy does all the work of\n    determining where the lines in the tree should go. All we have to do is draw them.\n\n    Returns\n    -------\n    `altair.Chart`\n    \"\"\"\n    plot_data = {\n        \"x\": [],\n        \"y\": [],\n        \"o\": [],  # order these points should be connected in\n        \"b\": [],  # one number per branch\n    }\n\n    for idx, (i, d) in enumerate(zip(tree[\"icoord\"], tree[\"dcoord\"])):\n        plot_data[\"x\"].extend(map(lambda x: -x, d))\n        plot_data[\"y\"].extend(map(lambda x: -x, i))\n        plot_data[\"o\"].extend([0, 1, 2, 3])\n        plot_data[\"b\"].extend([idx] * 4)\n\n    plot_data = pd.DataFrame(plot_data)\n\n    chart = (\n        alt.Chart(plot_data, width=100, height=15 * len(tree[\"leaves\"]) - 7.5)\n        .mark_line(point=False, opacity=0.5)\n        .encode(\n            x=alt.X(\"x\", axis=None),\n            y=alt.Y(\"y\", axis=None, scale=alt.Scale(zero=True, nice=False)),\n            order=\"o\",\n            color=alt.Color(\n                \"b:N\",\n                scale=alt.Scale(domain=list(range(idx + 1)), range=[\"black\"] * (idx + 1)),\n                legend=None,\n            ),\n        )\n    )\n\n    return chart"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the comments produced by the genstrings script to the strings file.", "response": "def add_genstrings_comments_to_file(localization_file, genstrings_err):\n    \"\"\" Adds the comments produced by the genstrings script for duplicate keys.\n\n    Args:\n        localization_file (str): The path to the strings file.\n\n    \"\"\"\n\n    errors_to_log = [line for line in genstrings_err.splitlines() if \"used with multiple comments\" not in line]\n\n    if len(errors_to_log) > 0:\n        logging.warning(\"genstrings warnings:\\n%s\", \"\\n\".join(errors_to_log))\n\n    loc_file = open_strings_file(localization_file, \"a\")\n\n    regex_matches = re.findall(r'Warning: Key \"(.*?)\" used with multiple comments (\"[^\"]*\" (& \"[^\"]*\")+)',\n                               genstrings_err)\n\n    logging.info(\"Adding multiple comments from genstrings output\")\n    for regex_match in regex_matches:\n        if len(regex_match) == 3:\n            key = regex_match[0]\n            comments = [comment.strip()[1:-1] for comment in regex_match[1].split(\"&\")]\n\n            logging.info(\"Found key with %d comments: %s\", len(comments), key)\n\n            loc_key = LocalizationEntry(comments, key, key)\n\n            loc_file.write(unicode(loc_key))\n            loc_file.write(u\"\\n\")\n\n    loc_file.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting beta diversity distance matrix for a specific species level.", "response": "def plot_distance(\n        self,\n        rank=\"auto\",\n        metric=\"braycurtis\",\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        tooltip=None,\n        return_chart=False,\n        linkage=\"average\",\n        label=None,\n    ):\n        \"\"\"Plot beta diversity distance matrix as a heatmap and dendrogram.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        metric : {'braycurtis', 'manhattan', 'jaccard', 'unifrac', 'unweighted_unifrac}, optional\n            Function to use when calculating the distance between two samples.\n        linkage : {'average', 'single', 'complete', 'weighted', 'centroid', 'median'}\n            The type of linkage to use when clustering axes.\n        title : `string`, optional\n            Text label at the top of the plot.\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n        tooltip : `string` or `list`, optional\n            A string or list containing strings representing metadata fields. When a point in the\n            plot is hovered over, the value of the metadata associated with that sample will be\n            displayed in a modal.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Plot the weighted UniFrac distance between all our samples, using counts at the genus level.\n\n        >>> plot_distance(rank='genus', metric='unifrac')\n        \"\"\"\n        if len(self._results) < 2:\n            raise OneCodexException(\n                \"`plot_distance` requires 2 or more valid classification results.\"\n            )\n\n        # this will be passed to the heatmap chart as a dataframe eventually\n        plot_data = {\"1) Label\": [], \"2) Label\": [], \"Distance\": [], \"classification_id\": []}\n\n        # here we figure out what to put in the tooltips and get the appropriate data\n        if tooltip:\n            if not isinstance(tooltip, list):\n                tooltip = [tooltip]\n        else:\n            tooltip = []\n\n        tooltip.insert(0, \"Label\")\n\n        magic_metadata, magic_fields = self._metadata_fetch(tooltip, label=label)\n        formatted_fields = []\n\n        for _, magic_field in magic_fields.items():\n            field_group = []\n\n            for i in (1, 2):\n                field = \"{}) {}\".format(i, magic_field)\n                plot_data[field] = []\n                field_group.append(field)\n\n            formatted_fields.append(field_group)\n\n        clust = self._cluster_by_sample(rank=rank, metric=metric, linkage=linkage)\n\n        # must convert to long format for heatmap plotting\n        for idx1, id1 in enumerate(clust[\"dist_matrix\"].index):\n            for idx2, id2 in enumerate(clust[\"dist_matrix\"].index):\n                if idx1 == idx2:\n                    plot_data[\"Distance\"].append(np.nan)\n                else:\n                    plot_data[\"Distance\"].append(clust[\"dist_matrix\"].iloc[idx1, idx2])\n\n                plot_data[\"classification_id\"].append(id1)\n\n                for field_group, magic_field in zip(formatted_fields, magic_fields.values()):\n                    plot_data[field_group[0]].append(magic_metadata[magic_field][id1])\n                    plot_data[field_group[1]].append(magic_metadata[magic_field][id2])\n\n        plot_data = pd.DataFrame(data=plot_data)\n\n        labels_in_order = magic_metadata[\"Label\"][clust[\"ids_in_order\"]].tolist()\n\n        # it's important to tell altair to order the cells in the heatmap according to the clustering\n        # obtained from scipy\n        alt_kwargs = dict(\n            x=alt.X(\"1) Label:N\", axis=alt.Axis(title=xlabel), sort=labels_in_order),\n            y=alt.Y(\n                \"2) Label:N\", axis=alt.Axis(title=ylabel, orient=\"right\"), sort=labels_in_order\n            ),\n            color=\"Distance:Q\",\n            tooltip=list(chain.from_iterable(formatted_fields)) + [\"Distance:Q\"],\n            href=\"url:N\",\n            url=\"https://app.onecodex.com/classification/\" + alt.datum.classification_id,\n        )\n\n        chart = (\n            alt.Chart(\n                plot_data,\n                width=15 * len(clust[\"dist_matrix\"].index),\n                height=15 * len(clust[\"dist_matrix\"].index),\n            )\n            .transform_calculate(url=alt_kwargs.pop(\"url\"))\n            .mark_rect()\n            .encode(**alt_kwargs)\n        )\n\n        if title:\n            chart = chart.properties(title=title)\n\n        dendro_chart = dendrogram(clust[\"scipy_tree\"])\n\n        if return_chart:\n            return dendro_chart | chart\n        else:\n            (dendro_chart | chart).display()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot beta diversity distance matrix for a specific metadata class.", "response": "def plot_mds(\n        self,\n        rank=\"auto\",\n        metric=\"braycurtis\",\n        method=\"pcoa\",\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        color=None,\n        size=None,\n        tooltip=None,\n        return_chart=False,\n        label=None,\n    ):\n        \"\"\"Plot beta diversity distance matrix using multidimensional scaling (MDS).\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n        metric : {'braycurtis', 'manhattan', 'jaccard', 'unifrac', 'unweighted_unifrac}, optional\n            Function to use when calculating the distance between two samples.\n        method : {'pcoa', 'smacof'}\n            Algorithm to use for ordination. PCoA uses eigenvalue decomposition and is not well\n            suited to non-euclidean distance functions. SMACOF is an iterative optimization strategy\n            that can be used as an alternative.\n        title : `string`, optional\n            Text label at the top of the plot.\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n        size : `string` or `tuple`, optional\n            A string or a tuple containing strings representing metadata fields. The size of points\n            in the resulting plot will change based on the metadata associated with each sample.\n        color : `string` or `tuple`, optional\n            A string or a tuple containing strings representing metadata fields. The color of points\n            in the resulting plot will change based on the metadata associated with each sample.\n        tooltip : `string` or `list`, optional\n            A string or list containing strings representing metadata fields. When a point in the\n            plot is hovered over, the value of the metadata associated with that sample will be\n            displayed in a modal.\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Scatter plot of weighted UniFrac distance between all our samples, using counts at the genus\n        level.\n\n        >>> plot_mds(rank='genus', metric='unifrac')\n\n        Notes\n        -----\n        **For `smacof`**: The values reported on the axis labels are Pearson's correlations between\n        the distances between points on each axis alone, and the corresponding distances in the\n        distance matrix calculated using the user-specified metric. These values are related to the\n        effectiveness of the MDS algorithm in placing points on the scatter plot in such a way that\n        they truly represent the calculated distances. They do not reflect how well the distance\n        metric captures similarities between the underlying data (in this case, an OTU table).\n        \"\"\"\n        if len(self._results) < 2:\n            raise OneCodexException(\"`plot_mds` requires 2 or more valid classification results.\")\n\n        dists = self._compute_distance(rank, metric).to_data_frame()\n\n        # here we figure out what to put in the tooltips and get the appropriate data\n        if tooltip:\n            if not isinstance(tooltip, list):\n                tooltip = [tooltip]\n        else:\n            tooltip = []\n\n        tooltip.insert(0, \"Label\")\n\n        if color and color not in tooltip:\n            tooltip.insert(1, color)\n\n        if size and size not in tooltip:\n            tooltip.insert(2, size)\n\n        magic_metadata, magic_fields = self._metadata_fetch(tooltip, label=label)\n\n        if method == \"smacof\":\n            # adapted from https://scikit-learn.org/stable/auto_examples/manifold/plot_mds.html\n            x_field = \"MDS1\"\n            y_field = \"MDS2\"\n\n            seed = np.random.RandomState(seed=3)\n            mds = manifold.MDS(\n                max_iter=3000, eps=1e-12, random_state=seed, dissimilarity=\"precomputed\", n_jobs=1\n            )\n            pos = mds.fit(dists).embedding_\n            plot_data = pd.DataFrame(pos, columns=[x_field, y_field], index=dists.index)\n            plot_data = plot_data.div(plot_data.abs().max(axis=0), axis=1)  # normalize to [0,1]\n\n            # determine how much of the original distance is captured by each of the axes after MDS.\n            # this implementation of MDS does not use eigen decomposition and so there's no simple\n            # way of returning a 'percent of variance explained' value\n            r_squared = []\n\n            for axis in [0, 1]:\n                mds_dist = pos.copy()\n                mds_dist[::, axis] = 0\n                mds_dist = squareform(euclidean_distances(mds_dist).round(6))\n                r_squared.append(pearsonr(mds_dist, squareform(dists))[0])\n\n            # label the axes\n            x_extra_label = \"r\u00b2 = %.02f\" % (r_squared[0],)\n            y_extra_label = \"r\u00b2 = %.02f\" % (r_squared[1],)\n        elif method == \"pcoa\":\n            # suppress eigenvalue warning from skbio--not because it's an invalid warning, but\n            # because lots of folks in the field run pcoa on these distances functions, even if\n            # statistically inappropriate. perhaps this will change if we ever become more\n            # opinionated about the analyses that we allow our users to do (roo)\n            with warnings.catch_warnings():\n                warnings.simplefilter(\"ignore\")\n                ord_result = ordination.pcoa(\n                    dists.round(6)\n                )  # round to avoid float precision errors\n\n            plot_data = ord_result.samples.iloc[:, [0, 1]]  # get first two components\n            plot_data = plot_data.div(plot_data.abs().max(axis=0), axis=1)  # normalize to [0,1]\n            plot_data.index = dists.index\n            x_field, y_field = plot_data.columns.tolist()  # name of first two components\n\n            x_extra_label = \"%0.02f%%\" % (ord_result.proportion_explained[0] * 100,)\n            y_extra_label = \"%0.02f%%\" % (ord_result.proportion_explained[1] * 100,)\n        else:\n            raise OneCodexException(\"MDS method must be one of: smacof, pcoa\")\n\n        # label the axes\n        if xlabel is None:\n            xlabel = \"{} ({})\".format(x_field, x_extra_label)\n        if ylabel is None:\n            ylabel = \"{} ({})\".format(y_field, y_extra_label)\n\n        plot_data = pd.concat([plot_data, magic_metadata], axis=1).reset_index()\n\n        alt_kwargs = dict(\n            x=alt.X(x_field, axis=alt.Axis(title=xlabel)),\n            y=alt.Y(y_field, axis=alt.Axis(title=ylabel)),\n            tooltip=[magic_fields[t] for t in tooltip],\n            href=\"url:N\",\n            url=\"https://app.onecodex.com/classification/\" + alt.datum.classification_id,\n        )\n\n        # only add these parameters if they are in use\n        if color:\n            alt_kwargs[\"color\"] = magic_fields[color]\n        if size:\n            alt_kwargs[\"size\"] = magic_fields[size]\n\n        chart = (\n            alt.Chart(plot_data)\n            .transform_calculate(url=alt_kwargs.pop(\"url\"))\n            .mark_circle()\n            .encode(**alt_kwargs)\n        )\n\n        if title:\n            chart = chart.properties(title=title)\n\n        if return_chart:\n            return chart\n        else:\n            chart.interactive().display()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading the file from One Codex and saves it to a file - like object.", "response": "def download(self, path=None, file_obj=None, progressbar=False):\n        \"\"\"Downloads files from One Codex.\n\n        Parameters\n        ----------\n        path : `string`, optional\n            Full path to save the file to. If omitted, defaults to the original filename\n            in the current working directory.\n        file_obj : file-like object, optional\n            Rather than save the file to a path, write it to this file-like object.\n        progressbar : `bool`\n            Display a progress bar using Click for the download?\n\n        Returns\n        -------\n        `string`\n            The path the file was downloaded to, if applicable. Otherwise, None.\n\n        Notes\n        -----\n        If no arguments specified, defaults to download the file as the original filename\n        in the current working directory. If `file_obj` given, will write data into the\n        passed file-like object. If `path` given, will download the file to the path provided,\n        but will not overwrite any existing files.\n        \"\"\"\n        if path and file_obj:\n            raise OneCodexException(\"Please specify only one of: path, file_obj\")\n\n        if path is None and file_obj is None:\n            path = os.path.join(os.getcwd(), self.filename)\n\n        if path and os.path.exists(path):\n            raise OneCodexException(\"{} already exists! Will not overwrite.\".format(path))\n\n        try:\n            url_data = self._resource.download_uri()\n            resp = requests.get(url_data[\"download_uri\"], stream=True)\n\n            with (open(path, \"wb\") if path else file_obj) as f_out:\n                if progressbar:\n                    with click.progressbar(length=self.size, label=self.filename) as bar:\n                        for data in resp.iter_content(chunk_size=1024):\n                            bar.update(len(data))\n                            f_out.write(data)\n                else:\n                    for data in resp.iter_content(chunk_size=1024):\n                        f_out.write(data)\n        except KeyboardInterrupt:\n            if path:\n                os.remove(path)\n            raise\n        except requests.exceptions.HTTPError as exc:\n            if exc.response.status_code == 401:\n                raise OneCodexException(\"You must be logged in to download files.\")\n            elif exc.response.status_code == 402:\n                raise OneCodexException(\n                    \"You must either have a premium platform account or be in \"\n                    \"a notebook environment to download files.\"\n                )\n            elif exc.response.status_code == 403:\n                raise OneCodexException(\"You are not authorized to download this file.\")\n            else:\n                raise OneCodexException(\n                    \"Download failed with an HTTP status code {}.\".format(exc.response.status_code)\n                )\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upload(cls, file_path, progressbar=None):\n        res = cls._resource\n        doc_id = upload_document(file_path, res._client.session, res, progressbar=progressbar)\n\n        return cls.get(doc_id)", "response": "Uploads a series of files to One Codex server."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprepares the localization bundle for translation.", "response": "def prepare_for_translation(localization_bundle_path):\n    \"\"\" Prepares the localization bundle for translation.\n\n    This means, after creating the strings files using genstrings.sh, this will produce '.pending' files, that contain\n    the files that are yet to be translated.\n\n    Args:\n        localization_bundle_path (str): The path to the localization bundle.\n\n    \"\"\"\n\n    logging.info(\"Preparing for translation..\")\n\n    for strings_file in os.listdir(os.path.join(localization_bundle_path, DEFAULT_LANGUAGE_DIRECTORY_NAME)):\n        if not strings_file.endswith(\".strings\"):\n            continue\n\n        strings_path = os.path.join(localization_bundle_path, DEFAULT_LANGUAGE_DIRECTORY_NAME, strings_file)\n        for lang_dir in os.listdir(localization_bundle_path):\n            if lang_dir == DEFAULT_LANGUAGE_DIRECTORY_NAME or lang_dir.startswith(\".\"):\n                continue\n\n            dest_strings_path = os.path.join(localization_bundle_path, lang_dir, strings_file)\n            pending_path = dest_strings_path + \".pending\"\n            excluded_path = dest_strings_path + \".excluded\"\n            if not os.path.exists(dest_strings_path):\n                open_strings_file(dest_strings_path, \"a\").close()\n\n            logging.info(\"Preparing diff for %s in %s\", lang_dir, pending_path)\n            localization_diff(strings_path, dest_strings_path, excluded_path, pending_path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef documents_upload(ctx, max_threads, files):\n    if len(files) == 0:\n        click.echo(ctx.get_help())\n        return\n\n    files = list(files)\n\n    bar = click.progressbar(length=sum([_file_size(x) for x in files]), label=\"Uploading... \")\n    run_via_threadpool(\n        ctx.obj[\"API\"].Documents.upload,\n        files,\n        {\"progressbar\": bar},\n        max_threads=max_threads,\n        graceful_exit=False,\n    )", "response": "Upload a document to One Codex"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving performed metagenomic classifications", "response": "def classifications(ctx, classifications, results, readlevel, readlevel_path):\n    \"\"\"Retrieve performed metagenomic classifications\"\"\"\n\n    # basic operation -- just print\n    if not readlevel and not results:\n        cli_resource_fetcher(ctx, \"classifications\", classifications)\n\n    # fetch the results\n    elif not readlevel and results:\n        if len(classifications) != 1:\n            log.error(\"Can only request results data on one Classification at a time\")\n        else:\n            classification = ctx.obj[\"API\"].Classifications.get(classifications[0])\n            if not classification:\n                log.error(\n                    \"Could not find classification {} (404 status code)\".format(classifications[0])\n                )\n                return\n            results = classification.results(json=True)\n            pprint(results, ctx.obj[\"NOPPRINT\"])\n\n    # fetch the readlevel\n    elif readlevel is not None and not results:\n        if len(classifications) != 1:\n            log.error(\"Can only request read-level data on one Classification at a time\")\n        else:\n            classification = ctx.obj[\"API\"].Classifications.get(classifications[0])\n            if not classification:\n                log.error(\n                    \"Could not find classification {} (404 status code)\".format(classifications[0])\n                )\n                return\n            tsv_url = classification._readlevel()[\"url\"]\n            log.info(\"Downloading tsv data from: {}\".format(tsv_url))\n            download_file_helper(tsv_url, readlevel_path)\n\n    # both given -- complain\n    else:\n        log.error(\"Can only request one of read-level data or results data at a time\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nuploads a FASTA or FASTQ file to One Codex.", "response": "def upload(\n    ctx, files, max_threads, prompt, forward, reverse, tags, metadata, project_id, coerce_ascii\n):\n    \"\"\"Upload a FASTA or FASTQ (optionally gzip'd) to One Codex\"\"\"\n\n    appendables = {}\n    if tags:\n        appendables[\"tags\"] = []\n        for tag in tags:\n            appendables[\"tags\"].append(tag)\n\n    if metadata:\n        appendables[\"metadata\"] = {}\n        for metadata_kv in metadata:\n            split_metadata = metadata_kv.split(\"=\", 1)\n            if len(split_metadata) > 1:\n                metadata_value = split_metadata[1]\n                appendables[\"metadata\"][split_metadata[0]] = metadata_value\n\n    appendables = validate_appendables(appendables, ctx.obj[\"API\"])\n\n    if (forward or reverse) and not (forward and reverse):\n        click.echo(\"You must specify both forward and reverse files\", err=True)\n        ctx.exit(1)\n\n    if forward and reverse:\n        if len(files) > 0:\n            click.echo(\n                \"You may not pass a FILES argument when using the \"\n                \" --forward and --reverse options.\",\n                err=True,\n            )\n            ctx.exit(1)\n        files = [(forward, reverse)]\n    elif len(files) == 0:\n        click.echo(ctx.get_help())\n        return\n    else:\n        files = list(files)\n\n        # \"intelligently\" find paired files and tuple them\n        paired_files = []\n        single_files = set(files)\n\n        for filename in files:\n            # convert \"read 1\" filenames into \"read 2\" and check that they exist; if they do\n            # upload the files as a pair, autointerleaving them\n            pair = re.sub(\"[._][Rr]1[._]\", lambda x: x.group().replace(\"1\", \"2\"), filename)\n\n            # we don't necessary need the R2 to have been passed in; we infer it anyways\n            if pair != filename and os.path.exists(pair):\n                if not prompt and pair not in single_files:\n                    # if we're not prompting, don't automatically pull in files\n                    # not in the list the user passed in\n                    continue\n\n                paired_files.append((filename, pair))\n\n                if pair in single_files:\n                    single_files.remove(pair)\n\n                single_files.remove(filename)\n\n        auto_pair = True\n\n        if prompt and len(paired_files) > 0:\n            pair_list = \"\"\n            for p in paired_files:\n                pair_list += \"\\n  {}  &  {}\".format(os.path.basename(p[0]), os.path.basename(p[1]))\n\n            answer = click.confirm(\n                \"It appears there are paired files:{}\\nInterleave them after upload?\".format(\n                    pair_list\n                ),\n                default=\"Y\",\n            )\n\n            if not answer:\n                auto_pair = False\n\n        if auto_pair:\n            files = paired_files + list(single_files)\n\n    total_size = sum(\n        [\n            (_file_size(x[0], uncompressed=True) + _file_size(x[1], uncompressed=True))\n            if isinstance(x, tuple)\n            else _file_size(x, uncompressed=False)\n            for x in files\n        ]\n    )\n\n    upload_kwargs = {\n        \"metadata\": appendables[\"valid_metadata\"],\n        \"tags\": appendables[\"valid_tags\"],\n        \"project\": project_id,\n        \"coerce_ascii\": coerce_ascii,\n        \"progressbar\": progressbar(length=total_size, label=\"Uploading...\"),\n    }\n\n    run_via_threadpool(\n        ctx.obj[\"API\"].Samples.upload,\n        files,\n        upload_kwargs,\n        max_threads=max_threads,\n        graceful_exit=False,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to One Codex and check if the credentials match.", "response": "def login(ctx):\n    \"\"\"Add an API key (saved in ~/.onecodex)\"\"\"\n    base_url = os.environ.get(\"ONE_CODEX_API_BASE\", \"https://app.onecodex.com\")\n    if not ctx.obj[\"API_KEY\"]:\n        _login(base_url)\n    else:\n        email = _login(base_url, api_key=ctx.obj[\"API_KEY\"])\n        ocx = Api(api_key=ctx.obj[\"API_KEY\"], telemetry=ctx.obj[\"TELEMETRY\"])\n\n        # TODO: This should be protected or built in as a first class resource\n        # with, e.g., connection error catching (it's not part of our formally documeted API at the moment)\n        if ocx._client.Account.instances()[\"email\"] != email:\n            click.echo(\"Your login credentials do not match the provided email!\", err=True)\n            _remove_creds()\n            ctx.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_taxonomy_dict(classification, parent=False):\n\n    tax_id_map = {}\n\n    if parent:\n        for row in classification.results()[\"table\"]:\n            if row[\"parent_tax_id\"] is not None and row[\"tax_id\"] is not None:\n                tax_id_map[row[\"tax_id\"]] = row[\"parent_tax_id\"]\n    else:\n        for row in classification.results()[\"table\"]:\n            if row[\"parent_tax_id\"] is not None and row[\"tax_id\"] is not None:\n                try:\n                    tax_id_map[row[\"parent_tax_id\"]].add(row[\"tax_id\"])\n                except KeyError:\n                    tax_id_map[row[\"parent_tax_id\"]] = set([row[\"tax_id\"]])\n\n    return tax_id_map", "response": "Takes a classification data frame and parses it into a dictionary mapping a tax_id to its children or parent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recurse_taxonomy_map(tax_id_map, tax_id, parent=False):\n\n    if parent:\n        # TODO: allow filtering on tax_id and its parents, too\n        pass\n    else:\n\n        def _child_recurse(tax_id, visited):\n            try:\n                children = [tax_id] + list(tax_id_map[tax_id])\n            except KeyError:\n                children = [tax_id]\n\n            for child in children:\n                if child not in visited:\n                    visited.append(child)\n                    children.extend(_child_recurse(child, visited))\n\n            return children\n\n        return list(set(_child_recurse(tax_id, [])))", "response": "Recursively walks the taxonomy tree to get all the children of the given tax_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reference(text=None, label=None):\n    if text is None and label is None:\n        raise OneCodexException(\"Please specify at least one of: text, label\")\n\n    try:\n        ipy = get_ipython()\n        ref_list = ipy.meta.get(\"references\", {})\n    except NameError:\n        raise OneCodexException(\"Must be run from within IPython\")\n\n    def to_html(ref_num):\n        return '<sup class=\"reference\">{}</sup>'.format(ref_num)\n\n    if text is not None:\n        # has this reference already been cited?\n        for ref_label, (ref_num, ref_text) in ref_list.items():\n            if text == ref_text:\n                if label is not None and label != ref_label:\n                    raise OneCodexException(\n                        \"Citation already in use with label={}\".format(ref_label)\n                    )\n                else:\n                    break\n        else:\n            # reference has not been cited. is the label already in use?\n            if label is not None and label in ref_list.keys():\n                raise OneCodexException(\"Citation label={} already in use\".format(label))\n\n            # create the citation and assign next number\n            if not ref_list:\n                ref_num = 1\n            else:\n                ref_num = max([x[0] for x in ref_list.values()]) + 1\n\n            if label is None:\n                ref_label = ref_num\n            else:\n                ref_label = label\n\n            ref_list[ref_label] = (ref_num, text)\n            ipy.meta[\"references\"] = ref_list\n\n        return to_html(ref_num)\n    elif label is not None:\n        if label not in ref_list.keys():\n            raise OneCodexException(\"Cannot find citation with label={}\".format(label))\n\n        return to_html(ref_list[label][0])", "response": "Add a reference to the bibliography and insert a superscript number."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new resource with a given schema.", "response": "def resource_factory(self, name, schema, resource_cls=None):\n        \"\"\"\n        Registers a new resource with a given schema. The schema must not have any unresolved references\n        (such as `{\"$ref\": \"#\"}` for self-references, or otherwise). A subclass of :class:`Resource`\n        may be provided to add specific functionality to the resulting :class:`Resource`.\n\n        :param str name:\n        :param dict schema:\n        :param Resource resource_cls: a subclass of :class:`Resource` or None\n        :return: The new :class:`Resource`.\n        \"\"\"\n        cls = type(str(upper_camel_case(name)), (resource_cls or Resource, collections.MutableMapping), {\n            '__doc__': schema.get('description', '')\n        })\n\n        cls._schema = schema\n        cls._client = self\n        cls._links = links = {}\n\n        for link_schema in schema['links']:\n            link = Link(self,\n                        rel=link_schema['rel'],\n                        href=link_schema['href'],\n                        method=link_schema['method'],\n                        schema=link_schema.get('schema', None),\n                        target_schema=link_schema.get('targetSchema', None))\n\n            # Set Resource._self, etc. for the special methods as they are managed by the Resource class\n            if link.rel in ('self', 'instances', 'create', 'update', 'destroy'):\n                setattr(cls, '_{}'.format(link.rel), link)\n            links[link.rel] = link\n\n            if link.rel != 'update':  # 'update' is a special case because of MutableMapping.update()\n                setattr(cls, snake_case(link.rel), link)\n\n        # TODO routes (instance & non-instance)\n\n        for property_name, property_schema in schema.get('properties', {}).items():\n            # skip $uri and $id as these are already implemented in Resource and overriding them causes unnecessary\n            # fetches.\n            if property_name.startswith('$'):\n                continue\n\n            if property_schema.get('readOnly', False):\n                # TODO better error message. Raises AttributeError(\"can't set attribute\")\n                setattr(cls,\n                        property_name,\n                        property(fget=partial((lambda name, obj: getitem(obj, name)), property_name),\n                                 doc=property_schema.get('description', None)))\n            else:\n                setattr(cls,\n                        property_name,\n                        property(fget=partial((lambda name, obj: getitem(obj, name)), property_name),\n                                 fset=partial((lambda name, obj, value: setitem(obj, name, value)), property_name),\n                                 fdel=partial((lambda name, obj: delitem(obj, name)), property_name),\n                                 doc=property_schema.get('description', None)))\n\n        root = None\n        if 'instances' in links:\n            root = cls._instances.href\n        elif 'self' in links:\n            root = cls._self.href[:cls._self.href.rfind('/')]\n        else:\n            root = self._root_path + '/' + name.replace('_', '-')\n\n        self._resources[root] = cls\n        return cls"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract all string pairs in the given directory.", "response": "def extract_string_pairs_in_dir(directory, exclude_dirs, special_ui_components_prefix):\n    \"\"\" Extract string pairs in the given directory's xib/storyboard files.\n\n    Args:\n        directory (str): The path to the directory.\n        exclude_dirs (str): A list of directories to exclude from extraction.\n        special_ui_components_prefix (str):\n            If not None, extraction will not warn about internationalized UI components with this class prefix.\n\n    Returns:\n        list: The extracted string pairs for all IB files in the directory.\n\n    \"\"\"\n    result = []\n    for ib_file_path in find_files(directory, [\".xib\", \".storyboard\"], exclude_dirs):\n        result += extract_string_pairs_in_ib_file(ib_file_path, special_ui_components_prefix)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extract_element_internationalized_comment(element):\n    element_entry_comment = get_element_attribute_or_empty(element, 'userLabel')\n    if element_entry_comment == \"\":\n        try:\n            element_entry_comment = element.getElementsByTagName('string')[0].firstChild.nodeValue\n        except Exception:\n            element_entry_comment = \"\"\n    if not element_entry_comment.lower().startswith(JT_INTERNATIONALIZED_COMMENT_PREFIX):\n        return None\n    else:\n        return element_entry_comment[len(JT_INTERNATIONALIZED_COMMENT_PREFIX):]", "response": "Extracts the element s internationalized comment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging a warning if the element is not of the given type.", "response": "def warn_if_element_not_of_class(element, class_suffix, special_ui_components_prefix):\n    \"\"\" Log a warning if the element is not of the given type (indicating that it is not internationalized).\n\n    Args:\n        element: The xib's XML element.\n        class_name: The type the element should be, but is missing.\n        special_ui_components_prefix: If provided, will not warn about class with this prefix (default is only 'JT')\n    \"\"\"\n    valid_class_names = [\"%s%s\" % (DEFAULT_UI_COMPONENTS_PREFIX, class_suffix)]\n    if special_ui_components_prefix is not None:\n        valid_class_names.append(\"%s%s\" % (special_ui_components_prefix, class_suffix))\n\n    if (not element.hasAttribute('customClass')) or element.attributes['customClass'].value not in valid_class_names:\n        logging.warn(\"WARNING: %s is internationalized but isn't one of %s\",\n                     extract_element_internationalized_comment(element), valid_class_names)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_string_pairs_from_attributed_ui_element(results, ui_element, comment_prefix):\n    attributed_strings = ui_element.getElementsByTagName('attributedString')\n    if attributed_strings.length == 0:\n        return False\n\n    attributed_element = attributed_strings[0]\n    fragment_index = 1\n    for fragment in attributed_element.getElementsByTagName('fragment'):\n        # The fragment text is either as an attribute <fragment content=\"TEXT\">\n        # or a child in the format <string key='content'>TEXT</string>\n        try:\n            label_entry_key = fragment.attributes['content'].value\n        except KeyError:\n            label_entry_key = fragment.getElementsByTagName('string')[0].firstChild.nodeValue\n\n        comment = \"%s Part %d\" % (comment_prefix, fragment_index)\n        results.append((label_entry_key, comment))\n        fragment_index += 1\n\n    return fragment_index > 1", "response": "Adds string pairs from an attributed UI element with attributed text as a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_string_pairs_from_label_element(xib_file, results, label, special_ui_components_prefix):\n    label_entry_comment = extract_element_internationalized_comment(label)\n    if label_entry_comment is None:\n        return\n\n    warn_if_element_not_of_class(label, 'Label', special_ui_components_prefix)\n\n    if label.hasAttribute('usesAttributedText') and label.attributes['usesAttributedText'].value == 'YES':\n        add_string_pairs_from_attributed_ui_element(results, label, label_entry_comment)\n    else:\n        try:\n            label_entry_key = label.attributes['text'].value\n        except KeyError:\n            try:\n                label_entry_key = label.getElementsByTagName('string')[0].firstChild.nodeValue\n            except Exception:\n                label_entry_key = 'N/A'\n                logging.warn(\"%s: Missing text entry in %s\", xib_file, label.toxml('UTF8'))\n        results.append((label_entry_key, label_entry_comment))", "response": "Adds string pairs from a label element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_string_pairs_from_text_field_element(xib_file, results, text_field, special_ui_components_prefix):\n    text_field_entry_comment = extract_element_internationalized_comment(text_field)\n    if text_field_entry_comment is None:\n        return\n\n    if text_field.hasAttribute('usesAttributedText') and text_field.attributes['usesAttributedText'].value == 'YES':\n        add_string_pairs_from_attributed_ui_element(results, text_field, text_field_entry_comment)\n    else:\n        try:\n            text_field_entry_key = text_field.attributes['text'].value\n            results.append((text_field_entry_key, text_field_entry_comment + ' default text value'))\n        except KeyError:\n            pass\n    try:\n        text_field_entry_key = text_field.attributes['placeholder'].value\n        results.append((text_field_entry_key, text_field_entry_comment + ' placeholder text value'))\n    except KeyError:\n        pass\n    warn_if_element_not_of_class(text_field, 'TextField', special_ui_components_prefix)", "response": "Adds string pairs from a textfield element to the list of results."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_string_pairs_from_text_view_element(xib_file, results, text_view, special_ui_components_prefix):\n    text_view_entry_comment = extract_element_internationalized_comment(text_view)\n    if text_view_entry_comment is None:\n        return\n\n    if text_view.hasAttribute('usesAttributedText') and text_view.attributes['usesAttributedText'].value == 'YES':\n        add_string_pairs_from_attributed_ui_element(results, text_view, text_view_entry_comment)\n    else:\n        try:\n            text_view_entry_key = text_view.attributes['text'].value\n            results.append((text_view_entry_key, text_view_entry_comment + ' default text value'))\n        except KeyError:\n            pass\n    warn_if_element_not_of_class(text_view, 'TextView', special_ui_components_prefix)", "response": "Adds string pairs from a textview element to the list of results."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_string_pairs_from_button_element(xib_file, results, button, special_ui_components_prefix):\n    button_entry_comment = extract_element_internationalized_comment(button)\n    if button_entry_comment is None:\n        return\n\n    for state in button.getElementsByTagName('state'):\n        state_name = state.attributes['key'].value\n        state_entry_comment = button_entry_comment + \" - \" + state_name + \" state of button\"\n        if not add_string_pairs_from_attributed_ui_element(results, state, state_entry_comment):\n            try:\n                button_entry_key = state.attributes['title'].value\n            except KeyError:\n                try:\n                    button_entry_key = state.getElementsByTagName('string')[0].firstChild.nodeValue\n                except Exception:\n                    continue\n\n            results.append((button_entry_key, state_entry_comment))\n\n    warn_if_element_not_of_class(button, 'Button', special_ui_components_prefix)", "response": "Adds strings pairs from a button element to the list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extract_string_pairs_in_ib_file(file_path, special_ui_components_prefix):\n    try:\n        results = []\n        xmldoc = minidom.parse(file_path)\n\n        element_name_to_add_func = {'label': add_string_pairs_from_label_element,\n                                    'button': add_string_pairs_from_button_element,\n                                    'textField': add_string_pairs_from_text_field_element,\n                                    'textView': add_string_pairs_from_text_view_element}\n\n        for element_name in element_name_to_add_func:\n            add_func = element_name_to_add_func[element_name]\n            elements = xmldoc.getElementsByTagName(element_name)\n            for element in elements:\n                add_func(file_path, results, element, special_ui_components_prefix)\n\n        # Find strings of format JTL('Key Name', 'Key Comment') and add them to the results\n        jtl_brackets_find_results = re.findall(JTL_REGEX, open(file_path).read())\n        unescaped_jtl_brackets_find_results = [(unescape(x), unescape(y)) for (x, y) in jtl_brackets_find_results]\n        results += unescaped_jtl_brackets_find_results\n\n        if len(results) > 0:\n            results = [(None, os.path.basename(file_path))] + results\n        return results\n\n    except Exception, e:\n        logging.warn(\"ERROR: Error processing %s (%s: %s)\", file_path, type(e), str(e))\n        return []", "response": "Extracts the strings pairs from an xib file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef localization_diff(localizable_file, translated_file, excluded_strings_file, output_translation_file):\n    old_translated_file_dictionary = generate_localization_key_to_entry_dictionary_from_file(translated_file)\n    if excluded_strings_file is not None and os.path.isfile(excluded_strings_file):\n        excluded_file_dictionary = generate_localization_key_to_entry_dictionary_from_file(excluded_strings_file)\n    else:\n        excluded_file_dictionary = {}\n\n    # The reason we keep a list of the keys, and not just pop is because values can repeat themselves.\n    translated_list = old_translated_file_dictionary.keys()\n    output_dictionary = {}\n    output_file_elements = []\n    f = open_strings_file(localizable_file, \"r\")\n\n    output_file_elements.append(Comment(u\"\"\"\n/**\n * This file contains all the strings that were extracted from our app and that need to be translated.\n * Each entry may or may not have a comment explaining context, and a \"key\" = \"%s\" equation.\n * To localize, you need to fill the right side of the equation with the translation of the left side.\n * Please keep special expressions such as '%%@' or '%%1$@' as is. Usually the comment will explain their context.\n */\n\"\"\" % (VALUE_PLACEHOLDER,)))\n\n    for _header_comment, comments, key, value in extract_header_comment_key_value_tuples_from_file(f):\n        if key in translated_list or key in excluded_file_dictionary:\n            if key in old_translated_file_dictionary:\n                old_translated_file_dictionary.pop(key)\n        elif value in output_dictionary:\n            output_dictionary[value].add_comments(comments)\n            output_file_elements.append(Comment(\n                u\"/* There was a value '%s' here but it was a duplicate of an older value and removed. */\\n\" % value))\n        else:\n            loc_obj = LocalizationEntry(comments, value, VALUE_PLACEHOLDER)\n            output_dictionary[value] = loc_obj\n            output_file_elements.append(loc_obj)\n\n    for key, removed_trans in old_translated_file_dictionary.items():\n        output_file_elements.append(Comment(u\"\"\"\n/*\n * Entry removed from previous translation file:\n * %s\n * \"%s\" = \"%s\";\n */\n\"\"\" % (\", \".join(removed_trans.comments), removed_trans.key, removed_trans.value)))\n\n    write_file_elements_to_strings_file(output_translation_file, output_file_elements)", "response": "Generates a new string file that contains all the strings that need to be translated and the original strings that need to be translated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplot an arbitrary metadata field versus an arbitrary quantity as a boxplot or scatter plot.", "response": "def plot_metadata(\n        self,\n        rank=\"auto\",\n        haxis=\"Label\",\n        vaxis=\"simpson\",\n        title=None,\n        xlabel=None,\n        ylabel=None,\n        return_chart=False,\n        plot_type=\"auto\",\n        label=None,\n    ):\n        \"\"\"Plot an arbitrary metadata field versus an arbitrary quantity as a boxplot or scatter plot.\n\n        Parameters\n        ----------\n        rank : {'auto', 'kingdom', 'phylum', 'class', 'order', 'family', 'genus', 'species'}, optional\n            Analysis will be restricted to abundances of taxa at the specified level.\n\n        haxis : `string`, optional\n            The metadata field (or tuple containing multiple categorical fields) to be plotted on\n            the horizontal axis.\n\n        vaxis : `string`, optional\n            Data to be plotted on the vertical axis. Can be any one of the following:\n\n            - A metadata field: the name of a metadata field containing numerical data\n            - {'simpson', 'chao1', 'shannon'}: an alpha diversity statistic to calculate for each sample\n            - A taxon name: the name of a taxon in the analysis\n            - A taxon ID: the ID of a taxon in the analysis\n\n        title : `string`, optional\n            Text label at the top of the plot.\n\n        xlabel : `string`, optional\n            Text label along the horizontal axis.\n\n        ylabel : `string`, optional\n            Text label along the vertical axis.\n\n        plot_type : {'auto', 'boxplot', 'scatter'}\n            By default, will determine plot type automatically based on the data. Otherwise, specify\n            one of 'boxplot' or 'scatter' to set the type of plot manually.\n\n        label : `string` or `callable`, optional\n            A metadata field (or function) used to label each analysis. If passing a function, a\n            dict containing the metadata for each analysis is passed as the first and only\n            positional argument. The callable function must return a string.\n\n        Examples\n        --------\n        Generate a boxplot of the abundance of Bacteroides (genus) of samples grouped by whether the\n        individuals are allergy to dogs, cats, both, or neither.\n\n        >>> plot_metadata(haxis=('allergy_dogs', 'allergy_cats'), vaxis='Bacteroides')\n        \"\"\"\n        if rank is None:\n            raise OneCodexException(\"Please specify a rank or 'auto' to choose automatically\")\n\n        if plot_type not in (\"auto\", \"boxplot\", \"scatter\"):\n            raise OneCodexException(\"Plot type must be one of: auto, boxplot, scatter\")\n\n        # alpha diversity is only allowed on vertical axis--horizontal can be magically mapped\n        df, magic_fields = self._metadata_fetch([haxis, \"Label\"], label=label)\n\n        if vaxis in (\"simpson\", \"chao1\", \"shannon\"):\n            df.loc[:, vaxis] = self.alpha_diversity(vaxis, rank=rank)\n            magic_fields[vaxis] = vaxis\n        else:\n            # if it's not alpha diversity, vertical axis can also be magically mapped\n            vert_df, vert_magic_fields = self._metadata_fetch([vaxis])\n\n            # we require the vertical axis to be numerical otherwise plots get weird\n            if (\n                pd.api.types.is_bool_dtype(vert_df[vert_magic_fields[vaxis]])\n                or pd.api.types.is_categorical_dtype(vert_df[vert_magic_fields[vaxis]])\n                or pd.api.types.is_object_dtype(vert_df[vert_magic_fields[vaxis]])\n                or not pd.api.types.is_numeric_dtype(vert_df[vert_magic_fields[vaxis]])\n            ):  # noqa\n                raise OneCodexException(\"Metadata field on vertical axis must be numerical\")\n\n            df = pd.concat([df, vert_df], axis=1).dropna(subset=[vert_magic_fields[vaxis]])\n            magic_fields.update(vert_magic_fields)\n\n        # plots can look different depending on what the horizontal axis contains\n        if pd.api.types.is_datetime64_any_dtype(df[magic_fields[haxis]]):\n            category_type = \"T\"\n\n            if plot_type == \"auto\":\n                plot_type = \"boxplot\"\n        elif \"date\" in magic_fields[haxis].split(\"_\"):\n            df.loc[:, magic_fields[haxis]] = df.loc[:, magic_fields[haxis]].apply(\n                pd.to_datetime, utc=True\n            )\n\n            category_type = \"T\"\n\n            if plot_type == \"auto\":\n                plot_type = \"boxplot\"\n        elif (\n            pd.api.types.is_bool_dtype(df[magic_fields[haxis]])\n            or pd.api.types.is_categorical_dtype(df[magic_fields[haxis]])\n            or pd.api.types.is_object_dtype(df[magic_fields[haxis]])\n        ):  # noqa\n            df = df.fillna({field: \"N/A\" for field in df.columns})\n\n            category_type = \"N\"\n\n            if plot_type == \"auto\":\n                # if data is categorical but there is only one value per sample, scatter plot instead\n                if len(df[magic_fields[haxis]].unique()) == len(df[magic_fields[haxis]]):\n                    plot_type = \"scatter\"\n                else:\n                    plot_type = \"boxplot\"\n        elif pd.api.types.is_numeric_dtype(df[magic_fields[haxis]]):\n            df = df.dropna(subset=[magic_fields[vaxis]])\n\n            category_type = \"O\"\n\n            if plot_type == \"auto\":\n                plot_type = \"scatter\"\n        else:\n            raise OneCodexException(\n                \"Unplottable column type for horizontal axis ({})\".format(haxis)\n            )\n\n        if xlabel is None:\n            xlabel = magic_fields[haxis]\n\n        if ylabel is None:\n            ylabel = magic_fields[vaxis]\n\n        if plot_type == \"scatter\":\n            df = df.reset_index()\n\n            alt_kwargs = dict(\n                x=alt.X(magic_fields[haxis], axis=alt.Axis(title=xlabel)),\n                y=alt.Y(magic_fields[vaxis], axis=alt.Axis(title=ylabel)),\n                tooltip=[\"Label\", \"{}:Q\".format(vaxis)],\n                href=\"url:N\",\n                url=\"https://app.onecodex.com/classification/\" + alt.datum.classification_id,\n            )\n\n            chart = (\n                alt.Chart(df)\n                .transform_calculate(url=alt_kwargs.pop(\"url\"))\n                .mark_circle()\n                .encode(**alt_kwargs)\n            )\n\n            if title:\n                chart = chart.properties(title=title)\n        elif plot_type == \"boxplot\":\n            chart = boxplot(\n                df,\n                magic_fields[haxis],\n                magic_fields[vaxis],\n                category_type=category_type,\n                title=title,\n                xlabel=xlabel,\n                ylabel=ylabel,\n            )\n\n        if return_chart:\n            return chart\n        else:\n            chart.interactive().display()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_404_page(app):\n    is_epub = isinstance(app.builder, EpubBuilder)\n    config_pages = app.config.html_additional_pages\n\n    if not is_epub and \"404\" not in config_pages:\n        yield (\"404\", {}, \"404.html\")", "response": "Add a 404. html page if no key is in the the\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef canonical_url(app, pagename, templatename, context, doctree):\n    base = context.get(\"canonical_url\")\n\n    if not base:\n        return\n\n    target = app.builder.get_target_uri(pagename)\n    context[\"page_canonical_url\"] = base + target", "response": "Build the canonical URL for a page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nskipping rendering internal modules.", "response": "def skip_internal(app, what, name, obj, skip, options):\n    \"\"\"Skip rendering autodoc when the docstring contains a line with\n    only the string `:internal:`.\n    \"\"\"\n    docstring = inspect.getdoc(obj) or \"\"\n\n    if skip or re.search(r\"^\\s*:internal:\\s*$\", docstring, re.M) is not None:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncuts module meta information from source code.", "response": "def cut_module_meta(app, what, name, obj, options, lines):\n    \"\"\"Don't render lines that start with ``:copyright:`` or\n    ``:license:`` when rendering module autodoc. These lines are useful\n    meta information in the source code, but are noisy in the docs.\n    \"\"\"\n    if what != \"module\":\n        return\n\n    lines[:] = [\n        line for line in lines if not line.startswith((\":copyright:\", \":license:\"))\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that the named package is installed and returns version strings to be used by Sphinx.", "response": "def get_version(name, version_length=2, placeholder=\"x\"):\n    \"\"\"Ensures that the named package is installed and returns version\n    strings to be used by Sphinx.\n\n    Sphinx uses ``version`` to mean an abbreviated form of the full\n    version string, which is called ``release``. In ``conf.py``::\n\n        release, version = get_version(\"Flask\")\n        # release = 1.0.x, version = 1.0.3.dev0\n\n    :param name: Name of package to get.\n    :param version_length: How many values from ``release`` to use for\n        ``version``.\n    :param placeholder: Extra suffix to add to the version. The default\n        produces versions like ``1.2.x``.\n    :return: ``(release, version)`` tuple.\n    \"\"\"\n    try:\n        release = pkg_resources.get_distribution(name).version\n    except ImportError:\n        print(\n            textwrap.fill(\n                \"'{name}' must be installed to build the documentation.\"\n                \" Install from source using `pip install -e .` in a\"\n                \" virtualenv.\".format(name=name)\n            )\n        )\n        sys.exit(1)\n\n    version = \".\".join(release.split(\".\", version_length)[:version_length])\n\n    if placeholder:\n        version = \"{}.{}\".format(version, placeholder)\n\n    return release, version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the is_pallets_theme config to True if the current theme is a decedent of the pocoo theme.", "response": "def set_is_pallets_theme(app):\n    \"\"\"Set the ``is_pallets_theme`` config to ``True`` if the current\n    theme is a decedent of the ``pocoo`` theme.\n    \"\"\"\n    if app.config.is_pallets_theme is not None:\n        return\n\n    theme = getattr(app.builder, \"theme\", None)\n\n    while theme is not None:\n        if theme.name == \"pocoo\":\n            app.config.is_pallets_theme = True\n            break\n\n        theme = theme.base\n    else:\n        app.config.is_pallets_theme = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a decorator that calls a function only if the the is_pallets_theme config is True.", "response": "def only_pallets_theme(default=None):\n    \"\"\"Create a decorator that calls a function only if the\n    ``is_pallets_theme`` config is ``True``.\n\n    Used to prevent Sphinx event callbacks from doing anything if the\n    Pallets themes are installed but not used. ::\n\n        @only_pallets_theme()\n        def inject_value(app):\n            ...\n\n        app.connect(\"builder-inited\", inject_value)\n\n    :param default: Value to return if a Pallets theme is not in use.\n    :return: A decorator.\n    \"\"\"\n\n    def decorator(f):\n        @wraps(f)\n        def wrapped(app, *args, **kwargs):\n            if not app.config.is_pallets_theme:\n                return default\n\n            return f(app, *args, **kwargs)\n\n        return wrapped\n\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef patch_modules():\n    old_call = subprocess.call\n\n    def dummy_call(*args, **kwargs):\n        with tempfile.TemporaryFile(\"wb+\") as f:\n            kwargs[\"stdout\"] = f\n            kwargs[\"stderr\"] = f\n            rv = subprocess.Popen(*args, **kwargs).wait()\n            f.seek(0)\n            click.echo(f.read().decode(\"utf-8\", \"replace\").rstrip())\n        return rv\n\n    subprocess.call = dummy_call\n\n    try:\n        yield\n    finally:\n        subprocess.call = old_call", "response": "Patch subprocess. call to work better with example runner. invoke."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_example_runner(document):\n    runner = getattr(document, \"click_example_runner\", None)\n    if runner is None:\n        runner = document.click_example_runner = ExampleRunner()\n    return runner", "response": "Get or create the ExampleRunner instance associated with\n    a document."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlikes CLIRunner. invoke but displays what the user would enter in the terminal for the specified command args and environment variables.", "response": "def invoke(\n        self,\n        cli,\n        args=None,\n        prog_name=None,\n        input=None,\n        terminate_input=False,\n        env=None,\n        _output_lines=None,\n        **extra\n    ):\n        \"\"\"Like :meth:`CliRunner.invoke` but displays what the user\n        would enter in the terminal for env vars, command args, and\n        prompts.\n\n        :param terminate_input: Whether to display \"^D\" after a list of\n            input.\n        :param _output_lines: A list used internally to collect lines to\n            be displayed.\n        \"\"\"\n        output_lines = _output_lines if _output_lines is not None else []\n\n        if env:\n            for key, value in sorted(env.items()):\n                value = shlex.quote(value)\n                output_lines.append(\"$ export {}={}\".format(key, value))\n\n        args = args or []\n\n        if prog_name is None:\n            prog_name = cli.name.replace(\"_\", \"-\")\n\n        output_lines.append(\n            \"$ {} {}\".format(prog_name, \" \".join(shlex.quote(x) for x in args)).rstrip()\n        )\n        # remove \"python\" from command\n        prog_name = prog_name.rsplit(\" \", 1)[-1]\n\n        if isinstance(input, (tuple, list)):\n            input = \"\\n\".join(input) + \"\\n\"\n\n            if terminate_input:\n                input += \"\\x04\"\n\n        result = super(ExampleRunner, self).invoke(\n            cli=cli, args=args, input=input, env=env, prog_name=prog_name, **extra\n        )\n        output_lines.extend(result.output.splitlines())\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nexecuting the given code adding it to the runner s namespace.", "response": "def declare_example(self, source):\n        \"\"\"Execute the given code, adding it to the runner's namespace.\"\"\"\n        with patch_modules():\n            code = compile(source, \"<docs>\", \"exec\")\n            exec(code, self.namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run_example(self, source):\n        code = compile(source, \"<docs>\", \"exec\")\n        buffer = []\n        invoke = partial(self.invoke, _output_lines=buffer)\n\n        def println(text=\"\"):\n            buffer.append(text)\n\n        exec(\n            code,\n            self.namespace,\n            {\n                \"invoke\": invoke,\n                \"println\": println,\n                \"isolated_filesystem\": self.isolated_filesystem,\n            },\n        )\n        return buffer", "response": "Runs the given code and returns the lines of input and output."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setup(\n    *,\n    verbose: bool = False,\n    quiet: bool = False,\n    color: str = \"auto\",\n    title: str = \"auto\",\n    timestamp: bool = False\n) -> None:\n    \"\"\" Configure behavior of message functions.\n\n    :param verbose: Whether :func:`debug` messages should get printed\n    :param quiet: Hide every message except :func:`warning`, :func:`error`, and\n                  :func:`fatal`\n    :param color: Choices: 'auto', 'always', or 'never'. Whether to color output.\n                  By default ('auto'), only use color when output is a terminal.\n    :param title: Ditto for setting terminal title\n    :param timestamp: Whether to prefix every message with a time stamp\n    \"\"\"\n    _setup(verbose=verbose, quiet=quiet, color=color, title=title, timestamp=timestamp)", "response": "Configure behavior of message functions."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn two strings from a list of tokens. One containing ASCII escape codes the other only the normal characters", "response": "def process_tokens(\n    tokens: Sequence[Token], *, end: str = \"\\n\", sep: str = \" \"\n) -> Tuple[str, str]:\n    \"\"\" Returns two strings from a list of tokens.\n    One containing ASCII escape codes, the other\n    only the 'normal' characters\n\n    \"\"\"\n    # Flatten the list of tokens in case some of them are of\n    # class UnicodeSequence:\n    flat_tokens = list()  # type: List[Token]\n    for token in tokens:\n        if isinstance(token, UnicodeSequence):\n            flat_tokens.extend(token.tuple())\n        else:\n            flat_tokens.append(token)\n\n    with_color = _process_tokens(flat_tokens, end=end, sep=sep, color=True)\n    without_color = _process_tokens(flat_tokens, end=end, sep=sep, color=False)\n    return (with_color, without_color)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef message(\n    *tokens: Token,\n    end: str = \"\\n\",\n    sep: str = \" \",\n    fileobj: FileObj = sys.stdout,\n    update_title: bool = False\n) -> None:\n    \"\"\" Helper method for error, warning, info, debug\n\n    \"\"\"\n    if using_colorama():\n        global _INITIALIZED\n        if not _INITIALIZED:\n            colorama.init()\n            _INITIALIZED = True\n    with_color, without_color = process_tokens(tokens, end=end, sep=sep)\n    if CONFIG[\"record\"]:\n        _MESSAGES.append(without_color)\n    if update_title and with_color:\n        write_title_string(without_color, fileobj)\n    to_write = with_color if config_color(fileobj) else without_color\n    write_and_flush(fileobj, to_write)", "response": "Write a message to the fileobj."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints an error message and call sys. exit.", "response": "def fatal(*tokens: Token, **kwargs: Any) -> None:\n    \"\"\" Print an error message and call ``sys.exit`` \"\"\"\n    error(*tokens, **kwargs)\n    sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef error(*tokens: Token, **kwargs: Any) -> None:\n    tokens = [bold, red, \"Error:\"] + list(tokens)  # type: ignore\n    kwargs[\"fileobj\"] = sys.stderr\n    message(*tokens, **kwargs)", "response": "Print an error message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef warning(*tokens: Token, **kwargs: Any) -> None:\n    tokens = [brown, \"Warning:\"] + list(tokens)  # type: ignore\n    kwargs[\"fileobj\"] = sys.stderr\n    message(*tokens, **kwargs)", "response": "Print a warning message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef info_section(*tokens: Token, **kwargs: Any) -> None:\n    # We need to know the length of the section:\n    process_tokens_kwargs = kwargs.copy()\n    process_tokens_kwargs[\"color\"] = False\n    no_color = _process_tokens(tokens, **process_tokens_kwargs)\n    info(*tokens, **kwargs)\n    info(\"-\" * len(no_color), end=\"\\n\\n\")", "response": "Print an underlined section name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting an important informative message", "response": "def info_1(*tokens: Token, **kwargs: Any) -> None:\n    \"\"\" Print an important informative message \"\"\"\n    info(bold, blue, \"::\", reset, *tokens, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dot(*, last: bool = False, fileobj: Any = None) -> None:\n    end = \"\\n\" if last else \"\"\n    info(\".\", end=end, fileobj=fileobj)", "response": "Print a dot without a newline unless it is the last one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef info_count(i: int, n: int, *rest: Token, **kwargs: Any) -> None:\n    num_digits = len(str(n))\n    counter_format = \"(%{}d/%d)\".format(num_digits)\n    counter_str = counter_format % (i + 1, n)\n    info(green, \"*\", reset, counter_str, reset, *rest, **kwargs)", "response": "Display a counter before the rest of the message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndisplays info progress in percent.", "response": "def info_progress(prefix: str, value: float, max_value: float) -> None:\n    \"\"\" Display info progress in percent.\n\n    :param value: the current value\n    :param max_value: the max value\n    :param prefix: the prefix message to print\n\n\n    \"\"\"\n    if sys.stdout.isatty():\n        percent = float(value) / max_value * 100\n        sys.stdout.write(prefix + \": %.0f%%\\r\" % percent)\n        sys.stdout.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprints a debug message.", "response": "def debug(*tokens: Token, **kwargs: Any) -> None:\n    \"\"\" Print a debug message.\n\n    Messages are shown only when ``CONFIG[\"verbose\"]`` is true\n    \"\"\"\n    if not CONFIG[\"verbose\"] or CONFIG[\"record\"]:\n        return\n    message(*tokens, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nindent a piece of text.", "response": "def indent(text: str, num: int = 2) -> str:\n    \"\"\"Indent a piece of text.\"\"\"\n    lines = text.splitlines()\n    return \"\\n\".join(indent_iterable(lines, num=num))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef message_for_exception(exception: Exception, message: str) -> Sequence[Token]:\n    tb = sys.exc_info()[2]\n    buffer = io.StringIO()\n    traceback.print_tb(tb, file=io)  # type: ignore\n    # fmt: off\n    return (\n        red, message + \"\\n\",\n        exception.__class__.__name__,\n        str(exception), \"\\n\",\n        reset, buffer.getvalue()\n    )", "response": "Returns a tuple suitable for cli_ui. error"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ask_string(*question: Token, default: Optional[str] = None) -> Optional[str]:\n    tokens = get_ask_tokens(question)\n    if default:\n        tokens.append(\"(%s)\" % default)\n    info(*tokens)\n    answer = read_input()\n    if not answer:\n        return default\n    return answer", "response": "Ask the user to enter a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nasks the user to enter a password.", "response": "def ask_password(*question: Token) -> str:\n    \"\"\"Ask the user to enter a password.\n    \"\"\"\n    tokens = get_ask_tokens(question)\n    info(*tokens)\n    answer = read_password()\n    return answer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking the user to choose from a list of choices.", "response": "def ask_choice(\n    *prompt: Token, choices: List[Any], func_desc: Optional[FuncDesc] = None\n) -> Any:\n    \"\"\"Ask the user to choose from a list of choices.\n\n    :return: the selected choice\n\n    ``func_desc`` will be called on every list item for displaying\n    and sorting the list. If not given, will default to\n    the identity function.\n\n    Will loop until:\n        * the user enters a valid index\n        * or leaves the prompt empty\n\n    In the last case, None will be returned\n    \"\"\"\n    if func_desc is None:\n        func_desc = lambda x: str(x)\n    tokens = get_ask_tokens(prompt)\n    info(*tokens)\n    choices.sort(key=func_desc)\n    for i, choice in enumerate(choices, start=1):\n        choice_desc = func_desc(choice)\n        info(\"  \", blue, \"%i\" % i, reset, choice_desc)\n    keep_asking = True\n    res = None\n    while keep_asking:\n        answer = read_input()\n        if not answer:\n            return None\n        try:\n            index = int(answer)\n        except ValueError:\n            info(\"Please enter a valid number\")\n            continue\n        if index not in range(1, len(choices) + 1):\n            info(str(index), \"is out of range\")\n            continue\n        res = choices[index - 1]\n        keep_asking = False\n\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nasking the user to answer by yes or no.", "response": "def ask_yes_no(*question: Token, default: bool = False) -> bool:\n    \"\"\"Ask the user to answer by yes or no\"\"\"\n    while True:\n        tokens = [green, \"::\", reset] + list(question) + [reset]\n        if default:\n            tokens.append(\"(Y/n)\")\n        else:\n            tokens.append(\"(y/N)\")\n        info(*tokens)\n        answer = read_input()\n        if answer.lower() in [\"y\", \"yes\"]:\n            return True\n        if answer.lower() in [\"n\", \"no\"]:\n            return False\n        if not answer:\n            return default\n        warning(\"Please answer by 'y' (yes) or 'n' (no) \")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a list of choices and an invalid user input display the closest available item in the list that match the input.", "response": "def did_you_mean(message: str, user_input: str, choices: Sequence[str]) -> str:\n    \"\"\" Given a list of choices and an invalid user input, display the closest\n    items in the list that match the input.\n\n    \"\"\"\n    if not choices:\n        return message\n    else:\n        result = {\n            difflib.SequenceMatcher(a=user_input, b=choice).ratio(): choice\n            for choice in choices\n        }\n        message += \"\\nDid you mean: %s?\" % result[max(result)]\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stop(self) -> None:\n        end_time = datetime.datetime.now()\n        elapsed_time = end_time - self.start_time\n        elapsed_seconds = elapsed_time.seconds\n        hours, remainder = divmod(int(elapsed_seconds), 3600)\n        minutes, seconds = divmod(remainder, 60)\n        as_str = \"%sh %sm %ss %dms\" % (\n            hours,\n            minutes,\n            seconds,\n            elapsed_time.microseconds / 1000,\n        )\n        info(\"%s took %s\" % (self.description, as_str))", "response": "Stop the timer and emit a nice log"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading a file from the workspace to a local file.", "response": "def download_to_directory(self, directory, url, basename=None, overwrite=False, subdir=None):\n        \"\"\"\n        Download a file to the workspace.\n\n        Early Shortcut: If url is a file://-URL and that file is already in the directory, keep it there.\n\n        If basename is not given but subdir is, assume user knows what she's doing and use last URL segment as the basename.\n        If basename is not given and no subdir is given, use the alnum characters in the URL as the basename.\n\n        Args:\n            directory (string): Directory to download files to\n            basename (string, None): basename part of the filename on disk.\n            url (string): URL to download from\n            overwrite (boolean): Whether to overwrite existing files with that name\n            subdir (string, None): Subdirectory to create within the directory. Think fileGrp.\n\n        Returns:\n            Local filename\n        \"\"\"\n        log = getLogger('ocrd.resolver.download_to_directory') # pylint: disable=redefined-outer-name\n        log.debug(\"directory=|%s| url=|%s| basename=|%s| overwrite=|%s| subdir=|%s|\", directory, url, basename, overwrite, subdir)\n\n        if url is None:\n            raise Exception(\"'url' must be a string\")\n        if directory is None:\n            raise Exception(\"'directory' must be a string\")\n\n        if basename is None:\n            if (subdir is not None) or \\\n                (directory and url.startswith('file://%s' % directory)): # in case downloading a url 'file:///tmp/foo/bar' to directory '/tmp/foo'\n                basename = url.rsplit('/', 1)[-1]\n            else:\n                basename = safe_filename(url)\n\n        if subdir is not None:\n            basename = join(subdir, basename)\n\n        outfilename = join(directory, basename)\n\n        if exists(outfilename) and not overwrite:\n            log.debug(\"File already exists and overwrite=False: %s\", outfilename)\n            return outfilename\n\n        outfiledir = outfilename.rsplit('/', 1)[0]\n        #  print(outfiledir)\n        if not isdir(outfiledir):\n            makedirs(outfiledir)\n\n        log.debug(\"Downloading <%s> to '%s'\", url, outfilename)\n\n        # de-scheme file:// URL\n        if url.startswith('file://'):\n            url = url[len('file://'):]\n\n        # Copy files or download remote assets\n        if '://' not in url:\n            copyfile(url, outfilename)\n        else:\n            response = requests.get(url)\n            if response.status_code != 200:\n                raise Exception(\"Not found: %s (HTTP %d)\" % (url, response.status_code))\n            with open(outfilename, 'wb') as outfile:\n                outfile.write(response.content)\n\n        return outfilename"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a workspace from a METS file.", "response": "def workspace_from_url(self, mets_url, dst_dir=None, clobber_mets=False, mets_basename=None, download=False, baseurl=None):\n        \"\"\"\n        Create a workspace from a METS by URL.\n\n        Sets the mets.xml file\n\n        Arguments:\n            mets_url (string): Source mets URL\n            dst_dir (string, None): Target directory for the workspace\n            clobber_mets (boolean, False): Whether to overwrite existing mets.xml. By default existing mets.xml will raise an exception.\n            download (boolean, False): Whether to download all the files\n            baseurl (string, None): Base URL for resolving relative file locations\n\n        Returns:\n            Workspace\n        \"\"\"\n        if dst_dir and not dst_dir.startswith('/'):\n            dst_dir = abspath(dst_dir)\n\n        if mets_url is None:\n            if baseurl is None:\n                raise Exception(\"Must pass mets_url and/or baseurl to workspace_from_url\")\n            else:\n                mets_url = 'file://%s/%s' % (baseurl, mets_basename if mets_basename else 'mets.xml')\n        if baseurl is None:\n            baseurl = mets_url.rsplit('/', 1)[0]\n        log.debug(\"workspace_from_url\\nmets_url='%s'\\nbaseurl='%s'\\ndst_dir='%s'\", mets_url, baseurl, dst_dir)\n\n        # resolve to absolute\n        if '://' not in mets_url:\n            mets_url = 'file://%s' % abspath(mets_url)\n\n        if dst_dir is None:\n            # if mets_url is a file-url assume working directory is source directory\n            if mets_url.startswith('file://'):\n                # if dst_dir was not given and mets_url is a file assume that\n                # dst_dir should be the directory where the mets.xml resides\n                dst_dir = dirname(mets_url[len('file://'):])\n            else:\n                dst_dir = tempfile.mkdtemp(prefix=TMP_PREFIX)\n                log.debug(\"Creating workspace '%s' for METS @ <%s>\", dst_dir, mets_url)\n\n        # if mets_basename is not given, use the last URL segment of the mets_url\n        if mets_basename is None:\n            mets_basename = mets_url \\\n                .rsplit('/', 1)[-1] \\\n                .split('?')[0] \\\n                .split('#')[0]\n\n        dst_mets = join(dst_dir, mets_basename)\n        log.debug(\"Copying mets url '%s' to '%s'\", mets_url, dst_mets)\n        if 'file://' + dst_mets == mets_url:\n            log.debug(\"Target and source mets are identical\")\n        else:\n            if exists(dst_mets) and not clobber_mets:\n                raise Exception(\"File '%s' already exists but clobber_mets is false\" % dst_mets)\n            else:\n                self.download_to_directory(dst_dir, mets_url, basename=mets_basename)\n\n        workspace = Workspace(self, dst_dir, mets_basename=mets_basename, baseurl=baseurl)\n\n        if download:\n            for f in workspace.mets.find_files():\n                workspace.download_file(f)\n\n        return workspace"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an empty workspace.", "response": "def workspace_from_nothing(self, directory, mets_basename='mets.xml', clobber_mets=False):\n        \"\"\"\n        Create an empty workspace.\n        \"\"\"\n        if directory is None:\n            directory = tempfile.mkdtemp(prefix=TMP_PREFIX)\n        if not exists(directory):\n            makedirs(directory)\n\n        mets_fpath = join(directory, mets_basename)\n        if not clobber_mets and exists(mets_fpath):\n            raise Exception(\"Not clobbering existing mets.xml in '%s'.\" % directory)\n        mets = OcrdMets.empty_mets()\n        with open(mets_fpath, 'wb') as fmets:\n            log.info(\"Writing %s\", mets_fpath)\n            fmets.write(mets.to_xml(xmllint=True))\n\n        return Workspace(self, directory, mets)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nserialize all properties as XML", "response": "def to_xml(self, xmllint=False):\n        \"\"\"\n        Serialize all properties as pretty-printed XML\n\n        Args:\n            xmllint (boolean): Format with ``xmllint`` in addition to pretty-printing\n        \"\"\"\n        root = self._tree.getroot()\n        ret = ET.tostring(ET.ElementTree(root), pretty_print=True)\n        if xmllint:\n            ret = xmllint_format(ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a workspace from a METS_URL and return the directory containing the METS_URL.", "response": "def workspace_clone(ctx, clobber_mets, download, mets_url, workspace_dir):\n    \"\"\"\n    Create a workspace from a METS_URL and return the directory\n\n    METS_URL can be a URL, an absolute path or a path relative to $PWD.\n\n    If WORKSPACE_DIR is not provided, creates a temporary directory.\n    \"\"\"\n    workspace = ctx.resolver.workspace_from_url(\n        mets_url,\n        dst_dir=os.path.abspath(workspace_dir if workspace_dir else mkdtemp(prefix=TMP_PREFIX)),\n        mets_basename=ctx.mets_basename,\n        clobber_mets=clobber_mets,\n        download=download,\n    )\n    workspace.save_mets()\n    print(workspace.directory)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a workspace with an empty METS file in DIRECTORY.", "response": "def workspace_create(ctx, clobber_mets, directory):\n    \"\"\"\n    Create a workspace with an empty METS file in DIRECTORY.\n\n    Use '.' for $PWD\"\n    \"\"\"\n    workspace = ctx.resolver.workspace_from_nothing(\n        directory=os.path.abspath(directory),\n        mets_basename=ctx.mets_basename,\n        clobber_mets=clobber_mets\n    )\n    workspace.save_mets()\n    print(workspace.directory)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a file to METS in a workspace.", "response": "def workspace_add_file(ctx, file_grp, file_id, mimetype, page_id, force, local_filename):\n    \"\"\"\n    Add a file LOCAL_FILENAME to METS in a workspace.\n    \"\"\"\n    workspace = Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup)\n\n    if not local_filename.startswith(ctx.directory):\n        log.debug(\"File '%s' is not in workspace, copying\", local_filename)\n        local_filename = ctx.resolver.download_to_directory(ctx.directory, \"file://\" + local_filename, subdir=file_grp)\n\n    url = \"file://\" + local_filename\n\n    workspace.mets.add_file(\n        fileGrp=file_grp,\n        ID=file_id,\n        mimetype=mimetype,\n        url=url,\n        pageId=page_id,\n        force=force,\n        local_filename=local_filename\n    )\n    workspace.save_mets()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef workspace_find(ctx, file_grp, mimetype, page_id, file_id, output_field, download):\n    workspace = Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename)\n    for f in workspace.mets.find_files(\n            ID=file_id,\n            fileGrp=file_grp,\n            mimetype=mimetype,\n            pageId=page_id,\n        ):\n        if download:\n            workspace.download_file(f)\n            workspace.save_mets()\n        ret = '\\t'.join([getattr(f, field) or '' for field in output_field])\n        print(ret)", "response": "Find files in a workspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new backup", "response": "def workspace_backup_add(ctx):\n    \"\"\"\n    Create a new backup\n    \"\"\"\n    backup_manager = WorkspaceBackupManager(Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup))\n    backup_manager.add()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef workspace_backup_list(ctx):\n    backup_manager = WorkspaceBackupManager(Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup))\n    for b in backup_manager.list():\n        print(b)", "response": "List backups in workspace"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef workspace_backup_restore(ctx, choose_first, bak):\n    backup_manager = WorkspaceBackupManager(Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup))\n    backup_manager.restore(bak, choose_first)", "response": "Restore a backup of a workspace."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrestore the last backup", "response": "def workspace_backup_undo(ctx):\n    \"\"\"\n    Restore the last backup\n    \"\"\"\n    backup_manager = WorkspaceBackupManager(Workspace(ctx.resolver, directory=ctx.directory, mets_basename=ctx.mets_basename, automatic_backup=ctx.automatic_backup))\n    backup_manager.undo()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextends a jsonschema validation class with a default - setting mechanism.", "response": "def extend_with_default(validator_class):\n    \"\"\"\n    Add a default-setting mechanism to a ``jsonschema`` validation class.\n    \"\"\"\n    validate_properties = validator_class.VALIDATORS[\"properties\"]\n\n    def set_defaults(validator, properties, instance, schema):\n        \"\"\"\n        Set defaults in subschemas\n        \"\"\"\n        for prop, subschema in properties.items():\n            if \"default\" in subschema:\n                instance.setdefault(prop, subschema[\"default\"])\n\n        for error in validate_properties(validator, properties, instance, schema):\n            yield error\n\n    return validators.extend(validator_class, {\"properties\": set_defaults})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates an object against a schema", "response": "def validate(obj, schema):\n        \"\"\"\n        Validate an object against a schema\n\n        Args:\n            obj (dict):\n            schema (dict):\n        \"\"\"\n        if isinstance(obj, str):\n            obj = json.loads(obj)\n        return JsonValidator(schema)._validate(obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _validate(self, obj):\n        report = ValidationReport()\n        if not self.validator.is_valid(obj):\n            for v in self.validator.iter_errors(obj):\n                report.add_error(\"[%s] %s\" % ('.'.join(str(vv) for vv in v.path), v.message))\n        return report", "response": "Validate the object and return a ValidationReport object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_processor(\n        processorClass,\n        ocrd_tool=None,\n        mets_url=None,\n        resolver=None,\n        workspace=None,\n        page_id=None,\n        log_level=None,\n        input_file_grp=None,\n        output_file_grp=None,\n        parameter=None,\n        working_dir=None,\n): # pylint: disable=too-many-locals\n    \"\"\"\n    Create a workspace for mets_url and run processor through it\n\n    Args:\n        parameter (string): URL to the parameter\n    \"\"\"\n    workspace = _get_workspace(\n        workspace,\n        resolver,\n        mets_url,\n        working_dir\n    )\n    if parameter is not None:\n        if not '://' in parameter:\n            fname = os.path.abspath(parameter)\n        else:\n            fname = workspace.download_url(parameter)\n        with open(fname, 'r') as param_json_file:\n            parameter = json.load(param_json_file)\n    else:\n        parameter = {}\n    log.debug(\"Running processor %s\", processorClass)\n    processor = processorClass(\n        workspace,\n        ocrd_tool=ocrd_tool,\n        page_id=page_id,\n        input_file_grp=input_file_grp,\n        output_file_grp=output_file_grp,\n        parameter=parameter\n    )\n    ocrd_tool = processor.ocrd_tool\n    name = '%s v%s' % (ocrd_tool['executable'], processor.version)\n    otherrole = ocrd_tool['steps'][0]\n    log.debug(\"Processor instance %s (%s doing %s)\", processor, name, otherrole)\n    processor.process()\n    workspace.mets.add_agent(\n        name=name,\n        _type='OTHER',\n        othertype='SOFTWARE',\n        role='OTHER',\n        otherrole=otherrole\n    )\n    workspace.save_mets()\n    return processor", "response": "Runs a processor through the mets_url and saves the result in a workspace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun MP CLI through it", "response": "def run_cli(\n        executable,\n        mets_url=None,\n        resolver=None,\n        workspace=None,\n        page_id=None,\n        log_level=None,\n        input_file_grp=None,\n        output_file_grp=None,\n        parameter=None,\n        working_dir=None,\n):\n    \"\"\"\n    Create a workspace for mets_url and run MP CLI through it\n    \"\"\"\n    workspace = _get_workspace(workspace, resolver, mets_url, working_dir)\n    args = [executable, '--working-dir', workspace.directory]\n    args += ['--mets', mets_url]\n    if log_level:\n        args += ['--log-level', log_level]\n    if page_id:\n        args += ['--page-id', page_id]\n    if input_file_grp:\n        args += ['--input-file-grp', input_file_grp]\n    if output_file_grp:\n        args += ['--output-file-grp', output_file_grp]\n    if parameter:\n        args += ['--parameter', parameter]\n    log.debug(\"Running subprocess '%s'\", ' '.join(args))\n    return subprocess.call(args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef input_files(self):\n        return self.workspace.mets.find_files(fileGrp=self.input_file_grp, pageId=self.page_id)", "response": "List the input files in the workspace"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a PcGtsType object representing a single image.", "response": "def page_from_image(input_file):\n    \"\"\"\n    Create `OcrdPage </../../ocrd_models/ocrd_models.ocrd_page.html>`_\n    from an `OcrdFile </../../ocrd_models/ocrd_models.ocrd_file.html>`_\n    representing an image (i.e. should have ``mimetype`` starting with ``image/``).\n\n    Arguments:\n        * input_file (OcrdFile):\n    \"\"\"\n    if input_file.local_filename is None:\n        raise Exception(\"input_file must have 'local_filename' property\")\n    exif = exif_from_filename(input_file.local_filename)\n    now = datetime.now()\n    return PcGtsType(\n        Metadata=MetadataType(\n            Creator=\"OCR-D/core %s\" % VERSION,\n            Created=now,\n            LastChange=now\n        ),\n        Page=PageType(\n            imageWidth=exif.width,\n            imageHeight=exif.height,\n            # XXX brittle\n            imageFilename=input_file.url if input_file.url is not None else 'file://' + input_file.local_filename\n        )\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef page_from_file(input_file):\n    #  print(\"PARSING PARSING '%s'\" % input_file)\n    if input_file.mimetype.startswith('image'):\n        return page_from_image(input_file)\n    if input_file.mimetype == MIMETYPE_PAGE:\n        return parse(input_file.local_filename, silence=True)\n    raise Exception(\"Unsupported mimetype '%s'\" % input_file.mimetype)", "response": "Create a new PAGE - XML from a METS file representing a PAGE - XML or an image."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef concat_padded(base, *args):\n    ret = base\n    for n in args:\n        if is_string(n):\n            ret = \"%s_%s\" % (ret, n)\n        else:\n            ret = \"%s_%04i\"  % (ret, n + 1)\n    return ret", "response": "Concatenate string and zero - padded 4 digit number of a base element."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a polygon representation from a rectangle described as a dict with keys x y w h.", "response": "def points_from_xywh(box):\n    \"\"\"\n    Constructs a polygon representation from a rectangle described as a dict with keys x, y, w, h.\n    \"\"\"\n    x, y, w, h = box['x'], box['y'], box['w'], box['h']\n    # tesseract uses a different region representation format\n    return \"%i,%i %i,%i %i,%i %i,%i\" % (\n        x, y,\n        x + w, y,\n        x + w, y + h,\n        x, y + h\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct a polygon representation from a list of y0 x0 y1 x1 coordinates.", "response": "def points_from_y0x0y1x1(yxyx):\n    \"\"\"\n    Constructs a polygon representation from a rectangle described as a list [y0, x0, y1, x1]\n    \"\"\"\n    y0 = yxyx[0]\n    x0 = yxyx[1]\n    y1 = yxyx[2]\n    x1 = yxyx[3]\n    return \"%s,%s %s,%s %s,%s %s,%s\" % (\n        x0, y0,\n        x1, y0,\n        x1, y1,\n        x0, y1\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a polygon representation from a list of x0 y0 x1 y1 coordinates.", "response": "def points_from_x0y0x1y1(xyxy):\n    \"\"\"\n    Constructs a polygon representation from a rectangle described as a list [x0, y0, x1, y1]\n    \"\"\"\n    x0 = xyxy[0]\n    y0 = xyxy[1]\n    x1 = xyxy[2]\n    y1 = xyxy[3]\n    return \"%s,%s %s,%s %s,%s %s,%s\" % (\n        x0, y0,\n        x1, y0,\n        x1, y1,\n        x0, y1\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef polygon_from_points(points):\n    polygon = []\n    for pair in points.split(\" \"):\n        x_y = pair.split(\",\")\n        polygon.append([float(x_y[0]), float(x_y[1])])\n    return polygon", "response": "Constructs a numpy - compatible polygon from a page representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nunzips a ZIP file to a directory", "response": "def unzip_file_to_dir(path_to_zip, output_directory):\n    \"\"\"\n    Extract a ZIP archive to a directory\n    \"\"\"\n    z = ZipFile(path_to_zip, 'r')\n    z.extractall(output_directory)\n    z.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconstructs a dictionary representing a rectangle with keys x y w h where the keys x y w h are the number of points in the image.", "response": "def xywh_from_points(points):\n    \"\"\"\n    Constructs an dict representing a rectangle with keys x, y, w, h\n    \"\"\"\n    xys = [[int(p) for p in pair.split(',')] for pair in points.split(' ')]\n    minx = sys.maxsize\n    miny = sys.maxsize\n    maxx = 0\n    maxy = 0\n    for xy in xys:\n        if xy[0] < minx:\n            minx = xy[0]\n        if xy[0] > maxx:\n            maxx = xy[0]\n        if xy[1] < miny:\n            miny = xy[1]\n        if xy[1] > maxy:\n            maxy = xy[1]\n\n    return {\n        'x': minx,\n        'y': miny,\n        'w': maxx - minx,\n        'h': maxy - miny,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_profile(self, bag):\n        if not self.profile_validator.validate(bag):\n            raise Exception(str(self.profile_validator.report))", "response": "Validate that the bag - info fields are valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_bag(self, bag, **kwargs):\n        failed = None\n        try:\n            bag.validate(**kwargs)\n        except BagValidationError as e:\n            failed = e\n            #  for d in e.details:\n            #      if isinstance(d, ChecksumMismatch):\n            #          log.error(\"Validation Error: expected %s to have %s checksum of %s but found %s\", d.path, d.algorithm, d.expected, d.found)\n            #      else:\n            #          log.error(\"Validation Error: %s\", d)\n        if failed:\n            raise BagValidationError(\"%s\" % failed)", "response": "Validate BagIt (checksums, payload.oxum etc)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating an OCRD - ZIP file for profile bag and workspace.", "response": "def validate(self, skip_checksums=False, skip_bag=False, skip_unzip=False, skip_delete=False, processes=2):\n        \"\"\"\n        Validate an OCRD-ZIP file for profile, bag and workspace conformance\n\n        Arguments:\n            skip_bag (boolean): Whether to skip all checks of manifests and files\n            skip_checksums (boolean): Whether to omit checksum checks but still check basic BagIt conformance\n            skip_unzip (boolean): Whether the OCRD-ZIP is unzipped, i.e. a directory\n            skip_delete (boolean): Whether to skip deleting the unpacked OCRD-ZIP dir after valdiation\n            processes (integer): Number of processes used for checksum validation\n\n        \"\"\"\n        if skip_unzip:\n            bagdir = self.path_to_zip\n            skip_delete = True\n        else:\n            #  try:\n            self.profile_validator.validate_serialization(self.path_to_zip)\n            #  except IOError as err:\n            #      raise err\n            #  except ProfileValidationError as err:\n            #      self.report.add_error(err.value)\n            bagdir = mkdtemp(prefix=TMP_BAGIT_PREFIX)\n            unzip_file_to_dir(self.path_to_zip, bagdir)\n\n\n        try:\n            bag = Bag(bagdir)\n            self._validate_profile(bag)\n\n            if not skip_bag:\n                self._validate_bag(bag, fast=skip_checksums, processes=processes)\n\n        finally:\n            if not skip_delete:\n                # remove tempdir\n                rmtree(bagdir)\n        return self.report"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef quote_xml(inStr):\n    \"Escape markup chars, but do not modify CDATA sections.\"\n    if not inStr:\n        return ''\n    s1 = (isinstance(inStr, BaseStrType_) and inStr or '%s' % inStr)\n    s2 = ''\n    pos = 0\n    matchobjects = CDATA_pattern_.finditer(s1)\n    for mo in matchobjects:\n        s3 = s1[pos:mo.start()]\n        s2 += quote_xml_aux(s3)\n        s2 += s1[mo.start():mo.end()]\n        pos = mo.end()\n    s3 = s1[pos:]\n    s2 += quote_xml_aux(s3)\n    return s2", "response": "Escape markup chars but do not modify CDATA sections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parseString(inString, silence=False):\n    '''Parse a string, create the object tree, and export it.\n\n    Arguments:\n    - inString -- A string.  This XML fragment should not start\n      with an XML declaration containing an encoding.\n    - silence -- A boolean.  If False, export the object.\n    Returns -- The root object in the tree.\n    '''\n    parser = None\n    rootNode= parsexmlstring_(inString, parser)\n    rootTag, rootClass = get_root_tag(rootNode)\n    if rootClass is None:\n        rootTag = 'PcGts'\n        rootClass = PcGts\n    rootObj = rootClass.factory()\n    rootObj.build(rootNode)\n    # Enable Python to collect the space used by the DOM.\n    if not silence:\n        sys.stdout.write('<?xml version=\"1.0\" ?>\\n')\n        rootObj.export(\n            sys.stdout, 0, name_=rootTag,\n            namespacedef_='xmlns:pc=\"http://schema.primaresearch.org/PAGE/gts/pagecontent/2018-07-15\"')\n    return rootObj", "response": "Parse a string and create the object tree and export it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse parameters with fallback to defaults and output as shell - evalable assignments to params var.", "response": "def ocrd_tool_tool_parse_params(ctx, parameters, json):\n    \"\"\"\n    Parse parameters with fallback to defaults and output as shell-eval'able assignments to params var.\n    \"\"\"\n    if parameters is None or parameters == \"\":\n        parameters = {}\n    else:\n        with open(parameters, 'r') as f:\n            parameters = loads(f.read())\n    parameterValidator = ParameterValidator(ctx.json['tools'][ctx.tool_name])\n    report = parameterValidator.validate(parameters)\n    if not report.is_valid:\n        print(report.to_xml())\n        sys.exit(1)\n    if json:\n        print(dumps(parameters))\n    else:\n        for k in parameters:\n            print('params[\"%s\"]=\"%s\"' % (k, parameters[k]))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the OTHERTYPE attribute value.", "response": "def othertype(self, othertype):\n        \"\"\"\n        Set the ``OTHERTYPE`` attribute value.\n        \"\"\"\n        if othertype is not None:\n            self._el.set('TYPE', 'OTHER')\n            self._el.set('OTHERTYPE', othertype)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef otherrole(self, otherrole):\n        if otherrole is not None:\n            self._el.set('ROLE', 'OTHER')\n            self._el.set('OTHERROLE', otherrole)", "response": "Sets the OTHERROLE attribute value."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the name of the user s company.", "response": "def name(self):\n        \"\"\"\n        Get the ``mets:name`` element value.\n        \"\"\"\n        el_name = self._el.find('mets:name', NS)\n        if el_name is not None:\n            return el_name.text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef name(self, name):\n        if name is not None:\n            el_name = self._el.find('mets:name', NS)\n            if el_name is None:\n                el_name = ET.SubElement(self._el, TAG_METS_NAME)\n            el_name.text = name", "response": "Sets the name of the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a parameter dict against a parameter schema from an ocrd - tool. json", "response": "def validate(self, *args, **kwargs): # pylint: disable=arguments-differ\n        \"\"\"\n        Validate a parameter dict against a parameter schema from an ocrd-tool.json\n\n        Args:\n            obj (dict):\n            schema (dict):\n        \"\"\"\n        return super(ParameterValidator, self)._validate(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether the text results on an element is consistent with its child element text results.", "response": "def handle_inconsistencies(node, strictness, strategy, report):\n    \"\"\"\n    Check whether the text results on an element is consistent with its child element text results.\n    \"\"\"\n    if isinstance(node, PcGtsType):\n        node = node.get_Page()\n    elif isinstance(node, GlyphType):\n        return report\n\n    _, tag, getter, concatenate_with = [x for x in _HIERARCHY if isinstance(node, x[0])][0]\n    children_are_consistent = True\n    children = getattr(node, getter)()\n    for child in children:\n        errors_before = len(report.errors)\n        handle_inconsistencies(child, strictness, strategy, report)\n        if len(report.errors) > errors_before:\n            children_are_consistent = False\n    if concatenate_with is not None:\n        concatenated_children = concatenate_children(node, concatenate_with, strategy)\n        text_results = get_text(node, strategy)\n        if concatenated_children and text_results and concatenated_children != text_results:\n            if strictness == 'fix':\n                set_text(node, concatenated_children, strategy)\n                #  if children_are_consistent:\n                #  else:\n                #      # TODO fix text results recursively\n                #      report.add_warning(\"Fixing inconsistencies recursively not implemented\")\n            elif strictness == 'lax':\n                if not compare_without_whitespace(concatenated_children, text_results):\n                    report.add_error(ConsistencyError(tag, node.id, text_results, concatenated_children))\n            else:\n                report.add_error(ConsistencyError(tag, node.id, text_results, concatenated_children))\n    return report"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef concatenate_children(node, concatenate_with, strategy):\n    _, _, getter, concatenate_with = [x for x in _HIERARCHY if isinstance(node, x[0])][0]\n    tokens = [get_text(x, strategy) for x in getattr(node, getter)()]\n    return concatenate_with.join(tokens).strip()", "response": "Concatenate children of node according to https://ocr - d. github. io. page#consistency - of - text - results - on - different - levels\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_text(node, strategy):\n    textEquivs = node.get_TextEquiv()\n    if not textEquivs:\n        log.debug(\"No text results on %s %s\", node, node.id)\n        return ''\n    #  elif strategy == 'index1':\n    else:\n        if len(textEquivs) > 1:\n            index1 = [x for x in textEquivs if x.index == 1]\n            if index1:\n                return index1[0].get_Unicode().strip()\n        return textEquivs[0].get_Unicode().strip()", "response": "Get the most confident text results for a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_text(node, text, strategy):\n    text = text.strip()\n    textEquivs = node.get_TextEquiv()\n    if not textEquivs:\n        node.add_TextEquiv(TextEquivType(Unicode=text))\n    #  elif strategy == 'index1':\n    else:\n        if len(textEquivs) > 1:\n            index1 = [x for x in textEquivs if x.index == 1]\n            if index1:\n                index1[0].set_Unicode(text)\n                return\n        textEquivs[0].set_Unicode(text)", "response": "Set the most confident text results for a node."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating a PAGE file for consistency by filename OcrdPage or OcrdFile directly.", "response": "def validate(filename=None, ocrd_page=None, ocrd_file=None, strictness='strict', strategy='index1'):\n        \"\"\"\n        Validates a PAGE file for consistency by filename, OcrdFile or passing OcrdPage directly.\n\n        Arguments:\n            filename (string): Path to PAGE\n            ocrd_page (OcrdPage): OcrdPage instance\n            ocrd_file (OcrdFile): OcrdFile instance wrapping OcrdPage\n            strictness (string): 'strict', 'lax', 'fix' or 'off'\n            strategy (string): Currently only 'index1'\n\n        Returns:\n            report (:class:`ValidationReport`) Report on the validity\n        \"\"\"\n        if ocrd_page:\n            validator = PageValidator(ocrd_page, strictness, strategy)\n        elif ocrd_file:\n            validator = PageValidator(page_from_file(ocrd_file), strictness, strategy)\n        elif filename:\n            validator = PageValidator(parse(filename, silence=True), strictness, strategy)\n        else:\n            raise Exception(\"At least one of ocrd_page, ocrd_file or filename must be set\")\n        return validator._validate()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates the current page and report.", "response": "def _validate(self):\n        \"\"\"\n        Do the actual validation\n        \"\"\"\n        if self.strictness == 'off':\n            return self.report\n        handle_inconsistencies(self.page, self.strictness, self.strategy, self.report)\n        return self.report"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nimplements MP CLI. Usage:: import ocrd_click_cli from ocrd.utils @click.command() @ocrd_click_cli def cli(mets_url): print(mets_url)", "response": "def ocrd_cli_options(f):\n    \"\"\"\n    Implement MP CLI.\n\n    Usage::\n\n        import ocrd_click_cli from ocrd.utils\n\n        @click.command()\n        @ocrd_click_cli\n        def cli(mets_url):\n            print(mets_url)\n    \"\"\"\n    params = [\n        click.option('-m', '--mets', help=\"METS URL to validate\"),\n        click.option('-w', '--working-dir', help=\"Working Directory\"),\n        click.option('-I', '--input-file-grp', help='File group(s) used as input.', default='INPUT'),\n        click.option('-O', '--output-file-grp', help='File group(s) used as output.', default='OUTPUT'),\n        click.option('-g', '--page-id', help=\"ID(s) of the pages to process\"),\n        click.option('-p', '--parameter', type=click.Path()),\n        click.option('-J', '--dump-json', help=\"Dump tool description as JSON and exit\", is_flag=True, default=False),\n        loglevel_option,\n        click.option('-V', '--version', help=\"Show version\", is_flag=True, default=False)\n    ]\n    for param in params:\n        param(f)\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges another report into this one.", "response": "def merge_report(self, otherself):\n        \"\"\"\n        Merge another report into this one.\n        \"\"\"\n        self.notices += otherself.notices\n        self.warnings += otherself.warnings\n        self.errors += otherself.errors"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses a series of tasks in the cli", "response": "def process_cli(log_level, mets, page_id, tasks):\n    \"\"\"\n    Process a series of tasks\n    \"\"\"\n    log = getLogger('ocrd.cli.process')\n\n    run_tasks(mets, log_level, page_id, tasks)\n    log.info(\"Finished\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbag a new NCSC - ZIP workspace at DEST.", "response": "def bag(directory, mets_basename, dest, identifier, in_place, manifestation_depth, mets, base_version_checksum, tag_file, skip_zip, processes):\n    \"\"\"\n    Bag workspace as OCRD-ZIP at DEST\n    \"\"\"\n    resolver = Resolver()\n    workspace = Workspace(resolver, directory=directory, mets_basename=mets_basename)\n    workspace_bagger = WorkspaceBagger(resolver)\n    workspace_bagger.bag(\n        workspace,\n        dest=dest,\n        ocrd_identifier=identifier,\n        ocrd_manifestation_depth=manifestation_depth,\n        ocrd_mets=mets,\n        ocrd_base_version_checksum=base_version_checksum,\n        processes=processes,\n        tag_files=tag_file,\n        skip_zip=skip_zip,\n        in_place=in_place\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nspilling a single file into a single file in a directory.", "response": "def spill(directory, src):\n    \"\"\"\n    Spill/unpack OCRD-ZIP bag at SRC to DEST\n\n    SRC must exist an be an OCRD-ZIP\n    DEST must not exist and be a directory\n    \"\"\"\n    resolver = Resolver()\n    workspace_bagger = WorkspaceBagger(resolver)\n    workspace = workspace_bagger.spill(src, directory)\n    print(workspace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate(src, **kwargs):\n    resolver = Resolver()\n    validator = OcrdZipValidator(resolver, src)\n    report = validator.validate(**kwargs)\n    print(report)\n    if not report.is_valid:\n        sys.exit(1)", "response": "Validate OCRD - ZIP\n    SRC must exist an be an OCRD - ZIP file or a directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef restore(self, chksum, choose_first=False):\n        log = getLogger('ocrd.workspace_backup.restore')\n        bak = None\n        candidates = glob(join(self.backup_directory, '%s*' % chksum))\n        if not candidates:\n            log.error(\"No backup found: %s\" % chksum)\n            return\n        if len(candidates) > 1 and not choose_first:\n            raise Exception(\"Not unique, could be\\n%s\" % '\\n'.join(candidates))\n        bak = candidates[0]\n        self.add()\n        log.info(\"Restoring from %s/mets.xml\" % bak)\n        src = join(bak, 'mets.xml')\n        dest = self.workspace.mets_target\n        log.debug('cp \"%s\" \"%s\"', src, dest)\n        copy(src, dest)\n        self.workspace.reload_mets()", "response": "Restore the mets. xml to the previous state."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new backup in self. backup_directory", "response": "def add(self):\n        \"\"\"\n        Create a backup in <self.backup_directory>\n        \"\"\"\n        log = getLogger('ocrd.workspace_backup.add')\n        mets_str = self.workspace.mets.to_xml()\n        chksum = _chksum(mets_str)\n        backups = self.list()\n        if backups and backups[0].chksum == chksum:\n            log.info('No changes since last backup: %s' % backups[0])\n        else:\n            timestamp = datetime.now().timestamp()\n            d = join(self.backup_directory, '%s.%s' % (chksum, timestamp))\n            mets_file = join(d, 'mets.xml')\n            log.info(\"Backing up to %s\" % mets_file)\n            makedirs(d)\n            with open(mets_file, 'wb') as f:\n                f.write(mets_str)\n        return chksum"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting all backups as WorkspaceBackup objects sorted descending by lastmod.", "response": "def list(self):\n        \"\"\"\n        List all backups as WorkspaceBackup objects, sorted descending by lastmod.\n        \"\"\"\n        backups = []\n        for d in glob(join(self.backup_directory, '*')):\n            backups.append(WorkspaceBackup.from_path(d))\n        backups.sort(key=lambda b: b.lastmod, reverse=True)\n        return backups"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrestore to last version of the backup", "response": "def undo(self):\n        \"\"\"\n        Restore to last version\n        \"\"\"\n        log = getLogger('ocrd.workspace_backup.undo')\n        backups = self.list()\n        if backups:\n            last_backup = backups[0]\n            self.restore(last_backup.chksum, choose_first=True)\n        else:\n            log.info(\"No backups, nothing to undo.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_xml(el):\n    sio = StringIO()\n    el.export(sio, 0, name_='PcGts', namespacedef_='xmlns:pc=\"%s\"' % NAMESPACES['page'])\n    return '<?xml version=\"1.0\" encoding=\"UTF-8\"?>\\n' + sio.getvalue()", "response": "Serialize a PCC element into an XML document."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the name of a python logging level.", "response": "def getLevelName(lvl):\n    \"\"\"\n    Get (numerical) python logging level for (string) spec-defined log level name.\n    \"\"\"\n    lvl = _ocrdLevel2pythonLevel.get(lvl, lvl)\n    return logging.getLevelName(lvl)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\noverrides all loggers to include lvl and above.", "response": "def setOverrideLogLevel(lvl):\n    \"\"\"\n    Override all logger filter levels to include lvl and above.\n\n\n    - Set root logger level\n    - iterates all existing loggers and sets their log level to ``NOTSET``.\n\n    Args:\n        lvl (string): Log level name.\n    \"\"\"\n    if lvl is None:\n        return\n    logging.info('Overriding log level globally to %s', lvl)\n    lvl = getLevelName(lvl)\n    global _overrideLogLevel # pylint: disable=global-statement\n    _overrideLogLevel = lvl\n    logging.getLogger('').setLevel(lvl)\n    for loggerName in logging.Logger.manager.loggerDict:\n        logger = logging.Logger.manager.loggerDict[loggerName]\n        if isinstance(logger, logging.PlaceHolder):\n            continue\n        logger.setLevel(logging.NOTSET)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping around logging. getLogger that respects overrideLogLevel", "response": "def getLogger(*args, **kwargs):\n    \"\"\"\n    Wrapper around ``logging.getLogger`` that respects `overrideLogLevel <#setOverrideLogLevel>`_.\n    \"\"\"\n    logger = logging.getLogger(*args, **kwargs)\n    if _overrideLogLevel is not None:\n        logger.setLevel(logging.NOTSET)\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initLogging():\n    logging.basicConfig(\n        level=logging.INFO,\n        format='%(asctime)s.%(msecs)03d %(levelname)s %(name)s - %(message)s',\n        datefmt='%H:%M:%S')\n    logging.getLogger('').setLevel(logging.INFO)\n    #  logging.getLogger('ocrd.resolver').setLevel(logging.INFO)\n    #  logging.getLogger('ocrd.resolver.download_to_directory').setLevel(logging.INFO)\n    #  logging.getLogger('ocrd.resolver.add_files_to_mets').setLevel(logging.INFO)\n    logging.getLogger('PIL').setLevel(logging.INFO)\n\n    # Allow overriding\n\n    CONFIG_PATHS = [\n        os.path.curdir,\n        os.path.join(os.path.expanduser('~')),\n        '/etc',\n    ]\n\n\n    for p in CONFIG_PATHS:\n        config_file = os.path.join(p, 'ocrd_logging.py')\n        if os.path.exists(config_file):\n            logging.info(\"Loading logging configuration from '%s'\", config_file)\n            with open(config_file) as f:\n                code = compile(f.read(), config_file, 'exec')\n                exec(code, globals(), locals())", "response": "Initializes the logging module"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bag(self,\n            workspace,\n            ocrd_identifier,\n            dest=None,\n            ocrd_mets='mets.xml',\n            ocrd_manifestation_depth='full',\n            ocrd_base_version_checksum=None,\n            processes=1,\n            skip_zip=False,\n            in_place=False,\n            tag_files=None\n           ):\n        \"\"\"\n        Bag a workspace\n\n        See https://ocr-d.github.com/ocrd_zip#packing-a-workspace-as-ocrd-zip\n\n        Arguments:\n            workspace (ocrd.Workspace): workspace to bag\n            ord_identifier (string): Ocrd-Identifier in bag-info.txt\n            dest (string): Path of the generated OCRD-ZIP.\n            ord_mets (string): Ocrd-Mets in bag-info.txt\n            ord_manifestation_depth (string): Ocrd-Manifestation-Depth in bag-info.txt\n            ord_base_version_checksum (string): Ocrd-Base-Version-Checksum in bag-info.txt\n            processes (integer): Number of parallel processes checksumming\n            skip_zip (boolean): Whether to leave directory unzipped\n            in_place (boolean): Whether to **replace** the workspace with its BagIt variant\n            tag_files (list<string>): Path names of additional tag files to be bagged at the root of the bag\n        \"\"\"\n        if ocrd_manifestation_depth not in ('full', 'partial'):\n            raise Exception(\"manifestation_depth must be 'full' or 'partial'\")\n        if in_place and (dest is not None):\n            raise Exception(\"Setting 'dest' and 'in_place' is a contradiction\")\n        if in_place and not skip_zip:\n            raise Exception(\"Setting 'skip_zip' and not 'in_place' is a contradiction\")\n\n        if tag_files is None:\n            tag_files = []\n\n        # create bagdir\n        bagdir = mkdtemp(prefix=TMP_BAGIT_PREFIX)\n\n        if dest is None:\n            if in_place:\n                dest = workspace.directory\n            elif not skip_zip:\n                dest = '%s.ocrd.zip' % workspace.directory\n            else:\n                dest = '%s.ocrd' % workspace.directory\n\n        log.info(\"Bagging %s to %s (temp dir %s)\", workspace.directory, '(in-place)' if in_place else dest, bagdir)\n\n        # create data dir\n        makedirs(join(bagdir, 'data'))\n\n        # create bagit.txt\n        with open(join(bagdir, 'bagit.txt'), 'wb') as f:\n            f.write(BAGIT_TXT.encode('utf-8'))\n\n        # create manifests\n        total_bytes, total_files = self._bag_mets_files(workspace, bagdir, ocrd_manifestation_depth, ocrd_mets, processes)\n\n        # create bag-info.txt\n        bag = Bag(bagdir)\n        self._set_bag_info(bag, total_bytes, total_files, ocrd_identifier, ocrd_manifestation_depth, ocrd_base_version_checksum)\n\n        for tag_file in tag_files:\n            copyfile(tag_file, join(bagdir, basename(tag_file)))\n\n        # save bag\n        bag.save()\n\n        # ZIP it\n        self._serialize_bag(workspace, bagdir, dest, in_place, skip_zip)\n\n        log.info('Created bag at %s', dest)\n        return dest", "response": "Bag a workspace into a new file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef spill(self, src, dest):\n        #  print(dest)\n\n        if exists(dest) and not isdir(dest):\n            raise Exception(\"Not a directory: %s\" % dest)\n\n        # If dest is an existing directory, try to derive its name from src\n        if isdir(dest):\n            workspace_name = re.sub(r'(\\.ocrd)?\\.zip$', '', basename(src))\n            new_dest = join(dest, workspace_name)\n            if exists(new_dest):\n                raise Exception(\"Directory exists: %s\" % new_dest)\n            dest = new_dest\n\n        log.info(\"Spilling %s to %s\", src, dest)\n\n        bagdir = mkdtemp(prefix=TMP_BAGIT_PREFIX)\n        unzip_file_to_dir(src, bagdir)\n\n        datadir = join(bagdir, 'data')\n        for root, _, files in walk(datadir):\n            for f in files:\n                srcfile = join(root, f)\n                destdir = join(dest, relpath(root, datadir))\n                destfile = join(destdir, f)\n                if not exists(destdir):\n                    makedirs(destdir)\n                log.debug(\"Copy %s -> %s\", srcfile, destfile)\n                copyfile(srcfile, destfile)\n\n        # TODO copy allowed tag files if present\n\n        # TODO validate bagit\n\n        # Drop tempdir\n        rmtree(bagdir)\n\n        # Create workspace\n        workspace = Workspace(self.resolver, directory=dest)\n\n        # TODO validate workspace\n\n        return workspace", "response": "Spill a workspace from src into dest."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading a URL to the workspace.", "response": "def download_url(self, url, **kwargs):\n        \"\"\"\n        Download a URL to the workspace.\n\n        Args:\n            url (string): URL to download to directory\n            **kwargs : See :py:mod:`ocrd.resolver.Resolver`\n\n        Returns:\n            The local filename of the downloaded file\n        \"\"\"\n        if self.baseurl and '://' not in url:\n            url = join(self.baseurl, url)\n        return self.resolver.download_to_directory(self.directory, url, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_file(self, f):\n        #  os.chdir(self.directory)\n        #  log.info('f=%s' % f)\n        oldpwd = os.getcwd()\n        try:\n            os.chdir(self.directory)\n            if is_local_filename(f.url):\n                f.local_filename = abspath(f.url)\n            else:\n                if f.local_filename:\n                    log.debug(\"Already downloaded: %s\", f.local_filename)\n                else:\n                    f.local_filename = self.download_url(f.url, basename='%s/%s' % (f.fileGrp, f.ID))\n        finally:\n            os.chdir(oldpwd)\n\n        #  print(f)\n        return f", "response": "Download a file to the workspace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding an output file to the Mets output section.", "response": "def add_file(self, file_grp, content=None, **kwargs):\n        \"\"\"\n        Add an output file. Creates an :class:`OcrdFile` to pass around and adds that to the\n        OcrdMets OUTPUT section.\n        \"\"\"\n        log.debug(\n            'outputfile file_grp=%s local_filename=%s content=%s',\n            file_grp,\n            kwargs.get('local_filename'),\n            content is not None)\n        if content is not None and 'local_filename' not in kwargs:\n            raise Exception(\"'content' was set but no 'local_filename'\")\n\n        oldpwd = os.getcwd()\n        try:\n            os.chdir(self.directory)\n            if 'local_filename' in kwargs:\n                local_filename_dir = kwargs['local_filename'].rsplit('/', 1)[0]\n                if not os.path.isdir(local_filename_dir):\n                    os.makedirs(local_filename_dir)\n                if 'url' not in kwargs:\n                    kwargs['url'] = kwargs['local_filename']\n\n            #  print(kwargs)\n            ret = self.mets.add_file(file_grp, **kwargs)\n\n            if content is not None:\n                with open(kwargs['local_filename'], 'wb') as f:\n                    if isinstance(content, str):\n                        content = bytes(content, 'utf-8')\n                    f.write(content)\n        finally:\n            os.chdir(oldpwd)\n\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwriting out the METS file.", "response": "def save_mets(self):\n        \"\"\"\n        Write out the current state of the METS file.\n        \"\"\"\n        log.info(\"Saving mets '%s'\" % self.mets_target)\n        if self.automatic_backup:\n            WorkspaceBackupManager(self).add()\n        with open(self.mets_target, 'wb') as f:\n            f.write(self.mets.to_xml(xmllint=True))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_image_exif(self, image_url):\n        files = self.mets.find_files(url=image_url)\n        if files:\n            image_filename = self.download_file(files[0]).local_filename\n        else:\n            image_filename = self.download_url(image_url)\n\n        if image_url not in self.image_cache['exif']:\n            self.image_cache['exif'][image_url] = OcrdExif(Image.open(image_filename))\n        return self.image_cache['exif'][image_url]", "response": "Resolve an image URL to an EXIF metadata object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve an image URL to a PIL image.", "response": "def resolve_image_as_pil(self, image_url, coords=None):\n        \"\"\"\n        Resolve an image URL to a PIL image.\n\n        Args:\n            coords (list) : Coordinates of the bounding box to cut from the image\n\n        Returns:\n            Image or region in image as PIL.Image\n        \"\"\"\n        files = self.mets.find_files(url=image_url)\n        if files:\n            image_filename = self.download_file(files[0]).local_filename\n        else:\n            image_filename = self.download_url(image_url)\n\n        if image_url not in self.image_cache['pil']:\n            self.image_cache['pil'][image_url] = Image.open(image_filename)\n\n        pil_image = self.image_cache['pil'][image_url]\n\n        if coords is None:\n            return pil_image\n        if image_url not in self.image_cache['cv2']:\n            log.debug(\"Converting PIL to OpenCV: %s\", image_url)\n            color_conversion = cv2.COLOR_GRAY2BGR if pil_image.mode in ('1', 'L') else  cv2.COLOR_RGB2BGR\n            pil_as_np_array = np.array(pil_image).astype('uint8') if pil_image.mode == '1' else np.array(pil_image)\n            self.image_cache['cv2'][image_url] = cv2.cvtColor(pil_as_np_array, color_conversion)\n        cv2_image = self.image_cache['cv2'][image_url]\n        poly = np.array(coords, np.int32)\n        log.debug(\"Cutting region %s from %s\", coords, image_url)\n        region_cut = cv2_image[\n            np.min(poly[:, 1]):np.max(poly[:, 1]),\n            np.min(poly[:, 0]):np.max(poly[:, 0])\n        ]\n        return Image.fromarray(region_cut)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize all properties as XML", "response": "def to_xml(self):\n        \"\"\"\n        Serialize all properties as XML\n        \"\"\"\n        ret = '<exif>'\n        for k in self.__dict__:\n            ret += '<%s>%s</%s>' % (k, self.__dict__[k], k)\n        ret += '</exif>'\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef basename_without_extension(self):\n        ret = self.basename.rsplit('.', 1)[0]\n        if ret.endswith('.tar'):\n            ret = ret[0:len(ret)-4]\n        return ret", "response": "Get the os. path. basename of the local file without the extension removed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ID of the physical page this file manifests belongs to.", "response": "def pageId(self):\n        \"\"\"\n        Get the ID of the physical page this file manifests.\n        \"\"\"\n        if self.mets is None:\n            raise Exception(\"OcrdFile %s has no member 'mets' pointing to parent OcrdMets\" % self)\n        return self.mets.get_physical_page_for_file(self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the ID of the physical page this file manifests.", "response": "def pageId(self, pageId):\n        \"\"\"\n        Set the ID of the physical page this file manifests.\n        \"\"\"\n        if pageId is None:\n            return\n        if self.mets is None:\n            raise Exception(\"OcrdFile %s has no member 'mets' pointing to parent OcrdMets\" % self)\n        self.mets.set_physical_page_for_file(pageId, self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef url(self):\n        el_FLocat = self._el.find(TAG_METS_FLOCAT)\n        if el_FLocat is not None:\n            return el_FLocat.get(\"{%s}href\" % NS[\"xlink\"])\n        return ''", "response": "Get the xlink href of this file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the xlink : href of this file.", "response": "def url(self, url):\n        \"\"\"\n        Set the ``xlink:href`` of this file.\n        \"\"\"\n        if url is None:\n            return\n        el_FLocat = self._el.find('mets:FLocat', NS)\n        if el_FLocat is None:\n            el_FLocat = ET.SubElement(self._el, TAG_METS_FLOCAT)\n        el_FLocat.set(\"{%s}href\" % NS[\"xlink\"], url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef empty_mets():\n        tpl = METS_XML_EMPTY.decode('utf-8')\n        tpl = tpl.replace('{{ VERSION }}', VERSION)\n        tpl = tpl.replace('{{ NOW }}', '%s' % datetime.now())\n        return OcrdMets(content=tpl.encode('utf-8'))", "response": "Create an empty METS file from bundled template."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unique_identifier(self):\n        for t in IDENTIFIER_PRIORITY:\n            found = self._tree.getroot().find('.//mods:identifier[@type=\"%s\"]' % t, NS)\n            if found is not None:\n                return found.text", "response": "Get the unique identifier of the moderation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the unique identifier by looking through mods. identifier and then looking through the identifier.", "response": "def unique_identifier(self, purl):\n        \"\"\"\n        Set the unique identifier by looking through ``mods:identifier``\n\n        See `specs <https://ocr-d.github.io/mets#unique-id-for-the-document-processed>`_ for details.\n        \"\"\"\n        id_el = None\n        for t in IDENTIFIER_PRIORITY:\n            id_el = self._tree.getroot().find('.//mods:identifier[@type=\"%s\"]' % t, NS)\n            if id_el is not None:\n                break\n        if id_el is None:\n            mods = self._tree.getroot().find('.//mods:mods', NS)\n            id_el = ET.SubElement(mods, TAG_MODS_IDENTIFIER)\n            id_el.set('type', 'purl')\n        id_el.text = purl"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist all OcrdAgent elements", "response": "def agents(self):\n        \"\"\"\n        List all `OcrdAgent </../../ocrd_models/ocrd_models.ocrd_agent.html>`_\n        \"\"\"\n        return [OcrdAgent(el_agent) for el_agent in self._tree.getroot().findall('mets:metsHdr/mets:agent', NS)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd an OcrdAgent to the list of agents in the metsHdr.", "response": "def add_agent(self, *args, **kwargs):\n        \"\"\"\n        Add an `OcrdAgent </../../ocrd_models/ocrd_models.ocrd_agent.html>`_ to the list of agents in the metsHdr.\n        \"\"\"\n        el_metsHdr = self._tree.getroot().find('.//mets:metsHdr', NS)\n        if el_metsHdr is None:\n            el_metsHdr = ET.Element(TAG_METS_METSHDR)\n            self._tree.getroot().insert(0, el_metsHdr)\n        #  assert(el_metsHdr is not None)\n        el_agent = ET.SubElement(el_metsHdr, TAG_METS_AGENT)\n        #  print(ET.tostring(el_metsHdr))\n        return OcrdAgent(el_agent, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlist the USE attributes of all fileGrp.", "response": "def file_groups(self):\n        \"\"\"\n        List the ``USE`` attributes of all ``mets:fileGrp``.\n        \"\"\"\n        return [el.get('USE') for el in self._tree.getroot().findall('.//mets:fileGrp', NS)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsearches METS document for files of a given file.", "response": "def find_files(self, ID=None, fileGrp=None, pageId=None, mimetype=None, url=None, local_only=False):\n        \"\"\"\n        Search ``mets:file`` in this METS document.\n\n        Args:\n            ID (string) : ID of the file\n            fileGrp (string) : USE of the fileGrp to list files of\n            pageId (string) : ID of physical page manifested by matching files\n            url (string) : @xlink:href of mets:Flocat of mets:file\n            mimetype (string) : MIMETYPE of matching files\n            local (boolean) : Whether to restrict results to local files, i.e. file://-URL\n\n        Return:\n            List of files.\n        \"\"\"\n        ret = []\n        fileGrp_clause = '' if fileGrp is None else '[@USE=\"%s\"]' % fileGrp\n        file_clause = ''\n        if ID is not None:\n            file_clause += '[@ID=\"%s\"]' % ID\n        if mimetype is not None:\n            file_clause += '[@MIMETYPE=\"%s\"]' % mimetype\n        if url is not None:\n            file_clause += '[mets:FLocat[@xlink:href = \"%s\"]]' % url\n        # TODO lxml says invalid predicate. I disagree\n        #  if local_only:\n        #      file_clause += \"[mets:FLocat[starts-with(@xlink:href, 'file://')]]\"\n\n        # Search\n        file_ids = self._tree.getroot().xpath(\"//mets:fileGrp%s/mets:file%s/@ID\" % (fileGrp_clause, file_clause), namespaces=NS)\n        if pageId is not None:\n            by_pageid = self._tree.getroot().xpath('//mets:div[@TYPE=\"page\"][@ID=\"%s\"]/mets:fptr/@FILEID' % pageId, namespaces=NS)\n            file_ids = [i for i in by_pageid if i in file_ids]\n\n        # instantiate / get from cache\n        for file_id in file_ids:\n            el = self._tree.getroot().find('.//mets:file[@ID=\"%s\"]' % file_id, NS)\n            if file_id not in self._file_by_id:\n                self._file_by_id[file_id] = OcrdFile(el, mets=self)\n\n            # If only local resources should be returned and file is neither a\n            # file:// URL nor a file path: skip the file\n            url = self._file_by_id[file_id].url\n            if local_only and not (url.startswith('file://') or '://' not in url):\n                continue\n            ret.append(self._file_by_id[file_id])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_file_group(self, fileGrp):\n        el_fileSec = self._tree.getroot().find('mets:fileSec', NS)\n        if el_fileSec is None:\n            el_fileSec = ET.SubElement(self._tree.getroot(), TAG_METS_FILESEC)\n        el_fileGrp = el_fileSec.find('mets:fileGrp[@USE=\"%s\"]' % fileGrp, NS)\n        if el_fileGrp is None:\n            el_fileGrp = ET.SubElement(el_fileSec, TAG_METS_FILEGRP)\n            el_fileGrp.set('USE', fileGrp)\n        return el_fileGrp", "response": "Adds a new filegroup to the file group."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_file(self, fileGrp, mimetype=None, url=None, ID=None, pageId=None, force=False, local_filename=None, **kwargs):\n        if not ID:\n            raise Exception(\"Must set ID of the mets:file\")\n        el_fileGrp = self._tree.getroot().find(\".//mets:fileGrp[@USE='%s']\" % (fileGrp), NS)\n        if el_fileGrp is None:\n            el_fileGrp = self.add_file_group(fileGrp)\n        if ID is not None and self.find_files(ID=ID) != []:\n            if not force:\n                raise Exception(\"File with ID='%s' already exists\" % ID)\n            mets_file = self.find_files(ID=ID)[0]\n        else:\n            mets_file = OcrdFile(ET.SubElement(el_fileGrp, TAG_METS_FILE), mets=self)\n        mets_file.url = url\n        mets_file.mimetype = mimetype\n        mets_file.ID = ID\n        mets_file.pageId = pageId\n        mets_file.local_filename = local_filename\n\n        self._file_by_id[ID] = mets_file\n\n        return mets_file", "response": "Add a file to the Mets object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the physical page for a file.", "response": "def set_physical_page_for_file(self, pageId, ocrd_file, order=None, orderlabel=None):\n        \"\"\"\n        Create a new physical page\n        \"\"\"\n        #  print(pageId, ocrd_file)\n        # delete any page mapping for this file.ID\n        for el_fptr in self._tree.getroot().findall(\n                'mets:structMap[@TYPE=\"PHYSICAL\"]/mets:div[@TYPE=\"physSequence\"]/mets:div[@TYPE=\"page\"]/mets:fptr[@FILEID=\"%s\"]' %\n                ocrd_file.ID, namespaces=NS):\n            el_fptr.getparent().remove(el_fptr)\n\n        # find/construct as necessary\n        el_structmap = self._tree.getroot().find('mets:structMap[@TYPE=\"PHYSICAL\"]', NS)\n        if el_structmap is None:\n            el_structmap = ET.SubElement(self._tree.getroot(), TAG_METS_STRUCTMAP)\n            el_structmap.set('TYPE', 'PHYSICAL')\n        el_seqdiv = el_structmap.find('mets:div[@TYPE=\"physSequence\"]', NS)\n        if el_seqdiv is None:\n            el_seqdiv = ET.SubElement(el_structmap, TAG_METS_DIV)\n            el_seqdiv.set('TYPE', 'physSequence')\n        el_pagediv = el_seqdiv.find('mets:div[@ID=\"%s\"]' % pageId, NS)\n        if el_pagediv is None:\n            el_pagediv = ET.SubElement(el_seqdiv, TAG_METS_DIV)\n            el_pagediv.set('TYPE', 'page')\n            el_pagediv.set('ID', pageId)\n            if order:\n                el_pagediv.set('ORDER', order)\n            if orderlabel:\n                el_pagediv.set('ORDERLABEL', orderlabel)\n        el_fptr = ET.SubElement(el_pagediv, TAG_METS_FPTR)\n        el_fptr.set('FILEID', ocrd_file.ID)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the physical page for a given ocrd_file", "response": "def get_physical_page_for_file(self, ocrd_file):\n        \"\"\"\n        Get the pageId for a ocrd_file\n        \"\"\"\n        ret = self._tree.getroot().xpath(\n            '/mets:mets/mets:structMap[@TYPE=\"PHYSICAL\"]/mets:div[@TYPE=\"physSequence\"]/mets:div[@TYPE=\"page\"][./mets:fptr[@FILEID=\"%s\"]]/@ID' %\n            ocrd_file.ID, namespaces=NS)\n        if ret:\n            return ret[0]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _resolve_workspace(self):\n        if self.workspace is None:\n            self.workspace = self.resolver.workspace_from_url(self.mets_url, baseurl=self.src_dir, download=self.download)\n            self.mets = self.workspace.mets", "response": "Clone workspace from mets_url unless workspace was provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_pixel_density(self):\n        for f in [f for f in self.mets.find_files() if f.mimetype.startswith('image/')]:\n            if not f.local_filename and not self.download:\n                self.report.add_notice(\"Won't download remote image <%s>\" % f.url)\n                continue\n            exif = self.workspace.resolve_image_exif(f.url)\n            for k in ['xResolution', 'yResolution']:\n                v = exif.__dict__.get(k)\n                if v is None or v <= 72:\n                    self.report.add_error(\"Image %s: %s (%s pixels per %s) is too low\" % (f.ID, k, v, exif.resolutionUnit))", "response": "Validate image pixel density of images."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_mets_file_group_names(self):\n        for fileGrp in self.mets.file_groups:\n            if not fileGrp.startswith(FILE_GROUP_PREFIX):\n                self.report.add_notice(\"fileGrp USE does not begin with '%s': %s\" % (FILE_GROUP_PREFIX, fileGrp))\n            else:\n                # OCR-D-FOO-BAR -> ('FOO', 'BAR')\n                # \\____/\\_/ \\_/\n                #   |    |   |\n                # Prefix |  Name\n                #     Category\n                category = fileGrp[len(FILE_GROUP_PREFIX):]\n                name = None\n                if '-' in category:\n                    category, name = category.split('-', 1)\n                if category not in FILE_GROUP_CATEGORIES:\n                    self.report.add_error(\"Unspecified USE category '%s' in fileGrp '%s'\" % (category, fileGrp))\n                if name is not None and not re.match(r'^[A-Z0-9-]{3,}$', name):\n                    self.report.add_error(\"Invalid USE name '%s' in fileGrp '%s'\" % (name, fileGrp))", "response": "Validate USE attributes of file groups."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that the files in the Mets database are sane.", "response": "def _validate_mets_files(self):\n        \"\"\"\n        Validate ``mets:file`` URLs are sane.\n        \"\"\"\n        if not self.mets.find_files():\n            self.report.add_error(\"No files\")\n        for f in self.mets.find_files():\n            if f._el.get('GROUPID'): # pylint: disable=protected-access\n                self.report.add_notice(\"File '%s' has GROUPID attribute - document might need an update\" % f.ID)\n            if not f.pageId:\n                self.report.add_error(\"File '%s' does not manifest any physical page.\" % f.ID)\n            if 'url' not in self.skip and ':/' in f.url:\n                if re.match(r'^file:/[^/]', f.url):\n                    self.report.add_warning(\"File '%s' has an invalid (Java-specific) file URL '%s'\" % (f.ID, f.url))\n                scheme = f.url[0:f.url.index(':')]\n                if scheme not in ('http', 'https', 'file'):\n                    self.report.add_warning(\"File '%s' has non-HTTP, non-file URL '%s'\" % (f.ID, f.url))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning PageValidator on the PAGE-XML documents referenced in the METS.", "response": "def _validate_page(self):\n        \"\"\"\n        Run PageValidator on the PAGE-XML documents referenced in the METS.\n        \"\"\"\n        for ocrd_file in self.mets.find_files(mimetype=MIMETYPE_PAGE, local_only=True):\n            self.workspace.download_file(ocrd_file)\n            page_report = PageValidator.validate(ocrd_file=ocrd_file, strictness=self.page_strictness)\n            self.report.merge_report(page_report)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_log_action(self, **kwargs):\n        instance = kwargs.get('instance', None)\n        if instance is not None:\n            del kwargs['instance']\n\n        request = kwargs.get('request', None)\n        if request is not None:\n            del kwargs['request']\n            # Let's grab the current IP of the user.\n            x_forwarded_for = request.META.get('HTTP_X_FORWARDED_FOR')\n            if x_forwarded_for:\n                remote_ip = x_forwarded_for.split(',')[0]\n            else:\n                remote_ip = request.META.get('REMOTE_ADDR')\n            kwargs.setdefault('remote_ip', remote_ip)\n\n        if instance is not None:\n            pk = self._get_pk_value(instance)\n\n            kwargs.setdefault(\n                'content_type',\n                ContentType.objects.get_for_model(instance)\n            )\n            kwargs.setdefault('object_pk', pk)\n            kwargs.setdefault('object_repr', smart_text(instance))\n\n            if isinstance(pk, integer_types):\n                kwargs.setdefault('object_id', pk)\n\n            get_object_extra_info = getattr(\n                instance,\n                'get_object_extra_info',\n                None\n            )\n\n            if callable(get_object_extra_info):\n                kwargs.setdefault('object_extra_info', get_object_extra_info())\n\n            # Delete log entries with the same pk as a newly created model.\n            # This should only be necessary when an pk is used twice.\n            if kwargs.get('action', None) is app_conf.CREATE:\n                is_obj_exists = self.filter(\n                    content_type=kwargs.get('content_type'),\n                    object_id=kwargs.get('object_id')\n                ).exists()\n\n                if kwargs.get('object_id', None) is not None and is_obj_exists:\n                    self.filter(\n                        content_type=kwargs.get('content_type'),\n                        object_id=kwargs.get('object_id')\n                    ).delete()\n                else:\n                    self.filter(\n                        content_type=kwargs.get('content_type'),\n                        object_pk=kwargs.get('object_pk', '')\n                    ).delete()\n\n        action_log = self.create(**kwargs)\n        action_logged.send(sender=LogAction, action=action_log)\n        return action_log", "response": "This method creates a new log entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_for_objects(self, queryset):\n        if not isinstance(queryset, QuerySet) or queryset.count() == 0:\n            return self.none()\n\n        content_type = ContentType.objects.get_for_model(queryset.model)\n        primary_keys = queryset.values_list(queryset.model._meta.pk.name, flat=True)\n\n        if isinstance(primary_keys[0], integer_types):\n            return self.filter(content_type=content_type).filter(Q(object_id__in=primary_keys)).distinct()\n        else:\n            return self.filter(content_type=content_type).filter(Q(object_pk__in=primary_keys)).distinct()", "response": "Get the log entries for the objects in the specified queryset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register(self, model, include_fields=[], exclude_fields=[]):\n        if issubclass(model, Model):\n            self._registry[model] = {\n                'include_fields': include_fields,\n                'exclude_fields': exclude_fields,\n            }\n            self._connect_signals(model)\n        else:\n            raise TypeError(\"Supplied model is not a valid model.\")", "response": "Register a model with actionslog."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef track_field(field):\n    from actionslog.models import LogAction\n    # Do not track many to many relations\n    if field.many_to_many:\n        return False\n\n    # Do not track relations to LogAction\n    if getattr(field, 'rel', None) is not None and field.rel.to == LogAction:\n        return False\n\n    return True", "response": "Returns whether the given field should be tracked by Actionslog."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the differences between two model instances.", "response": "def model_instance_diff(old, new):\n    \"\"\"\n    Calculates the differences between two model instances. One of the instances may be ``None`` (i.e., a newly\n    created model or deleted model). This will cause all fields with a value to have changed (from ``None``).\n\n    :param old: The old state of the model instance.\n    :type old: Model\n    :param new: The new state of the model instance.\n    :type new: Model\n    :return: A dictionary with the names of the changed fields as keys and a two tuple of the old and new field values\n             as value.\n    :rtype: dict\n    \"\"\"\n    from actionslog.registry import actionslog\n\n    if not(old is None or isinstance(old, Model)):\n        raise TypeError(\"The supplied old instance is not a valid model instance.\")\n    if not(new is None or isinstance(new, Model)):\n        raise TypeError(\"The supplied new instance is not a valid model instance.\")\n\n    diff = {}\n\n    if old is not None and new is not None:\n        fields = set(old._meta.fields + new._meta.fields)\n        model_fields = actionslog.get_model_fields(new._meta.model)\n    elif old is not None:\n        fields = set(get_fields_in_model(old))\n        model_fields = actionslog.get_model_fields(old._meta.model)\n    elif new is not None:\n        fields = set(get_fields_in_model(new))\n        model_fields = actionslog.get_model_fields(new._meta.model)\n    else:\n        fields = set()\n        model_fields = None\n\n    # Check if fields must be filtered\n    if model_fields and (model_fields['include_fields'] or model_fields['exclude_fields']) and fields:\n        filtered_fields = []\n        if model_fields['include_fields']:\n            filtered_fields = [field for field in fields\n                               if field.name in model_fields['include_fields']]\n        else:\n            filtered_fields = fields\n        if model_fields['exclude_fields']:\n            filtered_fields = [field for field in filtered_fields\n                               if field.name not in model_fields['exclude_fields']]\n        fields = filtered_fields\n\n    for field in fields:\n        try:\n            old_value = smart_text(getattr(old, field.name, None))\n        except ObjectDoesNotExist:\n            old_value = field.default if field.default is not NOT_PROVIDED else None\n\n        try:\n            new_value = smart_text(getattr(new, field.name, None))\n        except ObjectDoesNotExist:\n            new_value = None\n\n        if old_value != new_value:\n            diff[field.name] = (smart_text(old_value), smart_text(new_value))\n\n    if len(diff) == 0:\n        diff = None\n\n    return diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the current user from the request and prepares and connects a signal receiver with the user already attached to it.", "response": "def process_request(self, request):\n        \"\"\"\n        Gets the current user from the request and prepares and connects a signal receiver with the user already\n        attached to it.\n        \"\"\"\n        # Initialize thread local storage\n        threadlocal.actionslog = {\n            'signal_duid': (self.__class__, time.time()),\n            'remote_ip': request.META.get('REMOTE_ADDR'),\n        }\n\n        # In case of proxy, set 'original' address\n        if request.META.get('HTTP_X_FORWARDED_FOR'):\n            threadlocal.actionslog['remote_ip'] = request.META.get('HTTP_X_FORWARDED_FOR').split(',')[0]\n\n        # Connect signal for automatic logging\n        if hasattr(request, 'user') and hasattr(request.user, 'is_authenticated') and request.user.is_authenticated():\n            set_user = curry(self.set_user, request.user)\n            pre_save.connect(set_user, sender=LogAction, dispatch_uid=threadlocal.actionslog['signal_duid'], weak=False)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_response(self, request, response):\n        if hasattr(threadlocal, 'actionslog'):\n            pre_save.disconnect(sender=LogAction, dispatch_uid=threadlocal.actionslog['signal_duid'])\n\n        return response", "response": "Disconnects the signal receiver to prevent it from staying active."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisconnects the signal receiver to prevent it from staying active in case of an exception.", "response": "def process_exception(self, request, exception):\n        \"\"\"\n        Disconnects the signal receiver to prevent it from staying active in case of an exception.\n        \"\"\"\n        if hasattr(threadlocal, 'actionslog'):\n            pre_save.disconnect(sender=LogAction, dispatch_uid=threadlocal.actionslog['signal_duid'])\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the user of the log action instance.", "response": "def set_user(user, sender, instance, **kwargs):\n        \"\"\"\n        Signal receiver with an extra, required 'user' kwarg. This method becomes a real (valid) signal receiver when\n        it is curried with the user.\n        \"\"\"\n        try:\n            app_label, model_name = settings.AUTH_USER_MODEL.split('.')\n            auth_user_model = apps.get_model(app_label, model_name)\n        except ValueError:\n            auth_user_model = apps.get_model('auth', 'user')\n        if sender == LogAction and isinstance(user, auth_user_model) and instance.user is None:\n            instance.user = user\n        if hasattr(threadlocal, 'actionslog'):\n            instance.remote_ip = threadlocal.actionslog['remote_ip']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsignaling receiver that creates a log entry when a model instance is first saved to the database.", "response": "def action_log_create(sender, instance, created, **kwargs):\n    \"\"\"\n    Signal receiver that creates a log entry when a model instance is first saved to the database.\n\n    Direct use is discouraged, connect your model through :py:func:`actionslog.registry.register` instead.\n    \"\"\"\n    if created:\n        changes = model_instance_diff(None, instance)\n\n        log_entry = LogAction.objects.create_log_action(\n            instance=instance,\n            action=LogAction.CREATE,\n            changes=json.dumps(changes),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsignaling receiver that creates a log entry when a model instance is changed and saved to the database.", "response": "def action_log_update(sender, instance, **kwargs):\n    \"\"\"\n    Signal receiver that creates a log entry when a model instance is changed and saved to the database.\n\n    Direct use is discouraged, connect your model through :py:func:`actionslog.registry.register` instead.\n    \"\"\"\n    if instance.pk is not None:\n        try:\n            old = sender.objects.get(pk=instance.pk)\n        except sender.DoesNotExist:\n            pass\n        else:\n            new = instance\n\n            changes = model_instance_diff(old, new)\n\n            # Log an entry only if there are changes\n            if changes:\n                log_entry = LogAction.objects.create_log_action(\n                    instance=instance,\n                    action=LogAction.UPDATE,\n                    changes=json.dumps(changes),\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef action_log_delete(sender, instance, **kwargs):\n    if instance.pk is not None:\n        changes = model_instance_diff(instance, None)\n\n        log_entry = LogAction.objects.create_log_action(\n            instance=instance,\n            action=LogAction.DELETE,\n            changes=json.dumps(changes),\n        )", "response": "Signal receiver that creates a log entry when a model instance is deleted from the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef request(self, method, api_url, params={}, **kwargs):\n\n        LOG.debug(\"axapi_http: full url = %s\", self.url_base + api_url)\n        LOG.debug(\"axapi_http: %s url = %s\", method, api_url)\n        LOG.debug(\"axapi_http: params = %s\", json.dumps(logutils.clean(params), indent=4))\n\n        # Set \"data\" variable for the request\n        if params:\n            extra_params = kwargs.get('axapi_args', {})\n            params_copy = merge_dicts(params, extra_params)\n            LOG.debug(\"axapi_http: params_all = %s\", logutils.clean(params_copy))\n\n            payload = json.dumps(params_copy)\n        else:\n            try:\n                payload = kwargs.pop('payload', None)\n                self.headers = dict(self.HEADERS, **kwargs.pop('headers', {}))\n                LOG.debug(\"axapi_http: headers_all = %s\", logutils.clean(self.headers))\n            except KeyError:\n                payload = None\n\n        max_retries = kwargs.get('max_retries', self.max_retries)\n        timeout = kwargs.get('timeout', self.timeout)\n\n        # Create session to set HTTPAdapter or SSLAdapter\n        session = Session()\n        if self.port == 443:\n            # Add adapter for any https session to force TLS1_0 connection for v21 of AXAPI\n            session.mount('https://', SSLAdapter(max_retries=max_retries))\n        else:\n            session.mount('http://', HTTPAdapter(max_retries=max_retries))\n        session_request = getattr(session, method.lower())\n\n        # Make actual request and handle any errors\n        try:\n            device_response = session_request(\n                self.url_base + api_url, verify=False, data=payload, headers=self.HEADERS, timeout=timeout\n            )\n        except (Exception) as e:\n            LOG.error(\"acos_client failing with error %s after %s retries\", e.__class__.__name__, max_retries)\n            raise e\n        finally:\n            session.close()\n\n        # Log if the reponse is one of the known broken response\n        if device_response in broken_replies:\n            device_response = broken_replies[device_response]\n            LOG.debug(\"axapi_http: broken reply, new response: %s\", logutils.clean(device_response))\n\n        # Validate json response\n        try:\n            json_response = device_response.json()\n            LOG.debug(\"axapi_http: data = %s\", json.dumps(logutils.clean(json_response), indent=4))\n        except ValueError as e:\n            # The response is not JSON but it still succeeded.\n            LOG.debug(\"axapi_http: json = %s\", e)\n            return device_response\n\n        # Handle \"fail\" responses returned by AXAPI\n        if 'response' in json_response and 'status' in json_response['response']:\n            if json_response['response']['status'] == 'fail':\n                    acos_responses.raise_axapi_ex(json_response, action=extract_method(api_url))\n\n        # Return json portion of response\n        return json_response", "response": "Generate the API call to the device."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, host_list=[], serial=None, instance_name=None, use_mgmt_port=False,\n               interval=None, bandwidth_base=None, bandwidth_unrestricted=None):\n        \"\"\"Creates a license manager entry\n\n        Keyword arguments:\n        instance_name -- license manager instance name\n        host_list -- list(dict) a list of dictionaries of the format:\n            {'ip': '127.0.0.1', 'port': 443}\n        serial - (str) appliance serial number\n        use_mgmt_port - (bool) use management for license interactions\n        interval - (int) 1=Monthly, 2=Daily, 3=Hourly\n        bandwidth_base - (int) Configure feature bandwidth base (Mb)\n            Valid range - 10-102400\n        bandwidth_unrestricted - (bool) Set the bandwidth to maximum\n        \"\"\"\n        payload = self._build_payload(host_list=host_list, serial=serial,\n                                      instance_name=instance_name,\n                                      use_mgmt_port=use_mgmt_port,\n                                      interval=interval, bandwidth_base=bandwidth_base,\n                                      bandwidth_unrestricted=bandwidth_unrestricted)\n        return self._post(self.url_base, payload)", "response": "Creates a new entry in the license manager."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, host_list=[], serial=None, instance_name=None, use_mgmt_port=False,\n               interval=None, bandwidth_base=None, bandwidth_unrestricted=None):\n        \"\"\"Update a license manager entry\n\n        Keyword arguments:\n        instance_name -- license manager instance name\n        host_list -- list(dict) a list of dictionaries of the format:\n            {'ip': '127.0.0.1', 'port': 443}\n        serial - (str) appliance serial number\n        use_mgmt_port - (bool) use management for license interactions\n        interval - (int) 1=Monthly, 2=Daily, 3=Hourly\n        bandwidth_base - (int) Configure feature bandwidth base (Mb)\n            Valid range - 10-102400\n        bandwidth_unrestricted - (bool) Set the bandwidth to maximum\n        \"\"\"\n\n        return self.create(host_list=host_list, serial=serial, instance_name=instance_name,\n                           use_mgmt_port=use_mgmt_port,\n                           interval=interval, bandwidth_base=bandwidth_base,\n                           bandwidth_unrestricted=bandwidth_unrestricted)", "response": "Update a license manager entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nswitches the device - context of a specific object.", "response": "def switch(self, device_id, obj_slot_id):\n        \"\"\"Switching of device-context\"\"\"\n        payload = {\n            \"device-context\": self._build_payload(device_id, obj_slot_id)\n        }\n\n        return self._post(self.url_prefix, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, destination, mask, next_hops=[]):\n        payload = {\n            \"rib\": self._build_payload(destination, mask, next_hops)\n        }\n\n        return self._post(self.url_prefix, payload)", "response": "Create a new route to the specified destination."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a directory contains valid VASP input.", "response": "def contains_vasp_input(dir_name):\n    \"\"\"\n    Checks if a directory contains valid VASP input.\n\n    Args:\n        dir_name:\n            Directory name to check.\n\n    Returns:\n        True if directory contains all four VASP input files (INCAR, POSCAR,\n        KPOINTS and POTCAR).\n    \"\"\"\n    for f in [\"INCAR\", \"POSCAR\", \"POTCAR\", \"KPOINTS\"]:\n        if not os.path.exists(os.path.join(dir_name, f)) and \\\n                not os.path.exists(os.path.join(dir_name, f + \".orig\")):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the URI path for a directory.", "response": "def get_uri(dir_name):\n    \"\"\"\n    Returns the URI path for a directory. This allows files hosted on\n    different file servers to have distinct locations.\n\n    Args:\n        dir_name:\n            A directory name.\n\n    Returns:\n        Full URI path, e.g., fileserver.host.com:/full/path/of/dir_name.\n    \"\"\"\n    fullpath = os.path.abspath(dir_name)\n    try:\n        hostname = socket.gethostbyaddr(socket.gethostname())[0]\n    except:\n        hostname = socket.gethostname()\n    return \"{}:{}\".format(hostname, fullpath)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef assimilate(self, path):\n        try:\n            d = self.get_task_doc(path)\n            if self.mapi_key is not None and d[\"state\"] == \"successful\":\n                self.calculate_stability(d)\n            tid = self._insert_doc(d)\n            return tid\n        except Exception as ex:\n            import traceback\n            logger.error(traceback.format_exc())\n            return False", "response": "Assimilate a vasp run into the database."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_task_doc(self, path):\n        logger.info(\"Getting task doc for base dir :{}\".format(path))\n        files = os.listdir(path)\n        vasprun_files = OrderedDict()\n        if \"STOPCAR\" in files:\n            #Stopped runs. Try to parse as much as possible.\n            logger.info(path + \" contains stopped run\")\n        for r in self.runs:\n            if r in files: #try subfolder schema\n                for f in os.listdir(os.path.join(path, r)):\n                    if fnmatch(f, \"vasprun.xml*\"):\n                        vasprun_files[r] = os.path.join(r, f)\n            else: #try extension schema\n                for f in files:\n                    if fnmatch(f, \"vasprun.xml.{}*\".format(r)):\n                        vasprun_files[r] = f\n        if len(vasprun_files) == 0:\n            for f in files: #get any vasprun from the folder\n                if fnmatch(f, \"vasprun.xml*\") and \\\n                        f not in vasprun_files.values():\n                    vasprun_files['standard'] = f\n\n        if len(vasprun_files) > 0:\n            d = self.generate_doc(path, vasprun_files)\n            if not d:\n                d = self.process_killed_run(path)\n            self.post_process(path, d)\n        elif (not (path.endswith(\"relax1\") or\n              path.endswith(\"relax2\"))) and contains_vasp_input(path):\n            #If not Materials Project style, process as a killed run.\n            logger.warning(path + \" contains killed run\")\n            d = self.process_killed_run(path)\n            self.post_process(path, d)\n        else:\n            raise ValueError(\"No VASP files found!\")\n\n        return d", "response": "Get the entire task doc for a given path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a killed vasp run.", "response": "def process_killed_run(self, dir_name):\n        \"\"\"\n        Process a killed vasp run.\n        \"\"\"\n        fullpath = os.path.abspath(dir_name)\n        logger.info(\"Processing Killed run \" + fullpath)\n        d = {\"dir_name\": fullpath, \"state\": \"killed\", \"oszicar\": {}}\n\n        for f in os.listdir(dir_name):\n            filename = os.path.join(dir_name, f)\n            if fnmatch(f, \"INCAR*\"):\n                try:\n                    incar = Incar.from_file(filename)\n                    d[\"incar\"] = incar.as_dict()\n                    d[\"is_hubbard\"] = incar.get(\"LDAU\", False)\n                    if d[\"is_hubbard\"]:\n                        us = np.array(incar.get(\"LDAUU\", []))\n                        js = np.array(incar.get(\"LDAUJ\", []))\n                        if sum(us - js) == 0:\n                            d[\"is_hubbard\"] = False\n                            d[\"hubbards\"] = {}\n                    else:\n                        d[\"hubbards\"] = {}\n                    if d[\"is_hubbard\"]:\n                        d[\"run_type\"] = \"GGA+U\"\n                    elif incar.get(\"LHFCALC\", False):\n                        d[\"run_type\"] = \"HF\"\n                    else:\n                        d[\"run_type\"] = \"GGA\"\n                except Exception as ex:\n                    print(str(ex))\n                    logger.error(\"Unable to parse INCAR for killed run {}.\"\n                                 .format(dir_name))\n            elif fnmatch(f, \"KPOINTS*\"):\n                try:\n                    kpoints = Kpoints.from_file(filename)\n                    d[\"kpoints\"] = kpoints.as_dict()\n                except:\n                    logger.error(\"Unable to parse KPOINTS for killed run {}.\"\n                                 .format(dir_name))\n            elif fnmatch(f, \"POSCAR*\"):\n                try:\n                    s = Poscar.from_file(filename).structure\n                    comp = s.composition\n                    el_amt = s.composition.get_el_amt_dict()\n                    d.update({\"unit_cell_formula\": comp.as_dict(),\n                              \"reduced_cell_formula\": comp.to_reduced_dict,\n                              \"elements\": list(el_amt.keys()),\n                              \"nelements\": len(el_amt),\n                              \"pretty_formula\": comp.reduced_formula,\n                              \"anonymous_formula\": comp.anonymized_formula,\n                              \"nsites\": comp.num_atoms,\n                              \"chemsys\": \"-\".join(sorted(el_amt.keys()))})\n                    d[\"poscar\"] = s.as_dict()\n                except:\n                    logger.error(\"Unable to parse POSCAR for killed run {}.\"\n                                 .format(dir_name))\n            elif fnmatch(f, \"POTCAR*\"):\n                try:\n                    potcar = Potcar.from_file(filename)\n                    d[\"pseudo_potential\"] = {\n                        \"functional\": potcar.functional.lower(),\n                        \"pot_type\": \"paw\",\n                        \"labels\": potcar.symbols}\n                except:\n                    logger.error(\"Unable to parse POTCAR for killed run in {}.\"\n                                 .format(dir_name))\n            elif fnmatch(f, \"OSZICAR\"):\n                try:\n                    d[\"oszicar\"][\"root\"] = \\\n                        Oszicar(os.path.join(dir_name, f)).as_dict()\n                except:\n                    logger.error(\"Unable to parse OSZICAR for killed run in {}.\"\n                                 .format(dir_name))\n            elif re.match(\"relax\\d\", f):\n                if os.path.exists(os.path.join(dir_name, f, \"OSZICAR\")):\n                    try:\n                        d[\"oszicar\"][f] = Oszicar(\n                            os.path.join(dir_name, f, \"OSZICAR\")).as_dict()\n                    except:\n                        logger.error(\"Unable to parse OSZICAR for killed \"\n                                     \"run in {}.\".format(dir_name))\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing a vasprun. xml file.", "response": "def process_vasprun(self, dir_name, taskname, filename):\n        \"\"\"\n        Process a vasprun.xml file.\n        \"\"\"\n        vasprun_file = os.path.join(dir_name, filename)\n        if self.parse_projected_eigen and (self.parse_projected_eigen != 'final' or \\\n                             taskname == self.runs[-1]):\n            parse_projected_eigen = True\n        else:\n            parse_projected_eigen = False\n        r = Vasprun(vasprun_file,parse_projected_eigen=parse_projected_eigen)\n        d = r.as_dict()\n        d[\"dir_name\"] = os.path.abspath(dir_name)\n        d[\"completed_at\"] = \\\n            str(datetime.datetime.fromtimestamp(os.path.getmtime(\n                vasprun_file)))\n        d[\"cif\"] = str(CifWriter(r.final_structure))\n        d[\"density\"] = r.final_structure.density\n        if self.parse_dos and (self.parse_dos != 'final' \\\n                               or taskname == self.runs[-1]):\n            try:\n                d[\"dos\"] = r.complete_dos.as_dict()\n            except Exception:\n                logger.warning(\"No valid dos data exist in {}.\\n Skipping dos\"\n                               .format(dir_name))\n        if taskname == \"relax1\" or taskname == \"relax2\":\n            d[\"task\"] = {\"type\": \"aflow\", \"name\": taskname}\n        else:\n            d[\"task\"] = {\"type\": taskname, \"name\": taskname}\n        d[\"oxide_type\"] = oxide_type(r.final_structure)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generate_doc(self, dir_name, vasprun_files):\n        try:\n            fullpath = os.path.abspath(dir_name)\n            # Defensively copy the additional fields first.  This is a MUST.\n            # Otherwise, parallel updates will see the same object and inserts\n            # will be overridden!!\n            d = {k: v for k, v in self.additional_fields.items()}\n            d[\"dir_name\"] = fullpath\n            d[\"schema_version\"] = VaspToDbTaskDrone.__version__\n            d[\"calculations\"] = [\n                self.process_vasprun(dir_name, taskname, filename)\n                for taskname, filename in vasprun_files.items()]\n            d1 = d[\"calculations\"][0]\n            d2 = d[\"calculations\"][-1]\n\n            # Now map some useful info to the root level.\n            for root_key in [\"completed_at\", \"nsites\", \"unit_cell_formula\",\n                             \"reduced_cell_formula\", \"pretty_formula\",\n                             \"elements\", \"nelements\", \"cif\", \"density\",\n                             \"is_hubbard\", \"hubbards\", \"run_type\"]:\n                d[root_key] = d2[root_key]\n            d[\"chemsys\"] = \"-\".join(sorted(d2[\"elements\"]))\n\n            # store any overrides to the exchange correlation functional\n            xc = d2[\"input\"][\"incar\"].get(\"GGA\")\n            if xc:\n                xc = xc.upper()\n            d[\"input\"] = {\"crystal\": d1[\"input\"][\"crystal\"],\n                          \"is_lasph\": d2[\"input\"][\"incar\"].get(\"LASPH\", False),\n                          \"potcar_spec\": d1[\"input\"].get(\"potcar_spec\"),\n                          \"xc_override\": xc}\n            vals = sorted(d2[\"reduced_cell_formula\"].values())\n            d[\"anonymous_formula\"] = {string.ascii_uppercase[i]: float(vals[i])\n                                      for i in range(len(vals))}\n            d[\"output\"] = {\n                \"crystal\": d2[\"output\"][\"crystal\"],\n                \"final_energy\": d2[\"output\"][\"final_energy\"],\n                \"final_energy_per_atom\": d2[\"output\"][\"final_energy_per_atom\"]}\n            d[\"name\"] = \"aflow\"\n            p = d2[\"input\"][\"potcar_type\"][0].split(\"_\")\n            pot_type = p[0]\n            functional = \"lda\" if len(pot_type) == 1 else \"_\".join(p[1:])\n            d[\"pseudo_potential\"] = {\"functional\": functional.lower(),\n                                     \"pot_type\": pot_type.lower(),\n                                     \"labels\": d2[\"input\"][\"potcar\"]}\n            if len(d[\"calculations\"]) == len(self.runs) or \\\n                    list(vasprun_files.keys())[0] != \"relax1\":\n                d[\"state\"] = \"successful\" if d2[\"has_vasp_completed\"] \\\n                    else \"unsuccessful\"\n            else:\n                d[\"state\"] = \"stopped\"\n            d[\"analysis\"] = get_basic_analysis_and_error_checks(d)\n\n            sg = SpacegroupAnalyzer(Structure.from_dict(d[\"output\"][\"crystal\"]),\n                                    0.1)\n            d[\"spacegroup\"] = {\"symbol\": sg.get_space_group_symbol(),\n                               \"number\": sg.get_space_group_number(),\n                               \"point_group\": sg.get_point_group_symbol(),\n                               \"source\": \"spglib\",\n                               \"crystal_system\": sg.get_crystal_system(),\n                               \"hall\": sg.get_hall()}\n            d[\"oxide_type\"] = d2[\"oxide_type\"]\n            d[\"last_updated\"] = datetime.datetime.today()\n            return d\n        except Exception as ex:\n            import traceback\n            print(traceback.format_exc())\n            logger.error(\"Error in \" + os.path.abspath(dir_name) +\n                         \".\\n\" + traceback.format_exc())\n\n            return None", "response": "Generate a doc for a single vasp run."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of valid paths for the given path.", "response": "def get_valid_paths(self, path):\n        \"\"\"\n        There are some restrictions on the valid directory structures:\n\n        1. There can be only one vasp run in each directory. Nested directories\n           are fine.\n        2. Directories designated \"relax1\", \"relax2\" are considered to be 2\n           parts of an aflow style run.\n        3. Directories containing vasp output with \".relax1\" and \".relax2\" are\n           also considered as 2 parts of an aflow style run.\n        \"\"\"\n        (parent, subdirs, files) = path\n        if set(self.runs).intersection(subdirs):\n            return [parent]\n        if not any([parent.endswith(os.sep + r) for r in self.runs]) and \\\n                len(glob.glob(os.path.join(parent, \"vasprun.xml*\"))) > 0:\n            return [parent]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef total_size(o, handlers={}, verbose=False, count=False):\n    # How to make different types of objects iterable\n    dict_handler = lambda d: chain.from_iterable(d.items())\n    all_handlers = {tuple: iter,\n                    list: iter,\n                    deque: iter,\n                    dict: dict_handler,\n                    set: iter,\n                    frozenset: iter}\n    all_handlers.update(handlers)     # user handlers take precedence\n    seen = set()                      # track which object id's have already been seen\n    default_size = getsizeof(0)       # estimate sizeof object without __sizeof__\n\n    def sizeof(o):\n        \"Calculate size of `o` and all its children\"\n        if id(o) in seen:             # do not double count the same object\n            return 0\n        seen.add(id(o))\n        if count:\n            s = 1\n        else:\n            s = getsizeof(o, default_size)\n        # If `o` is iterable, add size of its members\n        for typ, handler in all_handlers.items():\n            if isinstance(o, typ):\n                s += sum(map(sizeof, handler(o)))\n                break\n        return s\n\n    return sizeof(o)", "response": "Returns the approximate memory footprint an object and all of its children."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse argument string as key = value pairs separated by commas.", "response": "def args_kvp_nodup(s):\n    \"\"\"Parse argument string as key=value pairs separated by commas.\n\n    :param s: Argument string\n    :return: Parsed value\n    :rtype: dict\n    :raises: ValueError for format violations or a duplicated key.\n    \"\"\"\n    if s is None:\n        return {}\n    d = {}\n    for item in [e.strip() for e in s.split(\",\")]:\n        try:\n            key, value = item.split(\"=\", 1)\n        except ValueError:\n            msg = \"argument item '{}' not in form key=value\".format(item)\n            if _argparse_is_dumb:\n                _alog.warn(msg)\n            raise ValueError(msg)\n        if key in d:\n            msg = \"Duplicate key for '{}' not allowed\".format(key)\n            if _argparse_is_dumb:\n                _alog.warn(msg)\n            raise ValueError(msg)\n        d[key] = value\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef walk(self, o):\n        if isinstance(o, dict):\n            d = o if self._dx is None else self._dx(o)\n            return {k: self.walk(v) for k, v in d.items()}\n        elif isinstance(o, list):\n            return [self.walk(v) for v in o]\n        else:\n            return o if self._vx is None else self._vx(o)", "response": "Walk a dict & transform.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dict_expand(o):\n        r = {}\n        for k, v in o.items():\n            if isinstance(k, str):\n                k = k.replace('$', '_')\n            if \".\" in k:\n                sub_r, keys = r, k.split('.')\n                # create sub-dicts until last part of key\n                for k2 in keys[:-1]:\n                    sub_r[k2] = {}\n                    sub_r = sub_r[k2]  # descend\n                    # assign last part of key to value\n                sub_r[keys[-1]] = v\n            else:\n                r[k] = v\n        return r", "response": "Expand keys in a dict with. in them into a dict with. in them into a dict with the keys a. b. c and foo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nswitching to another collection.", "response": "def collection_name(self, value):\n        \"\"\"Switch to another collection.\n        Note that you may have to set the aliases and default properties if the\n        schema of the new collection differs from the current collection.\n        \"\"\"\n        self._collection_name = value\n        self._mongo_coll = self.db[value]\n        self.collection = TrackedCollection(self._mongo_coll, operation=self._t_op,\n                                            field=self._t_field)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tracked_find(self, *args, **kwargs):\n        _log.info(\"tracked_find.begin\")\n        # if tracking is off, just call find (ie do nothing)\n        if self._tracking_off:\n            _log.info(\"tracked_find.end, tracking=off\")\n            return self._coll_find(*args, **kwargs)\n        # otherwise do somethin' real\n        # fish 'filter' out of args or kwargs\n        if len(args) > 0:\n            filt = args[0]\n        else:\n            if 'filter' not in kwargs:\n                kwargs['filter'] = {}\n            filt = kwargs['filter']\n        # update filter with tracker query\n        filt.update(self._mark.query)\n        # delegate to \"real\" find()\n        _log.info(\"tracked_find.end, call: {}.find(args={} kwargs={})\".format(self._coll.name, args, kwargs))\n        return self._coll_find(*args, **kwargs)", "response": "Replacement for regular find."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the position of the record in the collection.", "response": "def update(self):\n        \"\"\"Update the position of the mark in the collection.\n\n        :return: this object, for chaining\n        :rtype: Mark\n        \"\"\"\n        rec = self._c.find_one({}, {self._fld: 1}, sort=[(self._fld, -1)], limit=1)\n        if rec is None:\n            self._pos = self._empty_pos()\n        elif not self._fld in rec:\n            _log.error(\"Tracking field not found. field={} collection={}\"\n                       .format(self._fld, self._c.name))\n            _log.warn(\"Continuing without tracking\")\n            self._pos = self._empty_pos()\n        else:\n            self._pos = {self._fld: rec[self._fld]}\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_dict(self):\n        return {self.FLD_OP: self._op.name,\n                self.FLD_MARK: self._pos,\n                self.FLD_FLD: self._fld}", "response": "Representation as a dict for JSON serialization."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconstruct a Mark object from a dictionary.", "response": "def from_dict(cls, coll, d):\n        \"\"\"Construct from dict\n\n        :param coll: Collection for the mark\n        :param d: Input\n        :type d: dict\n        :return: new instance\n        :rtype: Mark\n        \"\"\"\n        return Mark(collection=coll, operation=Operation[d[cls.FLD_OP]],\n                    pos=d[cls.FLD_MARK], field=d[cls.FLD_FLD])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef query(self):\n        q = {}\n        for field, value in self._pos.items():\n            if value is None:\n                q.update({field: {'$exists': True}})\n            else:\n                q.update({field: {'$gt': value}})\n        return q", "response": "A mongdb query expression to find all records with higher values\n        for this mark s fields in the collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(self):\n        if self._track is None:\n            self._track = self.db[self.tracking_collection_name]", "response": "Create tracking collection. Does nothing if tracking collection already exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, mark):\n        self._check_exists()\n        obj = mark.as_dict()\n        try:\n            # Make a 'filter' to find/update existing record, which uses\n            # the field name and operation (but not the position).\n            filt = {k: obj[k] for k in (mark.FLD_FLD, mark.FLD_OP)}\n            _log.debug(\"save: upsert-spec={} upsert-obj={}\".format(filt, obj))\n            self._track.update(filt, obj, upsert=True)\n        except pymongo.errors.PyMongoError as err:\n            raise DBError(\"{}\".format(err))", "response": "Save a position in this collection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrieve(self, operation, field=None):\n        obj = self._get(operation, field)\n        if obj is None:\n            # empty Mark instance\n            return Mark(collection=self.collection, operation=operation, field=field)\n        return Mark.from_dict(self.collection, obj)", "response": "Retrieve a position in this collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the tracked position for a given operation and field.", "response": "def _get(self, operation, field):\n        \"\"\"Get tracked position for a given operation and field.\"\"\"\n        self._check_exists()\n        query = {Mark.FLD_OP: operation.name,\n                 Mark.FLD_MARK + \".\" + field: {\"$exists\": True}}\n        return self._track.find_one(query)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef diff(self, c1, c2, only_missing=False, only_values=False, allow_dup=False):\n        # Connect.\n        _log.info(\"connect.start\")\n        if isinstance(c1, QueryEngine):\n            engines = [c1, c2]\n        else:\n            engines = []\n            for cfg in c1, c2:\n                settings = util.get_settings(cfg)\n                if not normalize_auth(settings):\n                    _log.warn(\"Config file {} does not have a username/password\".format(cfg))\n                settings[\"aliases_config\"] = {\"aliases\": {}, \"defaults\": {}}\n                engine = QueryEngine(**settings)\n                engines.append(engine)\n        _log.info(\"connect.end\")\n\n        # Query DB.\n        keys = [set(), set()]\n        eqprops = [{}, {}]\n        numprops = [{}, {}]\n\n        # Build query fields.\n        fields = dict.fromkeys(self._info + self._all_props + [self._key_field], True)\n        if not '_id' in fields:  # explicitly remove _id if not given\n            fields['_id'] = False\n\n        # Initialize for query loop.\n        info = {}  # per-key information\n        has_info, has_props = bool(self._info), bool(self._all_props)\n        has_numprops, has_eqprops = bool(self._prop_deltas), bool(self._props)\n        _log.info(\"query.start query={} fields={}\".format(self._filter, fields))\n        t0 = time.time()\n\n        # Main query loop.\n        for i, coll in enumerate(engines):\n            _log.debug(\"collection {:d}\".format(i))\n            count, missing_props = 0, 0\n            for rec in coll.query(criteria=self._filter, properties=fields):\n                count += 1\n                # Extract key from record.\n                try:\n                    key = rec[self._key_field]\n                except KeyError:\n                    _log.critical(\"Key '{}' not found in record: {}. Abort.\".format(\n                        self._key_field, rec))\n                    return {}\n                if not allow_dup and key in keys[i]:\n                    raise ValueError(\"Duplicate key: {}\".format(key))\n                keys[i].add(key)\n                # Extract numeric properties.\n                if has_numprops:\n                    pvals = {}\n                    for pkey in self._prop_deltas.keys():\n                        try:\n                            pvals[pkey] = float(rec[pkey])\n                        except KeyError:\n                            #print(\"@@ missing {} on {}\".format(pkey, rec))\n                            missing_props += 1\n                            continue\n                        except (TypeError, ValueError):\n                            raise ValueError(\"Not a number: collection={c} key={k} {p}='{v}'\"\n                                             .format(k=key, c=(\"old\", \"new\")[i], p=pkey, v=rec[pkey]))\n                    numprops[i][key] = pvals\n                # Extract properties for exact match.\n                if has_eqprops:\n                    try:\n                        propval = tuple([(p, str(rec[p])) for p in self._props])\n                    except KeyError:\n                        missing_props += 1\n                        #print(\"@@ missing {} on {}\".format(pkey, rec))\n                        continue\n                    eqprops[i][key] = propval\n\n                # Extract informational fields.\n                if has_info:\n                    if key not in info:\n                        info[key] = {}\n                    for k in self._info:\n                        info[key][k] = rec[k]\n\n            # Stop if we don't have properties on any record at all\n            if 0 < count == missing_props:\n                _log.critical(\"Missing one or more properties on all {:d} records\"\n                              .format(count))\n                return {}\n            # ..but only issue a warning for partially missing properties.\n            elif missing_props > 0:\n                _log.warn(\"Missing one or more properties for {:d}/{:d} records\"\n                          .format(missing_props, count))\n        t1 = time.time()\n        _log.info(\"query.end sec={:f}\".format(t1 - t0))\n\n        # Compute missing and new keys.\n        if only_values:\n            missing, new = [], []\n        else:\n            _log.debug(\"compute_difference.start\")\n            missing, new = keys[0] - keys[1], []\n            if not only_missing:\n                new = keys[1] - keys[0]\n            _log.debug(\"compute_difference.end\")\n\n        # Compute mis-matched properties.\n        if has_props:\n            changed = self._changed_props(keys, eqprops, numprops, info,\n                                          has_eqprops=has_eqprops, has_numprops=has_numprops)\n        else:\n            changed = []\n\n        # Build result.\n        _log.debug(\"build_result.begin\")\n        result = {}\n        if not only_values:\n            result[self.MISSING] = []\n            for key in missing:\n                rec = {self._key_field: key}\n                if has_info:\n                    rec.update(info.get(key, {}))\n                result[self.MISSING].append(rec)\n            if not only_missing:\n                result[self.NEW] = []\n                for key in new:\n                    rec = {self._key_field: key}\n                    if has_info:\n                        rec.update(info.get(key, {}))\n                    result[self.NEW].append(rec)\n        result[self.CHANGED] = changed\n        _log.debug(\"build_result.end\")\n\n        return result", "response": "Perform a difference between two collections."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nswitching to another collection.", "response": "def collection_name(self, value):\n        \"\"\"Switch to another collection.\n        Note that you may have to set the aliases and default properties if the\n        schema of the new collection differs from the current collection.\n        \"\"\"\n        self._collection_name = value\n        self.collection = self.db[value]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_aliases_and_defaults(self, aliases_config=None,\n                                 default_properties=None):\n        \"\"\"\n        Set the alias config and defaults to use. Typically used when\n        switching to a collection with a different schema.\n\n        Args:\n            aliases_config:\n                An alias dict to use. Defaults to None, which means the default\n                aliases defined in \"aliases.json\" is used. See constructor\n                for format.\n            default_properties:\n                List of property names (strings) to use by default, if no\n                properties are given to the 'properties' argument of\n                query().\n        \"\"\"\n        if aliases_config is None:\n            with open(os.path.join(os.path.dirname(__file__),\n                                   \"aliases.json\")) as f:\n                d = json.load(f)\n                self.aliases = d.get(\"aliases\", {})\n                self.default_criteria = d.get(\"defaults\", {})\n        else:\n            self.aliases = aliases_config.get(\"aliases\", {})\n            self.default_criteria = aliases_config.get(\"defaults\", {})\n        # set default properties\n        if default_properties is None:\n            self._default_props, self._default_prop_dict = None, None\n        else:\n            self._default_props, self._default_prop_dict = \\\n                self._parse_properties(default_properties)", "response": "Sets the aliases and default criteria for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_entries_in_system(self, elements, inc_structure=False,\n                              optional_data=None, additional_criteria=None):\n        \"\"\"\n        Gets all entries in a chemical system, e.g. Li-Fe-O will return all\n        Li-O, Fe-O, Li-Fe, Li-Fe-O compounds.\n\n        .. note::\n\n            The get_entries_in_system and get_entries  methods should be used\n            with care. In essence, all entries, GGA, GGA+U or otherwise,\n            are returned.  The dataset is very heterogeneous and not\n            directly comparable.  It is highly recommended that you perform\n            post-processing using pymatgen.entries.compatibility.\n\n        Args:\n            elements:\n                Sequence of element symbols, e.g. ['Li','Fe','O']\n            inc_structure:\n                Optional parameter as to whether to include a structure with\n                the ComputedEntry. Defaults to False. Use with care - including\n                structures with a large number of entries can potentially slow\n                down your code to a crawl.\n            optional_data:\n                Optional data to include with the entry. This allows the data\n                to be access via entry.data[key].\n            additional_criteria:\n                Added ability to provide additional criteria other than just\n                the chemical system.\n\n        Returns:\n            List of ComputedEntries in the chemical system.\n        \"\"\"\n        chemsys_list = []\n        for i in range(len(elements)):\n            for combi in itertools.combinations(elements, i + 1):\n                chemsys = \"-\".join(sorted(combi))\n                chemsys_list.append(chemsys)\n        crit = {\"chemsys\": {\"$in\": chemsys_list}}\n        if additional_criteria is not None:\n            crit.update(additional_criteria)\n        return self.get_entries(crit, inc_structure,\n                                optional_data=optional_data)", "response": "Get all entries in a chemical system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_entries(self, criteria, inc_structure=False, optional_data=None):\n        all_entries = list()\n        optional_data = [] if not optional_data else list(optional_data)\n        optional_data.append(\"oxide_type\")\n        fields = [k for k in optional_data]\n        fields.extend([\"task_id\", \"unit_cell_formula\", \"energy\", \"is_hubbard\",\n                       \"hubbards\", \"pseudo_potential.labels\",\n                       \"pseudo_potential.functional\", \"run_type\",\n                       \"input.is_lasph\", \"input.xc_override\",\n                       \"input.potcar_spec\"])\n        if inc_structure:\n            fields.append(\"output.crystal\")\n\n        for c in self.query(fields, criteria):\n            func = c[\"pseudo_potential.functional\"]\n            labels = c[\"pseudo_potential.labels\"]\n            symbols = [\"{} {}\".format(func, label) for label in labels]\n            parameters = {\"run_type\": c[\"run_type\"],\n                          \"is_hubbard\": c[\"is_hubbard\"],\n                          \"hubbards\": c[\"hubbards\"],\n                          \"potcar_symbols\": symbols,\n                          \"is_lasph\": c.get(\"input.is_lasph\") or False,\n                          \"potcar_spec\": c.get(\"input.potcar_spec\"),\n                          \"xc_override\": c.get(\"input.xc_override\")}\n            optional_data = {k: c[k] for k in optional_data}\n            if inc_structure:\n                struct = Structure.from_dict(c[\"output.crystal\"])\n                entry = ComputedStructureEntry(struct, c[\"energy\"],\n                                               0.0, parameters=parameters,\n                                               data=optional_data,\n                                               entry_id=c[\"task_id\"])\n            else:\n                entry = ComputedEntry(Composition(c[\"unit_cell_formula\"]),\n                                      c[\"energy\"], 0.0, parameters=parameters,\n                                      data=optional_data,\n                                      entry_id=c[\"task_id\"])\n            all_entries.append(entry)\n\n        return all_entries", "response": "Get the entries in the system that satisfy the given criteria."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_criteria(self, criteria):\n        if criteria is None:\n            return dict()\n        parsed_crit = dict()\n        for k, v in self.default_criteria.items():\n            if k not in criteria:\n                parsed_crit[self.aliases.get(k, k)] = v\n\n        for key, crit in list(criteria.items()):\n            if key in [\"normalized_formula\", \"reduced_cell_formula\"]:\n                comp = Composition(crit)\n                parsed_crit[\"pretty_formula\"] = comp.reduced_formula\n            elif key == \"unit_cell_formula\":\n                comp = Composition(crit)\n                crit = comp.as_dict()\n                for el, amt in crit.items():\n                    parsed_crit[\"{}.{}\".format(self.aliases[key], el)] = amt\n                parsed_crit[\"nelements\"] = len(crit)\n                parsed_crit['pretty_formula'] = comp.reduced_formula\n            elif key in [\"$or\", \"$and\"]:\n                parsed_crit[key] = [self._parse_criteria(m) for m in crit]\n            else:\n                parsed_crit[self.aliases.get(key, key)] = crit\n        return parsed_crit", "response": "Internal method to perform mapping of criteria to proper mongo queries of the species."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping for pymongo. Collection. ensure_index", "response": "def ensure_index(self, key, unique=False):\n        \"\"\"Wrapper for pymongo.Collection.ensure_index\n        \"\"\"\n        return self.collection.ensure_index(key, unique=unique)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying the database for the specified properties and criteria.", "response": "def query(self, properties=None, criteria=None, distinct_key=None,\n              **kwargs):\n        \"\"\"\n        Convenience method for database access.  All properties and criteria\n        can be specified using simplified names defined in Aliases.  You can\n        use the supported_properties property to get the list of supported\n        properties.\n\n        Results are returned as an iterator of dicts to ensure memory and cpu\n        efficiency.\n\n        Note that the dict returned have keys also in the simplified names\n        form, not in the mongo format. For example, if you query for\n        \"analysis.e_above_hull\", the returned result must be accessed as\n        r['analysis.e_above_hull'] instead of mongo's\n        r['analysis']['e_above_hull']. This is a *feature* of the query engine\n        to allow simple access to deeply nested docs without having to resort\n        to some recursion to go deep into the result.\n\n        However, if you query for 'analysis', the entire 'analysis' key is\n        returned as r['analysis'] and then the subkeys can be accessed in the\n        usual form, i.e., r['analysis']['e_above_hull']\n\n        :param properties: Properties to query for. Defaults to None which means all supported properties.\n        :param criteria: Criteria to query for as a dict.\n        :param distinct_key: If not None, the key for which to get distinct results\n        :param \\*\\*kwargs: Other kwargs supported by pymongo.collection.find.\n            Useful examples are limit, skip, sort, etc.\n        :return: A QueryResults Iterable, which is somewhat like pymongo's\n            cursor except that it performs mapping. In general, the dev does\n            not need to concern himself with the form. It is sufficient to know\n            that the results are in the form of an iterable of dicts.\n        \"\"\"\n        if properties is not None:\n            props, prop_dict = self._parse_properties(properties)\n        else:\n            props, prop_dict = None, None\n\n        crit = self._parse_criteria(criteria)\n        if self.query_post:\n            for func in self.query_post:\n                func(crit, props)\n        cur = self.collection.find(filter=crit, projection=props, **kwargs)\n\n        if distinct_key is not None:\n            cur = cur.distinct(distinct_key)\n            return QueryListResults(prop_dict, cur, postprocess=self.result_post)\n        else:\n            return QueryResults(prop_dict, cur, postprocess=self.result_post)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_properties(self, properties):\n        props = {}\n        # TODO: clean up prop_dict?\n        prop_dict = OrderedDict()\n        # We use a dict instead of list to provide for a richer syntax\n        for p in properties:\n            if p in self.aliases:\n                if isinstance(properties, dict):\n                    props[self.aliases[p]] = properties[p]\n                else:\n                    props[self.aliases[p]] = 1\n                prop_dict[p] = self.aliases[p].split(\".\")\n            else:\n                if isinstance(properties, dict):\n                    props[p] = properties[p]\n                else:\n                    props[p] = 1\n                prop_dict[p] = p.split(\".\")\n        # including a lower-level key after a higher level key e.g.:\n        # {'output': 1, 'output.crystal': 1} instead of\n        # {'output.crystal': 1, 'output': 1}\n        # causes mongo to skip the other higher level keys.\n        # this is a (sketchy) workaround for that. Note this problem\n        # doesn't appear often in python2 because the dictionary ordering\n        # is more stable.\n        props = OrderedDict(sorted(props.items(), reverse=True))\n        return props, prop_dict", "response": "Parse the properties of a resource."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries the database for a single document.", "response": "def query_one(self, *args, **kwargs):\n        \"\"\"Return first document from :meth:`query`, with same parameters.\n        \"\"\"\n        for r in self.query(*args, **kwargs):\n            return r\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a structure from the database given the task id.", "response": "def get_structure_from_id(self, task_id, final_structure=True):\n        \"\"\"\n        Returns a structure from the database given the task id.\n\n        Args:\n            task_id:\n                The task_id to query for.\n            final_structure:\n                Whether to obtain the final or initial structure. Defaults to\n                True.\n        \"\"\"\n        args = {'task_id': task_id}\n        field = 'output.crystal' if final_structure else 'input.crystal'\n        results = tuple(self.query([field], args))\n\n        if len(results) > 1:\n            raise QueryError(\"More than one result found for task_id {}!\".format(task_id))\n        elif len(results) == 0:\n            raise QueryError(\"No structure found for task_id {}!\".format(task_id))\n        c = results[0]\n        return Structure.from_dict(c[field])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes a QueryEngine from a JSON config file generated using mgdb init.", "response": "def from_config(config_file, use_admin=False):\n        \"\"\"\n        Initialize a QueryEngine from a JSON config file generated using mgdb\n        init.\n\n        Args:\n            config_file:\n                Filename of config file.\n            use_admin:\n                If True, the admin user and password in the config file is\n                used. Otherwise, the readonly_user and password is used.\n                Defaults to False.\n\n        Returns:\n            QueryEngine\n        \"\"\"\n        with open(config_file) as f:\n            d = json.load(f)\n            user = d[\"admin_user\"] if use_admin else d[\"readonly_user\"]\n            password = d[\"admin_password\"] if use_admin \\\n                else d[\"readonly_password\"]\n            return QueryEngine(\n                host=d[\"host\"], port=d[\"port\"], database=d[\"database\"],\n                user=user, password=password, collection=d[\"collection\"],\n                aliases_config=d.get(\"aliases_config\", None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dos_from_id(self, task_id):\n        args = {'task_id': task_id}\n        fields = ['calculations']\n        structure = self.get_structure_from_id(task_id)\n        dosid = None\n        for r in self.query(fields, args):\n            dosid = r['calculations'][-1]['dos_fs_id']\n        if dosid is not None:\n            self._fs = gridfs.GridFS(self.db, 'dos_fs')\n            with self._fs.get(dosid) as dosfile:\n                s = dosfile.read()\n                try:\n                    d = json.loads(s)\n                except:\n                    s = zlib.decompress(s)\n                    d = json.loads(s.decode(\"utf-8\"))\n                tdos = Dos.from_dict(d)\n                pdoss = {}\n                for i in range(len(d['pdos'])):\n                    ados = d['pdos'][i]\n                    all_ados = {}\n                    for j in range(len(ados)):\n                        orb = Orbital(j)\n                        odos = ados[str(orb)]\n                        all_ados[orb] = {Spin(int(k)): v\n                                         for k, v\n                                         in odos['densities'].items()}\n                    pdoss[structure[i]] = all_ados\n                return CompleteDos(structure, tdos, pdoss)\n        return None", "response": "Get the Dos object for a given task_id."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wrapper(self, func):\n        def wrapped(*args, **kwargs):\n            ret_val = func(*args, **kwargs)\n            if isinstance(ret_val, pymongo.cursor.Cursor):\n                ret_val = self.from_cursor(ret_val)\n            return ret_val\n\n        return wrapped", "response": "A function that wraps all callable objects returned by self. __getattr__."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _mapped_result(self, r):\n        # Apply result_post funcs for pulling out sandbox properties\n        for func in self._pproc:\n            func(r)\n        # If we haven't asked for specific properties, just return object\n        if not self._prop_dict:\n            result = r\n        else:\n            result = dict()\n            # Map aliased keys back to original key\n            for k, v in self._prop_dict.items():\n                try:\n                    result[k] = self._mapped_result_path(v[1:], data=r[v[0]])\n                except (IndexError, KeyError, ValueError):\n                    result[k] = None\n        return result", "response": "Transform a result. r to a dictionary of key - value pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds schemas from files in path.", "response": "def add_schemas(path, ext=\"json\"):\n    \"\"\"Add schemas from files in 'path'.\n\n    :param path: Path with schema files. Schemas are named by their file,\n                 with the extension stripped. e.g., if path is \"/tmp/foo\",\n                 then the schema in \"/tmp/foo/bar.json\" will be named \"bar\".\n    :type path: str\n    :param ext: File extension that identifies schema files\n    :type ext: str\n    :return: None\n    :raise: SchemaPathError, if no such path. SchemaParseError, if a schema\n            is not valid JSON.\n    \"\"\"\n    if not os.path.exists(path):\n        raise SchemaPathError()\n    filepat = \"*.\" + ext if ext else \"*\"\n    for f in glob.glob(os.path.join(path, filepat)):\n        with open(f, 'r') as fp:\n            try:\n                schema = json.load(fp)\n            except ValueError:\n                raise SchemaParseError(\"error parsing '{}'\".format(f))\n        name = os.path.splitext(os.path.basename(f))[0]\n        schemata[name] = Schema(schema)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_schema(file_or_fp):\n    fp = open(file_or_fp, 'r') if isinstance(file_or_fp, str) else file_or_fp\n    obj = json.load(fp)\n    schema = Schema(obj)\n    return schema", "response": "Load schema from file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert our compact schema representation to the standard but more verbose.", "response": "def json_schema(self, **add_keys):\n        \"\"\"Convert our compact schema representation to the standard, but more verbose,\n        JSON Schema standard.\n\n        Example JSON schema: http://json-schema.org/examples.html\n        Core standard: http://json-schema.org/latest/json-schema-core.html\n\n        :param add_keys: Key, default value pairs to add in,\n                         e.g. description=\"\"\n        \"\"\"\n        self._json_schema_keys = add_keys\n        if self._json_schema is None:\n            self._json_schema = self._build_schema(self._schema)\n        return self._json_schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _jstype(self, stype, sval):\n        if stype == self.IS_LIST:\n            return \"array\"\n        if stype == self.IS_DICT:\n            return \"object\"\n        if isinstance(sval, Scalar):\n            return sval.jstype\n        # it is a Schema, so return type of contents\n        v = sval._schema\n        return self._jstype(self._whatis(v), v)", "response": "Get JavaScript name for given data type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_schema_dir(db_version=1):\n    v = str(db_version)\n    return os.path.join(_top_dir, '..', 'schemata', 'versions', v)", "response": "Get path to directory with schemata."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_schema_file(db_version=1, db=\"mg_core\", collection=\"materials\"):\n    d = get_schema_dir(db_version=db_version)\n    schemafile = \"{}.{}.json\".format(db, collection)\n    f = open(os.path.join(d, schemafile), \"r\")\n    return f", "response": "Get file with appropriate schema."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_settings(infile):\n    settings = yaml.load(_as_file(infile))\n    if not hasattr(settings, 'keys'):\n        raise ValueError(\"Settings not found in {}\".format(infile))\n\n    # Processing of namespaced parameters in .pmgrc.yaml.\n    processed_settings = {}\n    for k, v in settings.items():\n        if k.startswith(\"PMG_DB_\"):\n            processed_settings[k[7:].lower()] = v\n        else:\n            processed_settings[k] = v\n    auth_aliases(processed_settings)\n    return processed_settings", "response": "Read settings from input file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef auth_aliases(d):\n    for alias, real in ((USER_KEY, \"readonly_user\"),\n                        (PASS_KEY, \"readonly_password\")):\n        if alias in d:\n            d[real] = d[alias]\n            del d[alias]", "response": "Interpret user and password aliases."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform the readonly admin user and password to simple user and password.", "response": "def normalize_auth(settings, admin=True, readonly=True, readonly_first=False):\n    \"\"\"Transform the readonly/admin user and password to simple user/password,\n    as expected by QueryEngine. If return value is true, then\n    admin or readonly password will be in keys \"user\" and \"password\".\n\n    :param settings: Connection settings\n    :type settings: dict\n    :param admin: Check for admin password\n    :param readonly: Check for readonly password\n    :param readonly_first: Check for readonly password before admin\n    :return: Whether user/password were found\n    :rtype: bool\n    \"\"\"\n    U, P = USER_KEY, PASS_KEY\n    # If user/password, un-prefixed, exists, do nothing.\n    if U in settings and P in settings:\n        return True\n\n    # Set prefixes\n    prefixes = []\n    if readonly_first:\n        if readonly:\n            prefixes.append(\"readonly_\")\n        if admin:\n            prefixes.append(\"admin_\")\n    else:\n        if admin:\n            prefixes.append(\"admin_\")\n        if readonly:\n            prefixes.append(\"readonly_\")\n\n    # Look for first user/password matching.\n    found = False\n    for pfx in prefixes:\n        ukey, pkey = pfx + U, pfx + P\n        if ukey in settings and pkey in settings:\n            settings[U] = settings[ukey]\n            settings[P] = settings[pkey]\n            found = True\n            break\n\n    return found"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send(self, text, fmt):\n        main_fmt, sub_fmt = fmt.split('/')\n        if sub_fmt.lower() == \"text\":\n            msg = MIMEText(text, \"plain\")\n        elif sub_fmt.lower() == \"html\":\n            msg = MIMEText(text, \"html\")\n        else:\n            raise ValueError(\"Unknown message format: {}\".format(fmt))\n        msg['Subject'] = self._subject\n        msg['From'] = self._sender\n        msg['To'] = ', '.join(self._recipients)\n        if self._port is None:\n            conn_kwargs = dict(host=self._server)\n        else:\n            conn_kwargs = dict(host=self._server, port=self._port)\n        self._log.info(\"connect to email server {}\".format(conn_kwargs))\n        try:\n            s = smtplib.SMTP(**conn_kwargs)\n            #s.set_debuglevel(2)\n            refused = s.sendmail(self._sender, self._recipients, msg.as_string())\n            if refused:\n                self._log.warn(\"Email to {:d} recipients was refused\".format(len(refused)))\n                for person, (code, msg) in refused.items():\n                    self._log(\"Email to {p} was refused ({c}): {m}\".format(p=person, c=code, m=msg))\n            s.quit()\n            n_recip = len(self._recipients)\n        except Exception as err:\n            self._log.error(\"connection to SMTP server failed: {}\".format(err))\n            n_recip = 0\n        return n_recip", "response": "Send the text to the list of recipients."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef result_subsets(self, rs):\n        keyset, maxwid = set(), {}\n        for r in rs:\n            key = tuple(sorted(r.keys()))\n            keyset.add(key)\n            if key not in maxwid:\n                maxwid[key] = [len(k) for k in key]\n            for i, k in enumerate(key):\n                strlen = len(\"{}\".format(r[k]))\n                maxwid[key][i] = max(maxwid[key][i], strlen)\n        return keyset, maxwid", "response": "Break a result set into subsets with the same keys."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ordered_cols(self, columns, section):\n        columns = list(columns)  # might be a tuple\n        fixed_cols = [self.key]\n        if section.lower() == \"different\":\n            fixed_cols.extend([Differ.CHANGED_MATCH_KEY, Differ.CHANGED_OLD, Differ.CHANGED_NEW])\n        map(columns.remove, fixed_cols)\n        columns.sort()\n        return fixed_cols + columns", "response": "Return ordered list of columns from given columns and the name of the section\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsort the rows according to the section.", "response": "def sort_rows(self, rows, section):\n        \"\"\"Sort the rows, as appropriate for the section.\n\n        :param rows: List of tuples (all same length, same values in each position)\n        :param section: Name of section, should match const in Differ class\n        :return: None; rows are sorted in-place\n        \"\"\"\n        #print(\"@@ SORT ROWS:\\n{}\".format(rows))\n        # Section-specific determination of sort key\n        if section.lower() == Differ.CHANGED.lower():\n            sort_key = Differ.CHANGED_DELTA\n        else:\n            sort_key = None\n        if sort_key is not None:\n            rows.sort(key=itemgetter(sort_key))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef document(self, result):\n        self._add_meta(result)\n        walker = JsonWalker(JsonWalker.value_json, JsonWalker.dict_expand)\n        r = walker.walk(result)\n        return r", "response": "Build dict for MongoDB expanding result keys as we go."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format(self, result):\n        css = \"\\n\".join(self.css)\n        content = \"{}{}\".format(self._header(), self._body(result))\n        if self._email:\n            text = \"\"\"<!DOCTYPE html>\n            <html>\n            <div width=\"100%\" style=\"{sty}\">{content}</div>\n            </html>\n            \"\"\".format(css=css, content=content, sty=self.styles[\"content\"][\"_\"])\n        else:\n            text = \"\"\"<html>\n            <head><style>{css}</style></head>\n            <body>{content}</body>\n            </html>\n            \"\"\".format(css=css, content=content)\n        return text", "response": "Generate HTML report.\n\n        :return: Report body\n        :rtype: str"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format(self, result):\n        m = self.meta\n        lines = ['-' * len(self.TITLE),\n                 self.TITLE,\n                 '-' * len(self.TITLE),\n                 \"Compared: {db1} <-> {db2}\".format(**m),\n                 \"Filter: {filter}\".format(**m),\n                 \"Run time: {start_time} -- {end_time} ({elapsed:.1f} sec)\".format(**m),\n                 \"\"]\n        for section in result.keys():\n            lines.append(\"* \" + section.title())\n            indent = \" \" * 4\n            if len(result[section]) == 0:\n                lines.append(\"{}EMPTY\".format(indent))\n            else:\n                keyset, maxwid = self.result_subsets(result[section])\n                for columns in keyset:\n                    ocol = self.ordered_cols(columns, section)\n                    mw = maxwid[columns]\n                    mw_i = [columns.index(c) for c in ocol]  # reorder indexes\n                    fmt = '  '.join([\"{{:{:d}s}}\".format(mw[i]) for i in mw_i])\n                    lines.append(\"\")\n                    lines.append(indent + fmt.format(*ocol))\n                    lines.append(indent + '-_' * (sum(mw)/2 + len(columns)))\n                    rows = result[section]\n                    self.sort_rows(rows, section)\n                    for r in rows:\n                        key = tuple(sorted(r.keys()))\n                        if key == columns:\n                            values = [str(r[k]) for k in ocol]\n                            lines.append(indent + fmt.format(*values))\n        return '\\n'.join(lines)", "response": "Generate plain text report."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and return a new query engine object from the given DBConfig object.", "response": "def create_query_engine(config, clazz):\n    \"\"\"Create and return new query engine object from the\n    given `DBConfig` object.\n\n    :param config: Database configuration\n    :type config: dbconfig.DBConfig\n    :param clazz: Class to use for creating query engine. Should\n                  act like query_engine.QueryEngine.\n    :type clazz: class\n    :return: New query engine\n    \"\"\"\n    try:\n        qe = clazz(**config.settings)\n    except Exception as err:\n        raise CreateQueryEngineError(clazz, config.settings, err)\n    return qe"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_path(self, path, pattern=\"*.json\"):\n        if os.path.isdir(path):\n            configs = glob.glob(_opj(path, pattern))\n        else:\n            configs = [path]\n        for config in configs:\n                cfg = dbconfig.DBConfig(config_file=config)\n                cs = cfg.settings\n                if dbconfig.DB_KEY not in cs:\n                    raise ValueError(\"No database in '{}'\".format(config))\n                if dbconfig.COLL_KEY in cs:\n                    name = \"{}.{}\".format(cs[dbconfig.DB_KEY],\n                                          cs[dbconfig.COLL_KEY])\n                else:\n                    name = cs[dbconfig.DB_KEY]\n                self.add(name, cfg)\n        return self", "response": "Add configuration files in path."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a configuration object to the current object.", "response": "def add(self, name, cfg, expand=False):\n        \"\"\"Add a configuration object.\n\n        :param name: Name for later retrieval\n        :param cfg: Configuration object\n        :param expand: Flag for adding sub-configs for each sub-collection.\n                       See discussion in method doc.\n        :return: self, for chaining\n        :raises: CreateQueryEngineError (only if expand=True)\n        \"\"\"\n        self._d[name] = cfg\n        if expand:\n            self.expand(name)\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexpand config for name by adding a sub - configuration for everyCOOKIE that is not in the database.", "response": "def expand(self, name):\n        \"\"\"Expand config for `name` by adding a sub-configuration for every\n        dot-separated collection \"below\" the given one (or all, if none given).\n\n        For example, for a database 'mydb' with collections\n            ['spiderman.amazing', 'spiderman.spectacular', 'spiderman2']\n        and a configuration\n            {'host':'foo', 'database':'mydb', 'collection':'spiderman'}\n        then `expand(\"mydb.spiderman\")` would add keys for 'spiderman.amazing'\n        and 'spiderman.spectacular', but *not* 'spiderman2'.\n\n        :param name: Name, or glob-style pattern, for DB configurations.\n        :type name: basestring\n        :return: None\n        :raises: KeyError (if no such configuration)\n        \"\"\"\n        if self._is_pattern(name):\n            expr = re.compile(self._pattern_to_regex(name))\n            for cfg_name in self._d.keys():\n                if expr.match(cfg_name):\n                    self._expand(cfg_name)\n        else:\n            self._expand(name)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _expand(self, name):\n        cfg = self._d[name]\n        if cfg.collection is None:\n            base_coll = ''\n        else:\n            base_coll = cfg.collection + self.SEP\n        qe = self._get_qe(name, cfg)\n        coll, db = qe.collection, qe.db\n        cur_coll = coll.name\n        for coll_name in db.collection_names():\n            if coll_name == cur_coll or not coll_name.startswith(base_coll):\n                continue\n            ex_cfg = cfg.copy()\n            ex_cfg.collection = coll_name\n            group_name = name + self.SEP + coll_name[len(base_coll):]\n            self.add(group_name, ex_cfg, expand=False)", "response": "Perform real work of expand function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef uncache(self, name):\n        delme = []\n        if self._is_pattern(name):\n            expr = re.compile(self._pattern_to_regex(name))\n            for key, obj in self._cached.items():\n                if expr.match(key):\n                    delme.append(key)\n        else:\n            if name in self._cached:\n                delme.append(name)\n        for key in delme:\n            del self._cached[key]", "response": "Removes all created query engines that match name from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the prefix to use as a namespace for item lookup.", "response": "def set_prefix(self, prefix=None):\n        \"\"\"Set prefix to use as a namespace for item lookup.\n        A dot (.) will be automatically added to the given string.\n\n        :param prefix: Prefix, or None to unset\n        :return: None\n        \"\"\"\n        if prefix is None:\n            self._pfx = None\n        else:\n            self._pfx = prefix + self.SEP"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_qe(self, key, obj):\n        if key in self._cached:\n            return self._cached[key]\n        qe = create_query_engine(obj, self._class)\n        self._cached[key] = qe\n        return qe", "response": "Instantiate a query engine or retrieve a cached one."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef re_keys(self, pattern):\n        if not pattern.endswith(\"$\"):\n            pattern += \"$\"\n        expr = re.compile(pattern)\n        return list(filter(expr.match, self.keys()))", "response": "Find keys matching pattern."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn values whose key matches pattern", "response": "def re_get(self, pattern):\n        \"\"\"Return values whose key matches `pattern`\n\n        :param pattern: Regular expression\n        :return: Found values, as a dict.\n        \"\"\"\n        return {k: self[k] for k in self.re_keys(pattern)}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_items(self, source=None, target=None, crit=None):\n        self._target_coll = target.collection\n        if not crit:  # reduce any False-y crit value to None\n            crit = None\n        cur = source.query(criteria=crit)\n        _log.info(\"source.collection={} crit={} source_records={:d}\"\n                  .format(source.collection, crit, len(cur)))\n        return cur", "response": "Copy records from source to target collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the docstring of a function into a dictionary of parameter and return types.", "response": "def parse_fn_docstring(fn):\n    \"\"\"Get parameter and return types from function's docstring.\n\n    Docstrings must use this format::\n\n       :param foo: What is foo\n       :type foo: int\n       :return: What is returned\n       :rtype: double\n\n    :return: A map of names, each with keys 'type' and 'desc'.\n    :rtype: tuple(dict)\n    \"\"\"\n    doc = fn.__doc__\n    params, return_ = {}, {}\n    param_order = []\n    for line in doc.split(\"\\n\"):\n        line = line.strip()\n        if line.startswith(\":param\"):\n            _, name, desc = line.split(\":\", 2)\n            name = name[6:].strip()  # skip 'param '\n            params[name] = {'desc': desc.strip()}\n            param_order.append(name)\n        elif line.startswith(\":type\"):\n            _, name, desc = line.split(\":\", 2)\n            name = name[5:].strip()  # skip 'type '\n            if not name in params:\n                raise ValueError(\"'type' without 'param' for {}\".format(name))\n            params[name]['type'] = desc.strip()\n        elif line.startswith(\":return\"):\n            _1, _2, desc = line.split(\":\", 2)\n            return_['desc'] = desc\n        elif line.startswith(\":rtype\"):\n            _1, _2, desc = line.split(\":\", 2)\n            return_['type'] = desc.strip()\n    return params"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_tasks(core_collections, sandbox_collections, id_prefix, new_tasks, batch_size=100, wipe=False):\n    merged = copy.copy(sandbox_collections)\n    # create/clear target collection\n    target = merged.database[new_tasks]\n    if wipe:\n        _log.debug(\"merge_tasks.wipe.begin\")\n        target.remove()\n        merged.database['counter'].remove()\n        _log.debug(\"merge_tasks.wipe.end\")\n    # perform the merge\n    batch = []\n    for doc in core_collections.tasks.find():\n        batch.append(doc)\n        if len(batch) == batch_size:\n            target.insert(batch)\n            batch = []\n    if batch:\n        target.insert(batch)\n    batch = []\n    for doc in sandbox_collections.tasks.find():\n        doc['task_id'] = id_prefix + '-' + str(doc['task_id'])\n        batch.append(doc)\n        if len(batch) == batch_size:\n            target.insert(batch)\n            batch = []\n    if batch:\n        target.insert(batch)", "response": "Merge two collections into a temporary collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndumping a dict to a str", "response": "def alphadump(d, indent=2, depth=0):\n    \"\"\"Dump a dict to a str,\n    with keys in alphabetical order.\n    \"\"\"\n    sep = '\\n' + ' ' * depth * indent\n    return ''.join(\n        (\"{}: {}{}\".format(\n            k,\n            alphadump(d[k], depth=depth+1) if isinstance(d[k], dict)\n            else str(d[k]),\n            sep)\n         for k in sorted(d.keys()))\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates the examples against the schema.", "response": "def validate_examples(self, fail_fn):\n        \"\"\"Check the examples against the schema.\n\n        :param fail_fn: Pass failure messages to this function\n        :type fail_fn: function(str)\n        \"\"\"\n        for collection, doc in self.examples():\n            _log.debug(\"validating example in collection {}\".format(collection))\n            sch = schema.get_schema(collection)  # with more err. checking\n            result = sch.validate(doc)\n            _log.debug(\"validation result: {}\".format(\"OK\" if result is None else result))\n            if result is not None:\n                fail_fn(\"Failed to validate sample document: {}\".format(result))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the builder. :param user_kw: keywords from user :type user_kw: dict :param build_kw: internal settings :type build_kw: dict :return: Number of items processed :rtype: int", "response": "def run(self, user_kw=None, build_kw=None):\n        \"\"\"Run the builder.\n\n        :param user_kw: keywords from user\n        :type user_kw: dict\n        :param build_kw: internal settings\n        :type build_kw: dict\n        :return: Number of items processed\n        :rtype: int\n        \"\"\"\n        user_kw = {} if user_kw is None else user_kw\n        build_kw = {} if build_kw is None else build_kw\n        n = self._build(self.get_items(**user_kw), **build_kw)\n        finalized = self.finalize(self._status.has_failures())\n        if not finalized:\n            _log.error(\"Finalization failed\")\n        return n"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to database with given configuration.", "response": "def connect(self, config):\n        \"\"\"Connect to database with given configuration, which may be a dict or\n        a path to a pymatgen-db configuration.\n        \"\"\"\n        if isinstance(config, str):\n            conn = dbutil.get_database(config_file=config)\n        elif isinstance(config, dict):\n            conn = dbutil.get_database(settings=config)\n        else:\n            raise ValueError(\"Configuration, '{}',  must be a path to \"\n                             \"a configuration file or dict\".format(config))\n        return conn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build(self, items, chunk_size=10000):\n        _log.debug(\"_build, chunk_size={:d}\".format(chunk_size))\n        n, i = 0, 0\n        for i, item in enumerate(items):\n            if i == 0:\n                _log.debug(\"_build, first item\")\n            if 0 == (i + 1) % chunk_size:\n                if self._seq:\n                    self._run(0)\n                else:\n                    self._run_parallel_fn()  # process the chunk\n                if self._status.has_failures():\n                    break\n                n = i + 1\n            self._queue.put(item)\n        # process final chunk\n        if self._seq:\n            self._run(0)\n        else:\n            self._run_parallel_fn()\n        if not self._status.has_failures():\n            n = i + 1\n        return n", "response": "Builds the output in chunks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_parallel_multiprocess(self):\n        _log.debug(\"run.parallel.multiprocess.start\")\n        processes = []\n        ProcRunner.instance = self\n        for i in range(self._ncores):\n            self._status.running(i)\n            proc = multiprocessing.Process(target=ProcRunner.run, args=(i,))\n            proc.start()\n            processes.append(proc)\n        for i in range(self._ncores):\n            processes[i].join()\n            code = processes[i].exitcode\n            self._status.success(i) if 0 == code else self._status.fail(i)\n        _log.debug(\"run.parallel.multiprocess.end states={}\".format(self._status))", "response": "Run processes from queue\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _run(self, index):\n        while 1:\n            try:\n                item = self._queue.get(timeout=2)\n                self.process_item(item)\n            except Queue.Empty:\n                break\n            except Exception as err:\n                _log.error(\"In _run(): {}\".format(err))\n                if _log.isEnabledFor(logging.DEBUG):\n                    _log.error(traceback.format_exc())\n                self._status.fail(index)\n                raise\n        self._status.success(index)", "response": "This method is called by the process_item method of one thread or process\n       . It will pull an item off the queue and process it until the queue is empty."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef collection_keys(coll, sep='.'):\n    def _keys(x, pre=''):\n        for k in x:\n            yield (pre + k)\n            if isinstance(x[k], dict):\n                for nested in _keys(x[k], pre + k + sep):\n                    yield nested\n\n    return list(_keys(coll.find_one()))", "response": "Get a list of all keys in a collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat dict to a string with comma - separated values.", "response": "def csv_dict(d):\n    \"\"\"Format dict to a string with comma-separated values.\n    \"\"\"\n    if len(d) == 0:\n        return \"{}\"\n    return \"{\" + ', '.join([\"'{}': {}\".format(k, quotable(v))\n                            for k, v in d.items()]) + \"}\""}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nformat dict to key = value pairs.", "response": "def kvp_dict(d):\n    \"\"\"Format dict to key=value pairs.\n    \"\"\"\n    return ', '.join(\n        [\"{}={}\".format(k, quotable(v)) for k, v in d.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_items(self, source=None, target=None):\n        self._groups = self.shared_dict()\n        self._target_coll = target.collection\n        self._src = source\n        return source.query()", "response": "Get all records from source collection to add to target collection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_item(self, item):\n        group, value = item['group'], item['value']\n        if group in self._groups:\n            cur_val = self._groups[group]\n            self._groups[group] = max(cur_val, value)\n        else:\n            # New group. Could fetch old max. from target collection,\n            # but for the sake of illustration recalculate it from\n            # the source collection.\n            self._src.tracking = False  # examine entire collection\n            new_max = value\n            for rec in self._src.query(criteria={'group': group},\n                                       properties=['value']):\n                new_max = max(new_max, rec['value'])\n            self._src.tracking = True  # back to incremental mode\n            # calculate new max\n            self._groups[group] = new_max", "response": "Process a single item from the source collection and update the internal state of the internal state."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef finalize(self, errs):\n        for group, value in self._groups.items():\n            doc = {'group': group, 'value': value}\n            self._target_coll.update({'group': group}, doc, upsert=True)\n        return True", "response": "Update target collection with calculated maximum values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mongo_get(rec, key, default=None):\n    if not rec:\n        return default\n    if not isinstance(rec, collections.Mapping):\n        raise ValueError('input record must act like a dict')\n    if not '.' in key:\n        return rec.get(key, default)\n    for key_part in key.split('.'):\n        if not isinstance(rec, collections.Mapping):\n            return default\n        if not key_part in rec:\n            return default\n        rec = rec[key_part]\n    return rec", "response": "Get value from dict using MongoDB dot - separated path semantics."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add(self, field, op=None, val=None):\n        if field.has_subfield():\n            self._fields[field.full_name] = 1\n        else:\n            self._fields[field.name] = 1\n        if op and op.is_size() and not op.is_variable():\n            # get minimal part of array with slicing,\n            # but cannot use slice with variables\n            self._slices[field.name] = val + 1\n        if op and op.is_variable():\n            # add the variable too\n            self._fields[val] = 1", "response": "Update the report fields to include new one if it doesn t already."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_mongo(self):\n        d = copy.copy(self._fields)\n        for k, v in self._slices.items():\n            d[k] = {'$slice': v}\n        return d", "response": "Translate projection to MongoDB query form."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_violations(self, violations, record=None):\n        rec = {} if record is None else record\n        for v in violations:\n            self._viol.append((v, rec))", "response": "Adds constraint violations and associated record."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_complex_section(self, item):\n        # extract filter and constraints\n        try:\n            fltr = item[self.FILTER_SECT]\n        except KeyError:\n            raise ValueError(\"configuration requires '{}'\".format(self.FILTER_SECT))\n        sample = item.get(self.SAMPLE_SECT, None)\n        constraints = item.get(self.CONSTRAINT_SECT, None)\n\n        section = ConstraintSpecSection(fltr, constraints, sample)\n        key = section.get_key()\n        if key in self._sections:\n            self._sections[key].append(section)\n        else:\n            self._sections[key] = [section]", "response": "Add a section that has a filter and set of constraints\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates one section of a spec.", "response": "def _validate_section(self, subject, coll, parts):\n        \"\"\"Validate one section of a spec.\n\n        :param subject: Name of subject\n        :type subject: str\n        :param coll: The collection to validate\n        :type coll: pymongo.Collection\n        :param parts: Section parts\n        :type parts: Validator.SectionParts\n        :return: Group of constraint violations, if any, otherwise None\n        :rtype: ConstraintViolationGroup or None\n        \"\"\"\n        cvgroup = ConstraintViolationGroup()\n        cvgroup.subject = subject\n\n        # If the constraint is an 'import' of code, treat it differently here\n        # if self._is_python(parts):\n        #    num_found = self._run_python(cvgroup, coll, parts)\n        #    return None if num_found == 0 else cvgroup\n\n        query = parts.cond.to_mongo(disjunction=False)\n        query.update(parts.body.to_mongo())\n        cvgroup.condition = parts.cond.to_mongo(disjunction=False)\n        self._log.debug('Query spec: {}'.format(query))\n        self._log.debug('Query fields: {}'.format(parts.report_fields))\n        # Find records that violate 1 or more constraints\n        cursor = coll.find(query, parts.report_fields, **self._find_kw)\n        if parts.sampler is not None:\n            cursor = parts.sampler.sample(cursor)\n        nbytes, num_dberr, num_rec = 0, 0, 0\n        while 1:\n            try:\n                record = next(cursor)\n                nbytes += total_size(record)\n                num_rec += 1\n            except StopIteration:\n                self._log.info(\"collection {}: {:d} records, {:d} bytes, {:d} db-errors\"\n                               .format(subject, num_rec, nbytes, num_dberr))\n                break\n            except pymongo.errors.PyMongoError as err:\n                num_dberr += 1\n                if num_dberr > self._max_dberr > 0:\n                    raise DBError(\"Too many errors\")\n                self._log.warn(\"DB.{:d}: {}\".format(num_dberr, err))\n                continue\n\n            # report progress\n            if self._progress:\n                self._progress.update(num_dberr, nbytes)\n            # get reasons for badness\n            violations = self._get_violations(parts.body, record)\n            cvgroup.add_violations(violations, record)\n        return None if nbytes == 0 else cvgroup"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds the list of SectionParts for the specified constraint specification.", "response": "def _build(self, constraint_spec):\n        \"\"\"Generate queries to execute.\n\n        Sets instance variables so that Mongo query strings, etc. can now\n        be extracted from the object.\n\n        :param constraint_spec: Constraint specification\n        :type constraint_spec: ConstraintSpec\n        \"\"\"\n        self._sections = []\n\n        # For each condition in the spec\n\n        for sval in constraint_spec:\n            rpt_fld = self._base_report_fields.copy()\n            #print(\"@@ CONDS = {}\".format(sval.filters))\n            #print(\"@@ MAIN = {}\".format(sval.constraints))\n\n            # Constraints\n\n            # If the constraint is an external call to Python code\n            if self._is_python(sval.constraints):\n                query, proj = self._process_python(sval.constraints)\n                rpt_fld.update(proj.to_mongo())\n\n            # All other constraints, e.g. 'foo > 12'\n            else:\n                query = MongoQuery()\n                if sval.constraints is not None:\n                    groups = self._process_constraint_expressions(sval.constraints)\n                    projection = Projection()\n                    for cg in groups.values():\n                        for c in cg:\n                            projection.add(c.field, c.op, c.value)\n                            query.add_clause(MongoClause(c))\n                        if self._add_exists:\n                            for c in cg.existence_constraints:\n                                query.add_clause(MongoClause(c, exists_main=True))\n                    rpt_fld.update(projection.to_mongo())\n\n            # Filters\n\n            cond_query = MongoQuery()\n            if sval.filters is not None:\n                cond_groups = self._process_constraint_expressions(sval.filters, rev=False)\n                for cg in cond_groups.values():\n                    for c in cg:\n                        cond_query.add_clause(MongoClause(c, rev=False))\n\n            # Done. Add a new 'SectionPart' for the filter and constraint\n\n            result = self.SectionParts(cond_query, query, sval.sampler, rpt_fld)\n            self._sections.append(result)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _process_constraint_expressions(self, expr_list, conflict_check=True, rev=True):\n        # process expressions, grouping by field\n        groups = {}\n        for expr in expr_list:\n            field, raw_op, val = parse_expr(expr)\n            op = ConstraintOperator(raw_op)\n            if field not in groups:\n                groups[field] = ConstraintGroup(Field(field, self._aliases))\n            groups[field].add_constraint(op, val)\n\n        # add existence constraints\n        for cgroup in groups.values():\n            cgroup.add_existence(rev)\n\n        # optionally check for conflicts\n        if conflict_check:\n            # check for conflicts in each group\n            for field_name, group in groups.items():\n                conflicts = group.get_conflicts()\n                if conflicts:\n                    raise ValueError('Conflicts for field {}: {}'.format(field_name, conflicts))\n        return groups", "response": "Create and return constraints from expressions in expr_list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_python(self, constraint_list):\n        if len(constraint_list) == 1 and \\\n                PythonMethod.constraint_is_method(constraint_list[0]):\n            return True\n        if len(constraint_list) > 1 and \\\n                any(filter(PythonMethod.constraint_is_method, constraint_list)):\n            condensed_list = '/'.join(constraint_list)\n            err = PythonMethod.CANNOT_COMBINE_ERR\n            raise ValidatorSyntaxError(condensed_list, err)\n        return False", "response": "Checks whether the current version of the current version is a Python version."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_aliases(self, new_value):\n        \"Set aliases and wrap errors in ValueError\"\n        try:\n            self.aliases = new_value\n        except Exception as err:\n            raise ValueError(\"invalid value: {}\".format(err))", "response": "Set aliases and wrap errors in ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a generator that yields each item from the database randomly from the database.", "response": "def sample(self, cursor):\n        \"\"\"Extract records randomly from the database.\n        Continue until the target proportion of the items have been\n        extracted, or until `min_items` if this is larger.\n        If `max_items` is non-negative, do not extract more than these.\n\n        This function is a generator, yielding items incrementally.\n\n        :param cursor: Cursor to sample\n        :type cursor: pymongo.cursor.Cursor\n        :return: yields each item\n        :rtype: dict\n        :raise: ValueError, if max_items is valid and less than `min_items`\n                or if target collection is empty\n        \"\"\"\n        count = cursor.count()\n\n        # special case: empty collection\n        if count == 0:\n            self._empty = True\n            raise ValueError(\"Empty collection\")\n\n        # special case: entire collection\n        if self.p >= 1 and self.max_items <= 0:\n            for item in cursor:\n                yield item\n            return\n\n        # calculate target number of items to select\n        if self.max_items <= 0:\n            n_target = max(self.min_items, self.p * count)\n        else:\n            if self.p <= 0:\n                n_target = max(self.min_items, self.max_items)\n            else:\n                n_target = max(self.min_items, min(self.max_items, self.p * count))\n        if n_target == 0:\n            raise ValueError(\"No items requested\")\n\n        # select first `n_target` items that pop up with\n        # probability self.p\n        # This is actually biased to items at the beginning\n        # of the file if n_target is smaller than (p * count),\n        n = 0\n        while n < n_target:\n            try:\n                item = next(cursor)\n            except StopIteration:\n                # need to keep looping through data until\n                # we get all our items!\n                cursor.rewind()\n                item = next(cursor)\n            if self._keep():\n                yield item\n                n += 1"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the currently available backends", "response": "def available_backends():\n    \"\"\"Lists the currently available backend types\"\"\"\n    print 'The following LiveSync agents are available:'\n    for name, backend in current_plugin.backend_classes.iteritems():\n        print cformat('  - %{white!}{}%{reset}: {} ({})').format(name, backend.title, backend.description)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef agents():\n    print 'The following LiveSync agents are active:'\n    agent_list = LiveSyncAgent.find().order_by(LiveSyncAgent.backend_name, db.func.lower(LiveSyncAgent.name)).all()\n    table_data = [['ID', 'Name', 'Backend', 'Initial Export', 'Queue']]\n    for agent in agent_list:\n        initial = (cformat('%{green!}done%{reset}') if agent.initial_data_exported else\n                   cformat('%{yellow!}pending%{reset}'))\n        if agent.backend is None:\n            backend_title = cformat('%{red!}invalid backend ({})%{reset}').format(agent.backend_name)\n        else:\n            backend_title = agent.backend.title\n        table_data.append([unicode(agent.id), agent.name, backend_title, initial,\n                           unicode(agent.queue.filter_by(processed=False).count())])\n    table = AsciiTable(table_data)\n    table.justify_columns[4] = 'right'\n    print table.table\n    if not all(a.initial_data_exported for a in agent_list):\n        print\n        print \"You need to perform the initial data export for some agents.\"\n        print cformat(\"To do so, run \"\n                      \"%{yellow!}indico livesync initial_export %{reset}%{yellow}<agent_id>%{reset} for those agents.\")", "response": "Lists the currently active agents."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the initial data export for an agent", "response": "def initial_export(agent_id, force):\n    \"\"\"Performs the initial data export for an agent\"\"\"\n    agent = LiveSyncAgent.find_first(id=agent_id)\n    if agent is None:\n        print 'No such agent'\n        return\n    if agent.backend is None:\n        print cformat('Cannot run agent %{red!}{}%{reset} (backend not found)').format(agent.name)\n        return\n    print cformat('Selected agent: %{white!}{}%{reset} ({})').format(agent.name, agent.backend.title)\n    if agent.initial_data_exported and not force:\n        print 'The initial export has already been performed for this agent.'\n        print cformat('To re-run it, use %{yellow!}--force%{reset}')\n        return\n\n    agent.create_backend().run_initial_export(Event.find(is_deleted=False))\n    agent.initial_data_exported = True\n    db.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the livesync agent", "response": "def run(agent_id, force=False):\n    \"\"\"Runs the livesync agent\"\"\"\n    if agent_id is None:\n        agent_list = LiveSyncAgent.find_all()\n    else:\n        agent = LiveSyncAgent.find_first(id=agent_id)\n        if agent is None:\n            print 'No such agent'\n            return\n        agent_list = [agent]\n\n    for agent in agent_list:\n        if agent.backend is None:\n            print cformat('Skipping agent: %{red!}{}%{reset} (backend not found)').format(agent.name)\n            continue\n        if not agent.initial_data_exported and not force:\n            print cformat('Skipping agent: %{red!}{}%{reset} (initial export not performed)').format(agent.name)\n            continue\n        print cformat('Running agent: %{white!}{}%{reset}').format(agent.name)\n        try:\n            agent.create_backend().run()\n            db.session.commit()\n        except:\n            db.session.rollback()\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(cls, project=None, status=None, batch=None,\n              parent=None, created_from=None, created_to=None,\n              started_from=None, started_to=None, ended_from=None,\n              ended_to=None, offset=None, limit=None, order_by=None,\n              order=None, api=None):\n        \"\"\"\n        Query (List) tasks. Date parameters may be both strings and python date\n        objects.\n        :param project: Target project. optional.\n        :param status: Task status.\n        :param batch: Only batch tasks.\n        :param parent: Parent batch task identifier.\n        :param ended_to: All tasks that ended until this date.\n        :param ended_from: All tasks that ended from this date.\n        :param started_to: All tasks that were started until this date.\n        :param started_from: All tasks that were started from this date.\n        :param created_to: All tasks that were created until this date.\n        :param created_from: All tasks that were created from this date.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param order_by: Property to order by.\n        :param order: Ascending or descending ordering.\n        :param api: Api instance.\n        :return: Collection object.\n        \"\"\"\n        api = api or cls._API\n        if parent:\n            parent = Transform.to_task(parent)\n        if project:\n            project = Transform.to_project(project)\n        if created_from:\n            created_from = Transform.to_datestring(created_from)\n        if created_to:\n            created_to = Transform.to_datestring(created_to)\n        if started_from:\n            started_from = Transform.to_datestring(started_from)\n        if started_to:\n            started_to = Transform.to_datestring(started_to)\n        if ended_from:\n            ended_from = Transform.to_datestring(ended_from)\n        if ended_to:\n            ended_to = Transform.to_datestring(ended_to)\n\n        return super(Task, cls)._query(\n            url=cls._URL['query'], project=project, status=status, batch=batch,\n            parent=parent, created_from=created_from, created_to=created_to,\n            started_from=started_from, started_to=started_to,\n            ended_from=ended_from, ended_to=ended_to, offset=offset,\n            limit=limit, order_by=order_by, order=order, fields='_all', api=api\n        )", "response": "Query the list of tasks."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(cls, name, project, app, revision=None, batch_input=None,\n               batch_by=None, inputs=None, description=None, run=False,\n               disable_batch=False, interruptible=None,\n               execution_settings=None, api=None):\n\n        \"\"\"\n        Creates a task on server.\n        :param name: Task name.\n        :param project: Project identifier.\n        :param app: CWL app identifier.\n        :param revision: CWL app revision.\n        :param batch_input: Batch input.\n        :param batch_by: Batch criteria.\n        :param inputs: Input map.\n        :param description: Task description.\n        :param run: True if you want to run a task upon creation.\n        :param disable_batch: If True disables batching of a batch task.\n        :param interruptible: If True interruptible instance will be used.\n        :param execution_settings: Execution settings for the task.\n        :param api: Api instance.\n        :return: Task object.\n        :raises: TaskValidationError if validation Fails.\n        :raises: SbgError if any exception occurs during request.\n        \"\"\"\n        task_data = {}\n        params = {}\n        project = Transform.to_project(project)\n\n        app_id = Transform.to_app(app)\n\n        if revision:\n            app_id = app_id + \"/\" + six.text_type(revision)\n        else:\n            if isinstance(app, App):\n                app_id = app_id + \"/\" + six.text_type(app.revision)\n\n        task_inputs = {\n            'inputs': Task._serialize_inputs(inputs) if inputs else {}\n        }\n\n        if batch_input and batch_by:\n            task_data['batch_input'] = batch_input\n            task_data['batch_by'] = batch_by\n            if disable_batch:\n                params.update({'batch': False})\n\n        task_meta = {\n            'name': name,\n            'project': project,\n            'app': app_id,\n            'description': description,\n        }\n        task_data.update(task_meta)\n        task_data.update(task_inputs)\n\n        if interruptible is not None:\n            task_data['use_interruptible_instances'] = interruptible\n\n        if execution_settings:\n            task_data.update({'execution_settings': execution_settings})\n\n        if run:\n            params.update({'action': 'run'})\n\n        api = api if api else cls._API\n        created_task = api.post(cls._URL['query'], data=task_data,\n                                params=params).json()\n        if run and 'errors' in created_task:\n            if bool(created_task['errors']):\n                raise TaskValidationError(\n                    'Unable to run task! Task contains errors.',\n                    task=Task(api=api, **created_task)\n                )\n\n        return Task(api=api, **created_task)", "response": "Creates a new task on server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naborting the current task.", "response": "def abort(self, inplace=True):\n        \"\"\"\n        Abort task\n        :param inplace Apply action on the current object or return a new one.\n        :return: Task object.\n        \"\"\"\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {'id': self.id}\n        }\n        logger.info('Aborting task', extra=extra)\n        task_data = self._api.post(\n            url=self._URL['abort'].format(id=self.id)).json()\n        return Task(api=self._api, **task_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self, batch=True, interruptible=None, inplace=True):\n        params = {}\n        if not batch:\n            params['batch'] = False\n        if interruptible is not None:\n            params['use_interruptible_instances'] = interruptible\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {'id': self.id, 'batch': batch}\n        }\n        logger.info('Running task', extra=extra)\n        task_data = self._api.post(\n            url=self._URL['run'].format(id=self.id), params=params).json()\n        return Task(api=self._api, **task_data)", "response": "Run task\n        :param batch if False batching will be disabled.\n        :param interruptible: If true interruptible instance\n        will be used.\n        :param inplace Apply action on the current object or return a new one.\n        :return: Task object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone(self, run=True):\n        params = {}\n        if run:\n            params.update({'action': 'run'})\n\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {'id': self.id, 'run': run}\n        }\n        logger.info('Cloning task', extra=extra)\n        task_data = self._api.post(\n            url=self._URL['clone'].format(id=self.id), params=params).json()\n\n        return Task(api=self._api, **task_data)", "response": "Clone task\n        :param run: run task after cloning\n        :return: Task object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, inplace=True):\n        modified_data = self._modified_data()\n        if bool(modified_data):\n            task_request_data = {}\n            inputs = modified_data.pop('inputs', None)\n            execution_settings = modified_data.pop('execution_settings', None)\n            task_request_data.update(modified_data)\n\n            if inputs:\n                task_request_data['inputs'] = self._serialize_inputs(inputs)\n\n            if execution_settings:\n                task_request_data['execution_settings'] = (\n                    self._serialize_execution_settings(execution_settings)\n                )\n\n            extra = {\n                'resource': self.__class__.__name__,\n                'query': {'id': self.id, 'data': task_request_data}\n            }\n            logger.info('Saving task', extra=extra)\n            data = self._api.patch(url=self._URL['get'].format(id=self.id),\n                                   data=task_request_data).json()\n            task = Task(api=self._api, **data)\n            return task", "response": "Saves all modification to the task on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _serialize_inputs(inputs):\n        serialized_inputs = {}\n        for input_id, input_value in inputs.items():\n            if isinstance(input_value, list):\n                serialized_list = Task._serialize_input_list(input_value)\n                serialized_inputs[input_id] = serialized_list\n            else:\n                if isinstance(input_value, File):\n                    input_value = Task._to_api_file_format(input_value)\n                serialized_inputs[input_id] = input_value\n        return serialized_inputs", "response": "Serialize task input dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_execution_details(self):\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {'id': self.id}\n        }\n        logger.info('Get execution details', extra=extra)\n        data = self._api.get(\n            self._URL['execution_details'].format(id=self.id)).json()\n        return ExecutionDetails(api=self._api, **data)", "response": "Retrieves execution details for a task."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_batch_children(self):\n        if not self.batch:\n            raise SbgError(\"This task is not a batch task.\")\n        return self.query(parent=self.id, api=self._api)", "response": "Retrieves the batch child tasks for this task if its a batch task."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bulk_get(cls, tasks, api=None):\n        api = api or cls._API\n        task_ids = [Transform.to_task(task) for task in tasks]\n        data = {'task_ids': task_ids}\n\n        logger.info('Getting tasks in bulk.')\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return TaskBulkRecord.parse_records(response=response, api=api)", "response": "Retrieve tasks with specified ids in bulk."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef wait(self=None, period=10, callback=None, *args, **kwargs):\n        while self.status not in [\n            TaskStatus.COMPLETED,\n            TaskStatus.FAILED,\n            TaskStatus.ABORTED\n        ]:\n            self.reload()\n            time.sleep(period)\n\n        if callback:\n            return callback(*args, **kwargs)", "response": "Wait until the task is complete and return the result of the callback function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_result(self):\n        png = self.call()\n        if png is None:\n            return\n        if png.startswith('GD extension must be loaded'):\n            current_plugin.logger.warning('Piwik server answered on ImageGraph.get: %s', png)\n            return\n        return 'data:image/png;base64,{}'.format(b64encode(png))", "response": "Perform the call and return the graph data string"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(cls, project=None, visibility=None, q=None, id=None, offset=None,\n              limit=None, api=None):\n        \"\"\"\n        Query (List) apps.\n        :param project: Source project.\n        :param visibility: private|public for private or public apps.\n        :param q: List containing search terms.\n        :param id: List contains app ids. Fetch apps with specific ids.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: Api instance.\n        :return: collection object\n        \"\"\"\n        if project:\n            project = Transform.to_project(project)\n        api = api or cls._API\n        return super(App, cls)._query(url=cls._URL['query'], project=project,\n                                      visibility=visibility, q=q, id=id,\n                                      offset=offset, limit=limit, api=api)", "response": "Query the app s data for a specific app."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_revision(cls, id, revision, api=None):\n        api = api if api else cls._API\n        extra = {'resource': cls.__name__, 'query': {\n            'id': id,\n            'revision': revision\n        }}\n        logger.info('Get revision', extra=extra)\n        app = api.get(url=cls._URL['get_revision'].format(\n            id=id, revision=revision)).json()\n        return App(api=api, **app)", "response": "Get app revision.\n        :param id: App identifier.\n        :param revision: App revision\n        :param api: Api instance.\n        :return: App object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef install_app(cls, id, raw, api=None, raw_format=None):\n        api = api if api else cls._API\n        raw_format = raw_format.lower() if raw_format else AppRawFormat.JSON\n        extra = {\n            'resource': cls.__name__,\n            'query': {\n                'id': id,\n                'data': raw\n            }\n        }\n        logger.info('Installing app', extra=extra)\n\n        # Set content type for raw app data\n        if raw_format not in cls._CONTENT_TYPE.keys():\n            raise SbgError(\n                'Unsupported raw data format: \"{}\".'\n                .format(raw_format))\n        headers = {'Content-Type': cls._CONTENT_TYPE[raw_format]}\n\n        app = api.post(\n            url=cls._URL['raw'].format(id=id),\n            data=raw,\n            headers=headers,\n        ).json()\n        app_wrapper = api.get(\n            url=cls._URL['get'].format(id=app['sbg:id'])).json()\n        return App(api=api, **app_wrapper)", "response": "Installs and returns a new App object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_revision(cls, id, revision, raw, api=None):\n\n        api = api if api else cls._API\n        extra = {'resource': cls.__name__, 'query': {\n            'id': id,\n            'data': raw\n        }}\n        logger.info('Creating app revision', extra=extra)\n        app = api.post(url=cls._URL['create_revision'].format(\n            id=id, revision=revision), data=raw).json()\n        app_wrapper = api.get(\n            url=cls._URL['get'].format(id=app['sbg:id'])).json()\n        return App(api=api, **app_wrapper)", "response": "Create a new revision of an app."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy the current app.", "response": "def copy(self, project, name=None, strategy=None):\n        \"\"\"\n        Copies the current app.\n        :param project: Destination project.\n        :param name: Destination app name.\n        :param strategy: App copy strategy.\n        :return: Copied App object.\n\n        :Copy strategies:\n        clone         copy all revisions and continue getting updates form the\n                      original app (default method when the key is omitted)\n\n        direct        copy only the latest revision and get the updates from\n                      this point on\n\n        clone_direct  copy the app like the direct strategy, but keep all\n                      revisions\n\n        transient     copy only the latest revision and continue getting\n                      updates from the original app\n        \"\"\"\n        strategy = strategy or AppCopyStrategy.CLONE\n\n        project = Transform.to_project(project)\n        data = {\n            'project': project,\n            'strategy': strategy\n        }\n        if name:\n            data['name'] = name\n        extra = {'resource': self.__class__.__name__, 'query': {\n            'id': self.id,\n            'data': data\n        }}\n        logger.info('Copying app', extra=extra)\n        app = self._api.post(url=self._URL['copy'].format(id=self.id),\n                             data=data).json()\n        return App(api=self._api, **app)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sync(self):\n        app = self._api.post(url=self._URL['sync'].format(id=self.id)).json()\n        return App(api=self._api, **app)", "response": "Syncs the parent app changes with the current app instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Query that retrieves the chatrooms for an event.", "response": "def find_for_event(cls, event, include_hidden=False, **kwargs):\n        \"\"\"Returns a Query that retrieves the chatrooms for an event\n\n        :param event: an indico event (with a numeric ID)\n        :param include_hidden: if hidden chatrooms should be included, too\n        :param kwargs: extra kwargs to pass to ``find()``\n        \"\"\"\n        query = cls.find(event_id=event.id, **kwargs)\n        if not include_hidden:\n            query = query.filter(~cls.hidden)\n        return query"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the event chatroom and if necessary the chatroom too.", "response": "def delete(self, reason=''):\n        \"\"\"Deletes the event chatroom and if necessary the chatroom, too.\n\n        :param reason: reason for the deletion\n        :return: True if the associated chatroom was also\n                 deleted, otherwise False\n        \"\"\"\n        db.session.delete(self)\n        db.session.flush()\n        if not self.chatroom.events:\n            db.session.delete(self.chatroom)\n            db.session.flush()\n            delete_room(self.chatroom, reason)\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef obj_ref(obj):\n    from indico_livesync.models.queue import EntryType\n    if isinstance(obj, Category):\n        ref = {'type': EntryType.category, 'category_id': obj.id}\n    elif isinstance(obj, Event):\n        ref = {'type': EntryType.event, 'event_id': obj.id}\n    elif isinstance(obj, Session):\n        ref = {'type': EntryType.session, 'session_id': obj.id}\n    elif isinstance(obj, Contribution):\n        ref = {'type': EntryType.contribution, 'contrib_id': obj.id}\n    elif isinstance(obj, SubContribution):\n        ref = {'type': EntryType.subcontribution, 'subcontrib_id': obj.id}\n    else:\n        raise ValueError('Unexpected object: {}'.format(obj.__class__.__name__))\n    return ImmutableDict(ref)", "response": "Returns a tuple identifying a category event contributions or subcontributions"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the object identified by ref", "response": "def obj_deref(ref):\n    \"\"\"Returns the object identified by `ref`\"\"\"\n    from indico_livesync.models.queue import EntryType\n    if ref['type'] == EntryType.category:\n        return Category.get_one(ref['category_id'])\n    elif ref['type'] == EntryType.event:\n        return Event.get_one(ref['event_id'])\n    elif ref['type'] == EntryType.session:\n        return Session.get_one(ref['session_id'])\n    elif ref['type'] == EntryType.contribution:\n        return Contribution.get_one(ref['contrib_id'])\n    elif ref['type'] == EntryType.subcontribution:\n        return SubContribution.get_one(ref['subcontrib_id'])\n    else:\n        raise ValueError('Unexpected object type: {}'.format(ref['type']))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting obsolete entries from the queues", "response": "def clean_old_entries():\n    \"\"\"Deletes obsolete entries from the queues\"\"\"\n    from indico_livesync.plugin import LiveSyncPlugin\n    from indico_livesync.models.queue import LiveSyncQueueEntry\n\n    queue_entry_ttl = LiveSyncPlugin.settings.get('queue_entry_ttl')\n    if not queue_entry_ttl:\n        return\n    expire_threshold = now_utc() - timedelta(days=queue_entry_ttl)\n    LiveSyncQueueEntry.find(LiveSyncQueueEntry.processed,\n                            LiveSyncQueueEntry.timestamp < expire_threshold).delete(synchronize_session='fetch')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_excluded_categories():\n    from indico_livesync.plugin import LiveSyncPlugin\n    return {int(x['id']) for x in LiveSyncPlugin.settings.get('excluded_categories')}", "response": "Get excluded category IDs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a hierarchical compound ID separated by dots.", "response": "def compound_id(obj):\n    \"\"\"Generate a hierarchical compound ID, separated by dots.\"\"\"\n    if isinstance(obj, (Category, Session)):\n        raise TypeError('Compound IDs are not supported for this entry type')\n    elif isinstance(obj, Event):\n        return unicode(obj.id)\n    elif isinstance(obj, Contribution):\n        return '{}.{}'.format(obj.event_id, obj.id)\n    elif isinstance(obj, SubContribution):\n        return '{}.{}.{}'.format(obj.contribution.event_id, obj.contribution_id, obj.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntracks a download in Piwik", "response": "def track_download_request(download_url, download_title):\n    \"\"\"Track a download in Piwik\"\"\"\n    from indico_piwik.plugin import PiwikPlugin\n\n    if not download_url:\n        raise ValueError(\"download_url can't be empty\")\n    if not download_title:\n        raise ValueError(\"download_title can't be empty\")\n\n    request = PiwikRequest(server_url=PiwikPlugin.settings.get('server_api_url'),\n                           site_id=PiwikPlugin.settings.get('site_id_events'),\n                           api_token=PiwikPlugin.settings.get('server_token'),\n                           query_script=PiwikPlugin.track_script)\n\n    action_url = quote(download_url)\n    dt = datetime.now()\n    request.call(idsite=request.site_id,\n                 rec=1,\n                 action_name=quote(download_title.encode('utf-8')),\n                 url=action_url,\n                 download=action_url,\n                 h=dt.hour, m=dt.minute, s=dt.second)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef backend(self):\n        from indico_livesync.plugin import LiveSyncPlugin\n        return LiveSyncPlugin.instance.backend_classes.get(self.backend_name)", "response": "Returns the backend class"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notify_created(room, event, user):\n    tpl = get_plugin_template_module('emails/created.txt', chatroom=room, event=event, user=user)\n    _send(event, tpl)", "response": "Notifies about the creation of a chatroom."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef notify_attached(room, event, user):\n    tpl = get_plugin_template_module('emails/attached.txt', chatroom=room, event=event, user=user)\n    _send(event, tpl)", "response": "Notifies about an existing chatroom being attached to an event."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nnotify about the modification of a chatroom.", "response": "def notify_modified(room, event, user):\n    \"\"\"Notifies about the modification of a chatroom.\n\n    :param room: the chatroom\n    :param event: the event\n    :param user: the user performing the action\n    \"\"\"\n    tpl = get_plugin_template_module('emails/modified.txt', chatroom=room, event=event, user=user)\n    _send(event, tpl)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef notify_deleted(room, event, user, room_deleted):\n    tpl = get_plugin_template_module('emails/deleted.txt', chatroom=room, event=event, user=user,\n                                     room_deleted=room_deleted)\n    _send(event, tpl)", "response": "Notifies about the deletion of a chatroom."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef query(cls, automation=None, offset=None, limit=None, api=None):\n        automation_id = Transform.to_automation(automation)\n\n        api = api or cls._API\n        return super(AutomationMember, cls)._query(\n            url=cls._URL['query'].format(automation_id=automation_id),\n            automation_id=automation_id,\n            offset=offset,\n            limit=limit,\n            api=api,\n        )", "response": "Query ( List ) apps."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching the resource from the server.", "response": "def get(cls, id, automation, api=None):\n        \"\"\"\n        Fetches the resource from the server.\n        :param id: Automation member username\n        :param automation: Automation id or object\n        :param api: sevenbridges Api instance.\n        :return: AutomationMember object.\n        \"\"\"\n        username = Transform.to_resource(id)\n        automation = Transform.to_automation(automation)\n\n        api = api or cls._API\n        member = api.get(url=cls._URL['get'].format(\n            automation_id=automation,\n            id=username\n        )).json()\n        return AutomationMember(api=api, **member)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a member to the automation.", "response": "def add(cls, user, permissions, automation, api=None):\n        \"\"\"\n        Add a member to the automation.\n        :param user: Member username\n        :param permissions: Permissions dictionary.\n        :param automation: Automation object or id\n        :param api: sevenbridges Api instance\n        :return: Automation member object.\n        \"\"\"\n        user = Transform.to_user(user)\n        automation = Transform.to_automation(automation)\n\n        api = api or cls._API\n        data = {'username': user}\n\n        if isinstance(permissions, dict):\n            data.update({\n                'permissions': permissions\n            })\n\n        member_data = api.post(\n            url=cls._URL['query'].format(automation_id=automation),\n            data=data\n        ).json()\n        return AutomationMember(api=api, **member_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves a member from the automation.", "response": "def remove(cls, user, automation, api=None):\n        \"\"\"\n        Remove a member from the automation.\n        :param user: Member username\n        :param automation: Automation id\n        :param api: sevenbridges Api instance\n        :return: None\n        \"\"\"\n        user = Transform.to_user(user)\n        automation = Transform.to_automation(automation)\n\n        api = api or cls._API\n        api.delete(\n            cls._URL['get'].format(automation_id=automation, id=user)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, inplace=True):\n        modified = self._modified_data()\n        if bool(modified):\n            new_data = self.permissions.copy()\n            new_data.update(modified['permissions'])\n            data = {\n                'permissions': new_data\n            }\n            url = six.text_type(self.href)\n            self._api.patch(url=url, data=data, append_base=False)\n        else:\n            raise ResourceNotModified()", "response": "Saves the current object to the api server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(cls, name=None, offset=None, limit=None, api=None):\n\n        api = api or cls._API\n        return super(Automation, cls)._query(\n            url=cls._URL['query'],\n            name=name,\n            offset=offset,\n            limit=limit,\n            api=api,\n        )", "response": "Query the object by name or id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_packages(self, offset=None, limit=None, api=None):\n        api = api or self._API\n        return AutomationPackage.query(\n            automation=self.id, offset=offset, limit=limit, api=api\n        )", "response": "Get list of packages that belong to this automation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_member(self, username, api=None):\n        member = Transform.to_automation_member(username)\n        api = api or self._API\n        return AutomationMember.get(\n            id=member, automation=self.id, api=api\n        )", "response": "Get specified automation member"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_members(self, offset=None, limit=None, api=None):\n        api = api or self._API\n        return AutomationMember.query(\n            automation=self.id, offset=offset, limit=limit, api=api\n        )", "response": "Get list of automation members"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_member(self, user, permissions, api=None):\n        api = api or self._API\n        return AutomationMember.add(\n            automation=self.id, user=user, permissions=permissions, api=api\n        )", "response": "Add a member to the automation"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_member(self, user, api=None):\n        api = api or self._API\n        AutomationMember.remove(automation=self.id, user=user, api=api)", "response": "Removes a member from the automation set"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_runs(self, package=None, status=None, name=None,\n                 created_by=None, created_from=None, created_to=None,\n                 order_by=None, order=None, offset=None, limit=None, api=None):\n        \"\"\"\n        Query automation runs that belong to this automation\n        :param package: Package id\n        :param status: Run status\n        :param name: Automation run name\n        :param created_by: Username of member that created the run\n        :param created_from: Date the run was created after\n        :param created_to: Date the run was created before\n        :param order_by: Property by which to order results\n        :param order: Ascending or Descending (\"asc\" or \"desc\")\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: sevenbridges Api instance\n        :return: AutomationRun collection\n        \"\"\"\n        api = api or self._API\n        return AutomationRun.query(\n            automation=self.id, package=package, status=status, name=name,\n            created_by=created_by, created_from=created_from,\n            created_to=created_to, order_by=order_by, order=order,\n            offset=offset, limit=limit, api=api\n        )", "response": "Query the runs that belong to this automation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the automation runs.", "response": "def query(cls, automation=None, package=None, status=None, name=None,\n              created_by=None, created_from=None, created_to=None,\n              order_by=None, order=None, offset=None, limit=None, api=None):\n        \"\"\"\n        Query (List) automation runs.\n        :param name: Automation run name\n        :param automation: Automation template\n        :param package: Package\n        :param status: Run status\n        :param created_by: Username of user that created the run\n        :param order_by: Property by which to order results\n        :param order: Ascending or descending (\"asc\" or \"desc\")\n        :param created_from: Date the run is created after\n        :param created_to: Date the run is created before\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: Api instance.\n        :return: collection object\n        \"\"\"\n        if automation:\n            automation = Transform.to_automation(automation)\n\n        if package:\n            package = Transform.to_automation_package(package)\n\n        api = api or cls._API\n        return super(AutomationRun, cls)._query(\n            url=cls._URL['query'],\n            name=name,\n            automation=automation,\n            package=package,\n            status=status,\n            created_by=created_by,\n            created_from=created_from,\n            created_to=created_to,\n            order_by=order_by,\n            order=order,\n            offset=offset,\n            limit=limit,\n            api=api,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates and start a new automation run.", "response": "def create(cls, package, inputs=None, settings=None, resume_from=None,\n               name=None, secret_settings=None, api=None):\n        \"\"\"\n        Create and start a new run.\n        :param package: Automation package id\n        :param inputs: Input dictionary\n        :param settings: Settings override dictionary\n        :param resume_from: Run to resume from\n        :param name: Automation run name\n        :param secret_settings: dict to override secret_settings from\n        automation template\n        :param api: sevenbridges Api instance\n        :return: AutomationRun object\n        \"\"\"\n        package = Transform.to_automation_package(package)\n\n        data = {'package': package}\n        if inputs:\n            data['inputs'] = inputs\n        if settings:\n            data['settings'] = settings\n        if resume_from:\n            data['resume_from'] = resume_from\n        if name:\n            data['name'] = name\n        if secret_settings:\n            data['secret_settings'] = secret_settings\n\n        api = api or cls._API\n        automation_run = api.post(\n            url=cls._URL['query'],\n            data=data,\n        ).json()\n        return AutomationRun(api=api, **automation_run)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(self, api=None):\n        api = api or self._API\n\n        return api.post(\n            url=self._URL['actions'].format(\n                id=self.id, action=AutomationRunActions.STOP\n            )\n        ).content", "response": "Stop automation run.\n        :param api: sevenbridges Api instance.\n        :return: AutomationRun object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_log_file(self, api=None):\n        api = api or self._API\n        log_file_data = self.execution_details.get('log_file')\n        return File(api=api, **log_file_data) if log_file_data else None", "response": "Retrieve automation run log."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving automation run state.", "response": "def get_state(self, api=None):\n        \"\"\"\n        Retrieve automation run state.\n        :param api: sevenbridges Api instance\n        :return: State file json contents as string\n        \"\"\"\n        api = api or self._API\n        return api.get(self._URL['state'].format(id=self.id)).json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_part_url(api, url, upload, part):\n    try:\n        response = api.get(url.format(upload_id=upload, part_number=part))\n        return response.json()['url']\n    except Exception:\n        raise SbgError(\n            'Unable to get upload url for part number {}'.format(part)\n        )", "response": "Get the url for the part that is to be uploaded."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _report_part(api, url, upload, part, e_tag):\n    part_data = {\n        'part_number': part,\n        'response': {\n            'headers': {\n                'ETag': e_tag\n            }\n        }\n    }\n    try:\n        api.post(\n            url.format(upload_id=upload, part_number=''), data=part_data\n        )\n    except Exception as e:\n        raise SbgError(\n            'Unable to report part number {}. Reason: {}'.format(\n                part, six.text_type(e)\n            )\n        )", "response": "Report the completion of a part upload."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsubmitting the part data to the storage service URL.", "response": "def _submit_part(session, url, part, timeout):\n    \"\"\"\n    Used by the worker to submit the part data to the storage service URL.\n    :param session: Storage service session.\n    :param url: Part url.\n    :param part: Part data in bytes.\n    :param timeout: Timeout for storage session.\n    :return: ETag for the submitted part.\n    \"\"\"\n    try:\n        response = session.put(url, data=part, timeout=timeout)\n        return response.headers.get('etag').strip('\"')\n    except Exception as e:\n        raise SbgError(\n            'Failed to submit the part. Reason: {}'.format(\n                six.text_type(e)\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses by the worker to upload a part to the storage service. :param api: Api instance. :param session: Storage service session. :param url: Part url. :param upload: Upload identifier. :param part_number: Part number. :param part: Part data. :param retry_count: Number of times to retry. :param timeout: Timeout for storage session.", "response": "def _upload_part(api, session, url, upload, part_number, part, retry_count,\n                 timeout):\n    \"\"\"\n    Used by the worker to upload a part to the storage service.\n    :param api: Api instance.\n    :param session: Storage service session.\n    :param url: Part url.\n    :param upload: Upload identifier.\n    :param part_number: Part number.\n    :param part: Part data.\n    :param retry_count: Number of times to retry.\n    :param timeout: Timeout for storage session.\n    \"\"\"\n    part_url = retry(retry_count)(_get_part_url)(\n        api, url, upload, part_number\n    )\n\n    e_tag = retry(retry_count)(_submit_part)(\n        session, part_url, part, timeout\n    )\n\n    retry(retry_count)(_report_part)(api, url, upload, part_number, e_tag)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef submit(self):\n\n        \"\"\"\n        Partitions the file into chunks and submits them into group of 4\n        for upload on the api upload pool.\n        :return: Futures\n        \"\"\"\n        futures = []\n        while self.submitted < 4 and not self.done():\n            part = self.parts.pop(0)\n            part_number = part['part']\n            part_read_offset = part['offset']\n            part_read_limit = part['limit']\n\n            self.fp.seek(part_read_offset)\n            part_data = self.fp.read(part_read_limit - part_read_offset)\n\n            futures.append(\n                self.pool.submit(\n                    _upload_part, self.api, self.session,\n                    self._URL['upload_part'], self.upload_id,\n                    part_number, part_data, self.retry, self.timeout\n                )\n            )\n\n            self.submitted += 1\n            self.total_submitted += 1\n\n        return futures", "response": "Submits the file into chunks and returns a list of Futures\n            objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of the parts that are part of the file.", "response": "def get_parts(self):\n        \"\"\"\n        Partitions the file and saves the parts to be uploaded\n        in memory.\n        \"\"\"\n        parts = []\n        start_byte = 0\n        for i in range(1, self.total + 1):\n            end_byte = start_byte + self.part_size\n            if end_byte >= self.file_size - 1:\n                end_byte = self.file_size\n            parts.append({\n                'part': i,\n                'offset': start_byte,\n                'limit': end_byte\n            })\n            start_byte = end_byte\n        return parts"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _verify_part_number(self):\n        total = int(math.ceil(self._file_size / self._part_size))\n        if total > PartSize.MAXIMUM_TOTAL_PARTS:\n            self._status = TransferState.FAILED\n            raise SbgError(\n                'Total parts = {}. Maximum number of parts is {}'.format(\n                    total, PartSize.MAXIMUM_TOTAL_PARTS)\n            )", "response": "Verifies that the total number of parts is smaller than 10^5 which is the maximum number of parts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nverifying that the part size is smaller then the maximum part size.", "response": "def _verify_part_size(self):\n        \"\"\"\n        Verifies that the part size is smaller then the maximum part size\n        which is 5GB.\n        \"\"\"\n        if self._part_size > PartSize.MAXIMUM_UPLOAD_SIZE:\n            self._status = TransferState.FAILED\n            raise SbgError('Part size = {}b. Maximum part size is {}b'.format(\n                self._part_size, PartSize.MAXIMUM_UPLOAD_SIZE)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _verify_file_size(self):\n        if self._file_size > PartSize.MAXIMUM_OBJECT_SIZE:\n            self._status = TransferState.FAILED\n            raise SbgError('File size = {}b. Maximum file size is {}b'.format(\n                self._file_size, PartSize.MAXIMUM_OBJECT_SIZE)\n            )", "response": "Verifies that the file size is smaller than 5TB which is the maximum object size that is allowed for upload."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _initialize_upload(self):\n\n        \"\"\"\n        Initialized the upload on the API server by submitting the information\n        about the project, the file name, file size and the part size that is\n        going to be used during multipart upload.\n        \"\"\"\n        init_data = {\n            'name': self._file_name,\n            'part_size': self._part_size,\n            'size': self._file_size\n        }\n\n        if self._project:\n            init_data['project'] = self._project\n        elif self._parent:\n            init_data['parent'] = self._parent\n\n        init_params = {}\n        if self._overwrite:\n            init_params['overwrite'] = self._overwrite\n\n        try:\n            response = self._api.post(\n                self._URL['upload_init'], data=init_data, params=init_params\n            )\n            self._upload_id = response.json()['upload_id']\n        except SbgError as e:\n            self._status = TransferState.FAILED\n            raise SbgError(\n                'Unable to initialize upload! Failed to get upload id! '\n                'Reason: {}'.format(e.message)\n            )", "response": "Initializes the upload on the API server by submitting the information about the project the file name the part size and the part size that is going to be used during multipart upload."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _finalize_upload(self):\n        from sevenbridges.models.file import File\n        try:\n            response = self._api.post(\n                self._URL['upload_complete'].format(upload_id=self._upload_id)\n            ).json()\n            self._result = File(api=self._api, **response)\n            self._status = TransferState.COMPLETED\n\n        except SbgError as e:\n            self._status = TransferState.FAILED\n            raise SbgError(\n                'Failed to complete upload! Reason: {}'.format(e.message)\n            )", "response": "Finalizes the upload on the API server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _abort_upload(self):\n        try:\n            self._api.delete(\n                self._URL['upload_info'].format(upload_id=self._upload_id)\n            )\n        except SbgError as e:\n            self._status = TransferState.FAILED\n            raise SbgError(\n                'Failed to abort upload! Reason: {}'.format(e.message)\n            )", "response": "Aborts the upload on the API server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a callback that will be called when the upload finishes successfully.", "response": "def add_callback(self, callback=None, errorback=None):\n        \"\"\"\n        Adds a callback that will be called when the upload\n        finishes successfully or when error is raised.\n        \"\"\"\n        self._callback = callback\n        self._errorback = errorback"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self):\n        if self._status == TransferState.PREPARING:\n            super(Upload, self).start()\n        else:\n            raise SbgError(\n                'Unable to start. Upload not in PREPARING state.'\n            )", "response": "Starts the upload.\n        :raises SbgError: If upload is not in PREPARING state."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the thread! Should not be used use start() method instead.", "response": "def run(self):\n        \"\"\"\n        Runs the thread! Should not be used use start() method instead.\n        \"\"\"\n        self._running.set()\n        self._status = TransferState.RUNNING\n        self._time_started = time.time()\n\n        # Initializes the upload\n        self._initialize_upload()\n\n        # Opens the file for reading in binary mode.\n        try:\n            with io.open(self._file_path, mode='rb') as fp:\n                # Creates a partitioned file\n                parted_file = UPartedFile(\n                    fp, self._file_size, self._part_size, self._upload_id,\n                    self._retry, self._timeout, self.session, self._api\n                )\n\n                # Iterates over parts and submits them for upload.\n                for _ in parted_file:\n                    if self._stop_signal:\n                        return\n                    self._running.wait()\n                    self._bytes_done += self._part_size\n                    # If the progress callback is set we need to provide a\n                    # progress object for it.\n                    if self._progress_callback:\n                        progress = Progress(\n                            parted_file.total, parted_file.total_submitted,\n                            self._bytes_done, self._file_size, self.duration\n                        )\n                        self._progress_callback(progress)\n        except IOError:\n            raise SbgError('Unable to open file {}'.format(self._file_path))\n        except Exception as e:\n            # If the errorback callback is set call it with status\n            self._status = TransferState.FAILED\n            if self._errorback:\n                self._errorback(self._status)\n            else:\n                raise SbgError(six.text_type(e))\n\n        # Finalizes the upload.\n        self._finalize_upload()\n        self._status = TransferState.COMPLETED\n        # If the callback is set call it.\n        if self._callback:\n            self._callback(self._status)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_records(records):\n    changes = defaultdict(int)\n    cascaded_update_records = set()\n    cascaded_delete_records = set()\n\n    for record in records:\n        if record.change != ChangeType.deleted and record.object is None:\n            # Skip entries which are not deletions but have no corresponding objects.\n            # Probably they are updates for objects that got deleted afterwards.\n            continue\n        if record.change == ChangeType.created:\n            assert record.type != EntryType.category\n            changes[record.object] |= SimpleChange.created\n        elif record.change == ChangeType.deleted:\n            assert record.type != EntryType.category\n            cascaded_delete_records.add(record)\n        elif record.change in {ChangeType.moved, ChangeType.protection_changed}:\n            cascaded_update_records.add(record)\n        elif record.change == ChangeType.data_changed:\n            assert record.type != EntryType.category\n            changes[record.object] |= SimpleChange.updated\n\n    for obj in _process_cascaded_category_contents(cascaded_update_records):\n        changes[obj] |= SimpleChange.updated\n\n    for obj in _process_cascaded_event_contents(cascaded_delete_records):\n        changes[obj] |= SimpleChange.deleted\n\n    return changes", "response": "Converts a list of LiveSyncQueueEntry objects into object changes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding all elements that are in the category tree that have the same protection settings.", "response": "def _process_cascaded_category_contents(records):\n    \"\"\"\n    Travel from categories to subcontributions, flattening the whole event structure.\n\n    Yields everything that it finds (except for elements whose protection has changed\n    but are not inheriting their protection settings from anywhere).\n\n    :param records: queue records to process\n    \"\"\"\n    category_prot_records = {rec.category_id for rec in records if rec.type == EntryType.category\n                             and rec.change == ChangeType.protection_changed}\n    category_move_records = {rec.category_id for rec in records if rec.type == EntryType.category\n                             and rec.change == ChangeType.moved}\n\n    changed_events = set()\n\n    category_prot_records -= category_move_records  # A move already implies sending the whole record\n\n    # Protection changes are handled differently, as there may not be the need to re-generate the record\n    if category_prot_records:\n        for categ in Category.find(Category.id.in_(category_prot_records)):\n            cte = categ.get_protection_parent_cte()\n            # Update only children that inherit\n            inheriting_categ_children = (Event.query\n                                         .join(cte, db.and_((Event.category_id == cte.c.id),\n                                                            (cte.c.protection_parent == categ.id))))\n            inheriting_direct_children = Event.find((Event.category_id == categ.id) & Event.is_inheriting)\n\n            changed_events.update(itertools.chain(inheriting_direct_children, inheriting_categ_children))\n\n    # Add move operations and explicitly-passed event records\n    if category_move_records:\n        changed_events.update(Event.find(Event.category_chain_overlaps(category_move_records)))\n\n    for elem in _process_cascaded_event_contents(records, additional_events=changed_events):\n        yield elem"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_cascaded_event_contents(records, additional_events=None):\n    changed_events = additional_events or set()\n    changed_contributions = set()\n    changed_subcontributions = set()\n\n    session_records = {rec.session_id for rec in records if rec.type == EntryType.session}\n    contribution_records = {rec.contrib_id for rec in records if rec.type == EntryType.contribution}\n    subcontribution_records = {rec.subcontrib_id for rec in records if rec.type == EntryType.subcontribution}\n    event_records = {rec.event_id for rec in records if rec.type == EntryType.event}\n\n    if event_records:\n        changed_events.update(Event.find(Event.id.in_(event_records)))\n\n    for event in changed_events:\n        yield event\n\n    # Sessions are added (explicitly changed only, since they don't need to be sent anywhere)\n    if session_records:\n        changed_contributions.update(Contribution\n                                     .find(Contribution.session_id.in_(session_records), ~Contribution.is_deleted))\n\n    # Contributions are added (implictly + explicitly changed)\n    changed_event_ids = {ev.id for ev in changed_events}\n\n    condition = Contribution.event_id.in_(changed_event_ids) & ~Contribution.is_deleted\n    if contribution_records:\n        condition = db.or_(condition, Contribution.id.in_(contribution_records))\n    contrib_query = Contribution.find(condition).options(joinedload('subcontributions'))\n\n    for contribution in contrib_query:\n        yield contribution\n        changed_subcontributions.update(contribution.subcontributions)\n\n    # Same for subcontributions\n    if subcontribution_records:\n        changed_subcontributions.update(SubContribution.find(SubContribution.id.in_(subcontribution_records)))\n    for subcontrib in changed_subcontributions:\n        yield subcontrib", "response": "Yields results for processing the cascaded event contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new Vidyo room for an event given a VC room.", "response": "def create_room(self, vc_room, event):\n        \"\"\"Create a new Vidyo room for an event, given a VC room.\n\n        In order to create the Vidyo room, the function will try to do so with\n        all the available identities of the user based on the authenticators\n        defined in Vidyo plugin's settings, in that order.\n\n        :param vc_room: VCRoom -- The VC room from which to create the Vidyo\n                        room\n        :param event: Event -- The event to the Vidyo room will be attached\n        \"\"\"\n        client = AdminClient(self.settings)\n        owner = retrieve_principal(vc_room.data['owner'])\n        login_gen = iter_user_identities(owner)\n        login = next(login_gen, None)\n        if login is None:\n            raise VCRoomError(_(\"No valid Vidyo account found for this user\"), field='owner_user')\n\n        extension_gen = iter_extensions(self.settings.get('indico_room_prefix'), event.id)\n        extension = next(extension_gen)\n\n        while True:\n            room_mode = {\n                'isLocked': False,\n                'hasPIN': bool(vc_room.data['room_pin']),\n                'hasModeratorPIN': bool(vc_room.data['moderation_pin'])\n            }\n            if room_mode['hasPIN']:\n                room_mode['roomPIN'] = vc_room.data['room_pin']\n            if room_mode['hasModeratorPIN']:\n                room_mode['moderatorPIN'] = vc_room.data['moderation_pin']\n\n            room_obj = client.create_room_object(\n                name=vc_room.name,\n                RoomType='Public',\n                ownerName=login,\n                extension=extension,\n                groupName=self.settings.get('room_group_name'),\n                description=vc_room.data['description'],\n                RoomMode=room_mode)\n\n            if room_obj.RoomMode.hasPIN:\n                room_obj.RoomMode.roomPIN = vc_room.data['room_pin']\n            if room_obj.RoomMode.hasModeratorPIN:\n                room_obj.RoomMode.moderatorPIN = vc_room.data['moderation_pin']\n\n            try:\n                client.add_room(room_obj)\n            except APIException as err:\n                err_msg = err.message\n\n                if err_msg.startswith('Room exist for name'):\n                    raise VCRoomError(_(\"Room name already in use\"), field='name')\n                elif err_msg.startswith('Member not found for ownerName'):\n                    login = next(login_gen, None)\n                    if login is None:\n                        raise VCRoomError(_(\"No valid Vidyo account found for this user\"), field='owner_user')\n                elif err_msg.startswith('Room exist for extension'):\n                    extension = next(extension_gen)\n                else:\n                    raise\n\n            else:\n                # get room back, in order to fetch Vidyo-set parameters\n                created_room = client.find_room(extension)\n\n                if not created_room:\n                    raise VCRoomNotFoundError(_(\"Could not find newly created room in Vidyo\"))\n                vc_room.data.update({\n                    'vidyo_id': unicode(created_room.roomID),\n                    'url': created_room.RoomMode.roomURL,\n                    'owner_identity': created_room.ownerName\n                })\n                flag_modified(vc_room, 'data')\n                vc_room.vidyo_extension = VidyoExtension(vc_room_id=vc_room.id, extension=int(created_room.extension),\n                                                         owned_by_user=owner)\n\n                client.set_automute(created_room.roomID, vc_room.data['auto_mute'])\n                break"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef object(self):\n        if self.type == EntryType.category:\n            return self.category\n        elif self.type == EntryType.event:\n            return self.event\n        elif self.type == EntryType.session:\n            return self.session\n        elif self.type == EntryType.contribution:\n            return self.contribution\n        elif self.type == EntryType.subcontribution:\n            return self.subcontribution", "response": "Return the changed object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef object_ref(self):\n        return ImmutableDict(type=self.type, category_id=self.category_id, event_id=self.event_id,\n                             session_id=self.session_id, contrib_id=self.contrib_id, subcontrib_id=self.subcontrib_id)", "response": "Return the reference of the changed object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, changes, ref, excluded_categories=set()):\n        ref = dict(ref)\n        obj = obj_deref(ref)\n\n        if isinstance(obj, Category):\n            if any(c.id in excluded_categories for c in obj.chain_query):\n                return\n        else:\n            event = obj if isinstance(obj, Event) else obj.event\n            if event.category not in g.setdefault('livesync_excluded_categories_checked', {}):\n                g.livesync_excluded_categories_checked[event.category] = excluded_categories & set(event.category_chain)\n            if g.livesync_excluded_categories_checked[event.category]:\n                return\n\n        try:\n            agents = g.livesync_agents\n        except AttributeError:\n            agents = g.livesync_agents = LiveSyncAgent.query.all()\n\n        for change in changes:\n            for agent in agents:\n                entry = cls(agent=agent, change=change, **ref)\n                db.session.add(entry)\n\n        db.session.flush()", "response": "Create a new change in all queues."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve file copy async job", "response": "def get_file_copy_job(cls, id, api=None):\n        \"\"\"\n        Retrieve file copy async job\n        :param id: Async job identifier\n        :param api: Api instance\n        :return:\n        \"\"\"\n        id = Transform.to_async_job(id)\n\n        api = api if api else cls._API\n        async_job = api.get(\n            url=cls._URL['get_file_copy_job'].format(id=id)\n        ).json()\n        return AsyncJob(api=api, **async_job)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_result(self, api=None):\n        api = api or self._API\n        if not self.result:\n            return []\n        return AsyncFileBulkRecord.parse_records(\n            result=self.result,\n            api=api\n        )", "response": "Get async job result in bulk format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_file_jobs(cls, offset=None, limit=None, api=None):\n        api = api or cls._API\n        return super(AsyncJob, cls)._query(\n            api=api,\n            url=cls._URL['list_file_jobs'],\n            offset=offset,\n            limit=limit,\n        )", "response": "Query async jobs with optional pagination"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecomposing a MARC code into tag ind1 ind2 subcode", "response": "def decompose_code(code):\n    \"\"\"\n    Decomposes a MARC \"code\" into tag, ind1, ind2, subcode\n    \"\"\"\n    code = \"%-6s\" % code\n    ind1 = code[3:4]\n    if ind1 == \" \": ind1 = \"_\"\n    ind2 = code[4:5]\n    if ind2 == \" \": ind2 = \"_\"\n    subcode = code[5:6]\n    if subcode == \" \": subcode = None\n    return (code[0:3], ind1, ind2, subcode)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the browser with the appropriate way to prepare a logged in version of the user.", "response": "def _init_browser(self):\n        \"\"\"\n        Ovveride this method with the appropriate way to prepare a logged in\n        browser.\n        \"\"\"\n        self.browser = mechanize.Browser()\n        self.browser.set_handle_robots(False)\n        self.browser.open(self.server_url + \"/youraccount/login\")\n        self.browser.select_form(nr=0)\n        try:\n            self.browser['nickname'] = self.user\n            self.browser['password'] = self.password\n        except:\n            self.browser['p_un'] = self.user\n            self.browser['p_pw'] = self.password\n        # Set login_method to be writable\n        self.browser.form.find_control('login_method').readonly = False\n        self.browser['login_method'] = self.login_method\n        self.browser.submit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, read_cache=True, **kwparams):\n        parse_results = False\n        of = kwparams.get('of', \"\")\n        if of == \"\":\n            parse_results = True\n            of = \"xm\"\n            kwparams['of'] = of\n        params = urllib.urlencode(kwparams, doseq=1)\n\n        # Are we running locally? If so, better directly access the\n        # search engine directly\n        if self.local and of != 't':\n            # See if user tries to search any restricted collection\n            c = kwparams.get('c', \"\")\n            if c != \"\":\n                if type(c) is list:\n                    colls = c\n                else:\n                    colls = [c]\n                for collection in colls:\n                    if collection_restricted_p(collection):\n                        if self.user:\n                            self._check_credentials()\n                            continue\n                        raise InvenioConnectorAuthError(\"You are trying to search a restricted collection. Please authenticate yourself.\\n\")\n            kwparams['of'] = 'id'\n            results = perform_request_search(**kwparams)\n            if of.lower() != 'id':\n                results = format_records(results, of)\n        else:\n            if params + str(parse_results) not in self.cached_queries or not read_cache:\n                if self.user:\n                    results = self.browser.open(self.server_url + \"/search?\" + params)\n                else:\n                    results = urllib2.urlopen(self.server_url + \"/search?\" + params)\n                if 'youraccount/login' in results.geturl():\n                    # Current user not able to search collection\n                    raise InvenioConnectorAuthError(\"You are trying to search a restricted collection. Please authenticate yourself.\\n\")\n            else:\n                return self.cached_queries[params + str(parse_results)]\n\n        if parse_results:\n            # FIXME: we should not try to parse if results is string\n            parsed_records = self._parse_results(results, self.cached_records)\n            self.cached_queries[params + str(parse_results)] = parsed_records\n            return parsed_records\n        else:\n            # pylint: disable=E1103\n            # The whole point of the following code is to make sure we can\n            # handle two types of variable.\n            try:\n                res = results.read()\n            except AttributeError:\n                res = results\n            # pylint: enable=E1103\n\n            if of == \"id\":\n                try:\n                    if type(res) is str:\n                        # Transform to list\n                        res = [int(recid.strip()) for recid in \\\n                        res.strip(\"[]\").split(\",\") if recid.strip() != \"\"]\n                    res.reverse()\n                except (ValueError, AttributeError):\n                    res = []\n            self.cached_queries[params + str(parse_results)] = res\n            return self.cached_queries[params + str(parse_results)]", "response": "Perform a search and return the records corresponding to the given search query."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_records_from_basket(self, bskid, group_basket=False, read_cache=True):\n        if bskid not in self.cached_baskets or not read_cache:\n            if self.user:\n                if group_basket:\n                    group_basket = '&category=G'\n                else:\n                    group_basket = ''\n                results = self.browser.open(self.server_url + \\\n                        \"/yourbaskets/display?of=xm&bskid=\" + str(bskid) + group_basket)\n            else:\n                results = urllib2.urlopen(self.server_url + \\\n                        \"/yourbaskets/display_public?of=xm&bskid=\" + str(bskid))\n        else:\n            return self.cached_baskets[bskid]\n\n        parsed_records = self._parse_results(results, self.cached_records)\n        self.cached_baskets[bskid] = parsed_records\n        return parsed_records", "response": "Returns the records from the public basket with given bskid"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_record(self, recid, read_cache=True):\n        if recid in self.cached_records or not read_cache:\n            return self.cached_records[recid]\n        else:\n            return self.search(p=\"recid:\" + str(recid))", "response": "Returns the record with given recid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nuploading a record to the server.", "response": "def upload_marcxml(self, marcxml, mode):\n        \"\"\"\n        Uploads a record to the server\n\n        Parameters:\n          marcxml - *str* the XML to upload.\n             mode - *str* the mode to use for the upload.\n                    \"-i\" insert new records\n                    \"-r\" replace existing records\n                    \"-c\" correct fields of records\n                    \"-a\" append fields to records\n                    \"-ir\" insert record or replace if it exists\n        \"\"\"\n        if mode not in [\"-i\", \"-r\", \"-c\", \"-a\", \"-ir\"]:\n            raise NameError, \"Incorrect mode \" + str(mode)\n\n        # Are we running locally? If so, submit directly\n        if self.local:\n            (code, marcxml_filepath) = tempfile.mkstemp(prefix=\"upload_%s\" % \\\n                                                        time.strftime(\"%Y%m%d_%H%M%S_\",\n                                                                      time.localtime()))\n            marcxml_file_d = os.fdopen(code, \"w\")\n            marcxml_file_d.write(marcxml)\n            marcxml_file_d.close()\n            return task_low_level_submission(\"bibupload\", \"\", mode, marcxml_filepath)\n        else:\n            params = urllib.urlencode({'file': marcxml,\n                                        'mode': mode})\n            ## We don't use self.browser as batchuploader is protected by IP\n            opener = urllib2.build_opener()\n            opener.addheaders = [('User-Agent', CFG_USER_AGENT)]\n            return opener.open(self.server_url + \"/batchuploader/robotupload\", params,)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the given results and returns a list of the records that were parsed.", "response": "def _parse_results(self, results, cached_records):\n        \"\"\"\n        Parses the given results (in MARCXML format).\n\n        The given \"cached_records\" list is a pool of\n        already existing parsed records (in order to\n        avoid keeping several times the same records in memory)\n        \"\"\"\n        parser = xml.sax.make_parser()\n        handler = RecordsHandler(cached_records)\n        parser.setContentHandler(handler)\n        parser.parse(results)\n        return handler.records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_server_url(self):\n        try:\n            request = requests.head(self.server_url)\n            if request.status_code >= 400:\n                raise InvenioConnectorServerError(\n                    \"Unexpected status code '%d' accessing URL: %s\"\n                    % (request.status_code, self.server_url))\n        except (InvalidSchema, MissingSchema) as err:\n            raise InvenioConnectorServerError(\n                \"Bad schema, expecting http:// or https://:\\n %s\" % (err,))\n        except ConnectionError as err:\n            raise InvenioConnectorServerError(\n                \"Couldn't establish connection to '%s':\\n %s\"\n                % (self.server_url, err))\n        except InvalidURL as err:\n            raise InvenioConnectorServerError(\n                \"Invalid URL '%s':\\n %s\"\n                % (self.server_url, err))\n        except RequestException as err:\n            raise InvenioConnectorServerError(\n                \"Unknown error connecting to '%s':\\n %s\"\n                % (self.server_url, err))", "response": "Validates self. server_url raises InvenioConnectorServerError if the server_url is not valid."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_cumulative_results(self, results):\n        hits = {'total': 0, 'unique': 0}\n\n        day_hits = list(hits[0] for hits in results.values() if hits)\n        for metrics in day_hits:\n            hits['total'] += metrics['nb_hits']\n            hits['unique'] += metrics['nb_uniq_visitors']\n\n        return hits", "response": "Returns a dictionary of the total and unique hits for the the\n        date range."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_result(self):\n        result = get_json_from_remote_server(self.call)\n        referrers = list(result)\n        for referrer in referrers:\n            referrer['sum_visit_length'] = stringify_seconds(referrer['sum_visit_length'])\n        return sorted(referrers, key=itemgetter('nb_visits'), reverse=True)[0:10]", "response": "Perform the call and return a list of referrers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming the call and return a string with the time in hh : mm : ss", "response": "def get_result(self):\n        \"\"\"Perform the call and return a string with the time in hh:mm:ss\"\"\"\n        result = get_json_from_remote_server(self.call)\n        seconds = self._get_average_duration(result) if result else 0\n        return stringify_seconds(seconds)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_result(self):\n        result = get_json_from_remote_server(self.call)\n        if result:\n            date, value = max(result.iteritems(), key=itemgetter(1))\n            return {'date': date, 'users': value}\n        else:\n            return {'date': \"No Data\", 'users': 0}", "response": "Perform the call and return the peak date and how many users"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform a query to Piwik server and return the response.", "response": "def call(self, default_response=None, **query_params):\n        \"\"\"Perform a query to the Piwik server and return the response.\n\n        :param default_response: Return value in case the query fails\n        :param query_params: Dictionary with the parameters of the query\n        \"\"\"\n        query_url = self.get_query_url(**query_params)\n        return self._perform_call(query_url, default_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_query(self, query_params=None):\n        if query_params is None:\n            query_params = {}\n        query = ''\n        query_params['idSite'] = self.site_id\n        if self.api_token is not None:\n            query_params['token_auth'] = self.api_token\n        for key, value in query_params.iteritems():\n            if isinstance(value, list):\n                value = ','.join(value)\n            query += '{}={}&'.format(str(key), str(value))\n        return query[:-1]", "response": "Return a query string for the resource list."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a request to Piwik and return the raw response.", "response": "def _perform_call(self, query_url, default_response=None, timeout=10):\n        \"\"\"Returns the raw results from the API\"\"\"\n        try:\n            response = requests.get(query_url, timeout=timeout)\n        except socket.timeout:\n            current_plugin.logger.warning(\"Timeout contacting Piwik server\")\n            return default_response\n        except Exception:\n            current_plugin.logger.exception(\"Unable to connect\")\n            return default_response\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run(self, records):\n        self_name = type(self).__name__\n        for i, batch in enumerate(grouper(records, self.BATCH_SIZE, skip_missing=True), 1):\n            self.logger.info('%s processing batch %d', self_name, i)\n            try:\n                for j, proc_batch in enumerate(grouper(\n                        process_records(batch).iteritems(), self.BATCH_SIZE, skip_missing=True), 1):\n                    self.logger.info('%s uploading chunk #%d (batch %d)', self_name, j, i)\n                    self.upload_records({k: v for k, v in proc_batch}, from_queue=True)\n            except Exception:\n                self.logger.exception('%s could not upload batch', self_name)\n                return\n            self.logger.info('%s finished batch %d', self_name, i)\n            self.processed_records(batch)\n        self.logger.info('%s finished', self_name)", "response": "Runs the batch upload\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun the initial batch upload COOKIERecords", "response": "def run_initial(self, events):\n        \"\"\"Runs the initial batch upload\n\n        :param events: an iterable containing events\n        \"\"\"\n        self_name = type(self).__name__\n        for i, batch in enumerate(grouper(events, self.INITIAL_BATCH_SIZE, skip_missing=True), 1):\n            self.logger.debug('%s processing initial batch %d', self_name, i)\n\n            for j, processed_batch in enumerate(grouper(\n                    batch, self.BATCH_SIZE, skip_missing=True), 1):\n                self.logger.info('%s uploading initial chunk #%d (batch %d)', self_name, j, i)\n                self.upload_records(processed_batch, from_queue=False)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes after successfully uploading a batch of records from the queue.", "response": "def processed_records(self, records):\n        \"\"\"Executed after successfully uploading a batch of records from the queue.\n\n        :param records: a list of queue entries\n        \"\"\"\n        for record in records:\n            self.logger.debug('Marking as processed: %s', record)\n            record.processed = True\n        db.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmigrates data to S3. Use the `copy` subcommand to copy data to S3. This can be done safely while Indico is running. At the end it will show you what you need to add to your `indico.conf`. Once you updated your config with the new storage backends, you can use the `apply` subcommand to update your database so files will actually be loaded using the new S3 storage backends. In case you ever need to switch back to your previous storage, you can use `revert` to undo the database changes.", "response": "def cli():\n    \"\"\"Migrate data to S3.\n\n    Use the `copy` subcommand to copy data to S3. This can be done\n    safely while Indico is running. At the end it will show you what\n    you need to add to your `indico.conf`.\n\n    Once you updated your config with the new storage backends, you\n    can use the `apply` subcommand to update your database so files\n    will actually be loaded using the new S3 storage backends.\n\n    In case you ever need to switch back to your previous storage,\n    you can use `revert` to undo the database changes.\n    \"\"\"\n    if config.DB_LOG:\n        click.secho('Warning: The database logger is currently enabled (DB_LOG = True).\\n'\n                    'This will slow down the migration. Unless you database is very small, please disable it.',\n                    fg='yellow')\n        click.confirm('Continue anyway?', abort=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copy(source_backend_names, bucket_names, static_bucket_name, s3_endpoint, s3_profile, s3_bucket_policy_file,\n         rclone, output):\n    \"\"\"Copy files to S3.\n\n    This command copies files to S3 and records the necessary database changes\n    in a JSONL file.\n\n    Multiple bucket names can be specified; in that case the bucket name can change\n    based on the year a file was created in. The last bucket name will be the default,\n    while any other bucket name must include a conditional indicating when to use it:\n\n    \\b\n        -B '<2001:indico-pre-2001'\n        -B '<2009:indico-<year>'\n        -B 'indico-<year>-<month>'\n\n    The static bucket name cannot contain any placeholders.\n\n    The indico storage backend will get the same name as the bucket by default,\n    but this can be overridden, e.g. `-B 'indico-<year>/s3-<year>'` would name\n    the bucket 'indico-2018' but use a backend named 's3-2018'.  It is your\n    responsibility to ensure that placeholders match between the two names.\n\n    S3 credentials should be specified in the usual places, i.e.\n    `~/.aws/credentials` for regular S3 access and `~/.config/rclone/rclone.conf`\n    when using rclone.\n    \"\"\"\n    bucket_names = [tuple(x.split('/', 1)) if '/' in x else (x, x.split(':', 1)[-1]) for x in bucket_names]\n    if ':' in bucket_names[-1][0]:\n        raise click.UsageError('Last bucket name cannot contain criteria')\n    if not all(':' in x[0] for x in bucket_names[:-1]):\n        raise click.UsageError('All but the last bucket name need to contain criteria')\n    matches = [(re.match(r'^(<|>|==|<=|>=)\\s*(\\d{4}):(.+)$', name), backend) for name, backend in bucket_names[:-1]]\n    if not all(x[0] for x in matches):\n        raise click.UsageError(\"Could not parse '{}'\".format(bucket_names[matches.index(None)]))\n    criteria = [(match.groups(), backend) for match, backend in matches]\n    # Build and compile a function to get the bucket/backend name to avoid\n    # processing the criteria for every single file (can be millions for large\n    # instances)\n    code = ['def get_bucket_name(dt):']\n    if criteria:\n        for i, ((op, value, bucket), backend) in enumerate(criteria):\n            code.append('    {}if dt.year {} {}:'.format('el' if i else '', op, value))\n            code.append('        bucket, backend = {!r}'.format((bucket, backend)))\n        code.append('    else:')\n        code.append('        bucket, backend = {!r}'.format(bucket_names[-1]))\n    else:\n        code.append('    bucket, backend = {!r}'.format(bucket_names[-1]))\n    code.append('    bucket = bucket.replace(\"<year>\", dt.strftime(\"%Y\"))')\n    code.append('    bucket = bucket.replace(\"<month>\", dt.strftime(\"%m\"))')\n    code.append('    bucket = bucket.replace(\"<week>\", dt.strftime(\"%W\"))')\n    code.append('    backend = backend.replace(\"<year>\", dt.strftime(\"%Y\"))')\n    code.append('    backend = backend.replace(\"<month>\", dt.strftime(\"%m\"))')\n    code.append('    backend = backend.replace(\"<week>\", dt.strftime(\"%W\"))')\n    code.append('    return bucket, backend')\n    d = {}\n    exec '\\n'.join(code) in d\n    if not source_backend_names:\n        source_backend_names = [x for x in config.STORAGE_BACKENDS if not isinstance(get_storage(x), S3StorageBase)]\n    if rclone:\n        invalid = [x for x in source_backend_names if not isinstance(get_storage(x), FileSystemStorage)]\n        if invalid:\n            click.secho('Found unsupported storage backends: {}'.format(', '.join(sorted(invalid))), fg='yellow')\n            click.secho('The backends might not work together with `--rclone`', fg='yellow')\n            click.confirm('Continue anyway?', abort=True)\n    s3_bucket_policy = s3_bucket_policy_file.read() if s3_bucket_policy_file else None\n    imp = S3Importer(d['get_bucket_name'], static_bucket_name,\n                     output, source_backend_names, rclone,\n                     s3_endpoint, s3_profile, s3_bucket_policy)\n    with monkeypatch_registration_file_time():\n        imp.run()", "response": "Copy files to S3 and record the necessary database changes in a JSONL file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconnect websocket to deCONZ.", "response": "def start(self) -> None:\n        \"\"\"Connect websocket to deCONZ.\"\"\"\n        if self.config:\n            self.websocket = self.ws_client(\n                self.loop, self.session, self.host,\n                self.config.websocketport, self.async_session_handler)\n            self.websocket.start()\n        else:\n            _LOGGER.error('No deCONZ config available')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def async_load_parameters(self) -> bool:\n        data = await self.async_get_state('')\n\n        _LOGGER.debug(pformat(data))\n\n        config = data.get('config', {})\n        groups = data.get('groups', {})\n        lights = data.get('lights', {})\n        sensors = data.get('sensors', {})\n\n        if not self.config:\n            self.config = DeconzConfig(config)\n\n        # Update scene for existing groups\n        for group_id, group in groups.items():\n            if group_id in self.groups:\n                self.groups[group_id].async_add_scenes(\n                    group.get('scenes'), self.async_put_state)\n\n        self.groups.update({\n            group_id: DeconzGroup(group_id, group, self.async_put_state)\n            for group_id, group in groups.items()\n            if group_id not in self.groups\n        })\n\n        self.lights.update({\n            light_id: DeconzLight(light_id, light, self.async_put_state)\n            for light_id, light in lights.items()\n            if light_id not in self.lights\n        })\n        self.update_group_color(self.lights.keys())\n\n        self.scenes.update({\n            group.id + '_' + scene.id: scene\n            for group in self.groups.values()\n            for scene in group.scenes.values()\n            if group.id + '_' + scene.id not in self.scenes\n        })\n\n        self.sensors.update({\n            sensor_id: create_sensor(sensor_id, sensor, self.async_put_state)\n            for sensor_id, sensor in sensors.items()\n            if supported_sensor(sensor) and sensor_id not in self.sensors\n        })", "response": "Load parameters from the deCONZ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def async_put_state(self, field: str, data: dict) -> dict:\n        session = self.session.put\n        url = self.api_url + field\n        jsondata = json.dumps(data)\n        response_dict = await async_request(session, url, data=jsondata)\n        return response_dict", "response": "Set state of object in deCONZ."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def async_get_state(self, field: str) -> dict:\n        session = self.session.get\n        url = self.api_url + field\n        response_dict = await async_request(session, url)\n        return response_dict", "response": "Get state of object in deCONZ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef async_session_handler(self, signal: str) -> None:\n        if signal == 'data':\n            self.async_event_handler(self.websocket.data)\n        elif signal == 'state':\n            if self.async_connection_status_callback:\n                self.async_connection_status_callback(\n                    self.websocket.state == 'running')", "response": "Signalling from websocket.\n\n           data - new data available for processing.\n           state - network state has changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreceive event from websocket and update the internal state.", "response": "def async_event_handler(self, event: dict) -> None:\n        \"\"\"Receive event from websocket and identifies where the event belong.\n\n        {\n            \"t\": \"event\",\n            \"e\": \"changed\",\n            \"r\": \"sensors\",\n            \"id\": \"12\",\n            \"state\": { \"buttonevent\": 2002 }\n        }\n        \"\"\"\n        if event['e'] == 'added':\n\n            if event['r'] == 'lights' and event['id'] not in self.lights:\n                device_type = 'light'\n                device = self.lights[event['id']] = DeconzLight(\n                    event['id'], event['light'], self.async_put_state)\n\n            elif event['r'] == 'sensors' and event['id'] not in self.sensors:\n                if supported_sensor(event['sensor']):\n                    device_type = 'sensor'\n                    device = self.sensors[event['id']] = create_sensor(\n                        event['id'], event['sensor'], self.async_put_state)\n                else:\n                    _LOGGER.warning('Unsupported sensor %s', event)\n                    return\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n                return\n\n            if self.async_add_device_callback:\n                self.async_add_device_callback(device_type, device)\n\n        elif event['e'] == 'changed':\n\n            if event['r'] == 'groups' and event['id'] in self.groups:\n                self.groups[event['id']].async_update(event)\n\n            elif event['r'] == 'lights' and event['id'] in self.lights:\n                self.lights[event['id']].async_update(event)\n                self.update_group_color([event['id']])\n\n            elif event['r'] == 'sensors' and event['id'] in self.sensors:\n                self.sensors[event['id']].async_update(event)\n\n            else:\n                _LOGGER.debug('Unsupported event %s', event)\n\n        elif event['e'] == 'deleted':\n            _LOGGER.debug('Removed event %s', event)\n\n        else:\n            _LOGGER.debug('Unsupported event %s', event)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_group_color(self, lights: list) -> None:\n        for group in self.groups.values():\n            # Skip group if there are no common light ids.\n            if not any({*lights} & {*group.lights}):\n                continue\n\n            # More than one light means load_parameters called this method.\n            # Then we take first best light to be available.\n            light_ids = lights\n            if len(light_ids) > 1:\n                light_ids = group.lights\n\n            for light_id in light_ids:\n                if self.lights[light_id].reachable:\n                    group.update_color_state(self.lights[light_id])\n                    break", "response": "Update group colors based on light states."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsubmit new export job.", "response": "def submit_export(cls, file, volume, location, properties=None,\n                      overwrite=False, copy_only=False, api=None):\n\n        \"\"\"\n        Submit new export job.\n        :param file: File to be exported.\n        :param volume: Volume identifier.\n        :param location: Volume location.\n        :param properties: Properties dictionary.\n        :param overwrite: If true it will overwrite file if exists\n        :param copy_only: If true files are kept on SevenBridges bucket.\n        :param api: Api Instance.\n        :return: Export object.\n        \"\"\"\n        data = {}\n        params = {}\n\n        volume = Transform.to_volume(volume)\n        file = Transform.to_file(file)\n        destination = {\n            'volume': volume,\n            'location': location\n        }\n        source = {\n            'file': file\n        }\n        if properties:\n            data['properties'] = properties\n\n        data['source'] = source\n        data['destination'] = destination\n        data['overwrite'] = overwrite\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Submitting export', extra=extra)\n\n        api = api if api else cls._API\n        if copy_only:\n            params['copy_only'] = True\n            _export = api.post(\n                cls._URL['query'], data=data, params=params).json()\n        else:\n            _export = api.post(\n                cls._URL['query'], data=data).json()\n\n        return Export(api=api, **_export)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef query(cls, volume=None, state=None, offset=None,\n              limit=None, api=None):\n\n        \"\"\"\n        Query (List) exports.\n        :param volume: Optional volume identifier.\n        :param state: Optional import sate.\n        :param api: Api instance.\n        :return: Collection object.\n        \"\"\"\n        api = api or cls._API\n\n        if volume:\n            volume = Transform.to_volume(volume)\n\n        return super(Export, cls)._query(\n            url=cls._URL['query'], volume=volume, state=state, offset=offset,\n            limit=limit, fields='_all', api=api\n        )", "response": "Query ( List ) exports.\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bulk_get(cls, exports, api=None):\n        api = api or cls._API\n        export_ids = [Transform.to_export(export) for export in exports]\n        data = {'export_ids': export_ids}\n\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return ExportBulkRecord.parse_records(response=response, api=api)", "response": "Retrieve exports in bulk."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubmitting exports in bulk.", "response": "def bulk_submit(cls, exports, copy_only=False, api=None):\n        \"\"\"\n        Create exports in bulk.\n        :param exports: Exports to be submitted in bulk.\n        :param copy_only: If true files are kept on SevenBridges bucket.\n        :param api: Api instance.\n        :return: list of ExportBulkRecord objects.\n        \"\"\"\n        if not exports:\n            raise SbgError('Exports are required')\n\n        api = api or cls._API\n\n        items = []\n        for export in exports:\n            file_ = Transform.to_file(export.get('file'))\n            volume = Transform.to_volume(export.get('volume'))\n            location = Transform.to_location(export.get('location'))\n            properties = export.get('properties', {})\n            overwrite = export.get('overwrite', False)\n\n            item = {\n                'source': {\n                    'file': file_\n                },\n                'destination': {\n                    'volume': volume,\n                    'location': location\n                },\n                'properties': properties,\n                'overwrite': overwrite\n            }\n\n            items.append(item)\n\n        data = {'items': items}\n        params = {'copy_only': copy_only}\n\n        response = api.post(\n            url=cls._URL['bulk_create'], params=params, data=data\n        )\n        return ExportBulkRecord.parse_records(response=response, api=api)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef query(cls, offset=None, limit=None, api=None):\n        api = api if api else cls._API\n        return super(Division, cls)._query(\n            url=cls._URL['query'], offset=offset, limit=limit,\n            fields='_all', api=api\n        )", "response": "Query the list of all related items in the current division."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting a new API key for devicetype.", "response": "async def async_get_api_key(session, host, port, username=None, password=None, **kwargs):\n    \"\"\"Get a new API key for devicetype.\"\"\"\n    url = 'http://{host}:{port}/api'.format(host=host, port=str(port))\n\n    auth = None\n    if username and password:\n        auth = aiohttp.BasicAuth(username, password=password)\n\n    data = b'{\"devicetype\": \"pydeconz\"}'\n    response = await async_request(session.post, url, auth=auth, data=data)\n\n    api_key = response[0]['success']['username']\n    _LOGGER.info(\"API key: %s\", api_key)\n    return api_key"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def async_delete_api_key(session, host, port, api_key):\n    url = 'http://{host}:{port}/api/{api_key}/config/whitelist/{api_key}'.format(\n        host=host, port=str(port), api_key=api_key)\n\n    response = await async_request(session.delete, url)\n\n    _LOGGER.info(response)", "response": "Delete API key from deCONZ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def async_delete_all_keys(session, host, port, api_key, api_keys=[]):\n    url = 'http://{}:{}/api/{}/config'.format(host, str(port), api_key)\n\n    response = await async_request(session.get, url)\n\n    api_keys.append(api_key)\n    for key in response['whitelist'].keys():\n        if key not in api_keys:\n            await async_delete_api_key(session, host, port, key)", "response": "Delete all API keys except for the ones provided to the method."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def async_get_bridgeid(session, host, port, api_key, **kwargs):\n    url = 'http://{}:{}/api/{}/config'.format(host, str(port), api_key)\n\n    response = await async_request(session.get, url)\n\n    bridgeid = response['bridgeid']\n    _LOGGER.info(\"Bridge id: %s\", bridgeid)\n    return bridgeid", "response": "Get bridge id for bridge."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def async_discovery(session):\n    bridges = []\n    response = await async_request(session.get, URL_DISCOVER)\n\n    if not response:\n        _LOGGER.info(\"No discoverable bridges available.\")\n        return bridges\n\n    for bridge in response:\n        bridges.append({'bridgeid': bridge['id'],\n                        'host': bridge['internalipaddress'],\n                        'port': bridge['internalport']})\n\n    _LOGGER.info(\"Discovered the following bridges: %s.\", bridges)\n\n    return bridges", "response": "Find bridges allowing gateway discovery."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def async_request(session, url, **kwargs):\n    _LOGGER.debug(\"Sending %s to %s\", kwargs, url)\n\n    try:\n        res = await session(url, **kwargs)\n\n        if res.content_type != 'application/json':\n            raise ResponseError(\n                \"Invalid content type: {}\".format(res.content_type))\n\n        response = await res.json()\n        _LOGGER.debug(\"HTTP request response: %s\", response)\n\n        _raise_on_error(response)\n\n        return response\n\n    except aiohttp.client_exceptions.ClientError as err:\n        raise RequestError(\n            \"Error requesting data from {}: {}\".format(url, err)\n        ) from None", "response": "Do a web request and manage response."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretries to connect to deCONZ.", "response": "def retry(self):\n        \"\"\"Retry to connect to deCONZ.\"\"\"\n        self.state = STATE_STARTING\n        self.loop.call_later(RETRY_TIMER, self.start)\n        _LOGGER.debug('Reconnecting to deCONZ in %i.', RETRY_TIMER)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init_done(self, fut):\n        try:\n            if fut.exception():\n                fut.result()\n        except OSError as err:\n            _LOGGER.debug('Got exception %s', err)\n            self.retry()", "response": "Callback when the init operation is complete."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection_made(self, transport):\n        randomness = os.urandom(16)\n        key = base64encode(randomness).decode('utf-8').strip()\n        self.transport = transport\n        message = \"GET / HTTP/1.1\\r\\n\"\n        message += \"Host: \" + self.host + ':' + str(self.port) + '\\r\\n'\n        message += \"User-Agent: Python/3.5 websockets/3.4\\r\\n\"\n        message += \"Upgrade: Websocket\\r\\n\"\n        message += \"Connection: Upgrade\\r\\n\"\n        message += \"Sec-WebSocket-Key: \" + key + \"\\r\\n\"\n        message += \"Sec-WebSocket-Version: 13\\r\\n\"\n        message += \"\\r\\n\"\n        _LOGGER.debug('Websocket handshake: %s', message)\n        self.transport.write(message.encode())", "response": "Do the websocket handshake."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef data_received(self, data):\n        if self.state == STATE_STARTING:\n            self.state = STATE_RUNNING\n            _LOGGER.debug('Websocket handshake: %s', data.decode())\n            return\n        _LOGGER.debug('Websocket data: %s', data)\n\n        while len(data) > 0:\n            payload, extra_data = self.get_payload(data)\n            self._data = payload ###\n            self.async_session_handler_callback('data')###\n            #self.async_callback(payload)\n            data = extra_data", "response": "Data received over websocket."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef connection_lost(self, exc):\n        if self.state == STATE_RUNNING:\n            _LOGGER.warning('Lost connection to deCONZ')\n            self.retry()", "response": "Happen when device closes connection or stop has been called."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses length of payload and return it.", "response": "def get_payload(self, data):\n        \"\"\"Parse length of payload and return it.\"\"\"\n        start = 2\n        length = ord(data[1:2])\n        if length == 126:\n            # Payload information are an extra 2 bytes.\n            start = 4\n            length, = unpack(\">H\", data[2:4])\n        elif length == 127:\n            # Payload information are an extra 6 bytes.\n            start = 8\n            length, = unpack(\">I\", data[2:6])\n        end = start + length\n        payload = json.loads(data[start:end].decode())\n        extra_data = data[end:]\n        return payload, extra_data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_old_vidyo_rooms(max_room_event_age):\n    recently_used = (db.session.query(VCRoom.id)\n                     .filter(VCRoom.type == 'vidyo',\n                             Event.end_dt > (now_utc() - timedelta(days=max_room_event_age)))\n                     .join(VCRoom.events)\n                     .join(VCRoomEventAssociation.event)\n                     .group_by(VCRoom.id))\n\n    # non-deleted rooms with no recent associations\n    return VCRoom.find_all(VCRoom.status != VCRoomStatus.deleted, ~VCRoom.id.in_(recently_used))", "response": "Finds all Vidyo rooms that are older than max_room_event_age days."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef notify_owner(plugin, vc_room):\n    user = vc_room.vidyo_extension.owned_by_user\n    tpl = get_plugin_template_module('emails/remote_deleted.html', plugin=plugin, vc_room=vc_room, event=None,\n                                     vc_room_event=None, user=user)\n    _send('delete', user, plugin, None, vc_room, tpl)", "response": "Notifies about the deletion of a Vidyo room from the Vidyo server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rooms(status=None):\n\n    room_query = VCRoom.find(type='vidyo')\n    table_data = [['ID', 'Name', 'Status', 'Vidyo ID', 'Extension']]\n\n    if status:\n        room_query = room_query.filter(VCRoom.status == VCRoomStatus.get(status))\n\n    for room in room_query:\n        table_data.append([unicode(room.id), room.name, room.status.name,\n                           unicode(room.data['vidyo_id']), unicode(room.vidyo_extension.extension)])\n\n    table = AsciiTable(table_data)\n    for col in (0, 3, 4):\n        table.justify_columns[col] = 'right'\n    print table.table", "response": "Lists all Vidyo rooms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef config_vars(profiles, advance_access):\n    for profile in profiles:\n        try:\n            config = Config(profile, advance_access=advance_access)\n            url = config.api_endpoint\n            token = config.auth_token\n            proxies = config.proxies\n            aa = config.advance_access\n            return url, token, proxies, aa\n        except Exception:\n            pass\n    return None, None, None, None", "response": "Utility method to fetch config vars using ini section profile\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, request, **kwargs):\n        if len(request.url) > self.MAX_URL_LENGTH:\n            raise URITooLong(\n                message=(\n                    'Request url too large, '\n                    'likely too many query parameters provided.'\n                )\n            )\n        return super(RequestSession, self).send(request, **kwargs)", "response": "Send a request to the specified node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering a template from the engine plugin or the search plugin", "response": "def render_engine_or_search_template(template_name, **context):\n    \"\"\"Renders a template from the engine plugin or the search plugin\n\n    If the template is available in the engine plugin, it's taken\n    from there, otherwise the template from this plugin is used.\n\n    :param template_name: name of the template\n    :param context: the variables that should be available in the\n                    context of the template.\n    \"\"\"\n    from indico_search.plugin import SearchPlugin\n    assert current_plugin == SearchPlugin.instance\n\n    templates = ('{}:{}'.format(SearchPlugin.instance.engine_plugin.name, template_name),\n                 template_name)\n    return render_plugin_template(templates, **context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_user_identities(user):\n    from indico_vc_vidyo.plugin import VidyoPlugin\n    providers = authenticators_re.split(VidyoPlugin.settings.get('authenticators'))\n    done = set()\n    for provider in providers:\n        for _, identifier in user.iter_identifiers(check_providers=True, providers={provider}):\n            if identifier in done:\n                continue\n            done.add(identifier)\n            yield identifier", "response": "Iterates over all user identities that can be used with Vidyo"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_user_from_identifier(settings, identifier):\n    providers = list(auth.strip() for auth in settings.get('authenticators').split(','))\n    identities = Identity.find_all(Identity.provider.in_(providers), Identity.identifier == identifier)\n    if identities:\n        return sorted(identities, key=lambda x: providers.index(x.provider))[0].user\n    for provider in providers:\n        try:\n            identity_info = multipass.get_identity(provider, identifier)\n        except IdentityRetrievalFailed:\n            continue\n        if identity_info is None:\n            continue\n        if not identity_info.provider.settings.get('trusted_email'):\n            continue\n        emails = {email.lower() for email in identity_info.data.getlist('email') if email}\n        if not emails:\n            continue\n        user = User.find_first(~User.is_deleted, User.all_emails.in_(list(emails)))\n        if user:\n            return user", "response": "Get an actual User object from an identifier"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_extensions(prefix, event_id):\n    extension = '{prefix}{event_id}'.format(prefix=prefix, event_id=event_id)\n    yield extension\n    suffix = 1\n    while True:\n        yield '{extension}{suffix}'.format(extension=extension, suffix=suffix)\n        suffix += 1", "response": "Iterate over the available extensions for a given event_id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_room_from_obj(settings, vc_room, room_obj):\n    vc_room.name = room_obj.name\n    if room_obj.ownerName != vc_room.data['owner_identity']:\n        owner = get_user_from_identifier(settings, room_obj.ownerName) or User.get_system_user()\n        vc_room.vidyo_extension.owned_by_user = owner\n\n    vc_room.data.update({\n        'description': room_obj.description,\n        'vidyo_id': unicode(room_obj.roomID),\n        'url': room_obj.RoomMode.roomURL,\n        'owner_identity': room_obj.ownerName,\n        'room_pin': room_obj.RoomMode.roomPIN if room_obj.RoomMode.hasPIN else \"\",\n        'moderation_pin': room_obj.RoomMode.moderatorPIN if room_obj.RoomMode.hasModeratorPIN else \"\",\n    })\n    vc_room.vidyo_extension.extension = int(room_obj.extension)", "response": "Updates a VCRoom DB object using a SOAP room object returned by the API."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(cls, api=None, **kwargs):\n        api = api if api else cls._API\n        extra = {\n            'resource': cls.__name__,\n            'query': {}\n        }\n        logger.info('Getting resources', extra=extra)\n        endpoints = api.get(url=cls._URL['get']).json()\n        return Endpoints(api=api, **endpoints)", "response": "Get api links.\n        :param api: Api instance.\n        :return: Endpoints object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new parser that executes the passed parsers with the previous results and yields a tuple of the results.", "response": "def parse_chain(*args):\n\t\"\"\"Creates a new parser that executes the passed parsers (args) with the\n\tprevious results and yields a tuple of the results.\n\n\t>>> list(parse_chain(lambda: (None, 1), lambda one: (None, 2)))\n\t[None, None, (1, 2)]\n\n\t@param args: parsers\n\t@returns: parser\n\t\"\"\"\n\titems = []\n\tfor parser in args:\n\t\tfor element in parser(*items):  # pylint:disable=star-args\n\t\t\tif element is None:\n\t\t\t\tyield None\n\t\t\telse:\n\t\t\t\titems.append(element)\n\t\t\t\tbreak\n\tyield tuple(items)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pack_ip(ipstr):\n\tif not isinstance(ipstr, basestring):\n\t\traise ValueError(\"given ip address is not a string\")\n\tparts = ipstr.split('.')\n\tif len(parts) != 4:\n\t\traise ValueError(\"given ip address has an invalid number of dots\")\n\tparts = [int(x) for x in parts]  # raises ValueError\n\treturn int_seq_to_bytes(parts)", "response": "Converts an ip address given in dotted notation to a four byte string in network byte order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpack_ip(fourbytes):\n\tif not isinstance(fourbytes, bytes):\n\t\traise ValueError(\"given buffer is not a string\")\n\tif len(fourbytes) != 4:\n\t\traise ValueError(\"given buffer is not exactly four bytes long\")\n\treturn \".\".join([str(x) for x in bytes_to_int_seq(fourbytes)])", "response": "Converts an ip address given in a four byte string in network\n\tbyte order to a dotted notation string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a mac address given in colon delimited notation to a bytestring in network byte order.", "response": "def pack_mac(macstr):\n\t\"\"\"Converts a mac address given in colon delimited notation to a\n\tsix byte string in network byte order.\n\n\t>>> pack_mac(\"30:31:32:33:34:35\") == b'012345'\n\tTrue\n\t>>> pack_mac(\"bad\")\n\tTraceback (most recent call last):\n\t...\n\tValueError: given mac addresses has an invalid number of colons\n\n\n\t@type macstr: str\n\t@rtype: bytes\n\t@raises ValueError: for badly formatted mac addresses\n\t\"\"\"\n\tif not isinstance(macstr, basestring):\n\t\traise ValueError(\"given mac addresses is not a string\")\n\tparts = macstr.split(\":\")\n\tif len(parts) != 6:\n\t\traise ValueError(\"given mac addresses has an invalid number of colons\")\n\tparts = [int(part, 16) for part in parts]  # raises ValueError\n\treturn int_seq_to_bytes(parts)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a mac address given in a six byte string in network byte order to a string in colon delimited notation.", "response": "def unpack_mac(sixbytes):\n\t\"\"\"Converts a mac address given in a six byte string in network\n\tbyte order to a string in colon delimited notation.\n\n\t>>> unpack_mac(b\"012345\")\n\t'30:31:32:33:34:35'\n\t>>> unpack_mac(b\"bad\")\n\tTraceback (most recent call last):\n\t...\n\tValueError: given buffer is not exactly six bytes long\n\n\t@type sixbytes: bytes\n\t@rtype: str\n\t@raises ValueError: for bad input\n\t\"\"\"\n\tif not isinstance(sixbytes, bytes):\n\t\traise ValueError(\"given buffer is not a string\")\n\tif len(sixbytes) != 6:\n\t\traise ValueError(\"given buffer is not exactly six bytes long\")\n\treturn \":\".join([\"%2.2x\".__mod__(x) for x in bytes_to_int_seq(sixbytes)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding data to the end of the buffer", "response": "def add(self, data):\n\t\t\"\"\"\n\t\t>>> ob = OutBuffer().add(OutBuffer.sizelimit * b\"x\")\n\t\t>>> ob.add(b\"y\") # doctest: +ELLIPSIS\n\t\tTraceback (most recent call last):\n\t\t...\n\t\tOmapiSizeLimitError: ...\n\n\t\t@type data: bytes\n\t\t@returns: self\n\t\t@raises OmapiSizeLimitError:\n\t\t\"\"\"\n\t\tif len(self) + len(data) > self.sizelimit:\n\t\t\traise OmapiSizeLimitError()\n\t\tself.buff.write(data)\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a 32bit string to the internal buffer", "response": "def add_net32string(self, string):\n\t\t\"\"\"\n\t\t>>> r = b'\\\\x00\\\\x00\\\\x00\\\\x01x'\n\t\t>>> OutBuffer().add_net32string(b\"x\").getvalue() == r\n\t\tTrue\n\n\t\t@type string: bytes\n\t\t@param string: maximum length must fit in a 32bit integer\n\t\t@returns: self\n\t\t@raises OmapiSizeLimitError:\n\t\t\"\"\"\n\t\tif len(string) >= (1 << 32):\n\t\t\traise ValueError(\"string too long\")\n\t\treturn self.add_net32int(len(string)).add(string)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_net16string(self, string):\n\t\tif len(string) >= (1 << 16):\n\t\t\traise ValueError(\"string too long\")\n\t\treturn self.add_net16int(len(string)).add(string)", "response": "add a string to the internal buffer as a 16bit integer"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_bindict(self, items):\n\t\tif not isinstance(items, list):\n\t\t\titems = items.items()\n\t\tfor key, value in items:\n\t\t\tself.add_net16string(key).add_net32string(value)\n\t\treturn self.add(b\"\\x00\\x00\")", "response": "add a bindict to the output buffer"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconsumes a given number of bytes from the output buffer.", "response": "def consume(self, length):\n\t\t\"\"\"\n\t\t>>> OutBuffer().add(b\"spam\").consume(2).getvalue() == b\"am\"\n\t\tTrue\n\n\t\t@type length: int\n\t\t@returns: self\n\t\t\"\"\"\n\t\tself.buff = io.BytesIO(self.getvalue()[length:])\n\t\treturn self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking whether this OmapiStartupMessage matches the implementation.", "response": "def validate(self):\n\t\t\"\"\"Checks whether this OmapiStartupMessage matches the implementation.\n\t\t@raises OmapiError:\n\t\t\"\"\"\n\t\tif self.implemented_protocol_version != self.protocol_version:\n\t\t\traise OmapiError(\"protocol mismatch\")\n\t\tif self.implemented_header_size != self.header_size:\n\t\t\traise OmapiError(\"header size mismatch\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing this OmapiStartupMessage to the given outbuffer.", "response": "def serialize(self, outbuffer):\n\t\t\"\"\"Serialize this OmapiStartupMessage to the given outbuffer.\n\t\t@type outbuffer: OutBuffer\n\t\t\"\"\"\n\t\toutbuffer.add_net32int(self.protocol_version)\n\t\toutbuffer.add_net32int(self.header_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign(self, message):\n\t\treturn hmac.HMAC(self.key, message, digestmod=hashlib.md5).digest()", "response": "Sign a message with the HMAC - MD5 algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef as_string(self, forsigning=False):\n\t\tret = OutBuffer()\n\t\tself.serialize(ret, forsigning)\n\t\treturn ret.getvalue()", "response": "Returns the message as a string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sign(self, authenticator):\n\t\tself.authid = authenticator.authid\n\t\tself.signature = b\"\\0\" * authenticator.authlen  # provide authlen\n\t\tself.signature = authenticator.sign(self.as_string(forsigning=True))\n\t\tassert len(self.signature) == authenticator.authlen", "response": "Sign this OMAPI message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify this OMAPI message.", "response": "def verify(self, authenticators):\n\t\t\"\"\"Verify this OMAPI message.\n\n\t\t>>> a1 = OmapiHMACMD5Authenticator(b\"egg\", b\"spam\")\n\t\t>>> a2 = OmapiHMACMD5Authenticator(b\"egg\", b\"tomatoes\")\n\t\t>>> a1.authid = a2.authid = 5\n\t\t>>> m = OmapiMessage.open(b\"host\")\n\t\t>>> m.verify({a1.authid: a1})\n\t\tFalse\n\t\t>>> m.sign(a1)\n\t\t>>> m.verify({a1.authid: a1})\n\t\tTrue\n\t\t>>> m.sign(a2)\n\t\t>>> m.verify({a1.authid: a1})\n\t\tFalse\n\n\t\t@type authenticators: {int: OmapiAuthenticatorBase}\n\t\t@rtype: bool\n\t\t\"\"\"\n\t\ttry:\n\t\t\treturn authenticators[self.authid]. sign(self.as_string(forsigning=True)) == self.signature\n\t\texcept KeyError:\n\t\t\treturn False"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef open(cls, typename):\n\t\treturn cls(opcode=OMAPI_OP_OPEN, message=[(b\"type\", typename)], tid=-1)", "response": "Create an OMAPI open message with given typename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_net16string(self):\n\t\treturn parse_map(operator.itemgetter(1), parse_chain(self.parse_net16int, self.parse_fixedbuffer))", "response": "Parse a string containing a net16 integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_net32string(self):\n\t\treturn parse_map(operator.itemgetter(1), parse_chain(self.parse_net32int, self.parse_fixedbuffer))", "response": "Parse a net32 string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_bindict(self):\n\t\tentries = []\n\t\ttry:\n\t\t\twhile True:\n\t\t\t\tfor key in self.parse_net16string():\n\t\t\t\t\tif key is None:\n\t\t\t\t\t\tyield None\n\t\t\t\t\telif not key:\n\t\t\t\t\t\traise StopIteration()\n\t\t\t\t\telse:\n\t\t\t\t\t\tfor value in self.parse_net32string():\n\t\t\t\t\t\t\tif value is None:\n\t\t\t\t\t\t\t\tyield None\n\t\t\t\t\t\t\telse:\n\t\t\t\t\t\t\t\tentries.append((key, value))\n\t\t\t\t\t\t\t\tbreak\n\t\t\t\t\t\tbreak\n\t\t# Abusing StopIteration here, since nothing should be throwing\n\t\t# it at us.\n\t\texcept StopIteration:\n\t\t\tyield entries", "response": ">>> d = b\"\\\\0\\\\x01a\\\\0\\\\0\\\\0\\\\x01b\\\\0\\\\0spam\"\n\t\t>>> next(InBuffer(d).parse_bindict()) == [(b'a', b'b')]\n\t\tTrue"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_startup_message(self):\n\t\treturn parse_map(lambda args: OmapiStartupMessage(*args), parse_chain(self.parse_net32int, lambda _: self.parse_net32int()))", "response": "results in an OmapiStartupMessage"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nresult in an OmapiMessage", "response": "def parse_message(self):\n\t\t\"\"\"results in an OmapiMessage\"\"\"\n\t\tparser = parse_chain(self.parse_net32int,  # authid\n\t\t\t\t\t\t\tlambda *_: self.parse_net32int(),  # authlen\n\t\t\t\t\t\t\tlambda *_: self.parse_net32int(),  # opcode\n\t\t\t\t\t\t\tlambda *_: self.parse_net32int(),  # handle\n\t\t\t\t\t\t\tlambda *_: self.parse_net32int(),  # tid\n\t\t\t\t\t\t\tlambda *_: self.parse_net32int(),  # rid\n\t\t\t\t\t\t\tlambda *_: self.parse_bindict(),  # message\n\t\t\t\t\t\t\tlambda *_: self.parse_bindict(),  # object\n\t\t\t\t\t\t\tlambda *args: self.parse_fixedbuffer(args[1]))  # signature\n\t\treturn parse_map(lambda args:  # skip authlen in args:\n\t\t\t\tOmapiMessage(*(args[0:1] + args[2:])), parser)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread bytes from the connection and hand them to the protocol.", "response": "def fill_inbuffer(self):\n\t\t\"\"\"Read bytes from the connection and hand them to the protocol.\n\t\t@raises OmapiError:\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tif not self.connection:\n\t\t\traise OmapiError(\"not connected\")\n\t\ttry:\n\t\t\tdata = self.connection.recv(2048)\n\t\texcept socket.error:\n\t\t\tself.close()\n\t\t\traise\n\t\tif not data:\n\t\t\tself.close()\n\t\t\traise OmapiError(\"connection closed\")\n\t\ttry:\n\t\t\tself.protocol.data_received(data)\n\t\texcept OmapiSizeLimitError:\n\t\t\tself.close()\n\t\t\traise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, data):\n\t\ttry:\n\t\t\tself.connection.sendall(data)\n\t\texcept socket.error:\n\t\t\tself.close()\n\t\t\traise", "response": "Send all of data to the connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send_message(self, message, sign=True):\n\t\tif sign:\n\t\t\tmessage.sign(self.authenticators[self.defauth])\n\t\tlogger.debug(\"sending %s\", LazyStr(message.dump_oneline))\n\t\tself.transport.write(message.as_string())", "response": "Send a message to the connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef receive_message(self):\n\t\twhile not self.recv_message_queue:\n\t\t\tself.transport.fill_inbuffer()\n\t\tmessage = self.recv_message_queue.pop(0)\n\t\tassert message is not None\n\t\tif not message.verify(self.protocol.authenticators):\n\t\t\tself.close()\n\t\t\traise OmapiError(\"bad omapi message signature\")\n\t\treturn message", "response": "Read the next message from the connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef receive_response(self, message, insecure=False):\n\t\tresponse = self.receive_message()\n\t\tif not response.is_response(message):\n\t\t\traise OmapiError(\"received message is not the desired response\")\n\t\t# signature already verified\n\t\tif response.authid != self.protocol.defauth and not insecure:\n\t\t\traise OmapiError(\"received message is signed with wrong authenticator\")\n\t\treturn response", "response": "Read the response for the given message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends the given message to the connection.", "response": "def send_message(self, message, sign=True):\n\t\t\"\"\"Sends the given message to the connection.\n\t\t@type message: OmapiMessage\n\t\t@type sign: bool\n\t\t@param sign: whether the message needs to be signed\n\t\t@raises OmapiError:\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tself.check_connected()\n\t\tself.protocol.send_message(message, sign)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlooking for a lease object with given mac address and return theassigned ip address.", "response": "def lookup_ip(self, mac):\n\t\t\"\"\"Look for a lease object with given mac address and return the\n\t\tassigned ip address.\n\n\t\t@type mac: str\n\t\t@rtype: str or None\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises OmapiErrorNotFound: if no lease object with the given mac could be found\n\t\t@raises OmapiErrorAttributeNotFound: if lease could be found, but objects lacks a ip\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tres = self.lookup_by_lease(mac=mac)\n\t\ttry:\n\t\t\treturn res[\"ip-address\"]\n\t\texcept KeyError:\n\t\t\traise OmapiErrorAttributeNotFound()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lookup_mac(self, ip):\n\t\tres = self.lookup_by_lease(ip=ip)\n\t\ttry:\n\t\t\treturn res[\"hardware-address\"]\n\t\texcept KeyError:\n\t\t\traise OmapiErrorAttributeNotFound()", "response": "Look up a lease object with given ip address and return the\n\tassociated mac address."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks for a host object with given name and return the the name mac and ip address.", "response": "def lookup_host(self, name):\n\t\t\"\"\"Look for a host object with given name and return the\n\t\tname, mac, and ip address\n\n\t\t@type name: str\n\t\t@rtype: dict or None\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises OmapiErrorNotFound: if no host object with the given name could be found\n\t\t@raises OmapiErrorAttributeNotFound: if lease could be found, but objects lacks ip, mac or name\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tres = self.lookup_by_host(name=name)\n\t\ttry:\n\t\t\treturn dict(ip=res[\"ip-address\"], mac=res[\"hardware-address\"], hostname=res[\"name\"].decode('utf-8'))\n\t\texcept KeyError:\n\t\t\traise OmapiErrorAttributeNotFound()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lookup_hostname(self, ip):\n\t\tres = self.lookup_by_lease(ip=ip)\n\t\tif \"client-hostname\" not in res:\n\t\t\traise OmapiErrorAttributeNotFound()\n\t\treturn res[\"client-hostname\"].decode('utf-8')", "response": "Look up a lease object with given ip address and return the associated client hostname."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a host object with given ip address and mac address.", "response": "def add_host(self, ip, mac):\n\t\t\"\"\"Create a host object with given ip address and and mac address.\n\n\t\t@type ip: str\n\t\t@type mac: str\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tmsg = OmapiMessage.open(b\"host\")\n\t\tmsg.message.append((b\"create\", struct.pack(\"!I\", 1)))\n\t\tmsg.message.append((b\"exclusive\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"hardware-address\", pack_mac(mac)))\n\t\tmsg.obj.append((b\"hardware-type\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"ip-address\", pack_ip(ip)))\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"add failed\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a host with a fixed - address and override its hostname with the given name.", "response": "def add_host_supersede_name(self, ip, mac, name):  # pylint:disable=E0213\n\t\t\"\"\"Add a host with a fixed-address and override its hostname with the given name.\n\t\t@type self: Omapi\n\t\t@type ip: str\n\t\t@type mac: str\n\t\t@type name: str\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tmsg = OmapiMessage.open(b\"host\")\n\t\tmsg.message.append((b\"create\", struct.pack(\"!I\", 1)))\n\t\tmsg.message.append((b\"exclusive\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"hardware-address\", pack_mac(mac)))\n\t\tmsg.obj.append((b\"hardware-type\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"ip-address\", pack_ip(ip)))\n\t\tmsg.obj.append((b\"name\", name.encode('utf-8')))\n\t\tmsg.obj.append((b\"statements\", 'supersede host-name \"{0}\";'.format(name).encode('utf-8')))\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"add failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a host object with given ip mac name hostname router and domain.", "response": "def add_host_supersede(self, ip, mac, name, hostname=None, router=None, domain=None):  # pylint:disable=too-many-arguments\n\t\t\"\"\"Create a host object with given ip, mac, name, hostname, router and\n\t\tdomain. hostname, router and domain are optional arguments.\n\n\t\t@type ip: str\n\t\t@type mac: str\n\t\t@type name: str\n\t\t@type hostname: str\n\t\t@type router: str\n\t\t@type domain: str\n\t\t@raises OmapiError:\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tstmts = []\n\n\t\tmsg = OmapiMessage.open(b\"host\")\n\t\tmsg.message.append((b\"create\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"name\", name))\n\t\tmsg.obj.append((b\"hardware-address\", pack_mac(mac)))\n\t\tmsg.obj.append((b\"hardware-type\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((b\"ip-address\", pack_ip(ip)))\n\t\tif hostname:\n\t\t\tstmts.append('supersede host-name \"{0}\";\\n '.format(hostname))\n\t\tif router:\n\t\t\tstmts.append('supersede routers {0};\\n '.format(router))\n\t\tif domain:\n\t\t\tstmts.append('supersede domain-name \"{0}\";'.format(domain))\n\t\tif stmts:\n\t\t\tencoded_stmts = \"\".join(stmts).encode(\"utf-8\")\n\t\t\tmsg.obj.append((b\"statements\", encoded_stmts))\n\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"add failed\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a host object with given mac address.", "response": "def del_host(self, mac):\n\t\t\"\"\"Delete a host object with with given mac address.\n\n\t\t@type mac: str\n\t\t@raises ValueError:\n\t\t@raises OmapiError:\n\t\t@raises OmapiErrorNotFound: if no lease object with the given\n\t\t\t\tmac address could be found\n\t\t@raises socket.error:\n\t\t\"\"\"\n\t\tmsg = OmapiMessage.open(b\"host\")\n\t\tmsg.obj.append((b\"hardware-address\", pack_mac(mac)))\n\t\tmsg.obj.append((b\"hardware-type\", struct.pack(\"!I\", 1)))\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiErrorNotFound()\n\t\tif response.handle == 0:\n\t\t\traise OmapiError(\"received invalid handle from server\")\n\t\tresponse = self.query_server(OmapiMessage.delete(response.handle))\n\t\tif response.opcode != OMAPI_OP_STATUS:\n\t\t\traise OmapiError(\"delete failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a group to the server.", "response": "def add_group(self, groupname, statements):\n\t\t\"\"\"\n\t\tAdds a group\n\t\t@type groupname: bytes\n\t\t@type statements: str\n\t\t\"\"\"\n\t\tmsg = OmapiMessage.open(b\"group\")\n\t\tmsg.message.append((\"create\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((\"name\", groupname))\n\t\tmsg.obj.append((\"statements\", statements))\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"add group failed\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_host_with_group(self, ip, mac, groupname):\n\t\tmsg = OmapiMessage.open(b\"host\")\n\t\tmsg.message.append((\"create\", struct.pack(\"!I\", 1)))\n\t\tmsg.message.append((\"exclusive\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((\"hardware-address\", pack_mac(mac)))\n\t\tmsg.obj.append((\"hardware-type\", struct.pack(\"!I\", 1)))\n\t\tmsg.obj.append((\"ip-address\", pack_ip(ip)))\n\t\tmsg.obj.append((\"group\", groupname))\n\t\tresponse = self.query_server(msg)\n\t\tif response.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"add failed\")", "response": "Adds a host with given ip and mac in a group named groupname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the group of a host given the name of the host.", "response": "def change_group(self, name, group):\n\t\t\"\"\"Change the group of a host given the name of the host.\n\t\t@type name: str\n\t\t@type group: str\n\t\t\"\"\"\n\t\tm1 = OmapiMessage.open(b\"host\")\n\t\tm1.update_object(dict(name=name))\n\t\tr1 = self.query_server(m1)\n\t\tif r1.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"opening host %s failed\" % name)\n\t\tm2 = OmapiMessage.update(r1.handle)\n\t\tm2.update_object(dict(group=group))\n\t\tr2 = self.query_server(m2)\n\t\tif r2.opcode != OMAPI_OP_UPDATE:\n\t\t\traise OmapiError(\"changing group of host %s to %s failed\" % (name, group))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the resource from the server.", "response": "def get(cls, id, api=None):\n        \"\"\"\n        Fetches the resource from the server.\n        :param id: Resource identifier\n        :param api: sevenbridges Api instance.\n        :return: Resource object.\n        \"\"\"\n        id = Transform.to_resource(id)\n        api = api if api else cls._API\n        if 'get' in cls._URL:\n            extra = {'resource': cls.__name__, 'query': {'id': id}}\n            logger.info('Fetching {} resource'.format(cls), extra=extra)\n            resource = api.get(url=cls._URL['get'].format(id=id)).json()\n            return cls(api=api, **resource)\n        else:\n            raise SbgError('Unable to fetch resource!')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self):\n        if 'delete' in self._URL:\n            extra = {'resource': self.__class__.__name__, 'query': {\n                'id': self.id}}\n            logger.info(\"Deleting {} resource.\".format(self), extra=extra)\n            self._api.delete(url=self._URL['delete'].format(id=self.id))\n        else:\n            raise SbgError('Resource can not be deleted!')", "response": "Deletes the resource on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reload(self):\n        try:\n            if hasattr(self, 'href'):\n                data = self._api.get(self.href, append_base=False).json()\n                resource = self.__class__(api=self._api, **data)\n            elif hasattr(self, 'id') and hasattr(self, '_URL') and \\\n                    'get' in self._URL:\n                data = self._api.get(\n                    self._URL['get'].format(id=self.id)).json()\n                resource = self.__class__(api=self._api, **data)\n            else:\n                raise SbgError('Resource can not be refreshed!')\n\n            query = {'id': self.id} if hasattr(self, 'id') else {}\n            extra = {'resource': self.__class__.__name__, 'query': query}\n            logger.info('Reloading {} resource.'.format(self), extra=extra)\n\n        except Exception:\n            raise SbgError('Resource can not be refreshed!')\n\n        self._data = resource._data\n        self._dirty = resource._dirty\n        self._old = copy.deepcopy(self._data.data)\n        return self", "response": "Refreshes the resource with the data from the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert(cls, record):\n        if isinstance(record, list):\n            return [cls._convert(r) for r in record]\n        else:\n            return [cls._convert(record)]", "response": "Converts a single dictionary or list of dictionaries into a list of dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a single dictionary into converted dictionary or list of dictionaries into converted list of dictionaries. Used while passing dictionaries to another converter. Used while passing dictionaries to another converter. Used while passing dictionaries to another converter.", "response": "def _convert_internal(cls, record):\n        \"\"\"\n        Converts a single dictionary into converted dictionary or list of dictionaries into converted\n        list of dictionaries. Used while passing dictionaries to another converter.\n        \"\"\"\n        if isinstance(record, list):\n            return [cls._convert(r) for r in record]\n        else:\n            return cls._convert(record)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a single dictionary into another dictionary.", "response": "def _convert(cls, record):\n        \"\"\"\n        Core method of the converter. Converts a single dictionary into another dictionary.\n        \"\"\"\n        if not record:\n            return {}\n\n        converted_dict = {}\n        for field in cls.conversion:\n            key = field[0]\n            if len(field) >= 2 and field[1]:\n                converted_key = field[1]\n            else:\n                converted_key = key\n            if len(field) >= 3 and field[2]:\n                conversion_method = field[2]\n            else:\n                conversion_method = cls.default_conversion_method\n            if len(field) >= 4:\n                converter = field[3]\n            else:\n                converter = None\n            try:\n                value = conversion_method(record[key])\n            except KeyError:\n                continue\n            if converter:\n                value = converter._convert_internal(value)\n            if converted_key is APPEND:\n                if isinstance(value, list):\n                    for v in value:\n                        converted_dict.update(v)\n                else:\n                    converted_dict.update(value)\n            else:\n                converted_dict[converted_key] = value\n        return converted_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rate_limit_sleeper(api, response):\n    while response.status_code == 429:\n        headers = response.headers\n        remaining_time = headers.get('X-RateLimit-Reset')\n        sleep = int(remaining_time) - int(time.time())\n        logger.warning('Rate limit reached! Waiting for [%s]s', sleep)\n        time.sleep(sleep + 5)\n        response = api.session.send(response.request)\n    return response", "response": "Sleeper function that checks if rate limit is reached."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the API is under maintenance and returns the response.", "response": "def maintenance_sleeper(api, response, sleep=300):\n    \"\"\"\n    Pauses the execution if sevenbridges api is under maintenance.\n    :param api: Api instance.\n    :param response: requests.Response object.\n    :param sleep: Time to sleep in between the requests.\n    \"\"\"\n    while response.status_code == 503:\n        logger.info('Service unavailable: Response=[%s]',\n                    six.text_type(response.__dict__))\n        response_body = response.json()\n        if 'code' in response_body:\n            if response_body['code'] == 0:\n                logger.warning('API Maintenance in progress!'\n                               ' Waiting for [%s]s', sleep)\n                time.sleep(sleep)\n                response = api.session.send(response.request)\n            else:\n                return response\n        else:\n            return response\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef general_error_sleeper(api, response, sleep=300):\n    while response.status_code >= 500:\n        logger.warning('Caught [%s] status code! Waiting for [%s]s',\n                       response.status_code, sleep)\n        time.sleep(sleep)\n        response = api.session.send(response.request)\n    return response", "response": "Pauses the execution if response status code is > 500.\n    :param api: Api instance.\n    :param response: requests.Response object\n    :param sleep: Time to sleep in between the requests."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_google_volume(cls, name, bucket, client_email, private_key,\n                             access_mode, description=None, prefix=None,\n                             properties=None, api=None):\n\n        \"\"\"\n        Create s3 volume.\n        :param name: Volume name.\n        :param bucket: Referenced bucket.\n        :param client_email: Google client email.\n        :param private_key: Google client private key.\n        :param access_mode: Access Mode.\n        :param description: Volume description.\n        :param prefix: Volume prefix.\n        :param properties: Volume properties.\n        :param api: Api instance.\n        :return: Volume object.\n        \"\"\"\n        service = {'type': VolumeType.GOOGLE,\n                   'bucket': bucket,\n                   'credentials': {'client_email': client_email,\n                                   'private_key': private_key\n                                   }\n                   }\n        if prefix:\n            service['prefix'] = prefix\n        if properties:\n            service['properties'] = properties\n\n        data = {'name': name,\n                'service': service,\n                'access_mode': access_mode\n                }\n        if description:\n            data['description'] = description\n        api = api or cls._API\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Creating google volume', extra=extra)\n        response = api.post(url=cls._URL['query'], data=data).json()\n        return Volume(api=api, **response)", "response": "Create a new google volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_oss_volume(cls, name, bucket, endpoint, access_key_id,\n                          secret_access_key, access_mode, description=None,\n                          prefix=None, properties=None, api=None):\n        \"\"\"\n        Create oss volume.\n        :param name: Volume name.\n        :param bucket: Referenced bucket.\n        :param access_key_id: Access key identifier.\n        :param secret_access_key: Secret access key.\n        :param access_mode: Access Mode.\n        :param endpoint: Volume Endpoint.\n        :param description: Volume description.\n        :param prefix: Volume prefix.\n        :param properties: Volume properties.\n        :param api: Api instance.\n        :return: Volume object.\n        \"\"\"\n        service = {\n            'type': VolumeType.OSS,\n            'bucket': bucket,\n            'endpoint': endpoint,\n            'credentials': {\n                'access_key_id': access_key_id,\n                'secret_access_key': secret_access_key\n            }\n        }\n        if prefix:\n            service['prefix'] = prefix\n        if properties:\n            service['properties'] = properties\n\n        data = {\n            'name': name,\n            'service': service,\n            'access_mode': access_mode\n        }\n        if description:\n            data['description'] = description\n        api = api or cls._API\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Creating oss volume', extra=extra)\n        response = api.post(url=cls._URL['query'], data=data).json()\n        return Volume(api=api, **response)", "response": "Create an oss volume."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_volume_object_info(self, location):\n        param = {'location': location}\n        data = self._api.get(url=self._URL['object'].format(\n            id=self.id), params=param).json()\n        return VolumeObject(api=self._api, **data)", "response": "Fetches information about a single volume object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch information about a single volume member", "response": "def get_member(self, username, api=None):\n        \"\"\"\n        Fetches information about a single volume member\n        :param username: Member name\n        :param api: Api instance\n        :return: Member object\n        \"\"\"\n        api = api if api else self._API\n\n        response = api.get(\n            url=self._URL['member'].format(id=self.id, username=username),\n        )\n        data = response.json()\n        return Member(api=api, **data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsubmitting a new import job.", "response": "def submit_import(cls, volume, location, project=None, name=None,\n                      overwrite=False, properties=None, parent=None,\n                      preserve_folder_structure=True, api=None):\n        \"\"\"\n        Submits new import job.\n        :param volume: Volume identifier.\n        :param location: Volume location.\n        :param project: Project identifier.\n        :param name: Optional file name.\n        :param overwrite: If true it will overwrite file if exists.\n        :param properties: Properties dictionary.\n        :param parent: The ID of the target folder to which the item should be\n            imported. Should not be used together with project.\n        :param preserve_folder_structure: Whether to keep the exact source\n            folder structure. The default value is true if the item being\n            imported is a folder. Should not be used if you are importing\n            a file.\n        :param api: Api instance.\n        :return: Import object.\n        \"\"\"\n        data = {}\n        volume = Transform.to_volume(volume)\n\n        if project and parent:\n            raise SbgError(\n                'Project and parent identifiers are mutually exclusive'\n            )\n        elif project:\n            project = Transform.to_project(project)\n            destination = {\n                'project': project\n            }\n        elif parent:\n            parent = Transform.to_file(parent)\n            destination = {\n                'parent': parent\n            }\n        else:\n            raise SbgError('Project or parent identifier is required.')\n\n        source = {\n            'volume': volume,\n            'location': location\n        }\n\n        if name:\n            destination['name'] = name\n\n        data['source'] = source\n        data['destination'] = destination\n        data['overwrite'] = overwrite\n\n        if not preserve_folder_structure:\n            data['preserve_folder_structure'] = preserve_folder_structure\n\n        if properties:\n            data['properties'] = properties\n\n        api = api if api else cls._API\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Submitting import', extra=extra)\n        _import = api.post(cls._URL['query'], data=data).json()\n        return Import(api=api, **_import)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying ( List ) imports.", "response": "def query(cls, project=None, volume=None, state=None, offset=None,\n              limit=None, api=None):\n        \"\"\"\n        Query (List) imports.\n        :param project: Optional project identifier.\n        :param volume: Optional volume identifier.\n        :param state: Optional import sate.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: Api instance.\n        :return: Collection object.\n        \"\"\"\n        api = api or cls._API\n\n        if project:\n            project = Transform.to_project(project)\n        if volume:\n            volume = Transform.to_volume(volume)\n\n        return super(Import, cls)._query(\n            url=cls._URL['query'], project=project, volume=volume, state=state,\n            fields='_all', offset=offset, limit=limit, api=api\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving imports in bulk", "response": "def bulk_get(cls, imports, api=None):\n        \"\"\"\n        Retrieve imports in bulk\n        :param imports: Imports to be retrieved.\n        :param api: Api instance.\n        :return: List of ImportBulkRecord objects.\n        \"\"\"\n        api = api or cls._API\n        import_ids = [Transform.to_import(import_) for import_ in imports]\n        data = {'import_ids': import_ids}\n\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return ImportBulkRecord.parse_records(response=response, api=api)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubmitting imports in bulk.", "response": "def bulk_submit(cls, imports, api=None):\n        \"\"\"\n        Submit imports in bulk\n        :param imports: Imports to be retrieved.\n        :param api: Api instance.\n        :return: List of ImportBulkRecord objects.\n        \"\"\"\n        if not imports:\n            raise SbgError('Imports are required')\n\n        api = api or cls._API\n\n        items = []\n        for import_ in imports:\n            volume = Transform.to_volume(import_.get('volume'))\n            location = Transform.to_location(import_.get('location'))\n            project = Transform.to_project(import_.get('project'))\n            name = import_.get('name', None)\n            overwrite = import_.get('overwrite', False)\n\n            item = {\n                'source': {\n                    'volume': volume,\n                    'location': location\n                },\n                'destination': {\n                    'project': project\n                },\n                'overwrite': overwrite\n            }\n\n            if name:\n                item['destination']['name'] = name\n\n            items.append(item)\n\n        data = {'items': items}\n\n        response = api.post(url=cls._URL['bulk_create'], data=data)\n        return ImportBulkRecord.parse_records(response=response, api=api)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_room(room):\n\n    if room.custom_server:\n        return\n\n    def _create_room(xmpp):\n        muc = xmpp.plugin['xep_0045']\n        muc.joinMUC(room.jid, xmpp.requested_jid.user)\n        muc.configureRoom(room.jid, _set_form_values(xmpp, room))\n\n    current_plugin.logger.info('Creating room %s', room.jid)\n    _execute_xmpp(_create_room)", "response": "Creates a MUC room on the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_room(room):\n\n    if room.custom_server:\n        return\n\n    def _update_room(xmpp):\n        muc = xmpp.plugin['xep_0045']\n        muc.joinMUC(room.jid, xmpp.requested_jid.user)\n        muc.configureRoom(room.jid, _set_form_values(xmpp, room, muc.getRoomConfig(room.jid)))\n\n    current_plugin.logger.info('Updating room %s', room.jid)\n    _execute_xmpp(_update_room)", "response": "Updates a MUC room on the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_room(room, reason=''):\n\n    if room.custom_server:\n        return\n\n    def _delete_room(xmpp):\n        muc = xmpp.plugin['xep_0045']\n        muc.destroy(room.jid, reason=reason)\n\n    current_plugin.logger.info('Deleting room %s', room.jid)\n    _execute_xmpp(_delete_room)\n    delete_logs(room)", "response": "Deletes a MUC room from the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_room_config(jid):\n\n    mapping = {\n        'name': 'muc#roomconfig_roomname',\n        'description': 'muc#roomconfig_roomdesc',\n        'password': 'muc#roomconfig_roomsecret'\n    }\n\n    def _get_room_config(xmpp):\n        muc = xmpp.plugin['xep_0045']\n        try:\n            form = muc.getRoomConfig(jid)\n        except ValueError:  # probably the room doesn't exist\n            return None\n        fields = form.values['fields']\n        return {key: fields[muc_key].values['value'] for key, muc_key in mapping.iteritems()}\n\n    return _execute_xmpp(_get_room_config)", "response": "Retrieves basic data of a MUC room from the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a MUC room exists on the server.", "response": "def room_exists(jid):\n    \"\"\"Checks if a MUC room exists on the server.\"\"\"\n\n    def _room_exists(xmpp):\n        disco = xmpp.plugin['xep_0030']\n        try:\n            disco.get_info(jid)\n        except IqError as e:\n            if e.condition == 'item-not-found':\n                return False\n            raise\n        else:\n            return True\n\n    return _execute_xmpp(_room_exists)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a valid JID node identifier from a string", "response": "def sanitize_jid(s):\n    \"\"\"Generates a valid JID node identifier from a string\"\"\"\n    jid = unicode_to_ascii(s).lower()\n    jid = WHITESPACE.sub('-', jid)\n    jid = INVALID_JID_CHARS.sub('', jid)\n    return jid.strip()[:256]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a v alid JID based on the room name.", "response": "def generate_jid(name, append_date=None):\n    \"\"\"Generates a v alid JID based on the room name.\n\n    :param append_date: appends the given date to the JID\n    \"\"\"\n    if not append_date:\n        return sanitize_jid(name)\n    return '{}-{}'.format(sanitize_jid(name), append_date.strftime('%Y-%m-%d'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_form_values(xmpp, room, form=None):\n    if form is None:\n        form = xmpp.plugin['xep_0004'].make_form(ftype='submit')\n        form.add_field('FORM_TYPE', value='http://jabber.org/protocol/muc#roomconfig')\n        form.add_field('muc#roomconfig_publicroom', value='1')\n        form.add_field('muc#roomconfig_whois', value='moderators')\n        form.add_field('muc#roomconfig_membersonly', value='0')\n        form.add_field('muc#roomconfig_moderatedroom', value='1')\n        form.add_field('muc#roomconfig_changesubject', value='1')\n        form.add_field('muc#roomconfig_allowinvites', value='1')\n        form.add_field('muc#roomconfig_allowvisitorstatus', value='1')\n        form.add_field('muc#roomconfig_allowvisitornickchange', value='1')\n        form.add_field('muc#roomconfig_enablelogging', value='1')\n        form.add_field('public_list', value='1')\n        form.add_field('members_by_default', value='1')\n        form.add_field('allow_private_messages', value='1')\n        form.add_field('allow_query_users', value='1')\n    form.add_field('muc#roomconfig_persistentroom', value='1')\n    form.add_field('muc#roomconfig_roomname', value=room.name)\n    form.add_field('muc#roomconfig_passwordprotectedroom', value='1' if room.password else '0')\n    if room.description:\n        form.add_field('muc#roomconfig_roomdesc', value=room.description)\n    if room.password:\n        form.add_field('muc#roomconfig_roomsecret', value=room.password)\n    return form", "response": "Sets the values of the XMPP room config form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _execute_xmpp(connected_callback):\n    from indico_chat.plugin import ChatPlugin\n\n    check_config()\n    jid = ChatPlugin.settings.get('bot_jid')\n    password = ChatPlugin.settings.get('bot_password')\n    if '@' not in jid:\n        jid = '{}@{}'.format(jid, ChatPlugin.settings.get('server'))\n\n    result = [None, None]  # result, exception\n    app = current_app._get_current_object()  # callback runs in another thread\n\n    def _session_start(event):\n        try:\n            with app.app_context():\n                result[0] = connected_callback(xmpp)\n        except Exception as e:\n            result[1] = e\n            if isinstance(e, IqError):\n                current_plugin.logger.exception('XMPP callback failed: %s', e.condition)\n            else:\n                current_plugin.logger.exception('XMPP callback failed')\n        finally:\n            xmpp.disconnect(wait=0)\n\n    xmpp = ClientXMPP(jid, password)\n    xmpp.register_plugin('xep_0045')\n    xmpp.register_plugin('xep_0004')\n    xmpp.register_plugin('xep_0030')\n    xmpp.add_event_handler('session_start', _session_start)\n\n    try:\n        xmpp.connect()\n    except Exception:\n        current_plugin.logger.exception('XMPP connection failed')\n        xmpp.disconnect()\n        raise\n\n    try:\n        xmpp.process(threaded=False)\n    finally:\n        xmpp.disconnect(wait=0)\n\n    if result[1] is not None:\n        raise result[1]\n\n    return result[0]", "response": "Connects to the XMPP server and executes custom code"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retrieve_logs(room, start_date=None, end_date=None):\n    from indico_chat.plugin import ChatPlugin\n\n    base_url = ChatPlugin.settings.get('log_url')\n    if not base_url or room.custom_server:\n        return None\n\n    params = {'cr': room.jid}\n    if start_date:\n        params['sdate'] = start_date.strftime('%Y-%m-%d')\n    if end_date:\n        params['edate'] = end_date.strftime('%Y-%m-%d')\n\n    try:\n        response = requests.get(base_url, params=params)\n    except RequestException:\n        current_plugin.logger.exception('Could not retrieve logs for %s', room.jid)\n        return None\n    if response.headers.get('content-type') == 'application/json':\n        current_plugin.logger.warning('Could not retrieve logs for %s: %s', room.jid, response.json().get('error'))\n        return None\n    return response.text", "response": "Retrieves chat logs for a given chatroom"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsave modification to the api server.", "response": "def save(self, inplace=True):\n        \"\"\"\n        Saves modification to the api server.\n        \"\"\"\n        data = self._modified_data()\n        data = data['permissions']\n        if bool(data):\n            url = six.text_type(self.href) + self._URL['permissions']\n            extra = {'resource': self.__class__.__name__, 'query': data}\n            logger.info('Modifying permissions', extra=extra)\n            self._api.patch(url=url, data=data, append_base=False)\n        else:\n            raise ResourceNotModified()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(cls, name, division, api=None):\n\n        division = Transform.to_division(division)\n        api = api if api else cls._API\n        data = {\n            'name': name,\n            'division': division\n        }\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Creating team', extra=extra)\n        created_team = api.post(cls._URL['query'], data=data).json()\n        return Team(api=api, **created_team)", "response": "Create a new team within a division."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_members(self, offset=None, limit=None):\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {'id': self.id}\n        }\n        logger.info('Get team members', extra=extra)\n        response = self._api.get(\n            url=self._URL['members_query'].format(id=self.id),\n            params={'offset': offset, 'limit': limit}\n        )\n        data = response.json()\n        total = response.headers['x-total-matching-query']\n        members = [TeamMember(api=self._api, **member) for member in\n                   data['items']]\n        links = [Link(**link) for link in data['links']]\n        href = data['href']\n        return Collection(resource=TeamMember, href=href, total=total,\n                          items=members, links=links, api=self._api)", "response": "Fetch team members for current team."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_member(self, user):\n        user = Transform.to_user(user)\n        data = {\n            'id': user\n        }\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {\n                'id': self.id,\n                'data': data,\n            }\n        }\n        logger.info('Adding team member using id', extra=extra)\n        response = self._api.post(\n            url=self._URL['members_query'].format(id=self.id), data=data)\n        member_data = response.json()\n        return TeamMember(api=self._api, **member_data)", "response": "Add a user to the team s members."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying ( List ) projects AttributeNames", "response": "def query(cls, owner=None, name=None, offset=None, limit=None, api=None):\n        \"\"\"\n        Query (List) projects\n        :param owner: Owner username.\n        :param name: Project name\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: Api instance.\n        :return: Collection object.\n        \"\"\"\n        api = api if api else cls._API\n        query_params = {}\n        if owner:\n            url = cls._URL['query'].format(owner=owner)\n        else:\n            url = cls._URL['query'].format(owner='')\n        if name:\n            query_params['name'] = name\n        return super(Project, cls)._query(\n            url=url, offset=offset, limit=limit, fields='_all',\n            api=api, **query_params\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create(cls, name, billing_group=None, description=None, tags=None,\n               settings=None, api=None):\n        \"\"\"\n        Create a project.\n        :param name:  Project name.\n        :param billing_group: Project billing group.\n        :param description:  Project description.\n        :param tags: Project tags.\n        :param settings: Project settings.\n        :param api: Api instance.\n        :return:\n        \"\"\"\n        api = api if api else cls._API\n\n        if name is None:\n            raise SbgError('Project name is required!')\n\n        data = {\n            'name': name,\n        }\n\n        if billing_group:\n            data['billing_group'] = Transform.to_billing_group(billing_group)\n\n        if description:\n            data['description'] = description\n        if tags:\n            data['tags'] = tags\n\n        if settings:\n            data['settings'] = settings\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Creating project', extra=extra)\n        project_data = api.post(url=cls._URL['create'], data=data).json()\n        return Project(api=api, **project_data)", "response": "Create a new project."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_member_team(self, team, permissions):\n        team = Transform.to_team(team)\n        data = {'id': team, 'type': 'TEAM'}\n        if isinstance(permissions, dict):\n            data.update({\n                'permissions': permissions\n            })\n\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {\n                'id': self.id,\n                'data': data,\n            }\n        }\n        logger.info('Adding team member using team id', extra=extra)\n        response = self._api.post(\n            url=self._URL['members_query'].format(id=self.id), data=data)\n        member_data = response.json()\n        return Member(api=self._api, **member_data)", "response": "Add a member to a project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_member_email(self, email, permissions=None):\n        data = {'email': email}\n\n        if isinstance(permissions, dict):\n            data.update({\n                'permissions': permissions\n            })\n\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {\n                'id': self.id,\n                'data': data,\n            }\n        }\n        logger.info('Adding member using email', extra=extra)\n        response = self._api.post(\n            url=self._URL['members_query'].format(id=self.id), data=data)\n        member_data = response.json()\n        return Member(api=self._api, **member_data)", "response": "Adds a member to the project using an email address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_member(self, user):\n        username = Transform.to_user(user)\n        extra = {\n            'resource': self.__class__.__name__,\n            'query': {\n                'id': self.id,\n                'user': user,\n            }\n        }\n        logger.info('Removing member', extra=extra)\n        self._api.delete(\n            url=self._URL['member'].format(id=self.id, username=username)\n        )", "response": "Removes a user from the project."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_files(self, offset=None, limit=None):\n        params = {'project': self.id, 'offset': offset, 'limit': limit}\n        return self._api.files.query(api=self._api, **params)", "response": "Retrieves files in this project."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_files(self, files):\n        for file in files:\n            file.copy(project=self.id)", "response": "Adds files to this project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nretrieving the apps in this project.", "response": "def get_apps(self, offset=None, limit=None):\n        \"\"\"\n        Retrieves apps in this project.\n        :param offset:  Pagination offset.\n        :param limit: Pagination limit.\n        :return: Collection object.\n        \"\"\"\n        params = {'project': self.id, 'offset': offset,\n                  'limit': limit}\n        return self._api.apps.query(api=self._api, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tasks(self, status=None, offset=None, limit=None):\n        params = {'project': self.id, 'offset': offset, 'limit': limit}\n        if status:\n            params['status'] = status\n        return self._api.tasks.query(api=self._api, **params)", "response": "Retrieves all tasks in this project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching the imports for this project.", "response": "def get_imports(self, volume=None, state=None, offset=None, limit=None):\n        \"\"\"\n        Fetches imports for this project.\n        :param volume: Optional volume identifier.\n        :param state: Optional state.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :return: Collection object.\n        \"\"\"\n        return self._api.imports.query(project=self.id, volume=volume,\n                                       state=state, offset=offset, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch exports for this volume.", "response": "def get_exports(self, volume=None, state=None, offset=None, limit=None):\n        \"\"\"\n        Fetches exports for this volume.\n        :param volume: Optional volume identifier.\n        :param state: Optional state.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :return: Collection object.\n        \"\"\"\n        return self._api.exports.query(project=self.id, volume=volume,\n                                       state=state, offset=offset, limit=limit)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new task for this project.", "response": "def create_task(self, name, app, revision=None, batch_input=None,\n                    batch_by=None, inputs=None, description=None, run=False,\n                    disable_batch=False, interruptible=True,\n                    execution_settings=None):\n        \"\"\"\n        Creates a task for this project.\n\n        :param name: Task name.\n        :param app: CWL app identifier.\n        :param revision: CWL app revision.\n        :param batch_input: Batch input.\n        :param batch_by: Batch criteria.\n        :param inputs: Input map.\n        :param description: Task description.\n        :param run: True if you want to run a task upon creation.\n        :param disable_batch: True if you want to disable batching.\n        :param interruptible: True if you want to use interruptible instances.\n        :param execution_settings: Execution settings for the task.\n        :return: Task object.\n        \"\"\"\n        return self._api.tasks.create(\n            name=name, project=self, app=app, revision=revision,\n            batch_input=batch_input, batch_by=batch_by, inputs=inputs,\n            description=description, run=run, disable_batch=disable_batch,\n            interruptible=interruptible, execution_settings=execution_settings\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating a PayPal business string.", "response": "def validate_business(form, field):\n    \"\"\"Valiates a PayPal business string.\n\n    It can either be an email address or a paypal business account ID.\n    \"\"\"\n    if not is_valid_mail(field.data, multi=False) and not re.match(r'^[a-zA-Z0-9]{13}$', field.data):\n        raise ValidationError(_('Invalid email address / paypal ID'))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _download_part(path, session, url, retry, timeout, start_byte, end_byte):\n    try:\n        fp = os.open(path, os.O_CREAT | os.O_WRONLY)\n    except IOError:\n        raise SbgError('Unable to open file %s' % path)\n\n    # Prepare range headers.\n    headers = {}\n    if end_byte is not None:\n        headers['Range'] = 'bytes=%d-%d' % (int(start_byte), int(end_byte))\n\n    cause = None\n\n    # Retry\n    for retry in range(retry):\n        try:\n            response = session.get(\n                url, headers=headers, timeout=timeout, stream=True\n            )\n            response.raise_for_status()\n            part_size = response.headers.get('Content-Length')\n            os.lseek(fp, start_byte, os.SEEK_SET)\n            for part in response.iter_content(32 * PartSize.KB):\n                os.write(fp, part)\n            os.close(fp)\n        except requests.HTTPError as e:\n            cause = e\n            time.sleep(2 ** retry)\n            continue\n        except requests.RequestException as e:\n            cause = e\n            time.sleep(2 ** retry)\n            continue\n        else:\n            return Part(start=start_byte, size=float(part_size))\n\n    else:\n        os.close(fp)\n        raise SbgError('Failed to download file after {} attempts.'\n                       ' Response: {}'.format(retry, six.text_type(cause)))", "response": "Download a single part of a single file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef submit(self):\n        futures = []\n        while self.submitted < 4 and not self.done():\n            part = self.parts.pop(0)\n            futures.append(\n                self.pool.submit(\n                    _download_part, self.file_path, self.session, self.url,\n                    self.retry, self.timeout, *part)\n            )\n            self.submitted += 1\n            self.total_submitted += 1\n\n        return futures", "response": "Submits the file into chunks and returns a list of futures that are ready to be used to download the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_parts(self):\n        parts = []\n        start_b = 0\n        end_byte = start_b + PartSize.DOWNLOAD_MINIMUM_PART_SIZE - 1\n        for i in range(self.total):\n            parts.append([start_b, end_byte])\n            start_b = end_byte + 1\n            end_byte = start_b + PartSize.DOWNLOAD_MINIMUM_PART_SIZE - 1\n        return parts", "response": "Returns a list of the parts of the archive."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pause(self):\n        if self._status == TransferState.RUNNING:\n            self._running.clear()\n            self._status = TransferState.PAUSED\n        else:\n            raise SbgError('Can not pause. Download not in RUNNING state.')", "response": "Pauses the download.\n        :raises SbgError: If upload is not in RUNNING state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop(self):\n        if self.status in (TransferState.PAUSED, TransferState.RUNNING):\n            self._stop_signal = True\n            self.join()\n            self._status = TransferState.STOPPED\n            if self._callback:\n                return self._callback(self._status)\n        else:\n            raise SbgError(\n                'Can not stop. Download not in PAUSED or RUNNING state.'\n            )", "response": "Stops the download.\n        :raises SbgError: If download is not in PAUSED or RUNNING state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nresume the download. :raises SbgError: If download is not in RUNNING state.", "response": "def resume(self):\n        \"\"\"\n        Resumes the download.\n        :raises SbgError: If download is not in RUNNING state.\n        \"\"\"\n        if self._status != TransferState.PAUSED:\n            self._running.set()\n            self._status = TransferState.RUNNING\n        else:\n            raise SbgError('Can not pause. Download not in PAUSED state.')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting the download. :raises SbgError: If download is not in PREPARING state.", "response": "def start(self):\n        \"\"\"\n        Starts the download.\n        :raises SbgError: If download is not in PREPARING state.\n        \"\"\"\n        if self._status == TransferState.PREPARING:\n            self._running.set()\n            super(Download, self).start()\n            self._status = TransferState.RUNNING\n            self._time_started = time.time()\n        else:\n            raise SbgError(\n                'Unable to start. Download not in PREPARING state.'\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        self._running.set()\n        self._status = TransferState.RUNNING\n        self._time_started = time.time()\n\n        parted_file = DPartedFile(self._temp_file,\n                                  self._session,\n                                  self.url,\n                                  self._file_size,\n                                  self._part_size,\n                                  self._retry_count,\n                                  self._timeout,\n                                  self._api.download_pool)\n\n        try:\n            for part in parted_file:\n                if self._stop_signal:\n                    return\n                self._running.wait()\n                self._bytes_done += part.size\n                if self._progress_callback:\n                    progress = Progress(\n                        parted_file.total, parted_file.total_submitted,\n                        self._bytes_done, self._file_size, self.duration\n                    )\n                    self._progress_callback(progress)\n\n        except Exception as exc:\n            if self._errorback:\n                return self._errorback(exc)\n            else:\n                raise SbgError('Download failed! %s' % str(exc))\n\n        self._status = TransferState.COMPLETED\n        try:\n            os.rename(self._temp_file, self._file_path)\n        except Exception:\n            raise SbgError(\"Unable to rename the file.\")\n\n        if self._callback:\n            return self._callback(self._status)", "response": "Runs the thread! Should not be used use start() method instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfetches the file size by reading the Content - Length header and reading the file path.", "response": "def _get_file_size(self):\n        \"\"\"\n        Fetches file size by reading the Content-Length header\n        for the resource.\n        :return: File size.\n        \"\"\"\n        file_size = retry(self._retry_count)(_get_content_length)(\n            self._session, self.url, self._timeout\n        )\n        file_size = int(file_size)\n        if file_size == 0:\n            with io.open(self._file_path, 'a', encoding='utf-8'):\n                pass\n        return file_size"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns the livesync export", "response": "def run(self):\n        \"\"\"Runs the livesync export\"\"\"\n        if self.uploader is None:  # pragma: no cover\n            raise NotImplementedError\n\n        records = self.fetch_records()\n        uploader = self.uploader(self)\n        LiveSyncPlugin.logger.info('Uploading %d records', len(records))\n        uploader.run(records)\n        self.update_last_run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_initial_export(self, events):\n        if self.uploader is None:  # pragma: no cover\n            raise NotImplementedError\n\n        uploader = self.uploader(self)\n        uploader.run_initial(events)", "response": "Runs the initial export."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if all required config options are set otherwise raise IndicoError", "response": "def check_config(quiet=False):\n    \"\"\"Checks if all required config options are set\n\n    :param quiet: if True, return the result as a bool, otherwise\n                  raise `IndicoError` if any setting is missing\n    \"\"\"\n    from indico_chat.plugin import ChatPlugin\n    settings = ChatPlugin.settings.get_all()\n    missing = not all(settings[x] for x in ('server', 'muc_server', 'bot_jid', 'bot_password'))\n    if missing and not quiet:\n        raise IndicoError(_('Chat plugin is not configured properly'))\n    return not missing"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a user is a chat admin", "response": "def is_chat_admin(user):\n    \"\"\"Checks if a user is a chat admin\"\"\"\n    from indico_chat.plugin import ChatPlugin\n    return ChatPlugin.settings.acls.contains_user('admins', user)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef map_input_output(item, api):\n    if isinstance(item, list):\n        return [map_input_output(it, api) for it in item]\n\n    elif isinstance(item, dict) and 'class' in item:\n        if item['class'].lower() == 'file':\n            return File(id=item['path'], api=api)\n\n    else:\n        return item", "response": "Maps input output value to appropriate sevebridges object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap the given method and reloads the object AttributeNames with data returned from the server.", "response": "def inplace_reload(method):\n    \"\"\"\n    Executes the wrapped function and reloads the object\n    with data returned from the server.\n    \"\"\"\n\n    # noinspection PyProtectedMember\n    def wrapped(obj, *args, **kwargs):\n        in_place = True if kwargs.get('inplace') in (True, None) else False\n        api_object = method(obj, *args, **kwargs)\n        if in_place and api_object:\n            obj._data = api_object._data\n            obj._dirty = api_object._dirty\n            obj._data.fetched = False\n            return obj\n        elif api_object:\n            return api_object\n        else:\n            return obj\n\n    return wrapped"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretries decorator used to retry callables on for specific exceptions. :param excs: Exceptions tuple. :param retry_count: Retry count. :param delay: Delay in seconds between retries. :return: Wrapped function object.", "response": "def retry_on_excs(excs, retry_count=3, delay=5):\n    \"\"\"Retry decorator used to retry callables on for specific exceptions.\n\n    :param excs: Exceptions tuple.\n    :param retry_count: Retry count.\n    :param delay: Delay in seconds between retries.\n    :return: Wrapped function object.\n    \"\"\"\n\n    def wrapper(f):\n        @functools.wraps(f)\n        def deco(*args, **kwargs):\n            for i in range(0, retry_count):\n                try:\n                    return f(*args, **kwargs)\n                except excs:\n                    if logger:\n                        logger.warning(\n                            'HTTPError caught.Retrying ...'.format(f.__name__),\n                            exc_info=True\n                        )\n                    time.sleep(delay)\n            else:\n                logger.error(\n                    '{} failed after {} retries'.format(\n                        f.__name__, retry_count)\n                )\n            return f(*args, **kwargs)\n\n        return deco\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef retry(retry_count):\n\n    def func(f):\n        @functools.wraps(f)\n        def wrapper(*args, **kwargs):\n            for backoff in range(retry_count):\n                try:\n                    return f(*args, **kwargs)\n                except Exception:\n                    time.sleep(2 ** backoff)\n            else:\n                raise SbgError('{}: failed to complete: {}'.format(\n                    threading.current_thread().getName(), f.__name__)\n                )\n\n        return wrapper\n\n    return func", "response": "Decorator used during file upload and download."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexecuting the wrapped function and returns the response object for specific errors.", "response": "def check_for_error(func):\n    \"\"\"\n    Executes the wrapped function and inspects the response object\n    for specific errors.\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            response = func(*args, **kwargs)\n            status_code = response.status_code\n            if status_code in range(200, 204):\n                return response\n            if status_code == 204:\n                return\n            data = response.json()\n            e = {\n                400: BadRequest,\n                401: Unauthorized,\n                403: Forbidden,\n                404: NotFound,\n                405: MethodNotAllowed,\n                408: RequestTimeout,\n                409: Conflict,\n                429: TooManyRequests,\n                500: ServerError,\n                503: ServiceUnavailable,\n            }.get(status_code, SbgError)()\n            if 'message' in data:\n                e.message = data['message']\n            if 'code' in data:\n                e.code = data['code']\n            if 'status' in data:\n                e.status = data['status']\n            if 'more_info' in data:\n                e.more_info = data['more_info']\n            raise e\n        except requests.RequestException as e:\n            raise SbgError(message=six.text_type(e))\n        except JSONDecodeError:\n            message = (\n                'Service might be unavailable. Can also occur by providing '\n                'too many query parameters.'\n            )\n            raise_from(\n                ServiceUnavailable(message=six.text_type(message)), None\n            )\n        except ValueError as e:\n            raise SbgError(message=six.text_type(e))\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef all(self):\n        page = self._load(self.href)\n        while True:\n            try:\n                for item in page._items:\n                    yield item\n                page = page.next_page()\n            except PaginationError:\n                break", "response": "Fetches all available items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_page(self):\n        for link in self.links:\n            if link.rel.lower() == 'next':\n                return self._load(link.href)\n        raise PaginationError('No more entries.')", "response": "Fetches next result set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_page(self):\n        for link in self.links:\n            if link.next:\n                return self._load(link.next)\n        raise PaginationError('No more entries.')", "response": "Fetches next result set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls, *args, **kwargs):\n\n        from indico_piwik.plugin import PiwikPlugin\n\n        if not PiwikPlugin.settings.get('cache_enabled'):\n            return cls(*args, **kwargs).to_serializable()\n\n        cache = GenericCache('Piwik.Report')\n        key = u'{}-{}-{}'.format(cls.__name__, args, kwargs)\n\n        report = cache.get(key)\n        if not report:\n            report = cls(*args, **kwargs)\n            cache.set(key, report, PiwikPlugin.settings.get('cache_ttl'))\n        return report.to_serializable()", "response": "Create and return a serializable Report object if possible"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset date range defaults if no dates are passed", "response": "def _init_date_range(self, start_date=None, end_date=None):\n        \"\"\"Set date range defaults if no dates are passed\"\"\"\n        self.end_date = end_date\n        self.start_date = start_date\n        if self.end_date is None:\n            today = now_utc().date()\n            end_date = self.event.end_dt.date()\n            self.end_date = end_date if end_date < today else today\n        if self.start_date is None:\n            self.start_date = self.end_date - timedelta(days=ReportBase.default_report_interval)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the report by performing queries to Piwik", "response": "def _build_report(self):\n        \"\"\"Build the report by performing queries to Piwik\"\"\"\n        self.metrics = {}\n        queries = {'visits': PiwikQueryReportEventMetricVisits(**self.params),\n                   'unique_visits': PiwikQueryReportEventMetricUniqueVisits(**self.params),\n                   'visit_duration': PiwikQueryReportEventMetricVisitDuration(**self.params),\n                   'referrers': PiwikQueryReportEventMetricReferrers(**self.params),\n                   'peak': PiwikQueryReportEventMetricPeakDateAndVisitors(**self.params)}\n\n        for query_name, query in queries.iteritems():\n            self.metrics[query_name] = query.get_result()\n\n        self._fetch_contribution_info()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _fetch_contribution_info(self):\n        self.contributions = {}\n        query = (Contribution.query\n                 .with_parent(self.event)\n                 .options(joinedload('legacy_mapping'),\n                          joinedload('timetable_entry').lazyload('*')))\n        for contribution in query:\n            if not contribution.start_dt:\n                continue\n            cid = (contribution.legacy_mapping.legacy_contribution_id if contribution.legacy_mapping\n                   else contribution.id)\n            key = '{}t{}'.format(contribution.event_id, cid)\n            self.contributions[key] = u'{} ({})'.format(contribution.title,\n                                                        to_unicode(format_time(contribution.start_dt)))", "response": "Fetch the list of information entries for contributions of the event"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_callback(self, callback):\n        if callback in self._async_callbacks:\n            self._async_callbacks.remove(callback)", "response": "Remove callback previously registered."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates attributes in self. Return list of attributes with changed values.", "response": "def update_attr(self, attr):\n        \"\"\"Update input attr in self.\n\n        Return list of attributes with changed values.\n        \"\"\"\n        changed_attr = []\n        for key, value in attr.items():\n            if value is None:\n                continue\n            if getattr(self, \"_{0}\".format(key), None) != value:\n                changed_attr.append(key)\n                self.__setattr__(\"_{0}\".format(key), value)\n                _LOGGER.debug('%s: update %s with %s', self.name, key, value)\n        return changed_attr"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_feedback(cls, type=FeedbackType.IDEA, referrer=None, text=None,\n                      api=None):\n        \"\"\"\n        Sends feedback to sevenbridges.\n        :param type: FeedbackType wither IDEA, PROBLEM or THOUGHT.\n        :param text: Feedback text.\n        :param referrer: Feedback referrer.\n        :param api: Api instance.\n        \"\"\"\n        api = api if api else cls._API\n        data = {'type': type,\n                'text': text,\n                'referrer': referrer if referrer else six.text_type(\n                    client_info\n                )}\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Sending feedback', extra=extra)\n        api.post(url=cls._URL['send_feedback'], data=data)", "response": "Sends feedback to sevenbridges."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbulks copy of files.", "response": "def bulk_copy_files(cls, files, destination_project, api=None):\n        \"\"\"\n        Bulk copy of files.\n        :param files: List containing files to be copied.\n        :param destination_project: Destination project.\n        :param api: Api instance.\n        :return: MultiStatus copy result.\n        \"\"\"\n        api = api if api else cls._API\n        files = [Transform.to_file(file) for file in files]\n        data = {\n            'project': destination_project,\n            'file_ids': files\n        }\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Performing bulk copy', extra=extra)\n        return api.post(url=cls._URL['bulk_copy'], data=data).json()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting state of light group.", "response": "async def async_set_state(self, data):\n        \"\"\"Set state of light group.\n\n        {\n            \"on\": true,\n            \"bri\": 180,\n            \"hue\": 43680,\n            \"sat\": 255,\n            \"transitiontime\": 10\n        }\n\n        Also update local values of group since websockets doesn't.\n        \"\"\"\n        field = self.deconz_id + '/action'\n        await self._async_set_state_callback(field, data)\n        self.async_update({'state': data})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding scenes belonging to group.", "response": "def async_add_scenes(self, scenes, async_set_state_callback):\n        \"\"\"Add scenes belonging to group.\"\"\"\n        self._scenes = {\n            scene['id']: DeconzScene(self, scene, async_set_state_callback)\n            for scene in scenes\n            if scene['id'] not in self._scenes\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrecall scene to group.", "response": "async def async_set_state(self, data):\n        \"\"\"Recall scene to group.\"\"\"\n        field = self._deconz_id + '/recall'\n        await self._async_set_state_callback(field, data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns current user information.", "response": "def me(cls, api=None):\n        \"\"\"\n        Retrieves current user information.\n        :param api: Api instance.\n        :return: User object.\n        \"\"\"\n        api = api if api else cls._API\n        extra = {\n            'resource': cls.__name__,\n            'query': {}\n        }\n        logger.info('Fetching user information', extra=extra)\n        user_data = api.get(cls._URL['me']).json()\n        return User(api=api, **user_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying genome markers on a file.", "response": "def query(cls, file, offset=None, limit=None, api=None):\n        \"\"\"\n        Queries genome markers on a file.\n        :param file: Genome file - Usually bam file.\n        :param offset: Pagination offset.\n        :param limit: Pagination limit.\n        :param api: Api instance.\n        :return: Collection object.\n        \"\"\"\n        api = api if api else cls._API\n\n        file = Transform.to_file(file)\n        return super(Marker, cls)._query(\n            url=cls._URL['query'], offset=offset, limit=limit,\n            file=file, fields='_all', api=api\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(cls, file, name, position, chromosome, private=True, api=None):\n        api = api if api else cls._API\n\n        file = Transform.to_file(file)\n        data = {\n            'file': file,\n            'name': name,\n            'position': position,\n            'chromosome': chromosome,\n            'private': private\n        }\n\n        extra = {\n            'resource': cls.__name__,\n            'query': data\n        }\n        logger.info('Creating marker', extra=extra)\n        marker_data = api.post(url=cls._URL['query'], data=data).json()\n        return Marker(api=api, **marker_data)", "response": "Create a new marker on a file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving all modification to the marker on the server.", "response": "def save(self, inplace=True):\n        \"\"\"\n        Saves all modification to the marker on the server.\n        :param inplace Apply edits on the current instance or get a new one.\n        :return: Marker instance.\n        \"\"\"\n        modified_data = self._modified_data()\n        if bool(modified_data):\n            extra = {\n                'resource': self.__class__.__name__,\n                'query': {\n                    'id': self.id,\n                    'modified_data': modified_data\n                }\n            }\n            logger.info('Saving marker', extra=extra)\n            data = self._api.patch(url=self._URL['get'].format(id=self.id),\n                                   data=modified_data).json()\n            marker = Marker(api=self._api, **data)\n            return marker\n        else:\n            raise ResourceNotModified()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef async_update(self, event):\n        self.update_attr(event.get('state', {}))\n        super().async_update(event)", "response": "Update the state of the current light."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes project to id string", "response": "def to_project(project):\n        \"\"\"Serializes project to id string\n        :param project: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.project import Project\n        if not project:\n            raise SbgError('Project is required!')\n        elif isinstance(project, Project):\n            return project.id\n        elif isinstance(project, six.string_types):\n            return project\n        else:\n            raise SbgError('Invalid project parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing task to id string", "response": "def to_task(task):\n        \"\"\"Serializes task to id string\n        :param task: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.task import Task\n        if not task:\n            raise SbgError('Task is required!')\n        elif isinstance(task, Task):\n            return task.id\n        elif isinstance(task, six.string_types):\n            return task\n        else:\n            raise SbgError('Invalid task parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_app(app):\n        from sevenbridges.models.app import App\n        if not app:\n            raise SbgError('App is required!')\n        elif isinstance(app, App):\n            return app.id\n        elif isinstance(app, six.string_types):\n            return app\n        else:\n            raise SbgError('Invalid app parameter!')", "response": "Serializes an app to id string"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nserializes file to id string", "response": "def to_file(file_):\n        \"\"\"Serializes file to id string\n        :param file_: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.file import File\n        if not file_:\n            raise SbgError('File is required!')\n        elif isinstance(file_, File):\n            return file_.id\n        elif isinstance(file_, six.string_types):\n            return file_\n        else:\n            raise SbgError('Invalid file parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize user to id string", "response": "def to_user(user):\n        \"\"\"Serializes user to id string\n        :param user: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.user import User\n        if not user:\n            raise SbgError('User is required!')\n        elif isinstance(user, User):\n            return user.username\n        elif isinstance(user, six.string_types):\n            return user\n        else:\n            raise SbgError('Invalid user parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing a billing group to id string", "response": "def to_billing_group(billing_group):\n        \"\"\"Serializes billing_group to id string\n        :param billing_group: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.billing_group import BillingGroup\n        if not billing_group:\n            raise SbgError('Billing group is required!')\n        elif isinstance(billing_group, BillingGroup):\n            return billing_group.id\n        elif isinstance(billing_group, six.string_types):\n            return billing_group\n        else:\n            raise SbgError('Invalid billing group parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_volume(volume):\n        from sevenbridges.models.volume import Volume\n        if not volume:\n            raise SbgError('Volume is required!')\n        elif isinstance(volume, Volume):\n            return volume.id\n        elif isinstance(volume, six.string_types):\n            return volume\n        else:\n            raise SbgError('Invalid volume parameter!')", "response": "Serializes a volume object to id string"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes marker to string", "response": "def to_marker(marker):\n        \"\"\"Serializes marker to string\n        :param marker: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.marker import Marker\n        if not marker:\n            raise SbgError('Marker is required!')\n        elif isinstance(marker, Marker):\n            return marker.id\n        elif isinstance(marker, six.string_types):\n            return marker\n        else:\n            raise SbgError('Invalid marker parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_division(division):\n        from sevenbridges.models.division import Division\n        if not division:\n            raise SbgError('Division is required!')\n        elif isinstance(division, Division):\n            return division.id\n        elif isinstance(division, six.string_types):\n            return division\n        else:\n            raise SbgError('Invalid division parameter!')", "response": "Serializes a division to id string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing a Team object to id string", "response": "def to_team(team):\n        \"\"\"Serializes team to id string\n        :param team: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.team import Team\n        if not team:\n            raise SbgError('Team is required!')\n        elif isinstance(team, Team):\n            return team.id\n        elif isinstance(team, six.string_types):\n            return team\n        else:\n            raise SbgError('Invalid team parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing import to id string", "response": "def to_import(import_):\n        \"\"\"Serializes import to id string\n        :param import_: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.storage_import import Import\n        if not import_:\n            raise SbgError('Import is required!')\n        elif isinstance(import_, Import):\n            return import_.id\n        elif isinstance(import_, six.string_types):\n            return import_\n        else:\n            raise SbgError('Invalid import parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing export to id string", "response": "def to_export(export):\n        \"\"\"Serializes export to id string\n        :param export: object to serialize\n        :return: string id\n        \"\"\"\n        from sevenbridges.models.storage_export import Export\n        if not export:\n            raise SbgError('Export is required!')\n        elif isinstance(export, Export):\n            return export.id\n        elif isinstance(export, six.string_types):\n            return export\n        else:\n            raise SbgError('Invalid export parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nserializing location to string", "response": "def to_location(location):\n        \"\"\"Serializes location to string\n        :param location: object to serialize\n        :return: string\n        \"\"\"\n        if not location:\n            raise SbgError('Location is required!')\n        if isinstance(location, six.string_types):\n            return location\n        else:\n            raise SbgError('Invalid location parameter!')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery ( List ) datasets AttributeNames", "response": "def query(cls, visibility=None, api=None):\n        \"\"\"Query ( List ) datasets\n        :param visibility: If provided as 'public', retrieves public datasets\n        :param api: Api instance\n        :return: Collection object\n        \"\"\"\n        api = api if api else cls._API\n\n        return super(Dataset, cls)._query(\n            url=cls._URL['query'],\n            visibility=visibility,\n            fields='_all', api=api\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_owned_by(cls, username, api=None):\n        api = api if api else cls._API\n\n        return super(Dataset, cls)._query(\n            url=cls._URL['owned_by'].format(username=username),\n            fields='_all',\n            api=api\n        )", "response": "Query ( List ) datasets by owner and username"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self, inplace=True):\n        modified_data = self._modified_data()\n        if bool(modified_data):\n            dataset_request_data = {}\n\n            name = modified_data.pop('name', None)\n            description = modified_data.pop('description', None)\n            dataset_request_data.update(modified_data)\n\n            if name:\n                dataset_request_data['name'] = name\n\n            if description:\n                dataset_request_data['description'] = description\n\n            response = self._api.patch(\n                url=self._URL['get'].format(id=self.id),\n                data=dataset_request_data\n            )\n            data = response.json()\n\n            dataset = Dataset(api=self._api, **data)\n            return dataset", "response": "Save all modification to the dataset on the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves dataset members :param api: Api instance :return: Collection object", "response": "def get_members(self, api=None):\n        \"\"\"Retrieve dataset members\n        :param api: Api instance\n        :return: Collection object\n        \"\"\"\n        api = api or self._API\n\n        response = api.get(url=self._URL['members'].format(id=self.id))\n\n        data = response.json()\n        total = response.headers['x-total-matching-query']\n        members = [Member(api=api, **member) for member in data['items']]\n        links = [Link(**link) for link in data['links']]\n        href = data['href']\n\n        return Collection(\n            resource=Member,\n            href=href,\n            total=total,\n            items=members,\n            links=links,\n            api=api\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_member(self, username, permissions, api=None):\n        api = api or self._API\n        data = {\n            'username': username,\n            'permissions': permissions\n        }\n\n        response = api.post(\n            url=self._URL['members'].format(id=self.id),\n            data=data\n        )\n        data = response.json()\n        return Member(api=api, **data)", "response": "Add a member to a dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a member from a dataset", "response": "def remove_member(self, member, api=None):\n        \"\"\"Remove member from a dataset\n        :param member: Member username\n        :param api: Api instance\n        :return: None\n        \"\"\"\n        api = api or self._API\n        username = Transform.to_member(member)\n\n        api.delete(\n            url=self._URL['member'].format(\n                id=self.id,\n                username=username\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef query(cls, project=None, names=None, metadata=None, origin=None,\n              tags=None, offset=None, limit=None, dataset=None, api=None,\n              parent=None):\n        \"\"\"\n        Query ( List ) files, requires project or dataset\n        :param project: Project id\n        :param names: Name list\n        :param metadata: Metadata query dict\n        :param origin: Origin query dict\n        :param tags: List of tags to filter on\n        :param offset: Pagination offset\n        :param limit: Pagination limit\n        :param dataset: Dataset id\n        :param api: Api instance.\n        :param parent: Folder id or File object with type folder\n        :return: Collection object.\n        \"\"\"\n        api = api or cls._API\n\n        query_params = {}\n\n        if project:\n            project = Transform.to_project(project)\n            query_params['project'] = project\n\n        if dataset:\n            dataset = Transform.to_dataset(dataset)\n            query_params['dataset'] = dataset\n\n        if parent:\n            query_params['parent'] = Transform.to_file(parent)\n\n        if not (project or dataset or parent):\n            raise SbgError('Project, dataset or parent must be provided.')\n\n        if [project, parent, dataset].count(None) < 2:\n            raise SbgError(\n                'Only one out of project, parent or dataset must be provided.'\n            )\n\n        if names is not None and isinstance(names, list):\n            if len(names) == 0:\n                names.append(\"\")\n            query_params['name'] = names\n\n        metadata_params = {}\n        if metadata and isinstance(metadata, dict):\n            for k, v in metadata.items():\n                metadata_params['metadata.' + k] = metadata[k]\n\n        if tags:\n            query_params['tag'] = tags\n\n        query_params.update(metadata_params)\n\n        origin_params = {}\n        if origin and isinstance(origin, dict):\n            for k, v in origin.items():\n                origin_params['origin.' + k] = origin[k]\n\n        query_params.update(origin_params)\n\n        return super(File, cls)._query(\n            api=api, url=cls._URL['query'], offset=offset,\n            limit=limit, fields='_all', **query_params\n        )", "response": "Query the files in the specified list of names metadata origin and limit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef upload(cls, path, project=None, parent=None, file_name=None,\n               overwrite=False, retry=5, timeout=10,\n               part_size=PartSize.UPLOAD_MINIMUM_PART_SIZE, wait=True,\n               api=None):\n        \"\"\"\n        Uploads a file using multipart upload and returns an upload handle\n        if the wait parameter is set to False. If wait is set to True it\n        will block until the upload is completed.\n\n        :param path: File path on local disc.\n        :param project: Project identifier\n        :param parent: Parent folder identifier\n        :param file_name: Optional file name.\n        :param overwrite: If true will overwrite the file on the server.\n        :param retry:  Number of retries if error occurs during upload.\n        :param timeout:  Timeout for http requests.\n        :param part_size:  Part size in bytes.\n        :param wait:  If true will wait for upload to complete.\n        :param api: Api instance.\n        \"\"\"\n\n        api = api or cls._API\n        extra = {'resource': cls.__name__, 'query': {\n            'path': path,\n            'project': project,\n            'file_name': file_name,\n            'overwrite': overwrite,\n            'retry': retry,\n            'timeout': timeout,\n            'part_size': part_size,\n            'wait': wait,\n        }}\n        logger.info('Uploading file', extra=extra)\n\n        if not project and not parent:\n            raise SbgError('A project or parent identifier is required.')\n\n        if project and parent:\n            raise SbgError(\n                'Project and parent identifiers are mutually exclusive.'\n            )\n\n        if project:\n            project = Transform.to_project(project)\n\n        if parent:\n            parent = Transform.to_file(parent)\n\n        upload = Upload(\n            file_path=path, project=project, parent=parent,\n            file_name=file_name, overwrite=overwrite, retry_count=retry,\n            timeout=timeout, part_size=part_size, api=api\n        )\n        if wait:\n            upload.start()\n            upload.wait()\n            return upload\n        else:\n            return upload", "response": "Uploads a file using multipart upload and returns a handle that handles the upload."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies the current file.", "response": "def copy(self, project, name=None):\n        \"\"\"\n        Copies the current file.\n        :param project: Destination project.\n        :param name: Destination file name.\n        :return: Copied File object.\n        \"\"\"\n        project = Transform.to_project(project)\n        data = {\n            'project': project\n        }\n        if name:\n            data['name'] = name\n        extra = {'resource': self.__class__.__name__, 'query': {\n            'id': self.id,\n            'data': data\n        }}\n        logger.info('Copying file', extra=extra)\n        new_file = self._api.post(url=self._URL['copy'].format(id=self.id),\n                                  data=data).json()\n        return File(api=self._api, **new_file)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_info(self):\n        info = self._api.get(url=self._URL['download_info'].format(id=self.id))\n        return DownloadInfo(api=self._api, **info.json())", "response": "Fetches download information containing file url\n        that can be used to download file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading the file and returns a download handle.", "response": "def download(self, path, retry=5, timeout=10,\n                 chunk_size=PartSize.DOWNLOAD_MINIMUM_PART_SIZE, wait=True,\n                 overwrite=False):\n        \"\"\"\n        Downloads the file and returns a download handle.\n        Download will not start until .start() method is invoked.\n        :param path: Full path to the new file.\n        :param retry:  Number of retries if error occurs during download.\n        :param timeout:  Timeout for http requests.\n        :param chunk_size:  Chunk size in bytes.\n        :param wait: If true will wait for download to complete.\n        :param overwrite: If True will silently overwrite existing file.\n        :return: Download handle.\n        \"\"\"\n\n        if not overwrite and os.path.exists(path):\n            raise LocalFileAlreadyExists(message=path)\n\n        extra = {'resource': self.__class__.__name__, 'query': {\n            'id': self.id,\n            'path': path,\n            'overwrite': overwrite,\n            'retry': retry,\n            'timeout': timeout,\n            'chunk_size': chunk_size,\n            'wait': wait,\n        }}\n        logger.info('Downloading file', extra=extra)\n        info = self.download_info()\n        download = Download(\n            url=info.url, file_path=path, retry_count=retry, timeout=timeout,\n            part_size=chunk_size, api=self._api\n        )\n        if wait:\n            download.start()\n            download.wait()\n        else:\n            return download"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving all modification to the file on the server.", "response": "def save(self, inplace=True, silent=False):\n        \"\"\"\n        Saves all modification to the file on the server. By default this\n        method raises an error if you are trying to save an instance that was\n        not changed. Set check_if_modified param to False to disable\n        this behaviour.\n        :param inplace: Apply edits to the current instance or get a new one.\n        :param silent: If Raises exception if file wasn't modified.\n        :raise ResourceNotModified\n        :return: File instance.\n        \"\"\"\n        modified_data = self._modified_data()\n        if silent or bool(modified_data):\n            # If metadata is to be set\n            if 'metadata' in modified_data:\n                if hasattr(self, '_method'):\n                    self._api.put(\n                        url=self._URL['metadata'].format(id=self.id),\n                        data=modified_data['metadata']\n                    )\n                else:\n                    self._api.patch(\n                        url=self._URL['metadata'].format(id=self.id),\n                        data=modified_data['metadata']\n                    )\n                modified_data.pop('metadata')\n            if 'tags' in modified_data:\n                self._api.put(\n                    url=self._URL['tags'].format(id=self.id),\n                    data=modified_data['tags']\n                )\n                modified_data.pop('tags')\n            # Change everything else\n            if bool(modified_data):\n                self._api.patch(\n                    url=self._URL['get'].format(id=self.id), data=modified_data\n                )\n        else:\n            raise ResourceNotModified()\n\n        return self.reload()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stream(self, part_size=32 * PartSize.KB):\n        download_info = self.download_info()\n        response = self._api.get(\n            url=download_info.url, stream=True, append_base=False\n        )\n        for part in response.iter_content(part_size):\n            yield part", "response": "Returns an iterator which can be used to stream the content of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreloading the file with the data from the server.", "response": "def reload(self):\n        \"\"\"\n        Refreshes the file with the data from the server.\n        \"\"\"\n        try:\n            data = self._api.get(self.href, append_base=False).json()\n            resource = File(api=self._api, **data)\n        except Exception:\n            try:\n                data = self._api.get(\n                    self._URL['get'].format(id=self.id)).json()\n                resource = File(api=self._api, **data)\n            except Exception:\n                raise SbgError('Resource can not be refreshed!')\n\n        self._data = resource._data\n        self._dirty = resource._dirty\n        self._old = copy.deepcopy(self._data.data)\n\n        # If file.metadata = value was executed\n        # file object will have attribute _method='PUT', which tells us\n        # to force overwrite of metadata on the server. This is metadata\n        # specific. Once we reload the resource we delete the attribute\n        # _method from the instance.\n        try:\n            delattr(self, '_method')\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads the file and read it in memory.", "response": "def content(self, path=None, overwrite=True, encoding='utf-8'):\n        \"\"\"\n        Downloads file to the specified path or as temporary file\n        and reads the file content in memory.\n         Should not be used on very large files.\n\n        :param path: Path for file download If omitted tmp file will be used.\n        :param overwrite: Overwrite file if exists locally\n        :param encoding: File encoding, by default it is UTF-8\n        :return: File content.\n        \"\"\"\n        if path:\n            self.download(wait=True, path=path, overwrite=overwrite)\n            with io.open(path, 'r', encoding=encoding) as fp:\n                return fp.read()\n\n        with tempfile.NamedTemporaryFile() as tmpfile:\n            self.download(wait=True, path=tmpfile.name, overwrite=overwrite)\n            with io.open(tmpfile.name, 'r', encoding=encoding) as fp:\n                return fp.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bulk_get(cls, files, api=None):\n        api = api or cls._API\n        file_ids = [Transform.to_file(file_) for file_ in files]\n        data = {'file_ids': file_ids}\n\n        logger.info('Getting files in bulk.')\n        response = api.post(url=cls._URL['bulk_get'], data=data)\n        return FileBulkRecord.parse_records(response=response, api=api)", "response": "Retrieve files with specified ids in bulk."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bulk_update(cls, files, api=None):\n        if not files:\n            raise SbgError('Files are required.')\n\n        api = api or cls._API\n        data = {\n            'items': [\n                {\n                    'id': file_.id,\n                    'name': file_.name,\n                    'tags': file_.tags,\n                    'metadata': file_.metadata,\n                }\n                for file_ in files\n            ]\n        }\n\n        logger.info('Updating files in bulk.')\n        response = api.post(url=cls._URL['bulk_update'], data=data)\n        return FileBulkRecord.parse_records(response=response, api=api)", "response": "This method updates the details for multiple specified files."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist files in a folder.", "response": "def list_files(self, offset=None, limit=None, api=None):\n        \"\"\"List files in a folder\n        :param api: Api instance\n        :param offset: Pagination offset\n        :param limit: Pagination limit\n        :return: List of files\n        \"\"\"\n        api = api or self._API\n\n        if not self.is_folder():\n            raise SbgError('{name} is not a folder'.format(name=self.name))\n\n        url = self._URL['list_folder'].format(id=self.id)\n\n        return super(File, self.__class__)._query(\n            api=api, url=url, offset=offset, limit=limit, fields='_all'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new folder in a specific project.", "response": "def create_folder(cls, name, parent=None, project=None,\n                      api=None):\n        \"\"\"Create a new folder\n        :param name: Folder name\n        :param parent: Parent folder\n        :param project: Project to create folder in\n        :param api: Api instance\n        :return: New folder\n        \"\"\"\n        api = api or cls._API\n\n        data = {\n            'name': name,\n            'type': cls.FOLDER_TYPE\n        }\n\n        if not parent and not project:\n            raise SbgError('Parent or project must be provided')\n\n        if parent and project:\n            raise SbgError(\n                'Providing both \"parent\" and \"project\" is not allowed'\n            )\n\n        if parent:\n            data['parent'] = Transform.to_file(file_=parent)\n\n        if project:\n            data['project'] = Transform.to_project(project=project)\n\n        response = api.post(url=cls._URL['create_folder'], data=data).json()\n        return cls(api=api, **response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy file to folder", "response": "def copy_to_folder(self, parent, name=None, api=None):\n        \"\"\"Copy file to folder\n        :param parent: Folder to copy file to\n        :param name: New file name\n        :param api: Api instance\n        :return: New file instance\n        \"\"\"\n        api = api or self._API\n\n        if self.is_folder():\n            raise SbgError('Copying folders is not supported')\n\n        data = {\n            'parent': Transform.to_file(parent)\n        }\n\n        if name:\n            data['name'] = name\n\n        response = api.post(\n            url=self._URL['copy_to_folder'].format(file_id=self.id),\n            data=data\n        ).json()\n        return File(api=api, **response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reduce_json(data):\n    return reduce(lambda x, y: int(x) + int(y), data.values())", "response": "Reduce a JSON object"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes time as a value of seconds and deduces the delta in human - readable HHh MMm SSs format.", "response": "def stringify_seconds(seconds=0):\n    \"\"\"\n    Takes time as a value of seconds and deduces the delta in human-readable\n    HHh MMm SSs format.\n    \"\"\"\n    seconds = int(seconds)\n    minutes = seconds / 60\n    ti = {'h': 0, 'm': 0, 's': 0}\n\n    if seconds > 0:\n        ti['s'] = seconds % 60\n        ti['m'] = minutes % 60\n        ti['h'] = minutes / 60\n\n    return \"%dh %dm %ds\" % (ti['h'], ti['m'], ti['s'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_sensor(sensor_id, sensor, async_set_state_callback):\n    if sensor['type'] in CONSUMPTION:\n        return Consumption(sensor_id, sensor)\n    if sensor['type'] in CARBONMONOXIDE:\n        return CarbonMonoxide(sensor_id, sensor)\n    if sensor['type'] in DAYLIGHT:\n        return Daylight(sensor_id, sensor)\n    if sensor['type'] in FIRE:\n        return Fire(sensor_id, sensor)\n    if sensor['type'] in GENERICFLAG:\n        return GenericFlag(sensor_id, sensor)\n    if sensor['type'] in GENERICSTATUS:\n        return GenericStatus(sensor_id, sensor)\n    if sensor['type'] in HUMIDITY:\n        return Humidity(sensor_id, sensor)\n    if sensor['type'] in LIGHTLEVEL:\n        return LightLevel(sensor_id, sensor)\n    if sensor['type'] in OPENCLOSE:\n        return OpenClose(sensor_id, sensor)\n    if sensor['type'] in POWER:\n        return Power(sensor_id, sensor)\n    if sensor['type'] in PRESENCE:\n        return Presence(sensor_id, sensor)\n    if sensor['type'] in PRESSURE:\n        return Pressure(sensor_id, sensor)\n    if sensor['type'] in SWITCH:\n        return Switch(sensor_id, sensor)\n    if sensor['type'] in TEMPERATURE:\n        return Temperature(sensor_id, sensor)\n    if sensor['type'] in THERMOSTAT:\n        return Thermostat(sensor_id, sensor, async_set_state_callback)\n    if sensor['type'] in VIBRATION:\n        return Vibration(sensor_id, sensor)\n    if sensor['type'] in WATER:\n        return Water(sensor_id, sensor)", "response": "Simplify creating sensor by not needing to know type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if sensor is supported by pydeconz.", "response": "def supported_sensor(sensor):\n    \"\"\"Check if sensor is supported by pydeconz.\"\"\"\n    if sensor['type'] in DECONZ_BINARY_SENSOR + DECONZ_SENSOR + OTHER_SENSOR:\n        return True\n    _LOGGER.info('Unsupported sensor type %s (%s)',\n                 sensor['type'], sensor['name'])\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate attributes of the current object.", "response": "def async_update(self, event, reason={}):\n        \"\"\"New event for sensor.\n\n        Check if state or config is part of event.\n        Signal that sensor has updated attributes.\n        Inform what attributes got changed values.\n        \"\"\"\n        reason['attr'] = []\n        for data in ['state', 'config']:\n            changed_attr = self.update_attr(event.get(data, {}))\n            reason[data] = data in event\n            reason['attr'] += changed_attr\n        super().async_update(event, reason)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef status(self):\n        if self._status == 100:\n            return \"nadir\"\n        elif self._status == 110:\n            return \"night_end\"\n        elif self._status == 120:\n            return \"nautical_dawn\"\n        elif self._status == 130:\n            return \"dawn\"\n        elif self._status == 140:\n            return \"sunrise_start\"\n        elif self._status == 150:\n            return \"sunrise_end\"\n        elif self._status == 160:\n            return \"golden_hour_1\"\n        elif self._status == 170:\n            return \"solar_noon\"\n        elif self._status == 180:\n            return \"golden_hour_2\"\n        elif self._status == 190:\n            return \"sunset_start\"\n        elif self._status == 200:\n            return \"sunset_end\"\n        elif self._status == 210:\n            return \"dusk\"\n        elif self._status == 220:\n            return \"nautical_dusk\"\n        elif self._status == 230:\n            return \"night_start\"\n        else:\n            return \"unknown\"", "response": "Return the daylight status string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef state(self):\n        if self.humidity is None:\n            return None\n        humidity = round(float(self.humidity) / 100, 1)\n        return humidity", "response": "Main state of sensor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef state(self):\n        if self.lightlevel is None:\n            return None\n        lux = round(10 ** (float(self.lightlevel - 1) / 10000), 1)\n        return lux", "response": "Main state of sensor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def async_set_config(self, data):\n        field = self.deconz_id + '/config'\n        await self._async_set_state_callback(field, data)", "response": "Set config of thermostat."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _metric_value(value_str, metric_type):\n    if metric_type in (int, float):\n        try:\n            return metric_type(value_str)\n        except ValueError:\n            raise ValueError(\"Invalid {} metric value: {!r}\".\n                             format(metric_type.__class__.__name__, value_str))\n    elif metric_type is six.text_type:\n        # In Python 3, decode('unicode_escape) requires bytes, so we need\n        # to encode to bytes. This also works in Python 2.\n        return value_str.strip('\"').encode('utf-8').decode('unicode_escape')\n    else:\n        assert metric_type is bool\n        lower_str = value_str.lower()\n        if lower_str == 'true':\n            return True\n        elif lower_str == 'false':\n            return False\n        else:\n            raise ValueError(\"Invalid boolean metric value: {!r}\".\n                             format(value_str))", "response": "Convert a metric value string into a Python - typed metric value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a metric unit string for human consumption that is inferred from the metric name.", "response": "def _metric_unit_from_name(metric_name):\n    \"\"\"\n    Return a metric unit string for human consumption, that is inferred from\n    the metric name.\n\n    If a unit cannot be inferred, `None` is returned.\n    \"\"\"\n    for item in _PATTERN_UNIT_LIST:\n        pattern, unit = item\n        if pattern.match(metric_name):\n            return unit\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, properties):\n        result = self.session.post('/api/services/metrics/context',\n                                   body=properties)\n        mc_properties = properties.copy()\n        mc_properties.update(result)\n        new_metrics_context = MetricsContext(self,\n                                             result['metrics-context-uri'],\n                                             None,\n                                             mc_properties)\n        self._metrics_contexts.append(new_metrics_context)\n        return new_metrics_context", "response": "Creates a new Metrics Context resource in the HMC."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the dict of MetricGroupDefinition objects for this metrics context, by processing its 'metric-group-infos' property.", "response": "def _setup_metric_group_definitions(self):\n        \"\"\"\n        Return the dict of MetricGroupDefinition objects for this metrics\n        context, by processing its 'metric-group-infos' property.\n        \"\"\"\n        # Dictionary of MetricGroupDefinition objects, by metric group name\n        metric_group_definitions = dict()\n        for mg_info in self.properties['metric-group-infos']:\n            mg_name = mg_info['group-name']\n            mg_def = MetricGroupDefinition(\n                name=mg_name,\n                resource_class=_resource_class_from_group(mg_name),\n                metric_definitions=dict())\n            for i, m_info in enumerate(mg_info['metric-infos']):\n                m_name = m_info['metric-name']\n                m_def = MetricDefinition(\n                    index=i,\n                    name=m_name,\n                    type=_metric_type(m_info['metric-type']),\n                    unit=_metric_unit_from_name(m_name))\n                mg_def.metric_definitions[m_name] = m_def\n            metric_group_definitions[mg_name] = mg_def\n        return metric_group_definitions"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving the current metric values for this HMC resource from the HMC API.", "response": "def get_metrics(self):\n        \"\"\"\n        Retrieve the current metric values for this :term:`Metrics Context`\n        resource from the HMC.\n\n        The metric values are returned by this method as a string in the\n        `MetricsResponse` format described with the 'Get Metrics' operation in\n        the :term:`HMC API` book.\n\n        The :class:`~zhmcclient.MetricsResponse` class can be used to process\n        the `MetricsResponse` string returned by this method, and provides\n        structured access to the metrics values.\n\n        Returns:\n\n          :term:`string`:\n            The current metric values, in the `MetricsResponse` string format.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        metrics_response = self.manager.session.get(self.uri)\n        return metrics_response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete this Metrics Context resource.", "response": "def delete(self):\n        \"\"\"\n        Delete this :term:`Metrics Context` resource.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        self.manager.session.delete(self.uri)\n        self.manager._metrics_contexts.remove(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the list of MetricGroupValues objects for this metrics response, by processing its metrics response string. The lines in the metrics response string are:: MetricsResponse: MetricsGroup{0,*} <emptyline> a third empty line at the end MetricsGroup: MetricsGroupName ObjectValues{0,*} <emptyline> a second empty line after each MG ObjectValues: ObjectURI Timestamp ValueRow{1,*} <emptyline> a first empty line after this blk", "response": "def _setup_metric_group_values(self):\n        \"\"\"\n        Return the list of MetricGroupValues objects for this metrics response,\n        by processing its metrics response string.\n\n        The lines in the metrics response string are::\n\n            MetricsResponse: MetricsGroup{0,*}\n                             <emptyline>      a third empty line at the end\n\n            MetricsGroup:    MetricsGroupName\n                             ObjectValues{0,*}\n                             <emptyline>      a second empty line after each MG\n\n            ObjectValues:    ObjectURI\n                             Timestamp\n                             ValueRow{1,*}\n                             <emptyline>      a first empty line after this blk\n        \"\"\"\n\n        mg_defs = self._metrics_context.metric_group_definitions\n\n        metric_group_name = None\n        resource_uri = None\n        dt_timestamp = None\n\n        object_values = None\n        metric_group_values = list()\n        state = 0\n        for mr_line in self._metrics_response_str.splitlines():\n            if state == 0:\n                if object_values is not None:\n                    # Store the result from the previous metric group\n                    mgv = MetricGroupValues(metric_group_name, object_values)\n                    metric_group_values.append(mgv)\n                    object_values = None\n                if mr_line == '':\n                    # Skip initial (or trailing) empty lines\n                    pass\n                else:\n                    # Process the next metrics group\n                    metric_group_name = mr_line.strip('\"')  # No \" or \\ inside\n                    assert metric_group_name in mg_defs\n                    m_defs = mg_defs[metric_group_name].metric_definitions\n                    object_values = list()\n                    state = 1\n            elif state == 1:\n                if mr_line == '':\n                    # There are no (or no more) ObjectValues items in this\n                    # metrics group\n                    state = 0\n                else:\n                    # There are ObjectValues items\n                    resource_uri = mr_line.strip('\"')  # No \" or \\ inside\n                    state = 2\n            elif state == 2:\n                # Process the timestamp\n                assert mr_line != ''\n                try:\n                    dt_timestamp = datetime_from_timestamp(int(mr_line))\n                except ValueError:\n                    # Sometimes, the returned epoch timestamp values are way\n                    # too large, e.g. 3651584404810066 (which would translate\n                    # to the year 115791 A.D.). Python datetime supports\n                    # up to the year 9999. We circumvent this issue by\n                    # simply using the current date&time.\n                    # TODO: Remove the circumvention for too large timestamps.\n                    dt_timestamp = datetime.now(pytz.utc)\n                state = 3\n            elif state == 3:\n                if mr_line != '':\n                    # Process the metric values in the ValueRow line\n                    str_values = mr_line.split(',')\n                    metrics = dict()\n                    for m_name in m_defs:\n                        m_def = m_defs[m_name]\n                        m_type = m_def.type\n                        m_value_str = str_values[m_def.index]\n                        m_value = _metric_value(m_value_str, m_type)\n                        metrics[m_name] = m_value\n                    ov = MetricObjectValues(\n                        self._client, mg_defs[metric_group_name], resource_uri,\n                        dt_timestamp, metrics)\n                    object_values.append(ov)\n                    # stay in this state, for more ValueRow lines\n                else:\n                    # On the empty line after the last ValueRow line\n                    state = 1\n\n        return metric_group_values"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef resource(self):\n        if self._resource is not None:\n            return self._resource\n\n        resource_class = self.metric_group_definition.resource_class\n        resource_uri = self.resource_uri\n\n        if resource_class == 'cpc':\n            filter_args = {'object-uri': resource_uri}\n            resource = self.client.cpcs.find(**filter_args)\n        elif resource_class == 'logical-partition':\n            for cpc in self.client.cpcs.list():\n                try:\n                    filter_args = {'object-uri': resource_uri}\n                    resource = cpc.lpars.find(**filter_args)\n                    break\n                except NotFound:\n                    pass  # Try next CPC\n            else:\n                raise\n        elif resource_class == 'partition':\n            for cpc in self.client.cpcs.list():\n                try:\n                    filter_args = {'object-uri': resource_uri}\n                    resource = cpc.partitions.find(**filter_args)\n                    break\n                except NotFound:\n                    pass  # Try next CPC\n            else:\n                raise\n        elif resource_class == 'adapter':\n            for cpc in self.client.cpcs.list():\n                try:\n                    filter_args = {'object-uri': resource_uri}\n                    resource = cpc.adapters.find(**filter_args)\n                    break\n                except NotFound:\n                    pass  # Try next CPC\n            else:\n                raise\n        elif resource_class == 'nic':\n            for cpc in self.client.cpcs.list():\n                found = False\n                for partition in cpc.partitions.list():\n                    try:\n                        filter_args = {'element-uri': resource_uri}\n                        resource = partition.nics.find(**filter_args)\n                        found = True\n                        break\n                    except NotFound:\n                        pass  # Try next partition / next CPC\n                if found:\n                    break\n            else:\n                raise\n        else:\n            raise ValueError(\n                \"Invalid resource class: {!r}\".format(resource_class))\n\n        self._resource = resource\n        return self._resource", "response": "Returns the Python resource object of the current resource for this URI."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand(self):\n        assert not self._free  # free pool is empty\n        expand_end = self._expand_start + self._expand_len\n        if expand_end > self._range_end:\n            # This happens if the size of the value range is not a multiple\n            # of the expansion chunk size.\n            expand_end = self._range_end\n        if self._expand_start == expand_end:\n            raise ValueError(\"Out of capacity in ID pool\")\n        self._free = set(range(self._expand_start, expand_end))\n        self._expand_start = expand_end", "response": "Expand the free pool if possible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallocating an ID value and return it.", "response": "def alloc(self):\n        \"\"\"\n        Allocate an ID value and return it.\n\n        Raises:\n            ValueError: Out of capacity in ID pool.\n        \"\"\"\n        if not self._free:\n            self._expand()\n        id = self._free.pop()\n        self._used.add(id)\n        return id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(self, properties):\n        result = self.session.post(self.partition.uri + '/nics',\n                                   body=properties)\n        # There should not be overlaps, but just in case there are, the\n        # returned props should overwrite the input props:\n        props = copy.deepcopy(properties)\n        props.update(result)\n        name = props.get(self._name_prop, None)\n        uri = props[self._uri_prop]\n        nic = Nic(self, uri, name, props)\n        self._name_uri_cache.update(name, uri)\n        return nic", "response": "Create and configure a new NIC in this Partition."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete this NIC. Authorization requirements: * Object-access permission to the Partition containing this HBA. * Task permission to the \"Partition Details\" task. Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError`", "response": "def delete(self):\n        \"\"\"\n        Delete this NIC.\n\n        Authorization requirements:\n\n        * Object-access permission to the Partition containing this HBA.\n        * Task permission to the \"Partition Details\" task.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        self.manager.session.delete(self._uri)\n        self.manager._name_uri_cache.delete(\n            self.properties.get(self.manager._name_prop, None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_properties(self, properties):\n        self.manager.session.post(self.uri, body=properties)\n        is_rename = self.manager._name_prop in properties\n        if is_rename:\n            # Delete the old name from the cache\n            self.manager._name_uri_cache.delete(self.name)\n        self.properties.update(copy.deepcopy(properties))\n        if is_rename:\n            # Add the new name to the cache\n            self.manager._name_uri_cache.update(self.name, self.uri)", "response": "Update writeable properties of this NIC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef attached_partition(self):\n        if self._attached_partition is None:\n            part_mgr = self.manager.storage_group.manager.cpc.partitions\n            part = part_mgr.resource_object(self.get_property('partition-uri'))\n            self._attached_partition = part\n        return self._attached_partition", "response": "Return the virtual storage resource s attached partition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef adapter_port(self):\n        if self._adapter_port is None:\n            port_uri = self.get_property('adapter-port-uri')\n            assert port_uri is not None\n            m = re.match(r'^(/api/adapters/[^/]+)/.*', port_uri)\n            adapter_uri = m.group(1)\n            adapter_mgr = self.manager.storage_group.manager.cpc.adapters\n            filter_args = {'object-uri': adapter_uri}\n            adapter = adapter_mgr.find(**filter_args)\n            port_mgr = adapter.ports\n            port = port_mgr.resource_object(port_uri)\n            self._adapter_port = port\n        return self._adapter_port", "response": "Return the storage adapter port associated with this virtual storage resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, name):\n        self.auto_invalidate()\n        try:\n            return self._uris[name]\n        except KeyError:\n            self.refresh()\n            try:\n                return self._uris[name]\n            except KeyError:\n                raise NotFound({self._manager._name_prop: name}, self._manager)", "response": "Get the resource URI for a specified resource name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auto_invalidate(self):\n        current = datetime.now()\n        if current > self._invalidated + timedelta(seconds=self._timetolive):\n            self.invalidate()", "response": "Invalidate the cache if the current time is past the time to live."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrefreshing the Name - URI cache from the HMC.", "response": "def refresh(self):\n        \"\"\"\n        Refresh the Name-URI cache from the HMC.\n\n        This is done by invalidating the cache, listing the resources of this\n        manager from the HMC, and populating the cache with that information.\n        \"\"\"\n        self.invalidate()\n        full = not self._manager._list_has_name\n        res_list = self._manager.list(full_properties=full)\n        self.update_from(res_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the Name - URI cache from the provided list of resources.", "response": "def update_from(self, res_list):\n        \"\"\"\n        Update the Name-URI cache from the provided resource list.\n\n        This is done by going through the resource list and updating any cache\n        entries for non-empty resource names in that list. Other cache entries\n        remain unchanged.\n        \"\"\"\n        for res in res_list:\n            # We access the properties dictionary, in order to make sure\n            # we don't drive additional HMC interactions.\n            name = res.properties.get(self._manager._name_prop, None)\n            uri = res.properties.get(self._manager._uri_prop, None)\n            self.update(name, uri)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_optimized_lookup(self, filter_args):\n        if filter_args is None or len(filter_args) != 1 or \\\n                self._oid_prop not in filter_args:\n            return None\n\n        oid_match = filter_args[self._oid_prop]\n        if not isinstance(oid_match, six.string_types) or \\\n                not re.match(r'^[a-zA-Z0-9_\\-]+$', oid_match):\n            return None\n\n        # The match string is a plain string (not a reg.expression)\n\n        # Construct the resource URI from the filter property\n        # and issue a Get <Resource> Properties on that URI\n        uri = self._base_uri + '/' + oid_match\n\n        try:\n            props = self.session.get(uri)\n        except HTTPError as exc:\n            if exc.http_status == 404 and exc.reason == 1:\n                # No such resource\n                return None\n            raise\n\n        resource_obj = self.resource_class(\n            manager=self,\n            uri=props[self._uri_prop],\n            name=props.get(self._name_prop, None),\n            properties=props)\n\n        resource_obj._full_properties = True\n\n        return resource_obj", "response": "Try to optimize the lookup by making a Get Properties operation on that URI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndivide the filter arguments into a query parameter string and the remaining client - side filters.", "response": "def _divide_filter_args(self, filter_args):\n        \"\"\"\n        Divide the filter arguments into filter query parameters for filtering\n        on the server side, and the remaining client-side filters.\n\n        Parameters:\n\n          filter_args (dict):\n            Filter arguments that narrow the list of returned resources to\n            those that match the specified filter arguments. For details, see\n            :ref:`Filtering`.\n\n            `None` causes no filtering to happen, i.e. all resources are\n            returned.\n\n        Returns:\n\n          : tuple (query_parms_str, client_filter_args)\n        \"\"\"\n        query_parms = []  # query parameter strings\n        client_filter_args = {}\n\n        if filter_args is not None:\n            for prop_name in filter_args:\n                prop_match = filter_args[prop_name]\n                if prop_name in self._query_props:\n                    self._append_query_parms(query_parms, prop_name,\n                                             prop_match)\n                else:\n                    client_filter_args[prop_name] = prop_match\n        query_parms_str = '&'.join(query_parms)\n        if query_parms_str:\n            query_parms_str = '?{}'.format(query_parms_str)\n\n        return query_parms_str, client_filter_args"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _matches_filters(self, obj, filter_args):\n        if filter_args is not None:\n            for prop_name in filter_args:\n                prop_match = filter_args[prop_name]\n                if not self._matches_prop(obj, prop_name, prop_match):\n                    return False\n        return True", "response": "Returns a boolean indicating whether a resource object matches a set of filter arguments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _matches_prop(self, obj, prop_name, prop_match):\n        if isinstance(prop_match, (list, tuple)):\n            # List items are logically ORed, so one matching item suffices.\n            for pm in prop_match:\n                if self._matches_prop(obj, prop_name, pm):\n                    return True\n        else:\n            # Some lists of resources do not have all properties, for example\n            # Hipersocket adapters do not have a \"card-location\" property.\n            # If a filter property does not exist on a resource, the resource\n            # does not match.\n            try:\n                prop_value = obj.get_property(prop_name)\n            except KeyError:\n                return False\n            if isinstance(prop_value, six.string_types):\n                # HMC resource property is Enum String or (non-enum) String,\n                # and is both matched by regexp matching. Ideally, regexp\n                # matching should only be done for non-enum strings, but\n                # distinguishing them is not possible given that the client\n                # has no knowledge about the properties.\n\n                # The regexp matching implemented in the HMC requires begin and\n                # end of the string value to match, even if the '^' for begin\n                # and '$' for end are not specified in the pattern. The code\n                # here is consistent with that: We add end matching to the\n                # pattern, and begin matching is done by re.match()\n                # automatically.\n                re_match = prop_match + '$'\n                m = re.match(re_match, prop_value)\n                if m:\n                    return True\n            else:\n                if prop_value == prop_match:\n                    return True\n        return False", "response": "Returns a boolean indicating whether a resource object matches the specified property and the match value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef resource_object(self, uri_or_oid, props=None):\n        if uri_or_oid.startswith('/api/'):\n            assert uri_or_oid[-1] != '/'\n            uri = uri_or_oid\n            oid = uri.split('/')[-1]\n        else:\n            assert '/' not in uri_or_oid\n            oid = uri_or_oid\n            uri = '{}/{}'.format(self._base_uri, oid)\n        res_props = {\n            self._oid_prop: oid,\n            'parent': self.parent.uri if self.parent is not None else None,\n            'class': self.class_name,\n        }\n        name = None\n        if props:\n            res_props.update(props)\n            try:\n                name = props[self._name_prop]\n            except KeyError:\n                pass\n        return self.resource_class(self, uri, name, res_props)", "response": "Returns a minimalistic Python resource object for this resource class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef findall(self, **filter_args):\n        if len(filter_args) == 1 and self._name_prop in filter_args:\n            try:\n                obj = self.find_by_name(filter_args[self._name_prop])\n            except NotFound:\n                return []\n            return [obj]\n        else:\n            obj_list = self.list(filter_args=filter_args)\n            return obj_list", "response": "Find all resources in scope of this manager and return a list of their Python resource objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find(self, **filter_args):\n        obj_list = self.findall(**filter_args)\n        num_objs = len(obj_list)\n        if num_objs == 0:\n            raise NotFound(filter_args, self)\n        elif num_objs > 1:\n            raise NoUniqueMatch(filter_args, self, obj_list)\n        else:\n            return obj_list[0]", "response": "Find exactly one resource in scope of this manager object by matching the specified filter arguments and return the corresponding Python resource object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_by_name(self, name):\n        uri = self._name_uri_cache.get(name)\n        obj = self.resource_class(\n            manager=self,\n            uri=uri,\n            name=name,\n            properties=None)\n        return obj", "response": "Finds a resource by its name and returns its Python resource object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_resources(self, resources):\n        for child_attr in resources:\n            child_list = resources[child_attr]\n            self._process_child_list(self, child_attr, child_list)", "response": "Add faked child resources to this resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a faked resource to this manager.", "response": "def add(self, properties):\n        \"\"\"\n        Add a faked resource to this manager.\n\n        For URI-based lookup, the resource is also added to the faked HMC.\n\n        Parameters:\n\n          properties (dict):\n            Resource properties. If the URI property (e.g. 'object-uri') or the\n            object ID property (e.g. 'object-id') are not specified, they\n            will be auto-generated.\n\n        Returns:\n          FakedBaseResource: The faked resource object.\n        \"\"\"\n        resource = self.resource_class(self, properties)\n        self._resources[resource.oid] = resource\n        self._hmc.all_resources[resource.uri] = resource\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, oid):\n        uri = self._resources[oid].uri\n        del self._resources[oid]\n        del self._hmc.all_resources[uri]", "response": "Removes a faked resource from this manager."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list(self, filter_args=None):\n        res = list()\n        for oid in self._resources:\n            resource = self._resources[oid]\n            if self._matches_filters(resource, filter_args):\n                res.append(resource)\n        return res", "response": "Returns a list of all the faked resources of this manager."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef console(self):\n        if self._console is None:\n            self._console = self.list()[0]\n        return self._console", "response": "Returns the faked Console object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a faked HBA resource.", "response": "def add(self, properties):\n        \"\"\"\n        Add a faked HBA resource.\n\n        Parameters:\n\n          properties (dict):\n            Resource properties.\n\n            Special handling and requirements for certain properties:\n\n            * 'element-id' will be auto-generated with a unique value across\n              all instances of this resource type, if not specified.\n            * 'element-uri' will be auto-generated based upon the element ID,\n              if not specified.\n            * 'class' will be auto-generated to 'hba',\n              if not specified.\n            * 'adapter-port-uri' identifies the backing FCP port for this HBA\n              and is required to be specified.\n            * 'device-number' will be auto-generated with a unique value\n              within the partition in the range 0x8000 to 0xFFFF, if not\n              specified.\n\n            This method also updates the 'hba-uris' property in the parent\n            faked Partition resource, by adding the URI for the faked HBA\n            resource.\n\n        Returns:\n          :class:`~zhmcclient_mock.FakedHba`: The faked HBA resource.\n\n        Raises:\n          :exc:`zhmcclient_mock.InputError`: Some issue with the input\n            properties.\n        \"\"\"\n        new_hba = super(FakedHbaManager, self).add(properties)\n\n        partition = self.parent\n\n        # Reflect the new NIC in the partition\n        assert 'hba-uris' in partition.properties\n        partition.properties['hba-uris'].append(new_hba.uri)\n\n        # Create a default device-number if not specified\n        if 'device-number' not in new_hba.properties:\n            devno = partition.devno_alloc()\n            new_hba.properties['device-number'] = devno\n\n        # Create a default wwpn if not specified\n        if 'wwpn' not in new_hba.properties:\n            wwpn = partition.wwpn_alloc()\n            new_hba.properties['wwpn'] = wwpn\n\n        return new_hba"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, oid):\n        hba = self.lookup_by_oid(oid)\n        partition = self.parent\n        devno = hba.properties.get('device-number', None)\n        if devno:\n            partition.devno_free_if_allocated(devno)\n        wwpn = hba.properties.get('wwpn', None)\n        if wwpn:\n            partition.wwpn_free_if_allocated(wwpn)\n        assert 'hba-uris' in partition.properties\n        hba_uris = partition.properties['hba-uris']\n        hba_uris.remove(hba.uri)\n        super(FakedHbaManager, self).remove(oid)", "response": "Removes a faked HBA resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a faked NIC resource.", "response": "def add(self, properties):\n        \"\"\"\n        Add a faked NIC resource.\n\n        Parameters:\n\n          properties (dict):\n            Resource properties.\n\n            Special handling and requirements for certain properties:\n\n            * 'element-id' will be auto-generated with a unique value across\n              all instances of this resource type, if not specified.\n            * 'element-uri' will be auto-generated based upon the element ID,\n              if not specified.\n            * 'class' will be auto-generated to 'nic',\n              if not specified.\n            * Either 'network-adapter-port-uri' (for backing ROCE adapters) or\n              'virtual-switch-uri'(for backing OSA or Hipersockets adapters) is\n              required to be specified.\n            * 'device-number' will be auto-generated with a unique value\n              within the partition in the range 0x8000 to 0xFFFF, if not\n              specified.\n\n            This method also updates the 'nic-uris' property in the parent\n            faked Partition resource, by adding the URI for the faked NIC\n            resource.\n\n            This method also updates the 'connected-vnic-uris' property in the\n            virtual switch referenced by 'virtual-switch-uri' property,\n            and sets it to the URI of the faked NIC resource.\n\n        Returns:\n          :class:`zhmcclient_mock.FakedNic`: The faked NIC resource.\n\n        Raises:\n          :exc:`zhmcclient_mock.InputError`: Some issue with the input\n            properties.\n        \"\"\"\n        new_nic = super(FakedNicManager, self).add(properties)\n\n        partition = self.parent\n\n        # For OSA-backed NICs, reflect the new NIC in the virtual switch\n        if 'virtual-switch-uri' in new_nic.properties:\n            vswitch_uri = new_nic.properties['virtual-switch-uri']\n            # Even though the URI handler when calling this method ensures that\n            # the vswitch exists, this method can be called by the user as\n            # well, so we have to handle the possibility that it does not\n            # exist:\n            try:\n                vswitch = self.hmc.lookup_by_uri(vswitch_uri)\n            except KeyError:\n                raise InputError(\"The virtual switch specified in the \"\n                                 \"'virtual-switch-uri' property does not \"\n                                 \"exist: {!r}\".format(vswitch_uri))\n            connected_uris = vswitch.properties['connected-vnic-uris']\n            if new_nic.uri not in connected_uris:\n                connected_uris.append(new_nic.uri)\n\n        # Create a default device-number if not specified\n        if 'device-number' not in new_nic.properties:\n            devno = partition.devno_alloc()\n            new_nic.properties['device-number'] = devno\n\n        # Reflect the new NIC in the partition\n        assert 'nic-uris' in partition.properties\n        partition.properties['nic-uris'].append(new_nic.uri)\n\n        return new_nic"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove(self, oid):\n        nic = self.lookup_by_oid(oid)\n        partition = self.parent\n        devno = nic.properties.get('device-number', None)\n        if devno:\n            partition.devno_free_if_allocated(devno)\n        assert 'nic-uris' in partition.properties\n        nic_uris = partition.properties['nic-uris']\n        nic_uris.remove(nic.uri)\n        super(FakedNicManager, self).remove(oid)", "response": "Removes a faked NIC resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nallocates a device number unique to this partition in the range of 0x8000 to 0xFFFF.", "response": "def devno_alloc(self):\n        \"\"\"\n        Allocates a device number unique to this partition, in the range of\n        0x8000 to 0xFFFF.\n\n        Returns:\n          string: The device number as four hexadecimal digits in upper case.\n\n        Raises:\n          ValueError: No more device numbers available in that range.\n        \"\"\"\n        devno_int = self._devno_pool.alloc()\n        devno = \"{:04X}\".format(devno_int)\n        return devno"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef devno_free(self, devno):\n        devno_int = int(devno, 16)\n        self._devno_pool.free(devno_int)", "response": "Free a device number allocated with devno_alloc."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef devno_free_if_allocated(self, devno):\n        devno_int = int(devno, 16)\n        self._devno_pool.free_if_allocated(devno_int)", "response": "Free a device number allocated with devno_alloc."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wwpn_alloc(self):\n        wwpn_int = self._wwpn_pool.alloc()\n        wwpn = \"AFFEAFFE0000\" + \"{:04X}\".format(wwpn_int)\n        return wwpn", "response": "Allocates a WWPN unique to this partition in the range of 0xAFFEAFFE00008000 to 0xAFFEAFFE0000FFFF. Returns a string that is a WWPN unique to this partition in the upper case hexadecimal digits in upper case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wwpn_free(self, wwpn):\n        wwpn_int = int(wwpn[-4:], 16)\n        self._wwpn_pool.free(wwpn_int)", "response": "Free a WWPN allocated with : meth : wwpn_alloc."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfree a WWPN allocated with wwpn_alloc.", "response": "def wwpn_free_if_allocated(self, wwpn):\n\n        \"\"\"\n        Free a WWPN allocated with :meth:`wwpn_alloc`.\n\n        If the WWPN is not currently allocated or not in the pool\n        range, nothing happens.\n\n        Parameters:\n          WWPN (string): The WWPN as 16 hexadecimal digits.\n        \"\"\"\n        wwpn_int = int(wwpn[-4:], 16)\n        self._wwpn_pool.free_if_allocated(wwpn_int)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a faked Port resource.", "response": "def add(self, properties):\n        \"\"\"\n        Add a faked Port resource.\n\n        Parameters:\n\n          properties (dict):\n            Resource properties.\n\n            Special handling and requirements for certain properties:\n\n            * 'element-id' will be auto-generated with a unique value across\n              all instances of this resource type, if not specified.\n            * 'element-uri' will be auto-generated based upon the element ID,\n              if not specified.\n            * 'class' will be auto-generated to 'network-port' or\n              'storage-port', if not specified.\n\n            This method also updates the 'network-port-uris' or\n            'storage-port-uris' property in the parent Adapter resource, by\n            adding the URI for the faked Port resource.\n\n        Returns:\n          :class:`zhmcclient_mock.FakedPort`: The faked Port resource.\n        \"\"\"\n        new_port = super(FakedPortManager, self).add(properties)\n        adapter = self.parent\n        if 'network-port-uris' in adapter.properties:\n            adapter.properties['network-port-uris'].append(new_port.uri)\n        if 'storage-port-uris' in adapter.properties:\n            adapter.properties['storage-port-uris'].append(new_port.uri)\n        return new_port"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a faked Port resource from the Adapter resource by its object ID.", "response": "def remove(self, oid):\n        \"\"\"\n        Remove a faked Port resource.\n\n        This method also updates the 'network-port-uris' or 'storage-port-uris'\n        property in the parent Adapter resource, by removing the URI for the\n        faked Port resource.\n\n        Parameters:\n\n          oid (string):\n            The object ID of the faked Port resource.\n        \"\"\"\n        port = self.lookup_by_oid(oid)\n        adapter = self.parent\n        if 'network-port-uris' in adapter.properties:\n            port_uris = adapter.properties['network-port-uris']\n            port_uris.remove(port.uri)\n        if 'storage-port-uris' in adapter.properties:\n            port_uris = adapter.properties['storage-port-uris']\n            port_uris.remove(port.uri)\n        super(FakedPortManager, self).remove(oid)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, properties):\n        new_vf = super(FakedVirtualFunctionManager, self).add(properties)\n        partition = self.parent\n        assert 'virtual-function-uris' in partition.properties\n        partition.properties['virtual-function-uris'].append(new_vf.uri)\n        if 'device-number' not in new_vf.properties:\n            devno = partition.devno_alloc()\n            new_vf.properties['device-number'] = devno\n        return new_vf", "response": "Add a faked Virtual Function resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves a faked Virtual Function resource.", "response": "def remove(self, oid):\n        \"\"\"\n        Remove a faked Virtual Function resource.\n\n        This method also updates the 'virtual-function-uris' property in the\n        parent Partition resource, by removing the URI for the faked Virtual\n        Function resource.\n\n        Parameters:\n\n          oid (string):\n            The object ID of the faked Virtual Function resource.\n        \"\"\"\n        virtual_function = self.lookup_by_oid(oid)\n        partition = self.parent\n        devno = virtual_function.properties.get('device-number', None)\n        if devno:\n            partition.devno_free_if_allocated(devno)\n        assert 'virtual-function-uris' in partition.properties\n        vf_uris = partition.properties['virtual-function-uris']\n        vf_uris.remove(virtual_function.uri)\n        super(FakedVirtualFunctionManager, self).remove(oid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_metric_group_definition(self, definition):\n        assert isinstance(definition, FakedMetricGroupDefinition)\n        group_name = definition.name\n        if group_name in self._metric_group_defs:\n            raise ValueError(\"A metric group definition with this name \"\n                             \"already exists: {}\".format(group_name))\n        self._metric_group_defs[group_name] = definition\n        self._metric_group_def_names.append(group_name)", "response": "Adds a faked metric group definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_metric_group_definition(self, group_name):\n        if group_name not in self._metric_group_defs:\n            raise ValueError(\"A metric group definition with this name does \"\n                             \"not exist: {}\".format(group_name))\n        return self._metric_group_defs[group_name]", "response": "Get a faked metric group definition by its group name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_metric_values(self, values):\n        assert isinstance(values, FakedMetricObjectValues)\n        group_name = values.group_name\n        if group_name not in self._metric_values:\n            self._metric_values[group_name] = []\n        self._metric_values[group_name].append(values)\n        if group_name not in self._metric_value_names:\n            self._metric_value_names.append(group_name)", "response": "Adds one set of faked metric values for a particular resource to the internal list of available metric values for a particular metric group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_metric_values(self, group_name):\n        if group_name not in self._metric_values:\n            raise ValueError(\"Metric values for this group name do not \"\n                             \"exist: {}\".format(group_name))\n        return self._metric_values[group_name]", "response": "Get the faked metric values for a metric group by its metric group_name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_metric_group_definitions(self):\n        group_names = self.properties.get('metric-groups', None)\n        if not group_names:\n            group_names = self.manager.get_metric_group_definition_names()\n        mg_defs = []\n        for group_name in group_names:\n            try:\n                mg_def = self.manager.get_metric_group_definition(group_name)\n                mg_defs.append(mg_def)\n            except ValueError:\n                pass  # ignore metric groups without metric group defs\n        return mg_defs", "response": "Get the faked metric group definitions for this context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the metric group infos for this context", "response": "def get_metric_group_infos(self):\n        \"\"\"\n        Get the faked metric group definitions for this context object\n        that are to be returned from its create operation, in the format\n        needed for the \"Create Metrics Context\" operation response.\n\n        Returns:\n\n          \"metric-group-infos\" JSON object as described for the \"Create Metrics\n            Context \"operation response.\n        \"\"\"\n        mg_defs = self.get_metric_group_definitions()\n        mg_infos = []\n        for mg_def in mg_defs:\n            metric_infos = []\n            for metric_name, metric_type in mg_def.types:\n                metric_infos.append({\n                    'metric-name': metric_name,\n                    'metric-type': metric_type,\n                })\n            mg_info = {\n                'group-name': mg_def.name,\n                'metric-infos': metric_infos,\n            }\n            mg_infos.append(mg_info)\n        return mg_infos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_metric_values(self):\n        group_names = self.properties.get('metric-groups', None)\n        if not group_names:\n            group_names = self.manager.get_metric_values_group_names()\n        ret = []\n        for group_name in group_names:\n            try:\n                mo_val = self.manager.get_metric_values(group_name)\n                ret_item = (group_name, mo_val)\n                ret.append(ret_item)\n            except ValueError:\n                pass  # ignore metric groups without metric values\n        return ret", "response": "Get the faked metrics for all metric groups and all resources that have been prepared on this context object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_metric_values_response(self):\n        mv_list = self.get_metric_values()\n        resp_lines = []\n        for mv in mv_list:\n            group_name = mv[0]\n            resp_lines.append('\"{}\"'.format(group_name))\n            mo_vals = mv[1]\n            for mo_val in mo_vals:\n                resp_lines.append('\"{}\"'.format(mo_val.resource_uri))\n                resp_lines.append(\n                    str(timestamp_from_datetime(mo_val.timestamp)))\n                v_list = []\n                for n, v in mo_val.values:\n                    if isinstance(v, six.string_types):\n                        v_str = '\"{}\"'.format(v)\n                    else:\n                        v_str = str(v)\n                    v_list.append(v_str)\n                v_line = ','.join(v_list)\n                resp_lines.append(v_line)\n                resp_lines.append('')\n            resp_lines.append('')\n        resp_lines.append('')\n        return '\\n'.join(resp_lines) + '\\n'", "response": "Get the faked metrics for all metric groups and all resources that have been prepared on the manager object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create(self, properties):\n        result = self.session.post(self.console.uri + '/user-roles',\n                                   body=properties)\n        # There should not be overlaps, but just in case there are, the\n        # returned props should overwrite the input props:\n        props = copy.deepcopy(properties)\n        props.update(result)\n        name = props.get(self._name_prop, None)\n        uri = props[self._uri_prop]\n        user_role = UserRole(self, uri, name, props)\n        self._name_uri_cache.update(name, uri)\n        return user_role", "response": "Creates a new User Role in this HMC."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_permission(self, permitted_object, include_members=False,\n                       view_only=True):\n        \"\"\"\n        Add permission for the specified permitted object(s) to this User Role,\n        thereby granting that permission to all users that have this User Role.\n\n        The granted permission depends on the resource class of the permitted\n        object(s):\n\n        * For Task resources, the granted permission is task permission for\n          that task.\n\n        * For Group resources, the granted permission is object access\n          permission for the group resource, and optionally also for the\n          group members.\n\n        * For any other resources, the granted permission is object access\n          permission for these resources.\n\n        The User Role must be user-defined.\n\n        Authorization requirements:\n\n        * Task permission to the \"Manage User Roles\" task.\n\n        Parameters:\n\n          permitted_object (:class:`~zhmcclient.BaseResource` or :term:`string`):\n            Permitted object(s), either as a Python resource object (e.g.\n            :class:`~zhmcclient.Partition`), or as a resource class string (e.g.\n            'partition').\n\n            Must not be `None`.\n\n          include_members (bool): Controls whether for Group resources, the\n            operation applies additionally to its group member resources.\n\n            This parameter will be ignored when the permitted object does not\n            specify Group resources.\n\n          view_only (bool): Controls whether for Task resources, the operation\n            aplies to the view-only version of the task (if `True`), or to\n            the full version of the task (if `False`). Only certain tasks\n            support a view-only version.\n\n            This parameter will be ignored when the permitted object does not\n            specify Task resources.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"  # noqa: E501\n        if isinstance(permitted_object, BaseResource):\n            perm_obj = permitted_object.uri\n            perm_type = 'object'\n        elif isinstance(permitted_object, six.string_types):\n            perm_obj = permitted_object\n            perm_type = 'object-class'\n        else:\n            raise TypeError(\n                \"permitted_object must be a string or BaseResource, but is: \"\n                \"{}\".format(type(permitted_object)))\n        body = {\n            'permitted-object': perm_obj,\n            'permitted-object-type': perm_type,\n            'include-members': include_members,\n            'view-only-mode': view_only,\n        }\n        self.manager.session.post(\n            self.uri + '/operations/add-permission',\n            body=body)", "response": "Adds a permission to the User Role."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new storage volume in the HMC and returns a new object with the given properties.", "response": "def create(self, properties, email_to_addresses=None,\n               email_cc_addresses=None, email_insert=None):\n        \"\"\"\n        Create a :term:`storage volume` in this storage group on the HMC, and\n        optionally send emails to storage administrators requesting creation of\n        the storage volume on the storage subsystem and setup of any resources\n        related to the storage volume (e.g. LUN mask definition on the storage\n        subsystem).\n\n        This method performs the \"Modify Storage Group Properties\" operation,\n        requesting creation of the volume.\n\n        Authorization requirements:\n\n        * Object-access permission to this storage group.\n        * Task permission to the \"Configure Storage - System Programmer\" task.\n\n        Parameters:\n\n          properties (dict): Initial property values for the new volume.\n\n            Allowable properties are the fields defined in the\n            \"storage-volume-request-info\" nested object described for\n            operation \"Modify Storage Group Properties\" in the\n            :term:`HMC API` book.\n            The valid fields are those for the \"create\" operation. The\n            `operation` field must not be provided; it is set automatically\n            to the value \"create\".\n\n            The properties provided in this parameter will be copied and then\n            amended with the `operation=\"create\"` field, and then used as a\n            single array item for the `storage-volumes` field in the request\n            body of the \"Modify Storage Group Properties\" operation.\n\n            Note that for storage volumes, the HMC does auto-generate a value\n            for the \"name\" property, but that auto-generated name is not unique\n            within the parent storage group. If you depend on a unique name,\n            you need to specify a \"name\" property accordingly.\n\n          email_to_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be notified.\n            If `None` or empty, no email will be sent.\n\n          email_cc_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be copied\n            on the notification email.\n            If `None` or empty, nobody will be copied on the email.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n          email_insert (:term:`string`): Additional text to be inserted in the\n            notification email.\n            The text can include HTML formatting tags.\n            If `None`, no additional text will be inserted.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n        Returns:\n\n          StorageVolume:\n            The resource object for the new storage volume.\n            The object will have the following properties set:\n\n            - 'element-uri' as returned by the HMC\n            - 'element-id' as determined from the 'element-uri' property\n            - 'class' and 'parent'\n            - additional properties as specified in the input properties\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n\n        Example::\n\n            stovol1 = fcp_stogrp.storage_volumes.create(\n                properties=dict(\n                    name='vol1',\n                    size=30,  # GiB\n            ))\n        \"\"\"\n\n        volreq_obj = copy.deepcopy(properties)\n        volreq_obj['operation'] = 'create'\n        body = {\n            'storage-volumes': [volreq_obj],\n        }\n        if email_to_addresses:\n            body['email-to-addresses'] = email_to_addresses\n            if email_cc_addresses:\n                body['email-cc-addresses'] = email_cc_addresses\n            if email_insert:\n                body['email-insert'] = email_insert\n        else:\n            if email_cc_addresses:\n                raise ValueError(\"email_cc_addresses must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_cc_addresses)\n            if email_insert:\n                raise ValueError(\"email_insert must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_insert)\n\n        result = self.session.post(\n            self.storage_group.uri + '/operations/modify',\n            body=body)\n        uri = result['element-uris'][0]\n\n        storage_volume = self.resource_object(uri, properties)\n\n        # The name is not guaranteed to be unique, so we don't maintain\n        # a name-to-uri cache for storage volumes.\n\n        return storage_volume"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef oid(self):\n        m = re.match(r'^/api/storage-groups/[^/]*/storage-volumes/([^/]*)$',\n                     self.uri)\n        oid = m.group(1)\n        return oid", "response": "Return the object ID of this storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self, email_to_addresses=None, email_cc_addresses=None,\n               email_insert=None):\n        \"\"\"\n        Delete this storage volume on the HMC, and optionally send emails to\n        storage administrators requesting deletion of the storage volume on the\n        storage subsystem and cleanup of any resources related to the storage\n        volume (e.g. LUN mask definitions on a storage subsystem).\n\n        This method performs the \"Modify Storage Group Properties\" operation,\n        requesting deletion of the volume.\n\n        Authorization requirements:\n\n        * Object-access permission to the storage group owning this storage\n          volume.\n        * Task permission to the \"Configure Storage - System Programmer\" task.\n\n        Parameters:\n\n          email_to_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be notified.\n            If `None` or empty, no email will be sent.\n\n          email_cc_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be copied\n            on the notification email.\n            If `None` or empty, nobody will be copied on the email.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n          email_insert (:term:`string`): Additional text to be inserted in the\n            notification email.\n            The text can include HTML formatting tags.\n            If `None`, no additional text will be inserted.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n\n        volreq_obj = {\n            'operation': 'delete',\n            'element-uri': self.uri,\n        }\n        body = {\n            'storage-volumes': [\n                volreq_obj\n            ],\n        }\n        if email_to_addresses:\n            body['email-to-addresses'] = email_to_addresses\n            if email_cc_addresses:\n                body['email-cc-addresses'] = email_cc_addresses\n            if email_insert:\n                body['email-insert'] = email_insert\n        else:\n            if email_cc_addresses:\n                raise ValueError(\"email_cc_addresses must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_cc_addresses)\n            if email_insert:\n                raise ValueError(\"email_insert must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_insert)\n\n        self.manager.session.post(\n            self.manager.storage_group.uri + '/operations/modify',\n            body=body)\n\n        self.manager._name_uri_cache.delete(\n            self.properties.get(self.manager._name_prop, None))", "response": "Delete this storage volume on the HMC and optionally send emails to the storage administrators and cleanup of any resources related to the storage volume."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_properties(self, properties, email_to_addresses=None,\n                          email_cc_addresses=None, email_insert=None):\n        \"\"\"\n        Update writeable properties of this storage volume on the HMC, and\n        optionally send emails to storage administrators requesting\n        modification of the storage volume on the storage subsystem and of any\n        resources related to the storage volume.\n\n        This method performs the \"Modify Storage Group Properties\" operation,\n        requesting modification of the volume.\n\n        Authorization requirements:\n\n        * Object-access permission to the storage group owning this storage\n          volume.\n        * Task permission to the \"Configure Storage - System Programmer\" task.\n\n        Parameters:\n\n          properties (dict): New property values for the volume.\n            Allowable properties are the fields defined in the\n            \"storage-volume-request-info\" nested object for the \"modify\"\n            operation. That nested object is described in section \"Request body\n            contents\" for operation \"Modify Storage Group Properties\" in the\n            :term:`HMC API` book.\n\n            The properties provided in this parameter will be copied and then\n            amended with the `operation=\"modify\"` and `element-uri` properties,\n            and then used as a single array item for the `storage-volumes`\n            field in the request body of the \"Modify Storage Group Properties\"\n            operation.\n\n          email_to_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be notified.\n            If `None` or empty, no email will be sent.\n\n          email_cc_addresses (:term:`iterable` of :term:`string`): Email\n            addresses of one or more storage administrator to be copied\n            on the notification email.\n            If `None` or empty, nobody will be copied on the email.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n          email_insert (:term:`string`): Additional text to be inserted in the\n            notification email.\n            The text can include HTML formatting tags.\n            If `None`, no additional text will be inserted.\n            Must be `None` or empty if `email_to_addresses` is `None` or empty.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n\n        volreq_obj = copy.deepcopy(properties)\n        volreq_obj['operation'] = 'modify'\n        volreq_obj['element-uri'] = self.uri\n        body = {\n            'storage-volumes': [volreq_obj],\n        }\n        if email_to_addresses:\n            body['email-to-addresses'] = email_to_addresses\n            if email_cc_addresses:\n                body['email-cc-addresses'] = email_cc_addresses\n            if email_insert:\n                body['email-insert'] = email_insert\n        else:\n            if email_cc_addresses:\n                raise ValueError(\"email_cc_addresses must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_cc_addresses)\n            if email_insert:\n                raise ValueError(\"email_insert must not be specified if \"\n                                 \"there is no email_to_addresses: %r\" %\n                                 email_insert)\n\n        self.manager.session.post(\n            self.manager.storage_group.uri + '/operations/modify',\n            body=body)\n\n        self.properties.update(copy.deepcopy(properties))", "response": "This method is used to update writeable properties of the storage group with the provided properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef indicate_fulfillment_fcp(self, wwpn, lun, host_port):\n\n        # The operation requires exactly 16 characters in lower case\n        wwpn_16 = format(int(wwpn, 16), '016x')\n        lun_16 = format(int(lun, 16), '016x')\n\n        body = {\n            'world-wide-port-name': wwpn_16,\n            'logical-unit-number': lun_16,\n            'adapter-port-uri': host_port.uri,\n        }\n        self.manager.session.post(\n            self.uri + '/operations/fulfill-fcp-storage-volume',\n            body=body)", "response": "This method is used to indicate the completion of a FCP storage volume. This method is used to indicate the actual FCP storage volume and the actual FCP storage volume on the storage subsystem."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the exception as a string in a Python definition - style format.", "response": "def str_def(self):\n        \"\"\"\n        :term:`string`: The exception as a string in a Python definition-style\n        format, e.g. for parsing by scripts:\n\n        .. code-block:: text\n\n            classname={}; read_timeout={}; read_retries={}; message={};\n        \"\"\"\n        return \"classname={!r}; read_timeout={!r}; read_retries={!r}; \" \\\n            \"message={!r};\". \\\n            format(self.__class__.__name__, self.read_timeout,\n                   self.read_retries, self.args[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the exception as a string in a Python definition - style format.", "response": "def str_def(self):\n        \"\"\"\n        :term:`string`: The exception as a string in a Python definition-style\n        format, e.g. for parsing by scripts:\n\n        .. code-block:: text\n\n            classname={}; connect_retries={}; message={};\n        \"\"\"\n        return \"classname={!r}; connect_retries={!r}; message={!r};\". \\\n            format(self.__class__.__name__, self.connect_retries, self.args[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef str_def(self):\n        return \"classname={!r}; line={!r}; column={!r}; message={!r};\". \\\n            format(self.__class__.__name__, self.line, self.column,\n                   self.args[0])", "response": "The exception as a string in a Python definition - style\nTaxonomy format e. g. for parsing by scripts."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef str_def(self):\n        return \"classname={!r}; min_api_version={!r}; api_version={!r}; \" \\\n            \"message={!r};\". \\\n            format(self.__class__.__name__, self.min_api_version,\n                   self.api_version, self.args[0])", "response": "Return the exception as a string in a Python definition - style\n            format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef str_def(self):\n        return \"classname={!r}; operation_timeout={!r}; message={!r};\". \\\n            format(self.__class__.__name__, self.operation_timeout,\n                   self.args[0])", "response": "Return the exception as a string in a Python definition - style\n            format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reassign_port(self, port):\n        body = {'adapter-port-uri': port.uri}\n        self.manager.session.post(\n            self._uri + '/operations/reassign-storage-adapter-port',\n            body=body)\n        self.properties.update(body)", "response": "Reassign this HBA to a new underlying : term:`FCP port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list(self, full_properties=False, filter_args=None):\n        resource_obj_list = []\n        uris = self.partition.get_property('virtual-function-uris')\n        if uris:\n            for uri in uris:\n\n                resource_obj = self.resource_class(\n                    manager=self,\n                    uri=uri,\n                    name=None,\n                    properties=None)\n\n                if self._matches_filters(resource_obj, filter_args):\n                    resource_obj_list.append(resource_obj)\n                    if full_properties:\n                        resource_obj.pull_full_properties()\n\n        self._name_uri_cache.update_from(resource_obj_list)\n        return resource_obj_list", "response": "Returns a list of Virtual Functions of this Partition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_connected_nics(self):\n        result = self.manager.session.get(\n            self.uri + '/operations/get-connected-vnics')\n        nic_uris = result['connected-vnic-uris']\n        nic_list = []\n        parts = {}  # Key: Partition ID; Value: Partition object\n        for nic_uri in nic_uris:\n            m = re.match(r\"^/api/partitions/([^/]+)/nics/([^/]+)/?$\", nic_uri)\n            part_id = m.group(1)\n            nic_id = m.group(2)\n            # We remember created Partition objects and reuse them.\n            try:\n                part = parts[part_id]\n            except KeyError:\n                part = self.manager.cpc.partitions.resource_object(part_id)\n                parts[part_id] = part\n            nic = part.nics.resource_object(nic_id)\n            nic_list.append(nic)\n        return nic_list", "response": "Returns a list of NICS connected to this Virtual Switch."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of all ports of this Adapter.", "response": "def list(self, full_properties=False, filter_args=None):\n        \"\"\"\n        List the Ports of this Adapter.\n\n        If the adapter does not have any ports, an empty list is returned.\n\n        Authorization requirements:\n\n        * Object-access permission to this Adapter.\n\n        Parameters:\n\n          full_properties (bool):\n            Controls whether the full set of resource properties should be\n            retrieved, vs. only the short set as returned by the list\n            operation.\n\n          filter_args (dict):\n            Filter arguments that narrow the list of returned resources to\n            those that match the specified filter arguments. For details, see\n            :ref:`Filtering`.\n\n            `None` causes no filtering to happen, i.e. all resources are\n            returned.\n\n        Returns:\n\n          : A list of :class:`~zhmcclient.Port` objects.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        uris_prop = self.adapter.port_uris_prop\n        if not uris_prop:\n            # Adapter does not have any ports\n            return []\n\n        uris = self.adapter.get_property(uris_prop)\n        assert uris is not None\n\n        # TODO: Remove the following circumvention once fixed.\n        # The following line circumvents a bug for FCP adapters that sometimes\n        # causes duplicate URIs to show up in this property:\n        uris = list(set(uris))\n\n        resource_obj_list = []\n        for uri in uris:\n\n            resource_obj = self.resource_class(\n                manager=self,\n                uri=uri,\n                name=None,\n                properties=None)\n\n            if self._matches_filters(resource_obj, filter_args):\n                resource_obj_list.append(resource_obj)\n                if full_properties:\n                    resource_obj.pull_full_properties()\n\n        self._name_uri_cache.update_from(resource_obj_list)\n        return resource_obj_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the specified User Role to this User.", "response": "def add_user_role(self, user_role):\n        \"\"\"\n        Add the specified User Role to this User.\n\n        This User must not be a system-defined or pattern-based user.\n\n        Authorization requirements:\n\n        * Task permission to the \"Manage Users\" task to modify a standard user\n          or the \"Manage User Templates\" task to modify a template user.\n\n        Parameters:\n\n          user_role (:class:`~zhmcclient.UserRole`): User Role to be added.\n            Must not be `None`.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {\n            'user-role-uri': user_role.uri\n        }\n        self.manager.session.post(\n            self.uri + '/operations/add-user-role',\n            body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_user_role(self, user_role):\n        body = {\n            'user-role-uri': user_role.uri\n        }\n        self.manager.session.post(\n            self.uri + '/operations/remove-user-role',\n            body=body)", "response": "Removes the specified User Role from this User."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, uri, logon_required=True):\n        try:\n            return self._urihandler.get(self._hmc, uri, logon_required)\n        except HTTPError as exc:\n            raise zhmcclient.HTTPError(exc.response())\n        except ConnectionError as exc:\n            raise zhmcclient.ConnectionError(exc.message, None)", "response": "Perform a HTTP GET method against the resource identified by a URI."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nperform an HTTP POST operation against the resource identified by a URI.", "response": "def post(self, uri, body=None, logon_required=True,\n             wait_for_completion=True, operation_timeout=None):\n        \"\"\"\n        Perform the HTTP POST method against the resource identified by a URI,\n        using a provided request body, on the faked HMC.\n\n        HMC operations using HTTP POST are either synchronous or asynchronous.\n        Asynchronous operations return the URI of an asynchronously executing\n        job that can be queried for status and result.\n\n        Examples for synchronous operations:\n\n        * With no response body: \"Logon\", \"Update CPC Properties\"\n        * With a response body: \"Create Partition\"\n\n        Examples for asynchronous operations:\n\n        * With no ``job-results`` field in the completed job status response:\n          \"Start Partition\"\n        * With a ``job-results`` field in the completed job status response\n          (under certain conditions): \"Activate a Blade\", or \"Set CPC Power\n          Save\"\n\n        The `wait_for_completion` parameter of this method can be used to deal\n        with asynchronous HMC operations in a synchronous way.\n\n        Parameters:\n\n          uri (:term:`string`):\n            Relative URI path of the resource, e.g. \"/api/session\".\n            This URI is relative to the base URL of the session (see the\n            :attr:`~zhmcclient.Session.base_url` property).\n            Must not be `None`.\n\n          body (:term:`json object`):\n            JSON object to be used as the HTTP request body (payload).\n            `None` means the same as an empty dictionary, namely that no HTTP\n            body is included in the request.\n\n          logon_required (bool):\n            Boolean indicating whether the operation requires that the session\n            is logged on to the HMC. For example, the \"Logon\" operation does\n            not require that.\n\n            Because this is a faked HMC, this does not perform a real logon,\n            but it is still used to update the state in the faked HMC.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the requested\n              operation, regardless of whether the operation is synchronous or\n              asynchronous.\n\n              This will cause an additional entry in the time statistics to be\n              created for the asynchronous operation and waiting for its\n              completion. This entry will have a URI that is the targeted URI,\n              appended with \"+completion\".\n\n            * If `False`, this method will immediately return the result of the\n              HTTP POST method, regardless of whether the operation is\n              synchronous or asynchronous.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, when waiting for completion of an asynchronous\n            operation. The special value 0 means that no timeout is set. `None`\n            means that the default async operation timeout of the session is\n            used.\n\n            For `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised when the timeout\n            expires.\n\n            For `wait_for_completion=False`, this parameter has no effect.\n\n        Returns:\n\n          :term:`json object`:\n\n            If `wait_for_completion` is `True`, returns a JSON object\n            representing the response body of the synchronous operation, or the\n            response body of the completed job that performed the asynchronous\n            operation. If a synchronous operation has no response body, `None`\n            is returned.\n\n            If `wait_for_completion` is `False`, returns a JSON object\n            representing the response body of the synchronous or asynchronous\n            operation. In case of an asynchronous operation, the JSON object\n            will have a member named ``job-uri``, whose value can be used with\n            the :meth:`~zhmcclient.Session.query_job_status` method to\n            determine the status of the job and the result of the original\n            operation, once the job has completed.\n\n            See the section in the :term:`HMC API` book about the specific HMC\n            operation and about the 'Query Job Status' operation, for a\n            description of the members of the returned JSON objects.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError` (not implemented)\n          :exc:`~zhmcclient.AuthError` (not implemented)\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        try:\n            return self._urihandler.post(self._hmc, uri, body, logon_required,\n                                         wait_for_completion)\n        except HTTPError as exc:\n            raise zhmcclient.HTTPError(exc.response())\n        except ConnectionError as exc:\n            raise zhmcclient.ConnectionError(exc.message, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notifications(self):\n\n        while True:\n            with self._handover_cond:\n\n                # Wait until MessageListener has a new notification\n                while len(self._handover_dict) == 0:\n                    self._handover_cond.wait(self._wait_timeout)\n\n                if self._handover_dict['headers'] is None:\n                    return\n\n                # Process the notification\n                yield (self._handover_dict['headers'],\n                       self._handover_dict['message'])\n\n                # Indicate to MessageListener that we are ready for next\n                # notification\n                del self._handover_dict['headers']\n                del self._handover_dict['message']\n                self._handover_cond.notifyAll()", "response": "A method that yields all JMS messages received by this notification receiver."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of console objects that are connected to this HMC.", "response": "def list(self, full_properties=True, filter_args=None):\n        \"\"\"\n        List the (one) :term:`Console` representing the HMC this client is\n        connected to.\n\n        Authorization requirements:\n\n        * None\n\n        Parameters:\n\n          full_properties (bool):\n            Controls whether the full set of resource properties should be\n            retrieved, vs. only a short set consisting of 'object-uri'.\n\n          filter_args (dict):\n            This parameter exists for consistency with other list() methods\n            and will be ignored.\n\n        Returns:\n\n          : A list of :class:`~zhmcclient.Console` objects, containing the one\n          :term:`Console` representing the HMC this client is connected to.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        uri = self._base_uri  # There is only one console object.\n        if full_properties:\n            props = self.session.get(uri)\n        else:\n            # Note: The Console resource's Object ID is not part of its URI.\n            props = {\n                self._uri_prop: uri,\n            }\n        resource_obj = self.resource_class(\n            manager=self,\n            uri=props[self._uri_prop],\n            name=props.get(self._name_prop, None),\n            properties=props)\n        return [resource_obj]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a manager object for the Storage Groups in scope of this Console.", "response": "def storage_groups(self):\n        \"\"\"\n        :class:`~zhmcclient.StorageGroupManager`:\n          Manager object for the Storage Groups in scope of this Console.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._storage_groups:\n            self._storage_groups = StorageGroupManager(self)\n        return self._storage_groups"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naccessing to the users in this Console.", "response": "def users(self):\n        \"\"\"\n        :class:`~zhmcclient.UserManager`: Access to the :term:`Users <User>` in\n        this Console.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._users:\n            self._users = UserManager(self)\n        return self._users"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naccesses to the User Roles in this Console.", "response": "def user_roles(self):\n        \"\"\"\n        :class:`~zhmcclient.UserRoleManager`: Access to the\n        :term:`User Roles <User Role>` in this Console.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._user_roles:\n            self._user_roles = UserRoleManager(self)\n        return self._user_roles"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef user_patterns(self):\n        # We do here some lazy loading.\n        if not self._user_patterns:\n            self._user_patterns = UserPatternManager(self)\n        return self._user_patterns", "response": "Access to the User Patterns class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef password_rules(self):\n        # We do here some lazy loading.\n        if not self._password_rules:\n            self._password_rules = PasswordRuleManager(self)\n        return self._password_rules", "response": "Access to the password rules of the current Console."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tasks(self):\n        # We do here some lazy loading.\n        if not self._tasks:\n            self._tasks = TaskManager(self)\n        return self._tasks", "response": "Access to the tasks in this Console."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\naccessing to the LDAP Server Definitions in this Console.", "response": "def ldap_server_definitions(self):\n        \"\"\"\n        :class:`~zhmcclient.LdapServerDefinitionManager`: Access to the\n        :term:`LDAP Server Definitions <LDAP Server Definition>` in this\n        Console.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._ldap_server_definitions:\n            self._ldap_server_definitions = LdapServerDefinitionManager(self)\n        return self._ldap_server_definitions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unmanaged_cpcs(self):\n        # We do here some lazy loading.\n        if not self._unmanaged_cpcs:\n            self._unmanaged_cpcs = UnmanagedCpcManager(self)\n        return self._unmanaged_cpcs", "response": "Access to the UnmanagedCpcManager object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrestarts the HMC with the specified parameters.", "response": "def restart(self, force=False, wait_for_available=True,\n                operation_timeout=None):\n        \"\"\"\n        Restart the HMC represented by this Console object.\n\n        Once the HMC is online again, this Console object, as well as any other\n        resource objects accessed through this HMC, can continue to be used.\n        An automatic re-logon will be performed under the covers, because the\n        HMC restart invalidates the currently used HMC session.\n\n        Authorization requirements:\n\n        * Task permission for the \"Shutdown/Restart\" task.\n        * \"Remote Restart\" must be enabled on the HMC.\n\n        Parameters:\n\n          force (bool):\n            Boolean controlling whether the restart operation is processed when\n            users are connected (`True`) or not (`False`). Users in this sense\n            are local or remote GUI users. HMC WS API clients do not count as\n            users for this purpose.\n\n          wait_for_available (bool):\n            Boolean controlling whether this method should wait for the HMC to\n            become available again after the restart, as follows:\n\n            * If `True`, this method will wait until the HMC has restarted and\n              is available again. The\n              :meth:`~zhmcclient.Client.query_api_version` method will be used\n              to check for availability of the HMC.\n\n            * If `False`, this method will return immediately once the HMC\n              has accepted the request to be restarted.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for HMC availability after the\n            restart. The special value 0 means that no timeout is set. `None`\n            means that the default async operation timeout of the session is\n            used. If the timeout expires when `wait_for_available=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for the HMC to become available again after the restart.\n        \"\"\"\n        body = {'force': force}\n        self.manager.session.post(self.uri + '/operations/restart', body=body)\n        if wait_for_available:\n            time.sleep(10)\n            self.manager.client.wait_for_available(\n                operation_timeout=operation_timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nshutting down and power off the HMC.", "response": "def shutdown(self, force=False):\n        \"\"\"\n        Shut down and power off the HMC represented by this Console object.\n\n        While the HMC is powered off, any Python resource objects retrieved\n        from this HMC may raise exceptions upon further use.\n\n        In order to continue using Python resource objects retrieved from this\n        HMC, the HMC needs to be started again (e.g. by powering it on\n        locally). Once the HMC is available again, Python resource objects\n        retrieved from that HMC can continue to be used.\n        An automatic re-logon will be performed under the covers, because the\n        HMC startup invalidates the currently used HMC session.\n\n        Authorization requirements:\n\n        * Task permission for the \"Shutdown/Restart\" task.\n        * \"Remote Shutdown\" must be enabled on the HMC.\n\n        Parameters:\n\n          force (bool):\n            Boolean controlling whether the shutdown operation is processed\n            when users are connected (`True`) or not (`False`). Users in this\n            sense are local or remote GUI users. HMC WS API clients do not\n            count as users for this purpose.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {'force': force}\n        self.manager.session.post(self.uri + '/operations/shutdown', body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _time_query_parms(begin_time, end_time):\n        query_parms = []\n        if begin_time is not None:\n            begin_ts = timestamp_from_datetime(begin_time)\n            qp = 'begin-time={}'.format(begin_ts)\n            query_parms.append(qp)\n        if end_time is not None:\n            end_ts = timestamp_from_datetime(end_time)\n            qp = 'end-time={}'.format(end_ts)\n            query_parms.append(qp)\n        query_parms_str = '&'.join(query_parms)\n        if query_parms_str:\n            query_parms_str = '?' + query_parms_str\n        return query_parms_str", "response": "Return the URI query paramterer string for the specified begin time and end time."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the console audit log entries optionally filtered by their respective begin_time and end_time.", "response": "def get_audit_log(self, begin_time=None, end_time=None):\n        \"\"\"\n        Return the console audit log entries, optionally filtered by their\n        creation time.\n\n        Authorization requirements:\n\n        * Task permission to the \"Audit and Log Management\" task.\n\n        Parameters:\n\n          begin_time (:class:`~py:datetime.datetime`):\n            Begin time for filtering. Log entries with a creation time older\n            than the begin time will be omitted from the results.\n\n            If `None`, no such filtering is performed (and the oldest available\n            log entries will be included).\n\n          end_time (:class:`~py:datetime.datetime`):\n            End time for filtering. Log entries with a creation time newer\n            than the end time will be omitted from the results.\n\n            If `None`, no such filtering is performed (and the newest available\n            log entries will be included).\n\n        Returns:\n\n          :term:`json object`:\n            A JSON object with the log entries, as described in section\n            'Response body contents' of operation 'Get Console Audit Log' in\n            the :term:`HMC API` book.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        query_parms = self._time_query_parms(begin_time, end_time)\n        uri = self.uri + '/operations/get-audit-log' + query_parms\n        result = self.manager.session.post(uri)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of unmanaged CPCs of this HMC.", "response": "def list_unmanaged_cpcs(self, name=None):\n        \"\"\"\n        List the unmanaged CPCs of this HMC.\n\n        For details, see :meth:`~zhmcclient.UnmanagedCpc.list`.\n\n        Authorization requirements:\n\n        * None\n\n        Parameters:\n\n          name (:term:`string`):\n            Regular expression pattern for the CPC name, as a filter that\n            narrows the list of returned CPCs to those whose name property\n            matches the specified pattern.\n\n            `None` causes no filtering to happen, i.e. all unmanaged CPCs\n            discovered by the HMC are returned.\n\n        Returns:\n\n          : A list of :class:`~zhmcclient.UnmanagedCpc` objects.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        filter_args = dict()\n        if name is not None:\n            filter_args['name'] = name\n        cpcs = self.unmanaged_cpcs.list(filter_args=filter_args)\n        return cpcs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lpars(self):\n        # We do here some lazy loading.\n        if not self._lpars:\n            self._lpars = LparManager(self)\n        return self._lpars", "response": "Access to the LPARs of this CPC."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naccesses to the PartitionManager instance for this CPC.", "response": "def partitions(self):\n        \"\"\"\n        :class:`~zhmcclient.PartitionManager`: Access to the\n        :term:`Partitions <Partition>` in this CPC.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._partitions:\n            self._partitions = PartitionManager(self)\n        return self._partitions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adapters(self):\n        # We do here some lazy loading.\n        if not self._adapters:\n            self._adapters = AdapterManager(self)\n        return self._adapters", "response": "Access to the adapters in this CPC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef virtual_switches(self):\n        # We do here some lazy loading.\n        if not self._virtual_switches:\n            self._virtual_switches = VirtualSwitchManager(self)\n        return self._virtual_switches", "response": "Access to the Virtual Switches object in this CPC."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reset_activation_profiles(self):\n        # We do here some lazy loading.\n        if not self._reset_activation_profiles:\n            self._reset_activation_profiles = \\\n                ActivationProfileManager(self, profile_type='reset')\n        return self._reset_activation_profiles", "response": "Access to the ActivationProfileManager"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\naccess to the ActivationProfileManager for this CPC.", "response": "def image_activation_profiles(self):\n        \"\"\"\n        :class:`~zhmcclient.ActivationProfileManager`: Access to the\n        :term:`Image Activation Profiles <Image Activation Profile>` in this\n        CPC.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._image_activation_profiles:\n            self._image_activation_profiles = \\\n                ActivationProfileManager(self, profile_type='image')\n        return self._image_activation_profiles"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_activation_profiles(self):\n        # We do here some lazy loading.\n        if not self._load_activation_profiles:\n            self._load_activation_profiles = \\\n                ActivationProfileManager(self, profile_type='load')\n        return self._load_activation_profiles", "response": "Access to the ActivationProfileManager"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the maximum number of active logical partitions or partitions of this CPC.", "response": "def maximum_active_partitions(self):\n        \"\"\"\n        Integer: The maximum number of active logical partitions or partitions\n        of this CPC.\n\n        The following table shows the maximum number of active logical\n        partitions or partitions by machine generations supported at the HMC\n        API:\n\n        =========================  ==================\n        Machine generation         Maximum partitions\n        =========================  ==================\n        z196                                      60\n        z114                                      30\n        zEC12                                     60\n        zBC12                                     30\n        z13 / Emperor                             85\n        z13s / Rockhopper                         40\n        z14 / Emperor II                          85\n        z14-ZR1 / Rockhopper II                   40\n        =========================  ==================\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`ValueError`: Unknown machine type\n        \"\"\"\n        machine_type = self.get_property('machine-type')\n        try:\n            max_parts = self._MAX_PARTITIONS_BY_MACHINE_TYPE[machine_type]\n        except KeyError:\n            raise ValueError(\"Unknown machine type: {!r}\".format(machine_type))\n        return max_parts"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef feature_info(self):\n        feature_list = self.prop('available-features-list', None)\n        if feature_list is None:\n            raise ValueError(\"Firmware features are not supported on CPC %s\" %\n                             self.name)\n        return feature_list", "response": "Returns a list of information about the features available for this CPC."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting this HMC and return a new instance of a new HMC.", "response": "def start(self, wait_for_completion=True, operation_timeout=None):\n        \"\"\"\n        Start this CPC, using the HMC operation \"Start CPC\".\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Task permission for the \"Start (start a single DPM system)\" task.\n\n        Parameters:\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n        \"\"\"\n        result = self.manager.session.post(\n            self.uri + '/operations/start',\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimporting activation profiles and/or system activity profiles for this CPC from the SE hard drive into the CPC using the HMC operation \"Import Profiles\". This operation is not permitted when the CPC is in DPM mode. Authorization requirements: * Object-access permission to this CPC. * Task permission for the \"CIM Actions ExportSettingsData\" task. Parameters: profile_area (int): The numbered hard drive area (1-4) from which the profiles are imported. wait_for_completion (bool): Boolean controlling whether this method should wait for completion of the requested asynchronous HMC operation, as follows: * If `True`, this method will wait for completion of the asynchronous job performing the operation. * If `False`, this method will return immediately once the HMC has accepted the request to perform the operation. operation_timeout (:term:`number`): Timeout in seconds, for waiting for completion of the asynchronous job performing the operation. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.OperationTimeout` is raised. Returns: `None` or :class:`~zhmcclient.Job`: If `wait_for_completion` is `True`, returns `None`. If `wait_for_completion` is `False`, returns a :class:`~zhmcclient.Job` object representing the asynchronously executing job on the HMC. Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.OperationTimeout`: The timeout expired while waiting for completion of the operation.", "response": "def import_profiles(self, profile_area, wait_for_completion=True,\n                        operation_timeout=None):\n        \"\"\"\n        Import activation profiles and/or system activity profiles for this CPC\n        from the SE hard drive into the CPC using the HMC operation\n        \"Import Profiles\".\n\n        This operation is not permitted when the CPC is in DPM mode.\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Task permission for the \"CIM Actions ExportSettingsData\" task.\n\n        Parameters:\n\n          profile_area (int):\n            The numbered hard drive area (1-4) from which the profiles are\n            imported.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n        \"\"\"\n        body = {'profile-area': profile_area}\n        result = self.manager.session.post(\n            self.uri + '/operations/import-profiles',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_wwpns(self, partitions):\n        body = {'partitions': [p.uri for p in partitions]}\n        result = self.manager.session.post(self._uri + '/operations/'\n                                           'export-port-names-list', body=body)\n        # Parse the returned comma-separated string for each WWPN into a dict:\n        wwpn_list = []\n        dict_keys = ('partition-name', 'adapter-id', 'device-number', 'wwpn')\n        for wwpn_item in result['wwpn-list']:\n            dict_values = wwpn_item.split(',')\n            wwpn_list.append(dict(zip(dict_keys, dict_values)))\n        return wwpn_list", "response": "This method returns the WWPNs of the host ports of the HBA of the FCP Adapter."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of free crypto domains for the specified crypto adapter and cell.", "response": "def get_free_crypto_domains(self, crypto_adapters=None):\n        \"\"\"\n        Return a list of crypto domains that are free for usage on a list of\n        crypto adapters in this CPC.\n\n        A crypto domain is considered free for usage if it is not assigned to\n        any defined partition of this CPC in access mode 'control-usage' on any\n        of the specified crypto adapters.\n\n        For this test, all currently defined partitions of this CPC are\n        checked, regardless of whether or not they are active. This ensures\n        that a crypto domain that is found to be free for usage can be assigned\n        to a partition for 'control-usage' access to the specified crypto\n        adapters, without causing a crypto domain conflict when activating that\n        partition.\n\n        Note that a similar notion of free domains does not exist for access\n        mode 'control', because a crypto domain on a crypto adapter can be\n        in control access by multiple active partitions.\n\n        This method requires the CPC to be in DPM mode.\n\n        **Example:**\n\n            .. code-block:: text\n\n                           crypto domains\n               adapters     0   1   2   3\n                          +---+---+---+---+\n                 c1       |A,c|a,c|   | C |\n                          +---+---+---+---+\n                 c2       |b,c|B,c| B | C |\n                          +---+---+---+---+\n\n            In this example, the CPC has two crypto adapters c1 and c2. For\n            simplicity of the example, we assume these crypto adapters support\n            only 4 crypto domains.\n\n            Partition A uses only adapter c1 and has domain 0 in\n            'control-usage' access (indicated by an upper case letter 'A' in\n            the corresponding cell) and has domain 1 in 'control' access\n            (indicated by a lower case letter 'a' in the corresponding cell).\n\n            Partition B uses only adapter c2 and has domain 0 in 'control'\n            access and domains 1 and 2 in 'control-usage' access.\n\n            Partition C uses both adapters, and has domains 0 and 1 in\n            'control' access and domain 3 in 'control-usage' access.\n\n            The domains free for usage in this example are shown in the\n            following table, for each combination of crypto adapters to be\n            investigated:\n\n            ===============  ======================\n            crypto_adapters  domains free for usage\n            ===============  ======================\n            c1               1, 2\n            c2               0\n            c1, c2           (empty list)\n            ===============  ======================\n\n        **Experimental:** This method has been added in v0.14.0 and is\n        currently considered experimental. Its interface may change\n        incompatibly. Once the interface remains stable, this experimental\n        marker will be removed.\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Object-access permission to all of its Partitions.\n        * Object-access permission to all of its crypto Adapters.\n\n        Parameters:\n\n          crypto_adapters (:term:`iterable` of :class:`~zhmcclient.Adapter`):\n            The crypto :term:`Adapters <Adapter>` to be investigated.\n\n            `None` means to investigate all crypto adapters of this CPC.\n\n        Returns:\n\n          A sorted list of domain index numbers (integers) of the crypto\n          domains that are free for usage on the specified crypto adapters.\n\n          Returns `None`, if ``crypto_adapters`` was an empty list or if\n          ``crypto_adapters`` was `None` and the CPC has no crypto adapters.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        if crypto_adapters is None:\n            crypto_adapters = self.adapters.findall(type='crypto')\n\n        if not crypto_adapters:\n            # No crypto adapters were specified or defaulted.\n            return None\n\n        # We determine the maximum number of crypto domains independently\n        # of the partitions, because (1) it is possible that no partition\n        # has a crypto configuration and (2) further down we want the inner\n        # loop to be on the crypto adapters because accessing them multiple\n        # times does not drive additional HMC operations.\n        max_domains = None  # maximum number of domains across all adapters\n        for ca in crypto_adapters:\n            if max_domains is None:\n                max_domains = ca.maximum_crypto_domains\n            else:\n                max_domains = min(ca.maximum_crypto_domains, max_domains)\n\n        used_domains = set()  # Crypto domains used in control-usage mode\n        partitions = self.partitions.list(full_properties=True)\n        for partition in partitions:\n            crypto_config = partition.get_property('crypto-configuration')\n            if crypto_config:\n                adapter_uris = crypto_config['crypto-adapter-uris']\n                domain_configs = crypto_config['crypto-domain-configurations']\n                for ca in crypto_adapters:\n                    if ca.uri in adapter_uris:\n                        used_adapter_domains = list()\n                        for dc in domain_configs:\n                            if dc['access-mode'] == 'control-usage':\n                                used_adapter_domains.append(dc['domain-index'])\n                        used_domains.update(used_adapter_domains)\n\n        all_domains = set(range(0, max_domains))\n        free_domains = all_domains - used_domains\n        return sorted(list(free_domains))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the power save setting of this CPC. The current power save setting in effect for a CPC is described in the \"cpc-power-saving\" property of the CPC. This method performs the HMC operation \"Set CPC Power Save\". It requires that the feature \"Automate/advanced management suite\" (FC 0020) is installed and enabled, and fails otherwise. This method will also fail if the CPC is under group control. Whether a CPC currently allows this method is described in the \"cpc-power-save-allowed\" property of the CPC. Authorization requirements: * Object-access permission to this CPC. * Task permission for the \"Power Save\" task. Parameters: power_saving (:term:`string`): The new power save setting, with the possible values: * \"high-performance\" - The power consumption and performance of the CPC are not reduced. This is the default setting. * \"low-power\" - Low power consumption for all components of the CPC enabled for power saving. * \"custom\" - Components may have their own settings changed individually. No component settings are actually changed when this mode is entered. wait_for_completion (bool): Boolean controlling whether this method should wait for completion of the requested asynchronous HMC operation, as follows: * If `True`, this method will wait for completion of the asynchronous job performing the operation. * If `False`, this method will return immediately once the HMC has accepted the request to perform the operation. operation_timeout (:term:`number`): Timeout in seconds, for waiting for completion of the asynchronous job performing the operation. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.OperationTimeout` is raised. Returns: `None` or :class:`~zhmcclient.Job`: If `wait_for_completion` is `True`, returns `None`. If `wait_for_completion` is `False`, returns a :class:`~zhmcclient.Job` object representing the asynchronously executing job on the HMC. Raises: :exc:`~zhmcclient.HTTPError`: See the HTTP status and reason codes of operation \"Set CPC Power Save\" in the :term:`HMC API` book. :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.OperationTimeout`: The timeout expired while waiting for completion of the operation.", "response": "def set_power_save(self, power_saving, wait_for_completion=True,\n                       operation_timeout=None):\n        \"\"\"\n        Set the power save setting of this CPC.\n\n        The current power save setting in effect for a CPC is described in the\n        \"cpc-power-saving\" property of the CPC.\n\n        This method performs the HMC operation \"Set CPC Power Save\". It\n        requires that the feature \"Automate/advanced management suite\"\n        (FC 0020) is installed and enabled, and fails otherwise.\n\n        This method will also fail if the CPC is under group control.\n\n        Whether a CPC currently allows this method is described in the\n        \"cpc-power-save-allowed\" property of the CPC.\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Task permission for the \"Power Save\" task.\n\n        Parameters:\n\n          power_saving (:term:`string`):\n            The new power save setting, with the possible values:\n\n            * \"high-performance\" - The power consumption and performance of\n              the CPC are not reduced. This is the default setting.\n            * \"low-power\" - Low power consumption for all components of the\n              CPC enabled for power saving.\n            * \"custom\" - Components may have their own settings changed\n              individually. No component settings are actually changed when\n              this mode is entered.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`: See the HTTP status and reason codes of\n            operation \"Set CPC Power Save\" in the :term:`HMC API` book.\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n        \"\"\"\n        body = {'power-saving': power_saving}\n        result = self.manager.session.post(\n            self.uri + '/operations/set-cpc-power-save',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            # The HMC API book does not document what the result data of the\n            # completed job is. It turns out that the completed job has this\n            # dictionary as its result data:\n            #    {'message': 'Operation executed successfully'}\n            # We transform that to None.\n            return None\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_power_capping(self, power_capping_state, power_cap=None,\n                          wait_for_completion=True, operation_timeout=None):\n        \"\"\"\n        Set the power capping settings of this CPC. The power capping settings\n        of a CPC define whether or not the power consumption of the CPC is\n        limited and if so, what the limit is. Use this method to limit the\n        peak power consumption of a CPC, or to remove a power consumption\n        limit for a CPC.\n\n        The current power capping settings in effect for a CPC are described in\n        the \"cpc-power-capping-state\" and \"cpc-power-cap-current\" properties of\n        the CPC.\n\n        This method performs the HMC operation \"Set CPC Power Capping\". It\n        requires that the feature \"Automate/advanced management suite\"\n        (FC 0020) is installed and enabled, and fails otherwise.\n\n        This method will also fail if the CPC is under group control.\n\n        Whether a CPC currently allows this method is described in the\n        \"cpc-power-cap-allowed\" property of the CPC.\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Task permission for the \"Power Capping\" task.\n\n        Parameters:\n\n          power_capping_state (:term:`string`):\n            The power capping state to be set, with the possible values:\n\n            * \"disabled\" - The power cap of the CPC is not set and the peak\n              power consumption is not limited. This is the default setting.\n            * \"enabled\" - The peak power consumption of the CPC is limited to\n              the specified power cap value.\n            * \"custom\" - Individually configure the components for power\n              capping. No component settings are actually changed when this\n              mode is entered.\n\n          power_cap (:term:`integer`):\n            The power cap value to be set, as a power consumption in Watt. This\n            parameter is required not to be `None` if\n            `power_capping_state=\"enabled\"`.\n\n            The specified value must be between the values of the CPC\n            properties \"cpc-power-cap-minimum\" and \"cpc-power-cap-maximum\".\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`: See the HTTP status and reason codes of\n            operation \"Set CPC Power Save\" in the :term:`HMC API` book.\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n        \"\"\"\n        body = {'power-capping-state': power_capping_state}\n        if power_cap is not None:\n            body['power-cap-current'] = power_cap\n        result = self.manager.session.post(\n            self.uri + '/operations/set-cpc-power-capping',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            # The HMC API book does not document what the result data of the\n            # completed job is. Just in case there is similar behavior to the\n            # \"Set CPC Power Save\" operation, we transform that to None.\n            # TODO: Verify job result of a completed \"Set CPC Power Capping\".\n            return None\n        return result", "response": "Set the power capping settings of a specific CPC."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_energy_management_properties(self):\n        result = self.manager.session.get(self.uri + '/energy-management-data')\n        em_list = result['objects']\n        if len(em_list) != 1:\n            uris = [em_obj['object-uri'] for em_obj in em_list]\n            raise ParseError(\"Energy management data returned for no resource \"\n                             \"or for more than one resource: %r\" % uris)\n        em_cpc_obj = em_list[0]\n        if em_cpc_obj['object-uri'] != self.uri:\n            raise ParseError(\"Energy management data returned for an \"\n                             \"unexpected resource: %r\" %\n                             em_cpc_obj['object-uri'])\n        if em_cpc_obj['error-occurred']:\n            raise ParseError(\"Errors occurred when retrieving energy \"\n                             \"management data for CPC. Operation result: %r\" %\n                             result)\n        cpc_props = em_cpc_obj['properties']\n        return cpc_props", "response": "Get the energy management properties of the CPC."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_associated_storage_groups(\n            self, full_properties=False, filter_args=None):\n        \"\"\"\n        Return the :term:`storage groups <storage group>` that are associated\n        to this CPC.\n\n        If the CPC does not support the \"dpm-storage-management\" feature, or\n        does not have it enabled, an empty list is returned.\n\n        Storage groups for which the authenticated user does not have\n        object-access permission are not included.\n\n        Authorization requirements:\n\n        * Object-access permission to any storage groups to be included in the\n          result.\n\n        Parameters:\n\n          full_properties (bool):\n            Controls that the full set of resource properties for each returned\n            storage group is being retrieved, vs. only the following short set:\n            \"object-uri\", \"cpc-uri\", \"name\", \"fulfillment-state\", and\n            \"type\".\n\n          filter_args (dict):\n            Filter arguments that narrow the list of returned resources to\n            those that match the specified filter arguments. For details, see\n            :ref:`Filtering`.\n\n            `None` causes no filtering to happen.\n\n            The 'cpc-uri' property is automatically added to the filter\n            arguments and must not be specified in this parameter.\n\n        Returns:\n\n          : A list of :class:`~zhmcclient.StorageGroup` objects.\n\n        Raises:\n\n          ValueError: The filter_args parameter specifies the 'cpc-uri'\n            property\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n\n        if filter_args is None:\n            filter_args = {}\n        else:\n            filter_args = filter_args.copy()\n        if 'cpc-uri' in filter_args:\n            raise ValueError(\n                \"The filter_args parameter specifies the 'cpc-uri' property \"\n                \"with value: %s\" % filter_args['cpc-uri'])\n        filter_args['cpc-uri'] = self.uri\n\n        sg_list = self.manager.console.storage_groups.list(\n            full_properties, filter_args)\n\n        return sg_list", "response": "Returns the list of storage groups that are associated with this CPC."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate_lun_path(self, host_wwpn, host_port, wwpn, lun):\n\n        # The operation requires exactly 16 characters in lower case\n        host_wwpn_16 = format(int(host_wwpn, 16), '016x')\n        wwpn_16 = format(int(wwpn, 16), '016x')\n        lun_16 = format(int(lun, 16), '016x')\n\n        body = {\n            'host-world-wide-port-name': host_wwpn_16,\n            'adapter-port-uri': host_port.uri,\n            'target-world-wide-port-name': wwpn_16,\n            'logical-unit-number': lun_16,\n        }\n        self.manager.session.post(\n            self.uri + '/operations/validate-lun-path',\n            body=body)", "response": "This method checks if an FCP storage volume is reachable from this CPC through a specified host port and WWPN and LUN."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reorder(self, user_patterns):\n        body = {\n            'user-pattern-uris': [up.uri for up in user_patterns]\n        }\n        self.manager.session.post(\n            '/api/console/operations/reorder-user-patterns',\n            body=body)", "response": "Reorder the User Patterns of the HMC."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresetting the time statistics data for the current operation.", "response": "def reset(self):\n        \"\"\"\n        Reset the time statistics data for the operation.\n        \"\"\"\n        self._count = 0\n        self._sum = float(0)\n        self._min = float('inf')\n        self._max = float(0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef end(self):\n        if self.keeper.enabled:\n            if self._begin_time is None:\n                raise RuntimeError(\"end() called without preceding begin()\")\n            dt = time.time() - self._begin_time\n            self._begin_time = None\n            self._count += 1\n            self._sum += dt\n            if dt > self._max:\n                self._max = dt\n            if dt < self._min:\n                self._min = dt", "response": "This method is called after the operation returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the time statistics object for a name.", "response": "def get_stats(self, name):\n        \"\"\"\n        Get the time statistics for a name.\n        If a time statistics for that name does not exist yet, create one.\n\n        Parameters:\n\n          name (string):\n            Name of the time statistics.\n\n        Returns:\n\n          TimeStats: The time statistics for the specified name. If the\n          statistics keeper is disabled, a dummy time statistics object is\n          returned, in order to save resources.\n        \"\"\"\n        if not self.enabled:\n            return self._disabled_stats\n        if name not in self._time_stats:\n            self._time_stats[name] = TimeStats(self, name)\n        return self._time_stats[name]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of unmanaged CPCs exposed by this HMC.", "response": "def list(self, full_properties=False, filter_args=None):\n        \"\"\"\n        List the unmanaged CPCs exposed by the HMC this client is connected to.\n\n        Because the CPCs are unmanaged, the returned\n        :class:`~zhmcclient.UnmanagedCpc` objects cannot perform any operations\n        and will have only the following properties:\n\n        * ``object-uri``\n        * ``name``\n\n        Authorization requirements:\n\n        * None\n\n        Parameters:\n\n          full_properties (bool):\n            Ignored (exists for consistency with other list() methods).\n\n          filter_args (dict):\n            Filter arguments that narrow the list of returned resources to\n            those that match the specified filter arguments. For details, see\n            :ref:`Filtering`.\n\n            `None` causes no filtering to happen, i.e. all resources are\n            returned.\n\n        Returns:\n\n          : A list of :class:`~zhmcclient.UnmanagedCpc` objects.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        resource_obj_list = []\n        resource_obj = self._try_optimized_lookup(filter_args)\n        if resource_obj:\n            resource_obj_list.append(resource_obj)\n        else:\n            query_parms, client_filters = self._divide_filter_args(filter_args)\n\n            uri = self.parent.uri + '/operations/list-unmanaged-cpcs' + \\\n                query_parms\n\n            result = self.session.get(uri)\n            if result:\n                props_list = result['cpcs']\n                for props in props_list:\n\n                    resource_obj = self.resource_class(\n                        manager=self,\n                        uri=props[self._uri_prop],\n                        name=props.get(self._name_prop, None),\n                        properties=props)\n\n                    if self._matches_filters(resource_obj, client_filters):\n                        resource_obj_list.append(resource_obj)\n\n        self._name_uri_cache.update_from(resource_obj_list)\n        return resource_obj_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the specified query parms string and returns a dictionary of query parameters.", "response": "def parse_query_parms(method, uri, query_str):\n    \"\"\"\n    Parse the specified query parms string and return a dictionary of query\n    parameters. The key of each dict item is the query parameter name, and the\n    value of each dict item is the query parameter value. If a query parameter\n    shows up more than once, the resulting dict item value is a list of all\n    those values.\n\n    query_str is the query string from the URL, everything after the '?'. If\n    it is empty or None, None is returned.\n\n    If a query parameter is not of the format \"name=value\", an HTTPError 400,1\n    is raised.\n    \"\"\"\n    if not query_str:\n        return None\n    query_parms = {}\n    for query_item in query_str.split('&'):\n        # Example for these items: 'name=a%20b'\n        if query_item == '':\n            continue\n        items = query_item.split('=')\n        if len(items) != 2:\n            raise BadRequestError(\n                method, uri, reason=1,\n                message=\"Invalid format for URI query parameter: {!r} \"\n                \"(valid format is: 'name=value').\".\n                format(query_item))\n        name = unquote(items[0])\n        value = unquote(items[1])\n        if name in query_parms:\n            existing_value = query_parms[name]\n            if not isinstance(existing_value, list):\n                query_parms[name] = list()\n                query_parms[name].append(existing_value)\n            query_parms[name].append(value)\n        else:\n            query_parms[name] = value\n    return query_parms"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_required_fields(method, uri, body, field_names):\n\n    # Check presence of request body\n    if body is None:\n        raise BadRequestError(method, uri, reason=3,\n                              message=\"Missing request body\")\n\n    # Check required input fields\n    for field_name in field_names:\n        if field_name not in body:\n            raise BadRequestError(method, uri, reason=5,\n                                  message=\"Missing required field in request \"\n                                  \"body: {}\".format(field_name))", "response": "Checks that the request body contains the required fields."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_valid_cpc_status(method, uri, cpc):\n    status = cpc.properties.get('status', None)\n    if status is None:\n        # Do nothing if no status is set on the faked CPC\n        return\n    valid_statuses = ['active', 'service-required', 'degraded', 'exceptions']\n    if status not in valid_statuses:\n        if uri.startswith(cpc.uri):\n            # The uri targets the CPC (either is the CPC uri or some\n            # multiplicity under the CPC uri)\n            raise ConflictError(method, uri, reason=1,\n                                message=\"The operation cannot be performed \"\n                                \"because the targeted CPC {} has a status \"\n                                \"that is not valid for the operation: {}\".\n                                format(cpc.name, status))\n        else:\n            # The uri targets a resource hosted by the CPC\n            raise ConflictError(method, uri, reason=6,\n                                message=\"The operation cannot be performed \"\n                                \"because CPC {} hosting the targeted resource \"\n                                \"has a status that is not valid for the \"\n                                \"operation: {}\".\n                                format(cpc.name, status))", "response": "Checks that the CPC is in a valid status."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking that the partition status is valid and that the resource hosted by the partition is in one of the valid statuses.", "response": "def check_partition_status(method, uri, partition, valid_statuses=None,\n                           invalid_statuses=None):\n    \"\"\"\n    Check that the partition is in one of the valid statuses (if specified)\n    and not in one of the invalid statuses (if specified), as indicated by its\n    'status' property.\n\n    If the Partition object does not have a 'status' property set, this\n    function does nothing (in order to make the mock support easy to use).\n\n    Raises:\n      ConflictError with reason 1 (reason 6 is not used for partitions).\n    \"\"\"\n    status = partition.properties.get('status', None)\n    if status is None:\n        # Do nothing if no status is set on the faked partition\n        return\n    if valid_statuses and status not in valid_statuses or \\\n            invalid_statuses and status in invalid_statuses:\n        if uri.startswith(partition.uri):\n            # The uri targets the partition (either is the partition uri or\n            # some multiplicity under the partition uri)\n            raise ConflictError(method, uri, reason=1,\n                                message=\"The operation cannot be performed \"\n                                \"because the targeted partition {} has a \"\n                                \"status that is not valid for the operation: \"\n                                \"{}\".\n                                format(partition.name, status))\n        else:\n            # The uri targets a resource hosted by the partition\n            raise ConflictError(method, uri,\n                                reason=1,  # Note: 6 not used for partitions\n                                message=\"The operation cannot be performed \"\n                                \"because partition {} hosting the targeted \"\n                                \"resource has a status that is not valid for \"\n                                \"the operation: {}\".\n                                format(partition.name, status))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that the crypto - configuration property on the faked partition is initialized.", "response": "def ensure_crypto_config(partition):\n    \"\"\"\n    Ensure that the 'crypto-configuration' property on the faked partition\n    is initialized.\n    \"\"\"\n\n    if 'crypto-configuration' not in partition.properties or \\\n            partition.properties['crypto-configuration'] is None:\n        partition.properties['crypto-configuration'] = {}\n    crypto_config = partition.properties['crypto-configuration']\n\n    if 'crypto-adapter-uris' not in crypto_config or \\\n            crypto_config['crypto-adapter-uris'] is None:\n        crypto_config['crypto-adapter-uris'] = []\n    adapter_uris = crypto_config['crypto-adapter-uris']\n\n    if 'crypto-domain-configurations' not in crypto_config or \\\n            crypto_config['crypto-domain-configurations'] is None:\n        crypto_config['crypto-domain-configurations'] = []\n    domain_configs = crypto_config['crypto-domain-configurations']\n\n    return adapter_uris, domain_configs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Update <resource> Properties.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        try:\n            resource = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        resource.update(body)", "response": "Operation : Update <resource > Properties."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(method, hmc, uri, uri_parms, logon_required):\n        try:\n            resource = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        resource.manager.remove(resource.oid)", "response": "Operation: Delete <resource >."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Get Console Audit Log.\"\"\"\n        assert wait_for_completion is True  # synchronous operation\n        console_uri = '/api/console'\n        try:\n            hmc.lookup_by_uri(console_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        resp = []\n        # TODO: Add the ability to return audit log entries in mock support.\n        return resp", "response": "Operation: Get Console Audit Log."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(method, hmc, uri, uri_parms, logon_required):\n        query_str = uri_parms[0]\n        try:\n            console = hmc.consoles.lookup_by_oid(None)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        result_ucpcs = []\n        filter_args = parse_query_parms(method, uri, query_str)\n        for ucpc in console.unmanaged_cpcs.list(filter_args):\n            result_ucpc = {}\n            for prop in ucpc.properties:\n                if prop in ('object-uri', 'name'):\n                    result_ucpc[prop] = ucpc.properties[prop]\n            result_ucpcs.append(result_ucpc)\n        return {'cpcs': result_ucpcs}", "response": "Operation: List Unmanaged CPCs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Add User Role to User.\"\"\"\n        assert wait_for_completion is True  # synchronous operation\n        user_oid = uri_parms[0]\n        user_uri = '/api/users/' + user_oid\n        try:\n            user = hmc.lookup_by_uri(user_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        check_required_fields(method, uri, body, ['user-role-uri'])\n        user_type = user.properties['type']\n        if user_type in ('pattern-based', 'system-defined'):\n            raise BadRequestError(\n                method, uri, reason=314,\n                message=\"Cannot add user role to user of type {}: {}\".\n                format(user_type, user_uri))\n        user_role_uri = body['user-role-uri']\n        try:\n            hmc.lookup_by_uri(user_role_uri)\n        except KeyError:\n            raise InvalidResourceError(method, user_role_uri, reason=2)\n        if user.properties.get('user-roles', None) is None:\n            user.properties['user-roles'] = []\n        user.properties['user-roles'].append(user_role_uri)", "response": "Operation: Add User Role to User."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(method, hmc, uri, uri_parms, logon_required):\n        query_str = uri_parms[0]\n        try:\n            console = hmc.consoles.lookup_by_oid(None)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        result_user_roles = []\n        filter_args = parse_query_parms(method, uri, query_str)\n        for user_role in console.user_roles.list(filter_args):\n            result_user_role = {}\n            for prop in user_role.properties:\n                if prop in ('object-uri', 'name', 'type'):\n                    result_user_role[prop] = user_role.properties[prop]\n            result_user_roles.append(result_user_role)\n        return {'user-roles': result_user_roles}", "response": "Operation: List User Roles."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create User Role.\"\"\"\n        assert wait_for_completion is True  # synchronous operation\n        try:\n            console = hmc.consoles.lookup_by_oid(None)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        check_required_fields(method, uri, body, ['name'])\n        properties = copy.deepcopy(body)\n        if 'type' in properties:\n            raise BadRequestError(\n                method, uri, reason=6,\n                message=\"Type specified when creating a user role: {!r}\".\n                format(properties['type']))\n        properties['type'] = 'user-defined'\n        new_user_role = console.user_roles.add(properties)\n        return {'object-uri': new_user_role.uri}", "response": "Operation: Create User Role."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(method, hmc, uri, uri_parms, logon_required):\n        query_str = uri_parms[0]\n        result_cpcs = []\n        filter_args = parse_query_parms(method, uri, query_str)\n        for cpc in hmc.cpcs.list(filter_args):\n            result_cpc = {}\n            for prop in cpc.properties:\n                if prop in ('object-uri', 'name', 'status'):\n                    result_cpc[prop] = cpc.properties[prop]\n            result_cpcs.append(result_cpc)\n        return {'cpcs': result_cpcs}", "response": "Operation : List CPCs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Set CPC Power Capping (any CPC mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        cpc_oid = uri_parms[0]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        check_required_fields(method, uri, body, ['power-capping-state'])\n\n        power_capping_state = body['power-capping-state']\n        power_cap_current = body.get('power-cap-current', None)\n\n        if power_capping_state not in ['disabled', 'enabled', 'custom']:\n            raise BadRequestError(method, uri, reason=7,\n                                  message=\"Invalid power-capping-state value: \"\n                                  \"%r\" % power_capping_state)\n\n        if power_capping_state == 'enabled' and power_cap_current is None:\n            raise BadRequestError(method, uri, reason=7,\n                                  message=\"Power-cap-current must be provided \"\n                                  \"when enabling power capping\")\n\n        cpc.properties['cpc-power-capping-state'] = power_capping_state\n        cpc.properties['cpc-power-cap-current'] = power_cap_current\n        cpc.properties['zcpc-power-capping-state'] = power_capping_state\n        cpc.properties['zcpc-power-cap-current'] = power_cap_current", "response": "Set CPC Power Capping."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(method, hmc, uri, uri_parms, logon_required):\n        cpc_oid = uri_parms[0]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        energy_props = {\n            'cpc-power-cap-allowed':\n                cpc.properties.get('cpc-power-cap-allowed'),\n            'cpc-power-cap-current':\n                cpc.properties.get('cpc-power-cap-current'),\n            'cpc-power-cap-maximum':\n                cpc.properties.get('cpc-power-cap-maximum'),\n            'cpc-power-cap-minimum':\n                cpc.properties.get('cpc-power-cap-minimum'),\n            'cpc-power-capping-state':\n                cpc.properties.get('cpc-power-capping-state'),\n            'cpc-power-consumption':\n                cpc.properties.get('cpc-power-consumption'),\n            'cpc-power-rating':\n                cpc.properties.get('cpc-power-rating'),\n            'cpc-power-save-allowed':\n                cpc.properties.get('cpc-power-save-allowed'),\n            'cpc-power-saving':\n                cpc.properties.get('cpc-power-saving'),\n            'cpc-power-saving-state':\n                cpc.properties.get('cpc-power-saving-state'),\n            'zcpc-ambient-temperature':\n                cpc.properties.get('zcpc-ambient-temperature'),\n            'zcpc-dew-point':\n                cpc.properties.get('zcpc-dew-point'),\n            'zcpc-exhaust-temperature':\n                cpc.properties.get('zcpc-exhaust-temperature'),\n            'zcpc-heat-load':\n                cpc.properties.get('zcpc-heat-load'),\n            'zcpc-heat-load-forced-air':\n                cpc.properties.get('zcpc-heat-load-forced-air'),\n            'zcpc-heat-load-water':\n                cpc.properties.get('zcpc-heat-load-water'),\n            'zcpc-humidity':\n                cpc.properties.get('zcpc-humidity'),\n            'zcpc-maximum-potential-heat-load':\n                cpc.properties.get('zcpc-maximum-potential-heat-load'),\n            'zcpc-maximum-potential-power':\n                cpc.properties.get('zcpc-maximum-potential-power'),\n            'zcpc-power-cap-allowed':\n                cpc.properties.get('zcpc-power-cap-allowed'),\n            'zcpc-power-cap-current':\n                cpc.properties.get('zcpc-power-cap-current'),\n            'zcpc-power-cap-maximum':\n                cpc.properties.get('zcpc-power-cap-maximum'),\n            'zcpc-power-cap-minimum':\n                cpc.properties.get('zcpc-power-cap-minimum'),\n            'zcpc-power-capping-state':\n                cpc.properties.get('zcpc-power-capping-state'),\n            'zcpc-power-consumption':\n                cpc.properties.get('zcpc-power-consumption'),\n            'zcpc-power-rating':\n                cpc.properties.get('zcpc-power-rating'),\n            'zcpc-power-save-allowed':\n                cpc.properties.get('zcpc-power-save-allowed'),\n            'zcpc-power-saving':\n                cpc.properties.get('zcpc-power-saving'),\n            'zcpc-power-saving-state':\n                cpc.properties.get('zcpc-power-saving-state'),\n        }\n        cpc_data = {\n            'error-occurred': False,\n            'object-uri': cpc.uri,\n            'object-id': cpc.oid,\n            'class': 'cpcs',\n            'properties': energy_props,\n        }\n        result = {'objects': [cpc_data]}\n\n        return result", "response": "Operation: Get CPC Energy Management Data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Stop CPC (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        cpc_oid = uri_parms[0]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        if not cpc.dpm_enabled:\n            raise CpcNotInDpmError(method, uri, cpc)\n        cpc.properties['status'] = 'not-operating'", "response": "Operation: Stop CPC (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Export WWPN List (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # this operation is always synchr.\n        cpc_oid = uri_parms[0]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        if not cpc.dpm_enabled:\n            raise CpcNotInDpmError(method, uri, cpc)\n        check_required_fields(method, uri, body, ['partitions'])\n        partition_uris = body['partitions']\n        if len(partition_uris) == 0:\n            raise BadRequestError(\n                method, uri, reason=149,\n                message=\"'partitions' field in request body is empty.\")\n\n        wwpn_list = []\n        for partition_uri in partition_uris:\n            partition = hmc.lookup_by_uri(partition_uri)\n            partition_cpc = partition.manager.parent\n            if partition_cpc.oid != cpc_oid:\n                raise BadRequestError(\n                    method, uri, reason=149,\n                    message=\"Partition %r specified in 'partitions' field \"\n                    \"is not in the targeted CPC with ID %r (but in the CPC \"\n                    \"with ID %r).\" %\n                    (partition.uri, cpc_oid, partition_cpc.oid))\n            partition_name = partition.properties.get('name', '')\n            for hba in partition.hbas.list():\n                port_uri = hba.properties['adapter-port-uri']\n                port = hmc.lookup_by_uri(port_uri)\n                adapter = port.manager.parent\n                adapter_id = adapter.properties.get('adapter-id', '')\n                devno = hba.properties.get('device-number', '')\n                wwpn = hba.properties.get('wwpn', '')\n                wwpn_str = '%s,%s,%s,%s' % (partition_name, adapter_id,\n                                            devno, wwpn)\n                wwpn_list.append(wwpn_str)\n        return {\n            'wwpn-list': wwpn_list\n        }", "response": "Operation: Export WWPN List (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create Metrics Context.\"\"\"\n        assert wait_for_completion is True  # always synchronous\n        check_required_fields(method, uri, body,\n                              ['anticipated-frequency-seconds'])\n        new_metrics_context = hmc.metrics_contexts.add(body)\n        result = {\n            'metrics-context-uri': new_metrics_context.uri,\n            'metric-group-infos': new_metrics_context.get_metric_group_infos()\n        }\n        return result", "response": "Operation: Create Metrics Context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(method, hmc, uri, uri_parms, logon_required):\n        try:\n            metrics_context = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        hmc.metrics_contexts.remove(metrics_context.oid)", "response": "Operation: Delete Metrics Context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Change Crypto Type (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # HMC operation is synchronous\n        adapter_uri = uri.split('/operations/')[0]\n        try:\n            adapter = hmc.lookup_by_uri(adapter_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = adapter.manager.parent\n        assert cpc.dpm_enabled\n        check_required_fields(method, uri, body, ['crypto-type'])\n\n        # Check the validity of the new crypto_type\n        crypto_type = body['crypto-type']\n        if crypto_type not in ['accelerator', 'cca-coprocessor',\n                               'ep11-coprocessor']:\n            raise BadRequestError(\n                method, uri, reason=8,\n                message=\"Invalid value for 'crypto-type' field: %s\" %\n                crypto_type)\n\n        # Reflect the result of changing the crypto type\n        adapter.properties['crypto-type'] = crypto_type", "response": "Operation: Change Crypto Type of a resource in HMC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Change Adapter Type (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # HMC operation is synchronous\n        adapter_uri = uri.split('/operations/')[0]\n        try:\n            adapter = hmc.lookup_by_uri(adapter_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = adapter.manager.parent\n        assert cpc.dpm_enabled\n        check_required_fields(method, uri, body, ['type'])\n\n        new_adapter_type = body['type']\n\n        # Check the validity of the adapter family\n        adapter_family = adapter.properties.get('adapter-family', None)\n        if adapter_family != 'ficon':\n            raise BadRequestError(\n                method, uri, reason=18,\n                message=\"The adapter type cannot be changed for adapter \"\n                        \"family: %s\" % adapter_family)\n\n        # Check the adapter status\n        adapter_status = adapter.properties.get('status', None)\n        if adapter_status == 'exceptions':\n            raise BadRequestError(\n                method, uri, reason=18,\n                message=\"The adapter type cannot be changed for adapter \"\n                        \"status: %s\" % adapter_status)\n\n        # Check the validity of the new adapter type\n        if new_adapter_type not in ['fc', 'fcp', 'not-configured']:\n            raise BadRequestError(\n                method, uri, reason=8,\n                message=\"Invalid new value for 'type' field: %s\" %\n                        new_adapter_type)\n\n        # Check that the new adapter type is not already set\n        adapter_type = adapter.properties.get('type', None)\n        if new_adapter_type == adapter_type:\n            raise BadRequestError(\n                method, uri, reason=8,\n                message=\"New value for 'type' field is already set: %s\" %\n                        new_adapter_type)\n\n        # TODO: Reject if adapter is attached to a partition.\n\n        # Reflect the result of changing the adapter type\n        adapter.properties['type'] = new_adapter_type", "response": "Operation: Change Adapter Type (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(method, hmc, uri, uri_parms, logon_required):\n        cpc_oid = uri_parms[0]\n        query_str = uri_parms[1]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        # Reflect the result of listing the partition\n        result_partitions = []\n        if cpc.dpm_enabled:\n            filter_args = parse_query_parms(method, uri, query_str)\n            for partition in cpc.partitions.list(filter_args):\n                result_partition = {}\n                for prop in partition.properties:\n                    if prop in ('object-uri', 'name', 'status'):\n                        result_partition[prop] = partition.properties[prop]\n                result_partitions.append(result_partition)\n        return {'partitions': result_partitions}", "response": "Operation: List Partitions of a CPC."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create Partition (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        cpc_oid = uri_parms[0]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        if not cpc.dpm_enabled:\n            raise CpcNotInDpmError(method, uri, cpc)\n        check_valid_cpc_status(method, uri, cpc)\n        check_required_fields(method, uri, body,\n                              ['name', 'initial-memory', 'maximum-memory'])\n        # TODO: There are some more input properties that are required under\n        # certain conditions.\n\n        # Reflect the result of creating the partition\n        new_partition = cpc.partitions.add(body)\n        return {'object-uri': new_partition.uri}", "response": "Operation: Create Partition (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Mount ISO Image (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # synchronous operation\n        partition_oid = uri_parms[0]\n        partition_uri = '/api/partitions/' + partition_oid\n        try:\n            partition = hmc.lookup_by_uri(partition_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n\n        # Parse and check required query parameters\n        query_parms = parse_query_parms(method, uri, uri_parms[1])\n        try:\n            image_name = query_parms['image-name']\n        except KeyError:\n            raise BadRequestError(\n                method, uri, reason=1,\n                message=\"Missing required URI query parameter 'image-name'\")\n        try:\n            ins_file_name = query_parms['ins-file-name']\n        except KeyError:\n            raise BadRequestError(\n                method, uri, reason=1,\n                message=\"Missing required URI query parameter 'ins-file-name'\")\n\n        # Reflect the effect of mounting in the partition properties\n        partition.properties['boot-iso-image-name'] = image_name\n        partition.properties['boot-iso-ins-file'] = ins_file_name\n        return {}", "response": "Operation: Mount ISO Image (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create HBA (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        partition_uri = re.sub('/hbas$', '', uri)\n        try:\n            partition = hmc.lookup_by_uri(partition_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n        check_required_fields(method, uri, body, ['name', 'adapter-port-uri'])\n\n        # Check the port-related input property\n        port_uri = body['adapter-port-uri']\n        m = re.match(r'(^/api/adapters/[^/]+)/storage-ports/[^/]+$', port_uri)\n        if not m:\n            # We treat an invalid port URI like \"port not found\".\n            raise InvalidResourceError(method, uri, reason=6,\n                                       resource_uri=port_uri)\n        adapter_uri = m.group(1)\n        try:\n            hmc.lookup_by_uri(adapter_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri, reason=2,\n                                       resource_uri=adapter_uri)\n        try:\n            hmc.lookup_by_uri(port_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri, reason=6,\n                                       resource_uri=port_uri)\n\n        new_hba = partition.hbas.add(body)\n\n        return {'element-uri': new_hba.uri}", "response": "Create a new HBA."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete(method, hmc, uri, uri_parms, logon_required):\n        try:\n            hba = hmc.lookup_by_uri(uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        partition = hba.manager.parent\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n\n        partition.hbas.remove(hba.oid)", "response": "Operation: Delete HBA (requires DPM mode)."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Reassign Storage Adapter Port (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        partition_oid = uri_parms[0]\n        partition_uri = '/api/partitions/' + partition_oid\n        hba_oid = uri_parms[1]\n        hba_uri = '/api/partitions/' + partition_oid + '/hbas/' + hba_oid\n        try:\n            hba = hmc.lookup_by_uri(hba_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        partition = hmc.lookup_by_uri(partition_uri)  # assert it exists\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n        check_required_fields(method, uri, body, ['adapter-port-uri'])\n\n        # Reflect the effect of the operation on the HBA\n        new_port_uri = body['adapter-port-uri']\n        hba.properties['adapter-port-uri'] = new_port_uri", "response": "Reassign Storage Adapter Port."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create Virtual Function (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        partition_uri = re.sub('/virtual-functions$', '', uri)\n        try:\n            partition = hmc.lookup_by_uri(partition_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = partition.manager.parent\n        assert cpc.dpm_enabled\n        check_valid_cpc_status(method, uri, cpc)\n        check_partition_status(method, uri, partition,\n                               invalid_statuses=['starting', 'stopping'])\n        check_required_fields(method, uri, body, ['name'])\n\n        new_vf = partition.virtual_functions.add(body)\n        return {'element-uri': new_vf.uri}", "response": "Operation: Create Virtual Function."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Get Connected VNICs of a Virtual Switch\n        (requires DPM mode).\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        vswitch_oid = uri_parms[0]\n        vswitch_uri = '/api/virtual-switches/' + vswitch_oid\n        try:\n            vswitch = hmc.lookup_by_uri(vswitch_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        cpc = vswitch.manager.parent\n        assert cpc.dpm_enabled\n\n        connected_vnic_uris = vswitch.properties['connected-vnic-uris']\n        return {'connected-vnic-uris': connected_vnic_uris}", "response": "Operation: Get Connected VNICs of a Virtual Switch."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Create Storage Group.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        check_required_fields(method, uri, body, ['name', 'cpc-uri', 'type'])\n        cpc_uri = body['cpc-uri']\n        try:\n            cpc = hmc.lookup_by_uri(cpc_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        if not cpc.dpm_enabled:\n            raise CpcNotInDpmError(method, uri, cpc)\n        check_valid_cpc_status(method, uri, cpc)\n\n        # Reflect the result of creating the storage group\n\n        body2 = body.copy()\n        sv_requests = body2.pop('storage-volumes', None)\n        new_storage_group = hmc.consoles.console.storage_groups.add(body2)\n\n        sv_uris = []\n        if sv_requests:\n            for sv_req in sv_requests:\n                check_required_fields(method, uri, sv_req, ['operation'])\n                operation = sv_req['operation']\n                if operation == 'create':\n                    sv_props = sv_req.copy()\n                    del sv_props['operation']\n                    if 'element-uri' in sv_props:\n                        raise BadRequestError(\n                            method, uri, 7,\n                            \"The 'element-uri' field in storage-volumes is \"\n                            \"invalid for the create operation\")\n                    sv_uri = new_storage_group.storage_volumes.add(sv_props)\n                    sv_uris.append(sv_uri)\n                else:\n                    raise BadRequestError(\n                        method, uri, 5,\n                        \"Invalid value for storage-volumes 'operation' \"\n                        \"field: %s\" % operation)\n\n        return {\n            'object-uri': new_storage_group.uri,\n            'element-uris': sv_uris,\n        }", "response": "Create a new storage group."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmodify the storage group properties.", "response": "def post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Modify Storage Group Properties.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        # The URI is a POST operation, so we need to construct the SG URI\n        storage_group_oid = uri_parms[0]\n        storage_group_uri = '/api/storage-groups/' + storage_group_oid\n        try:\n            storage_group = hmc.lookup_by_uri(storage_group_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        # Reflect the result of modifying the storage group\n\n        body2 = body.copy()\n        sv_requests = body2.pop('storage-volumes', None)\n        storage_group.update(body2)\n\n        sv_uris = []\n        if sv_requests:\n            for sv_req in sv_requests:\n                check_required_fields(method, uri, sv_req, ['operation'])\n                operation = sv_req['operation']\n                if operation == 'create':\n                    sv_props = sv_req.copy()\n                    del sv_props['operation']\n                    if 'element-uri' in sv_props:\n                        raise BadRequestError(\n                            method, uri, 7,\n                            \"The 'element-uri' field in storage-volumes is \"\n                            \"invalid for the create operation\")\n                    sv_uri = storage_group.storage_volumes.add(sv_props)\n                    sv_uris.append(sv_uri)\n                elif operation == 'modify':\n                    check_required_fields(method, uri, sv_req, ['element-uri'])\n                    sv_uri = sv_req['element-uri']\n                    storage_volume = hmc.lookup_by_uri(sv_uri)\n                    storage_volume.update_properties(sv_props)\n                elif operation == 'delete':\n                    check_required_fields(method, uri, sv_req, ['element-uri'])\n                    sv_uri = sv_req['element-uri']\n                    storage_volume = hmc.lookup_by_uri(sv_uri)\n                    storage_volume.delete()\n                else:\n                    raise BadRequestError(\n                        method, uri, 5,\n                        \"Invalid value for storage-volumes 'operation' \"\n                        \"field: %s\" % operation)\n\n        return {\n            'element-uris': sv_uris,  # SVs created, maintaining the order\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Delete Storage Group.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        # The URI is a POST operation, so we need to construct the SG URI\n        storage_group_oid = uri_parms[0]\n        storage_group_uri = '/api/storage-groups/' + storage_group_oid\n        try:\n            storage_group = hmc.lookup_by_uri(storage_group_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        # TODO: Check that the SG is detached from any partitions\n\n        # Reflect the result of deleting the storage_group\n        storage_group.manager.remove(storage_group.oid)", "response": "Operation: Delete Storage Group."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Request Storage Group Fulfillment.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        # The URI is a POST operation, so we need to construct the SG URI\n        storage_group_oid = uri_parms[0]\n        storage_group_uri = '/api/storage-groups/' + storage_group_oid\n        try:\n            hmc.lookup_by_uri(storage_group_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        # Reflect the result of requesting fulfilment for the storage group\n        pass", "response": "Request the storage group filfillment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(method, hmc, uri, uri_parms, body, logon_required,\n             wait_for_completion):\n        \"\"\"Operation: Add Candidate Adapter Ports to an FCP Storage Group.\"\"\"\n        assert wait_for_completion is True  # async not supported yet\n        # The URI is a POST operation, so we need to construct the SG URI\n        storage_group_oid = uri_parms[0]\n        storage_group_uri = '/api/storage-groups/' + storage_group_oid\n        try:\n            storage_group = hmc.lookup_by_uri(storage_group_uri)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n\n        check_required_fields(method, uri, body, ['adapter-port-uris'])\n\n        # TODO: Check that storage group has type FCP\n\n        # Reflect the result of adding the candidate ports\n        candidate_adapter_port_uris = \\\n            storage_group.properties['candidate-adapter-port-uris']\n        for ap_uri in body['adapter-port-uris']:\n            if ap_uri in candidate_adapter_port_uris:\n                raise ConflictError(method, uri, 483,\n                                    \"Adapter port is already in candidate \"\n                                    \"list of storage group %s: %s\" %\n                                    (storage_group.name, ap_uri))\n            else:\n                candidate_adapter_port_uris.append(ap_uri)", "response": "Add Candidate Adapter Ports to an FCP Storage Group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(method, hmc, uri, uri_parms, logon_required):\n        cpc_oid = uri_parms[0]\n        query_str = uri_parms[1]\n        try:\n            cpc = hmc.cpcs.lookup_by_oid(cpc_oid)\n        except KeyError:\n            raise InvalidResourceError(method, uri)\n        assert not cpc.dpm_enabled  # TODO: Verify error or empty result?\n        result_profiles = []\n        filter_args = parse_query_parms(method, uri, query_str)\n        for profile in cpc.load_activation_profiles.list(filter_args):\n            result_profile = {}\n            for prop in profile.properties:\n                if prop in ('element-uri', 'name'):\n                    result_profile[prop] = profile.properties[prop]\n            result_profiles.append(result_profile)\n        return {'load-activation-profiles': result_profiles}", "response": "Operation: List Load Activation Profiles ( requires classic mode."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate and configure a new Partition in this CPC.", "response": "def create(self, properties):\n        \"\"\"\n        Create and configure a Partition in this CPC.\n\n        Authorization requirements:\n\n        * Object-access permission to this CPC.\n        * Task permission to the \"New Partition\" task.\n\n        Parameters:\n\n          properties (dict): Initial property values.\n            Allowable properties are defined in section 'Request body contents'\n            in section 'Create Partition' in the :term:`HMC API` book.\n\n        Returns:\n\n          Partition:\n            The resource object for the new Partition.\n            The object will have its 'object-uri' property set as returned by\n            the HMC, and will also have the input properties set.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        result = self.session.post(self.cpc.uri + '/partitions',\n                                   body=properties)\n        # There should not be overlaps, but just in case there are, the\n        # returned props should overwrite the input props:\n        props = copy.deepcopy(properties)\n        props.update(result)\n        name = props.get(self._name_prop, None)\n        uri = props[self._uri_prop]\n        part = Partition(self, uri, name, props)\n        self._name_uri_cache.update(name, uri)\n        return part"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef nics(self):\n        # We do here some lazy loading.\n        if not self._nics:\n            self._nics = NicManager(self)\n        return self._nics", "response": "Access to the NICs in this Partition."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef hbas(self):\n        # We do here some lazy loading.\n        if not self._hbas:\n            try:\n                dpm_sm = self.feature_enabled('dpm-storage-management')\n            except ValueError:\n                dpm_sm = False\n            if not dpm_sm:\n                self._hbas = HbaManager(self)\n        return self._hbas", "response": "Access to the HBAs <HBA > in this Partition."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\naccesses to the Virtual Functions <Virtual Function > in this Partition.", "response": "def virtual_functions(self):\n        \"\"\"\n        :class:`~zhmcclient.VirtualFunctionManager`: Access to the\n        :term:`Virtual Functions <Virtual Function>` in this Partition.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._virtual_functions:\n            self._virtual_functions = VirtualFunctionManager(self)\n        return self._virtual_functions"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef feature_enabled(self, feature_name):\n        feature_list = self.prop('available-features-list', None)\n        if feature_list is None:\n            raise ValueError(\"Firmware features are not supported on CPC %s\" %\n                             self.manager.cpc.name)\n        for feature in feature_list:\n            if feature['name'] == feature_name:\n                break\n        else:\n            raise ValueError(\"Firmware feature %s is not available on CPC %s\" %\n                             (feature_name, self.manager.cpc.name))\n        return feature['state']", "response": "Returns True if the specified feature is enabled for the HMC of this partition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef feature_info(self):\n        feature_list = self.prop('available-features-list', None)\n        if feature_list is None:\n            raise ValueError(\"Firmware features are not supported on CPC %s\" %\n                             self.manager.cpc.name)\n        return feature_list", "response": "Returns information about the features available for this HMC."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart the HMC operation for this partition.", "response": "def start(self, wait_for_completion=True, operation_timeout=None,\n              status_timeout=None):\n        \"\"\"\n        Start (activate) this Partition, using the HMC operation \"Start\n        Partition\".\n\n        This HMC operation has deferred status behavior: If the asynchronous\n        job on the HMC is complete, it takes a few seconds until the partition\n        status has reached the desired value (it still may show status\n        \"paused\"). If `wait_for_completion=True`, this method repeatedly checks\n        the status of the partition after the HMC operation has completed, and\n        waits until the status is in one of the desired states \"active\" or\n        \"degraded\".\n\n        TODO: Describe what happens if the maximum number of active partitions\n        is exceeded.\n\n        Authorization requirements:\n\n        * Object-access permission to this Partition.\n        * Object-access permission to the CPC containing this Partition.\n        * Task permission to the \"Start Partition\" task.\n\n        Parameters:\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n          status_timeout (:term:`number`):\n            Timeout in seconds, for waiting that the status of the partition\n            has reached the desired status, after the HMC operation has\n            completed.\n            The special value 0 means that no timeout is set. `None` means that\n            the default async operation timeout of the session is used.\n            If the timeout expires when `wait_for_completion=True`, a\n            :exc:`~zhmcclient.StatusTimeout` is raised.\n\n        Returns:\n\n          :class:`py:dict` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns an empty\n            :class:`py:dict` object.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n          :exc:`~zhmcclient.StatusTimeout`: The timeout expired while\n            waiting for the desired partition status.\n        \"\"\"\n        result = self.manager.session.post(\n            self.uri + '/operations/start',\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            statuses = [\"active\", \"degraded\"]\n            self.wait_for_status(statuses, status_timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_partition(self, parameters, wait_for_completion=True,\n                       operation_timeout=None):\n        \"\"\"\n        Dump this Partition, by loading a standalone dump program from a SCSI\n        device and starting its execution, using the HMC operation\n        'Dump Partition'.\n\n        Authorization requirements:\n\n        * Object-access permission to this Partition.\n        * Task permission to the \"Dump Partition\" task.\n\n        Parameters:\n\n          parameters (dict): Input parameters for the operation.\n            Allowable input parameters are defined in section\n            'Request body contents' in section 'Dump Partition' in the\n            :term:`HMC API` book.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n        Returns:\n\n          :class:`py:dict` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns an empty\n            :class:`py:dict` object.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n        \"\"\"\n        result = self.manager.session.post(\n            self.uri + '/operations/scsi-dump',\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout,\n            body=parameters)\n        return result", "response": "This method loads a standalone dump program from a SCSI device and starts its execution using the HMC operation\n            Dump Partition."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mount_iso_image(self, image, image_name, ins_file_name):\n        query_parms_str = '?image-name={}&ins-file-name={}'. \\\n            format(quote(image_name, safe=''), quote(ins_file_name, safe=''))\n        self.manager.session.post(\n            self.uri + '/operations/mount-iso-image' + query_parms_str,\n            body=image)", "response": "Mount an ISO image and associate it with this Partition."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening a JMS message channel to this partition s operating system and return the string topic representing the message channel.", "response": "def open_os_message_channel(self, include_refresh_messages=True):\n        \"\"\"\n        Open a JMS message channel to this partition's operating system,\n        returning the string \"topic\" representing the message channel.\n\n        Parameters:\n\n          include_refresh_messages (bool):\n            Boolean controlling whether refresh operating systems messages\n            should be sent, as follows:\n\n            * If `True`, refresh messages will be recieved when the user\n              connects to the topic. The default.\n\n            * If `False`, refresh messages will not be recieved when the user\n              connects to the topic.\n\n        Returns:\n\n          :term:`string`:\n\n            Returns a string representing the os-message-notification JMS\n            topic. The user can connect to this topic to start the flow of\n            operating system messages.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {'include-refresh-messages': include_refresh_messages}\n        result = self.manager.session.post(\n            self.uri + '/operations/open-os-message-channel', body)\n        return result['topic-name']"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_os_command(self, os_command_text, is_priority=False):\n        body = {'is-priority': is_priority,\n                'operating-system-command-text': os_command_text}\n        self.manager.session.post(\n            self.uri + '/operations/send-os-cmd', body)", "response": "Sends a command to the operating system running in this partition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef wait_for_status(self, status, status_timeout=None):\n        if status_timeout is None:\n            status_timeout = \\\n                self.manager.session.retry_timeout_config.status_timeout\n        if status_timeout > 0:\n            end_time = time.time() + status_timeout\n        if isinstance(status, (list, tuple)):\n            statuses = status\n        else:\n            statuses = [status]\n        while True:\n\n            # Fastest way to get actual status value:\n            parts = self.manager.cpc.partitions.list(\n                filter_args={'name': self.name})\n            assert len(parts) == 1\n            this_part = parts[0]\n            actual_status = this_part.get_property('status')\n\n            if actual_status in statuses:\n                return\n\n            if status_timeout > 0 and time.time() > end_time:\n                raise StatusTimeout(\n                    \"Waiting for partition {} to reach status(es) '{}' timed \"\n                    \"out after {} s - current status is '{}'\".\n                    format(self.name, statuses, status_timeout, actual_status),\n                    actual_status, statuses, status_timeout)\n\n            time.sleep(1)", "response": "Wait until the status of this partition has reached one of the desired status values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nincrease the crypto configuration of a specific set of cryptographic keys.", "response": "def increase_crypto_config(self, crypto_adapters,\n                               crypto_domain_configurations):\n        \"\"\"\n        Add crypto adapters and/or crypto domains to the crypto configuration\n        of this partition.\n\n        The general principle for maintaining crypto configurations of\n        partitions is as follows: Each adapter included in the crypto\n        configuration of a partition has all crypto domains included in the\n        crypto configuration. Each crypto domain included in the crypto\n        configuration has the same access mode on all adapters included in the\n        crypto configuration.\n\n        Example: Assume that the current crypto configuration of a partition\n        includes crypto adapter A and crypto domains 0 and 1. When this method\n        is called to add adapter B and domain configurations for domains 1 and\n        2, the resulting crypto configuration of the partition will include\n        domains 0, 1, and 2 on each of the adapters A and B.\n\n        Authorization requirements:\n\n        * Object-access permission to this Partition.\n        * Task permission to the \"Partition Details\" task.\n\n        Parameters:\n\n          crypto_adapters (:term:`iterable` of :class:`~zhmcclient.Adapter`):\n            Crypto adapters that should be added to the crypto configuration of\n            this partition.\n\n          crypto_domain_configurations (:term:`iterable` of `domain_config`):\n            Crypto domain configurations that should be added to the crypto\n            configuration of this partition.\n\n            A crypto domain configuration (`domain_config`) is a dictionary\n            with the following keys:\n\n            * ``\"domain-index\"`` (:term:`integer`): Domain index of the crypto\n              domain.\n\n              The domain index is a number in the range of 0 to a maximum that\n              depends on the model of the crypto adapter and the CPC model. For\n              the Crypto Express 5S adapter in a z13, the maximum domain index\n              is 84.\n\n            * ``\"access-mode\"`` (:term:`string`): Access mode for the crypto\n              domain.\n\n              The access mode specifies the way the partition can use the\n              crypto domain on the crypto adapter(s), using one of the\n              following string values:\n\n              * ``\"control\"`` - The partition can load cryptographic keys into\n                the domain, but it may not use the domain to perform\n                cryptographic operations.\n\n              * ``\"control-usage\"`` - The partition can load cryptographic keys\n                into the domain, and it can use the domain to perform\n                cryptographic operations.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        crypto_adapter_uris = [a.uri for a in crypto_adapters]\n        body = {'crypto-adapter-uris': crypto_adapter_uris,\n                'crypto-domain-configurations': crypto_domain_configurations}\n        self.manager.session.post(\n            self.uri + '/operations/increase-crypto-configuration', body)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecrease the crypto configuration of a specific set of crypto adapters and domains of a specific set of crypto domains.", "response": "def decrease_crypto_config(self, crypto_adapters,\n                               crypto_domain_indexes):\n        \"\"\"\n        Remove crypto adapters and/or crypto domains from the crypto\n        configuration of this partition.\n\n        For the general principle for maintaining crypto configurations of\n        partitions, see :meth:`~zhmcclient.Partition.increase_crypto_config`.\n\n        Example: Assume that the current crypto configuration of a partition\n        includes crypto adapters A, B and C and crypto domains 0, 1, and 2 (on\n        each of the adapters). When this method is called to remove adapter C\n        and domain 2, the resulting crypto configuration of the partition will\n        include domains 0 and 1 on each of the adapters A and B.\n\n        Authorization requirements:\n\n        * Object-access permission to this Partition.\n        * Task permission to the \"Partition Details\" task.\n\n        Parameters:\n\n          crypto_adapters (:term:`iterable` of :class:`~zhmcclient.Adapter`):\n            Crypto adapters that should be removed from the crypto\n            configuration of this partition.\n\n          crypto_domain_indexes (:term:`iterable` of :term:`integer`):\n            Domain indexes of the crypto domains that should be removed from\n            the crypto configuration of this partition. For values, see\n            :meth:`~zhmcclient.Partition.increase_crypto_config`.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        crypto_adapter_uris = [a.uri for a in crypto_adapters]\n        body = {'crypto-adapter-uris': crypto_adapter_uris,\n                'crypto-domain-indexes': crypto_domain_indexes}\n        self.manager.session.post(\n            self.uri + '/operations/decrease-crypto-configuration', body)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_crypto_domain_config(self, crypto_domain_index, access_mode):\n        body = {'domain-index': crypto_domain_index,\n                'access-mode': access_mode}\n        self.manager.session.post(\n            self.uri + '/operations/change-crypto-domain-configuration', body)", "response": "Changes the crypto configuration of a specific crypto domain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zeroize_crypto_domain(self, crypto_adapter, crypto_domain_index):\n        body = {\n            'crypto-adapter-uri': crypto_adapter.uri,\n            'domain-index': crypto_domain_index\n        }\n        self.manager.session.post(\n            self.uri + '/operations/zeroize-crypto-domain', body)", "response": "Zeroizes a single crypto domain on a crypto adapter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nattaching a storage group to this partition.", "response": "def attach_storage_group(self, storage_group):\n        \"\"\"\n        Attach a :term:`storage group` to this partition.\n\n        This will cause the :term:`storage volumes <storage volume>` of the\n        storage group to be attached to the partition, instantiating any\n        necessary :term:`virtual storage resource` objects.\n\n        A storage group can be attached to a partition regardless of its\n        fulfillment state. The fulfillment state of its storage volumes\n        and thus of the entire storage group changes as volumes are discovered\n        by DPM, and will eventually reach \"complete\".\n\n        The CPC must have the \"dpm-storage-management\" feature enabled.\n\n        Authorization requirements:\n\n        * Object-access permission to this partition.\n        * Object-access permission to the specified storage group.\n        * Task permission to the \"Partition Details\" task.\n\n        Parameters:\n\n          storage_group (:class:`~zhmcclient.StorageGroup`):\n            Storage group to be attached. The storage group must not currently\n            be attached to this partition.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {'storage-group-uri': storage_group.uri}\n        self.manager.session.post(\n            self.uri + '/operations/attach-storage-group', body)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetach a storage group from this partition.", "response": "def detach_storage_group(self, storage_group):\n        \"\"\"\n        Detach a :term:`storage group` from this partition.\n\n        This will cause the :term:`storage volumes <storage volume>` of the\n        storage group to be detached from the partition, removing any\n        :term:`virtual storage resource` objects that had been created upon\n        attachment.\n\n        A storage group can be detached from a partition regardless of its\n        fulfillment state. The fulfillment state of its storage volumes\n        changes as volumes are discovered by DPM.\n\n        The CPC must have the \"dpm-storage-management\" feature enabled.\n\n        Authorization requirements:\n\n        * Object-access permission to this partition.\n        * Task permission to the \"Partition Details\" task.\n\n        Parameters:\n\n          storage_group (:class:`~zhmcclient.StorageGroup`):\n            Storage group to be detached. The storage group must currently\n            be attached to this partition.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {'storage-group-uri': storage_group.uri}\n        self.manager.session.post(\n            self.uri + '/operations/detach-storage-group', body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of storage groups that are attached to this partition.", "response": "def list_attached_storage_groups(self, full_properties=False):\n        \"\"\"\n        Return the storage groups that are attached to this partition.\n\n        The CPC must have the \"dpm-storage-management\" feature enabled.\n\n        Authorization requirements:\n\n        * Object-access permission to this partition.\n        * Task permission to the \"Partition Details\" task.\n\n        Parameters:\n\n          full_properties (bool):\n            Controls that the full set of resource properties for each returned\n            storage group is being retrieved, vs. only the following short set:\n            \"object-uri\", \"object-id\", \"class\", \"parent\".\n\n            TODO: Verify short list of properties.\n\n        Returns:\n\n          List of :class:`~zhmcclient.StorageGroup` objects representing the\n          storage groups that are attached to this partition.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        sg_list = []\n        sg_uris = self.get_property('storage-group-uris')\n        if sg_uris:\n            cpc = self.manager.cpc\n            for sg_uri in sg_uris:\n                sg = cpc.storage_groups.resource_object(sg_uri)\n                sg_list.append(sg)\n                if full_properties:\n                    sg.pull_full_properties()\n        return sg_list"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates writeable properties of this PasswordRule.", "response": "def update_properties(self, properties):\n        \"\"\"\n        Update writeable properties of this PasswordRule.\n\n        The Password Rule must be user-defined. System-defined Password Rules\n        cannot be updated.\n\n        Authorization requirements:\n\n        * Task permission to the \"Manage Password Rules\" task.\n\n        Parameters:\n\n          properties (dict): New values for the properties to be updated.\n            Properties not to be updated are omitted.\n            Allowable properties are the properties with qualifier (w) in\n            section 'Data model' in section 'Password Rule object' in the\n            :term:`HMC API` book.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        self.manager.session.post(self.uri, body=properties)\n\n        # The name of Password Rules cannot be updated. An attempt to do so\n        # should cause HTTPError to be raised in the POST above, so we assert\n        # that here, because we omit the extra code for handling name updates:\n        assert self.manager._name_prop not in properties\n        self.properties.update(copy.deepcopy(properties))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef version_info(self):\n        if self._api_version is None:\n            self.query_api_version()\n        return self._api_version['api-major-version'],\\\n            self._api_version['api-minor-version']", "response": "Returns API version information for the HMC."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery API Version operation returns information about the level of Web Services API supported by the HMC.", "response": "def query_api_version(self):\n        \"\"\"\n        The Query API Version operation returns information about\n        the level of Web Services API supported by the HMC.\n\n        This operation does not require authentication.\n\n        Returns:\n\n          :term:`json object`:\n            A JSON object with members ``api-major-version``,\n            ``api-minor-version``, ``hmc-version`` and ``hmc-name``.\n            For details about these properties, see section\n            'Response body contents' in section 'Query API Version' in the\n            :term:`HMC API` book.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        version_resp = self._session.get('/api/version',\n                                         logon_required=False)\n        self._api_version = version_resp\n        return self._api_version"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a JSON object with the requested resources and their properties, that are managed by the HMC. This method performs the 'Get Inventory' HMC operation. Parameters: resources (:term:`iterable` of :term:`string`): Resource classes and/or resource classifiers specifying the types of resources that should be included in the result. For valid values, see the 'Get Inventory' operation in the :term:`HMC API` book. Element resources of the specified resource types are automatically included as children (for example, requesting 'partition' includes all of its 'hba', 'nic' and 'virtual-function' element resources). Must not be `None`. Returns: :term:`JSON object`: The resources with their properties, for the requested resource classes and resource classifiers. Example: resource_classes = ['partition', 'adapter'] result_dict = client.get_inventory(resource_classes) Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.ConnectionError`", "response": "def get_inventory(self, resources):\n        \"\"\"\n        Returns a JSON object with the requested resources and their\n        properties, that are managed by the HMC.\n\n        This method performs the 'Get Inventory' HMC operation.\n\n        Parameters:\n\n          resources (:term:`iterable` of :term:`string`):\n            Resource classes and/or resource classifiers specifying the types\n            of resources that should be included in the result. For valid\n            values, see the 'Get Inventory' operation in the :term:`HMC API`\n            book.\n\n            Element resources of the specified resource types are automatically\n            included as children (for example, requesting 'partition' includes\n            all of its 'hba', 'nic' and 'virtual-function' element resources).\n\n            Must not be `None`.\n\n        Returns:\n\n          :term:`JSON object`:\n            The resources with their properties, for the requested resource\n            classes and resource classifiers.\n\n        Example:\n\n            resource_classes = ['partition', 'adapter']\n            result_dict = client.get_inventory(resource_classes)\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        uri = '/api/services/inventory'\n        body = {'resources': resources}\n        result = self.session.post(uri, body=body)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwaiting for the Console to become available.", "response": "def wait_for_available(self, operation_timeout=None):\n        \"\"\"\n        Wait for the Console (HMC) this client is connected to, to become\n        available. The Console is considered available if the\n        :meth:`~zhmcclient.Client.query_api_version` method succeeds.\n\n        If the Console does not become available within the operation timeout,\n        an :exc:`~zhmcclient.OperationTimeout` exception is raised.\n\n        Parameters:\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, when waiting for the Console to become\n            available. The special value 0 means that no timeout is set. `None`\n            means that the default async operation timeout of the session is\n            used.\n\n            If the timeout expires, a :exc:`~zhmcclient.OperationTimeout`\n            is raised.\n\n        Raises:\n\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for the Console to become available.\n        \"\"\"\n        if operation_timeout is None:\n            operation_timeout = \\\n                self.session.retry_timeout_config.operation_timeout\n        if operation_timeout > 0:\n            start_time = time.time()\n        while True:\n            try:\n                self.query_api_version()\n            except Error:\n                pass\n            except Exception:\n                raise\n            else:\n                break\n            if operation_timeout > 0:\n                current_time = time.time()\n                if current_time > start_time + operation_timeout:\n                    raise OperationTimeout(\n                        \"Waiting for Console at {} to become available timed \"\n                        \"out (operation timeout: {} s)\".\n                        format(self.session.host, operation_timeout),\n                        operation_timeout)\n            time.sleep(10)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the full set of resource properties and cache them in self. _properties.", "response": "def pull_full_properties(self):\n        \"\"\"\n        Retrieve the full set of resource properties and cache them in this\n        object.\n\n        Authorization requirements:\n\n        * Object-access permission to this resource.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        full_properties = self.manager.session.get(self._uri)\n        self._properties = dict(full_properties)\n        self._properties_timestamp = int(time.time())\n        self._full_properties = True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a resource property.", "response": "def get_property(self, name):\n        \"\"\"\n        Return the value of a resource property.\n\n        If the resource property is not cached in this object yet, the full set\n        of resource properties is retrieved and cached in this object, and the\n        resource property is again attempted to be returned.\n\n        Authorization requirements:\n\n        * Object-access permission to this resource.\n\n        Parameters:\n\n          name (:term:`string`):\n            Name of the resource property, using the names defined in the\n            respective 'Data model' sections in the :term:`HMC API` book.\n\n        Returns:\n\n          The value of the resource property.\n\n        Raises:\n\n          KeyError: The resource property could not be found (also not in the\n            full set of resource properties).\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        try:\n            return self._properties[name]\n        except KeyError:\n            if self._full_properties:\n                raise\n            self.pull_full_properties()\n            return self._properties[name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_client(zhmc, userid=None, password=None):\n\n    global USERID, PASSWORD  # pylint: disable=global-statement\n\n    USERID = userid or USERID or \\\n        six.input('Enter userid for HMC {}: '.format(zhmc))\n    PASSWORD = password or PASSWORD or \\\n        getpass.getpass('Enter password for {}: '.format(USERID))\n\n    session = zhmcclient.Session(zhmc, USERID, PASSWORD)\n    session.logon()\n    client = zhmcclient.Client(session)\n    print('Established logged-on session with HMC {} using userid {}'.\n          format(zhmc, USERID))\n    return client", "response": "Create a new Zhmcclient object for the specified HMC and log that on."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a debug representation of a multi - line text.", "response": "def repr_text(text, indent):\n    \"\"\"Return a debug representation of a multi-line text (e.g. the result\n    of another repr...() function).\"\"\"\n    if text is None:\n        return 'None'\n    ret = _indent(text, amount=indent)\n    return ret.lstrip(' ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef repr_list(_list, indent):\n    # pprint represents lists and tuples in one row if possible. We want one\n    # per row, so we iterate ourselves.\n    if _list is None:\n        return 'None'\n    if isinstance(_list, MutableSequence):\n        bm = '['\n        em = ']'\n    elif isinstance(_list, Iterable):\n        bm = '('\n        em = ')'\n    else:\n        raise TypeError(\"Object must be an iterable, but is a %s\" %\n                        type(_list))\n    ret = bm + '\\n'\n    for value in _list:\n        ret += _indent('%r,\\n' % value, 2)\n    ret += em\n    ret = repr_text(ret, indent=indent)\n    return ret.lstrip(' ')", "response": "Return a debug representation of a list or tuple."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a debug representation of a dict or OrderedDict.", "response": "def repr_dict(_dict, indent):\n    \"\"\"Return a debug representation of a dict or OrderedDict.\"\"\"\n    # pprint represents OrderedDict objects using the tuple init syntax,\n    # which is not very readable. Therefore, dictionaries are iterated over.\n    if _dict is None:\n        return 'None'\n    if not isinstance(_dict, Mapping):\n        raise TypeError(\"Object must be a mapping, but is a %s\" %\n                        type(_dict))\n    if isinstance(_dict, OrderedDict):\n        kind = 'ordered'\n        ret = '%s {\\n' % kind  # non standard syntax for the kind indicator\n        for key in six.iterkeys(_dict):\n            value = _dict[key]\n            ret += _indent('%r: %r,\\n' % (key, value), 2)\n    else:  # dict\n        kind = 'sorted'\n        ret = '%s {\\n' % kind  # non standard syntax for the kind indicator\n        for key in sorted(six.iterkeys(_dict)):\n            value = _dict[key]\n            ret += _indent('%r: %r,\\n' % (key, value), 2)\n    ret += '}'\n    ret = repr_text(ret, indent=indent)\n    return ret.lstrip(' ')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef repr_timestamp(timestamp):\n    if timestamp is None:\n        return 'None'\n    dt = datetime_from_timestamp(timestamp)\n    ret = \"%d (%s)\" % (timestamp,\n                       dt.strftime('%Y-%m-%d %H:%M:%S.%f %Z'))\n    return ret", "response": "Return a debug representation of an HMC timestamp number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datetime_from_timestamp(ts):\n    # Note that in Python 2, \"None < 0\" is allowed and will return True,\n    # therefore we do an extra check for None.\n    if ts is None:\n        raise ValueError(\"HMC timestamp value must not be None.\")\n    if ts < 0:\n        raise ValueError(\n            \"Negative HMC timestamp value {} cannot be represented as \"\n            \"datetime.\".format(ts))\n    epoch_seconds = ts // 1000\n    delta_microseconds = ts % 1000 * 1000\n    try:\n        dt = datetime.fromtimestamp(epoch_seconds, pytz.utc)\n    except (ValueError, OSError) as exc:\n        raise ValueError(str(exc))\n    dt = dt.replace(microsecond=delta_microseconds)\n    return dt", "response": "Converts an HMC timestamp number <timestamp > into a special avecite datetime object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a datetime object into an HMC timestamp number.", "response": "def timestamp_from_datetime(dt):\n    \"\"\"\n    Convert a :class:`~py:datetime.datetime` object into an\n    :term:`HMC timestamp number <timestamp>`.\n\n    The date and time range supported by this function has the following\n    bounds:\n\n    * The upper bounds is :attr:`py:datetime.datetime.max`, as follows:\n\n      * 9999-12-31 23:59:59 UTC, for 32-bit and 64-bit CPython on Linux and\n        OS-X.\n      * 2038-01-19 03:14:07 UTC, for some 32-bit Python implementations,\n        due to the `Year 2038 problem\n        <https://en.wikipedia.org/wiki/Year_2038_problem>`_.\n\n    * The lower bounds is the UNIX epoch: 1970-01-01 00:00:00 UTC.\n\n    Parameters:\n\n      dt (:class:`~py:datetime.datetime`):\n        Point in time as a Python datetime object. The datetime object may be\n        timezone-aware or timezone-naive. If timezone-naive, the UTC timezone\n        is assumed.\n\n        Must not be `None`.\n\n    Returns:\n\n      :term:`timestamp`:\n        Point in time as an HMC timestamp number.\n\n    Raises:\n        ValueError\n    \"\"\"\n    if dt is None:\n        raise ValueError(\"datetime value must not be None.\")\n    if dt.tzinfo is None:\n        # Apply default timezone to the timezone-naive input\n        dt = pytz.utc.localize(dt)\n    epoch_seconds = (dt - _EPOCH_DT).total_seconds()\n    ts = int(epoch_seconds * 1000)\n    return ts"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create(self, properties):\n        if properties is None:\n            properties = {}\n\n        result = self.session.post(self._base_uri, body=properties)\n        # There should not be overlaps, but just in case there are, the\n        # returned props should overwrite the input props:\n        props = copy.deepcopy(properties)\n        props.update(result)\n        name = props.get(self._name_prop, None)\n        uri = props[self._uri_prop]\n        storage_group = StorageGroup(self, uri, name, props)\n        self._name_uri_cache.update(name, uri)\n        return storage_group", "response": "Creates and configure a new storage group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef storage_volumes(self):\n        # We do here some lazy loading.\n        if not self._storage_volumes:\n            self._storage_volumes = StorageVolumeManager(self)\n        return self._storage_volumes", "response": "Access to the storage volumes in this storage group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef virtual_storage_resources(self):\n        # We do here some lazy loading.\n        if not self._virtual_storage_resources:\n            self._virtual_storage_resources = \\\n                VirtualStorageResourceManager(self)\n        return self._virtual_storage_resources", "response": "Access to the virtual storage resources in this storage group."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the CPC object for this storage group.", "response": "def cpc(self):\n        \"\"\"\n        :class:`~zhmcclient.Cpc`: The :term:`CPC` to which this storage group\n        is associated.\n\n        The returned :class:`~zhmcclient.Cpc` has only a minimal set of\n        properties populated.\n        \"\"\"\n        # We do here some lazy loading.\n        if not self._cpc:\n            cpc_uri = self.get_property('cpc-uri')\n            cpc_mgr = self.manager.console.manager.client.cpcs\n            self._cpc = cpc_mgr.resource_object(cpc_uri)\n        return self._cpc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the partitions to which this storage group is currently attached optionally filtered by partition name and status.", "response": "def list_attached_partitions(self, name=None, status=None):\n        \"\"\"\n        Return the partitions to which this storage group is currently\n        attached, optionally filtered by partition name and status.\n\n        Authorization requirements:\n\n        * Object-access permission to this storage group.\n        * Task permission to the \"Configure Storage - System Programmer\" task.\n\n        Parameters:\n\n          name (:term:`string`): Filter pattern (regular expression) to limit\n            returned partitions to those that have a matching name. If `None`,\n            no filtering for the partition name takes place.\n\n          status (:term:`string`): Filter string to limit returned partitions\n            to those  that have a matching status. The value must be a valid\n            partition status property value. If `None`, no filtering for the\n            partition status takes place.\n\n        Returns:\n\n          List of :class:`~zhmcclient.Partition` objects representing the\n          partitions to whivch this storage group is currently attached,\n          with a minimal set of properties ('object-id', 'name', 'status').\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n\n        query_parms = []\n        if name is not None:\n            self.manager._append_query_parms(query_parms, 'name', name)\n        if status is not None:\n            self.manager._append_query_parms(query_parms, 'status', status)\n        query_parms_str = '&'.join(query_parms)\n        if query_parms_str:\n            query_parms_str = '?{}'.format(query_parms_str)\n\n        uri = '{}/operations/get-partitions{}'.format(\n            self.uri, query_parms_str)\n\n        sg_cpc = self.cpc\n        part_mgr = sg_cpc.partitions\n\n        result = self.manager.session.get(uri)\n        props_list = result['partitions']\n        part_list = []\n        for props in props_list:\n            part = part_mgr.resource_object(props['object-uri'], props)\n            part_list.append(part)\n        return part_list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a list of storage adapter ports to this storage group s candidate adapter ports list.", "response": "def add_candidate_adapter_ports(self, ports):\n        \"\"\"\n        Add a list of storage adapter ports to this storage group's candidate\n        adapter ports list.\n\n        This operation only applies to storage groups of type \"fcp\".\n\n        These adapter ports become candidates for use as backing adapters when\n        creating virtual storage resources when the storage group is attached\n        to a partition. The adapter ports should have connectivity to the\n        storage area network (SAN).\n\n        Candidate adapter ports may only be added before the CPC discovers a\n        working communications path, indicated by a \"verified\" status on at\n        least one of this storage group's WWPNs. After that point, all\n        adapter ports in the storage group are automatically detected and\n        manually adding them is no longer possible.\n\n        Because the CPC discovers working communications paths automatically,\n        candidate adapter ports do not need to be added by the user. Any\n        ports that are added, are validated by the CPC during discovery,\n        and may or may not actually be used.\n\n        Authorization requirements:\n\n        * Object-access permission to this storage group.\n        * Object-access permission to the adapter of each specified port.\n        * Task permission to the \"Configure Storage - System Programmer\" task.\n\n        Parameters:\n\n          ports (:class:`py:list`): List of :class:`~zhmcclient.Port` objects\n            representing the ports to be added. All specified ports must not\n            already be members of this storage group's candidate adapter ports\n            list.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {\n            'adapter-port-uris': [p.uri for p in ports],\n        }\n        self.manager.session.post(\n            self.uri + '/operations/add-candidate-adapter-ports',\n            body=body)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_candidate_adapter_ports(self, full_properties=False):\n        sg_cpc = self.cpc\n        adapter_mgr = sg_cpc.adapters\n        port_list = []\n        port_uris = self.get_property('candidate-adapter-port-uris')\n        if port_uris:\n            for port_uri in port_uris:\n                m = re.match(r'^(/api/adapters/[^/]*)/.*', port_uri)\n\n                adapter_uri = m.group(1)\n                adapter = adapter_mgr.resource_object(adapter_uri)\n\n                port_mgr = adapter.ports\n                port = port_mgr.resource_object(port_uri)\n                port_list.append(port)\n                if full_properties:\n                    port.pull_full_properties()\n\n        return port_list", "response": "Returns the current candidate storage adapter ports of this storage group."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and returns a HiperSockets Adapter in this CPC.", "response": "def create_hipersocket(self, properties):\n        \"\"\"\n        Create and configure a HiperSockets Adapter in this CPC.\n\n        Authorization requirements:\n\n        * Object-access permission to the scoping CPC.\n        * Task permission to the \"Create HiperSockets Adapter\" task.\n\n        Parameters:\n\n          properties (dict): Initial property values.\n            Allowable properties are defined in section 'Request body contents'\n            in section 'Create Hipersocket' in the :term:`HMC API` book.\n\n        Returns:\n\n          :class:`~zhmcclient.Adapter`:\n            The resource object for the new HiperSockets Adapter.\n            The object will have its 'object-uri' property set as returned by\n            the HMC, and will also have the input properties set.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        result = self.session.post(self.cpc.uri + '/adapters', body=properties)\n        # There should not be overlaps, but just in case there are, the\n        # returned props should overwrite the input props:\n        props = copy.deepcopy(properties)\n        props.update(result)\n        name = props.get(self._name_prop, None)\n        uri = props[self._uri_prop]\n        adapter = Adapter(self, uri, name, props)\n        self._name_uri_cache.update(name, uri)\n        return adapter"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ports(self):\n        # We do here some lazy loading.\n        if not self._ports:\n            family = self.get_property('adapter-family')\n            try:\n                port_type = self.port_type_by_family[family]\n            except KeyError:\n                port_type = None\n            self._ports = PortManager(self, port_type)\n        return self._ports", "response": "Access to the Ports of this Adapter."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef port_uri_segment(self):\n        if self._port_uri_segment is None:\n            family = self.get_property('adapter-family')\n            try:\n                self._port_uri_segment = self.port_uri_segment_by_family[\n                    family]\n            except KeyError:\n                self._port_uri_segment = ''\n        return self._port_uri_segment", "response": "Returns the port URI segment for the given adapter type specific entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the maximum number of crypto domains on this adapter.", "response": "def maximum_crypto_domains(self):\n        \"\"\"\n        Integer: The maximum number of crypto domains on this crypto adapter.\n\n        The following table shows the maximum number of crypto domains for\n        crypto adapters supported on IBM Z machine generations in DPM mode. The\n        corresponding LinuxONE machine generations are listed in the notes\n        below the table:\n\n        =================  =========================  ===============\n        Adapter type       Machine generations        Maximum domains\n        =================  =========================  ===============\n        Crypto Express 5S  z14 (3) / z13 (1)               85\n        Crypto Express 5S  z14-ZR1 (4) / z13s (2)          40\n        Crypto Express 6S  z14 (3)                         85\n        Crypto Express 6S  z14-ZR1 (4)                     40\n        =================  =========================  ===============\n\n        Notes:\n\n        (1) Supported for z13 and LinuxONE Emperor\n        (2) Supported for z13s and LinuxONE Rockhopper\n        (3) Supported for z14 and LinuxONE Emperor II\n        (4) Supported for z14-ZR1 and LinuxONE Rockhopper II\n\n        If this adapter is not a crypto adapter, `None` is returned.\n\n        If the crypto adapter card type is not known, :exc:`ValueError` is\n        raised.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`ValueError`: Unknown crypto card type\n        \"\"\"\n        if self.get_property('adapter-family') != 'crypto':\n            return None\n        card_type = self.get_property('detected-card-type')\n        if card_type.startswith('crypto-express-'):\n            max_domains = self.manager.cpc.maximum_active_partitions\n        else:\n            raise ValueError(\"Unknown crypto card type: {!r}\".\n                             format(card_type))\n        return max_domains"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange the crypto type of a cryptographic adapter.", "response": "def change_crypto_type(self, crypto_type, zeroize=None):\n        \"\"\"\n        Reconfigures a cryptographic adapter to a different crypto type.\n        This operation is only supported for cryptographic adapters.\n\n        The cryptographic adapter must be varied offline before its crypto\n        type can be reconfigured.\n\n        Authorization requirements:\n\n        * Object-access permission to this Adapter.\n        * Task permission to the \"Adapter Details\" task.\n\n        Parameters:\n\n          crypto_type (:term:`string`):\n            - ``\"accelerator\"``: Crypto Express5S Accelerator\n            - ``\"cca-coprocessor\"``: Crypto Express5S CCA Coprocessor\n            - ``\"ep11-coprocessor\"``: Crypto Express5S EP11 Coprocessor\n\n          zeroize (bool):\n            Specifies whether the cryptographic adapter will be zeroized when\n            it is reconfigured to a crypto type of ``\"accelerator\"``.\n            `None` means that the HMC-implemented default of `True` will be\n            used.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n        \"\"\"\n        body = {'crypto-type': crypto_type}\n        if zeroize is not None:\n            body['zeroize'] = zeroize\n        self.manager.session.post(\n            self.uri + '/operations/change-crypto-type', body)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_adapter_type(self, adapter_type):\n        body = {'type': adapter_type}\n        self.manager.session.post(\n            self.uri + '/operations/change-adapter-type', body)", "response": "Changes the type of the targeted storage adapter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scsi_load(self, load_address, wwpn, lun, load_parameter=None,\n                  disk_partition_id=None,\n                  operating_system_specific_load_parameters=None,\n                  boot_record_logical_block_address=None, force=False,\n                  wait_for_completion=True, operation_timeout=None,\n                  status_timeout=None, allow_status_exceptions=False):\n        \"\"\"\n        Load (boot) this LPAR from a designated SCSI device, using the\n        HMC operation \"SCSI Load\".\n\n        This HMC operation has deferred status behavior: If the asynchronous\n        job on the HMC is complete, it takes a few seconds until the LPAR\n        status has reached the desired value. If `wait_for_completion=True`,\n        this method repeatedly checks the status of the LPAR after the HMC\n        operation has completed, and waits until the status is in the desired\n        state \"operating\", or if `allow_status_exceptions` was\n        set additionally in the state \"exceptions\".\n\n        Authorization requirements:\n\n        * Object-access permission to the CPC containing this LPAR.\n        * Object-access permission to this LPAR.\n        * Task permission for the \"SCSI Load\" task.\n\n        Parameters:\n\n          load_address (:term:`string`):\n            Device number of the boot device.\n\n          wwpn (:term:`string`):\n            Worldwide port name (WWPN) of the target SCSI device to be\n            used for this operation, in hexadecimal.\n\n          lun (:term:`string`):\n            Hexadecimal logical unit number (LUN) to be used for the\n            SCSI Load.\n\n          load_parameter (:term:`string`):\n            Optional load control string.  If empty string or `None`,\n            it is not passed to the HMC.\n\n          disk_partition_id (:term:`integer`):\n             Optional disk-partition-id (also called the boot program\n             selector) to be used for the SCSI Load. If `None`, it is\n             not passed to the HMC.\n\n          operating_system_specific_load_parameters (:term:`string`):\n             Optional operating system specific load parameters to be\n             used for the SCSI Load.\n\n          boot_record_logical_block_address (:term:`string`):\n             Optional hexadecimal boot record logical block address to\n             be used for the SCSI Load.\n\n          force (bool):\n            Boolean controlling whether this operation is permitted when the\n            LPAR is in the \"operating\" status. The default value is `True`.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation, and for the status\n              becoming \"operating\" (or in addition \"exceptions\", if\n              `allow_status_exceptions` was set.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n          status_timeout (:term:`number`):\n            Timeout in seconds, for waiting that the status of the LPAR has\n            reached the desired status, after the HMC operation has completed.\n            The special value 0 means that no timeout is set. `None` means that\n            the default async operation timeout of the session is used.\n            If the timeout expires when `wait_for_completion=True`, a\n            :exc:`~zhmcclient.StatusTimeout` is raised.\n\n          allow_status_exceptions (bool):\n            Boolean controlling whether LPAR status \"exceptions\" is considered\n            an additional acceptable end status when `wait_for_completion` is\n            set.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n          :exc:`~zhmcclient.StatusTimeout`: The timeout expired while\n            waiting for the desired LPAR status.\n        \"\"\"\n        body = {}\n        body['load-address'] = load_address\n        body['world-wide-port-name'] = wwpn\n        body['logical-unit-number'] = lun\n        if load_parameter:\n            body['load-parameter'] = load_parameter\n        if disk_partition_id is not None:\n            body['disk-partition-id'] = disk_partition_id\n        if operating_system_specific_load_parameters:\n            body['operating-system-specific-load-parameters'] = \\\n                operating_system_specific_load_parameters\n        if boot_record_logical_block_address:\n            body['boot-record-logical-block-address'] = \\\n                boot_record_logical_block_address\n        if force:\n            body['force'] = force\n        result = self.manager.session.post(\n            self.uri + '/operations/scsi-load',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            statuses = [\"operating\"]\n            if allow_status_exceptions:\n                statuses.append(\"exceptions\")\n            self.wait_for_status(statuses, status_timeout)\n        return result", "response": "This method is used to perform a SCSI Load operation on a designated SCSI device. This method is used to perform a SCSI Load operation on a designated SCSI device."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload (boot) this LPAR from a load address (boot device), using the HMC operation \"Load Logical Partition\". This HMC operation has deferred status behavior: If the asynchronous job on the HMC is complete, it takes a few seconds until the LPAR status has reached the desired value. If `wait_for_completion=True`, this method repeatedly checks the status of the LPAR after the HMC operation has completed, and waits until the status is in the desired state \"operating\", or if `allow_status_exceptions` was set additionally in the state \"exceptions\". Authorization requirements: * Object-access permission to the CPC containing this LPAR. * Object-access permission to this LPAR. * Task permission for the \"Load\" task. Parameters: load_address (:term:`string`): Device number of the boot device. Up to z13, this parameter is required. Starting with z14, this parameter is optional and defaults to the load address specified in the 'last-used-load-address' property of the Lpar. load_parameter (:term:`string`): Optional load control string. If empty string or `None`, it is not passed to the HMC. clear_indicator (bool): Optional boolean controlling whether the memory should be cleared before performing the load or not cleared. The default value is `True`. store_status_indicator (bool): Optional boolean controlling whether the status should be stored before performing the Load. The default value is `False`. wait_for_completion (bool): Boolean controlling whether this method should wait for completion of the requested asynchronous HMC operation, as follows: * If `True`, this method will wait for completion of the asynchronous job performing the operation, and for the status becoming \"operating\" (or in addition \"exceptions\", if `allow_status_exceptions` was set. * If `False`, this method will return immediately once the HMC has accepted the request to perform the operation. operation_timeout (:term:`number`): Timeout in seconds, for waiting for completion of the asynchronous job performing the operation. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.OperationTimeout` is raised. status_timeout (:term:`number`): Timeout in seconds, for waiting that the status of the LPAR has reached the desired status, after the HMC operation has completed. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.StatusTimeout` is raised. allow_status_exceptions (bool): Boolean controlling whether LPAR status \"exceptions\" is considered an additional acceptable end status when `wait_for_completion` is set. force (bool): Boolean controlling whether this operation is permitted when the LPAR is in the \"operating\" status. TBD: What will happen with the LPAR in that case (deactivated then activated? nothing?) Returns: `None` or :class:`~zhmcclient.Job`: If `wait_for_completion` is `True`, returns `None`. If `wait_for_completion` is `False`, returns a :class:`~zhmcclient.Job` object representing the asynchronously executing job on the HMC. Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.OperationTimeout`: The timeout expired while waiting for completion of the operation. :exc:`~zhmcclient.StatusTimeout`: The timeout expired while waiting for the desired LPAR status.", "response": "def load(self, load_address=None, load_parameter=None,\n             clear_indicator=True, store_status_indicator=False,\n             wait_for_completion=True, operation_timeout=None,\n             status_timeout=None, allow_status_exceptions=False,\n             force=False):\n        \"\"\"\n        Load (boot) this LPAR from a load address (boot device), using the HMC\n        operation \"Load Logical Partition\".\n\n        This HMC operation has deferred status behavior: If the asynchronous\n        job on the HMC is complete, it takes a few seconds until the LPAR\n        status has reached the desired value. If `wait_for_completion=True`,\n        this method repeatedly checks the status of the LPAR after the HMC\n        operation has completed, and waits until the status is in the desired\n        state \"operating\", or if `allow_status_exceptions` was\n        set additionally in the state \"exceptions\".\n\n        Authorization requirements:\n\n        * Object-access permission to the CPC containing this LPAR.\n        * Object-access permission to this LPAR.\n        * Task permission for the \"Load\" task.\n\n        Parameters:\n\n          load_address (:term:`string`): Device number of the boot device.\n            Up to z13, this parameter is required.\n            Starting with z14, this parameter is optional and defaults to the\n            load address specified in the 'last-used-load-address' property of\n            the Lpar.\n\n          load_parameter (:term:`string`): Optional load control string.\n            If empty string or `None`, it is not passed to the HMC.\n\n          clear_indicator (bool):\n            Optional boolean controlling whether the memory should be\n            cleared before performing the load or not cleared. The\n            default value is `True`.\n\n          store_status_indicator (bool):\n            Optional boolean controlling whether the status should be\n            stored before performing the Load. The default value is `False`.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation, and for the status\n              becoming \"operating\" (or in addition \"exceptions\", if\n              `allow_status_exceptions` was set.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n          status_timeout (:term:`number`):\n            Timeout in seconds, for waiting that the status of the LPAR has\n            reached the desired status, after the HMC operation has completed.\n            The special value 0 means that no timeout is set. `None` means that\n            the default async operation timeout of the session is used.\n            If the timeout expires when `wait_for_completion=True`, a\n            :exc:`~zhmcclient.StatusTimeout` is raised.\n\n          allow_status_exceptions (bool):\n            Boolean controlling whether LPAR status \"exceptions\" is considered\n            an additional acceptable end status when `wait_for_completion` is\n            set.\n\n          force (bool):\n            Boolean controlling whether this operation is permitted when the\n            LPAR is in the \"operating\" status.\n\n            TBD: What will happen with the LPAR in that case (deactivated then\n            activated? nothing?)\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n          :exc:`~zhmcclient.StatusTimeout`: The timeout expired while\n            waiting for the desired LPAR status.\n        \"\"\"\n        body = {}\n        if load_address:\n            body['load-address'] = load_address\n        if load_parameter:\n            body['load-parameter'] = load_parameter\n        if force:\n            body['force'] = force\n        if not clear_indicator:\n            body['clear-indicator'] = clear_indicator\n        if store_status_indicator:\n            body['store-status-indicator'] = store_status_indicator\n        result = self.manager.session.post(\n            self.uri + '/operations/load',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            statuses = [\"operating\"]\n            if allow_status_exceptions:\n                statuses.append(\"exceptions\")\n            self.wait_for_status(statuses, status_timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping this LPAR, using the HMC operation \"Stop Logical Partition\". The stop operation stops the processors from processing instructions. This HMC operation has deferred status behavior: If the asynchronous job on the HMC is complete, it takes a few seconds until the LPAR status has reached the desired value. If `wait_for_completion=True`, this method repeatedly checks the status of the LPAR after the HMC operation has completed, and waits until the status is in the desired state \"operating\", or if `allow_status_exceptions` was set additionally in the state \"exceptions\". Authorization requirements: * Object-access permission to the CPC containing this LPAR. * Object-access permission to this LPAR. * Task permission for the \"Stop\" task. Parameters: wait_for_completion (bool): Boolean controlling whether this method should wait for completion of the requested asynchronous HMC operation, as follows: * If `True`, this method will wait for completion of the asynchronous job performing the operation, and for the status becoming \"operating\" (or in addition \"exceptions\", if `allow_status_exceptions` was set. * If `False`, this method will return immediately once the HMC has accepted the request to perform the operation. operation_timeout (:term:`number`): Timeout in seconds, for waiting for completion of the asynchronous job performing the operation. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.OperationTimeout` is raised. status_timeout (:term:`number`): Timeout in seconds, for waiting that the status of the LPAR has reached the desired status, after the HMC operation has completed. The special value 0 means that no timeout is set. `None` means that the default async operation timeout of the session is used. If the timeout expires when `wait_for_completion=True`, a :exc:`~zhmcclient.StatusTimeout` is raised. allow_status_exceptions (bool): Boolean controlling whether LPAR status \"exceptions\" is considered an additional acceptable end status when `wait_for_completion` is set. Returns: `None` or :class:`~zhmcclient.Job`: If `wait_for_completion` is `True`, returns `None`. If `wait_for_completion` is `False`, returns a :class:`~zhmcclient.Job` object representing the asynchronously executing job on the HMC. Raises: :exc:`~zhmcclient.HTTPError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.AuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.OperationTimeout`: The timeout expired while waiting for completion of the operation. :exc:`~zhmcclient.StatusTimeout`: The timeout expired while waiting for the desired LPAR status.", "response": "def stop(self, wait_for_completion=True, operation_timeout=None,\n             status_timeout=None, allow_status_exceptions=False):\n        \"\"\"\n        Stop this LPAR, using the HMC operation \"Stop Logical\n        Partition\". The stop operation stops the processors from\n        processing instructions.\n\n        This HMC operation has deferred status behavior: If the asynchronous\n        job on the HMC is complete, it takes a few seconds until the LPAR\n        status has reached the desired value. If `wait_for_completion=True`,\n        this method repeatedly checks the status of the LPAR after the HMC\n        operation has completed, and waits until the status is in the desired\n        state \"operating\", or if `allow_status_exceptions` was\n        set additionally in the state \"exceptions\".\n\n        Authorization requirements:\n\n        * Object-access permission to the CPC containing this LPAR.\n        * Object-access permission to this LPAR.\n        * Task permission for the \"Stop\" task.\n\n        Parameters:\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation, as follows:\n\n            * If `True`, this method will wait for completion of the\n              asynchronous job performing the operation, and for the status\n              becoming \"operating\" (or in addition \"exceptions\", if\n              `allow_status_exceptions` was set.\n\n            * If `False`, this method will return immediately once the HMC has\n              accepted the request to perform the operation.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, for waiting for completion of the asynchronous\n            job performing the operation. The special value 0 means that no\n            timeout is set. `None` means that the default async operation\n            timeout of the session is used. If the timeout expires when\n            `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised.\n\n          status_timeout (:term:`number`):\n            Timeout in seconds, for waiting that the status of the LPAR has\n            reached the desired status, after the HMC operation has completed.\n            The special value 0 means that no timeout is set. `None` means that\n            the default async operation timeout of the session is used.\n            If the timeout expires when `wait_for_completion=True`, a\n            :exc:`~zhmcclient.StatusTimeout` is raised.\n\n          allow_status_exceptions (bool):\n            Boolean controlling whether LPAR status \"exceptions\" is considered\n            an additional acceptable end status when `wait_for_completion` is\n            set.\n\n        Returns:\n\n          `None` or :class:`~zhmcclient.Job`:\n\n            If `wait_for_completion` is `True`, returns `None`.\n\n            If `wait_for_completion` is `False`, returns a\n            :class:`~zhmcclient.Job` object representing the asynchronously\n            executing job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.AuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the operation.\n          :exc:`~zhmcclient.StatusTimeout`: The timeout expired while\n            waiting for the desired LPAR status.\n        \"\"\"\n        body = {}\n        result = self.manager.session.post(\n            self.uri + '/operations/stop',\n            body,\n            wait_for_completion=wait_for_completion,\n            operation_timeout=operation_timeout)\n        if wait_for_completion:\n            statuses = [\"operating\"]\n            if allow_status_exceptions:\n                statuses.append(\"exceptions\")\n            self.wait_for_status(statuses, status_timeout)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a requests. exceptions. RequestException exception that was passed to the server.", "response": "def _handle_request_exc(exc, retry_timeout_config):\n    \"\"\"\n    Handle a :exc:`request.exceptions.RequestException` exception that was\n    raised.\n    \"\"\"\n    if isinstance(exc, requests.exceptions.ConnectTimeout):\n        raise ConnectTimeout(_request_exc_message(exc), exc,\n                             retry_timeout_config.connect_timeout,\n                             retry_timeout_config.connect_retries)\n    elif isinstance(exc, requests.exceptions.ReadTimeout):\n        raise ReadTimeout(_request_exc_message(exc), exc,\n                          retry_timeout_config.read_timeout,\n                          retry_timeout_config.read_retries)\n    elif isinstance(exc, requests.exceptions.RetryError):\n        raise RetriesExceeded(_request_exc_message(exc), exc,\n                              retry_timeout_config.connect_retries)\n    else:\n        raise ConnectionError(_request_exc_message(exc), exc)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _request_exc_message(exc):\n    if exc.args:\n        if isinstance(exc.args[0], Exception):\n            org_exc = exc.args[0]\n            if isinstance(org_exc, urllib3.exceptions.MaxRetryError):\n                reason_exc = org_exc.reason\n                message = str(reason_exc)\n            else:\n                message = str(org_exc.args[0])\n        else:\n            message = str(exc.args[0])\n\n        # Eliminate useless object repr at begin of the message\n        m = re.match(r'^(\\(<[^>]+>, \\'(.*)\\'\\)|<[^>]+>: (.*))$', message)\n        if m:\n            message = m.group(2) or m.group(3)\n    else:\n        message = \"\"\n    return message", "response": "Return a reasonable exception message from the specified exception."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _text_repr(text, max_len=1000):\n    if text is None:\n        text_repr = 'None'\n    elif len(text) > max_len:\n        text_repr = repr(text[0:max_len]) + '...'\n    else:\n        text_repr = repr(text)\n    return text_repr", "response": "Return the input text as a Python string representation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the HTTP response object as a Python dict.", "response": "def _result_object(result):\n    \"\"\"\n    Return the JSON payload in the HTTP response as a Python dict.\n\n    Parameters:\n        result (requests.Response): HTTP response object.\n\n    Raises:\n        zhmcclient.ParseError: Error parsing the returned JSON.\n    \"\"\"\n    content_type = result.headers.get('content-type', None)\n\n    if content_type is None or content_type.startswith('application/json'):\n        # This function is only called when there is content expected.\n        # Therefore, a response without content will result in a ParseError.\n        try:\n            return result.json(object_pairs_hook=OrderedDict)\n        except ValueError as exc:\n            raise ParseError(\n                \"JSON parse error in HTTP response: {}. \"\n                \"HTTP request: {} {}. \"\n                \"Response status {}. \"\n                \"Response content-type: {!r}. \"\n                \"Content (max.1000, decoded using {}): {}\".\n                format(exc.args[0],\n                       result.request.method, result.request.url,\n                       result.status_code, content_type, result.encoding,\n                       _text_repr(result.text, 1000)))\n    elif content_type.startswith('text/html'):\n        # We are in some error situation. The HMC returns HTML content\n        # for some 5xx status codes. We try to deal with it somehow,\n        # but we are not going as far as real HTML parsing.\n        m = re.search(r'charset=([^;,]+)', content_type)\n        if m:\n            encoding = m.group(1)  # e.g. RFC \"ISO-8859-1\"\n        else:\n            encoding = 'utf-8'\n        try:\n            html_uni = result.content.decode(encoding)\n        except LookupError:\n            html_uni = result.content.decode()\n\n        # We convert to one line to be regexp-friendly.\n        html_oneline = html_uni.replace('\\r\\n', '\\\\n').replace('\\r', '\\\\n').\\\n            replace('\\n', '\\\\n')\n\n        # Check for some well-known errors:\n        if re.search(r'javax\\.servlet\\.ServletException: '\n                     r'Web Services are not enabled\\.', html_oneline):\n            html_title = \"Console Configuration Error\"\n            html_details = \"Web Services API is not enabled on the HMC.\"\n            html_reason = HTML_REASON_WEB_SERVICES_DISABLED\n        else:\n            m = re.search(\n                r'<title>([^<]*)</title>.*'\n                r'<h2>Details:</h2>(.*)(<hr size=\"1\" noshade>)?</body>',\n                html_oneline)\n            if m:\n                html_title = m.group(1)\n                # Spend a reasonable effort to make the HTML readable:\n                html_details = m.group(2).replace('<p>', '\\\\n').\\\n                    replace('<br>', '\\\\n').replace('\\\\n\\\\n', '\\\\n').strip()\n            else:\n                html_title = \"Console Internal Error\"\n                html_details = \"Response body: {!r}\".format(html_uni)\n            html_reason = HTML_REASON_OTHER\n        message = \"{}: {}\".format(html_title, html_details)\n\n        # We create a minimal JSON error object (to the extent we use it\n        # when processing it):\n        result_obj = {\n            'http-status': result.status_code,\n            'reason': html_reason,\n            'message': message,\n            'request-uri': result.request.url,\n            'request-method': result.request.method,\n        }\n        return result_obj\n    elif content_type.startswith('application/vnd.ibm-z-zmanager-metrics'):\n        content_bytes = result.content\n        assert isinstance(content_bytes, six.binary_type)\n        return content_bytes.decode('utf-8')  # as a unicode object\n    else:\n        raise ParseError(\n            \"Unknown content type in HTTP response: {}. \"\n            \"HTTP request: {} {}. \"\n            \"Response status {}. \"\n            \"Response content-type: {!r}. \"\n            \"Content (max.1000, decoded using {}): {}\".\n            format(content_type,\n                   result.request.method, result.request.url,\n                   result.status_code, content_type, result.encoding,\n                   _text_repr(result.text, 1000)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef override_with(self, override_config):\n        ret = RetryTimeoutConfig()\n        for attr in RetryTimeoutConfig._attrs:\n            value = getattr(self, attr)\n            if override_config and getattr(override_config, attr) is not None:\n                value = getattr(override_config, attr)\n            setattr(ret, attr, value)\n        return ret", "response": "Returns a new configuration object that represents the configuration object that is the same as this one but with the specified configuration object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a boolean indicating whether the session is currently logged on.", "response": "def is_logon(self, verify=False):\n        \"\"\"\n        Return a boolean indicating whether the session is currently logged on\n        to the HMC.\n\n        By default, this method checks whether there is a session-id set\n        and considers that sufficient for determining that the session is\n        logged on. The `verify` parameter can be used to verify the validity\n        of a session-id that is already set, by issuing a dummy operation\n        (\"Get Console Properties\") to the HMC.\n\n        Parameters:\n\n          verify (bool): If a session-id is already set, verify its validity.\n        \"\"\"\n        if self._session_id is None:\n            return False\n        if verify:\n            try:\n                self.get('/api/console', logon_required=True)\n            except ServerAuthError:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging on, unconditionally. This can be used to re-logon. This requires credentials to be provided. Raises: :exc:`~zhmcclient.ClientAuthError` :exc:`~zhmcclient.ServerAuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.HTTPError`", "response": "def _do_logon(self):\n        \"\"\"\n        Log on, unconditionally. This can be used to re-logon.\n        This requires credentials to be provided.\n\n        Raises:\n\n          :exc:`~zhmcclient.ClientAuthError`\n          :exc:`~zhmcclient.ServerAuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.HTTPError`\n        \"\"\"\n        if self._userid is None:\n            raise ClientAuthError(\"Userid is not provided.\")\n        if self._password is None:\n            if self._get_password:\n                self._password = self._get_password(self._host, self._userid)\n            else:\n                raise ClientAuthError(\"Password is not provided.\")\n        logon_uri = '/api/sessions'\n        logon_body = {\n            'userid': self._userid,\n            'password': self._password\n        }\n        self._headers.pop('X-API-Session', None)  # Just in case\n        self._session = self._new_session(self.retry_timeout_config)\n        logon_res = self.post(logon_uri, logon_body, logon_required=False)\n        self._session_id = logon_res['api-session']\n        self._headers['X-API-Session'] = self._session_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a new requests. Session object.", "response": "def _new_session(retry_timeout_config):\n        \"\"\"\n        Return a new `requests.Session` object.\n        \"\"\"\n        retry = requests.packages.urllib3.Retry(\n            total=None,\n            connect=retry_timeout_config.connect_retries,\n            read=retry_timeout_config.read_retries,\n            method_whitelist=retry_timeout_config.method_whitelist,\n            redirect=retry_timeout_config.max_redirects)\n        session = requests.Session()\n        session.mount('https://',\n                      requests.adapters.HTTPAdapter(max_retries=retry))\n        session.mount('http://',\n                      requests.adapters.HTTPAdapter(max_retries=retry))\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging off, unconditionally. Raises: :exc:`~zhmcclient.ServerAuthError` :exc:`~zhmcclient.ConnectionError` :exc:`~zhmcclient.ParseError` :exc:`~zhmcclient.HTTPError`", "response": "def _do_logoff(self):\n        \"\"\"\n        Log off, unconditionally.\n\n        Raises:\n\n          :exc:`~zhmcclient.ServerAuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.HTTPError`\n        \"\"\"\n        session_uri = '/api/sessions/this-session'\n        self.delete(session_uri, logon_required=False)\n        self._session_id = None\n        self._session = None\n        self._headers.pop('X-API-Session', None)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog the HTTP request.", "response": "def _log_http_request(method, url, headers=None, content=None):\n        \"\"\"\n        Log the HTTP request of an HMC REST API call, at the debug level.\n\n        Parameters:\n\n          method (:term:`string`): HTTP method name in upper case, e.g. 'GET'\n\n          url (:term:`string`): HTTP URL (base URL and operation URI)\n\n          headers (iterable): HTTP headers used for the request\n\n          content (:term:`string`): HTTP body (aka content) used for the\n            request\n        \"\"\"\n        if method == 'POST' and url.endswith('/api/sessions'):\n            # In Python 3 up to 3.5, json.loads() requires unicode strings.\n            if sys.version_info[0] == 3 and sys.version_info[1] in (4, 5) and \\\n                    isinstance(content, six.binary_type):\n                content = content.decode('utf-8')\n            # Because zhmcclient has built the request, we are not handling\n            # any JSON parse errors from json.loads().\n            content_dict = json.loads(content)\n            content_dict['password'] = '********'\n            content = json.dumps(content_dict)\n        if headers and 'X-API-Session' in headers:\n            headers = headers.copy()\n            headers['X-API-Session'] = '********'\n        HMC_LOGGER.debug(\"Request: %s %s, headers: %r, \"\n                         \"content(max.1000): %.1000r\",\n                         method, url, headers, content)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging the HTTP response of an HMC REST API call at the debug level.", "response": "def _log_http_response(method, url, status, headers=None, content=None):\n        \"\"\"\n        Log the HTTP response of an HMC REST API call, at the debug level.\n\n        Parameters:\n\n          method (:term:`string`): HTTP method name in upper case, e.g. 'GET'\n\n          url (:term:`string`): HTTP URL (base URL and operation URI)\n\n          status (integer): HTTP status code\n\n          headers (iterable): HTTP headers returned in the response\n\n          content (:term:`string`): HTTP body (aka content) returned in the\n            response\n        \"\"\"\n        if method == 'POST' and url.endswith('/api/sessions'):\n            # In Python 3 up to 3.5, json.loads() requires unicode strings.\n            if sys.version_info[0] == 3 and sys.version_info[1] in (4, 5) and \\\n                    isinstance(content, six.binary_type):\n                content = content.decode('utf-8')\n            try:\n                content_dict = json.loads(content)\n            except ValueError as exc:\n                content = '\"Error: Cannot parse JSON payload of response: ' \\\n                    '{}\"'.format(exc)\n            else:\n                content_dict['api-session'] = '********'\n                content_dict['session-credential'] = '********'\n                content = json.dumps(content_dict)\n        if headers and 'X-API-Session' in headers:\n            headers = headers.copy()\n            headers['X-API-Session'] = '********'\n        HMC_LOGGER.debug(\"Respons: %s %s, status: %s, headers: %r, \"\n                         \"content(max.1000): %.1000r\",\n                         method, url, status, headers, content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms a HTTP POST request against the resource identified by a URI.", "response": "def post(self, uri, body=None, logon_required=True,\n             wait_for_completion=False, operation_timeout=None):\n        \"\"\"\n        Perform the HTTP POST method against the resource identified by a URI,\n        using a provided request body.\n\n        A set of standard HTTP headers is automatically part of the request.\n\n        HMC operations using HTTP POST are either synchronous or asynchronous.\n        Asynchronous operations return the URI of an asynchronously executing\n        job that can be queried for status and result.\n\n        Examples for synchronous operations:\n\n        * With no result: \"Logon\", \"Update CPC Properties\"\n        * With a result: \"Create Partition\"\n\n        Examples for asynchronous operations:\n\n        * With no result: \"Start Partition\"\n\n        The `wait_for_completion` parameter of this method can be used to deal\n        with asynchronous HMC operations in a synchronous way.\n\n        If executing the operation reveals that the HMC session token is\n        expired, this method re-logs on and retries the operation.\n\n        The timeout and retry\n\n        Parameters:\n\n          uri (:term:`string`):\n            Relative URI path of the resource, e.g. \"/api/session\".\n            This URI is relative to the base URL of the session (see the\n            :attr:`~zhmcclient.Session.base_url` property).\n            Must not be `None`.\n\n          body (:term:`json object`):\n            JSON object to be used as the HTTP request body (payload).\n            `None` means the same as an empty dictionary, namely that no HTTP\n            body is included in the request.\n\n          logon_required (bool):\n            Boolean indicating whether the operation requires that the session\n            is logged on to the HMC. For example, the \"Logon\" operation does\n            not require that.\n\n          wait_for_completion (bool):\n            Boolean controlling whether this method should wait for completion\n            of the requested asynchronous HMC operation.\n\n            A value of `True` will cause an additional entry in the time\n            statistics to be created that represents the entire asynchronous\n            operation including the waiting for its completion.\n            That time statistics entry will have a URI that is the targeted\n            URI, appended with \"+completion\".\n\n            For synchronous HMC operations, this parameter has no effect on\n            the operation execution or on the return value of this method, but\n            it should still be set (or defaulted) to `False` in order to avoid\n            the additional entry in the time statistics.\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, when waiting for completion of an asynchronous\n            operation. The special value 0 means that no timeout is set. `None`\n            means that the default async operation timeout of the session is\n            used.\n\n            For `wait_for_completion=True`, a\n            :exc:`~zhmcclient.OperationTimeout` is raised when the timeout\n            expires.\n\n            For `wait_for_completion=False`, this parameter has no effect.\n\n        Returns:\n\n          : A :term:`json object` or `None` or a :class:`~zhmcclient.Job`\n          object, as follows:\n\n          * For synchronous HMC operations, and for asynchronous HMC\n            operations with `wait_for_completion=True`:\n\n            If this method returns, the HMC operation has completed\n            successfully (otherwise, an exception is raised).\n            For asynchronous HMC operations, the associated job has been\n            deleted.\n\n            The return value is the result of the HMC operation as a\n            :term:`json object`, or `None` if the operation has no result.\n            See the section in the :term:`HMC API` book about the specific\n            HMC operation for a description of the members of the returned\n            JSON object.\n\n          * For asynchronous HMC operations with `wait_for_completion=False`:\n\n            If this method returns, the asynchronous execution of the HMC\n            operation has been started successfully as a job on the HMC (if\n            the operation could not be started, an exception is raised).\n\n            The return value is a :class:`~zhmcclient.Job` object\n            representing the job on the HMC.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.ClientAuthError`\n          :exc:`~zhmcclient.ServerAuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for completion of the asynchronous operation.\n          :exc:`TypeError`: Body has invalid type.\n        \"\"\"\n        if logon_required:\n            self.logon()\n        url = self.base_url + uri\n        headers = self.headers.copy()  # Standard headers\n\n        if body is None:\n            data = None\n        elif isinstance(body, dict):\n            data = json.dumps(body)\n            # Content-type is already set in standard headers.\n        elif isinstance(body, six.text_type):\n            data = body.encode('utf-8')\n            headers['Content-type'] = 'application/octet-stream'\n        elif isinstance(body, six.binary_type):\n            data = body\n            headers['Content-type'] = 'application/octet-stream'\n        elif isinstance(body, collections.Iterable):\n            # For example, open files: open(), io.open()\n            data = body\n            headers['Content-type'] = 'application/octet-stream'\n        else:\n            raise TypeError(\"Body has invalid type: {}\".format(type(body)))\n\n        self._log_http_request('POST', url, headers=headers, content=data)\n        req = self._session or requests\n        req_timeout = (self.retry_timeout_config.connect_timeout,\n                       self.retry_timeout_config.read_timeout)\n        if wait_for_completion:\n            stats_total = self.time_stats_keeper.get_stats(\n                'post ' + uri + '+completion')\n            stats_total.begin()\n        try:\n            stats = self.time_stats_keeper.get_stats('post ' + uri)\n            stats.begin()\n            try:\n                if data is None:\n                    result = req.post(url, headers=headers,\n                                      verify=False, timeout=req_timeout)\n                else:\n                    result = req.post(url, data=data, headers=headers,\n                                      verify=False, timeout=req_timeout)\n            except requests.exceptions.RequestException as exc:\n                _handle_request_exc(exc, self.retry_timeout_config)\n            finally:\n                stats.end()\n            self._log_http_response('POST', url,\n                                    status=result.status_code,\n                                    headers=result.headers,\n                                    content=result.content)\n\n            if result.status_code in (200, 201):\n                return _result_object(result)\n            elif result.status_code == 204:\n                # No content\n                return None\n            elif result.status_code == 202:\n                if result.content == '':\n                    # Some operations (e.g. \"Restart Console\",\n                    # \"Shutdown Console\" or \"Cancel Job\") return 202\n                    # with no response content.\n                    return None\n                else:\n                    # This is the most common case to return 202: An\n                    # asynchronous job has been started.\n                    result_object = _result_object(result)\n                    job_uri = result_object['job-uri']\n                    job = Job(self, job_uri, 'POST', uri)\n                    if wait_for_completion:\n                        return job.wait_for_completion(operation_timeout)\n                    else:\n                        return job\n            elif result.status_code == 403:\n                result_object = _result_object(result)\n                reason = result_object.get('reason', None)\n                if reason == 5:\n                    # API session token expired: re-logon and retry\n                    self._do_logon()\n                    return self.post(uri, body, logon_required)\n                else:\n                    msg = result_object.get('message', None)\n                    raise ServerAuthError(\"HTTP authentication failed: {}\".\n                                          format(msg),\n                                          HTTPError(result_object))\n            else:\n                result_object = _result_object(result)\n                raise HTTPError(result_object)\n        finally:\n            if wait_for_completion:\n                stats_total.end()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef delete(self, uri, logon_required=True):\n        if logon_required:\n            self.logon()\n        url = self.base_url + uri\n        self._log_http_request('DELETE', url, headers=self.headers)\n        stats = self.time_stats_keeper.get_stats('delete ' + uri)\n        stats.begin()\n        req = self._session or requests\n        req_timeout = (self.retry_timeout_config.connect_timeout,\n                       self.retry_timeout_config.read_timeout)\n        try:\n            result = req.delete(url, headers=self.headers, verify=False,\n                                timeout=req_timeout)\n        except requests.exceptions.RequestException as exc:\n            _handle_request_exc(exc, self.retry_timeout_config)\n        finally:\n            stats.end()\n        self._log_http_response('DELETE', url,\n                                status=result.status_code,\n                                headers=result.headers,\n                                content=result.content)\n\n        if result.status_code in (200, 204):\n            return\n        elif result.status_code == 403:\n            result_object = _result_object(result)\n            reason = result_object.get('reason', None)\n            if reason == 5:\n                # API session token expired: re-logon and retry\n                self._do_logon()\n                self.delete(uri, logon_required)\n                return\n            else:\n                msg = result_object.get('message', None)\n                raise ServerAuthError(\"HTTP authentication failed: {}\".\n                                      format(msg), HTTPError(result_object))\n        else:\n            result_object = _result_object(result)\n            raise HTTPError(result_object)", "response": "Perform a HTTP DELETE request against the resource identified by uri."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_for_completion(self):\n        job_result_obj = self.session.get(self.uri)\n        job_status = job_result_obj['status']\n        if job_status == 'complete':\n            self.session.delete(self.uri)\n            op_status_code = job_result_obj['job-status-code']\n            if op_status_code in (200, 201):\n                op_result_obj = job_result_obj.get('job-results', None)\n            elif op_status_code == 204:\n                # No content\n                op_result_obj = None\n            else:\n                error_result_obj = job_result_obj.get('job-results', None)\n                if not error_result_obj:\n                    message = None\n                elif 'message' in error_result_obj:\n                    message = error_result_obj['message']\n                elif 'error' in error_result_obj:\n                    message = error_result_obj['error']\n                else:\n                    message = None\n                error_obj = {\n                    'http-status': op_status_code,\n                    'reason': job_result_obj['job-reason-code'],\n                    'message': message,\n                    'request-method': self.op_method,\n                    'request-uri': self.op_uri,\n                }\n                raise HTTPError(error_obj)\n        else:\n            op_result_obj = None\n        return job_status, op_result_obj", "response": "Check once for completion of the job and return completion status and result if it has completed."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for completion of the job on the HMC and delete the job on the HMC and return the result of the original asynchronous HMC operation.", "response": "def wait_for_completion(self, operation_timeout=None):\n        \"\"\"\n        Wait for completion of the job, then delete the job on the HMC and\n        return the result of the original asynchronous HMC operation, if it\n        completed successfully.\n\n        If the job completed in error, an :exc:`~zhmcclient.HTTPError`\n        exception is raised.\n\n        Parameters:\n\n          operation_timeout (:term:`number`):\n            Timeout in seconds, when waiting for completion of the job. The\n            special value 0 means that no timeout is set. `None` means that the\n            default async operation timeout of the session is used.\n\n            If the timeout expires, a :exc:`~zhmcclient.OperationTimeout`\n            is raised.\n\n            This method gives completion of the job priority over strictly\n            achieving the timeout. This may cause a slightly longer duration of\n            the method than prescribed by the timeout.\n\n        Returns:\n\n          :term:`json object` or `None`:\n            The result of the original asynchronous operation that was\n            performed by the job, from the ``job-results`` field of the\n            response body of the \"Query Job Status\" HMC operation. That result\n            is a :term:`json object` as described for the asynchronous\n            operation, or `None` if the operation has no result.\n\n        Raises:\n\n          :exc:`~zhmcclient.HTTPError`: The job completed in error, or the job\n            status cannot be retrieved, or the job cannot be deleted.\n          :exc:`~zhmcclient.ParseError`\n          :exc:`~zhmcclient.ClientAuthError`\n          :exc:`~zhmcclient.ServerAuthError`\n          :exc:`~zhmcclient.ConnectionError`\n          :exc:`~zhmcclient.OperationTimeout`: The timeout expired while\n            waiting for job completion.\n        \"\"\"\n\n        if operation_timeout is None:\n            operation_timeout = \\\n                self.session.retry_timeout_config.operation_timeout\n        if operation_timeout > 0:\n            start_time = time.time()\n\n        while True:\n            job_status, op_result_obj = self.check_for_completion()\n\n            # We give completion of status priority over strictly achieving\n            # the timeout, so we check status first. This may cause a longer\n            # duration of the method than prescribed by the timeout.\n            if job_status == 'complete':\n                return op_result_obj\n\n            if operation_timeout > 0:\n                current_time = time.time()\n                if current_time > start_time + operation_timeout:\n                    raise OperationTimeout(\n                        \"Waiting for completion of job {} timed out \"\n                        \"(operation timeout: {} s)\".\n                        format(self.uri, operation_timeout),\n                        operation_timeout)\n\n            time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef logged_api_call(func):\n\n    # Note that in this decorator function, we are in a module loading context,\n    # where the decorated functions are being defined. When this decorator\n    # function is called, its call stack represents the definition of the\n    # decorated functions. Not all global definitions in the module have been\n    # defined yet, and methods of classes that are decorated with this\n    # decorator are still functions at this point (and not yet methods).\n\n    module = inspect.getmodule(func)\n    if not inspect.isfunction(func) or not hasattr(module, '__name__'):\n        raise TypeError(\"The @logged_api_call decorator must be used on a \"\n                        \"function or method (and not on top of the @property \"\n                        \"decorator)\")\n\n    try:\n        # We avoid the use of inspect.getouterframes() because it is slow,\n        # and use the pointers up the stack frame, instead.\n\n        this_frame = inspect.currentframe()  # this decorator function here\n        apifunc_frame = this_frame.f_back  # the decorated API function\n\n        apifunc_owner = inspect.getframeinfo(apifunc_frame)[2]\n\n    finally:\n        # Recommended way to deal with frame objects to avoid ref cycles\n        del this_frame\n        del apifunc_frame\n\n    # TODO: For inner functions, show all outer levels instead of just one.\n\n    if apifunc_owner == '<module>':\n        # The decorated API function is defined globally (at module level)\n        apifunc_str = '{func}()'.format(func=func.__name__)\n    else:\n        # The decorated API function is defined in a class or in a function\n        apifunc_str = '{owner}.{func}()'.format(owner=apifunc_owner,\n                                                func=func.__name__)\n\n    logger = get_logger(API_LOGGER_NAME)\n\n    def is_external_call():\n        \"\"\"\n        Return a boolean indicating whether the call to the decorated API\n        function is an external call (vs. b eing an internal call).\n        \"\"\"\n        try:\n            # We avoid the use of inspect.getouterframes() because it is slow,\n            # and use the pointers up the stack frame, instead.\n\n            log_it_frame = inspect.currentframe()  # this log_it() function\n            log_api_call_frame = log_it_frame.f_back  # the log_api_call() func\n            apifunc_frame = log_api_call_frame.f_back  # the decorated API func\n            apicaller_frame = apifunc_frame.f_back  # caller of API function\n            apicaller_module = inspect.getmodule(apicaller_frame)\n            if apicaller_module is None:\n                apicaller_module_name = \"<unknown>\"\n            else:\n                apicaller_module_name = apicaller_module.__name__\n        finally:\n            # Recommended way to deal with frame objects to avoid ref cycles\n            del log_it_frame\n            del log_api_call_frame\n            del apifunc_frame\n            del apicaller_frame\n            del apicaller_module\n\n        # Log only if the caller is not from the zhmcclient package\n        return apicaller_module_name.split('.')[0] != 'zhmcclient'\n\n    def log_api_call(func, *args, **kwargs):\n        \"\"\"\n        Log entry to and exit from the decorated function, at the debug level.\n\n        Note that this wrapper function is called every time the decorated\n        function/method is called, but that the log message only needs to be\n        constructed when logging for this logger and for this log level is\n        turned on. Therefore, we do as much as possible in the decorator\n        function, plus we use %-formatting and lazy interpolation provided by\n        the log functions, in order to save resources in this function here.\n\n        Parameters:\n\n          func (function object): The decorated function.\n\n          *args: Any positional arguments for the decorated function.\n\n          **kwargs: Any keyword arguments for the decorated function.\n        \"\"\"\n\n        # Note that in this function, we are in the context where the\n        # decorated function is actually called.\n\n        _log_it = is_external_call() and logger.isEnabledFor(logging.DEBUG)\n\n        if _log_it:\n            logger.debug(\"Called: {}, args: {:.500}, kwargs: {:.500}\".\n                         format(apifunc_str, log_escaped(repr(args)),\n                                log_escaped(repr(kwargs))))\n\n        result = func(*args, **kwargs)\n\n        if _log_it:\n            logger.debug(\"Return: {}, result: {:.1000}\".\n                         format(apifunc_str, log_escaped(repr(result))))\n\n        return result\n\n    if 'decorate' in globals():\n        return decorate(func, log_api_call)\n    else:\n        return decorator(log_api_call, func)", "response": "Decorator that causes the decorated API function or method to log the call stack to itself to a logger."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking whether this honeybadger configuration is a django request or not.", "response": "def supports(self, config, context):\n        \"\"\"\n        Check whether this is a django request or not.\n        :param config: honeybadger configuration.\n        :param context: current honeybadger configuration.\n        :return: True if this is a django request, False else.\n        \"\"\"\n        request = current_request()\n        return request is not None and re.match(r'^django\\.', request.__module__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the payload by checking Django request object.", "response": "def generate_payload(self, config, context):\n        \"\"\"\n        Generate payload by checking Django request object.\n        :param context: current context.\n        :param config: honeybadger configuration.\n        :return: a dict with the generated payload.\n        \"\"\"\n        request = current_request()\n\n        payload = {\n            'url': request.build_absolute_uri(),\n            'component': request.resolver_match.app_name,\n            'action': request.resolver_match.func.__name__,\n            'params': {},\n            'session': {},\n            'cgi_data': dict(request.META),\n            'context': context\n        }\n\n        if hasattr(request, 'session'):\n            payload['session'] = filter_dict(dict(request.session), config.params_filters)\n\n        payload['params'] = filter_dict(dict(getattr(request, request.method)), config.params_filters)\n\n        return payload"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generic_div(a, b):\n    logger.debug('Called generic_div({}, {})'.format(a, b))\n    return a / b", "response": "Simple function to divide two numbers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether we are in a django request context.", "response": "def supports(self, config, context):\n        \"\"\"\n        Check whether we are in a Flask request context.\n        :param config: honeybadger configuration.\n        :param context: current honeybadger configuration.\n        :return: True if this is a django request, False else.\n        \"\"\"\n        try:\n            from flask import request\n        except ImportError:\n            return False\n        else:\n            return bool(request)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_payload(self, config, context):\n        from flask import current_app, session, request as _request\n\n        current_view = current_app.view_functions[_request.endpoint]\n        if hasattr(current_view, 'view_class'):\n            component = '.'.join((current_view.__module__, current_view.view_class.__name__))\n        else:\n            component = current_view.__module__\n        cgi_data = {\n            k: v\n            for k, v in iteritems(_request.headers)\n        }\n        cgi_data.update({\n            'REQUEST_METHOD': _request.method\n        })\n        payload = {\n            'url': _request.base_url,\n            'component': component,\n            'action': _request.endpoint,\n            'params': {},\n            'session': filter_dict(dict(session), config.params_filters),\n            'cgi_data': cgi_data,\n            'context': context\n        }\n\n        # Add query params\n        params = filter_dict(dict(_request.args), config.params_filters)\n        params.update(filter_dict(dict(_request.form), config.params_filters))\n\n        payload['params'] = params\n\n        return payload", "response": "Generate the payload by checking Flask request object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing the Honeybadger application.", "response": "def init_app(self, app, report_exceptions=False, reset_context_after_request=False):\n        \"\"\"\n        Initialize honeybadger and listen for errors.\n        :param Flask app: the Flask application object.\n        :param bool report_exceptions: whether to automatically report exceptions raised by Flask on  requests\n         (i.e. by calling abort) or not.\n        :param bool reset_context_after_request: whether to reset honeybadger context after each request.\n        \"\"\"\n        from flask import request_tearing_down, got_request_exception\n\n        self.app = app\n\n        self.app.logger.info('Initializing Honeybadger')\n\n        self.report_exceptions = report_exceptions\n        self.reset_context_after_request = reset_context_after_request\n        self._initialize_honeybadger(app.config)\n\n        # Add hooks\n        if self.report_exceptions:\n            self._register_signal_handler('auto-reporting exceptions',\n                                          got_request_exception,\n                                          self._handle_exception)\n\n        if self.reset_context_after_request:\n            self._register_signal_handler('auto clear context on request end',\n                                          request_tearing_down,\n                                          self._reset_context)\n\n        logger.info('Honeybadger helper installed')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _register_signal_handler(self, description, signal, handler):\n        from flask import signals\n\n        if not signals.signals_available:\n            self.app.logger.warn('blinker needs to be installed in order to support %s'.format(description))\n        self.app.logger.info('Enabling {}'.format(description))\n        # Weak references won't work if handlers are methods rather than functions.\n        signal.connect(handler, sender=self.app, weak=False)", "response": "Register a handler for the given signal."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _initialize_honeybadger(self, config):\n        if config.get('DEBUG', False):\n            honeybadger.configure(environment='development')\n\n        honeybadger_config = {}\n        for key, value in iteritems(config):\n            if key.startswith(self.CONFIG_PREFIX):\n                honeybadger_config[key[len(self.CONFIG_PREFIX):].lower()] = value\n\n        honeybadger.configure(**honeybadger_config)\n        honeybadger.config.set_12factor_config()", "response": "Initializes the honeybadger with the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters the given plugin.", "response": "def register(self, plugin):\n        \"\"\"\n        Register the given plugin. Registration order is kept.\n        :param plugin: the plugin to register.\n        \"\"\"\n        if plugin.name not in self._registered:\n            logger.info('Registering plugin %s' % plugin.name)\n            self._registered[plugin.name] = plugin\n        else:\n            logger.warn('Plugin %s already registered' % plugin.name)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_payload(self, config=None, context=None):\n        for name, plugin in iteritems(self._registered):\n            if plugin.supports(config, context):\n                logger.debug('Returning payload from plugin %s' % name)\n                return plugin.generate_payload(config, context)\n\n        logger.debug('No active plugin to generate payload')\n        return {\n            'context': context\n        }", "response": "Generate payload by iterating over registered plugins and returning the payload."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ascii(graph):\n    from .._ascii import DAG\n    from .._echo import echo_via_pager\n\n    echo_via_pager(str(DAG(graph)))", "response": "Format graph as an ASCII art."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn formatted graph in JSON - LD format function.", "response": "def _jsonld(graph, format, *args, **kwargs):\n    \"\"\"Return formatted graph in JSON-LD ``format`` function.\"\"\"\n    import json\n\n    from pyld import jsonld\n    from renku.models._jsonld import asjsonld\n\n    output = getattr(jsonld, format)([\n        asjsonld(action) for action in graph.activities.values()\n    ])\n    return json.dumps(output, indent=2)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats graph as a dot file.", "response": "def dot(graph, simple=True, debug=False, landscape=False):\n    \"\"\"Format graph as a dot file.\"\"\"\n    import sys\n\n    from rdflib import ConjunctiveGraph\n    from rdflib.plugin import register, Parser\n    from rdflib.tools.rdf2dot import rdf2dot\n\n    register('json-ld', Parser, 'rdflib_jsonld.parser', 'JsonLDParser')\n\n    g = ConjunctiveGraph().parse(\n        data=_jsonld(graph, 'expand'),\n        format='json-ld',\n    )\n\n    g.bind('prov', 'http://www.w3.org/ns/prov#')\n    g.bind('wfdesc', 'http://purl.org/wf4ever/wfdesc#')\n    g.bind('wf', 'http://www.w3.org/2005/01/wf/flow#')\n    g.bind('wfprov', 'http://purl.org/wf4ever/wfprov#')\n\n    if debug:\n        rdf2dot(g, sys.stdout)\n        return\n\n    sys.stdout.write('digraph { \\n node [ fontname=\"DejaVu Sans\" ] ; \\n ')\n    if landscape:\n        sys.stdout.write('rankdir=\"LR\" \\n')\n    if simple:\n        _rdf2dot_simple(g, sys.stdout)\n        return\n    _rdf2dot_reduced(g, sys.stdout)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a simple graph of processes and artifacts.", "response": "def _rdf2dot_simple(g, stream):\n    \"\"\"Create a simple graph of processes and artifacts.\"\"\"\n    from itertools import chain\n\n    import re\n\n    path_re = re.compile(\n        r'file:///(?P<type>[a-zA-Z]+)/'\n        r'(?P<commit>\\w+)'\n        r'(?P<path>.+)?'\n    )\n\n    inputs = g.query(\n        \"\"\"\n        SELECT ?input ?role ?activity ?comment\n        WHERE {\n            ?activity (prov:qualifiedUsage/prov:entity) ?input .\n            ?activity prov:qualifiedUsage ?qual .\n            ?qual prov:hadRole ?role .\n            ?qual prov:entity ?input .\n            ?qual rdf:type ?type .\n            ?activity rdf:type wfprov:ProcessRun .\n            ?activity rdfs:comment ?comment .\n            FILTER NOT EXISTS {?activity rdf:type wfprov:WorkflowRun}\n        }\n        \"\"\"\n    )\n    outputs = g.query(\n        \"\"\"\n        SELECT ?activity ?role ?output ?comment\n        WHERE {\n            ?output (prov:qualifiedGeneration/prov:activity) ?activity .\n            ?output prov:qualifiedGeneration ?qual .\n            ?qual prov:hadRole ?role .\n            ?qual prov:activity ?activity .\n            ?qual rdf:type ?type .\n            ?activity rdf:type wfprov:ProcessRun ;\n                      rdfs:comment ?comment .\n            FILTER NOT EXISTS {?activity rdf:type wfprov:WorkflowRun}\n        }\n        \"\"\"\n    )\n\n    activity_nodes = {}\n    artifact_nodes = {}\n    for source, role, target, comment, in chain(inputs, outputs):\n        # extract the pieces of the process URI\n        src_path = path_re.match(source).groupdict()\n        tgt_path = path_re.match(target).groupdict()\n\n        # write the edge\n        stream.write(\n            '\\t\"{src_commit}:{src_path}\" -> '\n            '\"{tgt_commit}:{tgt_path}\" '\n            '[label={role}] \\n'.format(\n                src_commit=src_path['commit'][:5],\n                src_path=src_path.get('path') or '',\n                tgt_commit=tgt_path['commit'][:5],\n                tgt_path=tgt_path.get('path') or '',\n                role=role\n            )\n        )\n        if src_path.get('type') == 'commit':\n            activity_nodes.setdefault(source, {'comment': comment})\n            artifact_nodes.setdefault(target, {})\n        if tgt_path.get('type') == 'commit':\n            activity_nodes.setdefault(target, {'comment': comment})\n            artifact_nodes.setdefault(source, {})\n\n    # customize the nodes\n    for node, content in activity_nodes.items():\n        node_path = path_re.match(node).groupdict()\n        stream.write(\n            '\\t\"{commit}:{path}\" '\n            '[shape=box label=\"#{commit}:{path}:{comment}\"] \\n'.format(\n                comment=content['comment'],\n                commit=node_path['commit'][:5],\n                path=node_path.get('path') or ''\n            )\n        )\n    for node, content in artifact_nodes.items():\n        node_path = path_re.match(node).groupdict()\n        stream.write(\n            '\\t\"{commit}:{path}\" '\n            '[label=\"#{commit}:{path}\"] \\n'.format(\n                commit=node_path['commit'][:5],\n                path=node_path.get('path') or ''\n            )\n        )\n    stream.write('}\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a reduced graph into a DOT representation.", "response": "def _rdf2dot_reduced(g, stream):\n    \"\"\"\n    A reduced dot graph.\n\n    Adapted from original source:\n    https://rdflib.readthedocs.io/en/stable/_modules/rdflib/tools/rdf2dot.html\n    \"\"\"\n    import cgi\n    import collections\n\n    import rdflib\n\n    from rdflib.tools.rdf2dot import LABEL_PROPERTIES, NODECOLOR\n\n    types = collections.defaultdict(set)\n    fields = collections.defaultdict(set)\n    nodes = {}\n\n    def node(x):\n        \"\"\"Return a name of the given node.\"\"\"\n        return nodes.setdefault(x, 'node{0}'.format(len(nodes)))\n\n    def label(x, g):\n        \"\"\"Generate a label for the node.\"\"\"\n        for labelProp in LABEL_PROPERTIES:\n            label_ = g.value(x, labelProp)\n            if label_:\n                return label_\n\n        try:\n            return g.namespace_manager.compute_qname(x)[2]\n        except Exception:\n            return x\n\n    def formatliteral(l, g):\n        \"\"\"Format and escape literal.\"\"\"\n        v = cgi.escape(l)\n        if l.datatype:\n            return '&quot;%s&quot;^^%s' % (v, qname(l.datatype, g))\n        elif l.language:\n            return '&quot;%s&quot;@%s' % (v, l.language)\n        return '&quot;%s&quot;' % v\n\n    def qname(x, g):\n        \"\"\"Compute qname.\"\"\"\n        try:\n            q = g.compute_qname(x)\n            return q[0] + ':' + q[2]\n        except Exception:\n            return x\n\n    def color(p):\n        \"\"\"Choose node color.\"\"\"\n        return 'BLACK'\n\n    for s, p, o in g:\n        sn = node(s)\n        if p == rdflib.RDFS.label:\n            continue\n\n        # inject the type predicate into the node itself\n        if p == rdflib.RDF.type:\n            types[sn].add((qname(p, g), cgi.escape(o)))\n            continue\n        if p == rdflib.term.URIRef('http://purl.org/dc/terms/isPartOf'):\n            fields[sn].add((qname(p, g), cgi.escape(o)))\n            continue\n        if p == rdflib.term.URIRef('http://www.w3.org/ns/prov#wasInformedBy'):\n            continue\n\n        if isinstance(o, (rdflib.URIRef, rdflib.BNode)):\n            on = node(o)\n            opstr = (\n                '\\t%s -> %s [ color=%s, label=< <font point-size=\"12\" '\n                'color=\"#336633\">%s</font> > ] ;\\n'\n            )\n            stream.write(opstr % (sn, on, color(p), qname(p, g)))\n        else:\n            fields[sn].add((qname(p, g), formatliteral(o, g)))\n\n    for u, n in nodes.items():\n        stream.write(u\"# %s %s\\n\" % (u, n))\n        f = [\n            '<tr><td align=\"left\"><b>%s</b></td><td align=\"left\">'\n            '<b>%s</b></td></tr>' % x for x in sorted(types[n])\n        ]\n        f += [\n            '<tr><td align=\"left\">%s</td><td align=\"left\">%s</td></tr>' % x\n            for x in sorted(fields[n])\n        ]\n        opstr = (\n            '%s [ shape=none, color=%s label=< <table color=\"#666666\"'\n            ' cellborder=\"0\" cellspacing=\"0\" border=\"1\"><tr>'\n            '<td colspan=\"2\" bgcolor=\"grey\"><B>%s</B></td></tr><tr>'\n            '<td href=\"%s\" bgcolor=\"#eeeeee\" colspan=\"2\">'\n            '<font point-size=\"12\" color=\"#6666ff\">%s</font></td>'\n            '</tr>%s</table> > ] \\n'\n        )\n        stream.write(opstr % (n, NODECOLOR, label(u, g), u, u, ''.join(f)))\n\n    stream.write('}\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef makefile(graph):\n    from renku.models.provenance.activities import ProcessRun, WorkflowRun\n\n    for activity in graph.activities.values():\n        if not isinstance(activity, ProcessRun):\n            continue\n        elif isinstance(activity, WorkflowRun):\n            steps = activity.subprocesses.values()\n        else:\n            steps = [activity]\n\n        for step in steps:\n            click.echo(' '.join(step.outputs) + ': ' + ' '.join(step.inputs))\n            tool = step.process\n            click.echo(\n                '\\t@' + ' '.join(tool.to_argv()) + ' ' + ' '.join(\n                    tool.STD_STREAMS_REPR[key] + ' ' + str(path)\n                    for key, path in tool._std_streams().items()\n                )\n            )", "response": "Format graph as Makefile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef nt(graph):\n    from rdflib import ConjunctiveGraph\n    from rdflib.plugin import register, Parser\n\n    register('json-ld', Parser, 'rdflib_jsonld.parser', 'JsonLDParser')\n\n    click.echo(\n        ConjunctiveGraph().parse(\n            data=_jsonld(graph, 'expand'),\n            format='json-ld',\n        ).serialize(format='nt')\n    )", "response": "Format graph as n - tuples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tabulate(collection, headers, datetime_fmt='%Y-%m-%d %H:%M:%S', **kwargs):\n    if isinstance(headers, dict):\n        attrs = headers.keys()\n        # if mapping is not specified keep original\n        names = [\n            key if value is None else value for key, value in headers.items()\n        ]\n    else:\n        attrs = names = headers\n    table = [(\n        format_cell(cell, datetime_fmt=datetime_fmt)\n        for cell in attrgetter(*attrs)(c)\n    ) for c in collection]\n    return tblte(table, headers=[h.upper() for h in names], **kwargs)", "response": "Pretty - print a collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting dataset files to dict.", "response": "def _convert_dataset_files(value):\n    \"\"\"Convert dataset files.\"\"\"\n    output = {}\n    for k, v in value.items():\n        inst = DatasetFile.from_jsonld(v)\n        output[inst.path] = inst\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves files and check repository for potential problems.", "response": "def remove(ctx, client, sources):\n    \"\"\"Remove files and check repository for potential problems.\"\"\"\n    from renku.api._git import _expand_directories\n\n    def fmt_path(path):\n        \"\"\"Format path as relative to the client path.\"\"\"\n        return str(Path(path).absolute().relative_to(client.path))\n\n    files = {\n        fmt_path(source): fmt_path(file_or_dir)\n        for file_or_dir in sources\n        for source in _expand_directories((file_or_dir, ))\n    }\n\n    # 1. Update dataset metadata files.\n    with progressbar(\n        client.datasets.values(),\n        item_show_func=lambda item: str(item.short_id) if item else '',\n        label='Updating dataset metadata',\n        width=0,\n    ) as bar:\n        for dataset in bar:\n            remove = []\n            for key, file_ in dataset.files.items():\n                filepath = fmt_path(file_.full_path)\n                if filepath in files:\n                    remove.append(key)\n\n            if remove:\n                for key in remove:\n                    dataset.unlink_file(key)\n\n                dataset.to_yaml()\n\n    # 2. Manage .gitattributes for external storage.\n    tracked = tuple(\n        path for path, attr in client.find_attr(*files).items()\n        if attr.get('filter') == 'lfs'\n    )\n    client.untrack_paths_from_storage(*tracked)\n    existing = client.find_attr(*tracked)\n    if existing:\n        click.echo(WARNING + 'There are custom .gitattributes.\\n')\n        if click.confirm(\n            'Do you want to edit \".gitattributes\" now?', default=False\n        ):\n            click.edit(filename=str(client.path / '.gitattributes'))\n\n    # Finally remove the files.\n    final_sources = list(set(files.values()))\n    if final_sources:\n        run(['git', 'rm', '-rf'] + final_sources, check=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_release(package, releases, dependencies=None):\n    dependencies = dependencies if dependencies is not None else {}\n    for release in releases:\n        url = release['url']\n        old_priority = dependencies.get(package, {}).get('priority', 0)\n\n        for suffix, priority in SUFFIXES.items():\n            if url.endswith(suffix):\n                if old_priority < priority:\n                    sha256 = release['digests']['sha256']\n                    dependencies[package] = {\n                        'package': package,\n                        'url': url,\n                        'sha256': sha256,\n                        'priority': priority,\n                    }\n\n    return dependencies[package]", "response": "Return the best release for a given package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _safe_path(filepath, can_be_cwl=False):\n    # Should not be in ignore paths.\n    if filepath in {'.gitignore', '.gitattributes'}:\n        return False\n\n    # Ignore everything in .renku ...\n    if filepath.startswith('.renku'):\n        # ... unless it can be a CWL.\n        if can_be_cwl and filepath.endswith('.cwl'):\n            return True\n        return False\n\n    return True", "response": "Check if the path should be used in output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tabular(client, datasets):\n    from renku.models._tabulate import tabulate\n\n    click.echo(\n        tabulate(\n            datasets,\n            headers=OrderedDict((\n                ('short_id', 'id'),\n                ('name', None),\n                ('created', None),\n                ('authors_csv', 'authors'),\n            )),\n        )\n    )", "response": "Format datasets with a tabular output."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nformat datasets as JSON - LD.", "response": "def jsonld(client, datasets):\n    \"\"\"Format datasets as JSON-LD.\"\"\"\n    from renku.models._json import dumps\n    from renku.models._jsonld import asjsonld\n\n    data = [\n        asjsonld(\n            dataset,\n            basedir=os.path.relpath(\n                '.', start=str(dataset.__reference__.parent)\n            )\n        ) for dataset in datasets\n    ]\n    click.echo(dumps(data, indent=2))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning formatted text with the submodule information.", "response": "def _format_sha1(graph, node):\n    \"\"\"Return formatted text with the submodule information.\"\"\"\n    try:\n        submodules = node.submodules\n\n        if submodules:\n            submodule = ':'.join(submodules)\n            return click.style(submodule, fg='green') + '@' + click.style(\n                node.commit.hexsha[:8], fg='yellow'\n            )\n    except KeyError:\n        pass\n    return click.style(node.commit.hexsha[:8], fg='yellow')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts arguments from various input formats.", "response": "def convert_arguments(value):\n    \"\"\"Convert arguments from various input formats.\"\"\"\n    if isinstance(value, (list, tuple)):\n        return [\n            CommandLineBinding(**item) if isinstance(item, dict) else item\n            for item in value\n        ]\n    return shlex.split(value)"}
