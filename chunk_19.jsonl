{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _valid_comparison(time_a, time_b, event_a, event_b):\n    if time_a == time_b:\n        # Ties are only informative if exactly one event happened\n        return event_a != event_b\n    if event_a and event_b:\n        return True\n    if event_a and time_a < time_b:\n        return True\n    if event_b and time_b < time_a:\n        return True\n    return False", "response": "True if times can be compared."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict_percentile(self, X, ancillary_X=None, p=0.5):\n        exp_mu_, sigma_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        return pd.DataFrame(exp_mu_ * np.exp(np.sqrt(2) * sigma_ * erfinv(2 * p - 1)), index=_get_index(X))", "response": "Predict the median lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict_median(self, X, ancillary_X=None):\n        exp_mu_, _ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        return pd.DataFrame(exp_mu_, index=_get_index(X))", "response": "Predict the median lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict_expectation(self, X, ancillary_X=None):\n        exp_mu_, sigma_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        return pd.DataFrame(exp_mu_ * np.exp(sigma_ ** 2 / 2), index=_get_index(X))", "response": "Predict the expectation of lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef predict_cumulative_hazard(self, X, times=None, ancillary_X=None):\n        import numpy as np\n\n        times = coalesce(times, self.timeline, np.unique(self.durations))\n        exp_mu_, sigma_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        mu_ = np.log(exp_mu_)\n        Z = np.subtract.outer(np.log(times), mu_) / sigma_\n        return pd.DataFrame(-logsf(Z), columns=_get_index(X), index=times)", "response": "Predict the cumulative hazard rate of subjects in X at time points times."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef subtract(self, other):\n        self_estimate = getattr(self, self._estimate_name)\n        other_estimate = getattr(other, other._estimate_name)\n        new_index = np.concatenate((other_estimate.index, self_estimate.index))\n        new_index = np.unique(new_index)\n        return pd.DataFrame(\n            self_estimate.reindex(new_index, method=\"ffill\").values\n            - other_estimate.reindex(new_index, method=\"ffill\").values,\n            index=new_index,\n            columns=[\"diff\"],\n        )", "response": "Subtract the { 0 } of two { 1 } fitted instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npredicting the value at certain point in time.", "response": "def predict(self, times):\n        \"\"\"\n        Predict the {0} at certain point in time. Uses a linear interpolation if\n        points in time are not in the index.\n\n        Parameters\n        ----------\n        times: a scalar or an array of times to predict the value of {0} at.\n\n        Returns\n        -------\n        predictions: a scalar if time is a scalar, a numpy array if time in an array.\n        \"\"\"\n        if callable(self._estimation_method):\n            return pd.DataFrame(self._estimation_method(_to_array(times)), index=_to_array(times)).loc[times].squeeze()\n        estimate = getattr(self, self._estimation_method)\n        # non-linear interpolations can push the survival curves above 1 and below 0.\n        return dataframe_interpolate_at_times(estimate, times)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a DataFrame that estimates the median duration remaining until the death event given the survival up until time t.", "response": "def _conditional_time_to_event_(self):\n        \"\"\"\n        Return a DataFrame, with index equal to survival_function_, that estimates the median\n        duration remaining until the death event, given survival up until time t. For example, if an\n        individual exists until age 1, their expected life remaining *given they lived to time 1*\n        might be 9 years.\n\n        Returns\n        -------\n        conditional_time_to_: DataFrame\n            with index equal to survival_function_\n\n        \"\"\"\n        age = self.survival_function_.index.values[:, None]\n        columns = [\"%s - Conditional time remaining to event\" % self._label]\n        return (\n            pd.DataFrame(\n                qth_survival_times(self.survival_function_[self._label] * 0.5, self.survival_function_)\n                .sort_index(ascending=False)\n                .values,\n                index=self.survival_function_.index,\n                columns=columns,\n            )\n            - age\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summary(self):\n        ci = 1 - self.alpha\n        lower_upper_bounds = self._compute_confidence_bounds_of_parameters()\n        df = pd.DataFrame(index=self._fitted_parameter_names)\n        df[\"coef\"] = self._fitted_parameters_\n        df[\"se(coef)\"] = self._compute_standard_errors().loc[\"se\"]\n        df[\"lower %g\" % ci] = lower_upper_bounds[\"lower-bound\"]\n        df[\"upper %g\" % ci] = lower_upper_bounds[\"upper-bound\"]\n        df[\"p\"] = self._compute_p_values()\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n            df[\"-log2(p)\"] = -np.log2(df[\"p\"])\n        return df", "response": "Summary statistics describing the fit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_summary(self, decimals=2, **kwargs):\n        justify = string_justify(18)\n        print(self)\n        print(\"{} = {}\".format(justify(\"number of subjects\"), self.event_observed.shape[0]))\n        print(\"{} = {}\".format(justify(\"number of events\"), np.where(self.event_observed)[0].shape[0]))\n        print(\"{} = {:.{prec}f}\".format(justify(\"log-likelihood\"), self._log_likelihood, prec=decimals))\n        print(\n            \"{} = {}\".format(\n                justify(\"hypothesis\"),\n                \", \".join(\n                    \"%s != %d\" % (name, iv) for (name, iv) in zip(self._fitted_parameter_names, self._initial_values)\n                ),\n            )\n        )\n\n        for k, v in kwargs.items():\n            print(\"{} = {}\\n\".format(justify(k), v))\n\n        print(end=\"\\n\")\n        print(\"---\")\n\n        df = self.summary\n        print(\n            df.to_string(\n                float_format=format_floats(decimals),\n                formatters={\"p\": format_p_value(decimals), \"exp(coef)\": format_exp_floats(decimals)},\n            )\n        )", "response": "Prints summary statistics describing the fit coefficients and error bounds of the fit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfitting the estimate to the population of n - nite - curves.", "response": "def fit(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        label=None,\n        alpha=None,\n        ci_labels=None,\n        show_progress=False,\n        entry=None,\n        weights=None,\n        left_censorship=False,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Parameters\n        ----------\n        durations: an array, or pd.Series\n          length n, duration subject was observed for\n        event_observed: numpy array or pd.Series, optional\n          length n, True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n        timeline: list, optional\n            return the estimate at the values in timeline (positively increasing)\n        label: string, optional\n            a string to name the column of the estimate.\n        alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing\n           alpha for this call to fit only.\n        ci_labels: list, optional\n            add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<alpha>\n        show_progress: boolean, optional\n            since this is an iterative fitting algorithm, switching this to True will display some iteration details.\n        entry: an array, or pd.Series, of length n\n            relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n            entered study when they were \"born\": time zero.\n        weights: an array, or pd.Series, of length n\n            integer weights per observation\n        Returns\n        -------\n          self\n            self with new properties like ``cumulative_hazard_``, ``survival_function_``\n\n        \"\"\"\n        if left_censorship:\n            warnings.warn(\n                \"kwarg left_censorship is deprecated and will be removed in a future release. Please use ``.fit_left_censoring`` instead.\",\n                DeprecationWarning,\n            )\n            return self.fit_left_censoring(\n                durations, event_observed, timeline, label, alpha, ci_labels, show_progress, entry, weights\n            )\n\n        self.durations = np.asarray(pass_for_numeric_dtypes_or_raise_array(durations))\n        check_nans_or_infs(self.durations)\n        check_positivity(self.durations)\n        self._censoring_type = CensoringType.RIGHT\n\n        return self._fit(\n            (self.durations, None),\n            event_observed=event_observed,\n            timeline=timeline,\n            label=label,\n            alpha=alpha,\n            ci_labels=ci_labels,\n            show_progress=show_progress,\n            entry=entry,\n            weights=weights,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit the model to a left - censored dataset.", "response": "def fit_left_censoring(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        label=None,\n        alpha=None,\n        ci_labels=None,\n        show_progress=False,\n        entry=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Fit the model to a left-censored dataset\n\n        Parameters\n        ----------\n        durations: an array, or pd.Series\n          length n, duration subject was observed for\n        event_observed: numpy array or pd.Series, optional\n          length n, True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n        timeline: list, optional\n            return the estimate at the values in timeline (positively increasing)\n        label: string, optional\n            a string to name the column of the estimate.\n        alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing\n           alpha for this call to fit only.\n        ci_labels: list, optional\n            add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<alpha>\n        show_progress: boolean, optional\n            since this is an iterative fitting algorithm, switching this to True will display some iteration details.\n        entry: an array, or pd.Series, of length n\n            relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n            entered study when they were \"born\": time zero.\n        weights: an array, or pd.Series, of length n\n            integer weights per observation\n        Returns\n        -------\n          self\n            self with new properties like ``cumulative_hazard_``, ``survival_function_``\n\n        \"\"\"\n\n        self.durations = np.asarray(pass_for_numeric_dtypes_or_raise_array(durations))\n        check_nans_or_infs(self.durations)\n        check_positivity(self.durations)\n        self._censoring_type = CensoringType.LEFT\n        return self._fit(\n            (None, self.durations),\n            event_observed=event_observed,\n            timeline=timeline,\n            label=label,\n            alpha=alpha,\n            ci_labels=ci_labels,\n            show_progress=show_progress,\n            entry=entry,\n            weights=weights,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit_interval_censoring(\n        self,\n        lower_bound,\n        upper_bound,\n        event_observed=None,\n        timeline=None,\n        label=None,\n        alpha=None,\n        ci_labels=None,\n        show_progress=False,\n        entry=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Fit the model to an interval censored dataset.\n\n        Parameters\n        ----------\n        lower_bound: an array, or pd.Series\n          length n, the start of the period the subject experienced the event in.\n        upper_bound: an array, or pd.Series\n          length n, the end of the period the subject experienced the event in. If the value is equal to the corresponding value in lower_bound, then\n          the individual's event was observed (not censored).\n        event_observed: numpy array or pd.Series, optional\n          length n, if left optional, infer from ``lower_bound`` and ``upper_cound`` (if lower_bound==upper_bound then event observed, if lower_bound < upper_bound, then event censored)\n        timeline: list, optional\n            return the estimate at the values in timeline (positively increasing)\n        label: string, optional\n            a string to name the column of the estimate.\n        alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing\n           alpha for this call to fit only.\n        ci_labels: list, optional\n            add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<alpha>\n        show_progress: boolean, optional\n            since this is an iterative fitting algorithm, switching this to True will display some iteration details.\n        entry: an array, or pd.Series, of length n\n            relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n            entered study when they were \"born\": time zero.\n        weights: an array, or pd.Series, of length n\n            integer weights per observation\n        Returns\n        -------\n          self\n            self with new properties like ``cumulative_hazard_``, ``survival_function_``\n\n        \"\"\"\n        check_nans_or_infs(lower_bound)\n        check_positivity(upper_bound)\n\n        self.upper_bound = np.asarray(pass_for_numeric_dtypes_or_raise_array(upper_bound))\n        self.lower_bound = np.asarray(pass_for_numeric_dtypes_or_raise_array(lower_bound))\n\n        if (self.upper_bound < self.lower_bound).any():\n            raise ValueError(\"All upper_bound times must be greater than or equal to lower_bound times.\")\n\n        if event_observed is None:\n            event_observed = self.upper_bound == self.lower_bound\n\n        if ((self.lower_bound == self.upper_bound) != event_observed).any():\n            raise ValueError(\n                \"For all rows, lower_bound == upper_bound if and only if event observed = 1 (uncensored). Likewise, lower_bound < upper_bound if and only if event observed = 0 (censored)\"\n            )\n\n        self._censoring_type = CensoringType.INTERVAL\n\n        return self._fit(\n            (np.clip(self.lower_bound, 1e-20, 1e25), np.clip(self.upper_bound, 1e-20, 1e25)),\n            event_observed=event_observed,\n            timeline=timeline,\n            label=label,\n            alpha=alpha,\n            ci_labels=ci_labels,\n            show_progress=show_progress,\n            entry=entry,\n            weights=weights,\n        )", "response": "Fit the model to an interval censored dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef survival_function_at_times(self, times, label=None):\n        label = coalesce(label, self._label)\n        return pd.Series(self._survival_function(self._fitted_parameters_, times), index=_to_array(times), name=label)", "response": "Returns a Pandas series of the predicted survival value at specific times."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Pandas series of the predicted cumulative density function at specific times.", "response": "def cumulative_density_at_times(self, times, label=None):\n        \"\"\"\n        Return a Pandas series of the predicted cumulative density function (1-survival function) at specific times.\n\n        Parameters\n        -----------\n        times: iterable or float\n          values to return the survival function at.\n        label: string, optional\n          Rename the series returned. Useful for plotting.\n\n        Returns\n        --------\n        pd.Series\n\n        \"\"\"\n        label = coalesce(label, self._label)\n        return pd.Series(self._cumulative_density(self._fitted_parameters_, times), index=_to_array(times), name=label)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Pandas series of the predicted cumulative hazard value at specific times.", "response": "def cumulative_hazard_at_times(self, times, label=None):\n        \"\"\"\n        Return a Pandas series of the predicted cumulative hazard value at specific times.\n\n        Parameters\n        -----------\n        times: iterable or float\n          values to return the cumulative hazard at.\n        label: string, optional\n          Rename the series returned. Useful for plotting.\n\n        Returns\n        --------\n        pd.Series\n\n        \"\"\"\n        label = coalesce(label, self._label)\n        return pd.Series(self._cumulative_hazard(self._fitted_parameters_, times), index=_to_array(times), name=label)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Pandas series of the predicted hazard at specific times.", "response": "def hazard_at_times(self, times, label=None):\n        \"\"\"\n        Return a Pandas series of the predicted hazard at specific times.\n\n        Parameters\n        -----------\n        times: iterable or float\n          values to return the hazard at.\n        label: string, optional\n          Rename the series returned. Useful for plotting.\n\n        Returns\n        --------\n        pd.Series\n\n        \"\"\"\n        label = coalesce(label, self._label)\n        return pd.Series(self._hazard(self._fitted_parameters_, times), index=_to_array(times), name=label)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef confidence_interval_survival_function_(self):\n        return self._compute_confidence_bounds_of_transform(self._survival_function, self.alpha, self._ci_labels)", "response": "Returns the confidence interval of the survival function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef confidence_interval_cumulative_density_(self):\n        return self._compute_confidence_bounds_of_transform(self._cumulative_density, self.alpha, self._ci_labels)", "response": "The confidence interval of the cumulative density of the survival function."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot(self, **kwargs):\n        set_kwargs_drawstyle(kwargs, \"default\")\n        return _plot_estimate(\n            self, estimate=getattr(self, self._estimate_name), confidence_intervals=self.confidence_interval_, **kwargs\n        )", "response": "Produce a pretty - plot of the estimate."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfit accelerated failure time model to a right - censored dataset.", "response": "def fit(\n        self,\n        df,\n        duration_col,\n        event_col=None,\n        ancillary_df=None,\n        show_progress=False,\n        timeline=None,\n        weights_col=None,\n        robust=False,\n        initial_point=None,\n        entry_col=None,\n    ):\n        \"\"\"\n        Fit the accelerated failure time model to a right-censored dataset.\n\n        Parameters\n        ----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns `duration_col` and\n            `event_col` (see below), covariates columns, and special columns (weights).\n            `duration_col` refers to\n            the lifetimes of the subjects. `event_col` refers to whether\n            the 'death' events was observed: 1 if observed, 0 else (censored).\n\n        duration_col: string\n            the name of the column in DataFrame that contains the subjects'\n            lifetimes.\n\n        event_col: string, optional\n            the  name of the column in DataFrame that contains the subjects' death\n            observation. If left as None, assume all individuals are uncensored.\n\n        show_progress: boolean, optional (default=False)\n            since the fitter is iterative, show convergence\n            diagnostics. Useful if convergence is failing.\n\n        ancillary_df: None, boolean, or DataFrame, optional (default=None)\n            Choose to model the ancillary parameters.\n            If None or False, explicitly do not fit the ancillary parameters using any covariates.\n            If True, model the ancillary parameters with the same covariates as ``df``.\n            If DataFrame, provide covariates to model the ancillary parameters. Must be the same row count as ``df``.\n\n        timeline: array, optional\n            Specify a timeline that will be used for plotting and prediction\n\n        weights_col: string\n            the column in DataFrame that specifies weights per observation.\n\n        robust: boolean, optional (default=False)\n            Compute the robust errors using the Huber sandwich estimator.\n\n        initial_point: (d,) numpy array, optional\n            initialize the starting point of the iterative\n            algorithm. Default is the zero vector.\n\n        entry_col: specify a column in the DataFrame that denotes any late-entries (left truncation) that occurred. See\n            the docs on `left truncation <https://lifelines.readthedocs.io/en/latest/Survival%20analysis%20with%20lifelines.html#left-truncated-late-entry-data>`__\n\n        Returns\n        -------\n        self:\n            self with additional new properties: ``print_summary``, ``params_``, ``confidence_intervals_`` and more\n\n\n        Examples\n        --------\n        >>> from lifelines import WeibullAFTFitter, LogNormalAFTFitter, LogLogisticAFTFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'T': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'E': [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit(df, 'T', 'E')\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit(df, 'T', 'E', ancillary_df=df)\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n\n        \"\"\"\n        self.duration_col = duration_col\n        self._time_cols = [duration_col]\n        self._censoring_type = CensoringType.RIGHT\n\n        df = df.copy()\n\n        T = pass_for_numeric_dtypes_or_raise_array(df.pop(duration_col)).astype(float)\n        self.durations = T.copy()\n\n        self._fit(\n            self._log_likelihood_right_censoring,\n            df,\n            (T.values, None),\n            event_col=event_col,\n            ancillary_df=ancillary_df,\n            show_progress=show_progress,\n            timeline=timeline,\n            weights_col=weights_col,\n            robust=robust,\n            initial_point=initial_point,\n            entry_col=entry_col,\n        )\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits accelerated failure time model to a left - censored dataset.", "response": "def fit_interval_censoring(\n        self,\n        df,\n        lower_bound_col,\n        upper_bound_col,\n        event_col=None,\n        ancillary_df=None,\n        show_progress=False,\n        timeline=None,\n        weights_col=None,\n        robust=False,\n        initial_point=None,\n        entry_col=None,\n    ):\n        \"\"\"\n        Fit the accelerated failure time model to a left-censored dataset.\n\n        Parameters\n        ----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns ``lower_bound_col``, ``upper_bound_col``  (see below),\n            and any other covariates or weights.\n\n        lower_bound_col: string\n            the name of the column in DataFrame that contains the subjects'\n            left-most observation.\n\n        upper_bound_col: string\n            the name of the column in DataFrame that contains the subjects'\n            right-most observation. Values can be np.inf (and should be if the subject is right-censored).\n\n        event_col: string, optional\n            the  name of the column in DataFrame that contains the subjects' death\n            observation. If left as None, will be inferred from the start and stop columns (lower_bound==upper_bound means uncensored)\n\n        show_progress: boolean, optional (default=False)\n            since the fitter is iterative, show convergence\n            diagnostics. Useful if convergence is failing.\n\n        ancillary_df: None, boolean, or DataFrame, optional (default=None)\n            Choose to model the ancillary parameters.\n            If None or False, explicitly do not fit the ancillary parameters using any covariates.\n            If True, model the ancillary parameters with the same covariates as ``df``.\n            If DataFrame, provide covariates to model the ancillary parameters. Must be the same row count as ``df``.\n\n        timeline: array, optional\n            Specify a timeline that will be used for plotting and prediction\n\n        weights_col: string\n            the column in DataFrame that specifies weights per observation.\n\n        robust: boolean, optional (default=False)\n            Compute the robust errors using the Huber sandwich estimator.\n\n        initial_point: (d,) numpy array, optional\n            initialize the starting point of the iterative\n            algorithm. Default is the zero vector.\n\n        entry_col: specify a column in the DataFrame that denotes any late-entries (left truncation) that occurred. See\n            the docs on `left truncation <https://lifelines.readthedocs.io/en/latest/Survival%20analysis%20with%20lifelines.html#left-truncated-late-entry-data>`__\n\n        Returns\n        -------\n        self:\n            self with additional new properties: ``print_summary``, ``params_``, ``confidence_intervals_`` and more\n\n\n        Examples\n        --------\n        >>> from lifelines import WeibullAFTFitter, LogNormalAFTFitter, LogLogisticAFTFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'start': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'stop':  [5, 3, 9, 8, 7, 4, 8, 5, 2, 5, 6, np.inf],  # this last subject is right-censored.\n        >>>     'E':     [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit_interval_censoring(df, 'start', 'stop', 'E')\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit_interval_censoring(df, 'start', 'stop', 'E', ancillary_df=df)\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n        \"\"\"\n\n        self.lower_bound_col = lower_bound_col\n        self.upper_bound_col = upper_bound_col\n        self._time_cols = [lower_bound_col, upper_bound_col]\n        self._censoring_type = CensoringType.INTERVAL\n\n        df = df.copy()\n\n        lower_bound = pass_for_numeric_dtypes_or_raise_array(df.pop(lower_bound_col)).astype(float)\n        upper_bound = pass_for_numeric_dtypes_or_raise_array(df.pop(upper_bound_col)).astype(float)\n\n        if event_col is None:\n            event_col = \"E\"\n            df[\"E\"] = lower_bound == upper_bound\n\n        if ((lower_bound == upper_bound) != df[event_col]).any():\n            raise ValueError(\n                \"For all rows, lower_bound == upper_bound if and only if event observed = 1 (uncensored). Likewise, lower_bound < upper_bound if and only if event observed = 0 (censored)\"\n            )\n        if (lower_bound > upper_bound).any():\n            raise ValueError(\"All upper bound measurements must be greater than or equal to lower bound measurements.\")\n\n        self.lower_bound = lower_bound\n        self.upper_bound = upper_bound\n\n        self._fit(\n            self._log_likelihood_interval_censoring,\n            df,\n            (lower_bound.values, np.clip(upper_bound.values, 0, 1e25)),\n            event_col=event_col,\n            ancillary_df=ancillary_df,\n            show_progress=show_progress,\n            timeline=timeline,\n            weights_col=weights_col,\n            robust=robust,\n            initial_point=initial_point,\n            entry_col=entry_col,\n        )\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit accelerated failure time model to a left - censored dataset.", "response": "def fit_left_censoring(\n        self,\n        df,\n        duration_col=None,\n        event_col=None,\n        ancillary_df=None,\n        show_progress=False,\n        timeline=None,\n        weights_col=None,\n        robust=False,\n        initial_point=None,\n        entry_col=None,\n    ):\n        \"\"\"\n        Fit the accelerated failure time model to a left-censored dataset.\n\n        Parameters\n        ----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns `duration_col` and\n            `event_col` (see below), covariates columns, and special columns (weights).\n            `duration_col` refers to\n            the lifetimes of the subjects. `event_col` refers to whether\n            the 'death' events was observed: 1 if observed, 0 else (censored).\n\n        duration_col: string\n            the name of the column in DataFrame that contains the subjects'\n            lifetimes/measurements/etc. This column contains the (possibly) left-censored data.\n\n        event_col: string, optional\n            the  name of the column in DataFrame that contains the subjects' death\n            observation. If left as None, assume all individuals are uncensored.\n\n        show_progress: boolean, optional (default=False)\n            since the fitter is iterative, show convergence\n            diagnostics. Useful if convergence is failing.\n\n        ancillary_df: None, boolean, or DataFrame, optional (default=None)\n            Choose to model the ancillary parameters.\n            If None or False, explicitly do not fit the ancillary parameters using any covariates.\n            If True, model the ancillary parameters with the same covariates as ``df``.\n            If DataFrame, provide covariates to model the ancillary parameters. Must be the same row count as ``df``.\n\n        timeline: array, optional\n            Specify a timeline that will be used for plotting and prediction\n\n        weights_col: string\n            the column in DataFrame that specifies weights per observation.\n\n        robust: boolean, optional (default=False)\n            Compute the robust errors using the Huber sandwich estimator.\n\n        initial_point: (d,) numpy array, optional\n            initialize the starting point of the iterative\n            algorithm. Default is the zero vector.\n\n        entry_col: specify a column in the DataFrame that denotes any late-entries (left truncation) that occurred. See\n            the docs on `left truncation <https://lifelines.readthedocs.io/en/latest/Survival%20analysis%20with%20lifelines.html#left-truncated-late-entry-data>`__\n\n        Returns\n        -------\n        self:\n            self with additional new properties: ``print_summary``, ``params_``, ``confidence_intervals_`` and more\n\n\n        Examples\n        --------\n        >>> from lifelines import WeibullAFTFitter, LogNormalAFTFitter, LogLogisticAFTFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'T': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'E': [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit_left_censoring(df, 'T', 'E')\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n        >>>\n        >>> aft = WeibullAFTFitter()\n        >>> aft.fit_left_censoring(df, 'T', 'E', ancillary_df=df)\n        >>> aft.print_summary()\n        >>> aft.predict_median(df)\n        \"\"\"\n        self._censoring_type = CensoringType.LEFT\n        df = df.copy()\n\n        T = pass_for_numeric_dtypes_or_raise_array(df.pop(duration_col)).astype(float)\n        self.durations = T.copy()\n\n        self._fit(\n            self._log_likelihood_left_censoring,\n            df,\n            (None, T.values),\n            event_col=event_col,\n            ancillary_df=ancillary_df,\n            show_progress=show_progress,\n            timeline=timeline,\n            weights_col=weights_col,\n            robust=robust,\n            initial_point=initial_point,\n            entry_col=entry_col,\n        )\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the initial point for the current state of the censoring.", "response": "def _create_initial_point(self, Ts, E, entries, weights, Xs):\n        \"\"\"\n        See https://github.com/CamDavidsonPilon/lifelines/issues/664\n        \"\"\"\n        import lifelines  # kinda hacky but lol\n\n        def transform_ith_param(model, i):\n            param = model._fitted_parameters_[i]\n            if param <= 0:\n                return param\n            # technically this is suboptimal for log normal mu, but that's okay.\n            return np.log(param)\n\n        name = self.__class__.__name__.replace(\"AFT\", \"\")\n        uni_model = getattr(lifelines, name)()\n\n        if self._censoring_type == CensoringType.RIGHT:\n            uni_model.fit_right_censoring(Ts[0], event_observed=E, entry=entries, weights=weights)\n        elif self._censoring_type == CensoringType.INTERVAL:\n            uni_model.fit_interval_censoring(Ts[0], Ts[1], event_observed=E, entry=entries, weights=weights)\n        elif self._censoring_type == CensoringType.LEFT:\n            uni_model.fit_left_censoring(Ts[1], event_observed=E, entry=entries, weights=weights)\n\n        # we may use this later in print_summary\n        self._ll_null_ = uni_model._log_likelihood\n\n        return np.concatenate(\n            [\n                # tack on as the intercept\n                [0] * (_X.shape[1] - 1) + [transform_ith_param(uni_model, i)]\n                for i, _X in enumerate(Xs)\n            ]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_median(self, X, ancillary_X=None):\n        return self.predict_percentile(X, p=0.5, ancillary_X=ancillary_X)", "response": "Predict the median lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot(self, columns=None, parameter=None, **errorbar_kwargs):\n        from matplotlib import pyplot as plt\n\n        set_kwargs_ax(errorbar_kwargs)\n        ax = errorbar_kwargs.pop(\"ax\")\n        errorbar_kwargs.setdefault(\"c\", \"k\")\n        errorbar_kwargs.setdefault(\"fmt\", \"s\")\n        errorbar_kwargs.setdefault(\"markerfacecolor\", \"white\")\n        errorbar_kwargs.setdefault(\"markeredgewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"elinewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"capsize\", 3)\n\n        z = inv_normal_cdf(1 - self.alpha / 2)\n\n        params_ = self.params_.copy()\n        standard_errors_ = self.standard_errors_.copy()\n\n        if columns is not None:\n            params_ = params_.loc[:, columns]\n            standard_errors_ = standard_errors_.loc[:, columns]\n        if parameter is not None:\n            params_ = params_.loc[parameter]\n            standard_errors_ = standard_errors_.loc[parameter]\n\n        columns = params_.index\n\n        hazards = params_.loc[columns].to_frame(name=\"coefs\")\n        hazards[\"se\"] = z * standard_errors_.loc[columns]\n\n        if isinstance(hazards.index, pd.MultiIndex):\n            hazards = hazards.groupby(level=0, group_keys=False).apply(\n                lambda x: x.sort_values(by=\"coefs\", ascending=True)\n            )\n        else:\n            hazards = hazards.sort_values(by=\"coefs\", ascending=True)\n\n        yaxis_locations = list(range(len(columns)))\n\n        ax.errorbar(hazards[\"coefs\"], yaxis_locations, xerr=hazards[\"se\"], **errorbar_kwargs)\n        best_ylim = ax.get_ylim()\n        ax.vlines(0, -2, len(columns) + 1, linestyles=\"dashed\", linewidths=1, alpha=0.65)\n        ax.set_ylim(best_ylim)\n\n        if isinstance(columns[0], tuple):\n            tick_labels = [\"%s: %s\" % (c, p) for (p, c) in hazards.index]\n        else:\n            tick_labels = [i for i in hazards.index]\n\n        plt.yticks(yaxis_locations, tick_labels)\n        plt.xlabel(\"log(accelerated failure rate) (%g%% CI)\" % ((1 - self.alpha) * 100))\n\n        return ax", "response": "Plots the coefficients of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_covariate_groups(self, covariates, values, plot_baseline=True, **kwargs):\n        from matplotlib import pyplot as plt\n\n        covariates = _to_list(covariates)\n        values = _to_array(values)\n        if len(values.shape) == 1:\n            values = values[None, :].T\n\n        if len(covariates) != values.shape[1]:\n            raise ValueError(\"The number of covariates must equal to second dimension of the values array.\")\n\n        original_columns = self.params_.index.get_level_values(1)\n        for covariate in covariates:\n            if covariate not in original_columns:\n                raise KeyError(\"covariate `%s` is not present in the original dataset\" % covariate)\n\n        ax = kwargs.pop(\"ax\", None) or plt.figure().add_subplot(111)\n\n        # model X\n        x_bar = self._norm_mean.to_frame().T\n        X = pd.concat([x_bar] * values.shape[0])\n        if np.array_equal(np.eye(len(covariates)), values):\n            X.index = [\"%s=1\" % c for c in covariates]\n        else:\n            X.index = [\", \".join(\"%s=%g\" % (c, v) for (c, v) in zip(covariates, row)) for row in values]\n        for covariate, value in zip(covariates, values.T):\n            X[covariate] = value\n\n        # model ancillary X\n        x_bar_anc = self._norm_mean_ancillary.to_frame().T\n        ancillary_X = pd.concat([x_bar_anc] * values.shape[0])\n        for covariate, value in zip(covariates, values.T):\n            ancillary_X[covariate] = value\n\n        if self.fit_intercept:\n            X[\"_intercept\"] = 1.0\n            ancillary_X[\"_intercept\"] = 1.0\n\n        self.predict_survival_function(X, ancillary_X=ancillary_X).plot(ax=ax, **kwargs)\n        if plot_baseline:\n            self.predict_survival_function(x_bar, ancillary_X=x_bar_anc).rename(columns={0: \"baseline survival\"}).plot(\n                ax=ax, ls=\":\", color=\"k\"\n            )\n        return ax", "response": "Plots the categorical variables at once and then the predicted survival curve at all of the values in a group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef predict_percentile(self, X, ancillary_X=None, p=0.5):\n        alpha_, beta_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n\n        return pd.DataFrame(alpha_ * (1 / p - 1) ** beta_, index=_get_index(X))", "response": "Predict the median lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npredict the expectation of lifetimes for the individuals.", "response": "def predict_expectation(self, X, ancillary_X=None):\n        \"\"\"\n        Predict the expectation of lifetimes, :math:`E[T | x]`.\n\n        Parameters\n        ----------\n        X: numpy array or DataFrame\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n        ancillary_X: numpy array or DataFrame, optional\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n\n        Returns\n        -------\n        percentiles: DataFrame\n            the median lifetimes for the individuals. If the survival curve of an\n            individual does not cross 0.5, then the result is infinity.\n\n\n        See Also\n        --------\n        predict_median\n        \"\"\"\n        alpha_, beta_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        v = (alpha_ * np.pi / beta_) / np.sin(np.pi / beta_)\n        v = np.where(beta_ > 1, v, np.nan)\n        return pd.DataFrame(v, index=_get_index(X))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npredicts the cumulative hazard rate of subjects in X at time points times.", "response": "def predict_cumulative_hazard(self, X, times=None, ancillary_X=None):\n        \"\"\"\n        Return the cumulative hazard rate of subjects in X at time points.\n\n        Parameters\n        ----------\n        X: numpy array or DataFrame\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n        times: iterable, optional\n            an iterable of increasing times to predict the cumulative hazard at. Default\n            is the set of all durations (observed and unobserved). Uses a linear interpolation if\n            points in time are not in the index.\n        ancillary_X: numpy array or DataFrame, optional\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n\n        Returns\n        -------\n        cumulative_hazard_ : DataFrame\n            the cumulative hazard of individuals over the timeline\n        \"\"\"\n        times = coalesce(times, self.timeline, np.unique(self.durations))\n        alpha_, beta_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        return pd.DataFrame(np.log1p(np.outer(times, 1 / alpha_) ** beta_), columns=_get_index(X), index=times)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the censored estimate to the given n - tuple of properties.", "response": "def fit(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        entry=None,\n        label=\"NA_estimate\",\n        alpha=None,\n        ci_labels=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Parameters\n        -----------\n        durations: an array, or pd.Series, of length n\n          duration subject was observed for\n        timeline: iterable\n            return the best estimate at the values in timelines (positively increasing)\n        event_observed: an array, or pd.Series, of length n\n            True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n        entry: an array, or pd.Series, of length n\n           relative time when a subject entered the study. This is\n           useful for left-truncated observations, i.e the birth event was not observed.\n           If None, defaults to all 0 (all birth events observed.)\n        label: string\n            a string to name the column of the estimate.\n        alpha: float\n            the alpha value in the confidence intervals. Overrides the initializing\n           alpha for this call to fit only.\n        ci_labels: iterable\n            add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<1-alpha/2>\n        weights: n array, or pd.Series, of length n\n            if providing a weighted dataset. For example, instead\n            of providing every subject as a single element of `durations` and `event_observed`, one could\n            weigh subject differently.\n\n        Returns\n        -------\n          self, with new properties like ``cumulative_hazard_``.\n\n        \"\"\"\n        self._censoring_type = CensoringType.RIGHT\n        check_nans_or_infs(durations)\n        if event_observed is not None:\n            check_nans_or_infs(event_observed)\n\n        if weights is not None:\n            if (weights.astype(int) != weights).any():\n                warnings.warn(\n                    \"\"\"It looks like your weights are not integers, possibly prospenity scores then?\n  It's important to know that the naive variance estimates of the coefficients are biased. Instead use Monte Carlo to\n  estimate the variances. See paper \"Variance estimation when using inverse probability of treatment weighting (IPTW) with survival analysis\"\n  or \"Adjusted Kaplan-Meier estimator and log-rank test with inverse probability of treatment weighting for survival data.\"\n                  \"\"\",\n                    StatisticalWarning,\n                )\n\n        v = _preprocess_inputs(durations, event_observed, timeline, entry, weights)\n        self.durations, self.event_observed, self.timeline, self.entry, self.event_table = v\n\n        cumulative_hazard_, cumulative_sq_ = _additive_estimate(\n            self.event_table, self.timeline, self._additive_f, self._variance_f, False\n        )\n\n        # esimates\n        self._label = label\n        self.cumulative_hazard_ = pd.DataFrame(cumulative_hazard_, columns=[self._label])\n        self.confidence_interval_ = self._bounds(cumulative_sq_[:, None], alpha if alpha else self.alpha, ci_labels)\n        self._cumulative_sq = cumulative_sq_\n\n        # estimation methods\n        self._estimation_method = \"cumulative_hazard_\"\n        self._estimate_name = \"cumulative_hazard_\"\n        self._predict_label = label\n        self._update_docstrings()\n\n        # plotting\n        self.plot_cumulative_hazard = self.plot\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef smoothed_hazard_(self, bandwidth):\n        timeline = self.timeline\n        cumulative_hazard_name = self.cumulative_hazard_.columns[0]\n        hazard_name = \"differenced-\" + cumulative_hazard_name\n        hazard_ = self.cumulative_hazard_.diff().fillna(self.cumulative_hazard_.iloc[0])\n        C = (hazard_[cumulative_hazard_name] != 0.0).values\n        return pd.DataFrame(\n            1.0\n            / bandwidth\n            * np.dot(epanechnikov_kernel(timeline[:, None], timeline[C][None, :], bandwidth), hazard_.values[C, :]),\n            columns=[hazard_name],\n            index=timeline,\n        )", "response": "Returns a DataFrame of the smoothed hazard with the given bandwidth."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Pandas DataFrame of smoothed hazard rates and confidence intervals for the given bandwidth.", "response": "def smoothed_hazard_confidence_intervals_(self, bandwidth, hazard_=None):\n        \"\"\"\n        Parameters\n        ----------\n          bandwidth: float\n            the bandwidth to use in the Epanechnikov kernel. > 0\n          hazard_: numpy array\n            a computed (n,) numpy array of estimated hazard rates. If none, uses ``smoothed_hazard_``\n        \"\"\"\n        if hazard_ is None:\n            hazard_ = self.smoothed_hazard_(bandwidth).values[:, 0]\n\n        timeline = self.timeline\n        z = inv_normal_cdf(1 - self.alpha / 2)\n        self._cumulative_sq.iloc[0] = 0\n        var_hazard_ = self._cumulative_sq.diff().fillna(self._cumulative_sq.iloc[0])\n        C = var_hazard_.values != 0.0  # only consider the points with jumps\n        std_hazard_ = np.sqrt(\n            1.0\n            / (bandwidth ** 2)\n            * np.dot(\n                epanechnikov_kernel(timeline[:, None], timeline[C][None, :], bandwidth) ** 2, var_hazard_.values[C]\n            )\n        )\n        values = {\n            self.ci_labels[0]: hazard_ * np.exp(z * std_hazard_ / hazard_),\n            self.ci_labels[1]: hazard_ * np.exp(-z * std_hazard_ / hazard_),\n        }\n        return pd.DataFrame(values, index=timeline)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting the np. ndarray values into a balanced tree.", "response": "def _treeify(values):\n        \"\"\"Convert the np.ndarray `values` into a complete balanced tree.\n\n        Assumes `values` is sorted ascending. Returns a list `t` of the same length in which t[i] >\n        t[2i+1] and t[i] < t[2i+2] for all i.\"\"\"\n        if len(values) == 1:  # this case causes problems later\n            return values\n        tree = np.empty_like(values)\n        # Tree indices work as follows:\n        # 0 is the root\n        # 2n+1 is the left child of n\n        # 2n+2 is the right child of n\n        # So we now rearrange `values` into that format...\n\n        # The first step is to remove the bottom row of leaves, which might not be exactly full\n        last_full_row = int(np.log2(len(values) + 1) - 1)\n        len_ragged_row = len(values) - (2 ** (last_full_row + 1) - 1)\n        if len_ragged_row > 0:\n            bottom_row_ix = np.s_[: 2 * len_ragged_row : 2]\n            tree[-len_ragged_row:] = values[bottom_row_ix]\n            values = np.delete(values, bottom_row_ix)\n\n        # Now `values` is length 2**n - 1, so can be packed efficiently into a tree\n        # Last row of nodes is indices 0, 2, ..., 2**n - 2\n        # Second-last row is indices 1, 5, ..., 2**n - 3\n        # nth-last row is indices (2**n - 1)::(2**(n+1))\n        values_start = 0\n        values_space = 2\n        values_len = 2 ** last_full_row\n        while values_start < len(values):\n            tree[values_len - 1 : 2 * values_len - 1] = values[values_start::values_space]\n            values_start += int(values_space / 2)\n            values_space *= 2\n            values_len = int(values_len / 2)\n        return tree"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert(self, value):\n        i = 0\n        n = len(self._tree)\n        while i < n:\n            cur = self._tree[i]\n            self._counts[i] += 1\n            if value < cur:\n                i = 2 * i + 1\n            elif value > cur:\n                i = 2 * i + 2\n            else:\n                return\n        raise ValueError(\"Value %s not contained in tree.\" \"Also, the counts are now messed up.\" % value)", "response": "Insert an occurrence of value into the btree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the rank and count of the value in the btree.", "response": "def rank(self, value):\n        \"\"\"Returns the rank and count of the value in the btree.\"\"\"\n        i = 0\n        n = len(self._tree)\n        rank = 0\n        count = 0\n        while i < n:\n            cur = self._tree[i]\n            if value < cur:\n                i = 2 * i + 1\n                continue\n            elif value > cur:\n                rank += self._counts[i]\n                # subtract off the right tree if exists\n                nexti = 2 * i + 2\n                if nexti < n:\n                    rank -= self._counts[nexti]\n                    i = nexti\n                    continue\n                else:\n                    return (rank, count)\n            else:  # value == cur\n                count = self._counts[i]\n                lefti = 2 * i + 1\n                if lefti < n:\n                    nleft = self._counts[lefti]\n                    count -= nleft\n                    rank += nleft\n                    righti = lefti + 1\n                    if righti < n:\n                        count -= self._counts[righti]\n                return (rank, count)\n        return (rank, count)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a quantile - quantile plot of the empirical CDF against the univariate parametric CDF.", "response": "def qq_plot(model, **plot_kwargs):\n    \"\"\"\n    Produces a quantile-quantile plot of the empirical CDF against\n    the fitted parametric CDF. Large deviances away from the line y=x\n    can invalidate a model (though we expect some natural deviance in the tails).\n\n    Parameters\n    -----------\n    model: obj\n        A fitted lifelines univariate parametric model, like ``WeibullFitter``\n    plot_kwargs:\n        kwargs for the plot.\n\n    Returns\n    --------\n    ax: axis object\n\n    Examples\n    ---------\n\n    >>> from lifelines import *\n    >>> from lifelines.plotting import qq_plot\n    >>> from lifelines.datasets import load_rossi\n    >>> df = load_rossi()\n    >>> wf = WeibullFitter().fit(df['week'], df['arrest'])\n    >>> qq_plot(wf)\n\n\n    \"\"\"\n    from lifelines.utils import qth_survival_times\n    from lifelines import KaplanMeierFitter\n\n    set_kwargs_ax(plot_kwargs)\n    ax = plot_kwargs.pop(\"ax\")\n\n    dist = get_distribution_name_of_lifelines_model(model)\n    dist_object = create_scipy_stats_model_from_lifelines_model(model)\n\n    COL_EMP = \"empirical quantiles\"\n    COL_THEO = \"fitted %s quantiles\" % dist\n\n    if model._censoring_type == CensoringType.LEFT:\n        kmf = KaplanMeierFitter().fit_left_censoring(model.durations, model.event_observed, label=COL_EMP)\n    elif model._censoring_type == CensoringType.RIGHT:\n        kmf = KaplanMeierFitter().fit_right_censoring(model.durations, model.event_observed, label=COL_EMP)\n    elif model._censoring_type == CensoringType.INTERVAL:\n        raise NotImplementedError()\n\n    q = np.unique(kmf.cumulative_density_.values[:, 0])\n    quantiles = qth_survival_times(q, kmf.cumulative_density_, cdf=True)\n    quantiles[COL_THEO] = dist_object.ppf(q)\n    quantiles = quantiles.replace([-np.inf, 0, np.inf], np.nan).dropna()\n\n    max_, min_ = quantiles[COL_EMP].max(), quantiles[COL_EMP].min()\n\n    quantiles.plot.scatter(COL_THEO, COL_EMP, c=\"none\", edgecolor=\"k\", lw=0.5, ax=ax)\n    ax.plot([min_, max_], [min_, max_], c=\"k\", ls=\":\", lw=1.0)\n    ax.set_ylim(min_, max_)\n    ax.set_xlim(min_, max_)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_spines(ax, sides):\n    for side in sides:\n        ax.spines[side].set_visible(False)\n    return ax", "response": "Removes spines of axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_spines(ax, sides, dists):\n    for side, dist in zip(sides, dists):\n        ax.spines[side].set_position((\"axes\", dist))\n    return ax", "response": "Move the entire spine relative to the figure."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving ticks from axes.", "response": "def remove_ticks(ax, x=False, y=False):\n    \"\"\"\n    Remove ticks from axis.\n\n    Parameters:\n      ax: axes to work on\n      x: if True, remove xticks. Default False.\n      y: if True, remove yticks. Default False.\n\n    Examples:\n    removeticks(ax, x=True)\n    removeticks(ax, x=True, y=True)\n    \"\"\"\n    if x:\n        ax.xaxis.set_ticks_position(\"none\")\n    if y:\n        ax.yaxis.set_ticks_position(\"none\")\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd counts showing how many individuals were at risk at each time point in survival and hazard plots.", "response": "def add_at_risk_counts(*fitters, **kwargs):\n    \"\"\"\n    Add counts showing how many individuals were at risk at each time point in\n    survival/hazard plots.\n\n    Parameters\n    ----------\n    fitters:\n      One or several fitters, for example KaplanMeierFitter,\n      NelsonAalenFitter, etc...\n\n\n    Returns\n    --------\n      ax: The axes which was used.\n\n    Examples\n    --------\n    >>> # First train some fitters and plot them\n    >>> fig = plt.figure()\n    >>> ax = plt.subplot(111)\n    >>>\n    >>> f1 = KaplanMeierFitter()\n    >>> f1.fit(data)\n    >>> f1.plot(ax=ax)\n    >>>\n    >>> f2 = KaplanMeierFitter()\n    >>> f2.fit(data)\n    >>> f2.plot(ax=ax)\n    >>>\n    >>> # There are equivalent\n    >>> add_at_risk_counts(f1, f2)\n    >>> add_at_risk_counts(f1, f2, ax=ax, fig=fig)\n    >>>\n    >>> # This overrides the labels\n    >>> add_at_risk_counts(f1, f2, labels=['fitter one', 'fitter two'])\n    >>>\n    >>> # This hides the labels\n    >>> add_at_risk_counts(f1, f2, labels=None)\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    # Axes and Figure can't be None\n    ax = kwargs.get(\"ax\", None)\n    if ax is None:\n        ax = plt.gca()\n\n    fig = kwargs.get(\"fig\", None)\n    if fig is None:\n        fig = plt.gcf()\n\n    if \"labels\" not in kwargs:\n        labels = [f._label for f in fitters]\n    else:\n        # Allow None, in which case no labels should be used\n        labels = kwargs[\"labels\"]\n        if labels is None:\n            labels = [None] * len(fitters)\n    # Create another axes where we can put size ticks\n    ax2 = plt.twiny(ax=ax)\n    # Move the ticks below existing axes\n    # Appropriate length scaled for 6 inches. Adjust for figure size.\n    ax2_ypos = -0.15 * 6.0 / fig.get_figheight()\n    move_spines(ax2, [\"bottom\"], [ax2_ypos])\n    # Hide all fluff\n    remove_spines(ax2, [\"top\", \"right\", \"bottom\", \"left\"])\n    # Set ticks and labels on bottom\n    ax2.xaxis.tick_bottom()\n    # Match tick numbers and locations\n    ax2.set_xlim(ax.get_xlim())\n    ax2.set_xticks(ax.get_xticks())\n    # Remove ticks, need to do this AFTER moving the ticks\n    remove_ticks(ax2, x=True, y=True)\n    # Add population size at times\n    ticklabels = []\n    for tick in ax2.get_xticks():\n        lbl = \"\"\n        for f, l in zip(fitters, labels):\n            # First tick is prepended with the label\n            if tick == ax2.get_xticks()[0] and l is not None:\n                if is_latex_enabled():\n                    s = \"\\n{}\\\\quad\".format(l) + \"{}\"\n                else:\n                    s = \"\\n{}   \".format(l) + \"{}\"\n            else:\n                s = \"\\n{}\"\n            lbl += s.format(f.durations[f.durations >= tick].shape[0])\n        ticklabels.append(lbl.strip())\n    # Align labels to the right so numbers can be compared easily\n    ax2.set_xticklabels(ticklabels, ha=\"right\")\n\n    # Add a descriptive headline.\n    ax2.xaxis.set_label_coords(0, ax2_ypos)\n    ax2.set_xlabel(\"At risk\")\n\n    plt.tight_layout()\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_lifetimes(\n    durations,\n    event_observed=None,\n    entry=None,\n    left_truncated=False,\n    sort_by_duration=True,\n    event_observed_color=\"#A60628\",\n    event_censored_color=\"#348ABD\",\n    **kwargs\n):\n    \"\"\"\n    Returns a lifetime plot, see examples: https://lifelines.readthedocs.io/en/latest/Survival%20Analysis%20intro.html#Censoring\n\n    Parameters\n    -----------\n    durations: (n,) numpy array or pd.Series\n      duration subject was observed for.\n    event_observed: (n,) numpy array or pd.Series\n      array of booleans: True if event observed, else False.\n    entry: (n,) numpy array or pd.Series\n      offsetting the births away from t=0. This could be from left-truncation, or delayed entry into study.\n    left_truncated: boolean\n      if entry is provided, and the data is left-truncated, this will display additional information in the plot to reflect this.\n    sort_by_duration: boolean\n      sort by the duration vector\n    event_observed_color: str\n      default: \"#A60628\"\n    event_censored_color: str\n      default: \"#348ABD\"\n\n    Returns\n    -------\n    ax\n\n    Examples\n    ---------\n    >>> from lifelines.datasets import load_waltons\n    >>> from lifelines.plotting import plot_lifetimes\n    >>> T, E = load_waltons()[\"T\"], load_waltons()[\"E\"]\n    >>> ax = plot_lifetimes(T.loc[:50], event_observed=E.loc[:50])\n\n    \"\"\"\n    set_kwargs_ax(kwargs)\n    ax = kwargs.pop(\"ax\")\n\n    N = durations.shape[0]\n    if N > 80:\n        warnings.warn(\"For less visual clutter, you may want to subsample to less than 80 individuals.\")\n\n    if event_observed is None:\n        event_observed = np.ones(N, dtype=bool)\n\n    if entry is None:\n        entry = np.zeros(N)\n\n    assert durations.shape[0] == N\n    assert event_observed.shape[0] == N\n\n    if sort_by_duration:\n        # order by length of lifetimes;\n        ix = np.argsort(entry + durations, 0)\n        durations = durations[ix]\n        event_observed = event_observed[ix]\n        entry = entry[ix]\n\n    for i in range(N):\n        c = event_observed_color if event_observed[i] else event_censored_color\n        ax.hlines(i, entry[i], entry[i] + durations[i], color=c, lw=1.5)\n        if left_truncated:\n            ax.hlines(i, 0, entry[i], color=c, lw=1.0, linestyle=\"--\")\n        m = \"\" if not event_observed[i] else \"o\"\n        ax.scatter(entry[i] + durations[i], i, color=c, marker=m, s=10)\n\n    ax.set_ylim(-0.5, N)\n    return ax", "response": "Plots the lifetime of a subject."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _plot_estimate(\n    cls,\n    estimate=None,\n    confidence_intervals=None,\n    loc=None,\n    iloc=None,\n    show_censors=False,\n    censor_styles=None,\n    ci_legend=False,\n    ci_force_lines=False,\n    ci_alpha=0.25,\n    ci_show=True,\n    at_risk_counts=False,\n    **kwargs\n):\n\n    \"\"\"\n    Plots a pretty figure of {0}.{1}\n\n    Matplotlib plot arguments can be passed in inside the kwargs, plus\n\n    Parameters\n    -----------\n    show_censors: bool\n        place markers at censorship events. Default: False\n    censor_styles: bool\n        If show_censors, this dictionary will be passed into the plot call.\n    ci_alpha: bool\n        the transparency level of the confidence interval. Default: 0.3\n    ci_force_lines: bool\n        force the confidence intervals to be line plots (versus default shaded areas). Default: False\n    ci_show: bool\n        show confidence intervals. Default: True\n    ci_legend: bool\n        if ci_force_lines is True, this is a boolean flag to add the lines' labels to the legend. Default: False\n    at_risk_counts: bool\n        show group sizes at time points. See function ``add_at_risk_counts`` for details. Default: False\n    loc: slice\n        specify a time-based subsection of the curves to plot, ex:\n\n        >>> model.plot(loc=slice(0.,10.))\n\n        will plot the time values between t=0. and t=10.\n    iloc: slice\n        specify a location-based subsection of the curves to plot, ex:\n\n        >>> model.plot(iloc=slice(0,10))\n\n        will plot the first 10 time points.\n\n    Returns\n    -------\n    ax:\n        a pyplot axis object\n    \"\"\"\n    plot_estimate_config = PlotEstimateConfig(\n        cls, estimate, confidence_intervals, loc, iloc, show_censors, censor_styles, **kwargs\n    )\n\n    dataframe_slicer = create_dataframe_slicer(iloc, loc)\n\n    if show_censors and cls.event_table[\"censored\"].sum() > 0:\n        cs = {\"marker\": \"+\", \"ms\": 12, \"mew\": 1}\n        cs.update(plot_estimate_config.censor_styles)\n        times = dataframe_slicer(cls.event_table.loc[(cls.event_table[\"censored\"] > 0)]).index.values.astype(float)\n        v = cls.predict(times)\n        plot_estimate_config.ax.plot(times, v, linestyle=\"None\", color=plot_estimate_config.colour, **cs)\n\n    dataframe_slicer(plot_estimate_config.estimate_).rename(\n        columns=lambda _: plot_estimate_config.kwargs.pop(\"label\")\n    ).plot(**plot_estimate_config.kwargs)\n\n    # plot confidence intervals\n    if ci_show:\n        if ci_force_lines:\n            dataframe_slicer(plot_estimate_config.confidence_interval_).plot(\n                linestyle=\"-\",\n                linewidth=1,\n                color=[plot_estimate_config.colour],\n                legend=ci_legend,\n                drawstyle=plot_estimate_config.kwargs[\"drawstyle\"],\n                ax=plot_estimate_config.ax,\n                alpha=0.6,\n            )\n        else:\n            x = dataframe_slicer(plot_estimate_config.confidence_interval_).index.values.astype(float)\n            lower = dataframe_slicer(plot_estimate_config.confidence_interval_.filter(like=\"lower\")).values[:, 0]\n            upper = dataframe_slicer(plot_estimate_config.confidence_interval_.filter(like=\"upper\")).values[:, 0]\n\n            if plot_estimate_config.kwargs[\"drawstyle\"] == \"default\":\n                step = None\n            elif plot_estimate_config.kwargs[\"drawstyle\"].startswith(\"step\"):\n                step = plot_estimate_config.kwargs[\"drawstyle\"].replace(\"steps-\", \"\")\n\n            plot_estimate_config.ax.fill_between(\n                x, lower, upper, alpha=ci_alpha, color=plot_estimate_config.colour, linewidth=1.0, step=step\n            )\n\n    if at_risk_counts:\n        add_at_risk_counts(cls, ax=plot_estimate_config.ax)\n\n    return plot_estimate_config.ax", "response": "Plot an estimate of the current censoring hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntranslate a word document type of file and save the result as document.", "response": "def translate_doc(filename, destination='zh-CN', mix=True):\n    \"\"\"\n    translate a word document type of file and save the result as document and keep the exactly same file format. \n        :param filename: word doc file \n        :param destination='zh-CN': \n        :param mix=True: if True, will have original language and target language into the same doc. paragraphs by paragraphs.\n    \"\"\"\n    def tx(t): return Translator().translate(t, dest=destination).text\n    doc = Document(filename)\n    for p in doc.paragraphs:\n        txd = tx(p.text)\n\n        p.text = p.text + ('\\n' + txd if mix else '')\n\n    for table in doc.tables:\n        for row in table.rows:\n            for cell in row.cells:\n                txd = tx(cell.text)\n                p.text = cell.text + ('\\n' + txd if mix else '')\n\n    f = filename.replace('.doc', destination.lower() + '.doc')\n    doc.save(f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef translate(self, text, dest='en', src='auto'):\n        dest = dest.lower().split('_', 1)[0]\n        src = src.lower().split('_', 1)[0]\n\n        if src != 'auto' and src not in LANGUAGES:\n            if src in SPECIAL_CASES:\n                src = SPECIAL_CASES[src]\n            elif src in LANGCODES:\n                src = LANGCODES[src]\n            else:\n                raise ValueError('invalid source language')\n\n        if dest not in LANGUAGES:\n            if dest in SPECIAL_CASES:\n                dest = SPECIAL_CASES[dest]\n            elif dest in LANGCODES:\n                dest = LANGCODES[dest]\n            else:\n                raise ValueError('invalid destination language')\n\n        if isinstance(text, list):\n            result = []\n            for item in text:\n                translated = self.translate(item, dest=dest, src=src)\n                result.append(translated)\n            return result\n\n        origin = text\n        data = self._translate(text, dest, src)\n\n        # this code will be updated when the format is changed.\n        translated = ''.join([d[0] if d[0] else '' for d in data[0]])\n\n        extra_data = self._parse_extra_data(data)\n\n        # actual source language that will be recognized by Google Translator when the\n        # src passed is equal to auto.\n        try:\n            src = data[2]\n        except Exception:  # pragma: nocover\n            pass\n\n        pron = origin\n        try:\n            pron = data[0][1][-2]\n        except Exception:  # pragma: nocover\n            pass\n        if not PY3 and isinstance(pron, unicode) and isinstance(origin, str):  # pragma: nocover\n            origin = origin.decode('utf-8')\n        if dest in EXCLUDES and pron == origin:\n            pron = translated\n\n        # for python 2.x compatbillity\n        if not PY3:  # pragma: nocover\n            if isinstance(src, str):\n                src = src.decode('utf-8')\n            if isinstance(dest, str):\n                dest = dest.decode('utf-8')\n            if isinstance(translated, str):\n                translated = translated.decode('utf-8')\n\n        # put final values into a new Translated object\n        result = Translated(src=src, dest=dest, origin=origin,\n                            text=translated, pronunciation=pron, extra_data=extra_data)\n\n        return result", "response": "Translate text from source language to destination language."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect(self, text):\n        if isinstance(text, list):\n            result = []\n            for item in text:\n                lang = self.detect(item)\n                result.append(lang)\n            return result\n\n        data = self._translate(text, dest='en', src='auto')\n\n        # actual source language that will be recognized by Google Translator when the\n        # src passed is equal to auto.\n        src = ''\n        confidence = 0.0\n        try:\n            src = ''.join(data[8][0])\n            confidence = data[8][-2][0]\n        except Exception:  # pragma: nocover\n            pass\n        result = Detected(lang=src, confidence=confidence)\n\n        return result", "response": "Detect language of the input text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_wheel_filename_generic(wheel):\n    name, version, python, abi, platform = wheel.split(\"-\")\n\n    # our binary handles multiple abi/versions of python\n    python, abi = \"py2.py3\", \"none\"\n\n    # hack, lets pretend to be manylinux1 so we can do a binary distribution\n    if platform == \"linux_x86_64.whl\":\n        platform = \"manylinux1_x86_64.whl\"\n    elif platform == \"linux_i686.whl\":\n        platform = \"manylinux1_i686.whl\"\n\n    return \"-\".join((name, version, python, abi, platform))", "response": "Make a filename for a wheel."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef normalize_words(self, ord=2, inplace=False):\n    if ord == 2:\n      ord = None # numpy uses this flag to indicate l2.\n    vectors = self.vectors.T / np.linalg.norm(self.vectors, ord, axis=1)\n    if inplace:\n      self.vectors = vectors.T\n      return self\n    return Embedding(vectors=vectors.T, vocabulary=self.vocabulary)", "response": "Normalize embeddings matrix row - wise."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nearest_neighbors(self, word, top_k=10):\n    #TODO(rmyeid): Use scikit ball tree, if scikit is available\n    point = self[word]\n    diff = self.vectors - point\n    distances = np.linalg.norm(diff, axis=1)\n    top_ids = distances.argsort()[1:top_k+1]\n    return [self.vocabulary.id_word[i] for i in top_ids]", "response": "Return the nearest k words to the given word."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef distances(self, word, words):\n\n    point = self[word]\n    vectors = np.asarray([self[w] for w in words])\n    diff = vectors - point\n    distances = np.linalg.norm(diff, axis=1)\n    return distances", "response": "Calculate eucledean pairwise distances between word and words."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_word2vec(fname, fvocab=None, binary=False):\n    vocabulary = None\n    if fvocab is not None:\n      logger.info(\"loading word counts from %s\" % (fvocab))\n      vocabulary = Embedding.from_word2vec_vocab(fvocab)\n\n    logger.info(\"loading projection weights from %s\" % (fname))\n    if binary:\n      words, vectors = Embedding._from_word2vec_binary(fname)\n    else:\n      words, vectors = Embedding._from_word2vec_text(fname)\n\n    if not vocabulary:\n      vocabulary = OrderedVocabulary(words=words)\n\n    return Embedding(vocabulary=vocabulary, vectors=vectors)", "response": "Load the input - hidden weight matrix from a word2vec - tool format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading an embedding dump generated by save", "response": "def load(fname):\n    \"\"\"Load an embedding dump generated by `save`\"\"\"\n\n    content = _open(fname).read()\n    if PY2:\n      state = pickle.loads(content)\n    else:\n      state = pickle.loads(content, encoding='latin1')\n    voc, vec = state\n    if len(voc) == 2:\n      words, counts = voc\n      word_count = dict(zip(words, counts))\n      vocab = CountedVocabulary(word_count=word_count)\n    else:\n      vocab = OrderedVocabulary(voc)\n    return Embedding(vocabulary=vocab, vectors=vec)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save(self, fname):\n\n    vec = self.vectors\n    voc = self.vocabulary.getstate()\n    state = (voc, vec)\n    with open(fname, 'wb') as f:\n      pickle.dump(state, f, protocol=pickle.HIGHEST_PROTOCOL)", "response": "Save a pickled version of the embedding into fname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunzip the contents of the zip file filename into the directory root.", "response": "def unzip(filename, root, verbose=True):\n  \"\"\"\n  Extract the contents of the zip file ``filename`` into the\n  directory ``root``.\n  \"\"\"\n  for message in _unzip_iter(filename, root, verbose):\n    if isinstance(message, ErrorMessage):\n      raise Exception(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nbuilds a new data. xml index file from the given root directory.", "response": "def build_index(root, base_url):\n  \"\"\"\n  Create a new data.xml index file, by combining the xml description\n  files for various packages and collections.  ``root`` should be the\n  path to a directory containing the package xml and zip files; and\n  the collection xml files.  The ``root`` directory is expected to\n  have the following subdirectories::\n\n    root/\n    packages/ .................. subdirectory for packages\n      corpora/ ................. zip & xml files for corpora\n      grammars/ ................ zip & xml files for grammars\n      taggers/ ................. zip & xml files for taggers\n      tokenizers/ .............. zip & xml files for tokenizers\n      etc.\n    collections/ ............... xml files for collections\n\n  For each package, there should be two files: ``package.zip``\n  (where *package* is the package name)\n  which contains the package itself as a compressed zip file; and\n  ``package.xml``, which is an xml description of the package.  The\n  zipfile ``package.zip`` should expand to a single subdirectory\n  named ``package/``.  The base filename ``package`` must match\n  the identifier given in the package's xml file.\n\n  For each collection, there should be a single file ``collection.zip``\n  describing the collection, where *collection* is the name of the collection.\n\n  All identifiers (for both packages and collections) must be unique.\n  \"\"\"\n  # Find all packages.\n  packages = []\n  for pkg_xml, zf, subdir in _find_packages(os.path.join(root, 'packages')):\n    zipstat = os.stat(zf.filename)\n    url = '%s/%s/%s' % (base_url, subdir, os.path.split(zf.filename)[1])\n    unzipped_size = sum(zf_info.file_size for zf_info in zf.infolist())\n\n    # Fill in several fields of the package xml with calculated values.\n    pkg_xml.set('unzipped_size', '%s' % unzipped_size)\n    pkg_xml.set('size', '%s' % zipstat.st_size)\n    pkg_xml.set('subdir', subdir)\n    pkg_xml.set('url', url)\n\n    # Record the package.\n    packages.append(pkg_xml)\n\n  # Find all collections\n  collections = list(_find_collections(os.path.join(root, 'collections')))\n\n  # Check that all UIDs are unique\n  uids = set()\n  for item in packages+collections:\n    if item.get('id') in uids:\n      raise ValueError('Duplicate UID: %s' % item.get('id'))\n    uids.add(item.get('id'))\n\n  # Put it all together\n  top_elt = ElementTree.Element('polyglot_data')\n  top_elt.append(ElementTree.Element('packages'))\n  for package in packages: top_elt[0].append(package)\n  top_elt.append(ElementTree.Element('collections'))\n  for collection in collections: top_elt[1].append(collection)\n\n  _indent_xml(top_elt)\n  return top_elt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _indent_xml(xml, prefix=''):\n  if len(xml) > 0:\n    xml.text = (xml.text or '').strip() + '\\n' + prefix + '  '\n    for child in xml:\n      _indent_xml(child, prefix+'  ')\n    for child in xml[:-1]:\n      child.tail = (child.tail or '').strip() + '\\n' + prefix + '  '\n    xml[-1].tail = (xml[-1].tail or '').strip() + '\\n' + prefix", "response": "Helper for _indent_xml - Adds indentation to the XML element tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_package(pkg_xml, zipfilename, zf):\n  # The filename must patch the id given in the XML file.\n  uid = os.path.splitext(os.path.split(zipfilename)[1])[0]\n  if pkg_xml.get('id') != uid:\n    raise ValueError('package identifier mismatch (%s vs %s)' %\n             (pkg_xml.get('id'), uid))\n\n  # Zip file must expand to a subdir whose name matches uid.\n  if sum( (name!=uid and not name.startswith(uid+'/'))\n      for name in zf.namelist() ):\n    raise ValueError('Zipfile %s.zip does not expand to a single '\n             'subdirectory %s/' % (uid, uid))", "response": "Check that the given package is consistent with the given zip file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _svn_revision(filename):\n  p = subprocess.Popen(['svn', 'status', '-v', filename],\n             stdout=subprocess.PIPE,\n             stderr=subprocess.PIPE)\n  (stdout, stderr) = p.communicate()\n  if p.returncode != 0 or stderr or not stdout:\n    raise ValueError('Error determining svn_revision for %s: %s' %\n             (os.path.split(filename)[1], textwrap.fill(stderr)))\n  return stdout.split()[2]", "response": "Helper for build_index (). Calculate the revision number for a given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef status(self, info_or_id, download_dir=None):\n    if download_dir is None: download_dir = self._download_dir\n    info = self._info_or_id(info_or_id)\n\n    # Handle collections:\n    if isinstance(info, Collection):\n      pkg_status = [self.status(pkg.id) for pkg in info.packages]\n      if self.STALE in pkg_status:\n        return self.STALE\n      elif self.PARTIAL in pkg_status:\n        return self.PARTIAL\n      elif (self.INSTALLED in pkg_status and\n          self.NOT_INSTALLED in pkg_status):\n        return self.PARTIAL\n      elif self.NOT_INSTALLED in pkg_status:\n        return self.NOT_INSTALLED\n      else:\n        return self.INSTALLED\n\n    # Handle packages:\n    else:\n      filepath = os.path.join(download_dir, info.filename)\n      if download_dir != self._download_dir:\n        status = self._pkg_status(info, filepath)\n      else:\n        if info.id not in self._status_cache:\n          self._status_cache[info.id] = self._pkg_status(info,\n                                   filepath)\n        return self._status_cache[info.id]", "response": "Returns a constant describing the status of the given package or collection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the Package or Collection record for the given item.", "response": "def info(self, id):\n    \"\"\"Return the ``Package`` or ``Collection`` record for the\n       given item.\"\"\"\n    #self._update_index() # This is commented because it leads to\n                          # excessive network load\n    if id in self._packages: return self._packages[id]\n    if id in self._collections: return self._collections[id]\n    self._update_index() # If package is not found, most probably we did not\n                         # warm up the cache\n    if id in self._packages: return self._packages[id]\n    if id in self._collections: return self._collections[id]\n    raise ValueError('Package %r not found in index' % id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the collection that represents a specific language or task.", "response": "def get_collection(self, lang=None, task=None):\n    \"\"\" Return the collection that represents a specific language or task.\n\n    Args:\n      lang (string): Language code.\n      task (string): Task name.\n    \"\"\"\n    if lang: id = \"{}{}\".format(Downloader.LANG_PREFIX, lang)\n    elif task: id = \"{}{}\".format(Downloader.TASK_PREFIX, task)\n    else: raise ValueError(\"You should pass either the task or the lang\")\n    try:\n      return self.info(id)\n    except ValueError as e:\n      if lang: raise LanguageNotSupported(\"Language {} is not supported\".format(id))\n      if task: raise TaskNotSupported(\"Task {} is not supported\".format(id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef supported_language(lang):\n    try:\n      self.get_collection(lang=lang)\n      return True\n    except LanguageNotSupported as e:\n      return False", "response": "Return True if the polyglot supports the language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef supported_languages(self, task=None):\n    if task:\n      collection = self.get_collection(task=task)\n      return [isoLangs[x.id.split('.')[1]][\"name\"]\n                                         for x in collection.packages]\n    else:\n      return [x.name.split()[0] for x in self.collections()\n                                         if Downloader.LANG_PREFIX in x.id]", "response": "Returns a list of languages that are covered by a specific task."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all tasks that are supported by a specific language.", "response": "def supported_tasks(self, lang=None):\n    \"\"\"Languages that are covered by a specific task.\n\n    Args:\n      lang (string): Language code name.\n    \"\"\"\n    if lang:\n      collection = self.get_collection(lang=lang)\n      return [x.id.split('.')[0] for x in collection.packages]\n    else:\n      return [x.name.split()[0] for x in self.collections() if Downloader.TASK_PREFIX in x.id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the XML info record for the given item", "response": "def xmlinfo(self, id):\n    \"\"\"Return the XML info record for the given item\"\"\"\n    self._update_index()\n    for package in self._index.findall('packages/package'):\n      if package.get('id') == id:\n        return package\n    for collection in self._index.findall('collections/collection'):\n      if collection.get('id') == id:\n        return collection\n    raise ValueError('Package %r not found in index' % id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset a new URL for the data server.", "response": "def _set_url(self, url):\n    \"\"\"\n    Set a new URL for the data server. If we're unable to contact\n    the given url, then the original url is kept.\n    \"\"\"\n    original_url = self._url\n    try:\n      self._update_index(url)\n    except:\n      self._url = original_url\n      raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _decoder(self):\n    if self.target_lang == 'en':\n      return Transliterator._dummy_coder\n    else:\n      weights = load_transliteration_table(self.target_lang)\n      decoder_weights = weights[\"decoder\"]\n      return Transliterator._transliterate_string(decoder_weights)", "response": "Transliterate a string from English to the target language."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransliterate a string from the input language to English.", "response": "def _encoder(self):\n    \"\"\"Transliterate a string from the input language to English.\"\"\"\n    if self.source_lang == 'en':\n      return Transliterator._dummy_coder\n    else:\n      weights = load_transliteration_table(self.source_lang)\n      encoder_weights = weights[\"encoder\"]\n      return Transliterator._transliterate_string(encoder_weights)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntransliterate the word from its source language to the target language.", "response": "def transliterate(self, word):\n    \"\"\"Transliterate the word from its source language to the target one.\n\n    The method works by encoding the word into English then decoding the new\n    Enlgish word to the target language.\n    \"\"\"\n    encoded_word = self.encoder(word)\n    decoded_word = self.decoder(encoded_word)\n    return decoded_word"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tokens(self):\n    seq = self.word_tokenizer.transform(Sequence(self.raw))\n    return WordList(seq.tokens(), parent=self, language=self.language.code)", "response": "Return a list of tokens using this blob s tokenizer object\n    ( defaults to WordTokenizer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef polarity(self):\n    scores = [w.polarity for w in self.words if w.polarity != 0]\n    if not scores:\n      return 0.0\n    return sum(scores) / float(len(scores))", "response": "Return the polarity score as a float within the range [- 1. 0 1. 0 )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transliterate(self, target_language=\"en\"):\n    return WordList([w.transliterate(target_language) for w in self.words],\n                     language=target_language, parent=self)", "response": "Transliterate the string to the target language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of entities for this blob.", "response": "def entities(self):\n    \"\"\"Returns a list of entities for this blob.\"\"\"\n    start = 0\n    end = 0\n    prev_tag = u'O'\n    chunks = []\n    for i, (w, tag) in enumerate(self.ne_chunker.annotate(self.words)):\n      if tag != prev_tag:\n        if prev_tag == u'O':\n          start = i\n        else:\n          chunks.append(Chunk(self.words[start: i], start, i, tag=prev_tag,\n                              parent=self))\n        prev_tag = tag\n    if tag != u'O':\n      chunks.append(Chunk(self.words[start: i+1], start, i+1, tag=tag,\n                          parent=self))\n    return chunks"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pos_tags(self):\n    tagged_words = []\n    for word,t in self.pos_tagger.annotate(self.words):\n      word.pos_tag = t\n      tagged_words.append((word, t))\n    return tagged_words", "response": "Returns a list of tuples of the form word POS tag."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transfer_pos_tags(self):\n    tagged_words = []\n    for word,t in self.transfer_pos_tagger.annotate(self.words):\n      word.pos_tag = t\n      tagged_words.append((word, t))\n    return tagged_words", "response": "Returns an array of tuples of the form word POS tag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef np_counts(self):\n    counts = defaultdict(int)\n    for phrase in self.noun_phrases:\n        counts[phrase] += 1\n    return counts", "response": "Dictionary of noun phrase frequencies in this text."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ngrams(self, n=3):\n    if n <= 0:\n        return []\n    grams = [WordList(self.words[i:i+n], parent=self)\n                        for i in range(len(self.words) - n + 1)]\n    return grams", "response": "Return a list of n - grams for this\n    blob."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef correct(self):\n    # regex matches: contraction or word or punctuation or whitespace\n    tokens = nltk.tokenize.regexp_tokenize(self.raw, \"\\w*('\\w*)+|\\w+|[^\\w\\s]|\\s\")\n    corrected = (Word(w).correct() for w in tokens)\n    ret = ''.join(corrected)\n    return self.__class__(ret)", "response": "Correct the spelling of a blob."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a new WordList with the elements split by sep.", "response": "def split(self, sep=None, maxsplit=sys.maxsize):\n    \"\"\"Behaves like the built-in str.split() except returns a\n    WordList.\n    :rtype: :class:`WordList <WordList>`\n    \"\"\"\n    return WordList(self._strkey().split(sep, maxsplit), parent=self)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transliterate(self, target_language=\"en\"):\n    t = Transliterator(source_lang=self.language,\n                       target_lang=target_language)\n    return t.transliterate(self.string)", "response": "Transliterate the string to the target language."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the count of a word or phrase s within this WordList.", "response": "def count(self, strg, case_sensitive=False, *args, **kwargs):\n    \"\"\"Get the count of a word or phrase `s` within this WordList.\n    :param strg: The string to count.\n    :param case_sensitive: A boolean, whether or not the search is case-sensitive.\n    \"\"\"\n    if not case_sensitive:\n        return [word.lower() for word in self].count(strg.lower(), *args,\n                **kwargs)\n    return self._collection.count(strg, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append(self, obj):\n    if isinstance(obj, basestring):\n        return self._collection.append(Word(obj))\n    else:\n        return self._collection.append(obj)", "response": "Append an object to the end of the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef extend(self, iterable):\n    [self._collection.append(Word(e) if isinstance(e, basestring) else e)\n        for e in iterable]\n    return self", "response": "Extend this WordList by appending elements from iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _sentiment(self, distance=True):\n    sum_pos = 0\n    sum_neg = 0\n    text = self.parent\n    entity_positions = range(self.start, self.end)\n    non_entity_positions = set(range(len(text.words))).difference(entity_positions)\n    if not distance:\n      non_entity_polarities = np.array([text.words[i].polarity for i in non_entity_positions])\n      sum_pos = sum(non_entity_polarities == 1)\n      sum_neg = sum(non_entity_polarities == -1)\n    else:\n      polarities = np.array([w.polarity for w in text.words])\n      polarized_positions = np.argwhere(polarities != 0)[0]\n      polarized_non_entity_positions = non_entity_positions.intersection(polarized_positions)\n      sentence_len = len(text.words)\n      for i in polarized_non_entity_positions:\n        min_dist = min(abs(self.start - i), abs(self.end - i))\n        if text.words[i].polarity == 1:\n          sum_pos += 1.0 - (min_dist - 1.0) / (2.0 * sentence_len)\n        else:\n          sum_neg += 1.0 - (min_dist - 1.0) / (2.0 *sentence_len)\n    return (sum_pos, sum_neg)", "response": "Calculates the sentiment of an entity as it appears in text."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_json(self, *args, **kwargs):\n    '''Return a json representation (str) of this blob.\n    Takes the same arguments as json.dumps.\n    .. versionadded:: 0.5.1\n    '''\n    try:\n      import ujson as json\n    except ImportError:\n      import json\n    return json.dumps(self.serialized, *args, **kwargs)", "response": "Return a json representation of this blob."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_sentence_objects(self):\n    '''Returns a list of Sentence objects from the raw text.\n    '''\n    sentence_objects = []\n    sent_tokenizer = SentenceTokenizer(locale=self.language.code)\n    seq = Sequence(self.raw)\n    seq = sent_tokenizer.transform(seq)\n    for start_index, end_index in zip(seq.idx[:-1], seq.idx[1:]):\n      # Sentences share the same models as their parent blob\n      sent = seq.text[start_index: end_index].strip()\n      if not sent: continue\n      s = Sentence(sent, start_index=start_index, end_index=end_index)\n      s.detected_languages = self.detected_languages\n      sentence_objects.append(s)\n    return sentence_objects", "response": "Returns a list of Sentence objects from the raw text."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef detect(self, text):\n    t = text.encode(\"utf-8\")\n    reliable, index, top_3_choices = cld2.detect(t, bestEffort=False)\n\n    if not reliable:\n      self.reliable = False\n      reliable, index, top_3_choices = cld2.detect(t, bestEffort=True)\n      \n      if not self.quiet:\n        if not reliable:\n          raise UnknownLanguage(\"Try passing a longer snippet of text\")\n        else:\n          logger.warning(\"Detector is not able to detect the language reliably.\")\n\n    self.languages = [Language(x) for x in top_3_choices]\n    self.language = self.languages[0]\n    return self.language", "response": "Detect the language used to write the text."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlocating a resource in the index.", "response": "def locate_resource(name, lang, filter=None):\n  \"\"\"Return filename that contains specific language resource name.\n\n  Args:\n    name (string): Name of the resource.\n    lang (string): language code to be loaded.\n  \"\"\"\n  task_dir = resource_dir.get(name, name)\n  package_id = u\"{}.{}\".format(task_dir, lang)\n  p = path.join(polyglot_path, task_dir, lang)\n  if not path.isdir(p):\n    if downloader.status(package_id) != downloader.INSTALLED:\n      raise ValueError(\"This resource is available in the index \"\n                       \"but not downloaded, yet. Try to run\\n\\n\"\n                       \"polyglot download {}\".format(package_id))\n  return path.join(p, os.listdir(p)[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_embeddings(lang=\"en\", task=\"embeddings\", type=\"cw\", normalize=False):\n  src_dir = \"_\".join((type, task)) if type else task\n  p = locate_resource(src_dir, lang)\n  e = Embedding.load(p)\n  if type == \"cw\":\n    e.apply_expansion(CaseExpander)\n    e.apply_expansion(DigitExpander)\n  if type == \"sgns\":\n    e.apply_expansion(CaseExpander)\n  if type == \"ue\":\n    e.apply_expansion(CaseExpander)\n  if normalize:\n    e.normalize_words(inplace=True)\n  return e", "response": "Load word embeddings for lang and of type type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_vocabulary(lang=\"en\", type=\"wiki\"):\n  src_dir = \"{}_vocab\".format(type)\n  p = locate_resource(src_dir, lang)\n  return CountedVocabulary.from_vocabfile(p)", "response": "Load a CountedVocabulary object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a named entity extractor parameters for lang and of version version.", "response": "def load_ner_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a named entity extractor parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"ner{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  try:\n    return pickle.load(fh)\n  except UnicodeDecodeError:\n    fh.seek(0)\n    return pickle.load(fh, encoding='latin1')"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the pos model for lang and of version version.", "response": "def load_pos_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a part of speech tagger parameters for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"pos{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  fh = _open(p)\n  return dict(np.load(fh))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a Morfessor model for lang and of version version.", "response": "def load_morfessor_model(lang=\"en\", version=\"2\"):\n  \"\"\"Return a morfessor model for `lang` and of version `version`\n\n  Args:\n    lang (string): language code.\n    version (string): version of the parameters to be used.\n  \"\"\"\n  src_dir = \"morph{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  file_handler = _open(p)\n  tmp_file_ = NamedTemporaryFile(delete=False)\n  tmp_file_.write(file_handler.read())\n  tmp_file_.close()\n  io = morfessor.MorfessorIO()\n  model = io.read_any_model(tmp_file_.name)\n  os.remove(tmp_file_.name)\n  return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_transliteration_table(lang=\"en\", version=\"2\"):\n  src_dir = \"transliteration{}\".format(version)\n  p = locate_resource(src_dir, lang)\n  file_handler = _open(p)\n  return pickle.load(file_handler)", "response": "Load a transliteration table for lang and of version version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef detect(args):\n  for l in args.input:\n    if l.strip():\n      _print(\"{:<20}{}\".format(Detector(l).language.name, l.strip()))", "response": "Detect the language of each line."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntag a chunk named entities.", "response": "def tag(tagger, args):\n  \"\"\"Chunk named entities.\"\"\"\n  for l in args.input:\n    words = l.strip().split()\n    line_annotations = [u\"{:<16}{:<5}\".format(w,p) for w, p in tagger.annotate(words)]\n    _print(u\"\\n\".join(line_annotations))\n    _print(u\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransliterating words according to the target language.", "response": "def transliterate(args):\n  \"\"\"Transliterate words according to the target language.\"\"\"\n  t = Transliterator(source_lang=args.lang,\n                     target_lang=args.target)\n  for l in args.input:\n    words = l.strip().split()\n    line_annotations = [u\"{:<16}{:<16}\".format(w, t.transliterate(w)) for w in words]\n    _print(u\"\\n\".join(line_annotations))\n    _print(u\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsegmenting words according to their morphemes.", "response": "def morphemes(args):\n  \"\"\"Segment words according to their morphemes.\"\"\"\n  morfessor = load_morfessor_model(lang=args.lang)\n  for l in args.input:\n    words = l.strip().split()\n    morphemes = [(w, u\"_\".join(morfessor.viterbi_segment(w)[0])) for w in words]\n    line_annotations = [u\"{:<16}{:<5}\".format(w,p) for w, p in morphemes]\n    _print(u\"\\n\".join(line_annotations))\n    _print(u\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntagging words with their part of speech.", "response": "def pos_tag(args):\n  \"\"\"Tag words with their part of speech.\"\"\"\n  tagger = POSTagger(lang=args.lang)\n  tag(tagger, args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads polyglot packages and models.", "response": "def download(args):\n  \"\"\" Download polyglot packages and models.\"\"\"\n\n  downloader = Downloader(server_index_url = args.server_index_url)\n  if args.packages:\n    for pkg_id in args.packages:\n      rv = downloader.download(info_or_id=unicode(pkg_id), download_dir=args.dir,\n                               quiet=args.quiet, force=args.force,\n                               halt_on_error=args.halt_on_error)\n      if rv == False and args.halt_on_error:\n        break\n  else:\n    downloader.download(download_dir=args.dir, quiet=args.quiet, force=args.force,\n                        halt_on_error=args.halt_on_error)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nopening file object given filenames open files or even archives.", "response": "def _open(file_, mode='r'):\n  \"\"\"Open file object given filenames, open files or even archives.\"\"\"\n  if isinstance(file_, string_types):\n    _, ext = path.splitext(file_)\n    if ext in {'.bz2', '.gz'}:\n      s = tarfile.open(file_)\n      return s.extractfile(s.next())\n    else:\n      return open(file_, mode)\n  return file_"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unpickle_method(func_name, obj, cls):\n\n  if obj is None:\n    return cls.__dict__[func_name].__get__(obj, cls)\n  for cls in cls.__mro__:\n    try:\n      func = cls.__dict__[func_name]\n    except KeyError:\n      pass\n    else:\n      break\n  return func.__get__(obj, cls)", "response": "Unpickle methods properly including class methods."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _linkcode_resolve(domain, info, package, url_fmt, revision):\n\n    if revision is None:\n        return\n    if domain not in ('py', 'pyx'):\n        return\n    if not info.get('module') or not info.get('fullname'):\n        return\n\n    class_name = info['fullname'].split('.')[0]\n    if type(class_name) != str:\n        # Python 2 only\n        class_name = class_name.encode('utf-8')\n    module = __import__(info['module'], fromlist=[class_name])\n    try:\n      obj = attrgetter(info['fullname'])(module)\n    except AttributeError:\n      return\n\n    try:\n        fn = inspect.getsourcefile(obj)\n    except Exception:\n        fn = None\n    if not fn:\n        try:\n            fn = inspect.getsourcefile(sys.modules[obj.__module__])\n        except Exception:\n            fn = None\n    if not fn:\n        return\n\n    fn = os.path.relpath(fn,\n                         start=os.path.dirname(__import__(package).__file__))\n    try:\n        lineno = inspect.getsourcelines(obj)[1]\n    except Exception:\n        lineno = ''\n    return url_fmt.format(revision=revision, package=package,\n                          path=fn, lineno=lineno)", "response": "Return a link to online source for a class or method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_linkcode_resolve(package, url_fmt):\n    revision = _get_git_revision()\n    return partial(_linkcode_resolve, revision=revision, package=package,\n                   url_fmt=url_fmt)", "response": "Returns a function that returns a linkcode_resolve function for the given URL format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef count(lines):\n  words = [w for l in lines for w in l.strip().split()]\n  return Counter(words)", "response": "Counts the number of words in a list of sentences."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nguaranteeing that all textual symbols are unicode.", "response": "def sanitize_words(self, words):\n    \"\"\"Guarantees that all textual symbols are unicode.\n\n    Note:\n      We do not convert numbers, only strings to unicode.\n      We assume that the strings are encoded in utf-8.\n    \"\"\"\n    _words = []\n    for w in words:\n      if isinstance(w, string_types) and not isinstance(w, unicode):\n        _words.append(unicode(w, encoding=\"utf-8\"))\n      else:\n        _words.append(w)\n    return _words"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_vocabfile(cls, filename):\n    words = [x.strip() for x in _open(filename, 'r').read().splitlines()]\n    return cls(words=words)", "response": "Construct a CountedVocabulary object from a vocabulary file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new vocabulary from a text file.", "response": "def from_textfile(cls, textfile, workers=1, job_size=1000):\n    \"\"\" Count the set of words appeared in a text file.\n\n    Args:\n      textfile (string): The name of the text file or `TextFile` object.\n      min_count (integer): Minimum number of times a word/token appeared in the document\n                 to be considered part of the vocabulary.\n      workers (integer): Number of parallel workers to read the file simulatenously.\n      job_size (integer): Size of the batch send to each worker.\n      most_frequent (integer): if no min_count is specified, consider the most frequent k words for the vocabulary.\n\n    Returns:\n      A vocabulary of the most frequent words appeared in the document.\n    \"\"\"\n\n    c = Counter()\n    if isinstance(textfile, string_types):\n      textfile = TextFile(textfile)\n    for result in textfile.apply(count, workers, job_size):\n      c.update(result)\n    return CountedVocabulary(word_count=c)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef most_frequent(self, k):\n    word_count = {w:self.word_count[w] for w in self.words[:k]}\n    return CountedVocabulary(word_count=word_count)", "response": "Returns a vocabulary with the most frequent k words."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a vocabulary after eliminating the words that appear < n.", "response": "def min_count(self, n=1):\n    \"\"\" Returns a vocabulary after eliminating the words that appear < `n`.\n\n    Args:\n      n (integer): specifies the minimum word frequency allowed.\n    \"\"\"\n    word_count = {w:c for w,c in iteritems(self.word_count) if c >= n}\n    return CountedVocabulary(word_count=word_count)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a CountedVocabulary object from a vocabulary file.", "response": "def from_vocabfile(filename):\n    \"\"\" Construct a CountedVocabulary out of a vocabulary file.\n\n    Note:\n      File has the following format word1 count1\n                                    word2 count2\n    \"\"\"\n    word_count = [x.strip().split() for x in _open(filename, 'r').read().splitlines()]\n    word_count = {w:int(c) for w,c in word_count}\n    return CountedVocabulary(word_count=word_count)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(self, sequence):\n\n    major_idx = sequence.idx\n    idx2 = 0\n    for start, end in zip(major_idx[:-1], major_idx[1:]):\n      idx1 = self.idx.index(start, idx2)\n      idx2 = self.idx.index(end, idx2)\n      seq = Sequence(self.text[start:end])\n      seq.idx = [x-start for x in self.idx[idx1:idx2]]\n      yield seq", "response": "Split into subsequences according to sequence."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_delimiter(self, byte_size=8192):\n    partial = u''\n    while True:\n      read_chars = self.read(byte_size)\n      if not read_chars: break\n      partial += read_chars\n      lines = partial.split(self.delimiter)\n      partial = lines.pop()\n\n      for line in lines:\n        yield line + self.delimiter\n\n    if partial:\n      yield partial", "response": "This function yields the lines of the file in a delimited by \\ n."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads size bytes from the file.", "response": "def read(self, size=None):\n    \"\"\" Read `size` of bytes.\"\"\"\n    if size is None:\n      return self.buf.read() + self.open_file.read()\n    contents = self.buf.read(size)\n    if len(contents) < size:\n      contents += self.open_file.read(size - len(contents))\n    return contents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply(self, func, workers=1, job_size=10000):\n    if workers == 1:\n      for lines in self.iter_chunks(job_size):\n        yield func(lines)\n    else:\n      with ProcessPoolExecutor(max_workers=workers) as executor:\n        for result in executor.map(func, self.iter_chunks(job_size)):\n          yield result", "response": "Apply func to lines of text in parallel or sequential."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef annotate(self, sent):\n    preds = []\n    words = []\n    for word, fv in self.sent2examples(sent):\n      probs = self.predictor(fv)\n      tags = probs.argsort()\n      tag = self.ID_TAG[tags[-1]]\n\n      words.append(word)\n      preds.append(tag)\n\n    # fix_chunks(preds)\n    annotations = zip(words, preds)\n    return annotations", "response": "Annotate a squence of words with entity tags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sent2examples(self, sent):\n\n    # TODO(rmyeid): use expanders.\n    words = [w if w in self.embeddings else TaggerBase.UNK for w in sent]\n    ngrams = TaggerBase.ngrams(words, self.context, self.transfer)\n    fvs = []\n    for word, ngram in zip(sent, ngrams):\n      fv = np.array([self.embeddings.get(w, self.embeddings.zero_vector()) for w in ngram]).flatten()\n      if self.add_bias:\n        fv = np.hstack((fv, np.array(1)))\n      yield word, fv", "response": "Convert ngrams into feature vectors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild the predictor out of the model.", "response": "def _load_network(self):\n    \"\"\" Building the predictor out of the model.\"\"\"\n    self.embeddings = load_embeddings(self.lang, type='cw', normalize=True)\n    self.model = load_ner_model(lang=self.lang, version=2)\n    first_layer, second_layer = self.model\n\n    def predict_proba(input_):\n      hidden = np.tanh(np.dot(first_layer, input_))\n      hidden = np.hstack((hidden, np.ones((hidden.shape[0], 1))))\n      output =  (second_layer *  hidden).sum(axis=1)\n      output_ = 1.0/(1.0 + np.exp(-output))\n      probs = output_/output_.sum()\n      return probs\n    return predict_proba"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_network(self):\n    self.embeddings = load_embeddings(self.lang, type='cw')\n    self.model = load_pos_model(lang=self.lang, version=2)\n\n    def predict_proba(input_):\n      hidden = np.tanh(np.dot(input_, self.model[\"W1\"]) + self.model[\"b1\"])\n      output =  np.dot(hidden, self.model[\"W2\"]) + self.model[\"b2\"]\n      scores = np.exp(output)\n      probs = scores/scores.sum()\n      return probs\n    return predict_proba", "response": "Builds the predictor out of the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconstructs url based on base_api and params.", "response": "def _construct_url(self, base_api, params):\n        \"\"\"\n        Construct geocoding request url. Overridden.\n\n        :param str base_api: Geocoding function base address - self.api\n            or self.reverse_api.\n\n        :param dict params: Geocoding params.\n\n        :return: string URL.\n        \"\"\"\n        params['key'] = self.api_key\n        return super(OpenMapQuest, self)._construct_url(base_api, params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a location point by address or query.", "response": "def geocode(\n            self,\n            query,\n            limit=None,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param int limit: Defines the maximum number of items in the\n            response structure. If not provided and there are multiple\n            results the BAN API will return 5 results by default.\n            This will be reset to one if ``exactly_one`` is True.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n\n        params = {\n            'q': self.format_string % query,\n        }\n\n        if limit is not None:\n            params['limit'] = limit\n\n        url = \"?\".join((self.geocode_api, urlencode(params)))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps self. _call_geocoder handling tokens.", "response": "def _authenticated_call_geocoder(self, url, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Wrap self._call_geocoder, handling tokens.\n        \"\"\"\n        if self.token is None or int(time()) > self.token_expiry:\n            self._refresh_authentication_token()\n        request = Request(\n            \"&\".join((url, urlencode({\"token\": self.token}))),\n            headers={\"Referer\": self.referer}\n        )\n        return self._base_call_geocoder(request, timeout=timeout)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a location point by address or query.", "response": "def geocode(self, query, exactly_one=True, timeout=DEFAULT_SENTINEL,\n                out_fields=None):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param out_fields: A list of output fields to be returned in the\n            attributes field of the raw data. This can be either a python\n            list/tuple of fields or a comma-separated string. See\n            https://developers.arcgis.com/rest/geocode/api-reference/geocoding-service-output.htm\n            for a list of supported output fields. If you want to return all\n            supported output fields, set ``out_fields=\"*\"``.\n\n            .. versionadded:: 1.14.0\n        :type out_fields: str or iterable\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {'singleLine': self.format_string % query, 'f': 'json'}\n        if exactly_one:\n            params['maxLocations'] = 1\n        if out_fields is not None:\n            if isinstance(out_fields, string_compare):\n                params['outFields'] = out_fields\n            else:\n                params['outFields'] = \",\".join(out_fields)\n        url = \"?\".join((self.api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        response = self._call_geocoder(url, timeout=timeout)\n\n        # Handle any errors; recursing in the case of an expired token.\n        if 'error' in response:\n            if response['error']['code'] == self._TOKEN_EXPIRED:\n                self.retry += 1\n                self._refresh_authentication_token()\n                return self.geocode(\n                    query, exactly_one=exactly_one, timeout=timeout\n                )\n            raise GeocoderServiceError(str(response['error']))\n\n        # Success; convert from the ArcGIS JSON format.\n        if not len(response['candidates']):\n            return None\n        geocoded = []\n        for resource in response['candidates']:\n            geometry = resource['location']\n            geocoded.append(\n                Location(\n                    resource['address'], (geometry['y'], geometry['x']), resource\n                )\n            )\n        if exactly_one:\n            return geocoded[0]\n        return geocoded"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an address by location point.", "response": "def reverse(self, query, exactly_one=True, timeout=DEFAULT_SENTINEL,\n                distance=None, wkid=DEFAULT_WKID):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param int distance: Distance from the query location, in meters,\n            within which to search. ArcGIS has a default of 100 meters, if not\n            specified.\n\n        :param str wkid: WKID to use for both input and output coordinates.\n\n            .. deprecated:: 1.14.0\n               It wasn't working before because it was specified incorrectly\n               in the request parameters, and won't work even if we fix the\n               request, because :class:`geopy.point.Point` normalizes the\n               coordinates according to WKID 4326. Please open an issue in\n               the geopy issue tracker if you believe that custom wkid values\n               should be supported.\n               This parameter is scheduled for removal in geopy 2.0.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        location = self._coerce_point_to_string(query, \"%(lon)s,%(lat)s\")\n        if wkid != DEFAULT_WKID:\n            warnings.warn(\"%s.reverse: custom wkid value has been ignored.  \"\n                          \"It wasn't working before because it was specified \"\n                          \"incorrectly in the request parameters, and won't \"\n                          \"work even if we fix the request, because geopy.Point \"\n                          \"normalizes the coordinates according to WKID %s. \"\n                          \"Please open an issue in the geopy issue tracker \"\n                          \"if you believe that custom wkid values should be \"\n                          \"supported.\" % (type(self).__name__, DEFAULT_WKID),\n                          DeprecationWarning, stacklevel=2)\n            wkid = DEFAULT_WKID\n        params = {'location': location, 'f': 'json', 'outSR': wkid}\n        if distance is not None:\n            params['distance'] = distance\n        url = \"?\".join((self.reverse_api, urlencode(params)))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        response = self._call_geocoder(url, timeout=timeout)\n        if not len(response):\n            return None\n        if 'error' in response:\n            if response['error']['code'] == self._TOKEN_EXPIRED:\n                self.retry += 1\n                self._refresh_authentication_token()\n                return self.reverse(query, exactly_one=exactly_one,\n                                    timeout=timeout, distance=distance,\n                                    wkid=wkid)\n            # https://developers.arcgis.com/rest/geocode/api-reference/geocoding-service-output.htm\n            if response['error']['code'] == 400:\n                # 'details': ['Unable to find address for the specified location.']}\n                try:\n                    if 'Unable to find' in response['error']['details'][0]:\n                        return None\n                except (KeyError, IndexError):\n                    pass\n            raise GeocoderServiceError(str(response['error']))\n        address = (\n            \"%(Address)s, %(City)s, %(Region)s %(Postal)s,\"\n            \" %(CountryCode)s\" % response['address']\n        )\n        location = Location(\n            address,\n            (response['location']['y'], response['location']['x']),\n            response['address']\n        )\n        if exactly_one:\n            return location\n        else:\n            return [location]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nposting to ArcGIS requesting a new token.", "response": "def _refresh_authentication_token(self):\n        \"\"\"\n        POST to ArcGIS requesting a new token.\n        \"\"\"\n        if self.retry == self._MAX_RETRIES:\n            raise GeocoderAuthenticationFailure(\n                'Too many retries for auth: %s' % self.retry\n            )\n        token_request_arguments = {\n            'username': self.username,\n            'password': self.password,\n            'referer': self.referer,\n            'expiration': self.token_lifetime,\n            'f': 'json'\n        }\n        url = \"?\".join((self.auth_api, urlencode(token_request_arguments)))\n        logger.debug(\n            \"%s._refresh_authentication_token: %s\",\n            self.__class__.__name__, url\n        )\n        self.token_expiry = int(time()) + self.token_lifetime\n        response = self._base_call_geocoder(url)\n        if 'token' not in response:\n            raise GeocoderAuthenticationFailure(\n                'Missing token in auth request.'\n                'Request URL: %s; response JSON: %s' %\n                (url, json.dumps(response))\n            )\n        self.retry = 0\n        self.token = response['token']"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npairing an iterable of tuples e. g. 1 2 3 4", "response": "def pairwise(seq):\n    \"\"\"\n    Pair an iterable, e.g., (1, 2, 3, 4) -> ((1, 2), (2, 3), (3, 4))\n    \"\"\"\n    for i in range(0, len(seq) - 1):\n        yield (seq[i], seq[i + 1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\njoining with a filter.", "response": "def join_filter(sep, seq, pred=bool):\n    \"\"\"\n    Join with a filter.\n    \"\"\"\n    return sep.join([text_type(i) for i in seq if pred(i)])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode_page(page):\n    if hasattr(page, 'read'):  # urllib\n        if py3k:\n            encoding = page.headers.get_param(\"charset\") or \"utf-8\"\n        else:\n            encoding = page.headers.getparam(\"charset\") or \"utf-8\"\n        return text_type(page.read(), encoding=encoding)\n    else:  # requests?\n        encoding = page.headers.get(\"charset\") or \"utf-8\"\n        return text_type(page.content, encoding=encoding)", "response": "Decode a page into unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the destination of the current object.", "response": "def destination(self, point, bearing, distance=None):\n        \"\"\"\n        TODO docs.\n        \"\"\"\n        point = Point(point)\n        lat1 = units.radians(degrees=point.latitude)\n        lng1 = units.radians(degrees=point.longitude)\n        bearing = units.radians(degrees=bearing)\n\n        if distance is None:\n            distance = self\n        if isinstance(distance, Distance):\n            distance = distance.kilometers\n\n        d_div_r = float(distance) / self.RADIUS\n\n        lat2 = asin(\n            sin(lat1) * cos(d_div_r) +\n            cos(lat1) * sin(d_div_r) * cos(bearing)\n        )\n\n        lng2 = lng1 + atan2(\n            sin(bearing) * sin(d_div_r) * cos(lat1),\n            cos(d_div_r) - sin(lat1) * sin(lat2)\n        )\n\n        return Point(units.degrees(radians=lat2), units.degrees(radians=lng2))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the destination of the current instance.", "response": "def destination(self, point, bearing, distance=None):\n        \"\"\"\n        TODO docs.\n        \"\"\"\n        point = Point(point)\n        lat1 = point.latitude\n        lon1 = point.longitude\n        azi1 = bearing\n\n        if distance is None:\n            distance = self\n        if isinstance(distance, Distance):\n            distance = distance.kilometers\n\n        if not (isinstance(self.geod, Geodesic) and\n                self.geod.a == self.ELLIPSOID[0] and\n                self.geod.f == self.ELLIPSOID[2]):\n            self.geod = Geodesic(self.ELLIPSOID[0], self.ELLIPSOID[2])\n\n        r = self.geod.Direct(lat1, lon1, azi1, distance,\n                             Geodesic.LATITUDE | Geodesic.LONGITUDE)\n\n        return Point(r['lat2'], r['lon2'])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_ellipsoid(self, ellipsoid):\n        if not isinstance(ellipsoid, (list, tuple)):\n            try:\n                self.ELLIPSOID = ELLIPSOIDS[ellipsoid]\n                self.ellipsoid_key = ellipsoid\n            except KeyError:\n                raise Exception(\n                    \"Invalid ellipsoid. See geopy.distance.ELIPSOIDS\"\n                )\n        else:\n            self.ELLIPSOID = ellipsoid\n            self.ellipsoid_key = None\n        return", "response": "Change the ellipsoid used in the calculation."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize angle x to be within limit range.", "response": "def _normalize_angle(x, limit):\n    \"\"\"\n    Normalize angle `x` to be within `[-limit; limit)` range.\n    \"\"\"\n    double_limit = limit * 2.0\n    modulo = fmod(x, double_limit) or 0.0  # `or 0` is to turn -0 to +0.\n    if modulo < -limit:\n        return modulo + double_limit\n    if modulo >= limit:\n        return modulo - double_limit\n    return modulo"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nformats the latitude and longitude of the object as a string.", "response": "def format(self, altitude=None, deg_char='', min_char='m', sec_char='s'):\n        \"\"\"\n        Format decimal degrees (DD) to degrees minutes seconds (DMS)\n        \"\"\"\n        latitude = \"%s %s\" % (\n            format_degrees(abs(self.latitude), symbols={\n                'deg': deg_char, 'arcmin': min_char, 'arcsec': sec_char\n            }),\n            self.latitude >= 0 and 'N' or 'S'\n        )\n        longitude = \"%s %s\" % (\n            format_degrees(abs(self.longitude), symbols={\n                'deg': deg_char, 'arcmin': min_char, 'arcsec': sec_char\n            }),\n            self.longitude >= 0 and 'E' or 'W'\n        )\n        coordinates = [latitude, longitude]\n\n        if altitude is None:\n            altitude = bool(self.altitude)\n        if altitude:\n            if not isinstance(altitude, string_compare):\n                altitude = 'km'\n            coordinates.append(self.format_altitude(altitude))\n\n        return \", \".join(coordinates)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats decimal degrees with altitude", "response": "def format_decimal(self, altitude=None):\n        \"\"\"\n        Format decimal degrees with altitude\n        \"\"\"\n        coordinates = [str(self.latitude), str(self.longitude)]\n\n        if altitude is None:\n            altitude = bool(self.altitude)\n        if altitude:\n            if not isinstance(altitude, string_compare):\n                altitude = 'km'\n            coordinates.append(self.format_altitude(altitude))\n\n        return \", \".join(coordinates)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse degrees minutes seconds including direction", "response": "def parse_degrees(cls, degrees, arcminutes, arcseconds, direction=None):\n        \"\"\"\n        Parse degrees minutes seconds including direction (N, S, E, W)\n        \"\"\"\n        degrees = float(degrees)\n        negative = degrees < 0\n        arcminutes = float(arcminutes)\n        arcseconds = float(arcseconds)\n\n        if arcminutes or arcseconds:\n            more = units.degrees(arcminutes=arcminutes, arcseconds=arcseconds)\n            if negative:\n                degrees -= more\n            else:\n                degrees += more\n\n        if direction in [None, 'N', 'E']:\n            return degrees\n        elif direction in ['S', 'W']:\n            return -degrees\n        else:\n            raise ValueError(\"Invalid direction! Should be one of [NSEW].\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_altitude(cls, distance, unit):\n        if distance is not None:\n            distance = float(distance)\n            CONVERTERS = {\n                'km': lambda d: d,\n                'm': lambda d: units.kilometers(meters=d),\n                'mi': lambda d: units.kilometers(miles=d),\n                'ft': lambda d: units.kilometers(feet=d),\n                'nm': lambda d: units.kilometers(nautical=d),\n                'nmi': lambda d: units.kilometers(nautical=d)\n            }\n            try:\n                return CONVERTERS[unit](distance)\n            except KeyError:\n                raise NotImplementedError(\n                    'Bad distance unit specified, valid are: %r' %\n                    CONVERTERS.keys()\n                )\n        else:\n            return distance", "response": "Parse altitude managing units conversion\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_string(cls, string):\n        match = re.match(cls.POINT_PATTERN, re.sub(r\"''\", r'\"', string))\n        if match:\n            latitude_direction = None\n            if match.group(\"latitude_direction_front\"):\n                latitude_direction = match.group(\"latitude_direction_front\")\n            elif match.group(\"latitude_direction_back\"):\n                latitude_direction = match.group(\"latitude_direction_back\")\n\n            longitude_direction = None\n            if match.group(\"longitude_direction_front\"):\n                longitude_direction = match.group(\"longitude_direction_front\")\n            elif match.group(\"longitude_direction_back\"):\n                longitude_direction = match.group(\"longitude_direction_back\")\n            latitude = cls.parse_degrees(\n                match.group('latitude_degrees') or 0.0,\n                match.group('latitude_arcminutes') or 0.0,\n                match.group('latitude_arcseconds') or 0.0,\n                latitude_direction\n            )\n            longitude = cls.parse_degrees(\n                match.group('longitude_degrees') or 0.0,\n                match.group('longitude_arcminutes') or 0.0,\n                match.group('longitude_arcseconds') or 0.0,\n                longitude_direction\n            )\n            altitude = cls.parse_altitude(\n                match.group('altitude_distance'),\n                match.group('altitude_units')\n            )\n            return cls(latitude, longitude, altitude)\n        else:\n            raise ValueError(\n                \"Failed to create Point instance from string: unknown format.\"\n            )", "response": "Create and return a Point instance from a string containing the ISO - 8601 ISO - 8601 ISO - 8601 coordinates and optionally a altitude."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and return a new Point instance from an iterable with 0 to 3 elements.", "response": "def from_sequence(cls, seq):\n        \"\"\"\n        Create and return a new ``Point`` instance from any iterable with 0 to\n        3 elements.  The elements, if present, must be latitude, longitude,\n        and altitude, respectively.\n        \"\"\"\n        args = tuple(islice(seq, 4))\n        if len(args) > 3:\n            raise ValueError('When creating a Point from sequence, it '\n                             'must not have more than 3 items.')\n        return cls(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_point(cls, point):\n        return cls(point.latitude, point.longitude, point.altitude)", "response": "Create and return a new Point instance from another point."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geocode(\n            self,\n            query,\n            max_results=25,\n            set_back=0,\n            location_descriptor='any',\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param int max_results: The maximum number of resutls to request.\n\n        :param float set_back: The distance to move the accessPoint away\n            from the curb (in meters) and towards the interior of the parcel.\n            location_descriptor must be set to accessPoint for set_back to\n            take effect.\n\n        :param str location_descriptor: The type of point requested. It\n            can be any, accessPoint, frontDoorPoint, parcelPoint,\n            rooftopPoint and routingPoint.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {'addressString': self.format_string % query}\n        if set_back != 0:\n            params['setBack'] = set_back\n        if location_descriptor not in ['any',\n                                       'accessPoint',\n                                       'frontDoorPoint',\n                                       'parcelPoint',\n                                       'rooftopPoint',\n                                       'routingPoint']:\n            raise GeocoderQueryError(\n                \"You did not provided a location_descriptor \"\n                \"the webservice can consume. It should be any, accessPoint, \"\n                \"frontDoorPoint, parcelPoint, rooftopPoint or routingPoint.\"\n            )\n        params['locationDescriptor'] = location_descriptor\n        if exactly_one:\n            max_results = 1\n        params['maxResults'] = max_results\n\n        url = \"?\".join((self.api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        response = self._call_geocoder(url, timeout=timeout)\n\n        # Success; convert from GeoJSON\n        if not len(response['features']):\n            return None\n        geocoded = []\n        for feature in response['features']:\n            geocoded.append(self._parse_feature(feature))\n        if exactly_one:\n            return geocoded[0]\n        return geocoded", "response": "This method returns a location point by address."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of addresses by location point.", "response": "def reverse(self, query, exactly_one=True, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available. GeocodeFarm's API will always return at most one\n            result.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        try:\n            lat, lon = self._coerce_point_to_string(query).split(',')\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n        params = {\n            'lat': lat,\n            'lon': lon\n        }\n        if self.api_key:\n            params['key'] = self.api_key\n        url = \"?\".join((self.reverse_api, urlencode(params)))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks for API errors and raise any exceptions if there were problems reported.", "response": "def _check_for_api_errors(geocoding_results):\n        \"\"\"\n        Raise any exceptions if there were problems reported\n        in the api response.\n        \"\"\"\n        status_result = geocoding_results.get(\"STATUS\", {})\n        if \"NO_RESULTS\" in status_result.get(\"status\", \"\"):\n            return\n        api_call_success = status_result.get(\"status\", \"\") == \"SUCCESS\"\n        if not api_call_success:\n            access_error = status_result.get(\"access\")\n            access_error_to_exception = {\n                'API_KEY_INVALID': GeocoderAuthenticationFailure,\n                'OVER_QUERY_LIMIT': GeocoderQuotaExceeded,\n            }\n            exception_cls = access_error_to_exception.get(\n                access_error, GeocoderServiceError\n            )\n            raise exception_cls(access_error)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a geocoding service location point by address or query.", "response": "def geocode(\n            self,\n            query,\n            exactly_one=True,\n            user_location=None,\n            timeout=DEFAULT_SENTINEL,\n            culture=None,\n            include_neighborhood=None,\n            include_country_code=False\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n            For a structured query, provide a dictionary whose keys\n            are one of: `addressLine`, `locality` (city),\n            `adminDistrict` (state), `countryRegion`, or `postalcode`.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param user_location: Prioritize results closer to\n            this location.\n        :type user_location: :class:`geopy.point.Point`\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str culture: Affects the language of the response,\n            must be a two-letter country code.\n\n            .. versionadded:: 1.4.0\n\n        :param bool include_neighborhood: Sets whether to include the\n            neighborhood field in the response.\n\n            .. versionadded:: 1.4.0\n\n        :param bool include_country_code: Sets whether to include the\n            two-letter ISO code of the country in the response (field name\n            'countryRegionIso2').\n\n            .. versionadded:: 1.4.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        if isinstance(query, dict):\n            params = {\n                key: val\n                for key, val\n                in query.items()\n                if key in self.structured_query_params\n            }\n            params['key'] = self.api_key\n        else:\n            params = {\n                'query': self.format_string % query,\n                'key': self.api_key\n            }\n        if user_location:\n            params['userLocation'] = \",\".join(\n                (str(user_location.latitude), str(user_location.longitude))\n            )\n        if exactly_one:\n            params['maxResults'] = 1\n        if culture:\n            params['culture'] = culture\n        if include_neighborhood is not None:\n            params['includeNeighborhood'] = include_neighborhood\n        if include_country_code:\n            params['include'] = 'ciso2'  # the only acceptable value\n\n        url = \"?\".join((self.geocode_api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reverse(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            culture=None,\n            include_country_code=False\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str culture: Affects the language of the response,\n            must be a two-letter country code.\n\n        :param bool include_country_code: Sets whether to include the\n            two-letter ISO code of the country in the response (field name\n            'countryRegionIso2').\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        point = self._coerce_point_to_string(query)\n        params = {'key': self.api_key}\n        if culture:\n            params['culture'] = culture\n        if include_country_code:\n            params['include'] = 'ciso2'  # the only acceptable value\n\n        quoted_point = quote(point.encode('utf-8'))\n        url = \"?\".join((self.reverse_api % dict(point=quoted_point),\n                        urlencode(params)))\n\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )", "response": "Returns a list of addresses from the geocoding service with the specified location point."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the number of degrees in the current language", "response": "def degrees(radians=0, arcminutes=0, arcseconds=0):\n    \"\"\"\n    TODO docs.\n    \"\"\"\n    deg = 0.\n    if radians:\n        deg = math.degrees(radians)\n    if arcminutes:\n        deg += arcminutes / arcmin(degrees=1.)\n    if arcseconds:\n        deg += arcseconds / arcsec(degrees=1.)\n    return deg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef radians(degrees=0, arcminutes=0, arcseconds=0):\n    if arcminutes:\n        degrees += arcminutes / arcmin(degrees=1.)\n    if arcseconds:\n        degrees += arcseconds / arcsec(degrees=1.)\n    return math.radians(degrees)", "response": "Convert degrees to radians"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef kilometers(meters=0, miles=0, feet=0, nautical=0):\n    ret = 0.\n    if meters:\n        ret += meters / 1000.\n    if feet:\n        ret += feet / ft(1.)\n    if nautical:\n        ret += nautical / nm(1.)\n    ret += miles * 1.609344\n    return ret", "response": "calculate kilometers from meters miles feet and nautical"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the total kilometers of a single node.", "response": "def meters(kilometers=0, miles=0, feet=0, nautical=0):\n    \"\"\"\n    TODO docs.\n    \"\"\"\n    return (kilometers + km(nautical=nautical, miles=miles, feet=feet)) * 1000"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates kilometers from kilometers meters feet nautical and nautical", "response": "def miles(kilometers=0, meters=0, feet=0, nautical=0):\n    \"\"\"\n    TODO docs.\n    \"\"\"\n    ret = 0.\n    if nautical:\n        kilometers += nautical / nm(1.)\n    if feet:\n        kilometers += feet / ft(1.)\n    if meters:\n        kilometers += meters / 1000.\n    ret += kilometers / 1.609344\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef feet(kilometers=0, meters=0, miles=0, nautical=0):\n    ret = 0.\n    if nautical:\n        kilometers += nautical / nm(1.)\n    if meters:\n        kilometers += meters / 1000.\n    if kilometers:\n        miles += mi(kilometers=kilometers)\n    ret += miles * 5280\n    return ret", "response": "calculate feet of a base class"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            candidates=None,  # TODO: change default value to `1` in geopy 2.0\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param int candidates: An integer between 1 and 10 indicating the max\n            number of candidate addresses to return if a valid address\n            could be found. Defaults to `1`.\n\n            .. versionadded:: 1.19.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n\n        if candidates is None:\n            candidates = self.candidates\n\n        if candidates is None:\n            candidates = 1  # TODO: move to default args in geopy 2.0.\n\n        if candidates:\n            if not (1 <= candidates <= 10):\n                raise ValueError('candidates must be between 1 and 10')\n\n        query = {\n            'auth-id': self.auth_id,\n            'auth-token': self.auth_token,\n            'street': self.format_string % query,\n            'candidates': candidates,\n        }\n        url = '{url}?{query}'.format(url=self.api, query=urlencode(query))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(self._call_geocoder(url, timeout=timeout),\n                                exactly_one)", "response": "Returns a location point by address or query."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the response as JSON objects.", "response": "def _parse_json(self, response, exactly_one=True):\n        \"\"\"\n        Parse responses as JSON objects.\n        \"\"\"\n        if not len(response):\n            return None\n        if exactly_one:\n            return self._format_structured_address(response[0])\n        else:\n            return [self._format_structured_address(c) for c in response]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _format_structured_address(address):\n        latitude = address['metadata'].get('latitude')\n        longitude = address['metadata'].get('longitude')\n        return Location(\n            \", \".join((address['delivery_line_1'], address['last_line'])),\n            (latitude, longitude) if latitude and longitude else None,\n            address\n        )", "response": "Pretty - print address and return lat lon tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _construct_url(self, base_api, params):\n        params['key'] = self.api_key\n        return super(PickPoint, self)._construct_url(base_api, params)", "response": "Construct url based on base_api and params."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Premier account signed url. Docs on signature : see Premier account signed url. Docs on signature : see Premier account signed url. Docs on signature : see Premier account signed url. Docs on signature : see Premier account signed url. Docs on signature : see Premier account signed url. Docs on signature : see Premier account signed url.", "response": "def _get_signed_url(self, params):\n        \"\"\"\n        Returns a Premier account signed url. Docs on signature:\n            https://developers.google.com/maps/documentation/business/webservices/auth#digital_signatures\n        \"\"\"\n        params['client'] = self.client_id\n\n        if self.channel:\n            params['channel'] = self.channel\n\n        path = \"?\".join((self.api_path, urlencode(params)))\n        signature = hmac.new(\n            base64.urlsafe_b64decode(self.secret_key),\n            path.encode('utf-8'),\n            hashlib.sha1\n        )\n        signature = base64.urlsafe_b64encode(\n            signature.digest()\n        ).decode('utf-8')\n        return '%s://%s%s&signature=%s' % (\n            self.scheme, self.domain, path, signature\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a geocoding location point by address.", "response": "def geocode(\n            self,\n            query=None,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            bounds=None,\n            region=None,\n            components=None,\n            place_id=None,\n            language=None,\n            sensor=False,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode. Optional,\n            if ``components`` param is set::\n\n                >>> g.geocode(components={\"city\": \"Paris\", \"country\": \"FR\"})\n                Location(France, (46.227638, 2.213749, 0.0))\n\n            .. versionchanged:: 1.14.0\n               Now ``query`` is optional if ``components`` param is set.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :type bounds: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n        :param bounds: The bounding box of the viewport within which\n            to bias geocode results more prominently.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n\n            .. versionchanged:: 1.17.0\n                Previously the only supported format for bounds was a\n                list like ``[latitude, longitude, latitude, longitude]``.\n                This format is now deprecated in favor of a list/tuple\n                of a pair of geopy Points and will be removed in geopy 2.0.\n\n        :param str region: The region code, specified as a ccTLD\n            (\"top-level domain\") two-character value.\n\n        :param dict components: Restricts to an area. Can use any combination\n            of: route, locality, administrative_area, postal_code, country.\n\n        :param str place_id: Retrieve a Location using a Place ID.\n            Cannot be not used with ``query`` or ``bounds`` parameters.\n\n                >>> g.geocode(place_id='ChIJOcfP0Iq2j4ARDrXUa7ZWs34')\n\n            .. versionadded:: 1.19.0\n\n        :param str language: The language in which to return results.\n\n        :param bool sensor: Whether the geocoding request comes from a\n            device with a location sensor.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {\n            'sensor': str(sensor).lower()\n        }\n        if place_id and (bounds or query):\n            raise ValueError(\n                'Only one of the `query` or `place id` or `bounds` '\n                ' parameters must be entered.')\n\n        if place_id is not None:\n            params['place_id'] = place_id\n\n        if query is not None:\n            params['address'] = self.format_string % query\n\n        if query is None and place_id is None and not components:\n            raise ValueError('Either `query` or `components` or `place_id` '\n                             'must be set.')\n\n        if self.api_key:\n            params['key'] = self.api_key\n        if bounds:\n            if len(bounds) == 4:\n                warnings.warn(\n                    'GoogleV3 `bounds` format of '\n                    '`[latitude, longitude, latitude, longitude]` is now '\n                    'deprecated and will not be supported in geopy 2.0. '\n                    'Use `[Point(latitude, longitude), Point(latitude, longitude)]` '\n                    'instead.',\n                    DeprecationWarning,\n                    stacklevel=2\n                )\n                lat1, lon1, lat2, lon2 = bounds\n                bounds = [[lat1, lon1], [lat2, lon2]]\n            params['bounds'] = self._format_bounding_box(\n                bounds, \"%(lat1)s,%(lon1)s|%(lat2)s,%(lon2)s\")\n        if region:\n            params['region'] = region\n        if components:\n            params['components'] = self._format_components_param(components)\n        if language:\n            params['language'] = language\n\n        if self.premier:\n            url = self._get_signed_url(params)\n        else:\n            url = \"?\".join((self.api, urlencode(params)))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef timezone(self, location, at_time=None, timeout=DEFAULT_SENTINEL):\n\n        warnings.warn('%(cls)s.timezone method is deprecated in favor of '\n                      '%(cls)s.reverse_timezone, which returns geopy.Timezone '\n                      'object containing pytz timezone and a raw response '\n                      'instead of just pytz timezone. This method will '\n                      'be removed in geopy 2.0.' % dict(cls=type(self).__name__),\n                      DeprecationWarning, stacklevel=2)\n        timezone = self.reverse_timezone(location, at_time, timeout)\n        if timezone is None:\n            return None\n        return timezone.pytz_timezone", "response": "Return a pytz. timezone object for the location at the specified at_time."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the timezone a point in `query` was in for a specified `at_time`. .. versionadded:: 1.18.0 .. versionchanged:: 1.18.1 Previously a :class:`KeyError` was raised for a point without an assigned Olson timezone id (e.g. for Antarctica). Now this method returns None for such requests. :param query: The coordinates for which you want a timezone. :type query: :class:`geopy.point.Point`, list or tuple of (latitude, longitude), or string as \"%(latitude)s, %(longitude)s\" :param at_time: The time at which you want the timezone of this location. This is optional, and defaults to the time that the function is called in UTC. Timezone-aware datetimes are correctly handled and naive datetimes are silently treated as UTC. :type at_time: :class:`datetime.datetime` or None :param int timeout: Time, in seconds, to wait for the geocoding service to respond before raising a :class:`geopy.exc.GeocoderTimedOut` exception. Set this only if you wish to override, on this call only, the value set during the geocoder's initialization. :rtype: ``None`` or :class:`geopy.timezone.Timezone`", "response": "def reverse_timezone(self, query, at_time=None, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Find the timezone a point in `query` was in for a specified `at_time`.\n\n        .. versionadded:: 1.18.0\n\n        .. versionchanged:: 1.18.1\n           Previously a :class:`KeyError` was raised for a point without\n           an assigned Olson timezone id (e.g. for Antarctica).\n           Now this method returns None for such requests.\n\n        :param query: The coordinates for which you want a timezone.\n        :type query: :class:`geopy.point.Point`, list or tuple of (latitude,\n            longitude), or string as \"%(latitude)s, %(longitude)s\"\n\n        :param at_time: The time at which you want the timezone of this\n            location. This is optional, and defaults to the time that the\n            function is called in UTC. Timezone-aware datetimes are correctly\n            handled and naive datetimes are silently treated as UTC.\n        :type at_time: :class:`datetime.datetime` or None\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None`` or :class:`geopy.timezone.Timezone`\n        \"\"\"\n        ensure_pytz_is_installed()\n\n        location = self._coerce_point_to_string(query)\n\n        timestamp = self._normalize_timezone_at_time(at_time)\n\n        params = {\n            \"location\": location,\n            \"timestamp\": timestamp,\n        }\n        if self.api_key:\n            params['key'] = self.api_key\n        url = \"?\".join((self.tz_api, urlencode(params)))\n\n        logger.debug(\"%s.reverse_timezone: %s\", self.__class__.__name__, url)\n        return self._parse_json_timezone(\n            self._call_geocoder(url, timeout=timeout)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn location lat lon from json feed.", "response": "def _parse_json(self, page, exactly_one=True):\n        '''Returns location, (latitude, longitude) from json feed.'''\n\n        places = page.get('results', [])\n        if not len(places):\n            self._check_status(page.get('status'))\n            return None\n\n        def parse_place(place):\n            '''Get the location, lat, lng from a single json place.'''\n            location = place.get('formatted_address')\n            latitude = place['geometry']['location']['lat']\n            longitude = place['geometry']['location']['lng']\n            return Location(location, (latitude, longitude), place)\n\n        if exactly_one:\n            return parse_place(places[0])\n        else:\n            return [parse_place(place) for place in places]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks status of a given key.", "response": "def _check_status(status):\n        \"\"\"\n        Validates error statuses.\n        \"\"\"\n        if status == 'ZERO_RESULTS':\n            # When there are no results, just return.\n            return\n        if status == 'OVER_QUERY_LIMIT':\n            raise GeocoderQuotaExceeded(\n                'The given key has gone over the requests limit in the 24'\n                ' hour period or has submitted too many requests in too'\n                ' short a period of time.'\n            )\n        elif status == 'REQUEST_DENIED':\n            raise GeocoderQueryError(\n                'Your request was denied.'\n            )\n        elif status == 'INVALID_REQUEST':\n            raise GeocoderQueryError('Probably missing address or latlng.')\n        else:\n            raise GeocoderQueryError('Unknown error.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a location point by address. This implementation supports only a subset of all available parameters. A list of all parameters of the pure REST API is available here: https://developer.here.com/documentation/geocoder/topics/resource-geocode.html :param str query: The address or query you wish to geocode. For a structured query, provide a dictionary whose keys are one of: `city`, `county`, `district`, `country`, `state`, `street`, `housenumber`, or `postalcode`. :param bbox: A type of spatial filter, limits the search for any other attributes in the request. Specified by two coordinate (lat/lon) pairs -- corners of the box. `The bbox search is currently similar to mapview but it is not extended` (cited from the REST API docs). Relevant global results are also returned. Example: ``[Point(22, 180), Point(-22, -180)]``. :type bbox: list or tuple of 2 items of :class:`geopy.point.Point` or ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``. :param mapview: The app's viewport, given as two coordinate pairs, specified by two lat/lon pairs -- corners of the bounding box, respectively. Matches from within the set map view plus an extended area are ranked highest. Relevant global results are also returned. Example: ``[Point(22, 180), Point(-22, -180)]``. :type mapview: list or tuple of 2 items of :class:`geopy.point.Point` or ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``. :param bool exactly_one: Return one result or a list of results, if available. :param int maxresults: Defines the maximum number of items in the response structure. If not provided and there are multiple results the HERE API will return 10 results by default. This will be reset to one if ``exactly_one`` is True. :param int pageinformation: A key which identifies the page to be returned when the response is separated into multiple pages. Only useful when ``maxresults`` is also provided. :param str language: Affects the language of the response, must be a RFC 4647 language code, e.g. 'en-US'. :param str additional_data: A string with key-value pairs as described on https://developer.here.com/documentation/geocoder/topics/resource-params-additional.html. These will be added as one query parameter to the URL. :param int timeout: Time, in seconds, to wait for the geocoding service to respond before raising a :class:`geopy.exc.GeocoderTimedOut` exception. Set this only if you wish to override, on this call only, the value set during the geocoder's initialization. :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if ``exactly_one=False``.", "response": "def geocode(\n            self,\n            query,\n            bbox=None,\n            mapview=None,\n            exactly_one=True,\n            maxresults=None,\n            pageinformation=None,\n            language=None,\n            additional_data=False,\n            timeout=DEFAULT_SENTINEL\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        This implementation supports only a subset of all available parameters.\n        A list of all parameters of the pure REST API is available here:\n        https://developer.here.com/documentation/geocoder/topics/resource-geocode.html\n\n        :param str query: The address or query you wish to geocode.\n\n            For a structured query, provide a dictionary whose keys\n            are one of: `city`, `county`, `district`, `country`, `state`,\n            `street`, `housenumber`, or `postalcode`.\n\n        :param bbox: A type of spatial filter, limits the search for any other attributes\n            in the request. Specified by two coordinate (lat/lon)\n            pairs -- corners of the box. `The bbox search is currently similar\n            to mapview but it is not extended` (cited from the REST API docs).\n            Relevant global results are also returned.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n        :type bbox: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param mapview: The app's viewport, given as two coordinate pairs, specified\n            by two lat/lon pairs -- corners of the bounding box,\n            respectively. Matches from within the set map view plus an extended area\n            are ranked highest. Relevant global results are also returned.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n        :type mapview: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int maxresults: Defines the maximum number of items in the\n            response structure. If not provided and there are multiple results\n            the HERE API will return 10 results by default. This will be reset\n            to one if ``exactly_one`` is True.\n\n        :param int pageinformation: A key which identifies the page to be returned\n            when the response is separated into multiple pages. Only useful when\n            ``maxresults`` is also provided.\n\n        :param str language: Affects the language of the response,\n            must be a RFC 4647 language code, e.g. 'en-US'.\n\n        :param str additional_data: A string with key-value pairs as described on\n            https://developer.here.com/documentation/geocoder/topics/resource-params-additional.html.\n            These will be added as one query parameter to the URL.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        if isinstance(query, dict):\n            params = {\n                key: val\n                for key, val\n                in query.items()\n                if key in self.structured_query_params\n            }\n            params['app_id'] = self.app_id\n            params['app_code'] = self.app_code\n        else:\n            params = {\n                'searchtext': self.format_string % query,\n                'app_id': self.app_id,\n                'app_code': self.app_code\n            }\n        if bbox:\n            params['bbox'] = self._format_bounding_box(\n                bbox, \"%(lat2)s,%(lon1)s;%(lat1)s,%(lon2)s\")\n        if mapview:\n            params['mapview'] = self._format_bounding_box(\n                mapview, \"%(lat2)s,%(lon1)s;%(lat1)s,%(lon2)s\")\n        if pageinformation:\n            params['pageinformation'] = pageinformation\n        if maxresults:\n            params['maxresults'] = maxresults\n        if exactly_one:\n            params['maxresults'] = 1\n        if language:\n            params['language'] = language\n        if additional_data:\n            params['additionaldata'] = additional_data\n\n        url = \"?\".join((self.api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reverse(\n            self,\n            query,\n            radius=None,\n            exactly_one=True,\n            maxresults=None,\n            pageinformation=None,\n            language=None,\n            mode='retrieveAddresses',\n            timeout=DEFAULT_SENTINEL\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        This implementation supports only a subset of all available parameters.\n        A list of all parameters of the pure REST API is available here:\n        https://developer.here.com/documentation/geocoder/topics/resource-reverse-geocode.html\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param float radius: Proximity radius in meters.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int maxresults: Defines the maximum number of items in the\n            response structure. If not provided and there are multiple results\n            the HERE API will return 10 results by default. This will be reset\n            to one if ``exactly_one`` is True.\n\n        :param int pageinformation: A key which identifies the page to be returned\n            when the response is separated into multiple pages. Only useful when\n            ``maxresults`` is also provided.\n\n        :param str language: Affects the language of the response,\n            must be a RFC 4647 language code, e.g. 'en-US'.\n\n        :param str mode: Affects the type of returned response items, must be\n            one of: 'retrieveAddresses' (default), 'retrieveAreas', 'retrieveLandmarks',\n            'retrieveAll', or 'trackPosition'. See online documentation for more\n            information.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        point = self._coerce_point_to_string(query)\n        params = {\n            'app_id': self.app_id,\n            'app_code': self.app_code,\n            'mode': mode,\n            'prox': point,\n        }\n        if radius is not None:\n            params['prox'] = '%s,%s' % (params['prox'], float(radius))\n        if pageinformation:\n            params['pageinformation'] = pageinformation\n        if maxresults:\n            params['maxresults'] = maxresults\n        if exactly_one:\n            params['maxresults'] = 1\n        if language:\n            params['language'] = language\n        url = \"%s?%s\" % (self.reverse_api, urlencode(params))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )", "response": "This method returns an address by location point."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse a location name latitude and longitude from a JSON response.", "response": "def _parse_json(doc, exactly_one=True):\n        \"\"\"\n        Parse a location name, latitude, and longitude from an JSON response.\n        \"\"\"\n        status_code = doc.get(\"statusCode\", 200)\n        if status_code != 200:\n            err = doc.get(\"errorDetails\", \"\")\n            if status_code == 401:\n                raise GeocoderAuthenticationFailure(err)\n            elif status_code == 403:\n                raise GeocoderInsufficientPrivileges(err)\n            elif status_code == 429:\n                raise GeocoderQuotaExceeded(err)\n            elif status_code == 503:\n                raise GeocoderUnavailable(err)\n            else:\n                raise GeocoderServiceError(err)\n\n        try:\n            resources = doc['Response']['View'][0]['Result']\n        except IndexError:\n            resources = None\n        if not resources:\n            return None\n\n        def parse_resource(resource):\n            \"\"\"\n            Parse each return object.\n            \"\"\"\n            stripchars = \", \\n\"\n            addr = resource['Location']['Address']\n\n            address = addr.get('Label', '').strip(stripchars)\n            city = addr.get('City', '').strip(stripchars)\n            state = addr.get('State', '').strip(stripchars)\n            zipcode = addr.get('PostalCode', '').strip(stripchars)\n            country = addr.get('Country', '').strip(stripchars)\n\n            city_state = join_filter(\", \", [city, state])\n            place = join_filter(\" \", [city_state, zipcode])\n            location = join_filter(\", \", [address, place, country])\n\n            display_pos = resource['Location']['DisplayPosition']\n            latitude = float(display_pos['Latitude'])\n            longitude = float(display_pos['Longitude'])\n\n            return Location(location, (latitude, longitude), resource)\n\n        if exactly_one:\n            return parse_resource(resources[0])\n        else:\n            return [parse_resource(resource) for resource in resources]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _coerce_point_to_string(point, output_format=\"%(lat)s,%(lon)s\"):\n        try:\n            if not isinstance(point, Point):\n                point = Point(point)\n        except ValueError as e:\n            if isinstance(point, string_compare):\n                warnings.warn(\n                    'Unable to parse the string as Point: \"%s\". Using the value '\n                    'as-is for the query. In geopy 2.0 this will become an '\n                    'exception.' % str(e), DeprecationWarning, stacklevel=3\n                )\n                return point\n            raise\n        else:\n            # Altitude is silently dropped.\n            return output_format % dict(lat=point.latitude,\n                                        lon=point.longitude)", "response": "Coerce a point to a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform bounding box to a string matching", "response": "def _format_bounding_box(bbox, output_format=\"%(lat1)s,%(lon1)s,%(lat2)s,%(lon2)s\"):\n        \"\"\"\n        Transform bounding box boundaries to a string matching\n        `output_format` from the following formats:\n\n            - [Point(lat1, lon1), Point(lat2, lon2)]\n            - [[lat1, lon1], [lat2, lon2]]\n            - [\"lat1,lon1\", \"lat2,lon2\"]\n\n        It is guaranteed that lat1 <= lat2 and lon1 <= lon2.\n        \"\"\"\n        if len(bbox) != 2:\n            raise GeocoderQueryError(\"Unsupported format for a bounding box\")\n        p1, p2 = bbox\n        p1, p2 = Point(p1), Point(p2)\n        return output_format % dict(lat1=min(p1.latitude, p2.latitude),\n                                    lon1=min(p1.longitude, p2.longitude),\n                                    lat2=max(p1.latitude, p2.latitude),\n                                    lon2=max(p1.longitude, p2.longitude))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _call_geocoder(\n            self,\n            url,\n            timeout=DEFAULT_SENTINEL,\n            raw=False,\n            requester=None,\n            deserializer=json.loads,\n            **kwargs\n    ):\n        \"\"\"\n        For a generated query URL, get the results.\n        \"\"\"\n\n        if requester:\n            req = url  # Don't construct an urllib's Request for a custom requester.\n\n            # `requester` might be anything which can issue an HTTP request.\n            # Assume that `requester` is a method of the `requests` library.\n            # Requests, however, doesn't accept SSL context in its HTTP\n            # request methods. A custom HTTP adapter has to be created for that.\n            # So the current usage is not directly compatible with `requests`.\n            requester = functools.partial(requester, context=self.ssl_context,\n                                          proxies=self.proxies,\n                                          headers=self.headers)\n        else:\n            if isinstance(url, Request):\n                # copy Request\n                headers = self.headers.copy()\n                headers.update(url.header_items())\n                req = Request(url=url.get_full_url(), headers=headers)\n            else:\n                req = Request(url=url, headers=self.headers)\n\n        requester = requester or self.urlopen\n\n        if timeout is None:\n            warnings.warn(\n                ('`timeout=None` has been passed to a geocoder call. Using '\n                 'default geocoder timeout. In geopy 2.0 the '\n                 'behavior will be different: None will mean \"no timeout\" '\n                 'instead of \"default geocoder timeout\". Pass '\n                 'geopy.geocoders.base.DEFAULT_SENTINEL instead of None '\n                 'to get rid of this warning.'), DeprecationWarning, stacklevel=3)\n            timeout = DEFAULT_SENTINEL\n\n        timeout = (timeout if timeout is not DEFAULT_SENTINEL\n                   else self.timeout)\n\n        try:\n            page = requester(req, timeout=timeout, **kwargs)\n        except Exception as error:\n            message = (\n                str(error) if not py3k\n                else (\n                    str(error.args[0])\n                    if len(error.args)\n                    else str(error)\n                )\n            )\n            self._geocoder_exception_handler(error, message)\n            if isinstance(error, HTTPError):\n                code = error.getcode()\n                body = self._read_http_error_body(error)\n                if body:\n                    logger.info('Received an HTTP error (%s): %s', code, body,\n                                exc_info=False)\n                try:\n                    raise ERROR_CODE_MAP[code](message)\n                except KeyError:\n                    raise GeocoderServiceError(message)\n            elif isinstance(error, URLError):\n                if \"timed out\" in message:\n                    raise GeocoderTimedOut('Service timed out')\n                elif \"unreachable\" in message:\n                    raise GeocoderUnavailable('Service not available')\n            elif isinstance(error, SocketTimeout):\n                raise GeocoderTimedOut('Service timed out')\n            elif isinstance(error, SSLError):\n                if \"timed out\" in message:\n                    raise GeocoderTimedOut('Service timed out')\n            raise GeocoderServiceError(message)\n\n        if hasattr(page, 'getcode'):\n            status_code = page.getcode()\n        elif hasattr(page, 'status_code'):\n            status_code = page.status_code\n        else:\n            status_code = None\n        if status_code in ERROR_CODE_MAP:\n            raise ERROR_CODE_MAP[page.status_code](\"\\n%s\" % decode_page(page))\n\n        if raw:\n            return page\n\n        page = decode_page(page)\n\n        if deserializer is not None:\n            try:\n                return deserializer(page)\n            except ValueError:\n                raise GeocoderParseError(\n                    \"Could not deserialize using deserializer:\\n%s\" % page\n                )\n        else:\n            return page", "response": "Calls the geocoder function for a given URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a geocoding function for the specified IETF location point by address.", "response": "def geocode(\n            self,\n            query,\n            bounds=None,\n            country=None,\n            language=None,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param str language: an IETF format language code (such as `es`\n            for Spanish or pt-BR for Brazilian Portuguese); if this is\n            omitted a code of `en` (English) will be assumed by the remote\n            service.\n\n        :type bounds: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n        :param bounds: Provides the geocoder with a hint to the region\n            that the query resides in. This value will help the geocoder\n            but will not restrict the possible results to the supplied\n            region. The bounds parameter should be specified as 2\n            coordinate points -- corners of a bounding box.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n\n            .. versionchanged:: 1.17.0\n                Previously the only supported format for bounds was a\n                string of ``\"longitude,latitude,longitude,latitude\"``.\n                This format is now deprecated in favor of a list/tuple\n                of a pair of geopy Points and will be removed in geopy 2.0.\n\n        :param country: Restricts the results to the specified\n            country or countries. The country code is a 2 character code as\n            defined by the ISO 3166-1 Alpha 2 standard (e.g. ``fr``).\n            Might be a Python list of strings.\n\n            .. versionchanged:: 1.19.0\n                This parameter didn't seem to be respected previously.\n                Also, previously only a single string could be specified.\n                Now a Python list of individual countries is supported.\n\n        :type country: str or list\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        params = {\n            'key': self.api_key,\n            'q': self.format_string % query,\n        }\n        if bounds:\n            if isinstance(bounds, string_compare):\n                warnings.warn(\n                    'OpenCage `bounds` format of '\n                    '`\"longitude,latitude,longitude,latitude\"` is now '\n                    'deprecated and will not be supported in geopy 2.0. '\n                    'Use `[Point(latitude, longitude), Point(latitude, longitude)]` '\n                    'instead.',\n                    DeprecationWarning,\n                    stacklevel=2\n                )\n                lon1, lat1, lon2, lat2 = bounds.split(',')\n                bounds = [[lat1, lon1], [lat2, lon2]]\n            params['bounds'] = self._format_bounding_box(\n                bounds, \"%(lon1)s,%(lat1)s,%(lon2)s,%(lat2)s\")\n        if language:\n            params['language'] = language\n\n        if not country:\n            country = []\n        if isinstance(country, string_compare):\n            country = [country]\n        if country:\n            params['countrycode'] = \",\".join(country)\n\n        url = \"?\".join((self.api, urlencode(params)))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_status(status):\n        status_code = status['code']\n        if status_code == 429:\n            # Rate limit exceeded\n            raise GeocoderQuotaExceeded(\n                'The given key has gone over the requests limit in the 24'\n                ' hour period or has submitted too many requests in too'\n                ' short a period of time.'\n            )\n        if status_code == 200:\n            # When there are no results, just return.\n            return\n\n        if status_code == 403:\n            raise GeocoderQueryError(\n                'Your request was denied.'\n            )\n        else:\n            raise GeocoderQueryError('Unknown error.')", "response": "Validates status of a key in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef geocode(\n            self,\n            query,\n            country_codes=None,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n            For a structured query, provide a dictionary whose keys\n            are one of: `country`, `state`, `city`, `zipcode`, `street`, `address`,\n            `houseNumber` or `subNumber`.\n\n        :param country_codes: Provides the geocoder with a list\n            of country codes that the query may reside in. This value will\n            limit the geocoder to the supplied countries. The country code\n            is a 2 character code as defined by the ISO-3166-1 alpha-2\n            standard (e.g. ``FR``). Multiple countries can be specified with\n            a Python list.\n\n            .. versionchanged:: 1.19.0\n                Previously only a Python list of countries could be specified.\n                Now a single country as a string can be specified as well.\n\n        :type country_codes: str or list\n\n        :param bool exactly_one: Return one result or a list of one result.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n\n        if isinstance(query, dict):\n            params = {\n                key: val\n                for key, val\n                in query.items()\n                if key in self.structured_query_params\n            }\n            params['api_key'] = self.api_key\n        else:\n            params = {\n                'api_key': self.api_key,\n                'q': self.format_string % query,\n            }\n\n        if not country_codes:\n            country_codes = []\n        if isinstance(country_codes, string_compare):\n            country_codes = [country_codes]\n        if country_codes:\n            params['countryCodes'] = \",\".join(country_codes)\n\n        url = \"?\".join((self.api, urlencode(params)))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )", "response": "Returns a location point by address."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_json(self, page, exactly_one):\n\n        if not page.get('success'):\n            return None\n\n        latitude = page['latitude']\n        longitude = page['longitude']\n\n        place = page.get('place')\n        address = \", \".join([place['city'], place['countryCode']])\n        result = Location(address, (latitude, longitude), page)\n        if exactly_one:\n            return result\n        else:\n            return [result]", "response": "Returns location latitude longitude from json feed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef geocode(self,\n                query,\n                lang='en',\n                exactly_one=True,\n                timeout=DEFAULT_SENTINEL):\n\n        \"\"\"\n        Return a location point for a `3 words` query. If the `3 words` address\n        doesn't exist, a :class:`geopy.exc.GeocoderQueryError` exception will be\n        thrown.\n\n        :param str query: The 3-word address you wish to geocode.\n\n        :param str lang: two character language codes as supported by\n            the API (https://docs.what3words.com/api/v2/#lang).\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available. Due to the address scheme there is always exactly one\n            result for each `3 words` address, so this parameter is rather\n            useless for this geocoder.\n\n            .. versionchanged:: 1.14.0\n               ``exactly_one=False`` now returns a list of a single location.\n               This option wasn't respected before.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n\n        if not self._check_query(query):\n            raise exc.GeocoderQueryError(\n                \"Search string must be 'word.word.word'\"\n            )\n\n        params = {\n            'addr': self.format_string % query,\n            'lang': lang.lower(),\n            'key': self.api_key,\n        }\n\n        url = \"?\".join((self.geocode_api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one=exactly_one\n        )", "response": "Returns a location point for a 3 words query."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_json(self, resources, exactly_one=True):\n\n        code = resources['status'].get('code')\n\n        if code:\n            # https://docs.what3words.com/api/v2/#errors\n            exc_msg = \"Error returned by What3Words: %s\" % resources['status']['message']\n            if code == 401:\n                raise exc.GeocoderAuthenticationFailure(exc_msg)\n\n            raise exc.GeocoderQueryError(exc_msg)\n\n        def parse_resource(resource):\n            \"\"\"\n            Parse record.\n            \"\"\"\n\n            if 'geometry' in resource:\n                words = resource['words']\n                position = resource['geometry']\n                latitude, longitude = position['lat'], position['lng']\n                if latitude and longitude:\n                    latitude = float(latitude)\n                    longitude = float(longitude)\n\n                return Location(words, (latitude, longitude), resource)\n            else:\n                raise exc.GeocoderParseError('Error parsing result.')\n\n        location = parse_resource(resources)\n        if exactly_one:\n            return location\n        else:\n            return [location]", "response": "Parse a resource from what3words API response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of 3 words addresses by location point.", "response": "def reverse(self, query, lang='en', exactly_one=True,\n                timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Return a `3 words` address by location point. Each point on surface has\n        a `3 words` address, so there's always a non-empty response.\n\n        :param query: The coordinates for which you wish to obtain the 3 word\n            address.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param str lang: two character language codes as supported by the\n            API (https://docs.what3words.com/api/v2/#lang).\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available. Due to the address scheme there is always exactly one\n            result for each `3 words` address, so this parameter is rather\n            useless for this geocoder.\n\n            .. versionchanged:: 1.14.0\n               ``exactly_one=False`` now returns a list of a single location.\n               This option wasn't respected before.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        lang = lang.lower()\n\n        params = {\n            'coords': self._coerce_point_to_string(query),\n            'lang': lang.lower(),\n            'key': self.api_key,\n        }\n\n        url = \"?\".join((self.reverse_api, urlencode(params)))\n\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_reverse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one=exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a location point by address.", "response": "def geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            limit=None,\n            addressdetails=False,\n            language=False,\n            geometry=None,\n            extratags=False,\n            country_codes=None,\n            viewbox=None,\n            bounded=None,  # TODO: change default value to `False` in geopy 2.0\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param query: The address, query or a structured query\n            you wish to geocode.\n\n            .. versionchanged:: 1.0.0\n                For a structured query, provide a dictionary whose keys\n                are one of: `street`, `city`, `county`, `state`, `country`, or\n                `postalcode`. For more information, see Nominatim's\n                documentation for `structured requests`:\n\n                    https://wiki.openstreetmap.org/wiki/Nominatim\n\n        :type query: dict or str\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param int limit: Maximum amount of results to return from Nominatim.\n            Unless exactly_one is set to False, limit will always be 1.\n\n            .. versionadded:: 1.13.0\n\n        :param bool addressdetails: If you want in *Location.raw* to include\n            addressdetails such as city_district, etc set it to True\n\n        :param str language: Preferred language in which to return results.\n            Either uses standard\n            `RFC2616 <http://www.ietf.org/rfc/rfc2616.txt>`_\n            accept-language string or a simple comma-separated\n            list of language codes.\n\n            .. versionadded:: 1.0.0\n\n        :param str geometry: If present, specifies whether the geocoding\n            service should return the result's geometry in `wkt`, `svg`,\n            `kml`, or `geojson` formats. This is available via the\n            `raw` attribute on the returned :class:`geopy.location.Location`\n            object.\n\n            .. versionadded:: 1.3.0\n\n        :param bool extratags: Include additional information in the result if available,\n            e.g. wikipedia link, opening hours.\n\n            .. versionadded:: 1.17.0\n\n        :param country_codes: Limit search results\n            to a specific country (or a list of countries).\n            A country_code should be the ISO 3166-1alpha2 code,\n            e.g. ``gb`` for the United Kingdom, ``de`` for Germany, etc.\n\n            .. versionadded:: 1.19.0\n\n        :type country_codes: str or list\n\n        :type viewbox: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param viewbox: Coordinates to restrict search within.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n\n            .. versionadded:: 1.19.0\n\n        :param bool bounded: Restrict the results to only items contained\n            within the bounding view_box. Defaults to `False`.\n\n            .. versionadded:: 1.19.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n\n        if isinstance(query, dict):\n            params = {\n                key: val\n                for key, val\n                in query.items()\n                if key in self.structured_query_params\n            }\n        else:\n            params = {'q': self.format_string % query}\n\n        params.update({\n            'format': 'json'\n        })\n\n        if exactly_one:\n            params['limit'] = 1\n        elif limit is not None:\n            limit = int(limit)\n            if limit < 1:\n                raise ValueError(\"Limit cannot be less than 1\")\n            params['limit'] = limit\n\n        if viewbox is None:\n            viewbox = self.view_box\n        if viewbox:\n            if len(viewbox) == 4:\n                warnings.warn(\n                    '%s `viewbox` format of '\n                    '`[longitude, latitude, longitude, latitude]` is now '\n                    'deprecated and will not be supported in geopy 2.0. '\n                    'Use `[Point(latitude, longitude), Point(latitude, longitude)]` '\n                    'instead.' % type(self).__name__,\n                    DeprecationWarning,\n                    stacklevel=2\n                )\n                lon1, lat1, lon2, lat2 = viewbox\n                viewbox = [[lat1, lon1], [lat2, lon2]]\n            params['viewbox'] = self._format_bounding_box(\n                viewbox, \"%(lon1)s,%(lat1)s,%(lon2)s,%(lat2)s\")\n\n        if bounded is None:\n            bounded = self.bounded\n        if bounded:\n            params['bounded'] = 1\n\n        if country_codes is None:\n            country_codes = self.country_bias\n        if not country_codes:\n            country_codes = []\n        if isinstance(country_codes, string_compare):\n            country_codes = [country_codes]\n        if country_codes:\n            params['countrycodes'] = \",\".join(country_codes)\n\n        if addressdetails:\n            params['addressdetails'] = 1\n\n        if language:\n            params['accept-language'] = language\n\n        if extratags:\n            params['extratags'] = True\n\n        if geometry is not None:\n            geometry = geometry.lower()\n            if geometry == 'wkt':\n                params['polygon_text'] = 1\n            elif geometry == 'svg':\n                params['polygon_svg'] = 1\n            elif geometry == 'kml':\n                params['polygon_kml'] = 1\n            elif geometry == 'geojson':\n                params['polygon_geojson'] = 1\n            else:\n                raise GeocoderQueryError(\n                    \"Invalid geometry format. Must be one of: \"\n                    \"wkt, svg, kml, geojson.\"\n                )\n\n        url = self._construct_url(self.api, params)\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an address by location point.", "response": "def reverse(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            language=False,\n            addressdetails=True\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str language: Preferred language in which to return results.\n            Either uses standard\n            `RFC2616 <http://www.ietf.org/rfc/rfc2616.txt>`_\n            accept-language string or a simple comma-separated\n            list of language codes.\n\n            .. versionadded:: 1.0.0\n\n        :param bool addressdetails: Whether or not to include address details,\n            such as city, county, state, etc. in *Location.raw*\n\n            .. versionadded:: 1.14.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        try:\n            lat, lon = self._coerce_point_to_string(query).split(',')\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n        params = {\n            'lat': lat,\n            'lon': lon,\n            'format': 'json',\n        }\n        if language:\n            params['accept-language'] = language\n\n        params['addressdetails'] = 1 if addressdetails else 0\n\n        url = self._construct_url(self.reverse_api, params)\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a location point by address or query.", "response": "def geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            location_bias=None,\n            language=False,\n            limit=None,\n            osm_tag=None\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param location_bias: The coordinates to used as location bias.\n\n        :param str language: Preferred language in which to return results.\n\n        :param int limit: Limit the number of returned results, defaults to no\n            limit.\n\n            .. versionadded:: 1.12.0\n\n        :param osm_tag: The expression to filter (include/exclude) by key and/\n            or value, str as ``'key:value'`` or list/set of str if multiple\n            filters are required as ``['key:!val', '!key', ':!value']``.\n        :type osm_tag: str or list or set\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        params = {\n            'q': self.format_string % query\n        }\n        if limit:\n            params['limit'] = int(limit)\n        if exactly_one:\n            params['limit'] = 1\n        if language:\n            params['lang'] = language\n        if location_bias:\n            try:\n                lat, lon = self._coerce_point_to_string(location_bias).split(',')\n                params['lon'] = lon\n                params['lat'] = lat\n            except ValueError:\n                raise ValueError((\"Location bias must be a\"\n                                  \" coordinate pair or Point\"))\n        if osm_tag:\n            if isinstance(osm_tag, string_compare):\n                params['osm_tag'] = [osm_tag]\n            else:\n                if not isinstance(osm_tag, (list, set)):\n                    raise ValueError(\n                        \"osm_tag must be a string expression or \"\n                        \"a set/list of string expressions\"\n                    )\n                params['osm_tag'] = osm_tag\n        url = \"?\".join((self.api, urlencode(params, doseq=True)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_json(cls, resources, exactly_one=True):\n        if not len(resources['features']):  # pragma: no cover\n            return None\n        if exactly_one:\n            return cls.parse_resource(resources['features'][0])\n        else:\n            return [cls.parse_resource(resource) for resource\n                    in resources['features']]", "response": "Parse display name latitude and longitude from a JSON response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of addresses by location point.", "response": "def reverse(self, query, exactly_one=True, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available. Baidu's API will always return at most one result.\n\n            .. versionadded:: 1.14.0\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        params = {\n            'ak': self.api_key,\n            'output': 'json',\n            'location': self._coerce_point_to_string(query),\n        }\n\n        url = self._construct_url(params)\n\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_reverse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one=exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_reverse_json(self, page, exactly_one=True):\n        place = page.get('result')\n\n        if not place:\n            self._check_status(page.get('status'))\n            return None\n\n        location = place.get('formatted_address').encode('utf-8')\n        latitude = place['location']['lat']\n        longitude = place['location']['lng']\n\n        location = Location(location, (latitude, longitude), place)\n        if exactly_one:\n            return location\n        else:\n            return [location]", "response": "Parses a location from a single - result reverse API call."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_status(status):\n        if status == 0:\n            # When there are no results, just return.\n            return\n        if status == 1:\n            raise GeocoderServiceError(\n                'Internal server error.'\n            )\n        elif status == 2:\n            raise GeocoderQueryError(\n                'Invalid request.'\n            )\n        elif status == 3:\n            raise GeocoderAuthenticationFailure(\n                'Authentication failure.'\n            )\n        elif status == 4:\n            raise GeocoderQuotaExceeded(\n                'Quota validate failure.'\n            )\n        elif status == 5:\n            raise GeocoderQueryError(\n                'AK Illegal or Not Exist.'\n            )\n        elif status == 101:\n            raise GeocoderAuthenticationFailure(\n                'No AK'\n            )\n        elif status == 102:\n            raise GeocoderAuthenticationFailure(\n                'MCODE Error'\n            )\n        elif status == 200:\n            raise GeocoderAuthenticationFailure(\n                'Invalid AK'\n            )\n        elif status == 211:\n            raise GeocoderAuthenticationFailure(\n                'Invalid SN'\n            )\n        elif 200 <= status < 300:\n            raise GeocoderAuthenticationFailure(\n                'Authentication Failure'\n            )\n        elif 300 <= status < 500:\n            raise GeocoderQuotaExceeded(\n                'Quota Error.'\n            )\n        else:\n            raise GeocoderQueryError('Unknown error. Status: %r' % status)", "response": "Checks the status of a single object and raises appropriate exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            limit=None,\n            typeahead=False,\n            language=None,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param int limit: Maximum amount of results to return from the service.\n            Unless exactly_one is set to False, limit will always be 1.\n\n        :param bool typeahead: If the \"typeahead\" flag is set, the query\n            will be interpreted as a partial input and the search will\n            enter predictive mode.\n\n        :param str language: Language in which search results should be\n            returned. When data in specified language is not\n            available for a specific field, default language is used.\n            List of supported languages (case-insensitive):\n            https://developer.tomtom.com/online-search/online-search-documentation/supported-languages\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        query = self.format_string % query\n        params = self._geocode_params(query)\n        params['typeahead'] = self._boolean_value(typeahead)\n\n        if limit:\n            params['limit'] = str(int(limit))\n        if exactly_one:\n            params['limit'] = '1'\n\n        if language:\n            params['language'] = language\n\n        quoted_query = quote(query.encode('utf-8'))\n        url = \"?\".join((self.api % dict(query=quoted_query),\n                        urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )", "response": "Returns a location point by address or query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reverse(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            language=None,\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str language: Language in which search results should be\n            returned. When data in specified language is not\n            available for a specific field, default language is used.\n            List of supported languages (case-insensitive):\n            https://developer.tomtom.com/online-search/online-search-documentation/supported-languages\n\n            .. versionadded:: 1.18.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        position = self._coerce_point_to_string(query)\n        params = self._reverse_params(position)\n\n        if language:\n            params['language'] = language\n\n        quoted_position = quote(position.encode('utf-8'))\n        url = \"?\".join((self.api_reverse % dict(position=quoted_position),\n                        urlencode(params)))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n\n        return self._parse_reverse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )", "response": "Returns a list of addresses from the given location point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef geocode(\n            self,\n            query,\n            query_type='StreetAddress',\n            maximum_responses=25,\n            is_freeform=False,\n            filtering=None,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The query string to be geocoded.\n\n        :param str query_type: The type to provide for geocoding. It can be\n            `PositionOfInterest`, `StreetAddress` or `CadastralParcel`.\n            `StreetAddress` is the default choice if none provided.\n\n        :param int maximum_responses: The maximum number of responses\n            to ask to the API in the query body.\n\n        :param str is_freeform: Set if return is structured with\n            freeform structure or a more structured returned.\n            By default, value is False.\n\n        :param str filtering: Provide string that help setting geocoder\n            filter. It contains an XML string. See examples in documentation\n            and ignfrance.py file in directory tests.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n\n        query = self.format_string % query\n\n        # Check if acceptable query type\n        if query_type not in ['PositionOfInterest',\n                              'StreetAddress',\n                              'CadastralParcel']:\n            raise GeocoderQueryError(\"\"\"You did not provided a query_type the\n            webservice can consume. It should be PositionOfInterest,\n            'StreetAddress or CadastralParcel\"\"\")\n\n        # Check query validity for CadastralParcel\n        if query_type == 'CadastralParcel' and len(query.strip()) != 14:\n            raise GeocoderQueryError(\"\"\"You must send a string of fourteen\n                characters long to match the cadastre required code\"\"\")\n\n        sub_request = \"\"\"\n                <GeocodeRequest returnFreeForm=\"{is_freeform}\">\n                    <Address countryCode=\"{query_type}\">\n                        <freeFormAddress>{query}</freeFormAddress>\n                        {filtering}\n                    </Address>\n                </GeocodeRequest>\n        \"\"\"\n\n        xml_request = self.xml_request.format(\n            method_name='LocationUtilityService',\n            sub_request=sub_request,\n            maximum_responses=maximum_responses\n        )\n\n        # Manage type change for xml case sensitive\n        if is_freeform:\n            is_freeform = 'true'\n        else:\n            is_freeform = 'false'\n\n        # Manage filtering value\n        if filtering is None:\n            filtering = ''\n\n        # Create query using parameters\n        request_string = xml_request.format(\n            is_freeform=is_freeform,\n            query=query,\n            query_type=query_type,\n            filtering=filtering\n        )\n\n        params = {\n            'xls': request_string\n        }\n\n        url = \"?\".join((self.api, urlencode(params)))\n\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n\n        raw_xml = self._request_raw_content(url, timeout)\n\n        return self._parse_xml(\n            raw_xml,\n            is_freeform=is_freeform,\n            exactly_one=exactly_one\n        )", "response": "Returns a location point by address."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an address by location point.", "response": "def reverse(\n            self,\n            query,\n            reverse_geocode_preference=('StreetAddress', ),\n            maximum_responses=25,\n            filtering='',\n            exactly_one=DEFAULT_SENTINEL,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param list reverse_geocode_preference: Enable to set expected results\n            type. It can be `StreetAddress` or `PositionOfInterest`.\n            Default is set to `StreetAddress`.\n\n        :param int maximum_responses: The maximum number of responses\n            to ask to the API in the query body.\n\n        :param str filtering: Provide string that help setting geocoder\n            filter. It contains an XML string. See examples in documentation\n            and ignfrance.py file in directory tests.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n            .. versionchanged:: 1.14.0\n               Default value for ``exactly_one`` was ``False``, which differs\n               from the conventional default across geopy. Please always pass\n               this argument explicitly, otherwise you would get a warning.\n               In geopy 2.0 the default value will become ``True``.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        if exactly_one is DEFAULT_SENTINEL:\n            warnings.warn('%s.reverse: default value for `exactly_one` '\n                          'argument will become True in geopy 2.0. '\n                          'Specify `exactly_one=False` as the argument '\n                          'explicitly to get rid of this warning.' % type(self).__name__,\n                          DeprecationWarning, stacklevel=2)\n            exactly_one = False\n\n        sub_request = \"\"\"\n            <ReverseGeocodeRequest>\n                {reverse_geocode_preference}\n                <Position>\n                  <gml:Point>\n                    <gml:pos>{query}</gml:pos>\n                  </gml:Point>\n                  {filtering}\n                </Position>\n            </ReverseGeocodeRequest>\n        \"\"\"\n\n        xml_request = self.xml_request.format(\n            method_name='ReverseGeocodeRequest',\n            sub_request=sub_request,\n            maximum_responses=maximum_responses\n        )\n\n        for pref in reverse_geocode_preference:\n            if pref not in ('StreetAddress', 'PositionOfInterest'):\n                raise GeocoderQueryError(\n                    '`reverse_geocode_preference` must contain '\n                    'one or more of: StreetAddress, PositionOfInterest'\n                )\n\n        point = self._coerce_point_to_string(query, \"%(lat)s %(lon)s\")\n        reverse_geocode_preference = '\\n'.join((\n            '<ReverseGeocodePreference>%s</ReverseGeocodePreference>' % pref\n            for pref\n            in reverse_geocode_preference\n        ))\n\n        request_string = xml_request.format(\n            maximum_responses=maximum_responses,\n            query=point,\n            reverse_geocode_preference=reverse_geocode_preference,\n            filtering=filtering\n        )\n\n        url = \"?\".join((self.api, urlencode({'xls': request_string})))\n\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n\n        raw_xml = self._request_raw_content(url, timeout)\n\n        return self._parse_xml(\n            raw_xml,\n            exactly_one=exactly_one,\n            is_reverse=True,\n            is_freeform='false'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the XML feed and return a list of location latitude longitude.", "response": "def _parse_xml(self,\n                   page,\n                   is_reverse=False,\n                   is_freeform=False,\n                   exactly_one=True):\n        \"\"\"\n        Returns location, (latitude, longitude) from XML feed\n        and transform to json\n        \"\"\"\n        # Parse the page\n        tree = ET.fromstring(page.encode('utf-8'))\n\n        # Clean tree from namespace to facilitate XML manipulation\n        def remove_namespace(doc, namespace):\n            \"\"\"Remove namespace in the document in place.\"\"\"\n            ns = '{%s}' % namespace\n            ns = u(ns)\n            nsl = len(ns)\n            for elem in doc.getiterator():\n                if elem.tag.startswith(ns):\n                    elem.tag = elem.tag[nsl:]\n\n        remove_namespace(tree, 'http://www.opengis.net/gml')\n        remove_namespace(tree, 'http://www.opengis.net/xls')\n        remove_namespace(tree, 'http://www.opengis.net/xlsext')\n\n        # Return places as json instead of XML\n        places = self._xml_to_json_places(tree, is_reverse=is_reverse)\n\n        if exactly_one:\n            return self._parse_place(places[0], is_freeform=is_freeform)\n        else:\n            return [\n                self._parse_place(\n                    place,\n                    is_freeform=is_freeform\n                ) for place in places\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms the xml ElementTree due to XML webservice return to json", "response": "def _xml_to_json_places(tree, is_reverse=False):\n        \"\"\"\n        Transform the xml ElementTree due to XML webservice return to json\n        \"\"\"\n\n        select_multi = (\n            'GeocodedAddress'\n            if not is_reverse\n            else 'ReverseGeocodedLocation'\n        )\n\n        adresses = tree.findall('.//' + select_multi)\n        places = []\n\n        sel_pl = './/Address/Place[@type=\"{}\"]'\n        for adr in adresses:\n            el = {}\n            el['pos'] = adr.find('./Point/pos')\n            el['street'] = adr.find('.//Address/StreetAddress/Street')\n            el['freeformaddress'] = adr.find('.//Address/freeFormAddress')\n            el['municipality'] = adr.find(sel_pl.format('Municipality'))\n            el['numero'] = adr.find(sel_pl.format('Numero'))\n            el['feuille'] = adr.find(sel_pl.format('Feuille'))\n            el['section'] = adr.find(sel_pl.format('Section'))\n            el['departement'] = adr.find(sel_pl.format('Departement'))\n            el['commune_absorbee'] = adr.find(sel_pl.format('CommuneAbsorbee'))\n            el['commune'] = adr.find(sel_pl.format('Commune'))\n            el['insee'] = adr.find(sel_pl.format('INSEE'))\n            el['qualite'] = adr.find(sel_pl.format('Qualite'))\n            el['territoire'] = adr.find(sel_pl.format('Territoire'))\n            el['id'] = adr.find(sel_pl.format('ID'))\n            el['id_tr'] = adr.find(sel_pl.format('ID_TR'))\n            el['bbox'] = adr.find(sel_pl.format('Bbox'))\n            el['nature'] = adr.find(sel_pl.format('Nature'))\n            el['postal_code'] = adr.find('.//Address/PostalCode')\n            el['extended_geocode_match_code'] = adr.find(\n                './/ExtendedGeocodeMatchCode'\n            )\n\n            place = {}\n\n            def testContentAttrib(selector, key):\n                \"\"\"\n                Helper to select by attribute and if not attribute,\n                value set to empty string\n                \"\"\"\n                return selector.attrib.get(\n                    key,\n                    None\n                ) if selector is not None else None\n\n            place['accuracy'] = testContentAttrib(\n                adr.find('.//GeocodeMatchCode'), 'accuracy')\n\n            place['match_type'] = testContentAttrib(\n                adr.find('.//GeocodeMatchCode'), 'matchType')\n\n            place['building'] = testContentAttrib(\n                adr.find('.//Address/StreetAddress/Building'), 'number')\n\n            place['search_centre_distance'] = testContentAttrib(\n                adr.find('.//SearchCentreDistance'), 'value')\n\n            for key, value in iteritems(el):\n                if value is not None:\n                    place[key] = value.text\n                    if value.text is None:\n                        place[key] = None\n                else:\n                    place[key] = None\n\n            # We check if lat lng is not empty and unpack accordingly\n            if place['pos']:\n                lat, lng = place['pos'].split(' ')\n                place['lat'] = lat.strip()\n                place['lng'] = lng.strip()\n            else:\n                place['lat'] = place['lng'] = None\n\n            # We removed the unused key\n            place.pop(\"pos\", None)\n            places.append(place)\n\n        return places"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _request_raw_content(self, url, timeout):\n\n        request = Request(url)\n\n        if self.referer is not None:\n            request.add_header('Referer', self.referer)\n\n        raw_xml = self._call_geocoder(\n            request,\n            timeout=timeout,\n            deserializer=None\n        )\n\n        return raw_xml", "response": "Send the request to get raw content."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a single place and return a Location object.", "response": "def _parse_place(place, is_freeform=None):\n        \"\"\"\n        Get the location, lat, lng and place from a single json place.\n        \"\"\"\n        # When freeform already so full address\n        if is_freeform == 'true':\n            location = place.get('freeformaddress')\n        else:\n            # For parcelle\n            if place.get('numero'):\n                location = place.get('street')\n            else:\n                # When classic geocoding\n                # or when reverse geocoding\n                location = \"%s %s\" % (\n                    place.get('postal_code', ''),\n                    place.get('commune', ''),\n                )\n                if place.get('street'):\n                    location = \"%s, %s\" % (\n                        place.get('street', ''),\n                        location,\n                    )\n                if place.get('building'):\n                    location = \"%s %s\" % (\n                        place.get('building', ''),\n                        location,\n                    )\n\n        return Location(location, (place.get('lat'), place.get('lng')), place)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            country=None,\n            country_bias=None,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param country: Limit records to the specified countries.\n            Two letter country code ISO-3166 (e.g. ``FR``). Might be\n            a single string or a list of strings.\n\n            .. versionadded:: 1.19.0\n\n        :type country: str or list\n\n        :param str country_bias: Records from the country_bias are listed first.\n            Two letter country code ISO-3166.\n\n            .. versionadded:: 1.19.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = [\n            ('q', self.format_string % query),\n            ('username', self.username),\n        ]\n\n        if country_bias is None:\n            country_bias = self.country_bias\n        if country_bias:\n            params.append(('countryBias', country_bias))\n\n        if not country:\n            country = []\n        if isinstance(country, string_compare):\n            country = [country]\n        for country_item in country:\n            params.append(('country', country_item))\n\n        if exactly_one:\n            params.append(('maxRows', 1))\n        url = \"?\".join((self.api, urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one,\n        )", "response": "Returns a location point by address or query."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reverse(\n            self,\n            query,\n            exactly_one=DEFAULT_SENTINEL,\n            timeout=DEFAULT_SENTINEL,\n            feature_code=None,\n            lang=None,\n            find_nearby_type='findNearbyPlaceName',\n    ):\n        \"\"\"\n        Return an address by location point.\n\n            .. versionadded:: 1.2.0\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n            .. versionchanged:: 1.14.0\n               Default value for ``exactly_one`` was ``False``, which differs\n               from the conventional default across geopy. Please always pass\n               this argument explicitly, otherwise you would get a warning.\n               In geopy 2.0 the default value will become ``True``.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str feature_code: A GeoNames feature code\n\n            .. versionadded:: 1.18.0\n\n        :param str lang: language of the returned ``name`` element (the pseudo\n            language code 'local' will return it in local language)\n            Full list of supported languages can be found here:\n            https://www.geonames.org/countries/\n\n            .. versionadded:: 1.18.0\n\n        :param str find_nearby_type: A flag to switch between different\n            GeoNames API endpoints. The default value is ``findNearbyPlaceName``\n            which returns the closest populated place. Another currently\n            implemented option is ``findNearby`` which returns\n            the closest toponym for the lat/lng query.\n\n            .. versionadded:: 1.18.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n\n        \"\"\"\n        if exactly_one is DEFAULT_SENTINEL:\n            warnings.warn('%s.reverse: default value for `exactly_one` '\n                          'argument will become True in geopy 2.0. '\n                          'Specify `exactly_one=False` as the argument '\n                          'explicitly to get rid of this warning.' % type(self).__name__,\n                          DeprecationWarning, stacklevel=2)\n            exactly_one = False\n\n        try:\n            lat, lng = self._coerce_point_to_string(query).split(',')\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n\n        if find_nearby_type == 'findNearbyPlaceName':  # default\n            if feature_code:\n                raise ValueError(\n                    \"find_nearby_type=findNearbyPlaceName doesn't support \"\n                    \"the `feature_code` param\"\n                )\n            params = self._reverse_find_nearby_place_name_params(\n                lat=lat,\n                lng=lng,\n                lang=lang,\n            )\n            url = \"?\".join((self.api_reverse, urlencode(params)))\n        elif find_nearby_type == 'findNearby':\n            if lang:\n                raise ValueError(\n                    \"find_nearby_type=findNearby doesn't support the `lang` param\"\n                )\n            params = self._reverse_find_nearby_params(\n                lat=lat,\n                lng=lng,\n                feature_code=feature_code,\n            )\n            url = \"?\".join((self.api_reverse_nearby, urlencode(params)))\n        else:\n            raise GeocoderQueryError(\n                '`%s` find_nearby_type is not supported by geopy' % find_nearby_type\n            )\n\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )", "response": "Reverse the specified order of the addresses in the specified location point."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a timezone id for a point in query.", "response": "def reverse_timezone(self, query, timeout=DEFAULT_SENTINEL):\n        \"\"\"\n        Find the timezone for a point in `query`.\n\n        GeoNames always returns a timezone: if the point being queried\n        doesn't have an assigned Olson timezone id, a ``pytz.FixedOffset``\n        timezone is used to produce the :class:`geopy.timezone.Timezone`.\n\n        .. versionadded:: 1.18.0\n\n        :param query: The coordinates for which you want a timezone.\n        :type query: :class:`geopy.point.Point`, list or tuple of (latitude,\n            longitude), or string as \"%(latitude)s, %(longitude)s\"\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: :class:`geopy.timezone.Timezone`\n        \"\"\"\n        ensure_pytz_is_installed()\n\n        try:\n            lat, lng = self._coerce_point_to_string(query).split(',')\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n\n        params = {\n            \"lat\": lat,\n            \"lng\": lng,\n            \"username\": self.username,\n        }\n\n        url = \"?\".join((self.api_timezone, urlencode(params)))\n\n        logger.debug(\"%s.reverse_timezone: %s\", self.__class__.__name__, url)\n        return self._parse_json_timezone(\n            self._call_geocoder(url, timeout=timeout)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_json(self, doc, exactly_one):\n        places = doc.get('geonames', [])\n        self._raise_for_error(doc)\n        if not len(places):\n            return None\n\n        def parse_code(place):\n            \"\"\"\n            Parse each record.\n            \"\"\"\n            latitude = place.get('lat', None)\n            longitude = place.get('lng', None)\n            if latitude and longitude:\n                latitude = float(latitude)\n                longitude = float(longitude)\n            else:\n                return None\n\n            placename = place.get('name')\n            state = place.get('adminName1', None)\n            country = place.get('countryName', None)\n\n            location = ', '.join(\n                [x for x in [placename, state, country] if x]\n            )\n\n            return Location(location, (latitude, longitude), place)\n\n        if exactly_one:\n            return parse_code(places[0])\n        else:\n            return [parse_code(place) for place in places]", "response": "Parse the JSON response body."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a location point by address.", "response": "def geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            boundary_rect=None,\n            country_bias=None,\n    ):\n        \"\"\"\n        Return a location point by address.\n\n        :param str query: The address, query or structured query to geocode\n            you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :type boundary_rect: list or tuple of 2 items of :class:`geopy.point.Point`\n            or ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n        :param boundary_rect: Coordinates to restrict search within.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n\n            .. versionadded:: 1.19.0\n\n        :param str country_bias: Bias results to this country (ISO alpha-3).\n\n            .. versionadded:: 1.19.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {'text': self.format_string % query}\n\n        if self.api_key:\n            params.update({\n                'api_key': self.api_key\n            })\n\n        if boundary_rect is None:\n            boundary_rect = self.boundary_rect\n        if boundary_rect:\n            if len(boundary_rect) == 4:\n                warnings.warn(\n                    '%s `boundary_rect` format of '\n                    '`[longitude, latitude, longitude, latitude]` is now '\n                    'deprecated and will not be supported in geopy 2.0. '\n                    'Use `[Point(latitude, longitude), Point(latitude, longitude)]` '\n                    'instead.' % type(self).__name__,\n                    DeprecationWarning,\n                    stacklevel=2\n                )\n                lon1, lat1, lon2, lat2 = boundary_rect\n                boundary_rect = [[lat1, lon1], [lat2, lon2]]\n            lon1, lat1, lon2, lat2 = self._format_bounding_box(\n                boundary_rect, \"%(lon1)s,%(lat1)s,%(lon2)s,%(lat2)s\").split(',')\n            params['boundary.rect.min_lon'] = lon1\n            params['boundary.rect.min_lat'] = lat1\n            params['boundary.rect.max_lon'] = lon2\n            params['boundary.rect.max_lat'] = lat2\n\n        if country_bias is None:\n            country_bias = self.country_bias\n        if country_bias:\n            params['boundary.country'] = country_bias\n\n        url = \"?\".join((self.geocode_api, urlencode(params)))\n        logger.debug(\"%s.geocode_api: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn location latitude longitude from json feed.", "response": "def _parse_json(self, json, exactly_one=True):\n        '''Returns location, (latitude, longitude) from json feed.'''\n        features = json['features']\n        if features == []:\n            return None\n\n        def parse_feature(feature):\n            location = feature['place_name']\n            place = feature['text']\n            longitude = feature['geometry']['coordinates'][0]\n            latitude = feature['geometry']['coordinates'][1]\n            return Location(location, (latitude, longitude), place)\n        if exactly_one:\n            return parse_feature(features[0])\n        else:\n            return [parse_feature(feature) for feature in features]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geocode(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n            proximity=None,\n            country=None,\n            bbox=None,\n    ):\n        \"\"\"\n        Return a location point by address\n\n        :param str query: The address or query you wish to geocode.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param proximity: A coordinate to bias local results based on a provided\n            location.\n        :type proximity: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param country: Country to filter result in form of\n            ISO 3166-1 alpha-2 country code (e.g. ``FR``).\n            Might be a Python list of strings.\n\n            .. versionchanged:: 1.19.0\n                Previously only a single string could be specified.\n                Now a Python list of individual countries is supported.\n\n        :type country: str or list\n\n        :param bbox: The bounding box of the viewport within which\n            to bias geocode results more prominently.\n            Example: ``[Point(22, 180), Point(-22, -180)]``.\n        :type bbox: list or tuple of 2 items of :class:`geopy.point.Point` or\n            ``(latitude, longitude)`` or ``\"%(latitude)s, %(longitude)s\"``.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {}\n\n        params['access_token'] = self.api_key\n        query = self.format_string % query\n        if bbox:\n            params['bbox'] = self._format_bounding_box(\n                bbox, \"%(lon1)s,%(lat1)s,%(lon2)s,%(lat2)s\")\n\n        if not country:\n            country = []\n        if isinstance(country, string_compare):\n            country = [country]\n        if country:\n            params['country'] = \",\".join(country)\n\n        if proximity:\n            p = Point(proximity)\n            params['proximity'] = \"%s,%s\" % (p.longitude, p.latitude)\n\n        quoted_query = quote(query.encode('utf-8'))\n        url = \"?\".join((self.api % dict(query=quoted_query),\n                        urlencode(params)))\n        logger.debug(\"%s.geocode: %s\", self.__class__.__name__, url)\n\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout)\n        )", "response": "Returns a location point by address or query."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of addresses that are closest to the specified location point.", "response": "def reverse(\n            self,\n            query,\n            exactly_one=True,\n            timeout=DEFAULT_SENTINEL,\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        params = {}\n        params['access_token'] = self.api_key\n\n        point = self._coerce_point_to_string(query, \"%(lon)s,%(lat)s\")\n        quoted_query = quote(point.encode('utf-8'))\n        url = \"?\".join((self.api % dict(query=quoted_query),\n                        urlencode(params)))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout), exactly_one\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a geocoder class for the given service.", "response": "def get_geocoder_for_service(service):\n    \"\"\"\n    For the service provided, try to return a geocoder class.\n\n    >>> from geopy.geocoders import get_geocoder_for_service\n    >>> get_geocoder_for_service(\"nominatim\")\n    geopy.geocoders.osm.Nominatim\n\n    If the string given is not recognized, a\n    :class:`geopy.exc.GeocoderNotFound` exception is raised.\n\n    \"\"\"\n    try:\n        return SERVICE_TO_GEOCODER[service.lower()]\n    except KeyError:\n        raise GeocoderNotFound(\n            \"Unknown geocoder '%s'; options are: %s\" %\n            (service, SERVICE_TO_GEOCODER.keys())\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reverse(\n            self,\n            query,\n            exactly_one=DEFAULT_SENTINEL,\n            timeout=DEFAULT_SENTINEL,\n            kind=None,\n    ):\n        \"\"\"\n        Return an address by location point.\n\n        :param query: The coordinates for which you wish to obtain the\n            closest human-readable addresses.\n        :type query: :class:`geopy.point.Point`, list or tuple of ``(latitude,\n            longitude)``, or string as ``\"%(latitude)s, %(longitude)s\"``.\n\n        :param bool exactly_one: Return one result or a list of results, if\n            available.\n\n            .. versionchanged:: 1.14.0\n               Default value for ``exactly_one`` was ``False``, which differs\n               from the conventional default across geopy. Please always pass\n               this argument explicitly, otherwise you would get a warning.\n               In geopy 2.0 the default value will become ``True``.\n\n        :param int timeout: Time, in seconds, to wait for the geocoding service\n            to respond before raising a :class:`geopy.exc.GeocoderTimedOut`\n            exception. Set this only if you wish to override, on this call\n            only, the value set during the geocoder's initialization.\n\n        :param str kind: Type of toponym. Allowed values: `house`, `street`, `metro`,\n            `district`, `locality`.\n\n            .. versionadded:: 1.14.0\n\n        :rtype: ``None``, :class:`geopy.location.Location` or a list of them, if\n            ``exactly_one=False``.\n        \"\"\"\n        if exactly_one is DEFAULT_SENTINEL:\n            warnings.warn('%s.reverse: default value for `exactly_one` '\n                          'argument will become True in geopy 2.0. '\n                          'Specify `exactly_one=False` as the argument '\n                          'explicitly to get rid of this warning.' % type(self).__name__,\n                          DeprecationWarning, stacklevel=2)\n            exactly_one = False\n\n        try:\n            point = self._coerce_point_to_string(query, \"%(lon)s,%(lat)s\")\n        except ValueError:\n            raise ValueError(\"Must be a coordinate pair or Point\")\n        params = {\n            'geocode': point,\n            'format': 'json'\n        }\n        if self.api_key:\n            params['apikey'] = self.api_key\n        if self.lang:\n            params['lang'] = self.lang\n        if kind:\n            params['kind'] = kind\n        url = \"?\".join((self.api, urlencode(params)))\n        logger.debug(\"%s.reverse: %s\", self.__class__.__name__, url)\n        return self._parse_json(\n            self._call_geocoder(url, timeout=timeout),\n            exactly_one\n        )", "response": "Returns an address by location point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the JSON response body.", "response": "def _parse_json(self, doc, exactly_one):\n        \"\"\"\n        Parse JSON response body.\n        \"\"\"\n        if doc.get('error'):\n            raise GeocoderServiceError(doc['error']['message'])\n\n        try:\n            places = doc['response']['GeoObjectCollection']['featureMember']\n        except KeyError:\n            raise GeocoderParseError('Failed to parse server response')\n\n        def parse_code(place):\n            \"\"\"\n            Parse each record.\n            \"\"\"\n            try:\n                place = place['GeoObject']\n            except KeyError:\n                raise GeocoderParseError('Failed to parse server response')\n\n            longitude, latitude = [\n                float(_) for _ in place['Point']['pos'].split(' ')\n            ]\n\n            name_elements = ['name', 'description']\n            location = ', '.join([place[k] for k in name_elements if place.get(k)])\n\n            return Location(location, (latitude, longitude), place)\n\n        if exactly_one:\n            try:\n                return parse_code(places[0])\n            except IndexError:\n                return None\n        else:\n            return [parse_code(place) for place in places]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_degrees(degrees, fmt=DEGREES_FORMAT, symbols=None):\n    symbols = symbols or ASCII_SYMBOLS\n    arcminutes = units.arcminutes(degrees=degrees - int(degrees))\n    arcseconds = units.arcseconds(arcminutes=arcminutes - int(arcminutes))\n    format_dict = dict(\n        symbols,\n        degrees=degrees,\n        minutes=abs(arcminutes),\n        seconds=abs(arcseconds)\n    )\n    return fmt % format_dict", "response": "format_degrees - > string"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_distance(kilometers, fmt=DISTANCE_FORMAT, unit='km'):\n    magnitude = DISTANCE_UNITS[unit](kilometers)\n    return fmt % {'magnitude': magnitude, 'unit': unit}", "response": "Formats a distance in the given format."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _last_step(self):\n        if not os.path.exists(self._savepath):\n            return 0\n        try:\n            with open(self._savepath, 'r') as fh:\n                j = json.loads(fh.read())\n        except Exception:\n            logger.warning(\n                'Could not read or JSON-deserialize %s', self._savepath\n            )\n            return 0\n        if j.get('version', '') != VERSION:\n            return 0\n        return j.get('last_successful_step', 0)", "response": "Returns the last - run step for the current release."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a build object find the acceptance36 job", "response": "def _find_travis_job(self, build, toxenv):\n        \"\"\"Given a build object, find the acceptance36 job\"\"\"\n        for jobid in build.job_ids:\n            j = self._travis.job(jobid)\n            if 'TOXENV=%s' % toxenv in j.config['env']:\n                logger.debug('Found %s job: %s', toxenv, j.number)\n                return j\n        raise SystemExit(\n            'ERROR: could not find %s job for build %s (%s)' % (\n                toxenv, build.number, build.id\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the specified tox environment.", "response": "def _run_tox_env(self, env_name, extra_env_vars={}):\n        \"\"\"\n        Run the specified tox environment.\n\n        :param env_name: name of the tox environment to run\n        :type env_name: str\n        :param extra_env_vars: additional variables to set in the environment\n        :type extra_env_vars: dict\n        :raises: RuntimeError\n        :returns: combined STDOUT / STDERR\n        :rtype: str\n        \"\"\"\n        projdir = self.projdir\n        env = deepcopy(os.environ)\n        env['PATH'] = self._fixed_path(projdir)\n        env.update(extra_env_vars)\n        cmd = [os.path.join(projdir, 'bin', 'tox'), '-e', env_name]\n        logger.info(\n            'Running tox environment %s: args=\"%s\" cwd=%s '\n            'timeout=1800', env_name, ' '.join(cmd), projdir\n        )\n        res = subprocess.run(\n            cmd, stdout=subprocess.PIPE, stderr=subprocess.STDOUT,\n            cwd=projdir, timeout=1800, env=env\n        )\n        logger.info('tox process exited %d', res.returncode)\n        if res.returncode != 0:\n            logger.error(\n                'ERROR: tox environment %s exitcode %d',\n                env_name, res.returncode\n            )\n            logger.error(\n                'tox output:\\n%s', res.stdout.decode()\n            )\n            res.check_returncode()\n        return res.stdout.decode()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the current PATH fixed to remove the docker tox env.", "response": "def _fixed_path(self, projdir):\n        \"\"\"\n        Return the current PATH, fixed to remove the docker tox env.\n\n        :return: sanitized path\n        :rtype: str\n        \"\"\"\n        res = []\n        toxdir = os.path.join(projdir, '.tox')\n        for p in os.environ['PATH'].split(':'):\n            if not p.startswith(toxdir):\n                res.append(p)\n        return ':'.join(res)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_usage(self):\n        logger.debug(\"Getting usage for Lambda metrics\")\n        try:\n            self.connect()\n            resp = self.conn.get_account_settings()\n        except EndpointConnectionError as ex:\n            logger.warn('Skipping Lambda: %s', str(ex))\n            return\n        self.limits['Function Count']._add_current_usage(\n            resp['AccountUsage']['FunctionCount'])\n        self.limits['Total Code Size (MiB)']._add_current_usage(\n            int((resp['AccountUsage']['TotalCodeSize'])/1048576))\n        self._have_usage = True", "response": "Determine the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all known limits for this Lambda service as a dict of their namestoLimits.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        logger.debug(\"Getting limits for Lambda\")\n        if self.limits != {}:\n            return self.limits\n\n        self._construct_limits()\n\n        return self.limits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_limits_from_api(self):\n        logger.debug(\"Updating limits for Lambda from the AWS API\")\n        if len(self.limits) == 2:\n            return\n        self.connect()\n        lims = self.conn.get_account_settings()['AccountLimit']\n        self.limits['Total Code Size (MiB)']._set_api_limit(\n            (lims['TotalCodeSize']/1048576))\n        self.limits['Code Size Unzipped (MiB) per Function']._set_api_limit(\n            (lims['CodeSizeUnzipped']/1048576))\n        self.limits['Unreserved Concurrent Executions']._set_api_limit(\n            lims['UnreservedConcurrentExecutions'])\n        self.limits['Concurrent Executions']._set_api_limit(\n            lims['ConcurrentExecutions'])\n        self.limits['Code Size Zipped (MiB) per Function']._set_api_limit(\n            (lims['CodeSizeZipped']/1048576))", "response": "Update self. limits with the quotas returned from the Lambda s DescribeLimits API action."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the current usage for each limit of this service and update corresponding Limit via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n\n        self.limits['Auto Scaling groups']._add_current_usage(\n            len(\n                paginate_dict(\n                    self.conn.describe_auto_scaling_groups,\n                    alc_marker_path=['NextToken'],\n                    alc_data_path=['AutoScalingGroups'],\n                    alc_marker_param='NextToken'\n                )['AutoScalingGroups']\n            ),\n            aws_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n\n        self.limits['Launch configurations']._add_current_usage(\n            len(\n                paginate_dict(\n                    self.conn.describe_launch_configurations,\n                    alc_marker_path=['NextToken'],\n                    alc_data_path=['LaunchConfigurations'],\n                    alc_marker_param='NextToken'\n                )['LaunchConfigurations']\n            ),\n            aws_type='AWS::AutoScaling::LaunchConfiguration',\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all known limits for this service as a dict of their names to ~. AwsLimit objects.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        # autoscaleconnection.get_all_groups()\n        limits['Auto Scaling groups'] = AwsLimit(\n            'Auto Scaling groups',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::AutoScalingGroup',\n        )\n        # autoscaleconnection.get_all_launch_configurations()\n        limits['Launch configurations'] = AwsLimit(\n            'Launch configurations',\n            self,\n            200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::AutoScaling::LaunchConfiguration',\n        )\n        self.limits = limits\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_limits_from_api(self):\n        self.connect()\n        logger.info(\"Querying EC2 DescribeAccountAttributes for limits\")\n        lims = self.conn.describe_account_limits()\n        self.limits['Auto Scaling groups']._set_api_limit(\n            lims['MaxNumberOfAutoScalingGroups'])\n        self.limits['Launch configurations']._set_api_limit(\n            lims['MaxNumberOfLaunchConfigurations'])", "response": "Query EC2 s DescribeAccountAttributes API action and update self. limits with the quotas returned. Updates self. limits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_usage_clusters(self):\n        count = 0\n        fargate_task_count = 0\n        paginator = self.conn.get_paginator('list_clusters')\n        for page in paginator.paginate():\n            for cluster_arn in page['clusterArns']:\n                count += 1\n                resp = self.conn.describe_clusters(\n                    clusters=[cluster_arn], include=['STATISTICS']\n                )\n                cluster = resp['clusters'][0]\n                self.limits[\n                    'Container Instances per Cluster'\n                ]._add_current_usage(\n                    cluster['registeredContainerInstancesCount'],\n                    aws_type='AWS::ECS::ContainerInstance',\n                    resource_id=cluster['clusterName']\n                )\n                self.limits['Services per Cluster']._add_current_usage(\n                    cluster['activeServicesCount'],\n                    aws_type='AWS::ECS::Service',\n                    resource_id=cluster['clusterName']\n                )\n                # Note: 'statistics' is not always present in API responses,\n                # even if requested. As far as I can tell, it's omitted if\n                # a cluster has no Fargate tasks.\n                for stat in cluster.get('statistics', []):\n                    if stat['name'] != 'runningFargateTasksCount':\n                        continue\n                    logger.debug(\n                        'Found %s Fargate tasks in cluster %s',\n                        stat['value'], cluster_arn\n                    )\n                    fargate_task_count += int(stat['value'])\n                self._find_usage_one_cluster(cluster['clusterName'])\n        self.limits['Fargate Tasks']._add_current_usage(\n            fargate_task_count, aws_type='AWS::ECS::Task'\n        )\n        self.limits['Clusters']._add_current_usage(\n            count, aws_type='AWS::ECS::Cluster'\n        )", "response": "Find the ECS service usage for clusters. Calls _find_usage_one_cluster for each cluster."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind usage for services in each cluster.", "response": "def _find_usage_one_cluster(self, cluster_name):\n        \"\"\"\n        Find usage for services in each cluster.\n\n        :param cluster_name: name of the cluster to find usage for\n        :type cluster_name: str\n        \"\"\"\n        tps_lim = self.limits['EC2 Tasks per Service (desired count)']\n        paginator = self.conn.get_paginator('list_services')\n        for page in paginator.paginate(\n            cluster=cluster_name, launchType='EC2'\n        ):\n            for svc_arn in page['serviceArns']:\n                svc = self.conn.describe_services(\n                    cluster=cluster_name, services=[svc_arn]\n                )['services'][0]\n                if svc['launchType'] != 'EC2':\n                    continue\n                tps_lim._add_current_usage(\n                    svc['desiredCount'],\n                    aws_type='AWS::ECS::Service',\n                    resource_id='cluster=%s; service=%s' % (\n                        cluster_name, svc['serviceName']\n                    )\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_apis()\n        self._find_usage_api_keys()\n        self._find_usage_certs()\n        self._find_usage_plans()\n        self._find_usage_vpc_links()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find the current usage for each limit of this service and update the corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_usage_apis(self):\n        api_ids = []\n        logger.debug('Finding usage for APIs')\n        regional_count = 0\n        private_count = 0\n        edge_count = 0\n        paginator = self.conn.get_paginator('get_rest_apis')\n        for resp in paginator.paginate():\n            for api in resp['items']:\n                api_ids.append(api['id'])\n                epconf = api.get('endpointConfiguration', {}).get('types', [])\n                if epconf == ['PRIVATE']:\n                    private_count += 1\n                elif epconf == ['EDGE']:\n                    edge_count += 1\n                else:\n                    regional_count += 1\n        self.limits['Regional APIs per account']._add_current_usage(\n            regional_count, aws_type='AWS::ApiGateway::RestApi'\n        )\n        self.limits['Private APIs per account']._add_current_usage(\n            private_count, aws_type='AWS::ApiGateway::RestApi'\n        )\n        self.limits['Edge APIs per account']._add_current_usage(\n            edge_count, aws_type='AWS::ApiGateway::RestApi'\n        )\n        logger.debug('Found %d APIs', len(api_ids))\n        # now the per-API limits...\n        warn_stages_paginated = None\n        logger.debug('Finding usage for per-API limits')\n        for api_id in api_ids:\n            res_count = 0\n            paginator = self.conn.get_paginator('get_resources')\n            for resp in paginator.paginate(restApiId=api_id):\n                res_count += len(resp['items'])\n            self.limits['Resources per API']._add_current_usage(\n                res_count, resource_id=api_id,\n                aws_type='AWS::ApiGateway::Resource'\n            )\n            doc_parts = paginate_dict(\n                self.conn.get_documentation_parts,\n                restApiId=api_id,\n                alc_marker_path=['position'],\n                alc_data_path=['items'],\n                alc_marker_param='position'\n            )\n            self.limits['Documentation parts per API']._add_current_usage(\n                len(doc_parts), resource_id=api_id,\n                aws_type='AWS::ApiGateway::DocumentationPart'\n            )\n            # note that per the boto3 docs, there's no pagination of this...\n            stages = self.conn.get_stages(restApiId=api_id)\n            if len(set(stages.keys()) - set(['item', 'ResponseMetadata'])) > 0:\n                warn_stages_paginated = stages.keys()\n            self.limits['Stages per API']._add_current_usage(\n                len(stages['item']), resource_id=api_id,\n                aws_type='AWS::ApiGateway::Stage'\n            )\n            authorizers = paginate_dict(\n                self.conn.get_authorizers,\n                restApiId=api_id,\n                alc_marker_path=['position'],\n                alc_data_path=['items'],\n                alc_marker_param='position'\n            )\n            self.limits['Custom authorizers per API']._add_current_usage(\n                len(authorizers), resource_id=api_id,\n                aws_type='AWS::ApiGateway::Authorizer'\n            )\n        if warn_stages_paginated is not None:\n            logger.warning(\n                'APIGateway get_stages returned more keys than present in '\n                'boto3 docs: %s', sorted(warn_stages_paginated)\n            )", "response": "Find usage on APIs and Resources and RestAPIs and update self. limits."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds usage on API Keys. Update self. limits.", "response": "def _find_usage_api_keys(self):\n        \"\"\"\n        Find usage on API Keys.\n        Update `self.limits`.\n        \"\"\"\n        logger.debug('Finding usage for API Keys')\n        key_count = 0\n        paginator = self.conn.get_paginator('get_api_keys')\n        for resp in paginator.paginate():\n            key_count += len(resp['items'])\n        self.limits['API keys per account']._add_current_usage(\n            key_count, aws_type='AWS::ApiGateway::ApiKey'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_usage_certs(self):\n        logger.debug('Finding usage for Client Certificates')\n        cert_count = 0\n        paginator = self.conn.get_paginator('get_client_certificates')\n        for resp in paginator.paginate():\n            cert_count += len(resp['items'])\n        self.limits['Client certificates per account']._add_current_usage(\n            cert_count, aws_type='AWS::ApiGateway::ClientCertificate'\n        )", "response": "Find usage on Client Certificates. Update self. limits."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_usage_vpc_links(self):\n        logger.debug('Finding usage for VPC Links')\n        link_count = 0\n        paginator = self.conn.get_paginator('get_vpc_links')\n        for resp in paginator.paginate():\n            link_count += len(resp['items'])\n        self.limits['VPC Links per account']._add_current_usage(\n            link_count, aws_type='AWS::ApiGateway::VpcLink'\n        )", "response": "Find usage on VPC Links. Update self. limits."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding usage on Usage Plans and plans per API Key. Update self. limits.", "response": "def _find_usage_plans(self):\n        \"\"\"\n        Find usage on Usage Plans and plans per API Key. Update `self.limits`.\n        \"\"\"\n        logger.debug('Finding usage for Usage Plans')\n        plan_count = 0\n        paginator = self.conn.get_paginator('get_usage_plans')\n        for resp in paginator.paginate():\n            plan_count += len(resp['items'])\n        self.limits['Usage plans per account']._add_current_usage(\n            plan_count, aws_type='AWS::ApiGateway::UsagePlan'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        try:\n            self._find_usage_filesystems()\n        except (EndpointConnectionError, ClientError, ConnectTimeout) as ex:\n            logger.warning(\n                'Caught exception when trying to use EFS ('\n                'perhaps the EFS service is not available in this '\n                'region?): %s', ex\n            )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_limits_from_api(self):\n        region_limits = {\n            'us-east-1': 70\n        }\n        self.connect()\n        rname = self.conn._client_config.region_name\n        if rname in region_limits:\n            self.limits['File systems'].default_limit = region_limits[rname]\n            logger.debug(\n                'Running in region %s; setting EFS \"File systems\" default '\n                'limit value to: %d', rname, region_limits[rname]\n            )", "response": "Update limits for the current region."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the limits of the current TA if possible.", "response": "def update_limits(self):\n        \"\"\"\n        Poll 'Service Limits' check results from Trusted Advisor, if possible.\n        Iterate over all :py:class:`~.AwsLimit` objects for the given services\n        and update their limits from TA if present in TA checks.\n\n        :param services: dict of service name (string) to\n          :py:class:`~._AwsService` objects\n        :type services: dict\n        \"\"\"\n        if self.limits_updated:\n            logger.debug('Already polled TA; skipping update')\n            return\n        self.connect()\n        ta_results = self._poll()\n        self._update_services(ta_results)\n        self.limits_updated = True"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _poll(self):\n        logger.info(\"Beginning TrustedAdvisor poll\")\n        tmp = self._get_limit_check_id()\n        if not self.have_ta:\n            logger.info('TrustedAdvisor.have_ta is False; not polling TA')\n            return {}\n        if tmp is None:\n            logger.critical(\"Unable to find 'Service Limits' Trusted Advisor \"\n                            \"check; not using Trusted Advisor data.\")\n            return\n        check_id, metadata = tmp\n        checks = self._get_refreshed_check_result(check_id)\n        region = self.ta_region or self.conn._client_config.region_name\n        res = {}\n        if checks['result'].get('status', '') == 'not_available':\n            logger.warning(\n                'Trusted Advisor returned status \"not_available\" for '\n                'service limit check; cannot retrieve limits from TA.'\n            )\n            return {}\n        if 'flaggedResources' not in checks['result']:\n            logger.warning(\n                'Trusted Advisor returned no results for '\n                'service limit check; cannot retrieve limits from TA.'\n            )\n            return {}\n        for check in checks['result']['flaggedResources']:\n            if 'region' in check and check['region'] != region:\n                continue\n            data = dict(zip(metadata, check['metadata']))\n            if data['Service'] not in res:\n                res[data['Service']] = {}\n            try:\n                val = int(data['Limit Amount'])\n            except ValueError:\n                val = data['Limit Amount']\n                if val != 'Unlimited':\n                    logger.error('TrustedAdvisor returned unknown Limit '\n                                 'Amount %s for %s - %s', val, data['Service'],\n                                 data['Limit Name'])\n                    continue\n                else:\n                    logger.debug('TrustedAdvisor setting explicit \"Unlimited\" '\n                                 'limit for %s - %s', data['Service'],\n                                 data['Limit Name'])\n            res[data['Service']][data['Limit Name']] = val\n        logger.info(\"Finished TrustedAdvisor poll\")\n        return res", "response": "Poll Trusted Advisor API for limit checks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery currently - available TA checks return the ID and metadata of the Service Limits check.", "response": "def _get_limit_check_id(self):\n        \"\"\"\n        Query currently-available TA checks, return the check ID and metadata\n        of the 'performance/Service Limits' check.\n\n        :returns: 2-tuple of Service Limits TA check ID (string),\n          metadata (list), or (None, None).\n        :rtype: tuple\n        \"\"\"\n        logger.debug(\"Querying Trusted Advisor checks\")\n        try:\n            checks = self.conn.describe_trusted_advisor_checks(\n                language='en'\n            )['checks']\n        except ClientError as ex:\n            if ex.response['Error']['Code'] == 'SubscriptionRequiredException':\n                logger.warning(\n                    \"Cannot check TrustedAdvisor: %s\",\n                    ex.response['Error']['Message']\n                )\n                self.have_ta = False\n                return (None, None)\n            else:\n                raise ex\n        for check in checks:\n            if (\n                            check['category'] == 'performance' and\n                            check['name'] == 'Service Limits'\n            ):\n                logger.debug(\"Found TA check; id=%s\", check['id'])\n                return (\n                    check['id'],\n                    check['metadata']\n                )\n        logger.debug(\"Unable to find check with category 'performance' and \"\n                     \"name 'Service Limits'.\")\n        return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving the ``check_id``, return the dict of Trusted Advisor check results. This handles refreshing the Trusted Advisor check, if desired, according to ``self.refresh_mode`` and ``self.refresh_timeout``. :param check_id: the Trusted Advisor check ID :type check_id: str :returns: dict check result. The return value of :py:meth:`Support.Client.describe_trusted_advisor_check_result` :rtype: dict", "response": "def _get_refreshed_check_result(self, check_id):\n        \"\"\"\n        Given the ``check_id``, return the dict of Trusted Advisor check\n        results. This handles refreshing the Trusted Advisor check, if desired,\n        according to ``self.refresh_mode`` and ``self.refresh_timeout``.\n\n        :param check_id: the Trusted Advisor check ID\n        :type check_id: str\n        :returns: dict check result. The return value of\n          :py:meth:`Support.Client.describe_trusted_advisor_check_result`\n        :rtype: dict\n        \"\"\"\n        # handle a refresh_mode of None right off the bat\n        if self.refresh_mode is None:\n            logger.info(\"Not refreshing Trusted Advisor check (refresh mode \"\n                        \"is None)\")\n            return self._get_check_result(check_id)[0]\n        logger.debug(\"Handling refresh of check: %s\", check_id)\n        # if we want to refresh, step 1 is to see if we can yet...\n        if not self._can_refresh_check(check_id):\n            return self._get_check_result(check_id)[0]\n        # either it's not too soon to refresh, or we have no idea...\n        if isinstance(self.refresh_mode, type(1)):\n            # mode is an int, check the last refresh time and compare\n            checks, check_datetime = self._get_check_result(check_id)\n            logger.debug('ta_refresh_mode older; check last refresh: %s; '\n                         'threshold=%d seconds', check_datetime,\n                         self.refresh_mode)\n            if check_datetime >= datetime.now(utc) - timedelta(\n                    seconds=self.refresh_mode):\n                logger.warning('Trusted Advisor check %s last refresh time '\n                               'of %s is newer than refresh threshold of %d '\n                               'seconds.', check_id, check_datetime,\n                               self.refresh_mode)\n                return self._get_check_result(check_id)[0]\n        # do the refresh\n        logger.info(\"Refreshing Trusted Advisor check: %s\", check_id)\n        self.conn.refresh_trusted_advisor_check(checkId=check_id)\n        # if mode isn't trigger, wait for refresh up to timeout\n        if self.refresh_mode == 'trigger':\n            result = self._get_check_result(check_id)[0]\n        else:\n            result = self._poll_for_refresh(check_id)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _poll_for_refresh(self, check_id):\n        logger.warning('Polling for TA check %s refresh...', check_id)\n        if self.refresh_timeout is None:\n            # no timeout...\n            cutoff = datetime_now() + timedelta(days=365)\n        else:\n            cutoff = datetime_now() + timedelta(seconds=self.refresh_timeout)\n        last_status = None\n        while datetime_now() <= cutoff:\n            logger.debug('Checking refresh status')\n            status = self.conn.describe_trusted_advisor_check_refresh_statuses(\n                checkIds=[check_id]\n            )['statuses'][0]['status']\n            if status in ['success', 'abandoned']:\n                logger.info('Refresh status: %s; done polling', status)\n                break\n            if status == 'none' and last_status not in ['none', None]:\n                logger.warning('Trusted Advisor check refresh status went '\n                               'from \"%s\" to \"%s\"; refresh is either complete '\n                               'or timed out on AWS side. Continuing',\n                               last_status, status)\n                break\n            last_status = status\n            logger.info('Refresh status: %s; sleeping 30s', status)\n            sleep(30)\n        else:\n            logger.error('Timed out waiting for TA Check refresh; status=%s',\n                         status)\n        logger.info('Done polling for check refresh')\n        result, last_dt = self._get_check_result(check_id)\n        logger.debug('Check shows last refresh time of: %s', last_dt)\n        return result", "response": "Poll for a TA check refresh."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if the given check_id can be refreshed yet.", "response": "def _can_refresh_check(self, check_id):\n        \"\"\"\n        Determine if the given check_id can be refreshed yet.\n\n        :param check_id: the Trusted Advisor check ID\n        :type check_id: str\n        :return: whether or not the check can be refreshed yet\n        :rtype: bool\n        \"\"\"\n        try:\n            refresh_status = \\\n                self.conn.describe_trusted_advisor_check_refresh_statuses(\n                    checkIds=[check_id]\n                )\n            logger.debug(\"TA Check %s refresh status: %s\", check_id,\n                         refresh_status['statuses'][0])\n            ms = refresh_status['statuses'][0]['millisUntilNextRefreshable']\n            if ms > 0:\n                logger.warning(\"Trusted Advisor check cannot be refreshed for \"\n                               \"another %d milliseconds; skipping refresh and \"\n                               \"getting check results now\", ms)\n                return False\n            return True\n        except Exception:\n            logger.warning(\"Could not get refresh status for TA check %s\",\n                           check_id, exc_info=True)\n        # default to True if we don't know...\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the TrustedAdvisor limits for all services.", "response": "def _update_services(self, ta_results):\n        \"\"\"\n        Given a dict of TrustedAdvisor check results from :py:meth:`~._poll`\n        and a dict of Service objects passed in to :py:meth:`~.update_limits`,\n        updated the TrustedAdvisor limits for all services.\n\n        :param ta_results: results returned by :py:meth:`~._poll`\n        :type ta_results: dict\n        :param services: dict of service names to _AwsService objects\n        :type services: dict\n        \"\"\"\n        logger.debug(\"Updating TA limits on all services\")\n        for svc_name in sorted(ta_results.keys()):\n            svc_results = ta_results[svc_name]\n            if svc_name not in self.ta_services:\n                logger.info(\"TrustedAdvisor returned check results for \"\n                            \"unknown service '%s'\", svc_name)\n                continue\n            svc_limits = self.ta_services[svc_name]\n            for lim_name in sorted(svc_results):\n                if lim_name not in svc_limits:\n                    logger.info(\"TrustedAdvisor returned check results for \"\n                                \"unknown limit '%s' (service %s)\",\n                                lim_name,\n                                svc_name)\n                    continue\n                val = svc_results[lim_name]\n                if val == 'Unlimited':\n                    svc_limits[lim_name]._set_ta_unlimited()\n                else:\n                    svc_limits[lim_name]._set_ta_limit(val)\n        logger.info(\"Done updating TA limits on all services\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _make_ta_service_dict(self):\n        res = {}\n        for svc_name in self.all_services:\n            svc_obj = self.all_services[svc_name]\n            for lim_name, lim in svc_obj.get_limits().items():\n                if lim.ta_service_name not in res:\n                    res[lim.ta_service_name] = {}\n                res[lim.ta_service_name][lim.ta_limit_name] = lim\n        return res", "response": "Build our service and limits dict. This is laid out identical to self. all_services but keys limits by their TA service name and TA limit name properties."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the current usage for each limit of this service and update the Limit via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_applications()\n        self._find_usage_application_versions()\n        self._find_usage_environments()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds usage for ElasticBeanstalk applications", "response": "def _find_usage_applications(self):\n        \"\"\"find usage for ElasticBeanstalk applications\"\"\"\n        applications = self.conn.describe_applications()\n        self.limits['Applications']._add_current_usage(\n            len(applications['Applications']),\n            aws_type='AWS::ElasticBeanstalk::Application',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds usage for ElasticBeanstalk application verions", "response": "def _find_usage_application_versions(self):\n        \"\"\"find usage for ElasticBeanstalk application verions\"\"\"\n        versions = self.conn.describe_application_versions()\n        self.limits['Application versions']._add_current_usage(\n            len(versions['ApplicationVersions']),\n            aws_type='AWS::ElasticBeanstalk::ApplicationVersion',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_usage_environments(self):\n        environments = self.conn.describe_environments()\n        self.limits['Environments']._add_current_usage(\n            len(environments['Environments']),\n            aws_type='AWS::ElasticBeanstalk::Environment',\n        )", "response": "find usage for ElasticBeanstalk environments"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_cmd_output(cmd, output, name):\n    formatted = '.. code-block:: console\\n\\n'\n    formatted += '   (venv)$ {c}\\n'.format(c=cmd)\n    lines = output.split(\"\\n\")\n    if name != 'help':\n        for idx, line in enumerate(lines):\n            if len(line) > 100:\n                lines[idx] = line[:100] + ' (...)'\n        if len(lines) > 12:\n            tmp_lines = lines[:5] + ['(...)'] + lines[-5:]\n            if ' -l' in cmd or ' --list-defaults' in cmd:\n                # find a line that uses a limit from the API,\n                #  and a line with None (unlimited)\n                api_line = None\n                none_line = None\n                for line in lines:\n                    if '(API)' in line:\n                        api_line = line\n                        break\n                for line in lines:\n                    if line.strip().endswith('None'):\n                        none_line = line\n                        break\n                tmp_lines = lines[:5]\n                if api_line not in tmp_lines and api_line is not None:\n                    tmp_lines = tmp_lines + ['(...)'] + [api_line]\n                if none_line not in tmp_lines and none_line is not None:\n                    tmp_lines = tmp_lines + ['(...)'] + [none_line]\n                tmp_lines = tmp_lines + ['(...)'] + lines[-5:]\n            lines = tmp_lines\n    for line in lines:\n        if line.strip() == '':\n            continue\n        formatted += '   ' + line + \"\\n\"\n    formatted += '\\n'\n    return formatted", "response": "format command output for docs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntriggers rebuild of all dynamically generated documentation that is dynamically generated from awslimitchecker.", "response": "def build_docs():\n    \"\"\"\n    Trigger rebuild of all documentation that is dynamically generated\n    from awslimitchecker.\n    \"\"\"\n    if os.environ.get('CI', None) is not None:\n        print(\"Not building dynamic docs in CI environment\")\n        raise SystemExit(0)\n    region = os.environ.get('AWS_DEFAULT_REGION', None)\n    if region is None:\n        raise SystemExit(\"ERROR: Please export AWS_DEFAULT_REGION\")\n    logger.info(\"Beginning build of dynamically-generated docs\")\n    logger.info(\"Instantiating AwsLimitChecker\")\n    c = AwsLimitChecker(region=region)\n    build_iam_policy(c)\n    build_limits(c)\n    build_runner_examples()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_vpcs()\n        subnet_to_az = self._find_usage_subnets()\n        self._find_usage_ACLs()\n        self._find_usage_route_tables()\n        self._find_usage_gateways()\n        self._find_usage_nat_gateways(subnet_to_az)\n        self._find_usages_vpn_gateways()\n        self._find_usage_network_interfaces()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find usage for each limit of this service and update corresponding Limit via AwsLimit. _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_usage_vpcs(self):\n        # overall number of VPCs\n        vpcs = self.conn.describe_vpcs()\n        self.limits['VPCs']._add_current_usage(\n            len(vpcs['Vpcs']),\n            aws_type='AWS::EC2::VPC'\n        )", "response": "find usage for VPCs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_usage_subnets(self):\n        # subnets per VPC\n        subnet_to_az = {}\n        subnets = defaultdict(int)\n        for subnet in self.conn.describe_subnets()['Subnets']:\n            subnets[subnet['VpcId']] += 1\n            subnet_to_az[subnet['SubnetId']] = subnet['AvailabilityZone']\n        for vpc_id in subnets:\n            self.limits['Subnets per VPC']._add_current_usage(\n                subnets[vpc_id],\n                aws_type='AWS::EC2::VPC',\n                resource_id=vpc_id\n            )\n        return subnet_to_az", "response": "find usage for Subnets ; return dict of SubnetId to AZ"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding usage for ACLs", "response": "def _find_usage_ACLs(self):\n        \"\"\"find usage for ACLs\"\"\"\n        # Network ACLs per VPC\n        acls = defaultdict(int)\n        for acl in self.conn.describe_network_acls()['NetworkAcls']:\n            acls[acl['VpcId']] += 1\n            # Rules per network ACL\n            self.limits['Rules per network ACL']._add_current_usage(\n                len(acl['Entries']),\n                aws_type='AWS::EC2::NetworkAcl',\n                resource_id=acl['NetworkAclId']\n            )\n        for vpc_id in acls:\n            self.limits['Network ACLs per VPC']._add_current_usage(\n                acls[vpc_id],\n                aws_type='AWS::EC2::VPC',\n                resource_id=vpc_id,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_usage_route_tables(self):\n        # Route tables per VPC\n        tables = defaultdict(int)\n        for table in self.conn.describe_route_tables()['RouteTables']:\n            tables[table['VpcId']] += 1\n            # Entries per route table\n            routes = [\n                r for r in table['Routes']\n                if r['Origin'] != 'EnableVgwRoutePropagation'\n            ]\n            self.limits['Entries per route table']._add_current_usage(\n                len(routes),\n                aws_type='AWS::EC2::RouteTable',\n                resource_id=table['RouteTableId']\n            )\n        for vpc_id in tables:\n            self.limits['Route tables per VPC']._add_current_usage(\n                tables[vpc_id],\n                aws_type='AWS::EC2::VPC',\n                resource_id=vpc_id,\n            )", "response": "find usage for route tables"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_usage_gateways(self):\n        # Internet gateways\n        gws = self.conn.describe_internet_gateways()\n        self.limits['Internet gateways']._add_current_usage(\n            len(gws['InternetGateways']),\n            aws_type='AWS::EC2::InternetGateway',\n        )", "response": "find usage for Internet Gateways"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind usage for NAT Gateways", "response": "def _find_usage_nat_gateways(self, subnet_to_az):\n        \"\"\"\n        find usage for NAT Gateways\n\n        :param subnet_to_az: dict mapping subnet ID to AZ\n        :type subnet_to_az: dict\n        \"\"\"\n        # currently, some regions (sa-east-1 as one example) don't have NAT\n        # Gateway service; they return an AuthError,\n        # \"This request has been administratively disabled.\"\n        try:\n            gws_per_az = defaultdict(int)\n            for gw in paginate_dict(self.conn.describe_nat_gateways,\n                                    alc_marker_path=['NextToken'],\n                                    alc_data_path=['NatGateways'],\n                                    alc_marker_param='NextToken'\n                                    )['NatGateways']:\n                if gw['State'] not in ['pending', 'available']:\n                    logger.debug(\n                        'Skipping NAT Gateway %s in state: %s',\n                        gw['NatGatewayId'], gw['State']\n                    )\n                    continue\n                if gw['SubnetId'] not in subnet_to_az:\n                    logger.error('ERROR: NAT Gateway %s in SubnetId %s, but '\n                                 'SubnetId not found in subnet_to_az; Gateway '\n                                 'cannot be counted!', gw['NatGatewayId'],\n                                 gw['SubnetId'])\n                    continue\n                gws_per_az[subnet_to_az[gw['SubnetId']]] += 1\n            for az in sorted(gws_per_az.keys()):\n                self.limits['NAT Gateways per AZ']._add_current_usage(\n                    gws_per_az[az],\n                    resource_id=az,\n                    aws_type='AWS::EC2::NatGateway'\n                )\n        except ClientError:\n            logger.error('Caught exception when trying to list NAT Gateways; '\n                         'perhaps NAT service does not exist in this region?',\n                         exc_info=1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding usage of VPN gateways", "response": "def _find_usages_vpn_gateways(self):\n        \"\"\"find usage of vpn gateways\"\"\"\n\n        # do not include deleting and deleted in the results\n        vpngws = self.conn.describe_vpn_gateways(Filters=[\n            {\n                'Name': 'state',\n                'Values': [\n                    'available',\n                    'pending'\n                ]\n            }\n        ])['VpnGateways']\n\n        self.limits['Virtual private gateways']._add_current_usage(\n            len(vpngws),\n            aws_type='AWS::EC2::VPNGateway'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_usage_network_interfaces(self):\n        enis = paginate_dict(\n            self.conn.describe_network_interfaces,\n            alc_marker_path=['NextToken'],\n            alc_data_path=['NetworkInterfaces'],\n            alc_marker_param='NextToken'\n        )\n\n        self.limits['Network interfaces per Region']._add_current_usage(\n            len(enis['NetworkInterfaces']),\n            aws_type='AWS::EC2::NetworkInterface'\n        )", "response": "find usage of network interfaces"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery EC2 s DescribeAccountAttributes API action and update self. limits.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Query EC2's DescribeAccountAttributes API action and\n        update the network interface limit, as needed. Updates ``self.limits``.\n\n        More info on the network interface limit, from the docs:\n        'This limit is the greater of either the default limit (350) or your\n        On-Demand Instance limit multiplied by 5.\n        The default limit for On-Demand Instances is 20.'\n        \"\"\"\n        self.connect()\n        self.connect_resource()\n        logger.info(\"Querying EC2 DescribeAccountAttributes for limits\")\n        attribs = self.conn.describe_account_attributes()\n        for attrib in attribs['AccountAttributes']:\n            if attrib['AttributeName'] == 'max-instances':\n                val = attrib['AttributeValues'][0]['AttributeValue']\n                if int(val) * 5 > DEFAULT_ENI_LIMIT:\n                    limit_name = 'Network interfaces per Region'\n                    self.limits[limit_name]._set_api_limit(int(val) * 5)\n        logger.debug(\"Done setting limits from API\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        resp = self.conn.get_directory_limits()\n        directory_limits = resp['DirectoryLimits']\n        self.limits['CloudOnlyDirectories']._add_current_usage(\n            directory_limits['CloudOnlyDirectoriesCurrentCount'],\n            aws_type='AWS::DirectoryService'\n        )\n        self.limits['CloudOnlyMicrosoftAD']._add_current_usage(\n            directory_limits['CloudOnlyMicrosoftADCurrentCount'],\n            aws_type='AWS::DirectoryService'\n        )\n        self.limits['ConnectedDirectories']._add_current_usage(\n            directory_limits['ConnectedDirectoriesCurrentCount'],\n            aws_type='AWS::DirectoryService'\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Determine the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate self. limits with the limits from the API.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Call the service's API action to retrieve limit/quota information, and\n        update AwsLimit objects in ``self.limits`` with this information.\n        \"\"\"\n        logger.debug('Setting DirectoryService limits from API')\n        self.connect()\n        resp = self.conn.get_directory_limits()\n        directory_limits = resp['DirectoryLimits']\n        self.limits['CloudOnlyDirectories']._set_api_limit(\n            directory_limits['CloudOnlyDirectoriesLimit']\n        )\n        self.limits['CloudOnlyMicrosoftAD']._set_api_limit(\n            directory_limits['CloudOnlyMicrosoftADLimit']\n        )\n        self.limits['ConnectedDirectories']._set_api_limit(\n            directory_limits['ConnectedDirectoriesLimit']\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the current usage for each limit of this service and update corresponding Limit via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        ignore_statuses = [\n            'DELETE_COMPLETE'\n        ]\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        count = 0\n        paginator = self.conn.get_paginator('describe_stacks')\n        iter = paginator.paginate()\n        for page in iter:\n            for stk in page['Stacks']:\n                if stk['StackStatus'] not in ignore_statuses:\n                    count += 1\n        self.limits['Stacks']._add_current_usage(\n            count, aws_type='AWS::CloudFormation::Stack'\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_limits_from_api(self):\n        logger.debug('Setting CloudFormation limits from API')\n        self.connect()\n        resp = self.conn.describe_account_limits()\n        for lim in resp['AccountLimits']:\n            if lim['Name'] == 'StackLimit':\n                self.limits['Stacks']._set_api_limit(lim['Value'])\n                continue\n            logger.debug('API response contained unknown CloudFormation '\n                         'limit: %s', lim['Name'])", "response": "Update self. limits with the information from the API."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value for limit_override to override the default value for the Trusted Advisor information .", "response": "def set_limit_override(self, limit_value, override_ta=True):\n        \"\"\"\n        Set a new value for this limit, to override the default\n        (such as when AWS Support has increased a limit of yours).\n        If ``override_ta`` is True, this value will also supersede\n        any found through Trusted Advisor.\n\n        :param limit_value: the new limit value\n        :type limit_value: int\n        :param override_ta: whether or not to also override Trusted\n          Advisor information\n        :type override_ta: bool\n        \"\"\"\n        self.limit_override = limit_value\n        self.override_ta = override_ta"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_limit_source(self):\n        if self.limit_override is not None and (\n                self.override_ta is True or\n                (self.ta_limit is None and self.ta_unlimited is False)\n        ):\n            return SOURCE_OVERRIDE\n        if self.api_limit is not None:\n            return SOURCE_API\n        if self.ta_limit is not None or self.ta_unlimited is True:\n            return SOURCE_TA\n        return SOURCE_DEFAULT", "response": "Returns the limit source for the current limit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_limit(self):\n        limit_type = self.get_limit_source()\n        if limit_type == SOURCE_OVERRIDE:\n            return self.limit_override\n        elif limit_type == SOURCE_API:\n            return self.api_limit\n        elif limit_type == SOURCE_TA:\n            if self.ta_unlimited is True:\n                return None\n            return self.ta_limit\n        return self.default_limit", "response": "Returns the effective limit value for this Limit or None if the limit is explicitly unlimited."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_current_usage_str(self):\n        if len(self._current_usage) == 0:\n            return '<unknown>'\n        if len(self._current_usage) == 1:\n            return str(self._current_usage[0])\n        lim_str = ', '.join([str(x) for x in sorted(self._current_usage)])\n        s = 'max: {m} ({l})'.format(\n            m=str(max(self._current_usage)),\n            l=lim_str\n        )\n        return s", "response": "Returns a string describing the current usage for this limit."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_current_usage(self, value, maximum=None, resource_id=None,\n                           aws_type=None):\n        \"\"\"\n        Add a new current usage value for this limit.\n\n        Creates a new :py:class:`~.AwsLimitUsage` instance and\n        appends it to the internal list. If more than one usage value\n        is given to this service, they should have ``id`` and\n        ``aws_type`` set.\n\n        This method should only be called from the :py:class:`~._AwsService`\n        instance that created and manages this Limit.\n\n        :param value: the numeric usage value\n        :type value: :py:obj:`int` or :py:obj:`float`\n        :param resource_id: If there can be multiple usage values for one limit,\n          an AWS ID for the resource this instance describes\n        :type resource_id: str\n        :param aws_type: if ``id`` is not None, the AWS resource type\n          that ID represents. As a convention, we use the AWS Resource\n          Type names used by\n          `CloudFormation <http://docs.aws.amazon.com/AWSCloudFormation/latest/UserGuide/aws-template-resource-type-ref.html>`_  # noqa\n        :type aws_type: str\n        \"\"\"\n        self._current_usage.append(\n            AwsLimitUsage(\n                self,\n                value,\n                maximum=maximum,\n                resource_id=resource_id,\n                aws_type=aws_type\n            )\n        )", "response": "Adds a new current usage value for this limit."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the warning and critical thresholds for this Limit.", "response": "def _get_thresholds(self):\n        \"\"\"\n        Get the warning and critical thresholds for this Limit.\n\n        Return type is a 4-tuple of:\n\n        1. warning integer (usage) threshold, or None\n        2. warning percent threshold\n        3. critical integer (usage) threshold, or None\n        4. critical percent threshold\n\n        :rtype: tuple\n        \"\"\"\n        t = (\n            self.warn_count,\n            self.warn_percent or self.def_warning_threshold,\n            self.crit_count,\n            self.crit_percent or self.def_critical_threshold,\n        )\n        return t"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the default warning and critical thresholds used to evaluate the current usage count of the limit s usage.", "response": "def set_threshold_override(self, warn_percent=None, warn_count=None,\n                               crit_percent=None, crit_count=None):\n        \"\"\"\n        Override the default warning and critical thresholds used to evaluate\n        this limit's usage. Theresholds can be specified as a percentage\n        of the limit, or as a usage count, or both.\n\n        **Note:** The percent thresholds (``warn_percent`` and ``crit_percent``)\n        have default values that are set globally for awslimitchecker, unlike\n        the count thresholds. When setting threshold overrides to quiet or\n        suppress alerts for a limit, you **must** set the percent thresholds.\n        If you only set overrides for the ``count`` thresholds, the percent\n        thresholds will continue to be evaluated at their awslimitchecker-wide\n        default, and likely prevent alerts from being suppressed.\n\n        see :py:meth:`~.check_thresholds` for further information on threshold\n        evaluation.\n\n        :param warn_percent: new warning threshold, percentage used\n        :type warn_percent: int\n        :param warn_count: new warning threshold, actual count/number\n        :type warn_count: int\n        :param crit_percent: new critical threshold, percentage used\n        :type crit_percent: int\n        :param crit_count: new critical threshold, actual count/number\n        :type crit_count: int\n        \"\"\"\n        self.warn_percent = warn_percent\n        self.warn_count = warn_count\n        self.crit_percent = crit_percent\n        self.crit_count = crit_count"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks this limit s current usage against the specified default thresholds and returns True if any custom theresholds have been surpassed False otherwise.", "response": "def check_thresholds(self):\n        \"\"\"\n        Check this limit's current usage against the specified default\n        thresholds, and any custom theresholds that have been set on the\n        class instance. Return True if usage is within thresholds, or false if\n        warning or critical thresholds have been surpassed.\n\n        This method sets internal variables in this instance which can be\n        queried via :py:meth:`~.get_warnings` and :py:meth:`~.get_criticals`\n        to obtain further details about the thresholds that were crossed.\n\n        **Note** This function returns False if *any* thresholds were crossed.\n        Please be aware of this when setting threshold overrides to suppress\n        alerts. Each threshold (``warn_percent``, ``warn_count``,\n        ``crit_percent``, ``crit_count``) that has been set is evaluated\n        individually and the result appended to a list of warnings or criticals,\n        respectively. If *any* of these evaluations failed, the method returns\n        False.\n\n        :returns: False if any thresholds were crossed, True otherwise\n        :rtype: bool\n        \"\"\"\n        (warn_int, warn_pct, crit_int, crit_pct) = self._get_thresholds()\n        all_ok = True\n        for u in self._current_usage:\n            usage = u.get_value()\n            limit = u.get_maximum() or self.get_limit()\n            if limit is None:\n                continue\n            pct = (usage / (limit * 1.0)) * 100\n            if crit_int is not None and usage >= crit_int:\n                self._criticals.append(u)\n                all_ok = False\n            elif pct >= crit_pct:\n                self._criticals.append(u)\n                all_ok = False\n            elif warn_int is not None and usage >= warn_int:\n                self._warnings.append(u)\n                all_ok = False\n            elif pct >= warn_pct:\n                self._warnings.append(u)\n                all_ok = False\n        return all_ok"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_nodes()\n        self._find_usage_subnet_groups()\n        self._find_usage_parameter_groups()\n        self._find_usage_security_groups()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find the current usage for each limit of this service and update the corresponding Limit via AwsLimit. _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind usage for cache nodes", "response": "def _find_usage_nodes(self):\n        \"\"\"find usage for cache nodes\"\"\"\n        nodes = 0\n        paginator = self.conn.get_paginator('describe_cache_clusters')\n        for page in paginator.paginate(ShowCacheNodeInfo=True):\n            for cluster in page['CacheClusters']:\n                try:\n                    num_nodes = len(cluster['CacheNodes'])\n                except (IndexError, TypeError, KeyError):\n                    # sometimes CacheNodes is None...\n                    logger.debug(\n                        \"Cache Cluster '%s' returned dict with CacheNodes \"\n                        \"None\", cluster['CacheClusterId'])\n                    num_nodes = cluster['NumCacheNodes']\n                nodes += num_nodes\n                if cluster['Engine'] == 'memcached':\n                    self.limits['Nodes per Cluster']._add_current_usage(\n                        num_nodes,\n                        aws_type='AWS::ElastiCache::CacheCluster',\n                        resource_id=cluster['CacheClusterId'],\n                    )\n\n        self.limits['Nodes']._add_current_usage(\n            nodes,\n            aws_type='AWS::ElastiCache::CacheNode'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding usage for elasticache parameter groups", "response": "def _find_usage_parameter_groups(self):\n        \"\"\"find usage for elasticache parameter groups\"\"\"\n        num_groups = 0\n\n        paginator = self.conn.get_paginator('describe_cache_parameter_groups')\n        for page in paginator.paginate():\n            for group in page['CacheParameterGroups']:\n                num_groups += 1\n        self.limits['Parameter Groups']._add_current_usage(\n            num_groups,\n            aws_type='AWS::ElastiCache::ParameterGroup'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_usage_security_groups(self):\n        num_groups = 0\n        # If EC2-Classic isn't available (e.g., a new account)\n        # this method will fail with:\n        #   Code:    \"InvalidParameterValue\"\n        #   Message: \"Use of cache security groups is not permitted in\n        #             this API version for your account.\"\n        #   Type:    \"Sender\"\n        try:\n            paginator = self.conn.get_paginator(\n                'describe_cache_security_groups')\n            for page in paginator.paginate():\n                for secgroup in page['CacheSecurityGroups']:\n                    num_groups += 1\n        except ClientError as ex:\n            if ex.response['Error']['Code'] != 'InvalidParameterValue':\n                raise ex\n            logger.debug(\"caught ClientError checking ElastiCache security \"\n                         \"groups (account without EC2-Classic?)\")\n\n        self.limits['Security Groups']._add_current_usage(\n            num_groups,\n            aws_type='WS::ElastiCache::SecurityGroup'\n        )", "response": "find usage for elasticache security groups"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the current usage for each limit of this service and update limits via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._update_limits_from_api()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_limits(self):\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        limits['Groups'] = AwsLimit(\n            'Groups',\n            self,\n            300,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::Group',\n        )\n        limits['Users'] = AwsLimit(\n            'Users',\n            self,\n            5000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::User',\n        )\n        limits['Roles'] = AwsLimit(\n            'Roles',\n            self,\n            1000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::Role',\n        )\n        limits['Instance profiles'] = AwsLimit(\n            'Instance profiles',\n            self,\n            1000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::InstanceProfile',\n        )\n        limits['Server certificates'] = AwsLimit(\n            'Server certificates',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::ServerCertificate',\n        )\n        limits['Policies'] = AwsLimit(\n            'Policies',\n            self,\n            1500,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::Policy',\n        )\n        limits['Policy Versions In Use'] = AwsLimit(\n            'Policy Versions In Use',\n            self,\n            10000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::IAM::ServerCertificate',\n        )\n        self.limits = limits\n        return limits", "response": "Returns all known limits for this service as a dict of their namesto objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate self. limits with the information from the API.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Call the service's API action to retrieve limit/quota information, and\n        update AwsLimit objects in ``self.limits`` with this information.\n        \"\"\"\n        self.connect_resource()\n        summary = self.resource_conn.AccountSummary()\n        for k, v in sorted(summary.summary_map.items()):\n            if k in self.API_TO_LIMIT_NAME:\n                # this is a usage for one of our limits\n                lname = self.API_TO_LIMIT_NAME[k]\n                # if len(self.limits[lname].get_current_usage()) < 1:\n                self.limits[lname]._add_current_usage(v)\n            elif k.endswith('Quota') and k[:-5] in self.API_TO_LIMIT_NAME:\n                # quota for one of our limits\n                lname = self.API_TO_LIMIT_NAME[k[:-5]]\n                self.limits[lname]._set_api_limit(v)\n            else:\n                logger.debug(\"Ignoring IAM AccountSummary attribute: %s\", k)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse command line arguments and return a list of parsed arguments.", "response": "def parse_args(self, argv):\n        \"\"\"\n        parse arguments/options\n\n        :param argv: argument list to parse, usually ``sys.argv[1:]``\n        :type argv: list\n        :returns: parsed arguments\n        :rtype: :py:class:`argparse.Namespace`\n        \"\"\"\n        desc = 'Report on AWS service limits and usage via boto3, optionally ' \\\n               'warn about any services with usage nearing or exceeding their' \\\n               ' limits. For further help, see ' \\\n               '<http://awslimitchecker.readthedocs.org/>'\n        # ###### IMPORTANT license notice ##########\n        # Pursuant to Sections 5(b) and 13 of the GNU Affero General Public\n        # License, version 3, this notice MUST NOT be removed, and MUST be\n        # displayed to ALL USERS of this software, even if they interact with\n        # it remotely over a network.\n        #\n        # See the \"Development\" section of the awslimitchecker documentation\n        # (docs/source/development.rst or\n        # <http://awslimitchecker.readthedocs.org/en/latest/development.html> )\n        # for further information.\n        # ###### IMPORTANT license notice ##########\n        epilog = 'awslimitchecker is AGPLv3-licensed Free Software. Anyone ' \\\n                 'using this program, even remotely over a network, is ' \\\n                 'entitled to a copy of the source code. Use `--version` for ' \\\n                 'information on the source code location.'\n        p = argparse.ArgumentParser(description=desc, epilog=epilog)\n        p.add_argument('-S', '--service', action='store', nargs='*',\n                       help='perform action for only the specified service name'\n                            '; see -s|--list-services for valid names')\n        p.add_argument('--skip-service', action='append', default=[],\n                       dest='skip_service',\n                       help='avoid performing actions for the specified service'\n                            ' name; see -s|--list-services for valid names')\n        p.add_argument('--skip-check', action='append', default=[],\n                       dest='skip_check',\n                       help='avoid performing actions for the specified check'\n                            ' name')\n        p.add_argument('-s', '--list-services', action='store_true',\n                       default=False,\n                       help='print a list of all AWS service types that '\n                            'awslimitchecker knows how to check')\n        p.add_argument('-l', '--list-limits', action='store_true',\n                       default=False,\n                       help='print all AWS effective limits in \"service_name/'\n                       'limit_name\" format')\n        p.add_argument('--list-defaults', action='store_true', default=False,\n                       help='print all AWS default limits in \"service_name/'\n                       'limit_name\" format')\n        p.add_argument('-L', '--limit', action=StoreKeyValuePair,\n                       help='override a single AWS limit, specified in '\n                       '\"service_name/limit_name=value\" format; can be '\n                       'specified multiple times.')\n        p.add_argument('-u', '--show-usage', action='store_true',\n                       default=False,\n                       help='find and print the current usage of all AWS '\n                       'services with known limits')\n        p.add_argument('--iam-policy', action='store_true',\n                       default=False,\n                       help='output a JSON serialized IAM Policy '\n                       'listing the required permissions for '\n                       'awslimitchecker to run correctly.')\n        p.add_argument('-W', '--warning-threshold', action='store',\n                       type=int, default=80,\n                       help='default warning threshold (percentage of '\n                       'limit); default: 80')\n        p.add_argument('-C', '--critical-threshold', action='store',\n                       type=int, default=99,\n                       help='default critical threshold (percentage of '\n                       'limit); default: 99')\n        p.add_argument('-P', '--profile', action='store', dest='profile_name',\n                       type=str, default=None,\n                       help='Name of profile in the AWS cross-sdk credentials '\n                            'file to use credentials from; similar to the '\n                            'corresponding awscli option')\n        p.add_argument('-A', '--sts-account-id', action='store',\n                       type=str, default=None,\n                       help='for use with STS, the Account ID of the '\n                       'destination account (account to assume a role in)')\n        p.add_argument('-R', '--sts-account-role', action='store',\n                       type=str, default=None,\n                       help='for use with STS, the name of the IAM role to '\n                       'assume')\n        p.add_argument('-E', '--external-id', action='store', type=str,\n                       default=None, help='External ID to use when assuming '\n                       'a role via STS')\n        p.add_argument('-M', '--mfa-serial-number', action='store', type=str,\n                       default=None, help='MFA Serial Number to use when '\n                       'assuming a role via STS')\n        p.add_argument('-T', '--mfa-token', action='store', type=str,\n                       default=None, help='MFA Token to use when assuming '\n                       'a role via STS')\n        p.add_argument('-r', '--region', action='store',\n                       type=str, default=None,\n                       help='AWS region name to connect to; required for STS')\n        p.add_argument('--skip-ta', action='store_true', default=False,\n                       help='do not attempt to pull *any* information on limits'\n                       ' from Trusted Advisor')\n        g = p.add_mutually_exclusive_group()\n        g.add_argument('--ta-refresh-wait', dest='ta_refresh_wait',\n                       action='store_true', default=False,\n                       help='If applicable, refresh all Trusted Advisor '\n                            'limit-related checks, and wait for the refresh to'\n                            ' complete before continuing.')\n        g.add_argument('--ta-refresh-trigger', dest='ta_refresh_trigger',\n                       action='store_true', default=False,\n                       help='If applicable, trigger refreshes for all Trusted '\n                            'Advisor limit-related checks, but do not wait for '\n                            'them to finish refreshing; trigger the refresh '\n                            'and continue on (useful to ensure checks are '\n                            'refreshed before the next scheduled run).')\n        g.add_argument('--ta-refresh-older', dest='ta_refresh_older',\n                       action='store', type=int, default=None,\n                       help='If applicable, trigger refreshes for all Trusted '\n                            'Advisor limit-related checks with results more '\n                            'than this number of seconds old. Wait for the '\n                            'refresh to complete before continuing.')\n        p.add_argument('--ta-refresh-timeout', dest='ta_refresh_timeout',\n                       type=int, action='store', default=None,\n                       help='If waiting for TA checks to refresh, wait up to '\n                            'this number of seconds before continuing on '\n                            'anyway.')\n        p.add_argument('--no-color', action='store_true', default=False,\n                       help='do not colorize output')\n        p.add_argument('--no-check-version', action='store_false', default=True,\n                       dest='check_version',\n                       help='do not check latest version at startup')\n        p.add_argument('-v', '--verbose', dest='verbose', action='count',\n                       default=0,\n                       help='verbose output. specify twice for debug-level '\n                       'output.')\n        p.add_argument('-V', '--version', dest='version', action='store_true',\n                       default=False,\n                       help='print version number and exit.')\n        args = p.parse_args(argv)\n        args.ta_refresh_mode = None\n        if args.ta_refresh_wait:\n            args.ta_refresh_mode = 'wait'\n        elif args.ta_refresh_trigger:\n            args.ta_refresh_mode = 'trigger'\n        elif args.ta_refresh_older is not None:\n            args.ta_refresh_mode = args.ta_refresh_older\n        return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting the issue for the given service and limit.", "response": "def print_issue(self, service_name, limit, crits, warns):\n        \"\"\"\n        :param service_name: the name of the service\n        :type service_name: str\n        :param limit: the Limit this relates to\n        :type limit: :py:class:`~.AwsLimit`\n        :param crits: the specific usage values that crossed the critical\n          threshold\n        :type usage: :py:obj:`list` of :py:class:`~.AwsLimitUsage`\n        :param crits: the specific usage values that crossed the warning\n          threshold\n        :type usage: :py:obj:`list` of :py:class:`~.AwsLimitUsage`\n        \"\"\"\n        usage_str = ''\n        if len(crits) > 0:\n            tmp = 'CRITICAL: '\n            tmp += ', '.join([str(x) for x in sorted(crits)])\n            usage_str += self.color_output(tmp, 'red')\n        if len(warns) > 0:\n            if len(crits) > 0:\n                usage_str += ' '\n            tmp = 'WARNING: '\n            tmp += ', '.join([str(x) for x in sorted(warns)])\n            usage_str += self.color_output(tmp, 'yellow')\n        k = \"{s}/{l}\".format(\n            s=service_name,\n            l=limit.name,\n        )\n        v = \"(limit {v}) {u}\".format(\n            v=limit.get_limit(),\n            u=usage_str,\n        )\n        return (k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        for lim in self.limits.values():\n            lim._reset_usage()\n        elb_usage = self._find_usage_elbv1()\n        alb_usage = self._find_usage_elbv2()\n        logger.debug('ELBs in use: %d, ALBs in use: %d', elb_usage, alb_usage)\n        self.limits['Active load balancers']._add_current_usage(\n            (elb_usage + alb_usage),\n            aws_type='AWS::ElasticLoadBalancing::LoadBalancer',\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Determine the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_usage_elbv1(self):\n        logger.debug(\"Checking usage for ELBv1\")\n        self.connect()\n        lbs = paginate_dict(\n            self.conn.describe_load_balancers,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['LoadBalancerDescriptions'],\n            alc_marker_param='Marker'\n        )\n        for lb in lbs['LoadBalancerDescriptions']:\n            self.limits['Listeners per load balancer']._add_current_usage(\n                len(lb['ListenerDescriptions']),\n                aws_type='AWS::ElasticLoadBalancing::LoadBalancer',\n                resource_id=lb['LoadBalancerName'],\n            )\n            self.limits[\n                'Registered instances per load balancer'\n            ]._add_current_usage(\n                len(lb['Instances']),\n                aws_type='AWS::ElasticLoadBalancing::LoadBalancer',\n                resource_id=lb['LoadBalancerName']\n            )\n        logger.debug('Done with ELBv1 usage')\n        return len(lbs['LoadBalancerDescriptions'])", "response": "Find usage for ELBv1 and update limits."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind usage for ELBv2 and update the appropriate limits.", "response": "def _find_usage_elbv2(self):\n        \"\"\"\n        Find usage for ELBv2 / Application LB and update the appropriate limits.\n\n        :returns: number of Application LBs in use\n        :rtype: int\n        \"\"\"\n        logger.debug('Checking usage for ELBv2')\n        conn2 = client(\n            'elbv2',\n            config=Config(retries={'max_attempts': ELBV2_MAX_RETRY_ATTEMPTS}),\n            **self._boto3_connection_kwargs\n        )\n        logger.debug(\"Connected to %s in region %s (with max retry attempts \"\n                     \"overridden to %d)\", 'elbv2',\n                     conn2._client_config.region_name, ELBV2_MAX_RETRY_ATTEMPTS)\n        # Target groups\n        tgroups = paginate_dict(\n            conn2.describe_target_groups,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['TargetGroups'],\n            alc_marker_param='Marker'\n        )\n        self.limits['Target groups']._add_current_usage(\n            len(tgroups['TargetGroups']),\n            aws_type='AWS::ElasticLoadBalancingV2::TargetGroup'\n        )\n        # ALBs\n        lbs = paginate_dict(\n            conn2.describe_load_balancers,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['LoadBalancers'],\n            alc_marker_param='Marker'\n        )\n        logger.debug(\n            'Checking usage for each of %d ALBs', len(lbs['LoadBalancers'])\n        )\n        alb_count = 0\n        nlb_count = 0\n        for lb in lbs['LoadBalancers']:\n            if lb.get('Type') == 'network':\n                nlb_count += 1\n            else:\n                alb_count += 1\n                self._update_usage_for_alb(\n                    conn2,\n                    lb['LoadBalancerArn'],\n                    lb['LoadBalancerName']\n                )\n        self.limits['Network load balancers']._add_current_usage(\n            nlb_count,\n            aws_type='AWS::ElasticLoadBalancing::NetworkLoadBalancer'\n        )\n        logger.debug('Done with ELBv2 usage')\n        return alb_count"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate usage for a single ALB.", "response": "def _update_usage_for_alb(self, conn, alb_arn, alb_name):\n        \"\"\"\n        Update usage for a single ALB.\n\n        :param conn: elbv2 API connection\n        :type conn: :py:class:`ElasticLoadBalancing.Client`\n        :param alb_arn: Load Balancer ARN\n        :type alb_arn: str\n        :param alb_name: Load Balancer Name\n        :type alb_name: str\n        \"\"\"\n        logger.debug('Updating usage for ALB %s', alb_arn)\n        listeners = paginate_dict(\n            conn.describe_listeners,\n            LoadBalancerArn=alb_arn,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['Listeners'],\n            alc_marker_param='Marker'\n        )['Listeners']\n        num_rules = 0\n        num_certs = 0\n        for l in listeners:\n            certs = [\n                x for x in l.get('Certificates', [])\n                if x.get('IsDefault', False) is False\n            ]\n            num_certs += len(certs)\n            rules = paginate_dict(\n                conn.describe_rules,\n                ListenerArn=l['ListenerArn'],\n                alc_marker_path=['NextMarker'],\n                alc_data_path=['Rules'],\n                alc_marker_param='Marker'\n            )['Rules']\n            num_rules += len(rules)\n        self.limits[\n            'Listeners per application load balancer']._add_current_usage(\n            len(listeners),\n            aws_type='AWS::ElasticLoadBalancingV2::LoadBalancer',\n            resource_id=alb_name,\n        )\n        self.limits['Rules per application load balancer']._add_current_usage(\n            num_rules,\n            aws_type='AWS::ElasticLoadBalancingV2::LoadBalancer',\n            resource_id=alb_name,\n        )\n        self.limits[\n            'Certificates per application load balancer'\n        ]._add_current_usage(\n            num_certs,\n            aws_type='AWS::ElasticLoadBalancingV2::LoadBalancer',\n            resource_id=alb_name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_usage_for_nlb(self, conn, nlb_arn, nlb_name):\n        logger.debug('Updating usage for NLB %s', nlb_arn)\n        listeners = paginate_dict(\n            conn.describe_listeners,\n            LoadBalancerArn=nlb_arn,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['Listeners'],\n            alc_marker_param='Marker'\n        )['Listeners']\n        self.limits[\n            'Listeners per network load balancer']._add_current_usage(\n            len(listeners),\n            aws_type='AWS::ElasticLoadBalancingV2::NetworkLoadBalancer',\n            resource_id=nlb_name\n        )", "response": "Update usage for a single NLB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_limits_from_api(self):\n        self.connect()\n        logger.debug(\"Querying ELB DescribeAccountLimits for limits\")\n        attribs = self.conn.describe_account_limits()\n        name_to_limits = {\n            'classic-load-balancers': 'Active load balancers',\n            'classic-listeners': 'Listeners per load balancer',\n            'classic-registered-instances':\n                'Registered instances per load balancer'\n        }\n        for attrib in attribs['Limits']:\n            if int(attrib.get('Max', 0)) == 0:\n                continue\n            name = attrib.get('Name', 'unknown')\n            if name not in name_to_limits:\n                continue\n            self.limits[name_to_limits[name]]._set_api_limit(int(attrib['Max']))\n        # connect to ELBv2 API as well\n        self.conn2 = client('elbv2', **self._boto3_connection_kwargs)\n        logger.debug(\"Connected to %s in region %s\",\n                     'elbv2', self.conn2._client_config.region_name)\n        logger.debug(\"Querying ELBv2 (ALB) DescribeAccountLimits for limits\")\n        attribs = self.conn2.describe_account_limits()\n        name_to_limits = {\n            'target-groups': 'Target groups',\n            'listeners-per-application-load-balancer':\n                'Listeners per application load balancer',\n            'rules-per-application-load-balancer':\n                'Rules per application load balancer',\n            'network-load-balancers': 'Network load balancers',\n            'listeners-per-network-load-balancer':\n                'Listeners per network load balancer'\n        }\n        for attrib in attribs['Limits']:\n            if int(attrib.get('Max', 0)) == 0:\n                continue\n            name = attrib.get('Name', 'unknown')\n            if name not in name_to_limits:\n                continue\n            self.limits[name_to_limits[name]]._set_api_limit(int(attrib['Max']))\n        logger.debug(\"Done setting limits from API\")", "response": "Query ELB s DescribeAccountLimits API action and update limits with the quotas returned. Updates self. limits."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a dict of string keys and string values and returns a string with the keys and values formatted as two columns separated by at least spaces number of columns by separator characters.", "response": "def dict2cols(d, spaces=2, separator=' '):\n    \"\"\"\n    Take a dict of string keys and string values, and return a string with\n    them formatted as two columns separated by at least ``spaces`` number of\n    ``separator`` characters.\n\n    :param d: dict of string keys, string values\n    :type d: dict\n    :param spaces: number of spaces to separate columns by\n    :type spaces: int\n    :param separator: character to fill in between columns\n    :type separator: str\n    \"\"\"\n    if len(d) == 0:\n        return ''\n    s = ''\n    maxlen = max([len(k) for k in d.keys()])\n    fmt_str = '{k:' + separator + '<' + str(maxlen + spaces) + '}{v}\\n'\n    for k in sorted(d.keys()):\n        s += fmt_str.format(\n            k=k,\n            v=d[k],\n        )\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef paginate_dict(function_ref, *argv, **kwargs):\n    if 'alc_marker_path' not in kwargs:\n        raise Exception(\"alc_marker_path must be specified for queries \"\n                        \"that return a dict.\")\n    if 'alc_data_path' not in kwargs:\n        raise Exception(\"alc_data_path must be specified for queries \"\n                        \"that return a dict.\")\n    if 'alc_marker_param' not in kwargs:\n        raise Exception(\"alc_marker_param must be specified for queries \"\n                        \"that return a dict.\")\n\n    marker_path = kwargs['alc_marker_path']\n    data_path = kwargs['alc_data_path']\n    marker_param = kwargs['alc_marker_param']\n\n    # strip off \"^alc_\" args\n    pass_kwargs = {}\n    for k, v in kwargs.items():\n        if not k.startswith('alc_'):\n            pass_kwargs[k] = v\n\n    # first function call\n    result = function_ref(*argv, **pass_kwargs)\n\n    # check for marker, return if not present\n    marker = _get_dict_value_by_path(result, marker_path)\n    if marker is None:\n        return result\n    logger.debug(\"Found marker (%s) in result; iterating for more results\",\n                 marker_path)\n    # iterate results\n    results = []\n    results.extend(_get_dict_value_by_path(result, data_path))\n    while marker is not None:\n        logger.debug(\"Querying %s with %s=%s\", function_ref, marker_param,\n                     marker)\n        pass_kwargs[marker_param] = marker\n        result = function_ref(*argv, **pass_kwargs)\n        data = _get_dict_value_by_path(result, data_path)\n        results.extend(data)\n        marker = _get_dict_value_by_path(result, marker_path)\n    # drop the full results into the last result response\n    res = _set_dict_value_by_path(result, results, data_path)\n    return res", "response": "Paginate through a function that returns a dict result and return the combined result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a dict d and a list specifying the hierarchical path to a key return the value at that path or None if it does not exist.", "response": "def _get_dict_value_by_path(d, path):\n    \"\"\"\n    Given a dict (``d``) and a list specifying the hierarchical path to a key\n    in that dict (``path``), return the value at that path or None if it does\n    not exist.\n\n    :param d: the dict to search in\n    :type d: dict\n    :param path: the path to the key in the dict\n    :type path: list\n    \"\"\"\n    tmp_path = deepcopy(path)\n    try:\n        while len(tmp_path) > 0:\n            k = tmp_path.pop(0)\n            d = d[k]\n        return d\n    except:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a dict d and a value val and a list specifying the hierarchical path to a key in that dict d and a value val and a list specifying the hierarchical path to a key in that dict d.", "response": "def _set_dict_value_by_path(d, val, path):\n    \"\"\"\n    Given a dict (``d``), a value (``val``),  and a list specifying the\n    hierarchical path to a key in that dict (``path``), set the value in ``d``\n    at ``path`` to ``val``.\n\n    :param d: the dict to search in\n    :type d: dict\n    :param path: the path to the key in the dict\n    :type path: list\n    :raises: TypeError if the path is too short\n    :returns: the modified dict\n    \"\"\"\n    tmp_path = deepcopy(path)\n    tmp_d = deepcopy(d)\n    result = tmp_d\n    while len(tmp_path) > 0:\n        if len(tmp_path) == 1:\n            result[tmp_path[0]] = val\n            break\n        k = tmp_path.pop(0)\n        result = result[k]\n    return tmp_d"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _boto_conn_kwargs(self):\n        kwargs = {'region_name': self.region}\n        if self.account_id is not None:\n            logger.debug(\"Connecting for account %s role '%s' with STS \"\n                         \"(region: %s)\", self.account_id, self.account_role,\n                         self.region)\n            credentials = self._get_sts_token()\n            kwargs['aws_access_key_id'] = credentials.access_key\n            kwargs['aws_secret_access_key'] = credentials.secret_key\n            kwargs['aws_session_token'] = credentials.session_token\n        elif self.profile_name is not None:\n            # use boto3.Session to get credentials from the named profile\n            logger.debug(\"Using credentials profile: %s\", self.profile_name)\n            session = boto3.Session(profile_name=self.profile_name)\n            credentials = session._session.get_credentials()\n            kwargs['aws_access_key_id'] = credentials.access_key\n            kwargs['aws_secret_access_key'] = credentials.secret_key\n            kwargs['aws_session_token'] = credentials.token\n        else:\n            logger.debug(\"Connecting to region %s\", self.region)\n        return kwargs", "response": "Generate keyword arguments for boto3 connection functions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_services(self, services_to_remove=[]):\n        for sname in services_to_remove:\n            logger.warning('Skipping service: %s', sname)\n            self.services.pop(sname, None)", "response": "Removes all services specified in services_to_remove from self. services."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to STS and return the credentials.", "response": "def _get_sts_token(self):\n        \"\"\"\n        Assume a role via STS and return the credentials.\n\n        First connect to STS via :py:func:`boto3.client`, then\n        assume a role using `boto3.STS.Client.assume_role <https://boto3.readthe\n        docs.org/en/latest/reference/services/sts.html#STS.Client.assume_role>`_\n        using ``self.account_id`` and ``self.account_role`` (and optionally\n        ``self.external_id``, ``self.mfa_serial_number``, ``self.mfa_token``).\n        Return the resulting :py:class:`~.ConnectableCredentials`\n        object.\n\n        :returns: STS assumed role credentials\n        :rtype: :py:class:`~.ConnectableCredentials`\n        \"\"\"\n        logger.debug(\"Connecting to STS in region %s\", self.region)\n        sts = boto3.client('sts', region_name=self.region)\n        arn = \"arn:aws:iam::%s:role/%s\" % (self.account_id, self.account_role)\n        logger.debug(\"STS assume role for %s\", arn)\n        assume_kwargs = {\n            'RoleArn': arn,\n            'RoleSessionName': 'awslimitchecker'\n        }\n        if self.external_id is not None:\n            assume_kwargs['ExternalId'] = self.external_id\n        if self.mfa_serial_number is not None:\n            assume_kwargs['SerialNumber'] = self.mfa_serial_number\n        if self.mfa_token is not None:\n            assume_kwargs['TokenCode'] = self.mfa_token\n        role = sts.assume_role(**assume_kwargs)\n\n        creds = ConnectableCredentials(role)\n        creds.account_id = self.account_id\n\n        logger.debug(\"Got STS credentials for role; access_key_id=%s \"\n                     \"(account_id=%s)\", creds.access_key, creds.account_id)\n        return creds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds usage for each limit in the specified service.", "response": "def find_usage(self, service=None, use_ta=True):\n        \"\"\"\n        For each limit in the specified service (or all services if\n        ``service`` is ``None``), query the AWS API via ``boto3``\n        and find the current usage amounts for that limit.\n\n        This method updates the ``current_usage`` attribute of the\n        :py:class:`~.AwsLimit` objects for each service, which can\n        then be queried using :py:meth:`~.get_limits`.\n\n        :param service: list of :py:class:`~._AwsService` name(s), or ``None``\n          to check all services.\n        :type service: :py:obj:`None`, or :py:obj:`list` service names to get\n        :param use_ta: check Trusted Advisor for information on limits\n        :type use_ta: bool\n        \"\"\"\n        to_get = self.services\n        if service is not None:\n            to_get = dict((each, self.services[each]) for each in service)\n        if use_ta:\n            self.ta.update_limits()\n        for cls in to_get.values():\n            if hasattr(cls, '_update_limits_from_api'):\n                cls._update_limits_from_api()\n            logger.debug(\"Finding usage for service: %s\", cls.service_name)\n            cls.find_usage()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_limit_overrides(self, override_dict, override_ta=True):\n        for svc_name in override_dict:\n            for lim_name in override_dict[svc_name]:\n                self.services[svc_name].set_limit_override(\n                    lim_name,\n                    override_dict[svc_name][lim_name],\n                    override_ta=override_ta\n                )", "response": "Set manual overrides on the service limits."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_limit_override(self, service_name, limit_name,\n                           value, override_ta=True):\n        \"\"\"\n        Set a manual override on an AWS service limits, i.e. if you\n        had limits increased by AWS support.\n\n        This method calls :py:meth:`._AwsService.set_limit_override`\n        on the corresponding _AwsService instance.\n\n        Explicitly set limit overrides using this method will take\n        precedence over default limits. They will also take precedence over\n        limit information obtained via Trusted Advisor, unless ``override_ta``\n        is set to ``False``.\n\n        :param service_name: the name of the service to override limit for\n        :type service_name: str\n        :param limit_name: the name of the limit to override:\n        :type limit_name: str\n        :param value: the new (overridden) limit value)\n        :type value: int\n        :param override_ta: whether or not to use this value even if Trusted\n          Advisor supplies limit information\n        :type override_ta: bool\n        :raises: :py:exc:`ValueError` if limit_name is not known to the\n          service instance\n        \"\"\"\n        self.services[service_name].set_limit_override(\n            limit_name,\n            value,\n            override_ta=override_ta\n        )", "response": "Set a manual override on an AWS service limits."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets manual overrides on the threshold for all services in the specified hierarchy.", "response": "def set_threshold_overrides(self, override_dict):\n        \"\"\"\n        Set manual overrides on the threshold (used for determining\n        warning/critical status) a dict of limits. See\n        :py:class:`~.AwsLimitChecker` for information on Warning and\n        Critical thresholds.\n\n        Dict is composed of service name keys (string) to dict of\n        limit names (string), to dict of threshold specifications.\n        Each threhold specification dict can contain keys 'warning'\n        or 'critical', each having a value of a dict containing\n        keys 'percent' or 'count', to an integer value.\n\n        Example:\n        ::\n\n            {\n                'EC2': {\n                    'SomeLimit': {\n                        'warning': {\n                            'percent': 80,\n                            'count': 8,\n                        },\n                        'critical': {\n                            'percent': 90,\n                            'count': 9,\n                        }\n                    }\n                }\n            }\n\n        See :py:meth:`.AwsLimit.set_threshold_override`.\n\n        :param override_dict: nested dict of threshold overrides\n        :type override_dict: dict\n        \"\"\"\n        for svc_name in sorted(override_dict):\n            for lim_name in sorted(override_dict[svc_name]):\n                d = override_dict[svc_name][lim_name]\n                kwargs = {}\n                if 'warning' in d:\n                    if 'percent' in d['warning']:\n                        kwargs['warn_percent'] = d['warning']['percent']\n                    if 'count' in d['warning']:\n                        kwargs['warn_count'] = d['warning']['count']\n                if 'critical' in d:\n                    if 'percent' in d['critical']:\n                        kwargs['crit_percent'] = d['critical']['percent']\n                    if 'count' in d['critical']:\n                        kwargs['crit_count'] = d['critical']['count']\n                self.services[svc_name].set_threshold_override(\n                    lim_name,\n                    **kwargs\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a manual override on the warning and critical thresholds for a specific service.", "response": "def set_threshold_override(self, service_name, limit_name,\n                               warn_percent=None, warn_count=None,\n                               crit_percent=None, crit_count=None):\n        \"\"\"\n        Set a manual override on the threshold (used for determining\n        warning/critical status) for a specific limit. See\n        :py:class:`~.AwsLimitChecker` for information on Warning and\n        Critical thresholds.\n\n        See :py:meth:`.AwsLimit.set_threshold_override`.\n\n        :param service_name: the name of the service to override limit for\n        :type service_name: str\n        :param limit_name: the name of the limit to override:\n        :type limit_name: str\n        :param warn_percent: new warning threshold, percentage used\n        :type warn_percent: int\n        :param warn_count: new warning threshold, actual count/number\n        :type warn_count: int\n        :param crit_percent: new critical threshold, percentage used\n        :type crit_percent: int\n        :param crit_count: new critical threshold, actual count/number\n        :type crit_count: int\n        \"\"\"\n        self.services[service_name].set_threshold_override(\n            limit_name,\n            warn_percent=warn_percent,\n            warn_count=warn_count,\n            crit_percent=crit_percent,\n            crit_count=crit_count\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_required_iam_policy(self):\n        required_actions = [\n            'support:*',\n            'trustedadvisor:Describe*',\n            'trustedadvisor:RefreshCheck'\n        ]\n        for cls in self.services.values():\n            required_actions.extend(cls.required_iam_permissions())\n        policy = {\n            'Version': '2012-10-17',\n            'Statement': [{\n                'Effect': 'Allow',\n                'Resource': '*',\n                'Action': sorted(list(set(required_actions))),\n            }],\n        }\n        return policy", "response": "Returns an IAM policy granting all of the permissions needed for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nqueries Route53 s GetHostedZoneLimit API action and update self. limits with the quotas returned.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Query Route53's GetHostedZoneLimit API action, and update limits\n        with the quotas returned. Updates ``self.limits``.\n        \"\"\"\n        logger.info(\"Querying Route53 GetHostedZoneLimits for limits\")\n        self.connect()\n        self._find_limit_hosted_zone()\n        logger.debug('Done setting limits from API.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_limits(self):\n        if not self.limits:\n            self.limits = {}\n            for item in [self.MAX_RRSETS_BY_ZONE,\n                         self.MAX_VPCS_ASSOCIATED_BY_ZONE]:\n                self.limits[item[\"name\"]] = AwsLimit(\n                    item[\"name\"],\n                    self,\n                    item[\"default_limit\"],\n                    self.warning_threshold,\n                    self.critical_threshold,\n                    limit_type='AWS::Route53::HostedZone',\n                    limit_subtype=item[\"name\"]\n                )\n\n        return self.limits", "response": "Returns all known limits for this service as a dict of names to AwsLimit objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all available hosted zones in the system", "response": "def _get_hosted_zones(self):\n        \"\"\"\n        Return all available hosted zones\n\n        :returns: dict of hosted zones\n        :rtype: dict\n        \"\"\"\n        results = paginate_dict(\n            self.conn.list_hosted_zones,\n            alc_marker_path=['NextMarker'],\n            alc_data_path=['HostedZones'],\n            alc_marker_param='Marker'\n        )\n\n        return results[\"HostedZones\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_hosted_zone_limit(self, limit_type, hosted_zone_id):\n\n        result = self.conn.get_hosted_zone_limit(\n            Type=limit_type,\n            HostedZoneId=hosted_zone_id\n        )\n\n        return result", "response": "Returns a hosted zone limit [ recordsets|vpc_associations ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the maximum recordsets and vpc associations and the current values for the hosted zone.", "response": "def _find_limit_hosted_zone(self):\n        \"\"\"\n        Calculate the max recordsets and vpc associations and the current values\n        per hosted zone\n        \"\"\"\n        for limit_type in [self.MAX_RRSETS_BY_ZONE,\n                           self.MAX_VPCS_ASSOCIATED_BY_ZONE]:\n            self.limits[limit_type[\"name\"]]._reset_usage()\n\n        for hosted_zone in self._get_hosted_zones():\n            for limit_type in [self.MAX_RRSETS_BY_ZONE,\n                               self.MAX_VPCS_ASSOCIATED_BY_ZONE]:\n\n                if limit_type == self.MAX_VPCS_ASSOCIATED_BY_ZONE and \\\n                        not hosted_zone[\"Config\"][\"PrivateZone\"]:\n                    continue\n\n                limit = self._get_hosted_zone_limit(limit_type[\"type\"],\n                                                    hosted_zone['Id'])\n\n                self.limits[limit_type[\"name\"]]._add_current_usage(\n                    int(limit[\"Count\"]),\n                    maximum=int(limit[\"Limit\"][\"Value\"]),\n                    aws_type='AWS::Route53::HostedZone',\n                    resource_id=hosted_zone[\"Name\"]\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_iam_policy(self):\n        checker = AwsLimitChecker()\n        policy = checker.get_required_iam_policy()\n        return json.dumps(policy, sort_keys=True, indent=2)", "response": "Return the current IAM policy as a json - serialized string"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect_resource()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        count = 0\n        for bkt in self.resource_conn.buckets.all():\n            count += 1\n        self.limits['Buckets']._add_current_usage(\n            count, aws_type='AWS::S3::Bucket'\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Determine the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck GitHub for a release with the specified tag name.", "response": "def get_tag(self, version):\n        \"\"\"\n        Check GitHub for a release with the specified tag name. If present,\n        return a 2-tuple of the release name and release HTML URL. Otherwise,\n        return a 2-tuple of None, None (tag not present).\n\n        :param version: the tag name to check for\n        :type version: str\n        :return: 2-tuple of release name, release HTML URL for the specified\n          release tag, or 2-tuple of None, None if no such release\n        :rtype: tuple\n        \"\"\"\n        for rel in self._repo.releases():\n            if rel.tag_name == version:\n                return rel.name, rel.html_url\n        return None, None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        for lim in self.limits.values():\n            lim._reset_usage()\n        try:\n            self.connect()\n            resp = self.conn.get_send_quota()\n        except EndpointConnectionError as ex:\n            logger.warning('Skipping SES: %s', str(ex))\n            return\n        except ClientError as ex:\n            if ex.response['Error']['Code'] in ['AccessDenied', '503']:\n                logger.warning('Skipping SES: %s', ex)\n                return\n            raise\n        self.limits['Daily sending quota']._add_current_usage(\n            resp['SentLast24Hours']\n        )\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Determine the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_limits_from_api(self):\n        try:\n            self.connect()\n            resp = self.conn.get_send_quota()\n        except EndpointConnectionError as ex:\n            logger.warning('Skipping SES: %s', str(ex))\n            return\n        except ClientError as ex:\n            if ex.response['Error']['Code'] in ['AccessDenied', '503']:\n                logger.warning('Skipping SES: %s', ex)\n                return\n            raise\n        self.limits['Daily sending quota']._set_api_limit(resp['Max24HourSend'])", "response": "Get limit and quota information from API and update self. limits with this information."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self):\n        if self.conn is not None:\n            return\n        kwargs = self._boto3_connection_kwargs\n        self.conn = boto3.client(self.api_name, **kwargs)\n        logger.info(\"Connected to %s in region %s\",\n                    self.api_name, self.conn._client_config.region_name)", "response": "Connect to an AWS API via boto3 low - level client and set self. conn to the object holding the unique identifier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting to an AWS API and set the internal state of self. resource_conn to the resource object.", "response": "def connect_resource(self):\n        \"\"\"\n        Connect to an AWS API via boto3 high-level resource connection and set\n        ``self.resource_conn`` to the `boto3.resource <https://boto3.readthed\n        ocs.org/en/latest/reference/core/boto3.html#boto3.resource>`_ object\n        (a ``boto3.resources.factory.*.ServiceResource`` instance).\n        If ``self.resource_conn`` is not None,\n        do nothing. This connects to the API name given by ``self.api_name``.\n\n        :returns: None\n        \"\"\"\n        if self.resource_conn is not None:\n            return\n        kwargs = self._boto3_connection_kwargs\n        self.resource_conn = boto3.resource(self.api_name, **kwargs)\n        logger.info(\"Connected to %s (resource) in region %s\", self.api_name,\n                    self.resource_conn.meta.client._client_config.region_name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect_resource()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_dynamodb()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find the current usage for each limit in this service and update the corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_usage_dynamodb(self):\n        table_count = 0\n        region_read_capacity = 0\n        region_write_capacity = 0\n\n        logger.debug(\"Getting usage for DynamoDB tables\")\n        for table in self.resource_conn.tables.all():\n            table_count += 1\n            gsi_write = 0\n            gsi_read = 0\n            gsi_count = 0\n            if table.global_secondary_indexes is not None:\n                for gsi in table.global_secondary_indexes:\n                    gsi_count += 1\n                    gsi_read += gsi['ProvisionedThroughput'][\n                        'ReadCapacityUnits']\n                    gsi_write += gsi['ProvisionedThroughput'][\n                        'WriteCapacityUnits']\n            table_write_capacity = table.provisioned_throughput[\n                                       'WriteCapacityUnits'] + gsi_write\n            table_read_capacity = table.provisioned_throughput[\n                                      'ReadCapacityUnits'] + gsi_read\n            region_write_capacity += table_write_capacity\n            region_read_capacity += table_read_capacity\n\n            self.limits['Global Secondary Indexes']._add_current_usage(\n                gsi_count,\n                resource_id=table.name,\n                aws_type='AWS::DynamoDB::Table'\n            )\n\n            self.limits['Local Secondary Indexes']._add_current_usage(\n                len(table.local_secondary_indexes)\n                if table.local_secondary_indexes is not None else 0,\n                resource_id=table.name,\n                aws_type='AWS::DynamoDB::Table'\n            )\n\n            self.limits['Table Max Write Capacity Units']._add_current_usage(\n                table_write_capacity,\n                resource_id=table.name,\n                aws_type='AWS::DynamoDB::Table'\n            )\n\n            self.limits['Table Max Read Capacity Units']._add_current_usage(\n                table_read_capacity,\n                resource_id=table.name,\n                aws_type='AWS::DynamoDB::Table'\n            )\n\n        self.limits['Tables Per Region']._add_current_usage(\n            table_count,\n            aws_type='AWS::DynamoDB::Table'\n        )\n\n        self.limits['Account Max Write Capacity Units']._add_current_usage(\n            region_write_capacity,\n            aws_type='AWS::DynamoDB::Table'\n        )\n\n        self.limits['Account Max Read Capacity Units']._add_current_usage(\n            region_read_capacity,\n            aws_type='AWS::DynamoDB::Table'\n        )", "response": "finds usage for DynamoDB tables and adds them to self. limits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all known limits for this service as a dict of their namesto objects.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        self.connect()\n        region_name = self.conn._client_config.region_name\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n\n        limits['Tables Per Region'] = AwsLimit(\n            'Tables Per Region',\n            self,\n            256,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        limits['Account Max Write Capacity Units'] = AwsLimit(\n            'Account Max Write Capacity Units',\n            self,\n            80000 if region_name == 'us-east-1' else 20000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table', )\n\n        limits['Table Max Write Capacity Units'] = AwsLimit(\n            'Table Max Write Capacity Units',\n            self,\n            40000 if region_name == 'us-east-1' else 10000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        limits['Account Max Read Capacity Units'] = AwsLimit(\n            'Account Max Read Capacity Units',\n            self,\n            80000 if region_name == 'us-east-1' else 20000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        limits['Table Max Read Capacity Units'] = AwsLimit(\n            'Table Max Read Capacity Units',\n            self,\n            40000 if region_name == 'us-east-1' else 10000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        limits['Global Secondary Indexes'] = AwsLimit(\n            'Global Secondary Indexes',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        limits['Local Secondary Indexes'] = AwsLimit(\n            'Local Secondary Indexes',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::DynamoDB::Table',\n        )\n\n        self.limits = limits\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nquery DynamoDB s DescribeLimits API action and update limits with the quotas returned. Updates self. limits.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Query DynamoDB's DescribeLimits API action, and update limits\n        with the quotas returned. Updates ``self.limits``.\n        \"\"\"\n        self.connect()\n        logger.info(\"Querying DynamoDB DescribeLimits for limits\")\n        # no need to paginate\n        lims = self.conn.describe_limits()\n        self.limits['Account Max Read Capacity Units']._set_api_limit(\n            lims['AccountMaxReadCapacityUnits']\n        )\n        self.limits['Account Max Write Capacity Units']._set_api_limit(\n            lims['AccountMaxWriteCapacityUnits']\n        )\n        self.limits['Table Max Read Capacity Units']._set_api_limit(\n            lims['TableMaxReadCapacityUnits']\n        )\n        self.limits['Table Max Write Capacity Units']._set_api_limit(\n            lims['TableMaxWriteCapacityUnits']\n        )\n        logger.debug(\"Done setting limits from API\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_version_info():\n    if os.environ.get('VERSIONCHECK_DEBUG', '') != 'true':\n        for lname in ['versionfinder', 'pip', 'git']:\n            l = logging.getLogger(lname)\n            l.setLevel(logging.CRITICAL)\n            l.propagate = True\n    try:\n        vinfo = find_version('awslimitchecker')\n        dirty = ''\n        if vinfo.git_is_dirty:\n            dirty = '*'\n        tag = vinfo.git_tag\n        if tag is not None:\n            tag += dirty\n        commit = vinfo.git_commit\n        if commit is not None:\n            if len(commit) > 7:\n                commit = commit[:8]\n            commit += dirty\n        return AWSLimitCheckerVersion(\n            vinfo.version,\n            vinfo.url,\n            tag=tag,\n            commit=commit\n        )\n    except Exception:\n        logger.exception(\"Error checking installed version; this installation \"\n                         \"may not be in compliance with the AGPLv3 license:\")\n    # fall back to returning just the hard-coded release information\n    return AWSLimitCheckerVersion(_VERSION, _PROJECT_URL)", "response": "Returns the currently - installed awslimitchecker version and a best -effort version for the specified version."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef version_str(self):\n        vs = str(self.release)\n        if self.tag is not None:\n            vs += '@{t}'.format(t=self.tag)\n        elif self.commit is not None:\n            vs += '@{c}'.format(c=self.commit)\n        return vs", "response": "Returns the version string for the currently - running awslimitchecker ; includes git branch and tag information."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload configuration from config directory.", "response": "def _load_config(self):\n        \"\"\"load configuration from config/\"\"\"\n        logger.debug(\n            'Listing per-account config subdirectories in %s', self._conf_dir\n        )\n        for acct_id in os.listdir(self._conf_dir):\n            path = os.path.join(self._conf_dir, acct_id)\n            # skip if not a directory\n            if not os.path.isdir(path):\n                continue\n            # skip if doesn't match ^[0-9]+$\n            if not self.acct_id_re.match(acct_id):\n                continue\n            # call _load_account specifying the directory name (acct ID) & path\n            self._load_account(acct_id, path)\n        # Once all configuration is loaded, build a dict of Account Name to\n        # Account ID (``self._acct_name_to_id``) for faster access to configs\n        # by name.\n        for acct_id, data in self._config.items():\n            if data['name'] is None:\n                continue\n            self._acct_name_to_id[data['name']] = acct_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _load_account(self, acct_id, acct_dir_path):\n        # setup a default config dict for the account\n        self._config[acct_id] = {\n            'name': None,\n            'role_name': DEFAULT_ROLE_NAME,\n            'regions': {}\n        }\n        # read the account config file\n        # @TODO unhandled exception if file doesn't exist or isn't JSON\n        with open(os.path.join(acct_dir_path, 'config.json'), 'r') as fh:\n            acct_conf = json.loads(fh.read())\n        # overwrite defaults with what we read from the account JSON\n        self._config[acct_id].update(acct_conf)\n        # iterate over contents of the per-account directory\n        for region_name in os.listdir(acct_dir_path):\n            path = os.path.join(acct_dir_path, region_name)\n            # skip anything that isn't a directory\n            if not os.path.isdir(path):\n                continue\n            # load the per-region configs for the account...\n            # @TODO - should check that it's a valid region name\n            self._load_region(acct_id, region_name, path)", "response": "load configuration from one per - account subdirectory"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading config from a single per - region subdirectory of an account", "response": "def _load_region(self, acct_id, region_name, path):\n        \"\"\"load config from a single per-region subdirectory of an account\"\"\"\n        lim_path = os.path.join(path, 'limit_overrides.json')\n        thresh_path = os.path.join(path, 'threshold_overrides.json')\n        res = {'limit_overrides': {}, 'threshold_overrides': {}}\n        if os.path.exists(lim_path):\n            with open(lim_path, 'r') as fh:\n                res['limit_overrides'] = json.loads(fh.read())\n        if os.path.exists(thresh_path):\n            with open(thresh_path, 'r') as fh:\n                res['threshold_overrides'] = json.loads(fh.read())\n        self._config[acct_id]['regions'][region_name] = res"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of account configuration for the specified ID or name.", "response": "def get_account_config(self, id_or_name):\n        \"\"\"\n        Return a dictionary of account configuration for the account with the\n        specified ID or name.\n\n        :param id_or_name: ID or name of account\n        :type id_or_name: str\n        :return: configuration for specified account\n        :rtype: dict\n        \"\"\"\n        if id_or_name in self._config:\n            return self._config[id_or_name]\n        if id_or_name in self._acct_name_to_id:\n            return self._config[self._acct_name_to_id[id_or_name]]\n        raise RuntimeError('ERROR: Unknown account ID or name')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_limits(self, acct_id, region_name, role_name=None,\n                     limit_overrides={}, threshold_overrides={}):\n        \"\"\"\n        Run the actual usage and limit check, with overrides, against a specific\n        account in a specific region, optionally assuming a role in the account\n        and optionally setting limit and/or threshold overrides.\n\n        Return a 2-tuple of lists, warning strings and critical strings.\n\n        see: http://awslimitchecker.readthedocs.org/en/latest/python_usage.html\n\n        :returns: 2-tuple of lists of strings, warnings and criticals\n        \"\"\"\n        # instantiate the class\n        if role_name is not None:\n            checker = AwsLimitChecker(\n                account_id=acct_id, region=region_name, account_role=role_name\n            )\n        else:\n            checker = AwsLimitChecker(region=region_name)\n        # set your overrides\n        if len(threshold_overrides) > 0:\n            checker.set_threshold_overrides(threshold_overrides)\n        if len(limit_overrides) > 0:\n            checker.set_limit_overrides(limit_overrides)\n\n        # check usage against thresholds\n        checker.check_thresholds()\n\n        # save state for exit code and summary\n        warnings = []\n        criticals = []\n\n        # iterate the results\n        for service, svc_limits in sorted(checker.get_limits().items()):\n            for limit_name, limit in sorted(svc_limits.items()):\n                # check warnings and criticals for each Limit\n                for warn in limit.get_warnings():\n                    warnings.append(colored(\"{service} '{limit_name}' usage \"\n                                            \"({u}) exceeds warning threshold \"\n                                            \"(limit={l})\".format(\n                                                service=service,\n                                                limit_name=limit_name,\n                                                u=str(warn),\n                                                l=limit.get_limit(),\n                                            ), 'yellow'))\n                for crit in limit.get_criticals():\n                    criticals.append(colored(\"{service} '{limit_name}' usage \"\n                                             \"({u}) exceeds critical threshold\"\n                                             \" (limit={l})\".format(\n                                                 service=service,\n                                                 limit_name=limit_name,\n                                                 u=str(crit),\n                                                 l=limit.get_limit(),\n                                             ), 'red'))\n        return warnings, criticals", "response": "Run the actual usage and limit check against a specific region and optionally setting limit and threshold overrides."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding usage for all EBS volumes and update limits", "response": "def _find_usage_ebs(self):\n        \"\"\"calculate usage for all EBS limits and update Limits\"\"\"\n        vols = 0\n        piops = 0\n        piops_gb = 0\n        gp_gb = 0\n        mag_gb = 0\n        st_gb = 0\n        sc_gb = 0\n        logger.debug(\"Getting usage for EBS volumes\")\n        results = paginate_dict(\n            self.conn.describe_volumes,\n            alc_marker_path=['NextToken'],\n            alc_data_path=['Volumes'],\n            alc_marker_param='NextToken'\n        )\n        for vol in results['Volumes']:\n            vols += 1\n            if vol['VolumeType'] == 'io1':\n                piops_gb += vol['Size']\n                piops += vol['Iops']\n            elif vol['VolumeType'] == 'gp2':\n                gp_gb += vol['Size']\n            elif vol['VolumeType'] == 'standard':\n                mag_gb += vol['Size']\n            elif vol['VolumeType'] == 'st1':\n                st_gb += vol['Size']\n            elif vol['VolumeType'] == 'sc1':\n                sc_gb += vol['Size']\n            else:\n                logger.error(\n                    \"ERROR - unknown volume type '%s' for volume %s;\"\n                    \" not counting\",\n                    vol['VolumeType'],\n                    vol['VolumeId'])\n        self.limits['Provisioned IOPS']._add_current_usage(\n            piops,\n            aws_type='AWS::EC2::Volume'\n        )\n        self.limits['Provisioned IOPS (SSD) storage '\n                    '(GiB)']._add_current_usage(\n                        piops_gb,\n                        aws_type='AWS::EC2::Volume'\n                    )\n        self.limits['General Purpose (SSD) volume storage '\n                    '(GiB)']._add_current_usage(\n                        gp_gb,\n                        aws_type='AWS::EC2::Volume'\n                    )\n        self.limits['Magnetic volume storage '\n                    '(GiB)']._add_current_usage(\n                        mag_gb,\n                        aws_type='AWS::EC2::Volume'\n                    )\n        self.limits['Throughput Optimized (HDD) volume storage '\n                    '(GiB)']._add_current_usage(\n                        st_gb,\n                        aws_type='AWS::EC2::Volume'\n                    )\n        self.limits['Cold (HDD) volume storage '\n                    '(GiB)']._add_current_usage(\n                        sc_gb,\n                        aws_type='AWS::EC2::Volume'\n                    )\n\n        self.limits['Active volumes']._add_current_usage(\n            vols,\n            aws_type='AWS::EC2::Volume'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all known limits for this service as a dict of their names to ~. AwsLimit objects.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        limits.update(self._get_limits_ebs())\n        self.limits = limits\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_limits_ebs(self):\n        limits = {}\n        limits['Provisioned IOPS'] = AwsLimit(\n            'Provisioned IOPS',\n            self,\n            200000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='io1',\n        )\n        limits['Provisioned IOPS (SSD) storage (GiB)'] = AwsLimit(\n            'Provisioned IOPS (SSD) storage (GiB)',\n            self,\n            102400,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='io1',\n        )\n        limits['General Purpose (SSD) volume storage (GiB)'] = AwsLimit(\n            'General Purpose (SSD) volume storage (GiB)',\n            self,\n            102400,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='gp2',\n            ta_limit_name='General Purpose SSD (gp2) volume storage (GiB)'\n        )\n        limits['Magnetic volume storage (GiB)'] = AwsLimit(\n            'Magnetic volume storage (GiB)',\n            self,\n            20480,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='standard',\n            ta_limit_name='Magnetic (standard) volume storage (GiB)'\n        )\n        limits['Throughput Optimized (HDD) volume storage (GiB)'] = AwsLimit(\n            'Throughput Optimized (HDD) volume storage (GiB)',\n            self,\n            307200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='st1',\n        )\n        limits['Cold (HDD) volume storage (GiB)'] = AwsLimit(\n            'Cold (HDD) volume storage (GiB)',\n            self,\n            307200,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n            limit_subtype='sc1',\n        )\n        limits['Active snapshots'] = AwsLimit(\n            'Active snapshots',\n            self,\n            10000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::VolumeSnapshot',\n        )\n        limits['Active volumes'] = AwsLimit(\n            'Active volumes',\n            self,\n            5000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::Volume',\n        )\n        return limits", "response": "Return a dict of limits only."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns the actual usage and limit check.", "response": "def check_limits(self, verbose=False):\n        \"\"\"\n        Run the actual usage and limit check, with overrides.\n\n        see: http://awslimitchecker.readthedocs.org/en/latest/python_usage.html#ci-deployment-checks\n        \"\"\"\n        # instantiate the class\n        checker = AwsLimitChecker()\n        # set your overrides\n        checker.set_threshold_overrides(AWS_THRESHOLD_OVERRIDES)\n        checker.set_limit_overrides(AWS_LIMIT_OVERRIDES)\n\n        print(\"Checking AWS resource usage; WARNING threshold {w}% of \"\n              \"limit, CRITICAL threshold {c}% of limit\".format(\n                  w=checker.warning_threshold,\n                  c=checker.critical_threshold))\n\n        # check usage against thresholds\n        # if we didn't support verbose output, we could just iterate the return\n        # value of this to be a bit more efficient.\n        checker.check_thresholds()\n\n        # save state for exit code and summary\n        warnings = []\n        criticals = []\n\n        # iterate the results\n        for service, svc_limits in sorted(checker.get_limits().items()):\n            for limit_name, limit in sorted(svc_limits.items()):\n                have_alarms = False\n                # check warnings and criticals for each Limit\n                for warn in limit.get_warnings():\n                    warnings.append(colored(\"{service} '{limit_name}' usage \"\n                                            \"({u}) exceeds warning threshold \"\n                                            \"(limit={l})\".format(\n                                                service=service,\n                                                limit_name=limit_name,\n                                                u=str(warn),\n                                                l=limit.get_limit(),\n                                            ), 'yellow'))\n                    have_alarms = True\n                for crit in limit.get_criticals():\n                    criticals.append(colored(\"{service} '{limit_name}' usage \"\n                                             \"({u}) exceeds critical threshold\"\n                                             \" (limit={l})\".format(\n                                                 service=service,\n                                                 limit_name=limit_name,\n                                                 u=str(crit),\n                                                 l=limit.get_limit(),\n                                             ), 'red'))\n                    have_alarms = True\n                if not have_alarms and verbose:\n                    print(\"{service} '{limit_name}' OK: {u} (limit={l})\".format(\n                        service=service,\n                        limit_name=limit_name,\n                        u=limit.get_current_usage_str(),\n                        l=limit.get_limit()\n                    ))\n        if verbose:\n            print(\"\\n\\n\")\n        return (warnings, criticals)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a new value for the specified limit overriding the default value.", "response": "def set_limit_override(self, limit_name, value, override_ta=True):\n        \"\"\"\n        Set a new limit ``value`` for the specified limit, overriding\n        the default. If ``override_ta`` is True, also use this value\n        instead of any found by Trusted Advisor. This method simply\n        passes the data through to the\n        :py:meth:`~awslimitchecker.limit.AwsLimit.set_limit_override`\n        method of the underlying :py:class:`~.AwsLimit` instance.\n\n        :param limit_name: the name of the limit to override the value for\n        :type limit_name: str\n        :param value: the new value to set for the limit\n        :type value: int\n        :param override_ta: whether or not to also override Trusted\n          Advisor information\n        :type override_ta: bool\n        :raises: ValueError if limit_name is not known to this service\n        \"\"\"\n        try:\n            self.limits[limit_name].set_limit_override(\n                value,\n                override_ta=override_ta\n            )\n            logger.debug(\n                \"Overriding %s limit %s; default=%d override=%d\",\n                self.service_name,\n                limit_name,\n                value,\n                self.limits[limit_name].default_limit,\n            )\n        except KeyError:\n            raise ValueError(\"{s} service has no '{l}' limit\".format(\n                s=self.service_name,\n                l=limit_name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the value for the TA limit for the specified limit.", "response": "def _set_ta_limit(self, limit_name, value):\n        \"\"\"\n        Set the value for the limit as reported by Trusted Advisor,\n        for the specified limit.\n\n        This method should only be called by :py:class:`~.TrustedAdvisor`.\n\n        :param limit_name: the name of the limit to override the value for\n        :type limit_name: str\n        :param value: the Trusted Advisor limit value\n        :type value: int\n        :raises: ValueError if limit_name is not known to this service\n        \"\"\"\n        try:\n            self.limits[limit_name]._set_ta_limit(value)\n            logger.debug(\n                \"Setting %s limit %s TA limit to %d\",\n                self.service_name,\n                limit_name,\n                value,\n            )\n        except KeyError:\n            raise ValueError(\"{s} service has no '{l}' limit\".format(\n                s=self.service_name,\n                l=limit_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the default warning and critical thresholds used to evaluate the specified limit s usage count or both.", "response": "def set_threshold_override(self, limit_name, warn_percent=None,\n                               warn_count=None, crit_percent=None,\n                               crit_count=None):\n        \"\"\"\n        Override the default warning and critical thresholds used to evaluate\n        the specified limit's usage. Theresholds can be specified as a\n        percentage of the limit, or as a usage count, or both.\n\n        :param warn_percent: new warning threshold, percentage used\n        :type warn_percent: int\n        :param warn_count: new warning threshold, actual count/number\n        :type warn_count: int\n        :param crit_percent: new critical threshold, percentage used\n        :type crit_percent: int\n        :param crit_count: new critical threshold, actual count/number\n        :type crit_count: int\n        \"\"\"\n        try:\n            self.limits[limit_name].set_threshold_override(\n                warn_percent=warn_percent,\n                warn_count=warn_count,\n                crit_percent=crit_percent,\n                crit_count=crit_count\n            )\n        except KeyError:\n            raise ValueError(\"{s} service has no '{l}' limit\".format(\n                s=self.service_name,\n                l=limit_name))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_thresholds(self):\n        if not self._have_usage:\n            self.find_usage()\n        ret = {}\n        for name, limit in self.limits.items():\n            if limit.check_thresholds() is False:\n                ret[name] = limit\n        return ret", "response": "Checks current usage against all configured thresholds for all limits and returns a dict of limit name to limit instance for each limit that crossed one or more of their thresholds."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind the current usage for each limit of this service and update corresponding Limit via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_cluster_manual_snapshots()\n        self._find_cluster_subnet_groups()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a node that references a label", "response": "def label_ref_node(docname, ref_to, title):\n    \"\"\"Generate a node that references a label\"\"\"\n    txt = Text(title, rawsource=title)\n    newchild = inline(\n        ':ref:`%s`' % ref_to,\n        '',\n        txt,\n        classes=['xref', 'std', 'std-ref']\n    )\n    newnode = pending_xref(\n        ':ref:`%s`' % ref_to,\n        newchild,\n        reftype='ref',\n        refwarn='True',\n        reftarget=ref_to,\n        refexplicit='False',\n        refdomain='std',\n        refdoc=docname\n    )\n    return newnode"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvisiting a target node by replacing it with an internal version of the target node.", "response": "def visit_target(self, node):\n        \"\"\"\n        When we find a target node, first make sure it matches the last\n        reference node we saw. Assuming it does, see if its refuri (link URI)\n        is in our replacement list. If so, replace the link with an internal\n        reference.\n        \"\"\"\n        if self.lastref is None:\n            return\n        if (\n            self.lastref.attributes['name'].lower() not in\n            node.attributes['names'] and\n            self.lastref.attributes['name'].lower() not in\n            node.attributes['dupnames']\n        ):\n            # return if target doesn't match last reference found\n            return\n        if node.attributes['refuri'] not in self.replacements:\n            # return if the refuri isn't in our replacement mapping\n            return\n        # ok, we have a node to replace...\n        params = self.replacements[node.attributes['refuri']]\n        meth = params[0]\n        args = params[1:]\n        # remove the target itself; we'll just replace the reference\n        node.parent.remove(node)\n        self.lastref.parent.replace(self.lastref, meth(*args))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the current usage for each limit of this service and update the corresponding Limit via _add_current_usage.", "response": "def find_usage(self):\n        \"\"\"\n        Determine the current usage for each limit of this service,\n        and update corresponding Limit via\n        :py:meth:`~.AwsLimit._add_current_usage`.\n        \"\"\"\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        self.connect_resource()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_instances()\n        self._find_usage_networking_sgs()\n        self._find_usage_networking_eips()\n        self._find_usage_networking_eni_sg()\n        self._find_usage_spot_instances()\n        self._find_usage_spot_fleets()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind usage for all types and update Limits", "response": "def _find_usage_instances(self):\n        \"\"\"calculate On-Demand instance usage for all types and update Limits\"\"\"\n        # update our limits with usage\n        inst_usage = self._instance_usage()\n        res_usage = self._get_reserved_instance_count()\n        logger.debug('Reserved instance count: %s', res_usage)\n        total_ris = 0\n        running_ris = 0\n        # subtract reservations from instance usage\n        ondemand_usage = defaultdict(int)\n        for az in inst_usage:\n            if az not in res_usage:\n                for i_type, count in inst_usage[az].items():\n                    ondemand_usage[i_type] += count\n                continue\n            # else we have reservations for this AZ\n            for i_type, count in inst_usage[az].items():\n                if i_type not in res_usage[az]:\n                    # no reservations for this type\n                    ondemand_usage[i_type] += count\n                    continue\n                od = count - res_usage[az][i_type]\n                total_ris += res_usage[az][i_type]\n                if count < res_usage[az][i_type]:\n                    running_ris += count\n                else:\n                    running_ris += res_usage[az][i_type]\n                if od < 0:\n                    # we have unused reservations\n                    continue\n                ondemand_usage[i_type] += od\n        logger.debug(\n            'Found %d total RIs and %d running/used RIs',\n            total_ris, running_ris\n        )\n        total_instances = 0\n        for i_type, usage in ondemand_usage.items():\n            key = 'Running On-Demand {t} instances'.format(\n                t=i_type)\n            self.limits[key]._add_current_usage(\n                usage,\n                aws_type='AWS::EC2::Instance',\n            )\n            total_instances += usage\n        # limit for ALL On-Demand EC2 instances\n        key = 'Running On-Demand EC2 instances'\n        self.limits[key]._add_current_usage(\n            total_instances,\n            aws_type='AWS::EC2::Instance'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_usage_spot_instances(self):\n        logger.debug('Getting spot instance request usage')\n        try:\n            res = self.conn.describe_spot_instance_requests()\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == 'UnsupportedOperation':\n                return\n            raise\n        count = 0\n        for req in res['SpotInstanceRequests']:\n            if req['State'] in ['open', 'active']:\n                count += 1\n                logger.debug('Counting spot instance request %s state=%s',\n                             req['SpotInstanceRequestId'], req['State'])\n            else:\n                logger.debug('NOT counting spot instance request %s state=%s',\n                             req['SpotInstanceRequestId'], req['State'])\n        self.limits['Max spot instance requests per region']._add_current_usage(\n            count,\n            aws_type='AWS::EC2::SpotInstanceRequest'\n        )", "response": "find usage for spot instances and update limits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding usage for spot fleets and update limits", "response": "def _find_usage_spot_fleets(self):\n        \"\"\"calculate spot fleet request usage and update Limits\"\"\"\n        logger.debug('Getting spot fleet request usage')\n        try:\n            res = self.conn.describe_spot_fleet_requests()\n        except botocore.exceptions.ClientError as e:\n            if e.response['Error']['Code'] == 'UnsupportedOperation':\n                return\n            raise\n        if 'NextToken' in res:\n            logger.error('Error: describe_spot_fleet_requests() response '\n                         'includes pagination token, but pagination not '\n                         'configured in awslimitchecker.')\n        active_fleets = 0\n        total_target_cap = 0\n        lim_cap_per_fleet = self.limits['Max target capacity per spot fleet']\n        lim_launch_specs = self.limits[\n            'Max launch specifications per spot fleet']\n        for fleet in res['SpotFleetRequestConfigs']:\n            _id = fleet['SpotFleetRequestId']\n            if fleet['SpotFleetRequestState'] != 'active':\n                logger.debug('Skipping spot fleet request %s in state %s',\n                             _id, fleet['SpotFleetRequestState'])\n                continue\n            active_fleets += 1\n            cap = fleet['SpotFleetRequestConfig']['TargetCapacity']\n            launch_specs = len(\n                fleet['SpotFleetRequestConfig']['LaunchSpecifications'])\n            total_target_cap += cap\n            lim_cap_per_fleet._add_current_usage(\n                cap, resource_id=_id, aws_type='AWS::EC2::SpotFleetRequest')\n            lim_launch_specs._add_current_usage(\n                launch_specs, resource_id=_id,\n                aws_type='AWS::EC2::SpotFleetRequest')\n        self.limits['Max active spot fleets per region']._add_current_usage(\n            active_fleets, aws_type='AWS::EC2::SpotFleetRequest'\n        )\n        self.limits['Max target capacity for all spot '\n                    'fleets in region']._add_current_usage(\n            total_target_cap, aws_type='AWS::EC2::SpotFleetRequest'\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the count of current instance types and reservations of each availability zone.", "response": "def _get_reserved_instance_count(self):\n        \"\"\"\n        For each availability zone, get the count of current instance\n        reservations of each instance type. Return as a nested\n        dict of AZ name to dict of instance type to reservation count.\n\n        :rtype: dict\n        \"\"\"\n        reservations = defaultdict(int)\n        az_to_res = {}\n        logger.debug(\"Getting reserved instance information\")\n        res = self.conn.describe_reserved_instances()\n\n        for x in res['ReservedInstances']:\n            if x['State'] != 'active':\n                logger.debug(\"Skipping ReservedInstance %s with state %s\",\n                             x['ReservedInstancesId'], x['State'])\n                continue\n            if 'AvailabilityZone' not in x:\n                # \"Regional Benefit\" AZ-less reservation\n                x['AvailabilityZone'] = RI_NO_AZ\n            if x['AvailabilityZone'] not in az_to_res:\n                az_to_res[x['AvailabilityZone']] = deepcopy(reservations)\n            az_to_res[x['AvailabilityZone']][\n                x['InstanceType']] += x['InstanceCount']\n        # flatten and return\n        for x in az_to_res:\n            az_to_res[x] = dict(az_to_res[x])\n        return az_to_res"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a nested dict of AZ name to dict of instance type to count.", "response": "def _instance_usage(self):\n        \"\"\"\n        Find counts of currently-running EC2 Instances\n        (On-Demand or Reserved) by placement (Availability\n        Zone) and instance type (size). Return as a nested dict\n        of AZ name to dict of instance type to count.\n\n        :rtype: dict\n        \"\"\"\n        # On-Demand instances by type\n        ondemand = {}\n        for t in self._instance_types():\n            ondemand[t] = 0\n        az_to_inst = {}\n        logger.debug(\"Getting usage for on-demand instances\")\n        for inst in self.resource_conn.instances.all():\n            if inst.spot_instance_request_id:\n                logger.info(\"Spot instance found (%s); skipping from \"\n                            \"Running On-Demand Instances count\", inst.id)\n                continue\n            if inst.state['Name'] in ['stopped', 'terminated']:\n                logger.debug(\"Ignoring instance %s in state %s\", inst.id,\n                             inst.state['Name'])\n                continue\n            if inst.placement['AvailabilityZone'] not in az_to_inst:\n                az_to_inst[\n                    inst.placement['AvailabilityZone']] = deepcopy(ondemand)\n            try:\n                az_to_inst[\n                    inst.placement['AvailabilityZone']][inst.instance_type] += 1\n            except KeyError:\n                logger.error(\"ERROR - unknown instance type '%s'; not \"\n                             \"counting\", inst.instance_type)\n        return az_to_inst"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns all known limits for this service as a dict of their names to ~. AwsLimit objects.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        limits.update(self._get_limits_instances())\n        limits.update(self._get_limits_networking())\n        limits.update(self._get_limits_spot())\n        self.limits = limits\n        return self.limits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nquerying EC2 s DescribeAccountAttributes API action and update self. limits with the quotas returned. Updates self. limits.", "response": "def _update_limits_from_api(self):\n        \"\"\"\n        Query EC2's DescribeAccountAttributes API action, and update limits\n        with the quotas returned. Updates ``self.limits``.\n        \"\"\"\n        self.connect()\n        self.connect_resource()\n        logger.info(\"Querying EC2 DescribeAccountAttributes for limits\")\n        # no need to paginate\n        attribs = self.conn.describe_account_attributes()\n        for attrib in attribs['AccountAttributes']:\n            aname = attrib['AttributeName']\n            val = attrib['AttributeValues'][0]['AttributeValue']\n            lname = None\n            if aname == 'max-elastic-ips':\n                lname = 'Elastic IP addresses (EIPs)'\n            elif aname == 'max-instances':\n                lname = 'Running On-Demand EC2 instances'\n            elif aname == 'vpc-max-elastic-ips':\n                lname = 'VPC Elastic IP addresses (EIPs)'\n            elif aname == 'vpc-max-security-groups-per-interface':\n                lname = 'VPC security groups per elastic network interface'\n            if lname is not None:\n                if int(val) == 0:\n                    continue\n                self.limits[lname]._set_api_limit(int(val))\n        logger.debug(\"Done setting limits from API\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_limits_instances(self):\n        # from: http://aws.amazon.com/ec2/faqs/\n        # (On-Demand, Reserved, Spot)\n        default_limits = (20, 20, 5)\n        special_limits = {\n            'c4.4xlarge': (10, 20, 5),\n            'c4.8xlarge': (5, 20, 5),\n            'c5.4xlarge': (10, 20, 5),\n            'c5.9xlarge': (5, 20, 5),\n            'c5.18xlarge': (5, 20, 5),\n            'cg1.4xlarge': (2, 20, 5),\n            'cr1.8xlarge': (2, 20, 5),\n            'd2.4xlarge': (10, 20, 5),\n            'd2.8xlarge': (5, 20, 5),\n            'g2.2xlarge': (5, 20, 5),\n            'g2.8xlarge': (2, 20, 5),\n            'g3.4xlarge': (1, 20, 5),\n            'g3.8xlarge': (1, 20, 5),\n            'g3.16xlarge': (1, 20, 5),\n            'h1.8xlarge': (10, 20, 5),\n            'h1.16xlarge': (5, 20, 5),\n            'hi1.4xlarge': (2, 20, 5),\n            'hs1.8xlarge': (2, 20, 0),\n            'i2.2xlarge': (8, 20, 0),\n            'i2.4xlarge': (4, 20, 0),\n            'i2.8xlarge': (2, 20, 0),\n            'i2.xlarge': (8, 20, 0),\n            'i3.2xlarge': (2, 20, 0),\n            'i3.4xlarge': (2, 20, 0),\n            'i3.8xlarge': (2, 20, 0),\n            'i3.16xlarge': (2, 20, 0),\n            'i3.large': (2, 20, 0),\n            'i3.xlarge': (2, 20, 0),\n            'm4.4xlarge': (10, 20, 5),\n            'm4.10xlarge': (5, 20, 5),\n            'm4.16xlarge': (5, 20, 5),\n            'm5.4xlarge': (10, 20, 5),\n            'm5.12xlarge': (5, 20, 5),\n            'm5.24xlarge': (5, 20, 5),\n            'p2.8xlarge': (1, 20, 5),\n            'p2.16xlarge': (1, 20, 5),\n            'p2.xlarge': (1, 20, 5),\n            'p3.2xlarge': (1, 20, 5),\n            'p3.8xlarge': (1, 20, 5),\n            'p3.16xlarge': (1, 20, 5),\n            'p3dn.24xlarge': (1, 20, 5),\n            'r3.4xlarge': (10, 20, 5),\n            'r3.8xlarge': (5, 20, 5),\n            'r4.4xlarge': (10, 20, 5),\n            'r4.8xlarge': (5, 20, 5),\n            'r4.16xlarge': (1, 20, 5),\n        }\n        limits = {}\n        for i_type in self._instance_types():\n            key = 'Running On-Demand {t} instances'.format(\n                t=i_type)\n            lim = default_limits[0]\n            if i_type in special_limits:\n                lim = special_limits[i_type][0]\n            limits[key] = AwsLimit(\n                key,\n                self,\n                lim,\n                self.warning_threshold,\n                self.critical_threshold,\n                limit_type='On-Demand instances',\n                limit_subtype=i_type,\n                ta_limit_name='On-Demand instances - %s' % i_type\n            )\n        # limit for ALL running On-Demand instances\n        key = 'Running On-Demand EC2 instances'\n        limits[key] = AwsLimit(\n            key,\n            self,\n            default_limits[0],\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='On-Demand instances',\n        )\n        return limits", "response": "Returns a dict of limits for EC2 instances only."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dict of limits for spot requests only.", "response": "def _get_limits_spot(self):\n        \"\"\"\n        Return a dict of limits for spot requests only.\n        This method should only be used internally by\n        :py:meth:~.get_limits`.\n\n        :rtype: dict\n        \"\"\"\n        limits = {}\n        limits['Max spot instance requests per region'] = AwsLimit(\n            'Max spot instance requests per region',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='Spot instance requests'\n        )\n\n        limits['Max active spot fleets per region'] = AwsLimit(\n            'Max active spot fleets per region',\n            self,\n            1000,\n            self.warning_threshold,\n            self.critical_threshold,\n        )\n\n        limits['Max launch specifications per spot fleet'] = AwsLimit(\n            'Max launch specifications per spot fleet',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n        )\n\n        limits['Max target capacity per spot fleet'] = AwsLimit(\n            'Max target capacity per spot fleet',\n            self,\n            3000,\n            self.warning_threshold,\n            self.critical_threshold\n        )\n\n        limits['Max target capacity for all spot fleets in region'] = AwsLimit(\n            'Max target capacity for all spot fleets in region',\n            self,\n            5000,\n            self.warning_threshold,\n            self.critical_threshold\n        )\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _find_usage_networking_sgs(self):\n        logger.debug(\"Getting usage for EC2 VPC resources\")\n        sgs_per_vpc = defaultdict(int)\n        rules_per_sg = defaultdict(int)\n        for sg in self.resource_conn.security_groups.all():\n            if sg.vpc_id is not None:\n                sgs_per_vpc[sg.vpc_id] += 1\n                rules_per_sg[sg.id] = len(sg.ip_permissions)\n        # set usage\n        for vpc_id, count in sgs_per_vpc.items():\n            self.limits['Security groups per VPC']._add_current_usage(\n                count,\n                aws_type='AWS::EC2::VPC',\n                resource_id=vpc_id,\n            )\n        for sg_id, count in rules_per_sg.items():\n            self.limits['Rules per VPC security group']._add_current_usage(\n                count,\n                aws_type='AWS::EC2::SecurityGroupRule',\n                resource_id=sg_id,\n            )", "response": "find usage for EC2 VPC resources"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_limits_networking(self):\n        limits = {}\n        limits['Security groups per VPC'] = AwsLimit(\n            'Security groups per VPC',\n            self,\n            500,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::VPC',\n        )\n        limits['Rules per VPC security group'] = AwsLimit(\n            'Rules per VPC security group',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::VPC',\n        )\n        limits['VPC Elastic IP addresses (EIPs)'] = AwsLimit(\n            'VPC Elastic IP addresses (EIPs)',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::EIP',\n            limit_subtype='AWS::EC2::VPC',\n            ta_service_name='VPC'  # TA shows this as VPC not EC2\n        )\n        # the EC2 limits screen calls this 'EC2-Classic Elastic IPs'\n        # but Trusted Advisor just calls it 'Elastic IP addresses (EIPs)'\n        limits['Elastic IP addresses (EIPs)'] = AwsLimit(\n            'Elastic IP addresses (EIPs)',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::EIP',\n        )\n        limits['VPC security groups per elastic network interface'] = AwsLimit(\n            'VPC security groups per elastic network interface',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::EC2::SecurityGroup',\n            limit_subtype='AWS::EC2::NetworkInterface',\n        )\n        return limits", "response": "Returns a dict of VPC - related limits only."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all valid known EC2 instance types.", "response": "def _instance_types(self):\n        \"\"\"\n        Return a list of all known EC2 instance types\n\n        :returns: list of all valid known EC2 instance types\n        :rtype: list\n        \"\"\"\n        GENERAL_TYPES = [\n            'a1.2xlarge',\n            'a1.4xlarge',\n            'a1.large',\n            'a1.medium',\n            'a1.xlarge',\n            't2.nano',\n            't2.micro',\n            't2.small',\n            't2.medium',\n            't2.large',\n            't2.xlarge',\n            't2.2xlarge',\n            't3.nano',\n            't3.micro',\n            't3.small',\n            't3.medium',\n            't3.large',\n            't3.xlarge',\n            't3.2xlarge',\n            'm3.medium',\n            'm3.large',\n            'm3.xlarge',\n            'm3.2xlarge',\n            'm4.large',\n            'm4.xlarge',\n            'm4.2xlarge',\n            'm4.4xlarge',\n            'm4.10xlarge',\n            'm4.16xlarge',\n            'm5.12xlarge',\n            'm5.24xlarge',\n            'm5.2xlarge',\n            'm5.4xlarge',\n            'm5.large',\n            'm5.xlarge',\n            'm5d.12xlarge',\n            'm5d.24xlarge',\n            'm5d.2xlarge',\n            'm5d.4xlarge',\n            'm5d.large',\n            'm5d.xlarge',\n            'm5a.12xlarge',\n            'm5a.24xlarge',\n            'm5a.2xlarge',\n            'm5a.4xlarge',\n            'm5a.large',\n            'm5a.xlarge',\n        ]\n\n        PREV_GENERAL_TYPES = [\n            't1.micro',\n            'm1.small',\n            'm1.medium',\n            'm1.large',\n            'm1.xlarge',\n        ]\n\n        MEMORY_TYPES = [\n            'r3.2xlarge',\n            'r3.4xlarge',\n            'r3.8xlarge',\n            'r3.large',\n            'r3.xlarge',\n            'r4.2xlarge',\n            'r4.4xlarge',\n            'r4.8xlarge',\n            'r4.16xlarge',\n            'r4.large',\n            'r4.xlarge',\n            'r5.2xlarge',\n            'r5.4xlarge',\n            'r5.8xlarge',\n            'r5.12xlarge',\n            'r5.16xlarge',\n            'r5.24xlarge',\n            'r5.large',\n            'r5.metal',\n            'r5.xlarge',\n            'r5a.12xlarge',\n            'r5a.24xlarge',\n            'r5a.2xlarge',\n            'r5a.4xlarge',\n            'r5a.large',\n            'r5a.xlarge',\n            'r5d.2xlarge',\n            'r5d.4xlarge',\n            'r5d.8xlarge',\n            'r5d.12xlarge',\n            'r5d.16xlarge',\n            'r5d.24xlarge',\n            'r5d.large',\n            'r5d.metal',\n            'r5d.xlarge',\n            'x1.16xlarge',\n            'x1.32xlarge',\n            'x1e.2xlarge',\n            'x1e.4xlarge',\n            'x1e.8xlarge',\n            'x1e.16xlarge',\n            'x1e.32xlarge',\n            'x1e.xlarge',\n            'z1d.2xlarge',\n            'z1d.3xlarge',\n            'z1d.6xlarge',\n            'z1d.12xlarge',\n            'z1d.large',\n            'z1d.xlarge',\n        ]\n\n        PREV_MEMORY_TYPES = [\n            'm2.xlarge',\n            'm2.2xlarge',\n            'm2.4xlarge',\n            'cr1.8xlarge',\n        ]\n\n        COMPUTE_TYPES = [\n            'c3.large',\n            'c3.xlarge',\n            'c3.2xlarge',\n            'c3.4xlarge',\n            'c3.8xlarge',\n            'c4.large',\n            'c4.xlarge',\n            'c4.2xlarge',\n            'c4.4xlarge',\n            'c4.8xlarge',\n            'c5.18xlarge',\n            'c5.2xlarge',\n            'c5.4xlarge',\n            'c5.9xlarge',\n            'c5.large',\n            'c5.xlarge',\n            'c5d.18xlarge',\n            'c5d.2xlarge',\n            'c5d.4xlarge',\n            'c5d.9xlarge',\n            'c5d.large',\n            'c5d.xlarge',\n            'c5n.18xlarge',\n            'c5n.2xlarge',\n            'c5n.4xlarge',\n            'c5n.9xlarge',\n            'c5n.large',\n            'c5n.xlarge',\n        ]\n\n        PREV_COMPUTE_TYPES = [\n            'c1.medium',\n            'c1.xlarge',\n            'cc2.8xlarge',\n            'cc1.4xlarge',\n        ]\n\n        ACCELERATED_COMPUTE_TYPES = [\n            'f1.4xlarge',\n            'p2.xlarge',\n            'p2.8xlarge',\n            'p2.16xlarge',\n            'p3.16xlarge',\n            'p3.2xlarge',\n            'p3.8xlarge',\n            'p3dn.24xlarge',\n        ]\n\n        STORAGE_TYPES = [\n            'i2.xlarge',\n            'i2.2xlarge',\n            'i2.4xlarge',\n            'i2.8xlarge',\n            'i3.large',\n            'i3.xlarge',\n            'i3.2xlarge',\n            'i3.4xlarge',\n            'i3.8xlarge',\n            'i3.16xlarge',\n            'i3.metal',\n            'h1.16xlarge',\n            'h1.2xlarge',\n            'h1.4xlarge',\n            'h1.8xlarge',\n        ]\n\n        PREV_STORAGE_TYPES = [\n            # NOTE hi1.4xlarge is no longer in the instance type listings,\n            # but some accounts might still have a limit for it\n            'hi1.4xlarge',\n            'hs1.8xlarge',\n        ]\n\n        DENSE_STORAGE_TYPES = [\n            'd2.xlarge',\n            'd2.2xlarge',\n            'd2.4xlarge',\n            'd2.8xlarge',\n        ]\n\n        GPU_TYPES = [\n            'g2.2xlarge',\n            'g2.8xlarge',\n            'g3.16xlarge',\n            'g3.4xlarge',\n            'g3.8xlarge',\n            'g3s.xlarge',\n        ]\n\n        PREV_GPU_TYPES = [\n            'cg1.4xlarge',\n        ]\n\n        FPGA_TYPES = [\n            # note, as of 2016-12-17, these are still in Developer Preview;\n            # there isn't a published instance limit yet, so we'll assume\n            # it's the default...\n            'f1.2xlarge',\n            'f1.16xlarge',\n        ]\n\n        return (\n            GENERAL_TYPES +\n            PREV_GENERAL_TYPES +\n            MEMORY_TYPES +\n            PREV_MEMORY_TYPES +\n            COMPUTE_TYPES +\n            PREV_COMPUTE_TYPES +\n            ACCELERATED_COMPUTE_TYPES +\n            STORAGE_TYPES +\n            PREV_STORAGE_TYPES +\n            DENSE_STORAGE_TYPES +\n            GPU_TYPES +\n            PREV_GPU_TYPES +\n            FPGA_TYPES\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        self._find_usage_instances()\n        self._find_usage_subnet_groups()\n        self._find_usage_security_groups()\n        # RDS API also provides usage information\n        self._update_limits_from_api()\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find the current usage for each limit of this service and update corresponding Limit via AwsLimit. _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_usage_instances(self):\n        paginator = self.conn.get_paginator('describe_db_instances')\n        for page in paginator.paginate():\n            for instance in page['DBInstances']:\n                self.limits['Read replicas per master']._add_current_usage(\n                    len(instance['ReadReplicaDBInstanceIdentifiers']),\n                    aws_type='AWS::RDS::DBInstance',\n                    resource_id=instance['DBInstanceIdentifier']\n                )", "response": "find usage for DB Instances and related limits"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_usage_subnet_groups(self):\n        paginator = self.conn.get_paginator('describe_db_subnet_groups')\n        for page in paginator.paginate():\n            for group in page['DBSubnetGroups']:\n                self.limits['Subnets per Subnet Group']._add_current_usage(\n                    len(group['Subnets']),\n                    aws_type='AWS::RDS::DBSubnetGroup',\n                    resource_id=group[\"DBSubnetGroupName\"],\n                )", "response": "find usage for subnet groups"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _find_usage_security_groups(self):\n        vpc_count = 0\n\n        paginator = self.conn.get_paginator('describe_db_security_groups')\n        for page in paginator.paginate():\n            for group in page['DBSecurityGroups']:\n                if 'VpcId' in group and group['VpcId'] is not None:\n                    vpc_count += 1\n                self.limits['Max auths per security group']._add_current_usage(\n                    len(group[\"EC2SecurityGroups\"]) + len(group[\"IPRanges\"]),\n                    aws_type='AWS::RDS::DBSecurityGroup',\n                    resource_id=group['DBSecurityGroupName']\n                )\n\n        self.limits['VPC Security Groups']._add_current_usage(\n            vpc_count,\n            aws_type='AWS::RDS::DBSecurityGroup',\n        )", "response": "find usage for security groups"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_limits(self):\n        if self.limits != {}:\n            return self.limits\n        limits = {}\n        limits['DB instances'] = AwsLimit(\n            'DB instances',\n            self,\n            40,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBInstance'\n        )\n        limits['Reserved Instances'] = AwsLimit(\n            'Reserved Instances',\n            self,\n            40,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBInstance',\n        )\n        limits['Storage quota (GB)'] = AwsLimit(\n            'Storage quota (GB)',\n            self,\n            100000,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBInstance',\n        )\n        limits['DB snapshots per user'] = AwsLimit(\n            'DB snapshots per user',\n            self,\n            100,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSnapshot',\n        )\n        limits['DB parameter groups'] = AwsLimit(\n            'DB parameter groups',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBParameterGroup',\n        )\n        limits['DB security groups'] = AwsLimit(\n            'DB security groups',\n            self,\n            25,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSecurityGroup',\n        )\n        limits['VPC Security Groups'] = AwsLimit(\n            'VPC Security Groups',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSecurityGroup',\n        )\n        limits['Subnet Groups'] = AwsLimit(\n            'Subnet Groups',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSubnetGroup',\n            ta_limit_name='Subnet groups'\n        )\n        limits['Subnets per Subnet Group'] = AwsLimit(\n            'Subnets per Subnet Group',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSubnetGroup',\n            ta_limit_name='Subnets per subnet group'\n        )\n        limits['Option Groups'] = AwsLimit(\n            'Option Groups',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBOptionGroup',\n        )\n        limits['Event Subscriptions'] = AwsLimit(\n            'Event Subscriptions',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBEventSubscription',\n            ta_limit_name='Event subscriptions'\n        )\n        limits['Read replicas per master'] = AwsLimit(\n            'Read replicas per master',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBInstance',\n        )\n        # this is the number of rules per security group\n        limits['Max auths per security group'] = AwsLimit(\n            'Max auths per security group',\n            self,\n            20,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBSecurityGroup',\n            limit_subtype='AWS::RDS::DBSecurityGroupIngress',\n        )\n        limits['DB Clusters'] = AwsLimit(\n            'DB Clusters',\n            self,\n            40,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBCluster',\n            ta_limit_name='Clusters'\n        )\n        limits['DB Cluster Parameter Groups'] = AwsLimit(\n            'DB Cluster Parameter Groups',\n            self,\n            50,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type='AWS::RDS::DBClusterParameterGroup',\n            ta_limit_name='Cluster parameter groups'\n        )\n        self.limits = limits\n        return limits", "response": "Returns all known limits for this service as a dict of their namestoLimits objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_limits_from_api(self):\n        self.connect()\n        logger.info(\"Querying RDS DescribeAccountAttributes for limits\")\n        lims = self.conn.describe_account_attributes()['AccountQuotas']\n        for lim in lims:\n            if lim['AccountQuotaName'] not in self.API_NAME_TO_LIMIT:\n                logger.info('RDS DescribeAccountAttributes returned unknown'\n                            'limit: %s (max: %s; used: %s)',\n                            lim['AccountQuotaName'], lim['Max'], lim['Used'])\n                continue\n            lname = self.API_NAME_TO_LIMIT[lim['AccountQuotaName']]\n            self.limits[lname]._set_api_limit(lim['Max'])\n            if len(self.limits[lname].get_current_usage()) < 1:\n                self.limits[lname]._add_current_usage(lim['Used'])\n        logger.debug('Done setting limits from API.')", "response": "Query RDS s DescribeAccountAttributes API action and update self. limits with the quotas returned from the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_usage(self):\n        logger.debug(\"Checking usage for service %s\", self.service_name)\n        self.connect()\n        for lim in self.limits.values():\n            lim._reset_usage()\n        try:\n            self._find_delivery_streams()\n        except EndpointConnectionError as ex:\n            logger.warning(\n                'Caught exception when trying to use Firehose ('\n                'perhaps the Firehose service is not available in this '\n                'region?): %s', ex\n            )\n\n        self._have_usage = True\n        logger.debug(\"Done checking usage.\")", "response": "Find the current usage for each limit of this service and update corresponding Limit via _add_current_usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds usage for CloudTrail related metrics", "response": "def _find_usage_cloudtrail(self):\n        \"\"\"Calculate current usage for CloudTrail related metrics\"\"\"\n\n        trail_list = self.conn.describe_trails()['trailList']\n        trail_count = len(trail_list) if trail_list else 0\n\n        for trail in trail_list:\n            data_resource_count = 0\n            if self.conn._client_config.region_name == trail['HomeRegion']:\n                response = self.conn.get_event_selectors(\n                    TrailName=trail['Name']\n                )\n                event_selectors = response['EventSelectors']\n                for event_selector in event_selectors:\n                    data_resource_count += len(\n                        event_selector.get('DataResources', [])\n                    )\n                self.limits['Event Selectors Per Trail']._add_current_usage(\n                    len(event_selectors),\n                    aws_type='AWS::CloudTrail::EventSelector',\n                    resource_id=trail['Name']\n                )\n                self.limits['Data Resources Per Trail']._add_current_usage(\n                    data_resource_count,\n                    aws_type='AWS::CloudTrail::DataResource',\n                    resource_id=trail['Name']\n                )\n            else:\n                logger.debug(\n                    'Ignoring event selectors and data resources for '\n                    'CloudTrail %s in non-home region' % trail['Name']\n                )\n        self.limits['Trails Per Region']._add_current_usage(\n            trail_count,\n            aws_type=self.aws_type\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all known limits for this service as a dict of their namestoLimits.", "response": "def get_limits(self):\n        \"\"\"\n        Return all known limits for this service, as a dict of their names\n        to :py:class:`~.AwsLimit` objects.\n\n        :returns: dict of limit names to :py:class:`~.AwsLimit` objects\n        :rtype: dict\n        \"\"\"\n        logger.debug(\"Gathering %s's limits from AWS\", self.service_name)\n\n        if self.limits:\n            return self.limits\n        limits = {}\n\n        limits['Trails Per Region'] = AwsLimit(\n            'Trails Per Region',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type=self.aws_type\n        )\n\n        limits['Event Selectors Per Trail'] = AwsLimit(\n            'Event Selectors Per Trail',\n            self,\n            5,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type=self.aws_type,\n            limit_subtype='AWS::CloudTrail::EventSelector'\n        )\n\n        limits['Data Resources Per Trail'] = AwsLimit(\n            'Data Resources Per Trail',\n            self,\n            250,\n            self.warning_threshold,\n            self.critical_threshold,\n            limit_type=self.aws_type,\n            limit_subtype='AWS::CloudTrail::DataResource'\n        )\n\n        self.limits = limits\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes a new plot object with the last available frame.", "response": "def initialize_plot(self, ranges=None):\n        \"\"\"\n        Initializes a new plot object with the last available frame.\n        \"\"\"\n        # Get element key and ranges for frame\n        fig = self.generate_plot(self.keys[-1], ranges)\n        self.drawn = True\n        return fig"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the dimensions corresponding to each axis.", "response": "def _get_axis_dims(self, element):\n        \"\"\"Returns the dimensions corresponding to each axis.\n\n        Should return a list of dimensions or list of lists of\n        dimensions, which will be formatted to label the axis\n        and to link axes.\n        \"\"\"\n        dims = element.dimensions()[:3]\n        pad = [None]*max(3-len(dims), 0)\n        return dims + pad"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating an existing plot with data corresponding to the key.", "response": "def update_frame(self, key, ranges=None, element=None):\n        \"\"\"\n        Updates an existing plot with data corresponding\n        to the key.\n        \"\"\"\n        self.generate_plot(key, ranges, element)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize_plot(self, ranges=None):\n        # Get element key and ranges for frame\n        return self.generate_plot(list(self.hmap.data.keys())[0], ranges)", "response": "Initialize a new plot object with the last available frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_namespace(self):\n        if 'holoviews' not in sys.modules:\n            raise ImportError('HoloViews does not seem to be imported')\n        matches = [k for k,v in get_ipython().user_ns.items() # noqa (get_ipython)\n           if not k.startswith('_') and v is sys.modules['holoviews']]\n        if len(matches) == 0:\n            raise Exception(\"Could not find holoviews module in namespace\")\n        return '%s.archive' % matches[0]", "response": "Find the name of the user s holoviews module in the namespace"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef auto(self, enabled=True, **kwargs):\n        self.namespace = self.get_namespace()\n        self.notebook_name = \"{notebook}\"\n        self._timestamp = tuple(time.localtime())\n        kernel = r'var kernel = IPython.notebook.kernel; '\n        nbname = r\"var nbname = IPython.notebook.get_notebook_name(); \"\n        nbcmd = (r\"var name_cmd = '%s.notebook_name = \\\"' + nbname + '\\\"'; \" % self.namespace)\n        cmd = (kernel + nbname + nbcmd + \"kernel.execute(name_cmd); \")\n        display(Javascript(cmd))\n        time.sleep(0.5)\n        self._auto=enabled\n        self.param.set_param(**kwargs)\n\n        tstamp = time.strftime(\" [%Y-%m-%d %H:%M:%S]\", self._timestamp)\n        print(\"Automatic capture is now %s.%s\"\n              % ('enabled' if enabled else 'disabled',\n                 tstamp if enabled else ''))", "response": "Method to enable or disable automatic capture for the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the current notebook data and export.", "response": "def export(self, timestamp=None):\n        \"\"\"\n        Get the current notebook data and export.\n        \"\"\"\n        if self._timestamp is None:\n            raise Exception(\"No timestamp set. Has the archive been initialized?\")\n        if self.skip_notebook_export:\n            super(NotebookArchive, self).export(timestamp=self._timestamp,\n                                                info={'notebook':self.notebook_name})\n            return\n\n        self.export_success = None\n        name = self.get_namespace()\n        # Unfortunate javascript hacks to get at notebook data\n        capture_cmd = ((r\"var capture = '%s._notebook_data=r\\\"\\\"\\\"'\" % name)\n                       + r\"+json_string+'\\\"\\\"\\\"'; \")\n        cmd = (r'var kernel = IPython.notebook.kernel; '\n               + r'var json_data = IPython.notebook.toJSON(); '\n               + r'var json_string = JSON.stringify(json_data); '\n               + capture_cmd\n               + \"var pycmd = capture + ';%s._export_with_html()'; \" % name\n               + r\"kernel.execute(pycmd)\")\n\n        tstamp = time.strftime(self.timestamp_format, self._timestamp)\n        export_name = self._format(self.export_name, {'timestamp':tstamp, 'notebook':self.notebook_name})\n        print(('Export name: %r\\nDirectory    %r' % (export_name,\n                                                     os.path.join(os.path.abspath(self.root))))\n               + '\\n\\nIf no output appears, please check holoviews.archive.last_export_status()')\n        display(Javascript(cmd))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, obj=None, filename=None, data=None, info={}, html=None):\n        \"Similar to FileArchive.add but accepts html strings for substitution\"\n        initial_last_key = list(self._files.keys())[-1] if len(self) else None\n        if self._auto:\n            exporters = self.exporters[:]\n            # Can only associate html for one exporter at a time\n            for exporter in exporters:\n                self.exporters = [exporter]\n                super(NotebookArchive, self).add(obj, filename, data,\n                                                 info=dict(info,\n                                                           notebook=self.notebook_name))\n                # Only add substitution if file successfully added to archive.\n                new_last_key = list(self._files.keys())[-1] if len(self) else None\n                if new_last_key != initial_last_key:\n                    self._replacements[new_last_key] = html\n\n            # Restore the full list of exporters\n            self.exporters = exporters", "response": "Similar to FileArchive. add but accepts html strings for substitution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _export_with_html(self):                    # pragma: no cover\n        \"Computes substitutions before using nbconvert with preprocessors\"\n        self.export_success = False\n        try:\n            tstamp = time.strftime(self.timestamp_format, self._timestamp)\n            substitutions = {}\n            for (basename, ext), entry in self._files.items():\n                (_, info) = entry\n                html_key = self._replacements.get((basename, ext), None)\n                if html_key is None: continue\n                filename = self._format(basename, {'timestamp':tstamp,\n                                                   'notebook':self.notebook_name})\n                fpath = filename+(('.%s' % ext) if ext else '')\n                info = {'src':fpath, 'mime_type':info['mime_type']}\n                # No mime type\n                if 'mime_type' not in info: pass\n                # Not displayable in an HTML tag\n                elif info['mime_type'] not in self._tags: pass\n                else:\n                    basename, ext = os.path.splitext(fpath)\n                    truncated = self._truncate_name(basename, ext[1:])\n                    link_html = self._format(self._tags[info['mime_type']],\n                                             {'src':truncated,\n                                              'mime_type':info['mime_type'],\n                                              'css':''})\n                    substitutions[html_key] = (link_html, truncated)\n\n            node = self._get_notebook_node()\n            html = self._generate_html(node, substitutions)\n\n            export_filename = self.snapshot_name\n\n            # Add the html snapshot\n            super(NotebookArchive, self).add(filename=export_filename,\n                                             data=html, info={'file-ext':'html',\n                                                              'mime_type':'text/html',\n                                                              'notebook':self.notebook_name})\n            # Add cleared notebook\n            cleared = self._clear_notebook(node)\n            super(NotebookArchive, self).add(filename=export_filename,\n                                             data=cleared, info={'file-ext':'ipynb',\n                                                                 'mime_type':'text/json',\n                                                                 'notebook':self.notebook_name})\n            # If store cleared_notebook... save here\n            super(NotebookArchive, self).export(timestamp=self._timestamp,\n                                                info={'notebook':self.notebook_name})\n        except:\n            self.traceback = traceback.format_exc()\n        else:\n            self.export_success = True", "response": "Computes substitutions before using nbconvert with preprocessors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_notebook_node(self):                   # pragma: no cover\n        \"Load captured notebook node\"\n        size = len(self._notebook_data)\n        if size == 0:\n            raise Exception(\"Captured buffer size for notebook node is zero.\")\n        node = reader.reads(self._notebook_data)\n        self.nbversion = reader.get_version(node)\n        return node", "response": "Load captured notebook node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating a style and associated value.", "response": "def validate(style, value, scalar=False):\n    \"\"\"\n    Validates a style and associated value.\n\n    Arguments\n    ---------\n    style: str\n       The style to validate (e.g. 'color', 'size' or 'marker')\n    value: \n       The style value to validate\n    scalar: bool\n\n\n    Returns\n    -------\n    valid: boolean or None\n       If validation is supported returns boolean, otherwise None\n    \"\"\"\n    validator = get_validator(style)\n    if validator is None:\n        return None\n    if isinstance(value, (np.ndarray, list)):\n        if scalar:\n            return False\n        return all(validator(v) for v in value)\n    return validator(value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a tuple of the RGB color alphabetic values for the given rgba tuple.", "response": "def rgba_tuple(rgba):\n    \"\"\"\n    Ensures RGB(A) tuples in the range 0-1 are scaled to 0-255.\n    \"\"\"\n    if isinstance(rgba, tuple):\n        return tuple(int(c*255) if i<3 else c for i, c in enumerate(rgba))\n    else:\n        return COLOR_ALIASES.get(rgba, rgba)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexpand a style dictionary to a batched plot by iterating over the list of style options and expanding any options found in the order of the supplied style dictionary returning a data and mapping defining the style dictionary that should be added to the ColumnDataSource.", "response": "def expand_batched_style(style, opts, mapping, nvals):\n    \"\"\"\n    Computes styles applied to a batched plot by iterating over the\n    supplied list of style options and expanding any options found in\n    the supplied style dictionary returning a data and mapping defining\n    the data that should be added to the ColumnDataSource.\n    \"\"\"\n    opts = sorted(opts, key=lambda x: x in ['color', 'alpha'])\n    applied_styles = set(mapping)\n    style_data, style_mapping = {}, {}\n    for opt in opts:\n        if 'color' in opt:\n            alias = 'color'\n        elif 'alpha' in opt:\n            alias = 'alpha'\n        else:\n            alias = None\n        if opt not in style or opt in mapping:\n            continue\n        elif opt == alias:\n            if alias in applied_styles:\n                continue\n            elif 'line_'+alias in applied_styles:\n                if 'fill_'+alias not in opts:\n                    continue\n                opt = 'fill_'+alias\n                val = style[alias]\n            elif 'fill_'+alias in applied_styles:\n                opt = 'line_'+alias\n                val = style[alias]\n            else:\n                val = style[alias]\n        else:\n            val = style[opt]\n        style_mapping[opt] = {'field': opt}\n        applied_styles.add(opt)\n        if 'color' in opt and isinstance(val, tuple):\n            val = rgb2hex(val)\n        style_data[opt] = [val]*nvals\n    return style_data, style_mapping"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simple_equality(cls, first, second, msg=None):\n        if not first==second:\n            standardMsg = '%s != %s' % (safe_repr(first), safe_repr(second))\n            raise cls.failureException(msg or standardMsg)", "response": "Simple equality test for the two objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef assertEqual(cls, first, second, msg=None):\n        asserter = None\n        if type(first) is type(second):\n            asserter = cls.equality_type_funcs.get(type(first))\n\n            try:              basestring = basestring # Python 2\n            except NameError: basestring = str        # Python 3\n\n            if asserter is not None:\n                if isinstance(asserter, basestring):\n                    asserter = getattr(cls, asserter)\n\n        if asserter is None:\n            asserter = cls.simple_equality\n\n        if msg is None:\n            asserter(first, second)\n        else:\n            asserter(first, second, msg=msg)", "response": "Classmethod equivalent to unittest. TestCase method\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes a Bokeh glyph object.", "response": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        properties = mpl_to_bokeh(properties)\n        properties = dict(properties, **mapping)\n        if 'xs' in mapping:\n            renderer = plot.patches(**properties)\n        else:\n            renderer = plot.quad(**properties)\n        if self.colorbar and 'color_mapper' in self.handles:\n            self._draw_colorbar(plot, self.handles['color_mapper'])\n        return renderer, renderer.glyph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn true if the given point is contained within the base box.", "response": "def contains(self, x, y):\n        \"\"\"\n        Returns true if the given point is contained within the\n        bounding box, where all boundaries of the box are\n        considered to be inclusive.\n        \"\"\"\n        left, bottom, right, top = self.aarect().lbrt()\n        return (left <= x <= right) and (bottom <= y <= top)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the given point is contained within the base - box.", "response": "def contains_exclusive(self, x, y):\n        \"\"\"\n        Return True if the given point is contained within the\n        bounding box, where the bottom and right boundaries are\n        considered exclusive.\n        \"\"\"\n        left, bottom, right, top = self._aarect.lbrt()\n        return (left <= x < right) and (bottom < y <= top)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef containsbb_exclusive(self, x):\n        left, bottom, right, top = self.aarect().lbrt()\n        leftx, bottomx, rightx, topx = x.aarect().lbrt()\n        return (left <= leftx) and (bottom <= bottomx) and (right >= rightx) and (top >= topx) and\\\n               (not ((left == leftx) and (bottom == bottomx) and (right == rightx) and (top == topx)))", "response": "Returns true if the given BoundingBox x is contained within the current bounding box and where at least one of the boundaries of the bounding box has been exclusive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lbrt(self):\n        return self._left, self._bottom, self._right, self._top", "response": "Return left bottom right and top as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef centroid(self):\n        left, bottom, right, top = self.lbrt()\n        return (right + left) / 2.0, (top + bottom) / 2.0", "response": "Return the centroid of the rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_nested_dmaps(dmap):\n    if not isinstance(dmap, DynamicMap):\n        return []\n    dmaps = [dmap]\n    for o in dmap.callback.inputs:\n        dmaps.extend(get_nested_dmaps(o))\n    return list(set(dmaps))", "response": "Recurses DynamicMap to find DynamicMaps inputs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dynamicmap_memoization(callable_obj, streams):\n    memoization_state = bool(callable_obj._stream_memoization)\n    callable_obj._stream_memoization &= not any(s.transient and s._triggering for s in streams)\n    try:\n        yield\n    except:\n        raise\n    finally:\n        callable_obj._stream_memoization = memoization_state", "response": "A context manager that checks whether the Callable should have memoization enabled based on the supplied streams."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef overlay(self, dimensions=None, **kwargs):\n        dimensions = self._valid_dimensions(dimensions)\n        if len(dimensions) == self.ndims:\n            with item_check(False):\n                return NdOverlay(self, **kwargs).reindex(dimensions)\n        else:\n            dims = [d for d in self.kdims if d not in dimensions]\n            return self.groupby(dims, group_type=NdOverlay, **kwargs)", "response": "Groups data by supplied dimension and overlays each group\n            along the dimension ( s )."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup data by supplied dimensions and lay out groups along the dimensions along the GridSpace.", "response": "def grid(self, dimensions=None, **kwargs):\n        \"\"\"Group by supplied dimension(s) and lay out groups in grid\n\n        Groups data by supplied dimension(s) laying the groups along\n        the dimension(s) out in a GridSpace.\n\n        Args:\n        dimensions: Dimension/str or list\n            Dimension or list of dimensions to group by\n\n        Returns:\n            GridSpace with supplied dimensions\n        \"\"\"\n        dimensions = self._valid_dimensions(dimensions)\n        if len(dimensions) == self.ndims:\n            with item_check(False):\n                return GridSpace(self, **kwargs).reindex(dimensions)\n        return self.groupby(dimensions, container_type=GridSpace, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngroups by supplied dimension and lay out groups", "response": "def layout(self, dimensions=None, **kwargs):\n        \"\"\"Group by supplied dimension(s) and lay out groups\n\n        Groups data by supplied dimension(s) laying the groups along\n        the dimension(s) out in a NdLayout.\n\n        Args:\n            dimensions: Dimension(s) to group by\n\n        Returns:\n            NdLayout with supplied dimensions\n        \"\"\"\n        dimensions = self._valid_dimensions(dimensions)\n        if len(dimensions) == self.ndims:\n            with item_check(False):\n                return NdLayout(self, **kwargs).reindex(dimensions)\n        return self.groupby(dimensions, container_type=NdLayout, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply simplified option definition returning a new object containing the options applied to the objects in the HoloMap", "response": "def options(self, *args, **kwargs):\n        \"\"\"Applies simplified option definition returning a new object\n\n        Applies options defined in a flat format to the objects\n        returned by the DynamicMap. If the options are to be set\n        directly on the objects in the HoloMap a simple format may be\n        used, e.g.:\n\n            obj.options(cmap='viridis', show_title=False)\n\n        If the object is nested the options must be qualified using\n        a type[.group][.label] specification, e.g.:\n\n            obj.options('Image', cmap='viridis', show_title=False)\n\n        or using:\n\n            obj.options({'Image': dict(cmap='viridis', show_title=False)})\n\n        Args:\n            *args: Sets of options to apply to object\n                Supports a number of formats including lists of Options\n                objects, a type[.group][.label] followed by a set of\n                keyword options to apply and a dictionary indexed by\n                type[.group][.label] specs.\n            backend (optional): Backend to apply options to\n                Defaults to current selected backend\n            clone (bool, optional): Whether to clone object\n                Options can be applied inplace with clone=False\n            **kwargs: Keywords of options\n                Set of options to apply to the object\n\n        Returns:\n            Returns the cloned object with the options applied\n        \"\"\"\n        data = OrderedDict([(k, v.options(*args, **kwargs))\n                             for k, v in self.data.items()])\n        return self.clone(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split_overlays(self):\n        \"Deprecated method to split overlays inside the HoloMap.\"\n        if util.config.future_deprecations:\n            self.param.warning(\"split_overlays is deprecated and is now \"\n                               \"a private method.\")\n        return self._split_overlays()", "response": "Deprecated method to split overlays inside the HoloMap."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsplits overlays inside the HoloMap into list of HoloMaps", "response": "def _split_overlays(self):\n        \"Splits overlays inside the HoloMap into list of HoloMaps\"\n        if not issubclass(self.type, CompositeOverlay):\n            return None, self.clone()\n\n        item_maps = OrderedDict()\n        for k, overlay in self.data.items():\n            for key, el in overlay.items():\n                if key not in item_maps:\n                    item_maps[key] = [(k, el)]\n                else:\n                    item_maps[key].append((k, el))\n\n        maps, keys = [], []\n        for k, layermap in item_maps.items():\n            maps.append(self.clone(layermap))\n            keys.append(k)\n        return keys, maps"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dynamic_mul(self, dimensions, other, keys):\n        # If either is a HoloMap compute Dimension values\n        if not isinstance(self, DynamicMap) or not isinstance(other, DynamicMap):\n            keys = sorted((d, v) for k in keys for d, v in k)\n            grouped =  dict([(g, [v for _, v in group])\n                             for g, group in groupby(keys, lambda x: x[0])])\n            dimensions = [d(values=grouped[d.name]) for d in dimensions]\n            map_obj = None\n\n        # Combine streams\n        map_obj = self if isinstance(self, DynamicMap) else other\n        if isinstance(self, DynamicMap) and isinstance(other, DynamicMap):\n            self_streams = util.dimensioned_streams(self)\n            other_streams = util.dimensioned_streams(other)\n            streams = list(util.unique_iterator(self_streams+other_streams))\n        else:\n            streams = map_obj.streams\n\n        def dynamic_mul(*key, **kwargs):\n            key_map = {d.name: k for d, k in zip(dimensions, key)}\n            layers = []\n            try:\n                self_el = self.select(HoloMap, **key_map) if self.kdims else self[()]\n                layers.append(self_el)\n            except KeyError:\n                pass\n            try:\n                other_el = other.select(HoloMap, **key_map) if other.kdims else other[()]\n                layers.append(other_el)\n            except KeyError:\n                pass\n            return Overlay(layers)\n        callback = Callable(dynamic_mul, inputs=[self, other])\n        callback._is_overlay = True\n        if map_obj:\n            return map_obj.clone(callback=callback, shared_data=False,\n                                 kdims=dimensions, streams=streams)\n        else:\n            return DynamicMap(callback=callback, kdims=dimensions,\n                              streams=streams)", "response": "Implements dynamic version of overlaying operation overlaying\n            and HoloMap overlaying\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collate(self, merge_type=None, drop=[], drop_constant=False):\n        from .element import Collator\n        merge_type=merge_type if merge_type else self.__class__\n        return Collator(self, merge_type=merge_type, drop=drop,\n                        drop_constant=drop_constant)()", "response": "Collate the UniformNdMapping object into a single Layout or HoloMap object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new HoloMap with the elements collapsed along the supplied dimensions.", "response": "def collapse(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n        \"\"\"Concatenates and aggregates along supplied dimensions\n\n        Useful to collapse stacks of objects into a single object,\n        e.g. to average a stack of Images or Curves.\n\n        Args:\n            dimensions: Dimension(s) to collapse\n                Defaults to all key dimensions\n            function: Aggregation function to apply, e.g. numpy.mean\n            spreadfn: Secondary reduction to compute value spread\n                Useful for computing a confidence interval, spread, or\n                standard deviation.\n            **kwargs: Keyword arguments passed to the aggregation function\n\n        Returns:\n            Returns the collapsed element or HoloMap of collapsed\n            elements\n        \"\"\"\n        from .data import concat\n        if not dimensions:\n            dimensions = self.kdims\n        if not isinstance(dimensions, list): dimensions = [dimensions]\n        if self.ndims > 1 and len(dimensions) != self.ndims:\n            groups = self.groupby([dim for dim in self.kdims\n                                   if dim not in dimensions])\n        elif all(d in self.kdims for d in dimensions):\n            groups = HoloMap([(0, self)])\n        else:\n            raise KeyError(\"Supplied dimensions not found.\")\n\n        collapsed = groups.clone(shared_data=False)\n        for key, group in groups.items():\n            if hasattr(group.last, 'interface'):\n                group_data = concat(group)\n                if function:\n                    agg = group_data.aggregate(group.last.kdims, function, spreadfn, **kwargs)\n                    group_data = group.type(agg)\n            else:\n                group_data = [el.data for el in group]\n                args = (group_data, function, group.last.kdims)\n                data = group.type.collapse_data(*args, **kwargs)\n                group_data = group.last.clone(data)\n            collapsed[key] = group_data\n        return collapsed if self.ndims-len(dimensions) else collapsed.last"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsample the values at the specified coordinates in the HoloMap.", "response": "def sample(self, samples=[], bounds=None, **sample_values):\n        \"\"\"Samples element values at supplied coordinates.\n\n        Allows sampling of element with a list of coordinates matching\n        the key dimensions, returning a new object containing just the\n        selected samples. Supports multiple signatures:\n\n        Sampling with a list of coordinates, e.g.:\n\n            ds.sample([(0, 0), (0.1, 0.2), ...])\n\n        Sampling a range or grid of coordinates, e.g.:\n\n            1D: ds.sample(3)\n            2D: ds.sample((3, 3))\n\n        Sampling by keyword, e.g.:\n\n            ds.sample(x=0)\n\n        Args:\n            samples: List of nd-coordinates to sample\n            bounds: Bounds of the region to sample\n                Defined as two-tuple for 1D sampling and four-tuple\n                for 2D sampling.\n            closest: Whether to snap to closest coordinates\n            **kwargs: Coordinates specified as keyword pairs\n                Keywords of dimensions and scalar coordinates\n\n        Returns:\n            A Table containing the sampled coordinates\n        \"\"\"\n        if util.config.future_deprecations:\n            self.param.warning('The HoloMap.sample method is deprecated, '\n                               'for equivalent functionality use '\n                               'HoloMap.apply.sample().collapse().')\n\n        dims = self.last.ndims\n        if isinstance(samples, tuple) or np.isscalar(samples):\n            if dims == 1:\n                xlim = self.last.range(0)\n                lower, upper = (xlim[0], xlim[1]) if bounds is None else bounds\n                edges = np.linspace(lower, upper, samples+1)\n                linsamples = [(l+u)/2.0 for l,u in zip(edges[:-1], edges[1:])]\n            elif dims == 2:\n                (rows, cols) = samples\n                if bounds:\n                    (l,b,r,t) = bounds\n                else:\n                    l, r = self.last.range(0)\n                    b, t = self.last.range(1)\n\n                xedges = np.linspace(l, r, cols+1)\n                yedges = np.linspace(b, t, rows+1)\n                xsamples = [(lx+ux)/2.0 for lx,ux in zip(xedges[:-1], xedges[1:])]\n                ysamples = [(ly+uy)/2.0 for ly,uy in zip(yedges[:-1], yedges[1:])]\n\n                Y,X = np.meshgrid(ysamples, xsamples)\n                linsamples = list(zip(X.flat, Y.flat))\n            else:\n                raise NotImplementedError(\"Regular sampling not implemented \"\n                                          \"for elements with more than two dimensions.\")\n\n            samples = list(util.unique_iterator(self.last.closest(linsamples)))\n\n        sampled = self.clone([(k, view.sample(samples, closest=False,\n                                              **sample_values))\n                              for k, view in self.data.items()])\n\n        from ..element import Table\n        return Table(sampled.collapse())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies a function to each element of the Dataset and returns a new Dataset with reduced elements.", "response": "def reduce(self, dimensions=None, function=None, spread_fn=None, **reduce_map):\n        \"\"\"Applies reduction to elements along the specified dimension(s).\n\n        Allows reducing the values along one or more key dimension\n        with the supplied function. Supports two signatures:\n\n        Reducing with a list of dimensions, e.g.:\n\n            ds.reduce(['x'], np.mean)\n\n        Defining a reduction using keywords, e.g.:\n\n            ds.reduce(x=np.mean)\n\n        Args:\n            dimensions: Dimension(s) to apply reduction on\n                Defaults to all key dimensions\n            function: Reduction operation to apply, e.g. numpy.mean\n            spreadfn: Secondary reduction to compute value spread\n                Useful for computing a confidence interval, spread, or\n                standard deviation.\n            **reductions: Keyword argument defining reduction\n                Allows reduction to be defined as keyword pair of\n                dimension and function\n\n        Returns:\n            The Dataset after reductions have been applied.\n        \"\"\"\n        if util.config.future_deprecations:\n            self.param.warning('The HoloMap.reduce method is deprecated, '\n                               'for equivalent functionality use '\n                               'HoloMap.apply.reduce().collapse().')\n\n        from ..element import Table\n        reduced_items = [(k, v.reduce(dimensions, function, spread_fn, **reduce_map))\n                         for k, v in self.items()]\n        if not isinstance(reduced_items[0][1], Table):\n            params = dict(util.get_param_values(self.last),\n                          kdims=self.kdims, vdims=self.last.vdims)\n            return Table(reduced_items, **params)\n        return Table(self.clone(reduced_items).collapse())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relabel(self, label=None, group=None, depth=1):\n        return super(HoloMap, self).relabel(label=label, group=group, depth=depth)", "response": "Clone object and apply new label and group and depth."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, individually=True, **kwargs):\n        \"\"\"Computes and adjoins histogram along specified dimension(s).\n\n        Defaults to first value dimension if present otherwise falls\n        back to first key dimension.\n\n        Args:\n            dimension: Dimension(s) to compute histogram on\n            num_bins (int, optional): Number of bins\n            bin_range (tuple optional): Lower and upper bounds of bins\n            adjoin (bool, optional): Whether to adjoin histogram\n\n        Returns:\n            AdjointLayout of HoloMap and histograms or just the\n            histograms\n        \"\"\"\n        if dimension is not None and not isinstance(dimension, list):\n            dimension = [dimension]\n        histmaps = [self.clone(shared_data=False) for _ in (dimension or [None])]\n\n        if individually:\n            map_range = None\n        else:\n            if dimension is None:\n                raise Exception(\"Please supply the dimension to compute a histogram for.\")\n            map_range = self.range(kwargs['dimension'])\n\n        bin_range = map_range if bin_range is None else bin_range\n        style_prefix = 'Custom[<' + self.name + '>]_'\n        if issubclass(self.type, (NdOverlay, Overlay)) and 'index' not in kwargs:\n            kwargs['index'] = 0\n\n        for k, v in self.data.items():\n            hists = v.hist(adjoin=False, dimension=dimension,\n                           bin_range=bin_range, num_bins=num_bins,\n                           style_prefix=style_prefix, **kwargs)\n            if isinstance(hists, Layout):\n                for i, hist in enumerate(hists):\n                    histmaps[i][k] = hist\n            else:\n                histmaps[0][k] = hists\n\n        if adjoin:\n            layout = self\n            for hist in histmaps:\n                layout = (layout << hist)\n            if issubclass(self.type, (NdOverlay, Overlay)):\n                layout.main_layer = kwargs['index']\n            return layout\n        else:\n            if len(histmaps) > 1:\n                return Layout(histmaps)\n            else:\n                return histmaps[0]", "response": "Computes and adjoins histogram along specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the callable takes no arguments", "response": "def noargs(self):\n        \"Returns True if the callable takes no arguments\"\n        noargs = inspect.ArgSpec(args=[], varargs=None, keywords=None, defaults=None)\n        return self.argspec == noargs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a clone of the Callable optionally with new settings", "response": "def clone(self, callable=None, **overrides):\n        \"\"\"Clones the Callable optionally with new settings\n\n        Args:\n            callable: New callable function to wrap\n            **overrides: Parameter overrides to apply\n\n        Returns:\n            Cloned Callable object\n        \"\"\"\n        old = {k: v for k, v in self.get_param_values()\n               if k not in ['callable', 'name']}\n        params = dict(old, **overrides)\n        callable = self.callable if callable is None else callable\n        return self.__class__(callable, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unbounded(self):\n        unbounded_dims = []\n        # Dimensioned streams do not need to be bounded\n        stream_params = set(util.stream_parameters(self.streams))\n        for kdim in self.kdims:\n            if str(kdim) in stream_params:\n                continue\n            if kdim.values:\n                continue\n            if None in kdim.range:\n                unbounded_dims.append(str(kdim))\n        return unbounded_dims", "response": "Returns a list of key dimensions that are unbounded excluding the stream parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _initial_key(self):\n        key = []\n        undefined = []\n        stream_params = set(util.stream_parameters(self.streams))\n        for kdim in self.kdims:\n            if str(kdim) in stream_params:\n                key.append(None)\n            elif kdim.default:\n                key.append(kdim.default)\n            elif kdim.values:\n                if all(util.isnumeric(v) for v in kdim.values):\n                    key.append(sorted(kdim.values)[0])\n                else:\n                    key.append(kdim.values[0])\n            elif kdim.range[0] is not None:\n                key.append(kdim.range[0])\n            else:\n                undefined.append(kdim)\n        if undefined:\n            msg = ('Dimension(s) {undefined_dims} do not specify range or values needed '\n                   'to generate initial key')\n            undefined_dims = ', '.join(['%r' % str(dim) for dim in undefined])\n            raise KeyError(msg.format(undefined_dims=undefined_dims))\n\n        return tuple(key)", "response": "Construct an initial key for based on the lower range bounds or the values on the key dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate that the supplied key values are within the bounds of the current time series.", "response": "def _validate_key(self, key):\n        \"\"\"\n        Make sure the supplied key values are within the bounds\n        specified by the corresponding dimension range and soft_range.\n        \"\"\"\n        if key == () and len(self.kdims) == 0: return ()\n        key = util.wrap_tuple(key)\n        assert len(key) == len(self.kdims)\n        for ind, val in enumerate(key):\n            kdim = self.kdims[ind]\n            low, high = util.max_range([kdim.range, kdim.soft_range])\n            if util.is_number(low) and util.isfinite(low):\n                if val < low:\n                    raise KeyError(\"Key value %s below lower bound %s\"\n                                   % (val, low))\n            if util.is_number(high) and util.isfinite(high):\n                if val > high:\n                    raise KeyError(\"Key value %s above upper bound %s\"\n                                   % (val, high))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef event(self, **kwargs):\n        if self.callback.noargs and self.streams == []:\n            self.param.warning(\n                'No streams declared. To update a DynamicMaps using '\n                'generators (or callables without arguments) use streams=[Next()]')\n            return\n        if self.streams == []:\n            self.param.warning('No streams on DynamicMap, calling event '\n                               'will have no effect')\n            return\n\n        stream_params = set(util.stream_parameters(self.streams))\n        invalid = [k for k in kwargs.keys() if k not in stream_params]\n        if invalid:\n            msg = 'Key(s) {invalid} do not correspond to stream parameters'\n            raise KeyError(msg.format(invalid = ', '.join('%r' % i for i in invalid)))\n\n        streams = []\n        for stream in self.streams:\n            contents = stream.contents\n            applicable_kws = {k:v for k,v in kwargs.items()\n                              if k in set(contents.keys())}\n            if not applicable_kws and contents:\n                continue\n            streams.append(stream)\n            rkwargs = util.rename_stream_kwargs(stream, applicable_kws, reverse=True)\n            stream.update(**rkwargs)\n\n        Stream.trigger(streams)", "response": "Updates attached streams and triggers events on them."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _style(self, retval):\n        \"Applies custom option tree to values return by the callback.\"\n        if self.id not in Store.custom_options():\n            return retval\n        spec = StoreOptions.tree_to_dict(Store.custom_options()[self.id])\n        return retval.opts(spec)", "response": "Applies custom option tree to values return by the callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _execute_callback(self, *args):\n        \"Executes the callback with the appropriate args and kwargs\"\n        self._validate_key(args)      # Validate input key\n\n        # Additional validation needed to ensure kwargs don't clash\n        kdims = [kdim.name for kdim in self.kdims]\n        kwarg_items = [s.contents.items() for s in self.streams]\n        hash_items = tuple(tuple(sorted(s.hashkey.items())) for s in self.streams)+args\n        flattened = [(k,v) for kws in kwarg_items for (k,v) in kws\n                     if k not in kdims]\n\n        if self._posarg_keys:\n            kwargs = dict(flattened, **dict(zip(self._posarg_keys, args)))\n            args = ()\n        else:\n            kwargs = dict(flattened)\n        if not isinstance(self.callback, Generator):\n            kwargs['_memoization_hash_'] = hash_items\n\n        with dynamicmap_memoization(self.callback, self.streams):\n            retval = self.callback(*args, **kwargs)\n        return self._style(retval)", "response": "Executes the callback with the appropriate args and kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies simplified option definition returning a new object.", "response": "def options(self, *args, **kwargs):\n        \"\"\"Applies simplified option definition returning a new object.\n\n        Applies options defined in a flat format to the objects\n        returned by the DynamicMap. If the options are to be set\n        directly on the objects returned by the DynamicMap a simple\n        format may be used, e.g.:\n\n            obj.options(cmap='viridis', show_title=False)\n\n        If the object is nested the options must be qualified using\n        a type[.group][.label] specification, e.g.:\n\n            obj.options('Image', cmap='viridis', show_title=False)\n\n        or using:\n\n            obj.options({'Image': dict(cmap='viridis', show_title=False)})\n\n        Args:\n            *args: Sets of options to apply to object\n                Supports a number of formats including lists of Options\n                objects, a type[.group][.label] followed by a set of\n                keyword options to apply and a dictionary indexed by\n                type[.group][.label] specs.\n            backend (optional): Backend to apply options to\n                Defaults to current selected backend\n            clone (bool, optional): Whether to clone object\n                Options can be applied inplace with clone=False\n            **kwargs: Keywords of options\n                Set of options to apply to the object\n\n        Returns:\n            Returns the cloned object with the options applied\n        \"\"\"\n        if 'clone' not in kwargs:\n            kwargs['clone'] = True\n        return self.opts(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclone the object overriding data and parameters.", "response": "def clone(self, data=None, shared_data=True, new_type=None, link=True,\n              *args, **overrides):\n        \"\"\"Clones the object, overriding data and parameters.\n\n        Args:\n            data: New data replacing the existing data\n            shared_data (bool, optional): Whether to use existing data\n            new_type (optional): Type to cast object to\n            link (bool, optional): Whether clone should be linked\n                Determines whether Streams and Links attached to\n                original object will be inherited.\n            *args: Additional arguments to pass to constructor\n            **overrides: New keyword arguments to pass to constructor\n\n        Returns:\n            Cloned object\n        \"\"\"\n        if 'link_inputs' in overrides and util.config.future_deprecations:\n            self.param.warning(\n                'link_inputs argument to the clone method is deprecated, '\n                'use the more general link argument instead.')\n        link = link and overrides.pop('link_inputs', True)\n        callback = overrides.pop('callback', self.callback)\n        if data is None and shared_data:\n            data = self.data\n            if link and callback is self.callback:\n                overrides['plot_id'] = self._plot_id\n        clone = super(UniformNdMapping, self).clone(\n            callback, shared_data, new_type, link,\n            *(data,) + args, **overrides)\n\n        # Ensure the clone references this object to ensure\n        # stream sources are inherited\n        if clone.callback is self.callback:\n            with util.disable_constant(clone):\n                clone.callback = clone.callback.clone(inputs=[self],\n                                                      link_inputs=link)\n        return clone"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cross_product(self, tuple_key, cache, data_slice):\n        if not any(isinstance(el, (list, set)) for el in tuple_key):\n            return None\n        if len(tuple_key)==1:\n            product = tuple_key[0]\n        else:\n            args = [set(el) if isinstance(el, (list,set))\n                    else set([el]) for el in tuple_key]\n            product = itertools.product(*args)\n\n        data = []\n        for inner_key in product:\n            key = util.wrap_tuple(inner_key)\n            if key in cache:\n                val = cache[key]\n            else:\n                val = self._execute_callback(*key)\n            if data_slice:\n                val = self._dataslice(val, data_slice)\n            data.append((key, val))\n        product = self.clone(data)\n\n        if data_slice:\n            from ..util import Dynamic\n            dmap = Dynamic(self, operation=lambda obj, **dynkwargs: obj[data_slice],\n                           streams=self.streams)\n            dmap.data = product.data\n            return dmap\n        return product", "response": "Returns a new DynamicMap if the key expresses a cross product otherwise returns None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nslice DynamicMaps by setting the soft_ranges on the key dimensions and applies data slice to cached and dynamic values.", "response": "def _slice_bounded(self, tuple_key, data_slice):\n        \"\"\"\n        Slices bounded DynamicMaps by setting the soft_ranges on\n        key dimensions and applies data slice to cached and dynamic\n        values.\n        \"\"\"\n        slices = [el for el in tuple_key if isinstance(el, slice)]\n        if any(el.step for el in slices):\n            raise Exception(\"DynamicMap slices cannot have a step argument\")\n        elif len(slices) not in [0, len(tuple_key)]:\n            raise Exception(\"Slices must be used exclusively or not at all\")\n        elif not slices:\n            return None\n\n        sliced = self.clone(self)\n        for i, slc in enumerate(tuple_key):\n            (start, stop) = slc.start, slc.stop\n            if start is not None and start < sliced.kdims[i].range[0]:\n                raise Exception(\"Requested slice below defined dimension range.\")\n            if stop is not None and stop > sliced.kdims[i].range[1]:\n                raise Exception(\"Requested slice above defined dimension range.\")\n            sliced.kdims[i].soft_range = (start, stop)\n        if data_slice:\n            if not isinstance(sliced, DynamicMap):\n                return self._dataslice(sliced, data_slice)\n            else:\n                from ..util import Dynamic\n                if len(self):\n                    slices = [slice(None) for _ in range(self.ndims)] + list(data_slice)\n                    sliced = super(DynamicMap, sliced).__getitem__(tuple(slices))\n                dmap = Dynamic(self, operation=lambda obj, **dynkwargs: obj[data_slice],\n                               streams=self.streams)\n                dmap.data = sliced.data\n                return dmap\n        return sliced"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncaching a key - value pair.", "response": "def _cache(self, key, val):\n        \"\"\"\n        Request that a key/value pair be considered for caching.\n        \"\"\"\n        cache_size = (1 if util.dimensionless_contents(self.streams, self.kdims)\n                      else self.cache_size)\n        if len(self) >= cache_size:\n            first_key = next(k for k in self.data)\n            self.data.pop(first_key)\n        self[key] = val"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmapping a function to all objects matching the specs returning a new object.", "response": "def map(self, map_fn, specs=None, clone=True, link_inputs=True):\n        \"\"\"Map a function to all objects matching the specs\n\n        Recursively replaces elements using a map function when the\n        specs apply, by default applies to all objects, e.g. to apply\n        the function to all contained Curve objects:\n\n            dmap.map(fn, hv.Curve)\n\n        Args:\n            map_fn: Function to apply to each object\n            specs: List of specs to match\n                List of types, functions or type[.group][.label] specs\n                to select objects to return, by default applies to all\n                objects.\n            clone: Whether to clone the object or transform inplace\n\n        Returns:\n            Returns the object after the map_fn has been applied\n        \"\"\"\n        deep_mapped = super(DynamicMap, self).map(map_fn, specs, clone)\n        if isinstance(deep_mapped, type(self)):\n            from ..util import Dynamic\n            def apply_map(obj, **dynkwargs):\n                return obj.map(map_fn, specs, clone)\n            dmap = Dynamic(self, operation=apply_map, streams=self.streams,\n                           link_inputs=link_inputs)\n            dmap.data = deep_mapped.data\n            return dmap\n        return deep_mapped"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relabel(self, label=None, group=None, depth=1):\n        relabelled = super(DynamicMap, self).relabel(label, group, depth)\n        if depth > 0:\n            from ..util import Dynamic\n            def dynamic_relabel(obj, **dynkwargs):\n                return obj.relabel(group=group, label=label, depth=depth-1)\n            dmap = Dynamic(self, streams=self.streams, operation=dynamic_relabel)\n            dmap.data = relabelled.data\n            with util.disable_constant(dmap):\n                dmap.group = relabelled.group\n                dmap.label = relabelled.label\n            return dmap\n        return relabelled", "response": "Clone object and apply new label and group and depth."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a DynamicMap into its components. Only well defined for CompositeOverlays.", "response": "def _split_overlays(self):\n        \"\"\"\n        Splits a DynamicMap into its components. Only well defined for\n        DynamicMap with consistent number and order of layers.\n        \"\"\"\n        if not len(self):\n            raise ValueError('Cannot split DynamicMap before it has been initialized')\n        elif not issubclass(self.type, CompositeOverlay):\n            return None, self\n\n        from ..util import Dynamic\n        keys = list(self.last.data.keys())\n        dmaps = []\n        for key in keys:\n            el = self.last.data[key]\n            def split_overlay_callback(obj, overlay_key=key, overlay_el=el, **kwargs):\n                spec = util.get_overlay_spec(obj, overlay_key, overlay_el)\n                items = list(obj.data.items())\n                specs = [(i, util.get_overlay_spec(obj, k, v))\n                         for i, (k, v) in enumerate(items)]\n                match = util.closest_match(spec, specs)\n                if match is None:\n                    raise KeyError('{spec} spec not found in {otype}. The split_overlays method '\n                                   'only works consistently for a DynamicMap where the '\n                                   'layers of the {otype} do not change.'.format(\n                                       spec=spec, otype=type(obj).__name__))\n                return items[match][1]\n            dmap = Dynamic(self, streams=self.streams, operation=split_overlay_callback)\n            dmap.data = OrderedDict([(list(self.data.keys())[-1], self.last.data[key])])\n            dmaps.append(dmap)\n        return keys, dmaps"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef collate(self):\n        # Initialize\n        if self.last is not None:\n            initialized = self\n        else:\n            initialized = self.clone()\n            initialized[initialized._initial_key()]\n\n        if not isinstance(initialized.last, (Layout, NdLayout, GridSpace)):\n            return self\n\n        container = initialized.last.clone(shared_data=False)\n        type_counter = defaultdict(int)\n\n        # Get stream mapping from callback\n        remapped_streams = []\n        self_dstreams = util.dimensioned_streams(self)\n        streams = self.callback.stream_mapping\n        for i, (k, v) in enumerate(initialized.last.data.items()):\n            vstreams = streams.get(i, [])\n            if not vstreams:\n                if isinstance(initialized.last, Layout):\n                    for l in range(len(k)):\n                        path = '.'.join(k[:l])\n                        if path in streams:\n                            vstreams = streams[path]\n                            break\n                else:\n                    vstreams = streams.get(k, [])\n            if any(s in remapped_streams for s in vstreams):\n                raise ValueError(\n                    \"The stream_mapping supplied on the Callable \"\n                    \"is ambiguous please supply more specific Layout \"\n                    \"path specs.\")\n            remapped_streams += vstreams\n\n            # Define collation callback\n            def collation_cb(*args, **kwargs):\n                layout = self[args]\n                layout_type = type(layout).__name__\n                if len(container.keys()) != len(layout.keys()):\n                    raise ValueError('Collated DynamicMaps must return '\n                                     '%s with consistent number of items.'\n                                     % layout_type)\n\n                key = kwargs['selection_key']\n                index = kwargs['selection_index']\n                obj_type = kwargs['selection_type']\n                dyn_type_map = defaultdict(list)\n                for k, v in layout.data.items():\n                    if k == key:\n                        return layout[k]\n                    dyn_type_map[type(v)].append(v)\n\n                dyn_type_counter = {t: len(vals) for t, vals in dyn_type_map.items()}\n                if dyn_type_counter != type_counter:\n                    raise ValueError('The objects in a %s returned by a '\n                                     'DynamicMap must consistently return '\n                                     'the same number of items of the '\n                                     'same type.' % layout_type)\n                return dyn_type_map[obj_type][index]\n\n            callback = Callable(partial(collation_cb, selection_key=k,\n                                        selection_index=type_counter[type(v)],\n                                        selection_type=type(v)),\n                                inputs=[self])\n            vstreams = list(util.unique_iterator(self_dstreams + vstreams))\n            vdmap = self.clone(callback=callback, shared_data=False,\n                               streams=vstreams)\n            type_counter[type(v)] += 1\n\n            # Remap source of streams\n            for stream in vstreams:\n                if stream.source is self:\n                    stream.source = vdmap\n            container[k] = vdmap\n\n        unmapped_streams = [repr(stream) for stream in self.streams\n                            if (stream.source is self) and\n                            (stream not in remapped_streams)\n                            and stream.linked]\n        if unmapped_streams:\n            raise ValueError(\n                'The following streams are set to be automatically '\n                'linked to a plot, but no stream_mapping specifying '\n                'which item in the (Nd)Layout to link it to was found:\\n%s'\n                % ', '.join(unmapped_streams)\n            )\n        return container", "response": "Unpacks DynamicMaps into container of DynamicMaps with consistent number of items."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef groupby(self, dimensions=None, container_type=None, group_type=None, **kwargs):\n        if dimensions is None:\n            dimensions = self.kdims\n        if not isinstance(dimensions, (list, tuple)):\n            dimensions = [dimensions]\n\n        container_type = container_type if container_type else type(self)\n        group_type = group_type if group_type else type(self)\n\n        outer_kdims = [self.get_dimension(d) for d in dimensions]\n        inner_kdims = [d for d in self.kdims if not d in outer_kdims]\n\n        outer_dynamic = issubclass(container_type, DynamicMap)\n        inner_dynamic = issubclass(group_type, DynamicMap)\n\n        if ((not outer_dynamic and any(not d.values for d in outer_kdims)) or\n            (not inner_dynamic and any(not d.values for d in inner_kdims))):\n            raise Exception('Dimensions must specify sampling via '\n                            'values to apply a groupby')\n\n        if outer_dynamic:\n            def outer_fn(*outer_key, **dynkwargs):\n                if inner_dynamic:\n                    def inner_fn(*inner_key, **dynkwargs):\n                        outer_vals = zip(outer_kdims, util.wrap_tuple(outer_key))\n                        inner_vals = zip(inner_kdims, util.wrap_tuple(inner_key))\n                        inner_sel = [(k.name, v) for k, v in inner_vals]\n                        outer_sel = [(k.name, v) for k, v in outer_vals]\n                        return self.select(**dict(inner_sel+outer_sel))\n                    return self.clone([], callback=inner_fn, kdims=inner_kdims)\n                else:\n                    dim_vals = [(d.name, d.values) for d in inner_kdims]\n                    dim_vals += [(d.name, [v]) for d, v in\n                                   zip(outer_kdims, util.wrap_tuple(outer_key))]\n                    with item_check(False):\n                        selected = HoloMap(self.select(**dict(dim_vals)))\n                        return group_type(selected.reindex(inner_kdims))\n            if outer_kdims:\n                return self.clone([], callback=outer_fn, kdims=outer_kdims)\n            else:\n                return outer_fn(())\n        else:\n            outer_product = itertools.product(*[self.get_dimension(d).values\n                                                for d in dimensions])\n            groups = []\n            for outer in outer_product:\n                outer_vals = [(d.name, [o]) for d, o in zip(outer_kdims, outer)]\n                if inner_dynamic or not inner_kdims:\n                    def inner_fn(outer_vals, *key, **dynkwargs):\n                        inner_dims = zip(inner_kdims, util.wrap_tuple(key))\n                        inner_vals = [(d.name, k) for d, k in inner_dims]\n                        return self.select(**dict(outer_vals+inner_vals)).last\n                    if inner_kdims or self.streams:\n                        group = self.clone(callback=partial(inner_fn, outer_vals),\n                                           kdims=inner_kdims)\n                    else:\n                        group = inner_fn(outer_vals, ())\n                    groups.append((outer, group))\n                else:\n                    inner_vals = [(d.name, self.get_dimension(d).values)\n                                     for d in inner_kdims]\n                    with item_check(False):\n                        selected = HoloMap(self.select(**dict(outer_vals+inner_vals)))\n                        group = group_type(selected.reindex(inner_kdims))\n                    groups.append((outer, group))\n            return container_type(groups, kdims=outer_kdims)", "response": "Groups DynamicMap by one or more dimensions returning a dictionary - like object containing the groups of the specified dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngrouping data by supplied dimensions laying the groups along the dimensions out in a GridSpace.", "response": "def grid(self, dimensions=None, **kwargs):\n        \"\"\"\n        Groups data by supplied dimension(s) laying the groups along\n        the dimension(s) out in a GridSpace.\n\n        Args:\n        dimensions: Dimension/str or list\n            Dimension or list of dimensions to group by\n\n        Returns:\n        grid: GridSpace\n            GridSpace with supplied dimensions\n        \"\"\"\n        return self.groupby(dimensions, container_type=GridSpace, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef layout(self, dimensions=None, **kwargs):\n        return self.groupby(dimensions, container_type=NdLayout, **kwargs)", "response": "Groups data along the supplied dimension and returns a NdLayout with supplied dimensions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngroup data by supplied dimension and overlays each group along the dimension ( s ).", "response": "def overlay(self, dimensions=None, **kwargs):\n        \"\"\"Group by supplied dimension(s) and overlay each group\n\n        Groups data by supplied dimension(s) overlaying the groups\n        along the dimension(s).\n\n        Args:\n            dimensions: Dimension(s) of dimensions to group by\n\n        Returns:\n            NdOverlay object(s) with supplied dimensions\n        \"\"\"\n        if dimensions is None:\n            dimensions = self.kdims\n        else:\n            if not isinstance(dimensions, (list, tuple)):\n                dimensions = [dimensions]\n            dimensions = [self.get_dimension(d, strict=True)\n                          for d in dimensions]\n        dims = [d for d in self.kdims if d not in dimensions]\n        return self.groupby(dims, group_type=NdOverlay)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, **kwargs):\n        \"\"\"Computes and adjoins histogram along specified dimension(s).\n\n        Defaults to first value dimension if present otherwise falls\n        back to first key dimension.\n\n        Args:\n            dimension: Dimension(s) to compute histogram on\n            num_bins (int, optional): Number of bins\n            bin_range (tuple optional): Lower and upper bounds of bins\n            adjoin (bool, optional): Whether to adjoin histogram\n\n        Returns:\n            AdjointLayout of DynamicMap and adjoined histogram if\n            adjoin=True, otherwise just the histogram\n        \"\"\"\n        def dynamic_hist(obj, **dynkwargs):\n            if isinstance(obj, (NdOverlay, Overlay)):\n                index = kwargs.get('index', 0)\n                obj = obj.get(index)\n            return obj.hist(num_bins=num_bins, bin_range=bin_range,\n                            adjoin=False, **kwargs)\n\n        from ..util import Dynamic\n        hist = Dynamic(self, streams=self.streams, link_inputs=False,\n                       operation=dynamic_hist)\n        if adjoin:\n            return self << hist\n        else:\n            return hist", "response": "Computes and adjoins histogram along specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreorders the key dimensions on DynamicMap.", "response": "def reindex(self, kdims=[], force=False):\n        \"\"\"Reorders key dimensions on DynamicMap\n\n        Create a new object with a reordered set of key dimensions.\n        Dropping dimensions is not allowed on a DynamicMap.\n\n        Args:\n            kdims: List of dimensions to reindex the mapping with\n            force: Not applicable to a DynamicMap\n\n        Returns:\n            Reindexed DynamicMap\n        \"\"\"\n        if not isinstance(kdims, list):\n            kdims = [kdims]\n        kdims = [self.get_dimension(kd, strict=True) for kd in kdims]\n        dropped = [kd for kd in self.kdims if kd not in kdims]\n        if dropped:\n            raise ValueError(\"DynamicMap does not allow dropping dimensions, \"\n                             \"reindex may only be used to reorder dimensions.\")\n        return super(DynamicMap, self).reindex(kdims, force)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _transform_indices(self, key):\n        ndims = self.ndims\n        if all(not (isinstance(el, slice) or callable(el)) for el in key):\n            dim_inds = []\n            for dim in self.kdims:\n                dim_type = self.get_dimension_type(dim)\n                if isinstance(dim_type, type) and issubclass(dim_type, Number):\n                    dim_inds.append(self.get_dimension_index(dim))\n            str_keys = iter(key[i] for i in range(self.ndims)\n                            if i not in dim_inds)\n            num_keys = []\n            if len(dim_inds):\n                keys = list({tuple(k[i] if ndims > 1 else k for i in dim_inds)\n                             for k in self.keys()})\n                q = np.array([tuple(key[i] if ndims > 1 else key for i in dim_inds)])\n                idx = np.argmin([np.inner(q - np.array(x), q - np.array(x))\n                                 if len(dim_inds) == 2 else np.abs(q-x)\n                                     for x in keys])\n                num_keys = iter(keys[idx])\n            key = tuple(next(num_keys) if i in dim_inds else next(str_keys)\n                        for i in range(self.ndims))\n        elif any(not (isinstance(el, slice) or callable(el)) for el in key):\n            keys = self.keys()\n            for i, k in enumerate(key):\n                if isinstance(k, slice):\n                    continue\n                dim_keys = np.array([ke[i] for ke in keys])\n                if dim_keys.dtype.kind in 'OSU':\n                    continue\n                snapped_val = dim_keys[np.argmin(np.abs(dim_keys-k))]\n                key = list(key)\n                key[i] = snapped_val\n            key = tuple(key)\n        return key", "response": "Snaps indices into the GridSpace to the closest numeric coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef keys(self, full_grid=False):\n        keys = super(GridSpace, self).keys()\n        if self.ndims == 1 or not full_grid:\n            return keys\n        dim1_keys = sorted(set(k[0] for k in keys))\n        dim2_keys = sorted(set(k[1] for k in keys))\n        return [(d1, d2) for d1 in dim1_keys for d2 in dim2_keys]", "response": "Returns the keys of the GridSpace\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef last(self):\n        if self.type == HoloMap:\n            last_items = [(k, v.last if isinstance(v, HoloMap) else v)\n                          for (k, v) in self.data.items()]\n        else:\n            last_items = self.data\n        return self.clone(last_items)", "response": "Returns a new GridSpace with the last element in the list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shape(self):\n        \"Returns the 2D shape of the GridSpace as (rows, cols).\"\n        keys = self.keys()\n        if self.ndims == 1:\n            return (len(keys), 1)\n        return len(set(k[0] for k in keys)), len(set(k[1] for k in keys))", "response": "Returns the 2D shape of the GridSpace as ( rows cols )."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes hover data based on element dimension values.", "response": "def _get_hover_data(self, data, element):\n        \"\"\"\n        Initializes hover data based on Element dimension values.\n        \"\"\"\n        if 'hover' not in self.handles or self.static_source:\n            return\n\n        for k, v in self.overlay_dims.items():\n            dim = util.dimension_sanitizer(k.name)\n            if dim not in data:\n                data[dim] = [v for _ in range(len(list(data.values())[0]))]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_hover_data(self, data, element):\n        if 'hover' not in self.handles or self.static_source:\n            return\n\n        npath = len([vs for vs in data.values()][0])\n        for d in element.vdims:\n            dim = util.dimension_sanitizer(d.name)\n            if dim not in data:\n                if element.level is not None:\n                    data[dim] = np.full(npath, element.level)\n                elif element.interface.isscalar(element, d):\n                    data[dim] = element.dimension_values(d, expanded=False)\n                else:\n                    data[dim] = element.split(datatype='array', dimensions=[d])\n            elif isinstance(data[dim], np.ndarray) and data[dim].dtype.kind == 'M':\n                data[dim+'_dt_strings'] = [d.pprint_value(v) for v in data[dim]]\n\n        for k, v in self.overlay_dims.items():\n            dim = util.dimension_sanitizer(k.name)\n            if dim not in data:\n                data[dim] = [v for _ in range(len(list(data.values())[0]))]", "response": "Initializes hover data based on Element dimension values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing a new glyph object.", "response": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        plot_method = properties.pop('plot_method', None)\n        properties = mpl_to_bokeh(properties)\n        data = dict(properties, **mapping)\n        if self._has_holes:\n            plot_method = 'multi_polygons'\n        elif plot_method is None:\n            plot_method = self._plot_methods.get('single')\n        renderer = getattr(plot, plot_method)(**data)\n        if self.colorbar:\n            for k, v in list(self.handles.items()):\n                if not k.endswith('color_mapper'):\n                    continue\n                self._draw_colorbar(plot, v, k[:-12])\n        return renderer, renderer.glyph"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, index=0, **kwargs):\n        \"\"\"Computes and adjoins histogram along specified dimension(s).\n\n        Defaults to first value dimension if present otherwise falls\n        back to first key dimension.\n\n        Args:\n            dimension: Dimension(s) to compute histogram on\n            num_bins (int, optional): Number of bins\n            bin_range (tuple optional): Lower and upper bounds of bins\n            adjoin (bool, optional): Whether to adjoin histogram\n            index (int, optional): Index of layer to apply hist to\n\n        Returns:\n            AdjointLayout of element and histogram or just the\n            histogram\n        \"\"\"\n        valid_ind = isinstance(index, int) and (0 <= index < len(self))\n        valid_label = index in [el.label for el in self]\n        if not any([valid_ind, valid_label]):\n            raise TypeError(\"Please supply a suitable index or label for the histogram data\")\n\n        hists = self.get(index).hist(\n            adjoin=False, dimension=dimension, bin_range=bin_range,\n            num_bins=num_bins, **kwargs)\n        if not isinstance(hists, Layout):\n            hists = [hists]\n        if not isinstance(dimension, list):\n            dimension = ['Default']\n        if adjoin:\n            layout = self\n            for hist in hists:\n                layout = layout << hist\n            layout.main_layer = index\n        elif len(dimension) > 1:\n            layout = hists\n        else:\n            layout = hists[0]\n        return layout", "response": "Computes and adjoins histogram along specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dimension_values(self, dimension, expanded=True, flat=True):\n        values = []\n        found = False\n        for el in self:\n            if dimension in el.dimensions(label=True):\n                values.append(el.dimension_values(dimension))\n                found = True\n        if not found:\n            return super(CompositeOverlay, self).dimension_values(dimension, expanded, flat)\n        values = [v for v in values if v is not None and len(v)]\n        if not values:\n            return np.array()\n        vals = np.concatenate(values)\n        return vals if expanded else unique_array(vals)", "response": "Return the values along the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, identifier, default=None):\n        if isinstance(identifier, int):\n            values = list(self.data.values())\n            if 0 <= identifier < len(values):\n                return values[identifier]\n            else:\n                return default\n        return super(Overlay, self).get(identifier, default)", "response": "Get a particular layer in the Overlay using its path string\n            or an integer index."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeprecates method to collapse layers in the Overlay.", "response": "def collapse(self, function):\n        \"Deprecated method to collapse layers in the Overlay.\"\n        if config.future_deprecations:\n            self.param.warning('Overlay.collapse is deprecated, to'\n                               'collapse multiple elements use a HoloMap.')\n\n        elements = list(self)\n        types = [type(el) for el in elements]\n        values = [el.group for el in elements]\n        if not len(set(types)) == 1 and len(set(values)) == 1:\n            raise Exception(\"Overlay is not homogeneous in type or group \"\n                            \"and cannot be collapsed.\")\n        else:\n            return elements[0].clone(types[0].collapse_data([el.data for el in elements],\n                                                            function, self.kdims))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef displayable(obj):\n    if isinstance(obj, Overlay) and any(isinstance(o, (HoloMap, GridSpace))\n                                        for o in obj):\n        return False\n    if isinstance(obj, HoloMap):\n        return not (obj.type in [Layout, GridSpace, NdLayout, DynamicMap])\n    if isinstance(obj, (GridSpace, Layout, NdLayout)):\n        for el in obj.values():\n            if not displayable(el):\n                return False\n        return True\n    return True", "response": "Predicate that returns whether the object is displayable or not"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overlay_depth(obj):\n    if isinstance(obj, DynamicMap):\n        if isinstance(obj.last, CompositeOverlay):\n            return len(obj.last)\n        elif obj.last is None:\n            return None\n        return 1\n    else:\n        return 1", "response": "Computes the depth of a DynamicMap overlay."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_overlayable_zorders(obj, path=[]):\n    path = path+[obj]\n    zorder_map = defaultdict(list)\n\n    # Process non-dynamic layers\n    if not isinstance(obj, DynamicMap):\n        if isinstance(obj, CompositeOverlay):\n            for z, o in enumerate(obj):\n                zorder_map[z] = [o, obj]\n        elif isinstance(obj, HoloMap):\n            for el in obj.values():\n                if isinstance(el, CompositeOverlay):\n                    for k, v in compute_overlayable_zorders(el, path).items():\n                        zorder_map[k] += v + [obj]\n                else:\n                    zorder_map[0] += [obj, el]\n        else:\n            if obj not in zorder_map[0]:\n                zorder_map[0].append(obj)\n        return zorder_map\n\n    isoverlay = isinstance(obj.last, CompositeOverlay)\n    isdynoverlay = obj.callback._is_overlay\n    if obj not in zorder_map[0] and not isoverlay:\n        zorder_map[0].append(obj)\n    depth = overlay_depth(obj)\n\n    # Process the inputs of the DynamicMap callback\n    dmap_inputs = obj.callback.inputs if obj.callback.link_inputs else []\n    for z, inp in enumerate(dmap_inputs):\n        no_zorder_increment = False\n        if any(not (isoverlay_fn(p) or p.last is None) for p in path) and isoverlay_fn(inp):\n            # If overlay has been collapsed do not increment zorder\n            no_zorder_increment = True\n\n        input_depth = overlay_depth(inp)\n        if depth is not None and input_depth is not None and depth < input_depth:\n            # Skips branch of graph where the number of elements in an\n            # overlay has been reduced but still contains more than one layer\n            if depth > 1:\n                continue\n            else:\n                no_zorder_increment = True\n\n        # Recurse into DynamicMap.callback.inputs and update zorder_map\n        z = z if isdynoverlay else 0\n        deep_zorders = compute_overlayable_zorders(inp, path=path)\n        offset = max(zorder_map.keys())\n        for dz, objs in deep_zorders.items():\n            global_z = offset+z if no_zorder_increment else offset+dz+z\n            zorder_map[global_z] = list(unique_iterator(zorder_map[global_z]+objs))\n\n    # If object branches but does not declare inputs (e.g. user defined\n    # DynamicMaps returning (Nd)Overlay) add the items on the DynamicMap.last\n    found = any(isinstance(p, DynamicMap) and p.callback._is_overlay for p in path)\n    linked =  any(isinstance(s, LinkedStream) and s.linked for s in obj.streams)\n    if (found or linked) and isoverlay and not isdynoverlay:\n        offset = max(zorder_map.keys())\n        for z, o in enumerate(obj.last):\n            if isoverlay and linked:\n                zorder_map[offset+z].append(obj)\n            if o not in zorder_map[offset+z]:\n                zorder_map[offset+z].append(o)\n    return zorder_map", "response": "Given a CompositeOverlay or HoloMap object and a list of objects return a mapping between the z - order of each object and the list of objects that are associated with that object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine if any components of a DynamicMap are overlaid dynamically.", "response": "def is_dynamic_overlay(dmap):\n    \"\"\"\n    Traverses a DynamicMap graph and determines if any components\n    were overlaid dynamically (i.e. by * on a DynamicMap).\n    \"\"\"\n    if not isinstance(dmap, DynamicMap):\n        return False\n    elif dmap.callback._is_overlay:\n        return True\n    else:\n        return any(is_dynamic_overlay(dm) for dm in dmap.callback.inputs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsplits a DynamicMap into a list of original component layers that are overlaid by using OverlayPlot.", "response": "def split_dmap_overlay(obj, depth=0):\n    \"\"\"\n    Splits a DynamicMap into the original component layers it was\n    constructed from by traversing the graph to search for dynamically\n    overlaid components (i.e. constructed by using * on a DynamicMap).\n    Useful for assigning subplots of an OverlayPlot the streams that\n    are responsible for driving their updates. Allows the OverlayPlot\n    to determine if a stream update should redraw a particular\n    subplot.\n    \"\"\"\n    layers = []\n    if isinstance(obj, DynamicMap):\n        if issubclass(obj.type, NdOverlay) and not depth:\n            for v in obj.last.values():\n                layers.append(obj)\n        elif issubclass(obj.type, Overlay):\n            if obj.callback.inputs and is_dynamic_overlay(obj):\n                for inp in obj.callback.inputs:\n                    layers += split_dmap_overlay(inp, depth+1)\n            else:\n                for v in obj.last.values():\n                    layers.append(obj)\n        else:\n            layers.append(obj)\n        return layers\n    if isinstance(obj, Overlay):\n        for k, v in obj.items():\n            layers.append(v)\n    else:\n        layers.append(obj)\n    return layers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize_dynamic(obj):\n    dmaps = obj.traverse(lambda x: x, specs=[DynamicMap])\n    for dmap in dmaps:\n        if dmap.unbounded:\n            # Skip initialization until plotting code\n            continue\n        if not len(dmap):\n            dmap[dmap._initial_key()]", "response": "Initializes all DynamicMaps contained by the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_plot_frame(map_obj, key_map, cached=False):\n    if map_obj.kdims and len(map_obj.kdims) == 1 and map_obj.kdims[0] == 'Frame':\n        # Special handling for static plots\n        return map_obj.last\n    key = tuple(key_map[kd.name] for kd in map_obj.kdims if kd.name in key_map)\n    if key in map_obj.data and cached:\n        return map_obj.data[key]\n    else:\n        try:\n            return map_obj[key]\n        except KeyError:\n            return None\n        except StopIteration as e:\n            raise e\n        except Exception:\n            print(traceback.format_exc())\n            return None", "response": "Returns the current frame in a nested Dimensioned object given a key mapping."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nested_plot_frame(obj, key_map, cached=False):\n    clone = obj.map(lambda x: x)\n\n    # Ensure that DynamicMaps in the cloned frame have\n    # identical callback inputs to allow memoization to work\n    for it1, it2 in zip(obj.traverse(lambda x: x), clone.traverse(lambda x: x)):\n        if isinstance(it1, DynamicMap):\n            with disable_constant(it2.callback):\n                it2.callback.inputs = it1.callback.inputs\n    with item_check(False):\n        return clone.map(lambda x: get_plot_frame(x, key_map, cached=cached),\n                         [DynamicMap, HoloMap], clone=False)", "response": "Returns a nested object where each element in the object is a single frame."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef undisplayable_info(obj, html=False):\n    \"Generate helpful message regarding an undisplayable object\"\n\n    collate = '<tt>collate</tt>' if html else 'collate'\n    info = \"For more information, please consult the Composing Data tutorial (http://git.io/vtIQh)\"\n    if isinstance(obj, HoloMap):\n        error = \"HoloMap of %s objects cannot be displayed.\" % obj.type.__name__\n        remedy = \"Please call the %s method to generate a displayable object\" % collate\n    elif isinstance(obj, Layout):\n        error = \"Layout containing HoloMaps of Layout or GridSpace objects cannot be displayed.\"\n        remedy = \"Please call the %s method on the appropriate elements.\" % collate\n    elif isinstance(obj, GridSpace):\n        error = \"GridSpace containing HoloMaps of Layouts cannot be displayed.\"\n        remedy = \"Please call the %s method on the appropriate elements.\" % collate\n\n    if not html:\n        return '\\n'.join([error, remedy, info])\n    else:\n        return \"<center>{msg}</center>\".format(msg=('<br>'.join(\n            ['<b>%s</b>' % error, remedy, '<i>%s</i>' % info])))", "response": "Generate helpful message regarding an undisplayable object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the sizes of a single resource in a node.", "response": "def compute_sizes(sizes, size_fn, scaling_factor, scaling_method, base_size):\n    \"\"\"\n    Scales point sizes according to a scaling factor,\n    base size and size_fn, which will be applied before\n    scaling.\n    \"\"\"\n    if sizes.dtype.kind not in ('i', 'f'):\n        return None\n    if scaling_method == 'area':\n        pass\n    elif scaling_method == 'width':\n        scaling_factor = scaling_factor**2\n    else:\n        raise ValueError(\n            'Invalid value for argument \"scaling_method\": \"{}\". '\n            'Valid values are: \"width\", \"area\".'.format(scaling_method))\n    sizes = size_fn(sizes)\n    return (base_size*scaling_factor*sizes)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a padding value supplied as a tuple or number and returns the x - y - and z - axis padding values for x - y - and z - axis.", "response": "def get_axis_padding(padding):\n    \"\"\"\n    Process a padding value supplied as a tuple or number and returns\n    padding values for x-, y- and z-axis.\n    \"\"\"\n    if isinstance(padding, tuple):\n        if len(padding) == 2:\n            xpad, ypad = padding\n            zpad = 0\n        elif len(padding) == 3:\n            xpad, ypad, zpad = padding\n        else:\n            raise ValueError('Padding must be supplied as an number applied '\n                             'to all axes or a length two or three tuple '\n                             'corresponding to the x-, y- and optionally z-axis')\n    else:\n        xpad, ypad, zpad = (padding,)*3\n    return (xpad, ypad, zpad)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_minimum_span(low, high, span):\n    if is_number(low) and low == high:\n        if isinstance(low, np.datetime64):\n            span = span * np.timedelta64(1, 's')\n        low, high = low-span, high+span\n    return low, high", "response": "Returns the minimum span of the given low and high values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the data soft - and hard - range along a dimension given an element and a dictionary of ranges.", "response": "def get_range(element, ranges, dimension):\n    \"\"\"\n    Computes the data, soft- and hard-range along a dimension given\n    an element and a dictionary of ranges.\n    \"\"\"\n    if dimension and dimension != 'categorical':\n        if ranges and dimension.name in ranges:\n            drange = ranges[dimension.name]['data']\n            srange = ranges[dimension.name]['soft']\n            hrange = ranges[dimension.name]['hard']\n        else:\n            drange = element.range(dimension, dimension_range=False)\n            srange = dimension.soft_range\n            hrange = dimension.range\n    else:\n        drange = srange = hrange = (np.NaN, np.NaN)\n    return drange, srange, hrange"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck whether range1 is within the range specified by range2.", "response": "def within_range(range1, range2):\n    \"\"\"Checks whether range1 is within the range specified by range2.\"\"\"\n    range1 = [r if isfinite(r) else None for r in range1]\n    range2 = [r if isfinite(r) else None for r in range2]\n    return ((range1[0] is None or range2[0] is None or range1[0] >= range2[0]) and\n            (range1[1] is None or range2[1] is None or range1[1] <= range2[1]))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the common mode of the dynamic maps in given composite object", "response": "def get_dynamic_mode(composite):\n    \"Returns the common mode of the dynamic maps in given composite object\"\n    dynmaps = composite.traverse(lambda x: x, [DynamicMap])\n    holomaps = composite.traverse(lambda x: x, ['HoloMap'])\n    dynamic_unbounded = any(m.unbounded for m in dynmaps)\n    if holomaps:\n        validate_unbounded_mode(holomaps, dynmaps)\n    elif dynamic_unbounded and not holomaps:\n        raise Exception(\"DynamicMaps in unbounded mode must be displayed alongside \"\n                        \"a HoloMap to define the sampling.\")\n    return dynmaps and not holomaps, dynamic_unbounded"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef initialize_unbounded(obj, dimensions, key):\n    select = dict(zip([d.name for d in dimensions], key))\n    try:\n        obj.select([DynamicMap], **select)\n    except KeyError:\n        pass", "response": "Initializes any DynamicMaps in unbounded mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dynamic_update(plot, subplot, key, overlay, items):\n    match_spec = get_overlay_spec(overlay,\n                                  wrap_tuple(key),\n                                  subplot.current_frame)\n    specs = [(i, get_overlay_spec(overlay, wrap_tuple(k), el))\n             for i, (k, el) in enumerate(items)]\n    closest = closest_match(match_spec, specs)\n    if closest is None:\n        return closest, None, False\n    matched = specs[closest][1]\n    return closest, matched, match_spec == matched", "response": "Given a plot subplot and dynamic generated Overlay\n    find the closest matching Element for that plot and items."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef map_colors(arr, crange, cmap, hex=True):\n    if isinstance(crange, np.ndarray):\n        xsorted = np.argsort(crange)\n        ypos = np.searchsorted(crange, arr)\n        arr = xsorted[ypos]\n    else:\n        if isinstance(crange, tuple):\n            cmin, cmax = crange\n        else:\n            cmin, cmax = np.nanmin(arr), np.nanmax(arr)\n        arr = (arr - cmin) / (cmax-cmin)\n        arr = np.ma.array(arr, mask=np.logical_not(np.isfinite(arr)))\n    arr = cmap(arr)\n    if hex:\n        return rgb2hex(arr)\n    else:\n        return arr", "response": "Maps an array of values to RGB hex strings given a color range and colormap."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mplcmap_to_palette(cmap, ncolors=None, categorical=False):\n    from matplotlib.colors import Colormap, ListedColormap\n\n    ncolors = ncolors or 256\n    if not isinstance(cmap, Colormap):\n        import matplotlib.cm as cm\n        # Alias bokeh Category cmaps with mpl tab cmaps\n        if cmap.startswith('Category'):\n            cmap = cmap.replace('Category', 'tab')\n        try:\n            cmap = cm.get_cmap(cmap)\n        except:\n            cmap = cm.get_cmap(cmap.lower())\n    if isinstance(cmap, ListedColormap):\n        if categorical:\n            palette = [rgb2hex(cmap.colors[i%cmap.N]) for i in range(ncolors)]\n            return palette\n        elif cmap.N > ncolors:\n            palette = [rgb2hex(c) for c in cmap(np.arange(cmap.N))]\n            if len(palette) != ncolors:\n                palette = [palette[int(v)] for v in np.linspace(0, len(palette)-1, ncolors)]\n            return palette\n    return [rgb2hex(c) for c in cmap(np.linspace(0, 1, ncolors))]", "response": "Converts a matplotlib colormap to palette of RGB hex strings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninterpolating the color gradient between to hex colors", "response": "def linear_gradient(start_hex, finish_hex, n=10):\n    \"\"\"\n    Interpolates the color gradient between to hex colors\n    \"\"\"\n    s = hex2rgb(start_hex)\n    f = hex2rgb(finish_hex)\n    gradient = [s]\n    for t in range(1, n):\n        curr_vector = [int(s[j] + (float(t)/(n-1))*(f[j]-s[j])) for j in range(3)]\n        gradient.append(curr_vector)\n    return [rgb2hex([c/255. for c in rgb]) for rgb in gradient]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef polylinear_gradient(colors, n):\n    n_out = int(float(n) / (len(colors)-1))\n    gradient = linear_gradient(colors[0], colors[1], n_out)\n\n    if len(colors) == len(gradient):\n        return gradient\n\n    for col in range(1, len(colors) - 1):\n        next_colors = linear_gradient(colors[col], colors[col+1], n_out+1)\n        gradient += next_colors[1:] if len(next_colors) > 1 else next_colors\n    return gradient", "response": "Interpolates the color gradients between a list of hex colors."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _list_cmaps(provider=None, records=False):\n    if provider is None:\n        provider = providers\n    elif isinstance(provider, basestring):\n        if provider not in providers:\n            raise ValueError('Colormap provider %r not recognized, must '\n                             'be one of %r' % (provider, providers))\n        provider = [provider]\n\n    cmaps = []\n\n    def info(provider,names):\n        return [CMapInfo(name=n,provider=provider,category=None,source=None,bg=None) for n in names] \\\n               if records else list(names)\n\n    if 'matplotlib' in provider:\n        try:\n            import matplotlib.cm as cm\n            cmaps += info('matplotlib',\n                          [cmap for cmap in cm.cmap_d if not\n                           (cmap.startswith('cet_') or      # duplicates list below\n                            cmap.startswith('Vega') or      # deprecated in matplotlib=2.1\n                            cmap.startswith('spectral') )]) # deprecated in matplotlib=2.1\n        except:\n            pass\n    if 'bokeh' in provider:\n        try:\n            from bokeh import palettes\n            cmaps += info('bokeh', palettes.all_palettes)\n            cmaps += info('bokeh', [p+'_r' for p in palettes.all_palettes])\n        except:\n            pass\n    if 'colorcet' in provider:\n        try:\n            from colorcet import palette_n, glasbey_hv\n            cet_maps = palette_n.copy()\n            cet_maps['glasbey_hv'] = glasbey_hv # Add special hv-specific map\n            cmaps += info('colorcet', cet_maps) \n            cmaps += info('colorcet', [p+'_r' for p in cet_maps])\n        except:\n            pass\n    return sorted(unique_iterator(cmaps))", "response": "List available colormaps by combining matplotlib bokeh and colorcet colormaps or palettes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters colormaps with matplotlib.", "response": "def register_cmaps(category, provider, source, bg, names):\n    \"\"\"\n    Maintain descriptions of colormaps that include the following information:\n\n    name     - string name for the colormap\n    category - intended use or purpose, mostly following matplotlib\n    provider - package providing the colormap directly\n    source   - original source or creator of the colormaps\n    bg       - base/background color expected for the map\n               ('light','dark','medium','any' (unknown or N/A))\n    \"\"\"\n    for name in names:\n        bisect.insort(cmap_info, CMapInfo(name=name, provider=provider,\n                                          category=category, source=source,\n                                          bg=bg))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of colormaps matching the specified filters.", "response": "def list_cmaps(provider=None, records=False, name=None, category=None, source=None,\n               bg=None, reverse=None):\n    \"\"\"\n    Return colormap names matching the specified filters.\n    \"\"\"\n    # Only uses names actually imported and currently available\n    available = _list_cmaps(provider=provider, records=True)\n\n    matches = set()\n\n    for avail in available:\n        aname=avail.name\n        matched=False\n        basename=aname[:-2] if aname.endswith('_r') else aname\n\n        if (reverse is None or\n            (reverse==True and aname.endswith('_r')) or\n            (reverse==False and not aname.endswith('_r'))):\n            for r in cmap_info:\n               if (r.name==basename):\n                   matched=True\n\n                   # cmap_info stores only non-reversed info, so construct\n                   # suitable values for reversed version if appropriate\n                   r=r._replace(name=aname)\n                   if aname.endswith('_r') and (r.category != 'Diverging'):\n                       if r.bg=='light':\n                           r=r._replace(bg='dark')\n                       elif r.bg=='dark':\n                           r=r._replace(bg='light')\n\n                   if ((    name is None or     name in r.name) and\n                       (provider is None or provider in r.provider) and\n                       (category is None or category in r.category) and\n                       (  source is None or   source in r.source) and\n                       (      bg is None or       bg in r.bg)):\n                       matches.add(r)\n            if not matched and (category is None or category=='Miscellaneous'):\n                # Return colormaps that exist but are not found in cmap_info\n                # under the 'Miscellaneous' category, with no source or bg\n                r = CMapInfo(aname,provider=avail.provider,category='Miscellaneous',source=None,bg=None)\n                matches.add(r)\n\n    # Return results sorted by category if category information is provided\n    if records:\n        return list(unique_iterator(python2sort(matches,\n                    key=lambda r: (r.category.split(\" \")[-1],r.bg,r.name.lower(),r.provider,r.source))))\n    else:\n        return list(unique_iterator(sorted([rec.name for rec in matches], key=lambda n:n.lower())))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting valid colormap specifications to a list of colors.", "response": "def process_cmap(cmap, ncolors=None, provider=None, categorical=False):\n    \"\"\"\n    Convert valid colormap specifications to a list of colors.\n    \"\"\"\n    providers_checked=\"matplotlib, bokeh, or colorcet\" if provider is None else provider\n\n    if isinstance(cmap, Cycle):\n        palette = [rgb2hex(c) if isinstance(c, tuple) else c for c in cmap.values]\n    elif isinstance(cmap, list):\n        palette = cmap\n    elif isinstance(cmap, basestring):\n        mpl_cmaps = _list_cmaps('matplotlib')\n        bk_cmaps = _list_cmaps('bokeh')\n        cet_cmaps = _list_cmaps('colorcet')\n        if provider=='matplotlib' or (provider is None and (cmap in mpl_cmaps or cmap.lower() in mpl_cmaps)):\n            palette = mplcmap_to_palette(cmap, ncolors, categorical)\n        elif provider=='bokeh' or (provider is None and (cmap in bk_cmaps or cmap.capitalize() in bk_cmaps)):\n            palette = bokeh_palette_to_palette(cmap, ncolors, categorical)\n        elif provider=='colorcet' or (provider is None and cmap in cet_cmaps):\n            from colorcet import palette\n            if cmap.endswith('_r'):\n                palette = list(reversed(palette[cmap[:-2]]))\n            else:\n                palette = palette[cmap]\n        else:\n            raise ValueError(\"Supplied cmap %s not found among %s colormaps.\" %\n                             (cmap,providers_checked))\n    else:\n        try:\n            # Try processing as matplotlib colormap\n            palette = mplcmap_to_palette(cmap, ncolors)\n        except:\n            palette = None\n    if not isinstance(palette, list):\n        raise TypeError(\"cmap argument %s expects a list, Cycle or valid %s colormap or palette.\"\n                        % (cmap,providers_checked))\n    if ncolors and len(palette) != ncolors:\n        return [palette[i%len(palette)] for i in range(ncolors)]\n    return palette"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap the supplied colors into bins defined by the supplied levels.", "response": "def color_intervals(colors, levels, clip=None, N=255):\n    \"\"\"\n    Maps the supplied colors into bins defined by the supplied levels.\n    If a clip tuple is defined the bins are clipped to the defined\n    range otherwise the range is computed from the levels and returned.\n\n    Arguments\n    ---------\n    colors: list\n      List of colors (usually hex string or named colors)\n    levels: list or array_like\n      Levels specifying the bins to map the colors to\n    clip: tuple (optional)\n      Lower and upper limits of the color range\n    N: int\n      Number of discrete colors to map the range onto\n\n    Returns\n    -------\n    cmap: list\n      List of colors\n    clip: tuple\n      Lower and upper bounds of the color range\n    \"\"\"\n    if len(colors) != len(levels)-1:\n        raise ValueError('The number of colors in the colormap '\n                         'must match the intervals defined in the '\n                         'color_levels, expected %d colors found %d.'\n                         % (N, len(colors)))\n    intervals = np.diff(levels)\n    cmin, cmax = min(levels), max(levels)\n    interval = cmax-cmin\n    cmap = []\n    for intv, c in zip(intervals, colors):\n        cmap += [c]*int(round(N*(intv/interval)))\n    if clip is not None:\n        clmin, clmax = clip\n        lidx = int(round(N*((clmin-cmin)/interval)))\n        uidx = int(round(N*((cmax-clmax)/interval)))\n        uidx = N-uidx\n        if lidx == uidx:\n            uidx = lidx+1\n        cmap = cmap[lidx:uidx]\n        if clmin == clmax:\n            idx = np.argmin(np.abs(np.array(levels)-clmin))\n            clip = levels[idx: idx+2] if len(levels) > idx+2 else levels[idx-1: idx+1]\n    return cmap, clip"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an axis label for one or more dimensions.", "response": "def dim_axis_label(dimensions, separator=', '):\n    \"\"\"\n    Returns an axis label for one or more dimensions.\n    \"\"\"\n    if not isinstance(dimensions, list): dimensions = [dimensions]\n    return separator.join([d.pprint_label for d in dimensions])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef attach_streams(plot, obj, precedence=1.1):\n    def append_refresh(dmap):\n        for stream in get_nested_streams(dmap):\n            if plot.refresh not in stream._subscribers:\n                stream.add_subscriber(plot.refresh, precedence)\n    return obj.traverse(append_refresh, [DynamicMap])", "response": "Attaches plot refresh to all streams on the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the supplied attribute on the object. Supports Dimensioned and DimensionedPlot types.", "response": "def traverse_setter(obj, attribute, value):\n    \"\"\"\n    Traverses the object and sets the supplied attribute on the\n    object. Supports Dimensioned and DimensionedPlot types.\n    \"\"\"\n    obj.traverse(lambda x: setattr(x, attribute, value))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_min_distance(element):\n    try:\n        from scipy.spatial.distance import pdist\n        return pdist(element.array([0, 1])).min()\n    except:\n        return _get_min_distance_numpy(element)", "response": "Gets the minimum sampling distance of the x - and y - coordinates\n    in a grid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the paths for a directed graph.", "response": "def get_directed_graph_paths(element, arrow_length):\n    \"\"\"\n    Computes paths for a directed path which include an arrow to\n    indicate the directionality of each edge.\n    \"\"\"\n    edgepaths = element._split_edgepaths\n    edges = edgepaths.split(datatype='array', dimensions=edgepaths.kdims)\n    arrows = []\n    for e in edges:\n        sx, sy = e[0]\n        ex, ey = e[1]\n        rad = np.arctan2(ey-sy, ex-sx)\n        xa0 = ex - np.cos(rad+np.pi/8)*arrow_length\n        ya0 = ey - np.sin(rad+np.pi/8)*arrow_length\n        xa1 = ex - np.cos(rad-np.pi/8)*arrow_length\n        ya1 = ey - np.sin(rad-np.pi/8)*arrow_length\n        arrow = np.array([(sx, sy), (ex, ey), (np.nan, np.nan),\n                          (xa0, ya0), (ex, ey), (xa1, ya1)])\n        arrows.append(arrow)\n    return arrows"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert RGB tuple to hex.", "response": "def rgb2hex(rgb):\n    \"\"\"\n    Convert RGB(A) tuple to hex.\n    \"\"\"\n    if len(rgb) > 3:\n        rgb = rgb[:-1]\n    return \"#{0:02x}{1:02x}{2:02x}\".format(*(int(v*255) for v in rgb))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the key to look up a dimension range.", "response": "def dim_range_key(eldim):\n    \"\"\"\n    Returns the key to look up a dimension range.\n    \"\"\"\n    if isinstance(eldim, dim):\n        dim_name = repr(eldim)\n        if dim_name.startswith(\"'\") and dim_name.endswith(\"'\"):\n            dim_name = dim_name[1:-1]\n    else:\n        dim_name = eldim.name\n    return dim_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a Dimension creates a bokeh widget for that Dimension.", "response": "def create_widget(self, dim, holomap=None, editable=False):\n        \"\"\"\"\n        Given a Dimension creates bokeh widgets to select along that\n        dimension. For numeric data a slider widget is created which\n        may be either discrete, if a holomap is supplied or the\n        Dimension.values are set, or a continuous widget for\n        DynamicMaps. If the slider is discrete the returned mapping\n        defines a mapping between values and labels making it possible\n        sync the two slider and label widgets. For non-numeric data\n        a simple dropdown selection widget is generated.\n        \"\"\"\n        label, mapping = None, None\n        if holomap is None:\n            if dim.values:\n                if dim.default is None:\n                    default = dim.values[0]\n                elif dim.default not in dim.values:\n                    raise ValueError(\"%s dimension default %r is not in dimension values: %s\"\n                                     % (dim, dim.default, dim.values))\n                else:\n                    default = dim.default\n                value = dim.values.index(default)\n\n                if all(isnumeric(v) for v in dim.values):\n                    values = sorted(dim.values)\n                    labels = [unicode(dim.pprint_value(v)) for v in values]\n                    if editable:\n                        label = AutocompleteInput(value=labels[value], completions=labels,\n                                                  title=dim.pprint_label)\n                    else:\n                        label = Div(text='<b>%s</b>' % dim.pprint_value_string(labels[value]))\n                    widget = Slider(value=value, start=0, end=len(dim.values)-1, title=None, step=1)\n                    mapping = list(enumerate(zip(values, labels)))\n                else:\n                    values = [(v, dim.pprint_value(v)) for v in dim.values]\n                    widget = Select(title=dim.pprint_label, value=values[value][0],\n                                    options=values)\n            else:\n                start = dim.soft_range[0] if dim.soft_range[0] else dim.range[0]\n                end = dim.soft_range[1] if dim.soft_range[1] else dim.range[1]\n                dim_range = end - start\n                int_type = isinstance(dim.type, type) and issubclass(dim.type, int)\n                if dim.step is not None:\n                    step = dim.step\n                elif isinstance(dim_range, int) or int_type:\n                    step = 1\n                else:\n                    step = 10**((round(math.log10(dim_range))-3))\n\n                if dim.default is None:\n                    default = start\n                elif (dim.default < start or dim.default > end):\n                    raise ValueError(\"%s dimension default %r is not in the provided range: %s\"\n                                     % (dim, dim.default, (start, end)))\n                else:\n                    default = dim.default\n\n                if editable:\n                    label = TextInput(value=str(default), title=dim.pprint_label)\n                else:\n                    label = Div(text='<b>%s</b>' % dim.pprint_value_string(default))\n                widget = Slider(value=default, start=start,\n                                end=end, step=step, title=None)\n        else:\n            values = (dim.values if dim.values else\n                      list(unique_array(holomap.dimension_values(dim.name))))\n            if dim.default is None:\n                default = values[0]\n            elif dim.default not in values:\n                raise ValueError(\"%s dimension default %r is not in dimension values: %s\"\n                                 % (dim, dim.default, values))\n            else:\n                default = dim.default\n            if isinstance(values[0], np.datetime64) or isnumeric(values[0]):\n                values = sorted(values)\n                labels = [dim.pprint_value(v) for v in values]\n                value = values.index(default)\n                if editable:\n                    label = AutocompleteInput(value=labels[value], completions=labels,\n                                              title=dim.pprint_label)\n                else:\n                    label = Div(text='<b>%s</b>' % (dim.pprint_value_string(labels[value])))\n                widget = Slider(value=value, start=0, end=len(values)-1, title=None, step=1)\n            else:\n                labels = [dim.pprint_value(v) for v in values]\n                widget = Select(title=dim.pprint_label, value=default,\n                                options=list(zip(values, labels)))\n            mapping = list(enumerate(zip(values, labels)))\n        return widget, label, mapping"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_widgets(self):\n        widgets = OrderedDict()\n        mappings = {}\n        for dim in self.mock_obj.kdims:\n            holomap = None if self.plot.dynamic else self.mock_obj\n            widget, label, mapping = self.create_widget(dim, holomap, self.editable)\n            if label is not None and not isinstance(label, Div):\n                label.on_change('value', partial(self.on_change, dim, 'label'))\n            widget.on_change('value', partial(self.on_change, dim, 'widget'))\n            widgets[dim.pprint_label] = (label, widget)\n            if mapping:\n                mappings[dim.pprint_label] = OrderedDict(mapping)\n        return widgets, mappings", "response": "Creates a set of widgets representing the dimensions on the plot object used to instantiate the widgets class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self):\n        if not self._queue:\n            return\n\n        dim, widget_type, attr, old, new = self._queue[-1]\n        self._queue = []\n        dim_label = dim.pprint_label\n\n        label, widget = self.widgets[dim_label]\n        if widget_type == 'label':\n            if isinstance(label, AutocompleteInput):\n                value = [new]\n                widget.value = value\n            else:\n                widget.value = float(new)\n        elif label:\n            lookups = self.lookups.get(dim_label)\n            if not self.editable:\n                if lookups:\n                    new = lookups[widget.value][1]\n                label.text = '<b>%s</b>' % dim.pprint_value_string(new)\n            elif isinstance(label, AutocompleteInput):\n                text = lookups[new][1]\n                label.value = text\n            else:\n                label.value = dim.pprint_value(new)\n\n        key = []\n        for dim, (label, widget) in self.widgets.items():\n            lookups = self.lookups.get(dim)\n            if label and lookups:\n                val = lookups[widget.value][0]\n            else:\n                val = widget.value\n            key.append(val)\n        key = wrap_tuple_streams(tuple(key), self.plot.dimensions,\n                                 self.plot.streams)\n        self.plot.update(key)\n        self._active = False", "response": "Update the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the figure in html format on the oment", "response": "def _plot_figure(self, idx, fig_format='json'):\n        \"\"\"\n        Returns the figure in html format on the\n        first call and\n        \"\"\"\n        self.plot.update(idx)\n        if self.embed:\n            patch = self.renderer.diff(self.plot, binary=False)\n            msg = serialize_json(dict(content=patch.content,\n                                      root=self.plot.state._id))\n            return msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing the list of tools to be supplied to the plot.", "response": "def _init_tools(self, element, callbacks=[]):\n        \"\"\"\n        Processes the list of tools to be supplied to the plot.\n        \"\"\"\n        tooltips, hover_opts = self._hover_opts(element)\n        tooltips = [(ttp.pprint_label, '@{%s}' % util.dimension_sanitizer(ttp.name))\n                    if isinstance(ttp, Dimension) else ttp for ttp in tooltips]\n        if not tooltips: tooltips = None\n\n        callbacks = callbacks+self.callbacks\n        cb_tools, tool_names = [], []\n        hover = False\n        for cb in callbacks:\n            for handle in cb.models+cb.extra_models:\n                if handle and handle in known_tools:\n                    tool_names.append(handle)\n                    if handle == 'hover':\n                        tool = tools.HoverTool(\n                            tooltips=tooltips, tags=['hv_created'],\n                            **hover_opts)\n                        hover = tool\n                    else:\n                        tool = known_tools[handle]()\n                    cb_tools.append(tool)\n                    self.handles[handle] = tool\n\n        tool_list = [\n            t for t in cb_tools + self.default_tools + self.tools\n            if t not in tool_names]\n\n        copied_tools = []\n        for tool in tool_list:\n            if isinstance(tool, tools.Tool):\n                properties = tool.properties_with_values(include_defaults=False)\n                tool = type(tool)(**properties)\n            copied_tools.append(tool)\n\n        hover_tools = [t for t in copied_tools if isinstance(t, tools.HoverTool)]\n        if 'hover' in copied_tools:\n            hover = tools.HoverTool(tooltips=tooltips, tags=['hv_created'], **hover_opts)\n            copied_tools[copied_tools.index('hover')] = hover\n        elif any(hover_tools):\n            hover = hover_tools[0]\n        if hover:\n            self.handles['hover'] = hover\n        return copied_tools"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_hover_data(self, data, element, dimensions=None):\n        if 'hover' not in self.handles or self.static_source:\n            return\n\n        for d in (dimensions or element.dimensions()):\n            dim = util.dimension_sanitizer(d.name)\n            if dim not in data:\n                data[dim] = element.dimension_values(d)\n            values = np.asarray(data[dim])\n            if (values.dtype.kind == 'M' or (\n                    len(values) and isinstance(values[0], util.datetime_types))):\n                data[dim+'_dt_strings'] = [d.pprint_value(v) for v in values]\n\n        for k, v in self.overlay_dims.items():\n            dim = util.dimension_sanitizer(k.name)\n            if dim not in data:\n                data[dim] = [v for _ in range(len(list(data.values())[0]))]", "response": "Initializes hover data based on element dimensions and overlay_dims."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _merge_ranges(self, plots, xspecs, yspecs):\n        plot_ranges = {}\n        for plot in plots:\n            if plot is None:\n                continue\n            if hasattr(plot, 'x_range') and plot.x_range.tags and xspecs is not None:\n                if match_dim_specs(plot.x_range.tags[0], xspecs):\n                    plot_ranges['x_range'] = plot.x_range\n                if match_dim_specs(plot.x_range.tags[0], yspecs):\n                    plot_ranges['y_range'] = plot.x_range\n            if hasattr(plot, 'y_range') and plot.y_range.tags and yspecs is not None:\n                if match_dim_specs(plot.y_range.tags[0], yspecs):\n                    plot_ranges['y_range'] = plot.y_range\n                if match_dim_specs(plot.y_range.tags[0], xspecs):\n                    plot_ranges['x_range'] = plot.y_range\n        return plot_ranges", "response": "Given a list of plots return axes that are shared by matching the dimensions specs stored\n        as tags on the dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the dimensions corresponding to each axis.", "response": "def _get_axis_dims(self, element):\n        \"\"\"Returns the dimensions corresponding to each axis.\n\n        Should return a list of dimensions or list of lists of\n        dimensions, which will be formatted to label the axis\n        and to link axes.\n        \"\"\"\n        dims = element.dimensions()[:2]\n        if len(dims) == 1:\n            return dims + [None, None]\n        else:\n            return dims + [None]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes the basic Bokeh figure to draw Element into and sets basicPlotProperties and basicPlotProperties.", "response": "def _init_plot(self, key, element, plots, ranges=None):\n        \"\"\"\n        Initializes Bokeh figure to draw Element into and sets basic\n        figure and axis attributes including axes types, labels,\n        titles and plot height and width.\n        \"\"\"\n        subplots = list(self.subplots.values()) if self.subplots else []\n\n        axis_types, labels, plot_ranges = self._axes_props(plots, subplots, element, ranges)\n        xlabel, ylabel, _ = labels\n        x_axis_type, y_axis_type = axis_types\n        properties = dict(plot_ranges)\n        properties['x_axis_label'] = xlabel if 'x' in self.labelled or self.xlabel else ' '\n        properties['y_axis_label'] = ylabel if 'y' in self.labelled or self.ylabel else ' '\n\n        if not self.show_frame:\n            properties['outline_line_alpha'] = 0\n\n        if self.show_title and self.adjoined is None:\n            title = self._format_title(key, separator=' ')\n        else:\n            title = ''\n\n        if self.toolbar != 'disable':\n            tools = self._init_tools(element)\n            properties['tools'] = tools\n            properties['toolbar_location'] = self.toolbar\n        else:\n            properties['tools'] = []\n            properties['toolbar_location'] = None\n\n        if self.renderer.webgl:\n            properties['output_backend'] = 'webgl'\n\n        properties.update(**self._plot_properties(key, element))\n\n        with warnings.catch_warnings():\n            # Bokeh raises warnings about duplicate tools but these\n            # are not really an issue\n            warnings.simplefilter('ignore', UserWarning)\n            return bokeh.plotting.Figure(x_axis_type=x_axis_type,\n                                         y_axis_type=y_axis_type, title=title,\n                                         **properties)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the properties of the specific element of the current object.", "response": "def _plot_properties(self, key, element):\n        \"\"\"\n        Returns a dictionary of plot properties.\n        \"\"\"\n        init = 'plot' not in self.handles\n        size_multiplier = self.renderer.size/100.\n        options = self._traverse_options(element, 'plot', ['width', 'height'], defaults=False)\n\n        logger = self.param if init else None\n        aspect_props, dimension_props = compute_layout_properties(\n            self.width, self.height, self.frame_width, self.frame_height,\n            options['width'], options['height'], self.aspect, self.data_aspect,\n            self.responsive, size_multiplier, logger=logger)\n\n        if not init:\n            if aspect_props['aspect_ratio'] is None:\n                aspect_props['aspect_ratio'] = self.state.aspect_ratio\n\n        if self.dynamic and aspect_props['match_aspect']:\n            # Sync the plot size on dynamic plots to support accurate\n            # scaling of dimension ranges\n            stream = PlotSize(subscribers=[self._update_size])\n            self.callbacks.append(PlotSizeCallback(self, [stream], None))\n\n        plot_props = {\n            'margin':        self.margin,\n            'max_width':     self.max_width,\n            'max_height':    self.max_height,\n            'min_width':     self.min_width,\n            'min_height':    self.min_height\n        }\n        plot_props.update(aspect_props)\n        if not self.drawn:\n            plot_props.update(dimension_props)\n\n        if self.bgcolor:\n            plot_props['background_fill_color'] = self.bgcolor\n        if self.border is not None:\n            for p in ['left', 'right', 'top', 'bottom']:\n                plot_props['min_border_'+p] = self.border\n        lod = dict(self.param.defaults().get('lod', {}), **self.lod)\n        for lod_prop, v in lod.items():\n            plot_props['lod_'+lod_prop] = v\n        return plot_props"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nactivate the list of active tools", "response": "def _set_active_tools(self, plot):\n        \"Activates the list of active tools\"\n        for tool in self.active_tools:\n            if isinstance(tool, util.basestring):\n                tool_type = TOOL_TYPES[tool]\n                matching = [t for t in plot.toolbar.tools\n                            if isinstance(t, tool_type)]\n                if not matching:\n                    self.param.warning('Tool of type %r could not be found '\n                                       'and could not be activated by default.'\n                                       % tool)\n                    continue\n                tool = matching[0]\n            if isinstance(tool, tools.Drag):\n                plot.toolbar.active_drag = tool\n            if isinstance(tool, tools.Scroll):\n                plot.toolbar.active_scroll = tool\n            if isinstance(tool, tools.Tap):\n                plot.toolbar.active_tap = tool\n            if isinstance(tool, tools.Inspection):\n                plot.toolbar.active_inspect.append(tool)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _axis_properties(self, axis, key, plot, dimension=None,\n                         ax_mapping={'x': 0, 'y': 1}):\n        \"\"\"\n        Returns a dictionary of axis properties depending\n        on the specified axis.\n        \"\"\"\n        # need to copy dictionary by calling dict() on it\n        axis_props = dict(theme_attr_json(self.renderer.theme, 'Axis'))\n\n        if ((axis == 'x' and self.xaxis in ['bottom-bare', 'top-bare', 'bare']) or\n            (axis == 'y' and self.yaxis in ['left-bare', 'right-bare', 'bare'])):\n            axis_props['axis_label_text_font_size'] = value('0pt')\n            axis_props['major_label_text_font_size'] = value('0pt')\n            axis_props['major_tick_line_color'] = None\n            axis_props['minor_tick_line_color'] = None\n        else:\n            labelsize = self._fontsize('%slabel' % axis).get('fontsize')\n            if labelsize:\n                axis_props['axis_label_text_font_size'] = labelsize\n            ticksize = self._fontsize('%sticks' % axis, common=False).get('fontsize')\n            if ticksize:\n                axis_props['major_label_text_font_size'] = value(ticksize)\n            rotation = self.xrotation if axis == 'x' else self.yrotation\n            if rotation:\n                axis_props['major_label_orientation'] = np.radians(rotation)\n            ticker = self.xticks if axis == 'x' else self.yticks\n            if isinstance(ticker, Ticker):\n                axis_props['ticker'] = ticker\n            elif isinstance(ticker, int):\n                axis_props['ticker'] = BasicTicker(desired_num_ticks=ticker)\n            elif isinstance(ticker, (tuple, list)):\n                if all(isinstance(t, tuple) for t in ticker):\n                    ticks, labels = zip(*ticker)\n                    # Ensure floats which are integers are serialized as ints\n                    # because in JS the lookup fails otherwise\n                    ticks = [int(t) if isinstance(t, float) and t.is_integer() else t\n                             for t in ticks]\n                    labels = [l if isinstance(l, util.basestring) else str(l)\n                              for l in labels]\n                    axis_props['ticker'] = FixedTicker(ticks=ticks)\n                    axis_props['major_label_overrides'] = dict(zip(ticks, labels))\n                else:\n                    axis_props['ticker'] = FixedTicker(ticks=ticker)\n\n        formatter = self.xformatter if axis == 'x' else self.yformatter\n        if formatter:\n            if isinstance(formatter, TickFormatter):\n                pass\n            elif isinstance(formatter, FunctionType):\n                msg = ('%sformatter could not be '\n                       'converted to tick formatter. ' % axis)\n                jsfunc = py2js_tickformatter(formatter, msg)\n                if jsfunc:\n                    formatter = FuncTickFormatter(code=jsfunc)\n                else:\n                    formatter = None\n            else:\n                formatter = PrintfTickFormatter(format=formatter)\n            if formatter is not None:\n                axis_props['formatter'] = formatter\n        elif FuncTickFormatter is not None and ax_mapping and isinstance(dimension, Dimension):\n            formatter = None\n            if dimension.value_format:\n                formatter = dimension.value_format\n            elif dimension.type in dimension.type_formatters:\n                formatter = dimension.type_formatters[dimension.type]\n            if formatter:\n                msg = ('%s dimension formatter could not be '\n                       'converted to tick formatter. ' % dimension.name)\n                jsfunc = py2js_tickformatter(formatter, msg)\n                if jsfunc:\n                    formatter = FuncTickFormatter(code=jsfunc)\n                    axis_props['formatter'] = formatter\n\n        if axis == 'x':\n            axis_obj = plot.xaxis[0]\n        elif axis == 'y':\n            axis_obj = plot.yaxis[0]\n\n        if self.geographic and self.projection == 'mercator':\n            dimension = 'lon' if axis == 'x' else 'lat'\n            axis_props['ticker'] = MercatorTicker(dimension=dimension)\n            axis_props['formatter'] = MercatorTickFormatter(dimension=dimension)\n            box_zoom = self.state.select(type=tools.BoxZoomTool)\n            if box_zoom:\n                box_zoom[0].match_aspect = True\n        elif isinstance(axis_obj, CategoricalAxis):\n            for key in list(axis_props):\n                if key.startswith('major_label'):\n                    # set the group labels equal to major (actually minor)\n                    new_key = key.replace('major_label', 'group')\n                    axis_props[new_key] = axis_props[key]\n\n            # major ticks are actually minor ticks in a categorical\n            # so if user inputs minor ticks sizes, then use that;\n            # else keep major (group) == minor (subgroup)\n            msize = self._fontsize('minor_{0}ticks'.format(axis),\n                common=False).get('fontsize')\n            if msize is not None:\n                axis_props['major_label_text_font_size'] = msize\n\n        return axis_props", "response": "Returns a dictionary of axis properties depending on the axis."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the plot parameters on every frame", "response": "def _update_plot(self, key, plot, element=None):\n        \"\"\"\n        Updates plot parameters on every frame\n        \"\"\"\n        plot.update(**self._plot_properties(key, element))\n        self._update_labels(key, plot, element)\n        self._update_title(key, plot, element)\n        self._update_grid(plot)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms non - string integer types in datasource data columns and dimensions of the data to categorical.", "response": "def _categorize_data(self, data, cols, dims):\n        \"\"\"\n        Transforms non-string or integer types in datasource if the\n        axis to be plotted on is categorical. Accepts the column data\n        source data, the columns corresponding to the axes and the\n        dimensions for each axis, changing the data inplace.\n        \"\"\"\n        if self.invert_axes:\n            cols = cols[::-1]\n            dims = dims[:2][::-1]\n        ranges = [self.handles['%s_range' % ax] for ax in 'xy']\n        for i, col in enumerate(cols):\n            column = data[col]\n            if (isinstance(ranges[i], FactorRange) and\n                (isinstance(column, list) or column.dtype.kind not in 'SU')):\n                data[col] = [dims[i].pprint_value(v) for v in column]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_aspect(self, xspan, yspan):\n        if self.data_aspect:\n            return (yspan/xspan)*self.data_aspect\n        elif self.aspect == 'equal':\n            return xspan/yspan\n        elif self.aspect == 'square':\n            return 1\n        elif self.aspect is not None:\n            return self.aspect\n        elif self.width is not None and self.height is not None:\n            return self.width/self.height\n        else:\n            return 1", "response": "Compute the aspect ratio of the plot."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_factors(self, element):\n        xdim, ydim = element.dimensions()[:2]\n        xvals, yvals = [element.dimension_values(i, False)\n                        for i in range(2)]\n        coords = tuple([v if vals.dtype.kind in 'SU' else dim.pprint_value(v) for v in vals]\n                  for dim, vals in [(xdim, xvals), (ydim, yvals)])\n        if self.invert_axes: coords = coords[::-1]\n        return coords", "response": "Get factors for categorical axes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_legend(self):\n        for l in self.handles['plot'].legend:\n            l.items[:] = []\n            l.border_line_alpha = 0\n            l.background_fill_alpha = 0", "response": "Sets the legend color to transparent."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_glyph(self, plot, mapping, properties):\n        properties = mpl_to_bokeh(properties)\n        plot_method = self._plot_methods.get('batched' if self.batched else 'single')\n        if isinstance(plot_method, tuple):\n            # Handle alternative plot method for flipped axes\n            plot_method = plot_method[int(self.invert_axes)]\n        renderer = getattr(plot, plot_method)(**dict(properties, **mapping))\n        return renderer, renderer.glyph", "response": "Initialize a glyph object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _postprocess_hover(self, renderer, source):\n        hover = self.handles.get('hover')\n        if hover is None:\n            return\n        if hover.renderers == 'auto':\n            hover.renderers = []\n        hover.renderers.append(renderer)\n\n        # If datetime column is in the data replace hover formatter\n        for k, v in source.data.items():\n            if k+'_dt_strings' in source.data:\n                tooltips = []\n                for name, formatter in hover.tooltips:\n                    if formatter == '@{%s}' % k:\n                        formatter = '@{%s_dt_strings}' % k\n                    tooltips.append((name, formatter))\n                hover.tooltips = tooltips", "response": "Processes the hover data and adds formatter to the tooltips."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninitializing a new plot object with the last available frame.", "response": "def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):\n        \"\"\"\n        Initializes a new plot object with the last available frame.\n        \"\"\"\n        # Get element key and ranges for frame\n        if self.batched:\n            element = [el for el in self.hmap.data.values() if el][-1]\n        else:\n            element = self.hmap.last\n        key = util.wrap_tuple(self.hmap.last_key)\n        ranges = self.compute_ranges(self.hmap, key, ranges)\n        self.current_ranges = ranges\n        self.current_frame = element\n        self.current_key = key\n        style_element = element.last if self.batched else element\n        ranges = util.match_spec(style_element, ranges)\n        # Initialize plot, source and glyph\n        if plot is None:\n            plot = self._init_plot(key, style_element, ranges=ranges, plots=plots)\n            self._init_axes(plot)\n        else:\n            self.handles['xaxis'] = plot.xaxis[0]\n            self.handles['x_range'] = plot.x_range\n            self.handles['yaxis'] = plot.yaxis[0]\n            self.handles['y_range'] = plot.y_range\n        self.handles['plot'] = plot\n\n        self._init_glyphs(plot, element, ranges, source)\n        if not self.overlaid:\n            self._update_plot(key, plot, style_element)\n            self._update_ranges(style_element, ranges)\n\n        for cb in self.callbacks:\n            cb.initialize()\n\n        if not self.overlaid:\n            self._set_active_tools(plot)\n            self._process_legend()\n        self._execute_hooks(element)\n\n        self.drawn = True\n\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates an existing plot with data corresponding to the key.", "response": "def update_frame(self, key, ranges=None, plot=None, element=None):\n        \"\"\"\n        Updates an existing plot with data corresponding\n        to the key.\n        \"\"\"\n        reused = isinstance(self.hmap, DynamicMap) and (self.overlaid or self.batched)\n        if not reused and element is None:\n            element = self._get_frame(key)\n        elif element is not None:\n            self.current_key = key\n            self.current_frame = element\n\n        renderer = self.handles.get('glyph_renderer', None)\n        glyph = self.handles.get('glyph', None)\n        visible = element is not None\n        if hasattr(renderer, 'visible'):\n            renderer.visible = visible\n        if hasattr(glyph, 'visible'):\n            glyph.visible = visible\n\n        if ((self.batched and not element) or element is None or (not self.dynamic and self.static) or\n            (self.streaming and self.streaming[0].data is self.current_frame.data and not self.streaming[0]._triggering)):\n            return\n\n        if self.batched:\n            style_element = element.last\n            max_cycles = None\n        else:\n            style_element = element\n            max_cycles = self.style._max_cycles\n        style = self.lookup_options(style_element, 'style')\n        self.style = style.max_cycles(max_cycles) if max_cycles else style\n\n        ranges = self.compute_ranges(self.hmap, key, ranges)\n        self.param.set_param(**self.lookup_options(style_element, 'plot').options)\n        ranges = util.match_spec(style_element, ranges)\n        self.current_ranges = ranges\n        plot = self.handles['plot']\n        if not self.overlaid:\n            self._update_ranges(style_element, ranges)\n            self._update_plot(key, plot, style_element)\n            self._set_active_tools(plot)\n\n        if 'hover' in self.handles and 'hv_created' in self.handles['hover'].tags:\n            self._update_hover(element)\n\n        self._update_glyphs(element, ranges, self.style[self.cyclic_index])\n        self._execute_hooks(element)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining if the bokeh model has just changed on the frontend.", "response": "def model_changed(self, model):\n        \"\"\"\n        Determines if the bokeh model was just changed on the frontend.\n        Useful to suppress boomeranging events, e.g. when the frontend\n        just sent an update to the x_range this should not trigger an\n        update on the backend.\n        \"\"\"\n        callbacks = [cb for cbs in self.traverse(lambda x: x.callbacks)\n                     for cb in cbs]\n        stream_metadata = [stream._metadata for cb in callbacks\n                           for stream in cb.streams if stream._metadata]\n        return any(md['id'] == model.ref['id'] for models in stream_metadata\n                   for md in models.values())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef framewise(self):\n        current_frames = [el for f in self.traverse(lambda x: x.current_frame)\n                          for el in (f.traverse(lambda x: x, [Element])\n                                     if f else [])]\n        current_frames = util.unique_iterator(current_frames)\n        return any(self.lookup_options(frame, 'norm').options.get('framewise')\n                   for frame in current_frames)", "response": "Property to determine whether the current frame should have\n        framewise normalization enabled. Required for plotting\n        classes to determine whether to send updated ranges for each\n        frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a glyph object.", "response": "def _init_glyph(self, plot, mapping, properties, key):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        properties = mpl_to_bokeh(properties)\n        plot_method = '_'.join(key.split('_')[:-1])\n        renderer = getattr(plot, plot_method)(**dict(properties, **mapping))\n        return renderer, renderer.glyph"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Bokeh glyph object and optionally creates a colorbar.", "response": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object and optionally creates a colorbar.\n        \"\"\"\n        ret = super(ColorbarPlot, self)._init_glyph(plot, mapping, properties)\n        if self.colorbar:\n            for k, v in list(self.handles.items()):\n                if not k.endswith('color_mapper'):\n                    continue\n                self._draw_colorbar(plot, v, k[:-12])\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses the list of tools to be supplied to the plot.", "response": "def _init_tools(self, element, callbacks=[]):\n        \"\"\"\n        Processes the list of tools to be supplied to the plot.\n        \"\"\"\n        hover_tools = {}\n        init_tools, tool_types = [], []\n        for key, subplot in self.subplots.items():\n            el = element.get(key)\n            if el is not None:\n                el_tools = subplot._init_tools(el, self.callbacks)\n                for tool in el_tools:\n                    if isinstance(tool, util.basestring):\n                        tool_type = TOOL_TYPES.get(tool)\n                    else:\n                        tool_type = type(tool)\n                    if isinstance(tool, tools.HoverTool):\n                        tooltips = tuple(tool.tooltips) if tool.tooltips else ()\n                        if tooltips in hover_tools:\n                            continue\n                        else:\n                            hover_tools[tooltips] = tool\n                    elif tool_type in tool_types:\n                        continue\n                    else:\n                        tool_types.append(tool_type)\n                    init_tools.append(tool)\n        self.handles['hover_tools'] = hover_tools\n        return init_tools"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _merge_tools(self, subplot):\n        if self.batched and 'hover' in subplot.handles:\n            self.handles['hover'] = subplot.handles['hover']\n        elif 'hover' in subplot.handles and 'hover_tools' in self.handles:\n            hover = subplot.handles['hover']\n            # Datetime formatter may have been applied, remove _dt_strings\n            # to match on the hover tooltips, then merge tool renderers\n            if hover.tooltips and not isinstance(hover.tooltips, util.basestring):\n                tooltips = tuple((name, spec.replace('_dt_strings', ''))\n                                  for name, spec in hover.tooltips)\n            else:\n                tooltips = ()\n            tool = self.handles['hover_tools'].get(tooltips)\n            if tool:\n                tool_renderers = [] if tool.renderers == 'auto' else tool.renderers\n                hover_renderers = [] if hover.renderers == 'auto' else hover.renderers\n                renderers = tool_renderers + hover_renderers\n                tool.renderers = list(util.unique_iterator(renderers))\n            if 'hover' not in self.handles:\n                self.handles['hover'] = tool", "response": "Merges tools on the overlay with those on the subplots."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the internal state of the Plot to represent the given key tuple. Returns this state.", "response": "def update_frame(self, key, ranges=None, element=None):\n        \"\"\"\n        Update the internal state of the Plot to represent the given\n        key tuple (where integers represent frames). Returns this\n        state.\n        \"\"\"\n        reused = isinstance(self.hmap, DynamicMap) and self.overlaid\n        if not reused and element is None:\n            element = self._get_frame(key)\n        elif element is not None:\n            self.current_frame = element\n            self.current_key = key\n        items = [] if element is None else list(element.data.items())\n\n        if isinstance(self.hmap, DynamicMap):\n            range_obj = element\n        else:\n            range_obj = self.hmap\n\n        if element is not None:\n            ranges = self.compute_ranges(range_obj, key, ranges)\n\n        # Update plot options\n        plot_opts = self.lookup_options(element, 'plot').options\n        inherited = self._traverse_options(element, 'plot',\n                                           self._propagate_options,\n                                           defaults=False)\n        plot_opts.update(**{k: v[0] for k, v in inherited.items() if k not in plot_opts})\n        self.param.set_param(**plot_opts)\n\n        if element and not self.overlaid and not self.tabs and not self.batched:\n            self._update_ranges(element, ranges)\n\n        # Determine which stream (if any) triggered the update\n        triggering = [stream for stream in self.streams if stream._triggering]\n\n        for k, subplot in self.subplots.items():\n            el = None\n\n            # If in Dynamic mode propagate elements to subplots\n            if isinstance(self.hmap, DynamicMap) and element:\n                # In batched mode NdOverlay is passed to subplot directly\n                if self.batched:\n                    el = element\n                # If not batched get the Element matching the subplot\n                elif element is not None:\n                    idx, spec, exact = dynamic_update(self, subplot, k, element, items)\n                    if idx is not None:\n                        _, el = items.pop(idx)\n                        if not exact:\n                            self._update_subplot(subplot, spec)\n\n                # Skip updates to subplots when its streams is not one of\n                # the streams that initiated the update\n                if (triggering and all(s not in triggering for s in subplot.streams) and\n                    not subplot in self.dynamic_subplots):\n                    continue\n            subplot.update_frame(key, ranges, element=el)\n\n        if not self.batched and isinstance(self.hmap, DynamicMap) and items:\n            init_kwargs = {'plots': self.handles['plots']}\n            if not self.tabs:\n                init_kwargs['plot'] = self.handles['plot']\n            self._create_dynamic_subplots(key, items, ranges, **init_kwargs)\n            if not self.overlaid and not self.tabs:\n                self._process_legend()\n\n        if element and not self.overlaid and not self.tabs and not self.batched:\n            plot = self.handles['plot']\n            self._update_plot(key, plot, element)\n            self._set_active_tools(plot)\n\n        self._process_legend()\n\n        self._execute_hooks(element)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_formats(format_type, backend=None):\n    if backend is None:\n        backend = Store.current_backend\n        mode = Store.renderers[backend].mode if backend in Store.renderers else None\n    else:\n        split = backend.split(':')\n        backend, mode = split if len(split)==2 else (split[0], 'default')\n\n    if backend in Store.renderers:\n        return Store.renderers[backend].mode_formats[format_type][mode]\n    else:\n        return []", "response": "Returns list of supported formats for a particular backend."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_options(cls, items, options, warnfn):\n        \"Given a keyword specification, validate and compute options\"\n        options = cls.update_options(options, items)\n        for keyword in cls.defaults:\n            if keyword in items:\n                value = items[keyword]\n                allowed = cls.allowed[keyword]\n                if isinstance(allowed, set):  pass\n                elif isinstance(allowed, dict):\n                    if not isinstance(value, dict):\n                        raise ValueError(\"Value %r not a dict type\" % value)\n                    disallowed = set(value.keys()) - set(allowed.keys())\n                    if disallowed:\n                        raise ValueError(\"Keywords %r for %r option not one of %s\"\n                                         % (disallowed, keyword, allowed))\n                    wrong_type = {k: v for k, v in value.items()\n                                  if not isinstance(v, allowed[k])}\n                    if wrong_type:\n                        errors = []\n                        for k,v in wrong_type.items():\n                            errors.append(\"Value %r for %r option's %r attribute not of type %r\" %\n                                          (v, keyword, k, allowed[k]))\n                        raise ValueError('\\n'.join(errors))\n                elif isinstance(allowed, list) and value not in allowed:\n                    if keyword in cls.custom_exceptions:\n                        cls.custom_exceptions[keyword](value, keyword, allowed)\n                    else:\n                        raise ValueError(\"Value %r for key %r not one of %s\"\n                                         % (value, keyword, allowed))\n                elif isinstance(allowed, tuple):\n                    if not (allowed[0] <= value <= allowed[1]):\n                        info = (keyword,value)+allowed\n                        raise ValueError(\"Value %r for key %r not between %s and %s\" % info)\n                options[keyword] = value\n        return cls._validate(options, items, warnfn)", "response": "Given a keyword specification validate and compute options"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_keywords(cls, line, items):\n        unprocessed = list(reversed(line.split('=')))\n        while unprocessed:\n            chunk = unprocessed.pop()\n            key = None\n            if chunk.strip() in cls.allowed:\n                key = chunk.strip()\n            else:\n                raise SyntaxError(\"Invalid keyword: %s\" % chunk.strip())\n            # The next chunk may end in a subsequent keyword\n            value = unprocessed.pop().strip()\n            if len(unprocessed) != 0:\n                # Check if a new keyword has begun\n                for option in cls.allowed:\n                    if value.endswith(option):\n                        value = value[:-len(option)].strip()\n                        unprocessed.append(option)\n                        break\n                else:\n                    raise SyntaxError(\"Invalid keyword: %s\" % value.split()[-1])\n            keyword = '%s=%s' % (key, value)\n            try:\n                items.update(eval('dict(%s)' % keyword))\n            except:\n                raise SyntaxError(\"Could not evaluate keyword: %s\" % keyword)\n        return items", "response": "Given the keyword string parse a dictionary of options."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate(cls, options, items, warnfn):\n        \"Validation of edge cases and incompatible options\"\n\n        if 'html' in Store.display_formats:\n            pass\n        elif 'fig' in items and items['fig'] not in Store.display_formats:\n            msg = (\"Requesting output figure format %r \" % items['fig']\n                   + \"not in display formats %r\" % Store.display_formats)\n            if warnfn is None:\n                print('Warning: {msg}'.format(msg=msg))\n            else:\n                warnfn(msg)\n\n        backend = Store.current_backend\n        return Store.renderers[backend].validate(options)", "response": "Validation of edge cases and incompatible options"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates options with new backend and mode if new backend is supplied in items.", "response": "def update_options(cls, options, items):\n        \"\"\"\n        Switch default options and backend if new backend is supplied in\n        items.\n        \"\"\"\n        # Get new backend\n        backend_spec = items.get('backend', Store.current_backend)\n        split = backend_spec.split(':')\n        backend, mode = split if len(split)==2 else (split[0], 'default')\n        if ':' not in backend_spec:\n            backend_spec += ':default'\n\n        if 'max_branches' in items:\n            print('Warning: The max_branches option is now deprecated. Ignoring.')\n            del items['max_branches']\n\n        # Get previous backend\n        prev_backend = Store.current_backend\n        renderer = Store.renderers[prev_backend]\n        prev_backend_spec = prev_backend+':'+renderer.mode\n\n        # Update allowed formats\n        for p in ['fig', 'holomap']:\n            cls.allowed[p] = list_formats(p, backend_spec)\n\n        # Return if backend invalid and let validation error\n        if backend not in Store.renderers:\n            options['backend'] = backend_spec\n            return options\n\n        # Get backend specific options\n        backend_options = dict(cls._backend_options[backend_spec])\n        cls._backend_options[prev_backend_spec] = {k: v for k, v in cls.options.items()\n                                                   if k in cls.remembered}\n\n        # Fill in remembered options with defaults\n        for opt in cls.remembered:\n            if opt not in backend_options:\n                backend_options[opt] = cls.defaults[opt]\n\n        # Switch format if mode does not allow it\n        for p in ['fig', 'holomap']:\n            if backend_options.get(p) not in cls.allowed[p]:\n                backend_options[p] = cls.allowed[p][0]\n\n        # Ensure backend and mode are set\n        backend_options['backend'] = backend_spec\n        backend_options['mode'] = mode\n\n        return backend_options"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_render_options(cls, options, backend=None):\n        if backend:\n            backend = backend.split(':')[0]\n        else:\n            backend = Store.current_backend\n\n        cls.set_backend(backend)\n        if 'widgets' in options:\n            options['widget_mode'] = options['widgets']\n        renderer = Store.renderers[backend]\n        render_options = {k: options[k] for k in cls.render_params if k in options}\n        renderer.param.set_param(**render_options)", "response": "Set options on current Renderer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef help(obj, visualization=True, ansi=True, backend=None,\n         recursive=False, pattern=None):\n    \"\"\"\n    Extended version of the built-in help that supports parameterized\n    functions and objects. A pattern (regular expression) may be used to\n    filter the output and if recursive is set to True, documentation for\n    the supplied object is shown. Note that the recursive option will\n    only work with an object instance and not a class.\n\n    If ansi is set to False, all ANSI color\n    codes are stripped out.\n    \"\"\"\n    backend = backend if backend else Store.current_backend\n    info = Store.info(obj, ansi=ansi, backend=backend, visualization=visualization,\n                      recursive=recursive, pattern=pattern, elements=elements_list)\n\n    msg = (\"\\nTo view the visualization options applicable to this \"\n           \"object or class, use:\\n\\n\"\n           \"   holoviews.help(obj, visualization=True)\\n\\n\")\n    if info:\n        print((msg if visualization is False else '') + info)\n    else:\n        pydoc.help(obj)", "response": "Help for the object or class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes a glyph object.", "response": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        box = Span(level=properties.get('level', 'glyph'), **mapping)\n        plot.renderers.append(box)\n        return None, box"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _init_glyph(self, plot, mapping, properties, key):\n        properties.pop('legend', None)\n        if key == 'arrow':\n            properties.pop('source')\n            arrow_end = mapping.pop('arrow_end')\n            arrow_start = mapping.pop('arrow_start')\n            start = arrow_start(**properties) if arrow_start else None\n            end = arrow_end(**properties) if arrow_end else None\n            renderer = Arrow(start=start, end=end, **dict(**mapping))\n            glyph = renderer\n        else:\n            properties = {p if p == 'source' else 'text_'+p: v\n                          for p, v in properties.items()}\n            renderer, glyph = super(ArrowPlot, self)._init_glyph(\n                plot, mapping, properties, 'text_1')\n        plot.renderers.append(renderer)\n        return renderer, glyph", "response": "Initialize a glyph object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes a new plot object with the last available frame.", "response": "def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):\n        \"\"\"\n        Initializes a new plot object with the last available frame.\n        \"\"\"\n        # Get element key and ranges for frame\n        element = self.hmap.last\n        key = self.keys[-1]\n        self.current_frame = element\n        self.current_key = key\n\n        data, _, _ = self.get_data(element, ranges, {})\n        div = BkDiv(text=data, width=self.width, height=self.height)\n        self.handles['plot'] = div\n        self._execute_hooks(element)\n        self.drawn = True\n        return div"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate an existing plot with data corresponding to the key.", "response": "def update_frame(self, key, ranges=None, plot=None):\n        \"\"\"\n        Updates an existing plot with data corresponding\n        to the key.\n        \"\"\"\n        element = self._get_frame(key)\n        text, _, _ = self.get_data(element, ranges, {})\n        self.handles['plot'].text = text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_dataframe(path_df):\n    splits = np.where(path_df.iloc[:, 0].isnull())[0]+1\n    return [df for df in np.split(path_df, splits) if len(df) > 1]", "response": "Splits a dataframe of paths separated by NaNs into individual\n    dataframes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreduce any Overlay or NdOverlay of Elements into a single Dataset that can be aggregated.", "response": "def get_agg_data(cls, obj, category=None):\n        \"\"\"\n        Reduces any Overlay or NdOverlay of Elements into a single\n        xarray Dataset that can be aggregated.\n        \"\"\"\n        paths = []\n        if isinstance(obj, Graph):\n            obj = obj.edgepaths\n        kdims = list(obj.kdims)\n        vdims = list(obj.vdims)\n        dims = obj.dimensions()[:2]\n        if isinstance(obj, Path):\n            glyph = 'line'\n            for p in obj.split(datatype='dataframe'):\n                paths.append(p)\n        elif isinstance(obj, CompositeOverlay):\n            element = None\n            for key, el in obj.data.items():\n                x, y, element, glyph = cls.get_agg_data(el)\n                dims = (x, y)\n                df = PandasInterface.as_dframe(element)\n                if isinstance(obj, NdOverlay):\n                    df = df.assign(**dict(zip(obj.dimensions('key', True), key)))\n                paths.append(df)\n            if element is None:\n                dims = None\n            else:\n                kdims += element.kdims\n                vdims = element.vdims\n        elif isinstance(obj, Element):\n            glyph = 'line' if isinstance(obj, Curve) else 'points'\n            paths.append(PandasInterface.as_dframe(obj))\n\n        if dims is None or len(dims) != 2:\n            return None, None, None, None\n        else:\n            x, y = dims\n\n        if len(paths) > 1:\n            if glyph == 'line':\n                path = paths[0][:1]\n                if isinstance(path, dd.DataFrame):\n                    path = path.compute()\n                empty = path.copy()\n                empty.iloc[0, :] = (np.NaN,) * empty.shape[1]\n                paths = [elem for p in paths for elem in (p, empty)][:-1]\n            if all(isinstance(path, dd.DataFrame) for path in paths):\n                df = dd.concat(paths)\n            else:\n                paths = [p.compute() if isinstance(p, dd.DataFrame) else p for p in paths]\n                df = pd.concat(paths)\n        else:\n            df = paths[0] if paths else pd.DataFrame([], columns=[x.name, y.name])\n        if category and df[category].dtype.name != 'category':\n            df[category] = df[category].astype('category')\n\n        is_dask = isinstance(df, dd.DataFrame)\n        if any((not is_dask and len(df[d.name]) and isinstance(df[d.name].values[0], cftime_types)) or\n               df[d.name].dtype.kind == 'M' for d in (x, y)):\n            df = df.copy()\n\n        for d in (x, y):\n            vals = df[d.name]\n            if not is_dask and len(vals) and isinstance(vals.values[0], cftime_types):\n                vals = cftime_to_timestamp(vals, 'ns')\n            elif df[d.name].dtype.kind == 'M':\n                vals = vals.astype('datetime64[ns]')\n            else:\n                continue\n            df[d.name] = vals.astype('int64')\n        return x, y, Dataset(df, kdims=kdims, vdims=vdims), glyph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\noptimize aggregation for each item in an NdOverlay object by aggregating each item in an NdOverlay object by applying appropriate masking for NaN values.", "response": "def _aggregate_ndoverlay(self, element, agg_fn):\n        \"\"\"\n        Optimized aggregation for NdOverlay objects by aggregating each\n        Element in an NdOverlay individually avoiding having to concatenate\n        items in the NdOverlay. Works by summing sum and count aggregates and\n        applying appropriate masking for NaN values. Mean aggregation\n        is also supported by dividing sum and count aggregates. count_cat\n        aggregates are grouped by the categorical dimension and a separate\n        aggregate for each category is generated.\n        \"\"\"\n        # Compute overall bounds\n        x, y = element.last.dimensions()[0:2]\n        info = self._get_sampling(element, x, y)\n        (x_range, y_range), (xs, ys), (width, height), (xtype, ytype) = info\n        if xtype == 'datetime':\n            x_range = tuple((np.array(x_range)/1e3).astype('datetime64[us]'))\n        if ytype == 'datetime':\n            y_range = tuple((np.array(y_range)/1e3).astype('datetime64[us]'))\n        agg_params = dict({k: v for k, v in dict(self.get_param_values(), **self.p).items()\n                           if k in aggregate.params()},\n                          x_range=x_range, y_range=y_range)\n        bbox = BoundingBox(points=[(x_range[0], y_range[0]), (x_range[1], y_range[1])])\n\n        # Optimize categorical counts by aggregating them individually\n        if isinstance(agg_fn, ds.count_cat):\n            agg_params.update(dict(dynamic=False, aggregator=ds.count()))\n            agg_fn1 = aggregate.instance(**agg_params)\n            if element.ndims == 1:\n                grouped = element\n            else:\n                grouped = element.groupby([agg_fn.column], container_type=NdOverlay,\n                                          group_type=NdOverlay)\n            groups = []\n            for k, v in grouped.items():\n                agg = agg_fn1(v)\n                groups.append((k, agg.clone(agg.data, bounds=bbox)))\n            return grouped.clone(groups)\n\n        # Create aggregate instance for sum, count operations, breaking mean\n        # into two aggregates\n        column = agg_fn.column or 'Count'\n        if isinstance(agg_fn, ds.mean):\n            agg_fn1 = aggregate.instance(**dict(agg_params, aggregator=ds.sum(column)))\n            agg_fn2 = aggregate.instance(**dict(agg_params, aggregator=ds.count()))\n        else:\n            agg_fn1 = aggregate.instance(**agg_params)\n            agg_fn2 = None\n        is_sum = isinstance(agg_fn1.aggregator, ds.sum)\n\n        # Accumulate into two aggregates and mask\n        agg, agg2, mask = None, None, None\n        mask = None\n        for v in element:\n            # Compute aggregates and mask\n            new_agg = agg_fn1.process_element(v, None)\n            if is_sum:\n                new_mask = np.isnan(new_agg.data[column].values)\n                new_agg.data = new_agg.data.fillna(0)\n            if agg_fn2:\n                new_agg2 = agg_fn2.process_element(v, None)\n\n            if agg is None:\n                agg = new_agg\n                if is_sum: mask = new_mask\n                if agg_fn2: agg2 = new_agg2\n            else:\n                agg.data += new_agg.data\n                if is_sum: mask &= new_mask\n                if agg_fn2: agg2.data += new_agg2.data\n\n        # Divide sum by count to compute mean\n        if agg2 is not None:\n            agg2.data.rename({'Count': agg_fn.column}, inplace=True)\n            with np.errstate(divide='ignore', invalid='ignore'):\n                agg.data /= agg2.data\n\n        # Fill masked with with NaNs\n        if is_sum:\n            agg.data[column].values[mask] = np.NaN\n\n        return agg.clone(bounds=bbox)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef concatenate(cls, overlay):\n        if not isinstance(overlay, NdOverlay):\n            raise ValueError('Only NdOverlays can be concatenated')\n        xarr = xr.concat([v.data.transpose() for v in overlay.values()],\n                         pd.Index(overlay.keys(), name=overlay.kdims[0].name))\n        params = dict(get_param_values(overlay.last),\n                      vdims=overlay.last.vdims,\n                      kdims=overlay.kdims+overlay.last.kdims)\n        return Dataset(xarr.transpose(), datatype=['xarray'], **params)", "response": "Concatenates an NdOverlay of Image types into a single 3D Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uint32_to_uint8(cls, img):\n        return np.flipud(img.view(dtype=np.uint8).reshape(img.shape + (4,)))", "response": "Cast uint32 RGB image to 4 uint8 channels."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef embed_version(basepath, ref='v0.2.2'):\n    import io, zipfile, importlib\n    try:    from urllib.request import urlopen\n    except: from urllib import urlopen\n    try:\n        url = 'https://github.com/ioam/autover/archive/{ref}.zip'\n        response = urlopen(url.format(ref=ref))\n        zf = zipfile.ZipFile(io.BytesIO(response.read()))\n        ref = ref[1:] if ref.startswith('v') else ref\n        embed_version = zf.read('autover-{ref}/autover/version.py'.format(ref=ref))\n        with open(os.path.join(basepath, 'version.py'), 'wb') as f:\n            f.write(embed_version)\n        return importlib.import_module(\"version\")\n    except:\n        return None", "response": "This function embeds the version. py file in the basepath of the environment."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_pseudo_package(path):\n    if not os.path.isdir(path):\n        raise Exception(\"Please make sure pseudo-package %s exists.\" % path)\n    else:\n        assets = os.listdir(path)\n        if len(assets) == 0:\n            raise Exception(\"Please make sure pseudo-package %s is populated.\" % path)", "response": "Checks that a pseudo - package path for assets exists and is populated with files."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef walker(top, names):\n    global packages, extensions\n    if any(exc in top for exc in excludes):\n        return\n    package = top[top.rfind('holoviews'):].replace(os.path.sep, '.')\n    packages.append(package)\n    for name in names:\n        ext = '.'.join(name.split('.')[1:])\n        ext_str = '*.%s' % ext\n        if ext and ext not in excludes and ext_str not in extensions[package]:\n            extensions[package].append(ext_str)", "response": "Walks a directory and records all packages and file extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef examples(path='holoviews-examples', verbose=False, force=False, root=__file__):\n    filepath = os.path.abspath(os.path.dirname(root))\n    example_dir = os.path.join(filepath, './examples')\n    if not os.path.exists(example_dir):\n        example_dir = os.path.join(filepath, '../examples')\n    if os.path.exists(path):\n        if not force:\n            print('%s directory already exists, either delete it or set the force flag' % path)\n            return\n        shutil.rmtree(path)\n    ignore = shutil.ignore_patterns('.ipynb_checkpoints', '*.pyc', '*~')\n    tree_root = os.path.abspath(example_dir)\n    if os.path.isdir(tree_root):\n        shutil.copytree(tree_root, path, ignore=ignore, symlinks=True)\n    else:\n        print('Cannot find %s' % tree_root)", "response": "Copy the notebooks to the supplied path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef package_assets(example_path):\n    examples(example_path, force=True, root=__file__)\n    for root, dirs, files in os.walk(example_path):\n        walker(root, dirs+files)\n    setup_args['packages'] += packages\n    for p, exts in extensions.items():\n        if exts:\n            setup_args['package_data'][p] = exts", "response": "Generates pseudo - packages for the examples directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, selection_specs=None, selection_mode='edges', **selection):\n        selection = {dim: sel for dim, sel in selection.items()\n                     if dim in self.dimensions('ranges')+['selection_mask']}\n        if (selection_specs and not any(self.matches(sp) for sp in selection_specs)\n            or not selection):\n            return self\n\n        index_dim = self.nodes.kdims[2].name\n        dimensions = self.kdims+self.vdims\n        node_selection = {index_dim: v for k, v in selection.items()\n                          if k in self.kdims}\n        nodes = self.nodes.select(**dict(selection, **node_selection))\n        selection = {k: v for k, v in selection.items() if k in dimensions}\n\n        # Compute mask for edges if nodes were selected on\n        nodemask = None\n        if len(nodes) != len(self.nodes):\n            xdim, ydim = dimensions[:2]\n            indices = list(nodes.dimension_values(2, False))\n            if selection_mode == 'edges':\n                mask1 = self.interface.select_mask(self, {xdim.name: indices})\n                mask2 = self.interface.select_mask(self, {ydim.name: indices})\n                nodemask = (mask1 | mask2)\n                nodes = self.nodes\n            else:\n                nodemask = self.interface.select_mask(self, {xdim.name: indices,\n                                                             ydim.name: indices})\n\n        # Compute mask for edge selection\n        mask = None\n        if selection:\n            mask = self.interface.select_mask(self, selection)\n\n        # Combine masks\n        if nodemask is not None:\n            if mask is not None:\n                mask &= nodemask\n            else:\n                mask = nodemask\n\n        # Apply edge mask\n        if mask is not None:\n            data = self.interface.select(self, mask)\n            if not np.all(mask):\n                new_graph = self.clone((data, nodes))\n                source = new_graph.dimension_values(0, expanded=False)\n                target = new_graph.dimension_values(1, expanded=False)\n                unique_nodes = np.unique(np.concatenate([source, target]))\n                nodes = new_graph.nodes[:, :, list(unique_nodes)]\n            paths = None\n            if self._edgepaths:\n                edgepaths = self._split_edgepaths\n                paths = edgepaths.clone(edgepaths.interface.select_paths(edgepaths, mask))\n                if len(self._edgepaths.data) == 1:\n                    paths = paths.clone([paths.dframe() if pd else paths.array()])\n        else:\n            data = self.data\n            paths = self._edgepaths\n        return self.clone((data, nodes, paths))", "response": "Selects the data for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the list of nodes that are available for this instance.", "response": "def nodes(self):\n        \"\"\"\n        Computes the node positions the first time they are requested\n        if no explicit node information was supplied.\n        \"\"\"\n        if self._nodes is None:\n            self._nodes = layout_nodes(self, only_nodes=True)\n        return self._nodes"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef edgepaths(self):\n        if self._edgepaths:\n            return self._edgepaths\n        if pd is None:\n            paths = connect_edges(self)\n        else:\n            paths = connect_edges_pd(self)\n        return self.edge_type(paths, kdims=self.nodes.kdims[:2])", "response": "Returns the fixed EdgePaths or computes direct connections between the nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_networkx(cls, G, positions, nodes=None, **kwargs):\n        if not isinstance(positions, dict):\n            positions = positions(G, **kwargs)\n\n        # Unpack edges\n        edges = defaultdict(list)\n        for start, end in G.edges():\n            for attr, value in sorted(G.adj[start][end].items()):\n                if isinstance(value, (list, dict)):\n                    continue # Cannot handle list or dict attrs\n                edges[attr].append(value)\n\n            # Handle tuple node indexes (used in 2D grid Graphs)\n            if isinstance(start, tuple):\n                start = str(start)\n            if isinstance(end, tuple):\n                end = str(end)\n            edges['start'].append(start)\n            edges['end'].append(end)\n        edge_cols = sorted([k for k in edges if k not in ('start', 'end')\n                            and len(edges[k]) == len(edges['start'])])\n        edge_vdims = [str(col) if isinstance(col, int) else col for col in edge_cols]\n        edge_data = tuple(edges[col] for col in ['start', 'end']+edge_cols)\n\n        # Unpack user node info\n        xdim, ydim, idim = cls.node_type.kdims[:3]\n        if nodes:\n            node_columns = nodes.columns()\n            idx_dim = nodes.kdims[0].name\n            info_cols, values = zip(*((k, v) for k, v in node_columns.items() if k != idx_dim))\n            node_info = {i: vals for i, vals in zip(node_columns[idx_dim], zip(*values))}\n        else:\n            info_cols = []\n            node_info = None\n        node_columns = defaultdict(list)\n\n        # Unpack node positions\n        for idx, pos in sorted(positions.items()):\n            node = G.nodes.get(idx)\n            if node is None:\n                continue\n            x, y = pos\n            node_columns[xdim.name].append(x)\n            node_columns[ydim.name].append(y)\n            for attr, value in node.items():\n                if isinstance(value, (list, dict)):\n                    continue\n                node_columns[attr].append(value)\n            for i, col in enumerate(info_cols):\n                node_columns[col].append(node_info[idx][i])\n            if isinstance(idx, tuple):\n                idx = str(idx) # Tuple node indexes handled as strings\n            node_columns[idim.name].append(idx)\n        node_cols = sorted([k for k in node_columns if k not in cls.node_type.kdims\n                            and len(node_columns[k]) == len(node_columns[xdim.name])])\n        columns = [xdim.name, ydim.name, idim.name]+node_cols+list(info_cols)\n        node_data = tuple(node_columns[col] for col in columns)\n\n        # Construct nodes\n        vdims = []\n        for col in node_cols:\n            if isinstance(col, int):\n                dim = str(col)\n            elif nodes is not None and col in nodes.vdims:\n                dim = nodes.get_dimension(col)\n            else:\n                dim = col\n            vdims.append(dim)\n        nodes = cls.node_type(node_data, vdims=vdims)\n\n        # Construct graph\n        return cls((edge_data, nodes), vdims=edge_vdims)", "response": "Generate a HoloViews element from a networkx. Graph object and a dictionary of node positions."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_vertices(cls, data):\n        try:\n            from scipy.spatial import Delaunay\n        except:\n            raise ImportError(\"Generating triangles from points requires, \"\n                              \"SciPy to be installed.\")\n        if not isinstance(data, Points):\n            data = Points(data)\n        if not len(data):\n            return cls(([], []))\n        tris = Delaunay(data.array([0, 1]))\n        return cls((tris.simplices, data))", "response": "Generate a Triangulation from a list of vertices."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the edgepaths of the current resource.", "response": "def edgepaths(self):\n        \"\"\"\n        Returns the EdgePaths by generating a triangle for each simplex.\n        \"\"\"\n        if self._edgepaths:\n            return self._edgepaths\n        elif not len(self):\n            edgepaths = self.edge_type([], kdims=self.nodes.kdims[:2])\n            self._edgepaths = edgepaths\n            return edgepaths\n\n        simplices = self.array([0, 1, 2]).astype(np.int32)\n        pts = self.nodes.array([0, 1]).astype(float)\n        pts = pts[simplices]\n        paths = np.pad(pts[:, [0, 1, 2, 0], :],\n                       pad_width=((0, 0), (0, 1), (0, 0)),\n                       mode='constant',\n                       constant_values=np.nan).reshape(-1, 2)[:-1]\n        edgepaths = self.edge_type([paths],\n                                    kdims=self.nodes.kdims[:2])\n        self._edgepaths = edgepaths\n        return edgepaths"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nallow selecting data by slices sets and scalar values from the node tree.", "response": "def select(self, selection_specs=None, **selection):\n        \"\"\"\n        Allows selecting data by the slices, sets and scalar values\n        along a particular dimension. The indices should be supplied as\n        keywords mapping between the selected dimension and\n        value. Additionally selection_specs (taking the form of a list\n        of type.group.label strings, types or functions) may be\n        supplied, which will ensure the selection is only applied if the\n        specs match the selected object.\n        \"\"\"\n        # Ensure that edgepaths are initialized so they can be selected on\n        self.edgepaths\n        return super(TriMesh, self).select(selection_specs=None,\n                                           selection_mode='nodes',\n                                           **selection)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nextending the ElementPlot _finalize_axis method to set appropriate locations and axes options for 3D Plots.", "response": "def _finalize_axis(self, key, **kwargs):\n        \"\"\"\n        Extends the ElementPlot _finalize_axis method to set appropriate\n        labels, and axes options for 3D Plots.\n        \"\"\"\n        axis = self.handles['axis']\n        self.handles['fig'].set_frameon(False)\n        axis.grid(self.show_grid)\n        axis.view_init(elev=self.elevation, azim=self.azimuth)\n        axis.dist = self.distance\n\n        if self.xaxis is None:\n            axis.w_xaxis.line.set_lw(0.)\n            axis.w_xaxis.label.set_text('')\n        if self.yaxis is None:\n            axis.w_yaxis.line.set_lw(0.)\n            axis.w_yaxis.label.set_text('')\n        if self.zaxis is None:\n            axis.w_zaxis.line.set_lw(0.)\n            axis.w_zaxis.label.set_text('')\n        if self.disable_axes:\n            axis.set_axis_off()\n\n        if mpl_version <= '1.5.9':\n            axis.set_axis_bgcolor(self.bgcolor)\n        else:\n            axis.set_facecolor(self.bgcolor)\n        return super(Plot3D, self)._finalize_axis(key, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a dataset object and data in the appropriate format for the interface return a simple scalar.", "response": "def unpack_scalar(cls, dataset, data):\n        \"\"\"\n        Given a dataset object and data in the appropriate format for\n        the interface, return a simple scalar.\n        \"\"\"\n        import dask.dataframe as dd\n        if len(data.columns) > 1 or len(data) != 1:\n            return data\n        if isinstance(data, dd.DataFrame):\n            data = data.compute()\n        return data.iat[0,0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the iloc of the data set.", "response": "def iloc(cls, dataset, index):\n        \"\"\"\n        Dask does not support iloc, therefore iloc will execute\n        the call graph and lose the laziness of the operation.\n        \"\"\"\n        rows, cols = index\n        scalar = False\n        if isinstance(cols, slice):\n            cols = [d.name for d in dataset.dimensions()][cols]\n        elif np.isscalar(cols):\n            scalar = np.isscalar(rows)\n            cols = [dataset.get_dimension(cols).name]\n        else:\n            cols = [dataset.get_dimension(d).name for d in index[1]]\n        if np.isscalar(rows):\n            rows = [rows]\n\n        data = OrderedDict()\n        for c in cols:\n            data[c] = dataset.data[c].compute().iloc[rows].values\n        if scalar:\n            return data[cols[0]][0]\n        return tuple(data.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef computeNodeLinks(cls, element, graph):\n        index = element.nodes.kdims[-1]\n        node_map = {}\n        if element.nodes.vdims:\n            values = zip(*(element.nodes.dimension_values(d)\n                           for d in element.nodes.vdims))\n        else:\n            values = cycle([tuple()])\n        for index, vals in zip(element.nodes.dimension_values(index), values):\n            node = {'index': index, 'sourceLinks': [], 'targetLinks': [], 'values': vals}\n            graph['nodes'].append(node)\n            node_map[index] = node\n\n        links = [element.dimension_values(d) for d in element.dimensions()[:3]]\n        for i, (src, tgt, value) in enumerate(zip(*links)):\n            source, target = node_map[src], node_map[tgt]\n            link = dict(index=i, source=source, target=target, value=value)\n            graph['links'].append(link)\n            source['sourceLinks'].append(link)\n            target['targetLinks'].append(link)", "response": "Populate the sourceLinks and targetLinks for each node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef computeNodeValues(cls, graph):\n        for node in graph['nodes']:\n            source_val = np.sum([l['value'] for l in node['sourceLinks']])\n            target_val = np.sum([l['value'] for l in node['targetLinks']])\n            node['value'] = max([source_val, target_val])", "response": "Compute the value of each node by summing the associated links."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef computeNodeDepths(self, graph):\n        nodes = graph['nodes']\n        depth = 0\n        while nodes:\n            next_nodes = []\n            for node in nodes:\n                node['depth'] = depth\n                for link in node['sourceLinks']:\n                    if not any(link['target'] is node for node in next_nodes):\n                        next_nodes.append(link['target'])\n            nodes = next_nodes\n            depth += 1\n            if depth > 10000:\n                raise RecursionError('Sankey diagrams only support acyclic graphs.')\n\n        nodes = graph['nodes']\n        depth = 0\n        while nodes:\n            next_nodes = []\n            for node in nodes:\n                node['height'] = depth\n                for link in node['targetLinks']:\n                    if not any(link['source'] is node for node in next_nodes):\n                        next_nodes.append(link['source'])\n            nodes = next_nodes\n            depth += 1\n            if depth > 10000:\n                raise RecursionError('Sankey diagrams only support acyclic graphs.')\n\n        x0, _, x1, _ = self.p.bounds\n        dx = self.p.node_width\n        kx = (x1 - x0 - dx) / (depth - 1)\n        for node in graph['nodes']:\n            d  = node['depth'] if node['sourceLinks'] else depth - 1\n            node['x0'] = x0 + max([0, min([depth-1, np.floor(d)]) * kx])\n            node['x1'] = node['x0'] + dx", "response": "Compute the maximum depth of nodes in a graph."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprocessing the histogram data into a list of tuples.", "response": "def _process_hist(self, hist):\n        \"\"\"\n        Get data from histogram, including bin_ranges and values.\n        \"\"\"\n        self.cyclic = hist.get_dimension(0).cyclic\n        x = hist.kdims[0]\n        edges = hist.interface.coords(hist, x, edges=True)\n        values = hist.dimension_values(1)\n        hist_vals = np.array(values)\n        xlim = hist.range(0)\n        ylim = hist.range(1)\n        is_datetime = isdatetime(edges)\n        if is_datetime:\n            edges = np.array([dt64_to_dt(e) if isinstance(e, np.datetime64) else e for e in edges])\n            edges = date2num(edges)\n            xlim = tuple(dt_to_int(v, 'D') for v in xlim)\n        widths = np.diff(edges)\n        return edges[:-1], hist_vals, widths, xlim+ylim, is_datetime"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _compute_ticks(self, element, edges, widths, lims):\n        if self.xticks is None or not isinstance(self.xticks, int):\n            return None\n        if self.cyclic:\n            x0, x1, _, _ = lims\n            xvals = np.linspace(x0, x1, self.xticks)\n            labels = [\"%.0f\" % np.rad2deg(x) + '\\N{DEGREE SIGN}' for x in xvals]\n        elif self.xticks:\n            dim = element.get_dimension(0)\n            inds = np.linspace(0, len(edges), self.xticks, dtype=np.int)\n            edges = list(edges) + [edges[-1] + widths[-1]]\n            xvals = [edges[i] for i in inds]\n            labels = [dim.pprint_value(v) for v in xvals]\n        return [xvals, labels]", "response": "Compute the ticks for the current log entry."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting axis settings options including ticks, x- and y-labels and limits.", "response": "def _process_axsettings(self, hist, lims, ticks):\n        \"\"\"\n        Get axis settings options including ticks, x- and y-labels\n        and limits.\n        \"\"\"\n        axis_settings = dict(zip(self.axis_settings, [None, None, (None if self.overlaid else ticks)]))\n        return axis_settings"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_artists(self, key, hist, edges, hvals, widths, lims, ranges):\n        plot_vals = zip(self.handles['artist'], edges, hvals, widths)\n        for bar, edge, height, width in plot_vals:\n            if self.invert_axes:\n                bar.set_y(edge)\n                bar.set_width(height)\n                bar.set_height(width)\n            else:\n                bar.set_x(edge)\n                bar.set_height(height)\n                bar.set_width(width)", "response": "Update all the artists in the histogram. Subclassable to maintain order of artists."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_plot(self, key, element, bars, lims, ranges):\n        main = self.adjoined.main\n        _, y1 = element.range(1)\n        offset = self.offset * y1\n        range_item, main_range, dim = get_sideplot_ranges(self, element, main, ranges)\n\n        # Check if plot is colormapped\n        plot_type = Store.registry['matplotlib'].get(type(range_item))\n        if isinstance(plot_type, PlotSelector):\n            plot_type = plot_type.get_plot_class(range_item)\n        opts = self.lookup_options(range_item, 'plot')\n        if plot_type and issubclass(plot_type, ColorbarPlot):\n            cidx = opts.options.get('color_index', None)\n            if cidx is None:\n                opts = self.lookup_options(range_item, 'style')\n                cidx = opts.kwargs.get('color', None)\n                if cidx not in range_item:\n                    cidx = None\n            cdim = None if cidx is None else range_item.get_dimension(cidx)\n        else:\n            cdim = None\n\n        # Get colormapping options\n        if isinstance(range_item, (HeatMap, Raster)) or cdim:\n            style = self.lookup_options(range_item, 'style')[self.cyclic_index]\n            cmap = cm.get_cmap(style.get('cmap'))\n            main_range = style.get('clims', main_range)\n        else:\n            cmap = None\n\n        if offset and ('offset_line' not in self.handles):\n            self.handles['offset_line'] = self.offset_linefn(offset,\n                                                             linewidth=1.0,\n                                                             color='k')\n        elif offset:\n            self._update_separator(offset)\n\n        if cmap is not None:\n            self._colorize_bars(cmap, bars, element, main_range, dim)\n        return bars", "response": "Update the colorbars for the current key and the element."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _colorize_bars(self, cmap, bars, element, main_range, dim):\n        cmap_range = main_range[1] - main_range[0]\n        lower_bound = main_range[0]\n        colors = np.array(element.dimension_values(dim))\n        colors = (colors - lower_bound) / (cmap_range)\n        for c, bar in zip(colors, bars):\n            bar.set_facecolor(cmap(c))\n            bar.set_clip_on(False)", "response": "Apply the given cmap to color the bars."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_separator(self, offset):\n        offset_line = self.handles['offset_line']\n        if offset == 0:\n            offset_line.set_visible(False)\n        else:\n            offset_line.set_visible(True)\n            if self.invert_axes:\n                offset_line.set_xdata(offset)\n            else:\n                offset_line.set_ydata(offset)", "response": "Update separator line based on offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_values(self):\n        (gi, _), (ci, _), (si, _) = self._get_dims(self.hmap.last)\n        ndims = self.hmap.last.ndims\n        dims = self.hmap.last.kdims\n        dimensions = []\n        values = {}\n        for vidx, vtype in zip([gi, ci, si], self._dimensions):\n            if vidx < ndims:\n                dim = dims[vidx]\n                dimensions.append(dim)\n                vals = self.hmap.dimension_values(dim.name)\n            else:\n                dimensions.append(None)\n                vals = [None]\n            values[vtype] = list(unique_iterator(vals))\n        return values, dimensions", "response": "Get unique index value for each bar\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_styles(self, element, style_groups):\n        style = self.lookup_options(element, 'style')[0]\n        sopts = []\n        for sopt in ['color', 'hatch']:\n            if sopt in style:\n                sopts.append(sopt)\n                style.pop(sopt, None)\n        color_groups = []\n        for sg in style_groups:\n            color_groups.append(self.values[sg])\n        style_product = list(product(*color_groups))\n        wrapped_style = self.lookup_options(element, 'style').max_cycles(len(style_product))\n        color_groups = {k:tuple(wrapped_style[n][sopt] for sopt in sopts)\n                        for n,k in enumerate(style_product)}\n\n        return style, color_groups, sopts", "response": "Computes the style of the given element and style_groups."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\napply ticks with appropriate offsets.", "response": "def _finalize_ticks(self, axis, element, xticks, yticks, zticks):\n        \"\"\"\n        Apply ticks with appropriate offsets.\n        \"\"\"\n        yalignments = None\n        if xticks is not None:\n            ticks, labels, yalignments = zip(*sorted(xticks, key=lambda x: x[0]))\n            xticks = (list(ticks), list(labels))\n        super(BarPlot, self)._finalize_ticks(axis, element, xticks, yticks, zticks)\n        if yalignments:\n            for t, y in zip(axis.get_xticklabels(), yalignments):\n                t.set_y(y)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup inherited from main element", "response": "def group(self):\n        \"Group inherited from main element\"\n        if self.main and self.main.group != type(self.main).__name__:\n            return self.main.group\n        else:\n            return 'AdjointLayout'"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclone object and apply new label and group and depth.", "response": "def relabel(self, label=None, group=None, depth=1):\n        \"\"\"Clone object and apply new group and/or label.\n\n        Applies relabeling to child up to the supplied depth.\n\n        Args:\n            label (str, optional): New label to apply to returned object\n            group (str, optional): New group to apply to returned object\n            depth (int, optional): Depth to which relabel will be applied\n                If applied to container allows applying relabeling to\n                contained objects up to the specified depth\n\n        Returns:\n            Returns relabelled object\n        \"\"\"\n        return super(AdjointLayout, self).relabel(label=label, group=group, depth=depth)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dimension_values(self, dimension, expanded=True, flat=True):\n        dimension = self.get_dimension(dimension, strict=True).name\n        return self.main.dimension_values(dimension, expanded, flat)", "response": "Returns the values along the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef grid_items(self):\n        if list(self.keys()) == []:  return {}\n        cols = self._max_cols\n        return {(idx // cols, idx % cols): (key, item)\n                for idx, (key, item) in enumerate(self.data.items())}", "response": "Compute a dict of row column - > item pairs from the\n        current set of items and specified number of columns."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef last(self):\n        last_items = []\n        for (k, v) in self.items():\n            if isinstance(v, NdMapping):\n                item = (k, v.clone((v.last_key, v.last)))\n            elif isinstance(v, AdjointLayout):\n                item = (k, v.last)\n            else:\n                item = (k, v)\n            last_items.append(item)\n        return self.clone(last_items)", "response": "Returns another NdLayout constituted of the last views of the\n        individual elements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clone(self, *args, **overrides):\n        clone = super(NdLayout, self).clone(*args, **overrides)\n        clone._max_cols = self._max_cols\n        clone.id = self.id\n        return clone", "response": "Clones the NdLayout overriding data and parameters and returns a new NdLayout object with the same attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone(self, *args, **overrides):\n        clone = super(Layout, self).clone(*args, **overrides)\n        clone._max_cols = self._max_cols\n        return clone", "response": "Clones the Layout overriding data and parameters and returns a new object with the same attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a json diff of the current state of the plot with the latest plot data.", "response": "def diff(self, plot, serialize=True):\n        \"\"\"\n        Returns a json diff required to update an existing plot with\n        the latest plot data.\n        \"\"\"\n        diff = plot.state\n        if serialize:\n            return json.dumps(diff, cls=utils.PlotlyJSONEncoder)\n        else:\n            return diff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_nb(cls, inline=True):\n        from IPython.display import publish_display_data\n        cls._loaded = True\n        init_notebook_mode(connected=not inline)\n        publish_display_data(data={MIME_TYPES['jlab-hv-load']:\n                                   get_plotlyjs()})", "response": "Loads the plotly notebook resources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a dataset object and data in the appropriate format for the interface return a simple scalar.", "response": "def unpack_scalar(cls, dataset, data):\n        \"\"\"\n        Given a dataset object and data in the appropriate format for\n        the interface, return a simple scalar.\n        \"\"\"\n        if len(data) != 1:\n            return data\n        key = list(data.keys())[0]\n\n        if len(data[key]) == 1 and key in dataset.vdims:\n            scalar = data[key][0]\n            return scalar.compute() if hasattr(scalar, 'compute') else scalar\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef comment_out_magics(source):\n    filtered = []\n    for line in source.splitlines():\n        if line.strip().startswith('%'):\n            filtered.append('# ' +  line)\n        else:\n            filtered.append(line)\n    return '\\n'.join(filtered)", "response": "Utility used to make sure that AST parser does not choke on unrecognized\n    magics."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap a cell expression in a HoloViews object.", "response": "def wrap_cell_expression(source, template='{expr}'):\n    \"\"\"\n    If a cell ends in an expression that could be displaying a HoloViews\n    object (as determined using the AST), wrap it with a given prefix\n    and suffix string.\n\n    If the cell doesn't end in an expression, return the source unchanged.\n    \"\"\"\n    cell_output_types = (ast.IfExp, ast.BoolOp, ast.BinOp, ast.Call,\n                         ast.Name, ast.Attribute)\n    try:\n        node = ast.parse(comment_out_magics(source))\n    except SyntaxError:\n        return source\n    filtered = source.splitlines()\n    if node.body != []:\n        last_expr = node.body[-1]\n        if not isinstance(last_expr, ast.Expr):\n            pass # Not an expression\n        elif isinstance(last_expr.value, cell_output_types):\n            # CAREFUL WITH UTF8!\n            expr_end_slice = filtered[last_expr.lineno-1][:last_expr.col_offset]\n            expr_start_slice = filtered[last_expr.lineno-1][last_expr.col_offset:]\n            start = '\\n'.join(filtered[:last_expr.lineno-1]\n                              + ([expr_end_slice] if expr_end_slice else []))\n            ending = '\\n'.join(([expr_start_slice] if expr_start_slice else [])\n                            + filtered[last_expr.lineno:])\n            # BUG!! Adds newline for 'foo'; <expr>\n            return start + '\\n' + template.format(expr=ending)\n    return source"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_magic(source, magic, strip=True):\n    filtered, magic_lines=[],[]\n    for line in source.splitlines():\n        if line.strip().startswith(magic):\n            magic_lines.append(line)\n        else:\n            filtered.append(line)\n    if strip:\n        magic_lines = [el.replace(magic,'') for el in magic_lines]\n    return '\\n'.join(filtered), magic_lines", "response": "Given a source of a cell filter out the given magic and collect the lines using the magic."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef strip_magics(source):\n    filtered=[]\n    for line in source.splitlines():\n        if not line.startswith('%') or line.startswith('%%'):\n            filtered.append(line)\n    return '\\n'.join(filtered)", "response": "Given a source of a cell strip out all cell and line magics."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef replace_line_magic(source, magic, template='{line}'):\n    filtered = []\n    for line in source.splitlines():\n        if line.strip().startswith(magic):\n            substitution = template.format(line=line.replace(magic, ''))\n            filtered.append(substitution)\n        else:\n            filtered.append(line)\n    return '\\n'.join(filtered)", "response": "Given a cell s source replace line magics using a formatting\n    template where line is the string that follows the magic."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive some source html substitute and annotated as applicable", "response": "def replace(self, src):\n        \"Given some source html substitute and annotated as applicable\"\n        for html in self.substitutions.keys():\n            if src == html:\n                annotation = self.annotation % self.substitutions[src][1]\n                return annotation + self.substitutions[src][0]\n        return src"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_subplot_number(subplot_val):\n    match = _subplot_re.match(subplot_val)\n    if match:\n        subplot_number = int(match.group(1))\n    else:\n        subplot_number = 1\n    return subplot_number", "response": "Extract the subplot number from a string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _normalize_subplot_ids(fig):\n\n    layout = fig.setdefault('layout', {})\n    for trace in fig.get('data', None):\n        trace_type = trace.get('type', 'scatter')\n        subplot_types = _trace_to_subplot.get(trace_type, [])\n        for subplot_type in subplot_types:\n\n            subplot_prop_name = _get_subplot_prop_name(subplot_type)\n            subplot_val_prefix = _get_subplot_val_prefix(subplot_type)\n            subplot_val = trace.get(subplot_prop_name, subplot_val_prefix)\n\n            # extract trailing number (if any)\n            subplot_number = _get_subplot_number(subplot_val)\n\n            if subplot_number > 1:\n                layout_prop_name = subplot_type + str(subplot_number)\n            else:\n                layout_prop_name = subplot_type\n\n            if layout_prop_name not in layout:\n                layout[layout_prop_name] = {}", "response": "Normalizes subplot IDs in a figure to include only the first subplot that is referenced by a trace."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_max_subplot_ids(fig):\n    max_subplot_ids = {subplot_type: 0\n                       for subplot_type in _subplot_types}\n    max_subplot_ids['xaxis'] = 0\n    max_subplot_ids['yaxis'] = 0\n\n    for trace in fig.get('data', []):\n        trace_type = trace.get('type', 'scatter')\n        subplot_types = _trace_to_subplot.get(trace_type, [])\n        for subplot_type in subplot_types:\n\n            subplot_prop_name = _get_subplot_prop_name(subplot_type)\n            subplot_val_prefix = _get_subplot_val_prefix(subplot_type)\n            subplot_val = trace.get(subplot_prop_name, subplot_val_prefix)\n\n            # extract trailing number (if any)\n            subplot_number = _get_subplot_number(subplot_val)\n\n            max_subplot_ids[subplot_type] = max(\n                max_subplot_ids[subplot_type], subplot_number)\n\n    return max_subplot_ids", "response": "Given an input figure return a dict containing the max subplot number for each subplot type in the figure and each subplot type in the figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies offsets to the subplot id numbers in a figure.", "response": "def _offset_subplot_ids(fig, offsets):\n    \"\"\"\n    Apply offsets to the subplot id numbers in a figure.\n\n    Note: This function mutates the input figure dict\n\n    Note: This function assumes that the normalize_subplot_ids function has\n    already been run on the figure, so that all layout subplot properties in\n    use are explicitly present in the figure's layout.\n\n    Parameters\n    ----------\n    fig: dict\n        A plotly figure dict\n    offsets: dict\n        A dict from subplot types to the offset to be applied for each subplot\n        type.  This dict matches the form of the dict returned by\n        get_max_subplot_ids\n    \"\"\"\n    # Offset traces\n    for trace in fig.get('data', None):\n        trace_type = trace.get('type', 'scatter')\n        subplot_types = _trace_to_subplot.get(trace_type, [])\n\n        for subplot_type in subplot_types:\n            subplot_prop_name = _get_subplot_prop_name(subplot_type)\n\n            # Compute subplot value prefix\n            subplot_val_prefix = _get_subplot_val_prefix(subplot_type)\n            subplot_val = trace.get(subplot_prop_name, subplot_val_prefix)\n            subplot_number = _get_subplot_number(subplot_val)\n\n            offset_subplot_number = (\n                    subplot_number + offsets.get(subplot_type, 0))\n\n            if offset_subplot_number > 1:\n                trace[subplot_prop_name] = (\n                        subplot_val_prefix + str(offset_subplot_number))\n            else:\n                trace[subplot_prop_name] = subplot_val_prefix\n\n    # layout subplots\n    layout = fig.setdefault('layout', {})\n    new_subplots = {}\n\n    for subplot_type in offsets:\n        offset = offsets[subplot_type]\n        if offset < 1:\n            continue\n\n        for layout_prop in list(layout.keys()):\n            if layout_prop.startswith(subplot_type):\n                subplot_number = _get_subplot_number(layout_prop)\n                new_subplot_number = subplot_number + offset\n                new_layout_prop = subplot_type + str(new_subplot_number)\n                new_subplots[new_layout_prop] = layout.pop(layout_prop)\n\n    layout.update(new_subplots)\n\n    # xaxis/yaxis anchors\n    x_offset = offsets.get('xaxis', 0)\n    y_offset = offsets.get('yaxis', 0)\n\n    for layout_prop in list(layout.keys()):\n        if layout_prop.startswith('xaxis'):\n            xaxis = layout[layout_prop]\n            anchor = xaxis.get('anchor', 'y')\n            anchor_number = _get_subplot_number(anchor) + y_offset\n            if anchor_number > 1:\n                xaxis['anchor'] = 'y' + str(anchor_number)\n            else:\n                xaxis['anchor'] = 'y'\n        elif layout_prop.startswith('yaxis'):\n            yaxis = layout[layout_prop]\n            anchor = yaxis.get('anchor', 'x')\n            anchor_number = _get_subplot_number(anchor) + x_offset\n            if anchor_number > 1:\n                yaxis['anchor'] = 'x' + str(anchor_number)\n            else:\n                yaxis['anchor'] = 'x'\n\n    # annotations/shapes/images\n    for layout_prop in ['annotations', 'shapes', 'images']:\n        for obj in layout.get(layout_prop, []):\n            if x_offset:\n                xref = obj.get('xref', 'x')\n                if xref != 'paper':\n                    xref_number = _get_subplot_number(xref)\n                    obj['xref'] = 'x' + str(xref_number + x_offset)\n\n            if y_offset:\n                yref = obj.get('yref', 'y')\n                if yref != 'paper':\n                    yref_number = _get_subplot_number(yref)\n                    obj['yref'] = 'y' + str(yref_number + y_offset)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nscale a figure and translate it to sub-region of the original figure canvas. Note: If the input figure has a title, this title is converted into an annotation and scaled along with the rest of the figure. Note: This function mutates the input fig dict Note: This function assumes that the normalize_subplot_ids function has already been run on the figure, so that all layout subplot properties in use are explicitly present in the figure's layout. Parameters ---------- fig: dict A plotly figure dict scale_x: float Factor by which to scale the figure in the x-direction. This will typically be a value < 1. E.g. a value of 0.5 will cause the resulting figure to be half as wide as the original. scale_y: float Factor by which to scale the figure in the y-direction. This will typically be a value < 1 translate_x: float Factor by which to translate the scaled figure in the x-direction in normalized coordinates. translate_y: float Factor by which to translate the scaled figure in the x-direction in normalized coordinates.", "response": "def _scale_translate(fig, scale_x, scale_y, translate_x, translate_y):\n    \"\"\"\n    Scale a figure and translate it to sub-region of the original\n    figure canvas.\n\n    Note: If the input figure has a title, this title is converted into an\n    annotation and scaled along with the rest of the figure.\n\n    Note: This function mutates the input fig dict\n\n    Note: This function assumes that the normalize_subplot_ids function has\n    already been run on the figure, so that all layout subplot properties in\n    use are explicitly present in the figure's layout.\n\n    Parameters\n    ----------\n    fig: dict\n        A plotly figure dict\n    scale_x: float\n        Factor by which to scale the figure in the x-direction. This will\n        typically be a value < 1.  E.g. a value of 0.5 will cause the\n        resulting figure to be half as wide as the original.\n    scale_y: float\n        Factor by which to scale the figure in the y-direction. This will\n        typically be a value < 1\n    translate_x: float\n        Factor by which to translate the scaled figure in the x-direction in\n        normalized coordinates.\n    translate_y: float\n        Factor by which to translate the scaled figure in the x-direction in\n        normalized coordinates.\n    \"\"\"\n    data = fig.setdefault('data', [])\n    layout = fig.setdefault('layout', {})\n\n    def scale_translate_x(x):\n        return [x[0] * scale_x + translate_x,\n                x[1] * scale_x + translate_x]\n\n    def scale_translate_y(y):\n        return [y[0] * scale_y + translate_y,\n                y[1] * scale_y + translate_y]\n\n    def perform_scale_translate(obj):\n        domain = obj.setdefault('domain', {})\n        x = domain.get('x', [0, 1])\n        y = domain.get('y', [0, 1])\n\n        domain['x'] = scale_translate_x(x)\n        domain['y'] = scale_translate_y(y)\n\n    # Scale/translate traces\n    for trace in data:\n        trace_type = trace.get('type', 'scatter')\n        if trace_type in _domain_trace_types:\n            perform_scale_translate(trace)\n\n    # Scale/translate subplot containers\n    for prop in layout:\n        for subplot_type in _subplot_types:\n            if prop.startswith(subplot_type):\n                perform_scale_translate(layout[prop])\n\n    for prop in layout:\n        if prop.startswith('xaxis'):\n            xaxis = layout[prop]\n            x_domain = xaxis.get('domain', [0, 1])\n            xaxis['domain'] = scale_translate_x(x_domain)\n        elif prop.startswith('yaxis'):\n            yaxis = layout[prop]\n            y_domain = yaxis.get('domain', [0, 1])\n            yaxis['domain'] = scale_translate_y(y_domain)\n\n    # convert title to annotation\n    # This way the annotation will be scaled with the reset of the figure\n    annotations = layout.get('annotations', [])\n\n    title = layout.pop('title', None)\n    if title:\n        titlefont = layout.pop('titlefont', {})\n        title_fontsize = titlefont.get('size', 17)\n        min_fontsize = 12\n        titlefont['size'] = round(min_fontsize +\n                                  (title_fontsize - min_fontsize) * scale_x)\n\n        annotations.append({\n            'text': title,\n            'showarrow': False,\n            'xref': 'paper',\n            'yref': 'paper',\n            'x': 0.5,\n            'y': 1.01,\n            'xanchor': 'center',\n            'yanchor': 'bottom',\n            'font': titlefont\n        })\n        layout['annotations'] = annotations\n\n    # annotations\n    for obj in layout.get('annotations', []):\n        if obj.get('xref', None) == 'paper':\n            obj['x'] = obj.get('x', 0.5) * scale_x + translate_x\n            obj['y'] = obj.get('y', 0.5) * scale_y + translate_y"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmerges a sub - figure into a parent figure", "response": "def merge_figure(fig, subfig):\n    \"\"\"\n    Merge a sub-figure into a parent figure\n\n    Note: This function mutates the input fig dict, but it does not mutate\n    the subfig dict\n\n    Parameters\n    ----------\n    fig: dict\n        The plotly figure dict into which the sub figure will be merged\n    subfig: dict\n        The plotly figure dict that will be copied and then merged into `fig`\n    \"\"\"\n\n    # traces\n    data = fig.setdefault('data', [])\n    data.extend(copy.deepcopy(subfig.get('data', [])))\n\n    # layout\n    layout = fig.setdefault('layout', {})\n    _merge_layout_objs(layout, subfig.get('layout', {}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_layout_objs(obj, subobj):\n    for prop, val in subobj.items():\n        if isinstance(val, dict) and prop in obj:\n            # recursion\n            _merge_layout_objs(obj[prop], val)\n        elif (isinstance(val, list) and\n              obj.get(prop, None) and\n              isinstance(obj[prop][0], dict)):\n\n            # append\n            obj[prop].extend(val)\n        else:\n            # init/overwrite\n            obj[prop] = copy.deepcopy(val)", "response": "Recursively merges layout objects into the base dict obj."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_subplot_domains(widths, spacing):\n    # normalize widths\n    widths_sum = float(sum(widths))\n    total_spacing = (len(widths) - 1) * spacing\n    widths = [(w / widths_sum)*(1-total_spacing) for w in widths]\n    domains = []\n\n    for c in range(len(widths)):\n        domain_start = c * spacing + sum(widths[:c])\n        domain_stop = min(1, domain_start + widths[c])\n        domains.append((domain_start, domain_stop))\n\n    return domains", "response": "Compute normalized domain tuples for a list of widths and a subplot\n    spacing value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef figure_grid(figures_grid,\n                row_heights=None,\n                column_widths=None,\n                row_spacing=0.15,\n                column_spacing=0.15,\n                share_xaxis=False,\n                share_yaxis=False):\n    \"\"\"\n    Construct a figure from a 2D grid of sub-figures\n\n    Parameters\n    ----------\n    figures_grid: list of list of (dict or None)\n        2D list of plotly figure dicts that will be combined in a grid to\n        produce the resulting figure.  None values maybe used to leave empty\n        grid cells\n    row_heights: list of float (default None)\n        List of the relative heights of each row in the grid (these values\n        will be normalized by the function)\n    column_widths: list of float (default None)\n        List of the relative widths of each column in the grid (these values\n        will be normalized by the function)\n    row_spacing: float (default 0.15)\n        Vertical spacing between rows in the gird in normalized coordinates\n    column_spacing: float (default 0.15)\n        Horizontal spacing between columns in the grid in normalized\n        coordinates\n    share_xaxis: bool (default False)\n        Share x-axis between sub-figures in the same column. This will only\n        work if each sub-figure has a single x-axis\n    share_yaxis: bool (default False)\n        Share y-axis between sub-figures in the same row. This will only work\n        if each subfigure has a single y-axis\n\n    Returns\n    -------\n    dict\n        A plotly figure dict\n    \"\"\"\n\n    # compute number of rows/cols\n    rows = len(figures_grid)\n    columns = len(figures_grid[0])\n\n    # Initialize row heights / column widths\n    if not row_heights:\n        row_heights = [1 for _ in range(rows)]\n\n    if not column_widths:\n        column_widths = [1 for _ in range(columns)]\n\n    # Compute domain widths/heights for subplots\n    column_domains = _compute_subplot_domains(column_widths, column_spacing)\n    row_domains = _compute_subplot_domains(row_heights, row_spacing)\n\n    output_figure = {'data': [], 'layout': {}}\n\n    for r, (fig_row, row_domain) in enumerate(zip(figures_grid, row_domains)):\n        for c, (fig, column_domain) in enumerate(zip(fig_row, column_domains)):\n            if fig:\n                fig = copy.deepcopy(fig)\n\n                _normalize_subplot_ids(fig)\n\n                subplot_offsets = _get_max_subplot_ids(output_figure)\n\n                if share_xaxis:\n                    subplot_offsets['xaxis'] = c\n                    if r != 0:\n                        # Only use xaxes from bottom row\n                        fig.get('layout', {}).pop('xaxis', None)\n\n                if share_yaxis:\n                    subplot_offsets['yaxis'] = r\n                    if c != 0:\n                        # Only use yaxes from first column\n                        fig.get('layout', {}).pop('yaxis', None)\n\n                _offset_subplot_ids(fig, subplot_offsets)\n\n                scale_x = column_domain[1] - column_domain[0]\n                scale_y = row_domain[1] - row_domain[0]\n                _scale_translate(fig,\n                                 scale_x, scale_y,\n                                 column_domain[0], row_domain[0])\n\n                merge_figure(output_figure, fig)\n\n    return output_figure", "response": "Construct a figure from a 2D grid of figures."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a colormap spec to a plotly colorscale.", "response": "def get_colorscale(cmap, levels=None, cmin=None, cmax=None):\n    \"\"\"Converts a cmap spec to a plotly colorscale\n\n    Args:\n        cmap: A recognized colormap by name or list of colors\n        levels: A list or integer declaring the color-levels\n        cmin: The lower bound of the color range\n        cmax: The upper bound of the color range\n\n    Returns:\n        A valid plotly colorscale\n    \"\"\"\n    ncolors = levels if isinstance(levels, int) else None\n    if isinstance(levels, list):\n        ncolors = len(levels) - 1\n        if isinstance(cmap, list) and len(cmap) != ncolors:\n            raise ValueError('The number of colors in the colormap '\n                             'must match the intervals defined in the '\n                             'color_levels, expected %d colors found %d.'\n                             % (ncolors, len(cmap)))\n    try:\n        palette = process_cmap(cmap, ncolors)\n    except Exception as e:\n        colorscale = colors.PLOTLY_SCALES.get(cmap)\n        if colorscale is None:\n            raise e\n        return colorscale\n\n    if isinstance(levels, int):\n        colorscale = []\n        scale = np.linspace(0, 1, levels+1)\n        for i in range(levels+1):\n            if i == 0:\n                colorscale.append((scale[0], palette[i]))\n            elif i == levels:\n                colorscale.append((scale[-1], palette[-1]))\n            else:\n                colorscale.append((scale[i], palette[i-1]))\n                colorscale.append((scale[i], palette[i]))\n        return colorscale\n    elif isinstance(levels, list):\n        palette, (cmin, cmax) = color_intervals(\n            palette, levels, clip=(cmin, cmax))\n    return colors.make_colorscale(palette)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a HoloViews Viewable return a corresponding plot instance.", "response": "def get_plot(self_or_cls, obj, renderer=None, **kwargs):\n        \"\"\"\n        Given a HoloViews Viewable return a corresponding plot instance.\n        \"\"\"\n        if isinstance(obj, DynamicMap) and obj.unbounded:\n            dims = ', '.join('%r' % dim for dim in obj.unbounded)\n            msg = ('DynamicMap cannot be displayed without explicit indexing '\n                   'as {dims} dimension(s) are unbounded. '\n                   '\\nSet dimensions bounds with the DynamicMap redim.range '\n                   'or redim.values methods.')\n            raise SkipRendering(msg.format(dims=dims))\n\n        # Initialize DynamicMaps with first data item\n        initialize_dynamic(obj)\n\n        if not isinstance(obj, Plot):\n            if not displayable(obj):\n                obj = collate(obj)\n                initialize_dynamic(obj)\n            obj = Compositor.map(obj, mode='data', backend=self_or_cls.backend)\n\n        if not renderer:\n            renderer = self_or_cls\n            if not isinstance(self_or_cls, Renderer):\n                renderer = self_or_cls.instance()\n        if not isinstance(obj, Plot):\n            obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n            plot_opts = dict(self_or_cls.plot_options(obj, self_or_cls.size),\n                             **kwargs)\n            plot = self_or_cls.plotting_class(obj)(obj, renderer=renderer,\n                                                   **plot_opts)\n            defaults = [kd.default for kd in plot.dimensions]\n            init_key = tuple(v if d is None else d for v, d in\n                             zip(plot.keys[0], defaults))\n            plot.update(init_key)\n        else:\n            plot = obj\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate(self, obj, fmt, **kwargs):\n        if isinstance(obj, tuple(self.widgets.values())):\n            return obj, 'html'\n        plot = self.get_plot(obj, renderer=self, **kwargs)\n\n        fig_formats = self.mode_formats['fig'][self.mode]\n        holomap_formats = self.mode_formats['holomap'][self.mode]\n\n        if fmt in ['auto', None]:\n            if (((len(plot) == 1 and not plot.dynamic)\n                or (len(plot) > 1 and self.holomap is None) or\n                (plot.dynamic and len(plot.keys[0]) == 0)) or\n                not unbound_dimensions(plot.streams, plot.dimensions, no_duplicates=False)):\n                fmt = fig_formats[0] if self.fig=='auto' else self.fig\n            else:\n                fmt = holomap_formats[0] if self.holomap=='auto' else self.holomap\n\n        if fmt in self.widgets:\n            plot = self.get_widget(plot, fmt, display_options={'fps': self.fps})\n            fmt = 'html'\n\n        all_formats = set(fig_formats + holomap_formats)\n        if fmt not in all_formats:\n            raise Exception(\"Format %r not supported by mode %r. Allowed formats: %r\"\n                            % (fmt, self.mode, fig_formats + holomap_formats))\n        self.last_plot = plot\n        return plot, fmt", "response": "Validate the object and return the plot and format."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the post - render hooks to the data.", "response": "def _apply_post_render_hooks(self, data, obj, fmt):\n        \"\"\"\n        Apply the post-render hooks to the data.\n        \"\"\"\n        hooks = self.post_render_hooks.get(fmt,[])\n        for hook in hooks:\n            try:\n                data = hook(data, obj)\n            except Exception as e:\n                self.param.warning(\"The post_render_hook %r could not \"\n                                   \"be applied:\\n\\n %s\" % (hook, e))\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html(self, obj, fmt=None, css=None, **kwargs):\n        plot, fmt =  self._validate(obj, fmt)\n        figdata, _ = self(plot, fmt, **kwargs)\n        if css is None: css = self.css\n\n        if fmt in ['html', 'json']:\n            return figdata\n        else:\n            if fmt == 'svg':\n                figdata = figdata.encode(\"utf-8\")\n            elif fmt == 'pdf' and 'height' not in css:\n                _, h = self.get_size(plot)\n                css['height'] = '%dpx' % (h*self.dpi*1.15)\n\n        if isinstance(css, dict):\n            css = '; '.join(\"%s: %s\" % (k, v) for k, v in css.items())\n        else:\n            raise ValueError(\"CSS must be supplied as Python dictionary\")\n\n        b64 = base64.b64encode(figdata).decode(\"utf-8\")\n        (mime_type, tag) = MIME_TYPES[fmt], HTML_TAGS[fmt]\n        src = HTML_TAGS['base64'].format(mime_type=mime_type, b64=b64)\n        html = tag.format(src=src, mime_type=mime_type, css=css)\n        return html", "response": "Renders a plot or data structure and wraps the output in HTML."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning data and metadata dictionaries containing HTML and JS components for the object.", "response": "def components(self, obj, fmt=None, comm=True, **kwargs):\n        \"\"\"\n        Returns data and metadata dictionaries containing HTML and JS\n        components to include render in app, notebook, or standalone\n        document. Depending on the backend the fmt defines the format\n        embedded in the HTML, e.g. png or svg. If comm is enabled the\n        JS code will set up a Websocket comm channel using the\n        currently defined CommManager.\n        \"\"\"\n        if isinstance(obj, (Plot, NdWidget)):\n            plot = obj\n        else:\n            plot, fmt = self._validate(obj, fmt)\n\n        widget_id = None\n        data, metadata = {}, {}\n        if isinstance(plot, NdWidget):\n            js, html = plot(as_script=True)\n            plot_id = plot.plot_id\n            widget_id = plot.id\n        else:\n            html, js = self._figure_data(plot, fmt, as_script=True, **kwargs)\n            plot_id = plot.id\n            if comm and plot.comm is not None and self.comm_msg_handler:\n                msg_handler = self.comm_msg_handler.format(plot_id=plot_id)\n                html = plot.comm.html_template.format(init_frame=html,\n                                                      plot_id=plot_id)\n                comm_js = plot.comm.js_template.format(msg_handler=msg_handler,\n                                                       comm_id=plot.comm.id,\n                                                       plot_id=plot_id)\n                js = '\\n'.join([js, comm_js])\n            html = \"<div id='%s' style='display: table; margin: 0 auto;'>%s</div>\" % (plot_id, html)\n        if not os.environ.get('HV_DOC_HTML', False) and js is not None:\n            js = embed_js.format(widget_id=widget_id, plot_id=plot_id, html=html) + js\n\n        data['text/html'] = html\n        if js:\n            data[MIME_TYPES['js']] = js\n            data[MIME_TYPES['jlab-hv-exec']] = ''\n            metadata['id'] = plot_id\n            self._plots[plot_id] = plot\n        return (data, {MIME_TYPES['jlab-hv-exec']: metadata})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a static HTML with the rendered object in the specified format.", "response": "def static_html(self, obj, fmt=None, template=None):\n        \"\"\"\n        Generates a static HTML with the rendered object in the\n        supplied format. Allows supplying a template formatting string\n        with fields to interpolate 'js', 'css' and the main 'html'.\n        \"\"\"\n        js_html, css_html = self.html_assets()\n        if template is None: template = static_template\n        html = self.html(obj, fmt)\n        return template.format(js=js_html, css=css_html, html=html)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_widgets(self_or_cls, obj, filename, fmt=None, template=None,\n                       json=False, json_path='', **kwargs):\n        \"\"\"\n        Render and export object as a widget to a static HTML\n        file. Allows supplying a custom template formatting string\n        with fields to interpolate 'js', 'css' and the main 'html'\n        containing the widget. Also provides options to export widget\n        data to a json file in the supplied json_path (defaults to\n        current path).\n        \"\"\"\n        if fmt not in list(self_or_cls.widgets.keys())+['auto', None]:\n            raise ValueError(\"Renderer.export_widget may only export \"\n                             \"registered widget types.\")\n\n        if not isinstance(obj, NdWidget):\n            if not isinstance(filename, (BytesIO, StringIO)):\n                filedir = os.path.dirname(filename)\n                current_path = os.getcwd()\n                html_path = os.path.abspath(filedir)\n                rel_path = os.path.relpath(html_path, current_path)\n                save_path = os.path.join(rel_path, json_path)\n            else:\n                save_path = json_path\n            kwargs['json_save_path'] = save_path\n            kwargs['json_load_path'] = json_path\n            widget = self_or_cls.get_widget(obj, fmt, **kwargs)\n        else:\n            widget = obj\n\n        html = self_or_cls.static_html(widget, fmt, template)\n        encoded = self_or_cls.encode((html, {'mime_type': 'text/html'}))\n        if isinstance(filename, (BytesIO, StringIO)):\n            filename.write(encoded)\n            filename.seek(0)\n        else:\n            with open(filename, 'wb') as f:\n                f.write(encoded)", "response": "Render and export object as a widget to a static HTML file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plotting_class(cls, obj):\n        if isinstance(obj, AdjointLayout) or obj is AdjointLayout:\n            obj  = Layout\n        if isinstance(obj, type):\n            element_type = obj\n        else:\n            element_type = obj.type if isinstance(obj, HoloMap) else type(obj)\n        try:\n            plotclass = Store.registry[cls.backend][element_type]\n        except KeyError:\n            raise SkipRendering(\"No plotting class for {0} \"\n                                \"found\".format(element_type.__name__))\n        return plotclass", "response": "Given an object or Element class return the suitable plotting class needed to render it with the current renderer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef html_assets(cls, core=True, extras=True, backends=None, script=False):\n        if backends is None:\n            backends = [cls.backend] if cls.backend else []\n\n        # Get all the widgets and find the set of required js widget files\n        widgets = [wdgt for r in [Renderer]+Renderer.__subclasses__()\n                   for wdgt in r.widgets.values()]\n        css = list({wdgt.css for wdgt in widgets})\n        basejs = list({wdgt.basejs for wdgt in widgets})\n        extensionjs = list({wdgt.extensionjs for wdgt in widgets})\n\n        # Join all the js widget code into one string\n        path = os.path.dirname(os.path.abspath(__file__))\n\n        def open_and_read(path, f):\n            with open(find_file(path, f), 'r') as f:\n                txt = f.read()\n            return txt\n\n        widgetjs = '\\n'.join(open_and_read(path, f)\n                             for f in basejs + extensionjs if f is not None)\n        widgetcss = '\\n'.join(open_and_read(path, f)\n                              for f in css if f is not None)\n\n        dependencies = {}\n        if core:\n            dependencies.update(cls.core_dependencies)\n        if extras:\n            dependencies.update(cls.extra_dependencies)\n        for backend in backends:\n            dependencies[backend] = Store.renderers[backend].backend_dependencies\n\n        js_html, css_html = '', ''\n        for _, dep in sorted(dependencies.items(), key=lambda x: x[0]):\n            js_data = dep.get('js', [])\n            if isinstance(js_data, tuple):\n                for js in js_data:\n                    if script:\n                        js_html += js\n                    else:\n                        js_html += '\\n<script type=\"text/javascript\">%s</script>' % js\n            elif not script:\n                for js in js_data:\n                    js_html += '\\n<script src=\"%s\" type=\"text/javascript\"></script>' % js\n            css_data = dep.get('css', [])\n            if isinstance(js_data, tuple):\n                for css in css_data:\n                    css_html += '\\n<style>%s</style>' % css\n            else:\n                for css in css_data:\n                    css_html += '\\n<link rel=\"stylesheet\" href=\"%s\">' % css\n        if script:\n            js_html += widgetjs\n        else:\n            js_html += '\\n<script type=\"text/javascript\">%s</script>' % widgetjs\n        css_html += '\\n<style>%s</style>' % widgetcss\n\n        comm_js = cls.comm_manager.js_manager\n        if script:\n            js_html += comm_js\n        else:\n            js_html += '\\n<script type=\"text/javascript\">%s</script>' % comm_js\n\n        return unicode(js_html), unicode(css_html)", "response": "Returns the HTML assets for the widgets."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self_or_cls, obj, basename, fmt='auto', key={}, info={}, options=None, **kwargs):\n        if info or key:\n            raise Exception('Renderer does not support saving metadata to file.')\n\n        if isinstance(obj, (Plot, NdWidget)):\n            plot = obj\n        else:\n            with StoreOptions.options(obj, options, **kwargs):\n                plot = self_or_cls.get_plot(obj)\n\n        if (fmt in list(self_or_cls.widgets.keys())+['auto']) and len(plot) > 1:\n            with StoreOptions.options(obj, options, **kwargs):\n                if isinstance(basename, basestring):\n                    basename = basename+'.html'\n                self_or_cls.export_widgets(plot, basename, fmt)\n            return\n\n        rendered = self_or_cls(plot, fmt)\n        if rendered is None: return\n        (data, info) = rendered\n        encoded = self_or_cls.encode(rendered)\n        prefix = self_or_cls._save_prefix(info['file-ext'])\n        if prefix:\n            encoded = prefix + encoded\n        if isinstance(basename, (BytesIO, StringIO)):\n            basename.write(encoded)\n            basename.seek(0)\n        else:\n            filename ='%s.%s' % (basename, info['file-ext'])\n            with open(filename, 'wb') as f:\n                f.write(encoded)", "response": "Save a HoloViews object to file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_nb(cls, inline=True):\n        with param.logging_level('ERROR'):\n            cls.notebook_context = True\n            cls.comm_manager = JupyterCommManager", "response": "Loads any resources required for display of plots\n        in Jupyter notebook."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _delete_plot(cls, plot_id):\n        plot = cls._plots.get(plot_id)\n        if plot is None:\n            return\n        plot.cleanup()\n        del cls._plots[plot_id]", "response": "Deletes a plot from the cache and calls Plot. cleanup"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the strength of the match ( None if no match )", "response": "def _match(cls, el, spec):\n        \"Return the strength of the match (None if no match)\"\n        spec_dict = dict(zip(['type', 'group', 'label'], spec.split('.')))\n        if not isinstance(el, Image) or spec_dict['type'] != 'Image':\n            raise NotImplementedError(\"Only Image currently supported\")\n\n        sanitizers = {'group':group_sanitizer, 'label':label_sanitizer}\n        strength = 1\n        for key in ['group', 'label']:\n            attr_value = sanitizers[key](getattr(el, key))\n            if key in spec_dict:\n                if spec_dict[key] != attr_value: return None\n                strength += 1\n        return strength"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _match_overlay(self, raster, overlay_spec):\n        ordering = [None]*len(overlay_spec) # Elements to overlay\n        strengths = [0]*len(overlay_spec)   # Match strengths\n\n        elements = raster.values() if isinstance(raster, Overlay) else [raster]\n\n        for el in elements:\n            for pos in range(len(overlay_spec)):\n                strength = self._match(el, overlay_spec[pos])\n                if strength is None:               continue  # No match\n                elif (strength <= strengths[pos]): continue  # Weaker match\n                else:                                        # Stronger match\n                    ordering[pos] = el\n                    strengths[pos] = strength\n        return ordering, strengths", "response": "Given a raster or input overlay generate a list of matched\n        elements and corresponding tuple of match\n        strength values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a Dataset template used as a wrapper around the data contained within the multi - interface dataset.", "response": "def _inner_dataset_template(cls, dataset):\n        \"\"\"\n        Returns a Dataset template used as a wrapper around the data\n        contained within the multi-interface dataset.\n        \"\"\"\n        from . import Dataset\n        vdims = dataset.vdims if getattr(dataset, 'level', None) is None else []\n        return Dataset(dataset.data[0], datatype=cls.subtypes,\n                       kdims=dataset.kdims, vdims=vdims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef isscalar(cls, dataset, dim):\n        if not dataset.data:\n            return True\n        ds = cls._inner_dataset_template(dataset)\n        isscalar = []\n        for d in dataset.data:\n            ds.data = d\n            isscalar.append(ds.interface.isscalar(ds, dim))\n        return all(isscalar)", "response": "Tests if dimension is scalar in each subpath."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nselect all the related entries in the dataset.", "response": "def select(cls, dataset, selection_mask=None, **selection):\n        \"\"\"\n        Applies selectiong on all the subpaths.\n        \"\"\"\n        if not dataset.data:\n            return []\n        ds = cls._inner_dataset_template(dataset)\n        data = []\n        for d in dataset.data:\n            ds.data = d\n            sel = ds.interface.select(ds, **selection)\n            data.append(sel)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select_paths(cls, dataset, selection):\n        return [s[0] for s in np.array([{0: p} for p in dataset.data])[selection]]", "response": "Allows selecting paths with usual NumPy slicing index."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shape(cls, dataset):\n        if not dataset.data:\n            return (0, len(dataset.dimensions()))\n\n        rows, cols = 0, 0\n        ds = cls._inner_dataset_template(dataset)\n        for d in dataset.data:\n            ds.data = d\n            r, cols = ds.interface.shape(ds)\n            rows += r\n        return rows+len(dataset.data)-1, cols", "response": "Returns the shape of all subpaths making it appear like a\n            single array of concatenated subpaths separated by NaN values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the length of the multi - tabular dataset making it appear", "response": "def length(cls, dataset):\n        \"\"\"\n        Returns the length of the multi-tabular dataset making it appear\n        like a single array of concatenated subpaths separated by NaN\n        values.\n        \"\"\"\n        if not dataset.data:\n            return 0\n        length = 0\n        ds = cls._inner_dataset_template(dataset)\n        for d in dataset.data:\n            ds.data = d\n            length += ds.interface.length(ds)\n        return length+len(dataset.data)-1"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a single concatenated array of all subpaths separated by NaN values.", "response": "def values(cls, dataset, dimension, expanded, flat):\n        \"\"\"\n        Returns a single concatenated array of all subpaths separated\n        by NaN values. If expanded keyword is False an array of arrays\n        is returned.\n        \"\"\"\n        if not dataset.data:\n            return np.array([])\n        values = []\n        ds = cls._inner_dataset_template(dataset)\n        for d in dataset.data:\n            ds.data = d\n            dvals = ds.interface.values(ds, dimension, expanded, flat)\n            if not len(dvals):\n                continue\n            elif expanded:\n                values.append(dvals)\n                values.append([np.NaN])\n            else:\n                values.append(dvals)\n        if not values:\n            return np.array([])\n        elif expanded:\n            return np.concatenate(values[:-1])\n        else:\n            return np.concatenate(values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef split(cls, dataset, start, end, datatype, **kwargs):\n        objs = []\n        if datatype is None:\n            for d in dataset.data[start: end]:\n                objs.append(dataset.clone(d, datatype=cls.subtypes))\n            return objs\n        elif not dataset.data:\n            return objs\n        ds = cls._inner_dataset_template(dataset)\n        for d in dataset.data:\n            ds.data = d\n            if datatype == 'array':\n                obj = ds.array(**kwargs)\n            elif datatype == 'dataframe':\n                obj = ds.dframe(**kwargs)\n            elif datatype == 'columns':\n                if ds.interface.datatype == 'dictionary':\n                    obj = dict(ds.data)\n                else:\n                    obj = ds.columns(**kwargs)\n            else:\n                raise ValueError(\"%s datatype not support\" % datatype)\n            objs.append(obj)\n        return objs", "response": "Splits a multi - interface Dataset into regular Datasets using\n        regular tabular interfaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hist(self, dimension=None, num_bins=20, bin_range=None,\n             adjoin=True, **kwargs):\n        \"\"\"Computes and adjoins histogram along specified dimension(s).\n\n        Defaults to first value dimension if present otherwise falls\n        back to first key dimension.\n\n        Args:\n            dimension: Dimension(s) to compute histogram on\n            num_bins (int, optional): Number of bins\n            bin_range (tuple optional): Lower and upper bounds of bins\n            adjoin (bool, optional): Whether to adjoin histogram\n\n        Returns:\n            AdjointLayout of element and histogram or just the\n            histogram\n        \"\"\"\n        from ..operation import histogram\n        if not isinstance(dimension, list): dimension = [dimension]\n        hists = []\n        for d in dimension[::-1]:\n            hist = histogram(self, num_bins=num_bins, bin_range=bin_range,\n                             dimension=d, **kwargs)\n            hists.append(hist)\n        if adjoin:\n            layout = self\n            for didx in range(len(dimension)):\n                layout = layout << hists[didx]\n        elif len(dimension) > 1:\n            layout = Layout(hists)\n        else:\n            layout = hists[0]\n        return layout", "response": "Computes and adjoins histogram along specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dframe(self, dimensions=None, multi_index=False):\n        import pandas as pd\n        if dimensions is None:\n            dimensions = [d.name for d in self.dimensions()]\n        else:\n            dimensions = [self.get_dimension(d, strict=True).name for d in dimensions]\n        column_names = dimensions\n        dim_vals = OrderedDict([(dim, self.dimension_values(dim)) for dim in column_names])\n        df = pd.DataFrame(dim_vals)\n        if multi_index:\n            df = df.set_index([d for d in dimensions if d in self.kdims])\n        return df", "response": "Convert dimension values to DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert dimension values to columnar array.", "response": "def array(self, dimensions=None):\n        \"\"\"Convert dimension values to columnar array.\n\n        Args:\n            dimensions: List of dimensions to return\n\n        Returns:\n            Array of columns corresponding to each dimension\n        \"\"\"\n        if dimensions is None:\n            dims = [d for d in self.kdims + self.vdims]\n        else:\n            dims = [self.get_dimension(d, strict=True) for d in dimensions]\n\n        columns, types = [], []\n        for dim in dims:\n            column = self.dimension_values(dim)\n            columns.append(column)\n            types.append(column.dtype.kind)\n        if len(set(types)) > 1:\n            columns = [c.astype('object') for c in columns]\n        return np.column_stack(columns)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef table(self, datatype=None):\n        \"Deprecated method to convert any Element to a Table.\"\n        if config.future_deprecations:\n            self.param.warning(\n                \"The table method is deprecated and should no \"\n                \"longer be used. Instead cast the %s to a \"\n                \"a Table directly.\" % type(self).__name__)\n\n        if datatype and not isinstance(datatype, list):\n            datatype = [datatype]\n        from ..element import Table\n        return Table(self, **(dict(datatype=datatype) if datatype else {}))", "response": "Deprecated method to convert any Element to a Table."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeprecate method to convert data to dictionary", "response": "def mapping(self, kdims=None, vdims=None, **kwargs):\n        \"Deprecated method to convert data to dictionary\"\n        if config.future_deprecations:\n            self.param.warning(\n                \"The mapping method is deprecated and should no \"\n                \"longer be used. Use another one of the common \"\n                \"formats instead, e.g. .dframe, .array or .columns.\")\n\n        length = len(self)\n        if not kdims: kdims = self.kdims\n        if kdims:\n            keys = zip(*[self.dimension_values(dim.name)\n                         for dim in self.kdims])\n        else:\n            keys = [()]*length\n\n        if not vdims: vdims = self.vdims\n        if vdims:\n            values = zip(*[self.dimension_values(dim.name)\n                           for dim in vdims])\n        else:\n            values = [()]*length\n        return OrderedDict(zip(keys, values))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pprint_cell(self, row, col):\n        ndims = self.ndims\n        if col >= self.cols:\n            raise Exception(\"Maximum column index is %d\" % self.cols-1)\n        elif row >= self.rows:\n            raise Exception(\"Maximum row index is %d\" % self.rows-1)\n        elif row == 0:\n            if col >= ndims:\n                if self.vdims:\n                    return self.vdims[col - ndims].pprint_label\n                else:\n                    return ''\n            return self.kdims[col].pprint_label\n        else:\n            dim = self.get_dimension(col)\n            return dim.pprint_value(self.iloc[row-1, col])", "response": "Returns the contents of a table cell."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef static_dimensions(self):\n        dimensions = []\n        for dim in self.kdims:\n            if len(set(self.dimension_values(dim.name))) == 1:\n                dimensions.append(dim)\n        return dimensions", "response": "Return all constant dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_dimensions(self, item, dims, constant_keys):\n        if isinstance(item, Layout):\n            item.fixed = False\n\n        dim_vals = [(dim, val) for dim, val in dims[::-1]\n                    if dim not in self.drop]\n        if isinstance(item, self.merge_type):\n            new_item = item.clone(cdims=constant_keys)\n            for dim, val in dim_vals:\n                dim = asdim(dim)\n                if dim not in new_item.kdims:\n                    new_item = new_item.add_dimension(dim, 0, val)\n        elif isinstance(item, self._nest_order[self.merge_type]):\n            if len(dim_vals):\n                dimensions, key = zip(*dim_vals)\n                new_item = self.merge_type({key: item}, kdims=list(dimensions),\n                                           cdims=constant_keys)\n            else:\n                new_item = item\n        else:\n            new_item = item.clone(shared_data=False, cdims=constant_keys)\n            for k, v in item.items():\n                new_item[k] = self._add_dimensions(v, dims[::-1], constant_keys)\n        if isinstance(new_item, Layout):\n            new_item.fixed = True\n\n        return new_item", "response": "Recursively add the supplied dimension values to all contained HoloMaps objects."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestablishing support for a kernel density estimate.", "response": "def _kde_support(bin_range, bw, gridsize, cut, clip):\n    \"\"\"Establish support for a kernel density estimate.\"\"\"\n    kmin, kmax = bin_range[0] - bw * cut, bin_range[1] + bw * cut\n    if isfinite(clip[0]):\n        kmin = max(kmin, clip[0])\n    if isfinite(clip[1]):\n        kmax = min(kmax, clip[1])\n    return np.linspace(kmin, kmax, gridsize)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenders the supplied object and displays it using the active GUI backend.", "response": "def show(self, obj):\n        \"\"\"\n        Renders the supplied object and displays it using the active\n        GUI backend.\n        \"\"\"\n        if self.interactive:\n            if isinstance(obj, list):\n                return [self.get_plot(o) for o in obj]\n            return self.get_plot(obj)\n\n        from .plot import MPLPlot\n        MPLPlot._close_figures = False\n        try:\n            plots = []\n            objects = obj if isinstance(obj, list) else [obj]\n            for o in objects:\n                plots.append(self.get_plot(o))\n            plt.show()\n        except:\n            raise\n        finally:\n            MPLPlot._close_figures = True\n        return plots[0] if len(plots) == 1 else plots"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a holoviews object and a percentage size compute a suitable figure size for plotting the object.", "response": "def plot_options(cls, obj, percent_size):\n        \"\"\"\n        Given a holoviews object and a percentage size, apply heuristics\n        to compute a suitable figure size. For instance, scaling layouts\n        and grids linearly can result in unwieldy figure sizes when there\n        are a large number of elements. As ad hoc heuristics are used,\n        this functionality is kept separate from the plotting classes\n        themselves.\n\n        Used by the IPython Notebook display hooks and the save\n        utility. Note that this can be overridden explicitly per object\n        using the fig_size and size plot options.\n        \"\"\"\n        from .plot import MPLPlot\n        factor = percent_size / 100.0\n        obj = obj.last if isinstance(obj, HoloMap) else obj\n        options = Store.lookup_options(cls.backend, obj, 'plot').options\n        fig_size = options.get('fig_size', MPLPlot.fig_size)*factor\n\n        return dict({'fig_size':fig_size},\n                    **MPLPlot.lookup_options(obj, 'plot').options)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the latest plot data to update an existing plot.", "response": "def diff(self, plot):\n        \"\"\"\n        Returns the latest plot data to update an existing plot.\n        \"\"\"\n        if self.fig == 'auto':\n            figure_format = self.params('fig').objects[0]\n        else:\n            figure_format = self.fig\n        return self.html(plot, figure_format)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering matplotlib figure object and return the corresponding data.", "response": "def _figure_data(self, plot, fmt='png', bbox_inches='tight', as_script=False, **kwargs):\n        \"\"\"\n        Render matplotlib figure object and return the corresponding\n        data.  If as_script is True, the content will be split in an\n        HTML and a JS component.\n\n        Similar to IPython.core.pylabtools.print_figure but without\n        any IPython dependency.\n        \"\"\"\n        if fmt in ['gif', 'mp4', 'webm']:\n            if sys.version_info[0] == 3 and mpl.__version__[:-2] in ['1.2', '1.3']:\n                raise Exception(\"<b>Python 3 matplotlib animation support broken &lt;= 1.3</b>\")\n            with mpl.rc_context(rc=plot.fig_rcparams):\n                anim = plot.anim(fps=self.fps)\n            data = self._anim_data(anim, fmt)\n        else:\n            fig = plot.state\n\n            traverse_fn = lambda x: x.handles.get('bbox_extra_artists', None)\n            extra_artists = list(chain(*[artists for artists in plot.traverse(traverse_fn)\n                                         if artists is not None]))\n\n            kw = dict(\n                format=fmt,\n                facecolor=fig.get_facecolor(),\n                edgecolor=fig.get_edgecolor(),\n                dpi=self.dpi,\n                bbox_inches=bbox_inches,\n                bbox_extra_artists=extra_artists\n            )\n            kw.update(kwargs)\n\n            # Attempts to precompute the tight bounding box\n            try:\n                kw = self._compute_bbox(fig, kw)\n            except:\n                pass\n            bytes_io = BytesIO()\n            fig.canvas.print_figure(bytes_io, **kw)\n            data = bytes_io.getvalue()\n\n        if as_script:\n            b64 = base64.b64encode(data).decode(\"utf-8\")\n            (mime_type, tag) = MIME_TYPES[fmt], HTML_TAGS[fmt]\n            src = HTML_TAGS['base64'].format(mime_type=mime_type, b64=b64)\n            html = tag.format(src=src, mime_type=mime_type, css='')\n            return html, ''\n        if fmt == 'svg':\n            data = data.decode('utf-8')\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a matplotlib animation object and return the corresponding data.", "response": "def _anim_data(self, anim, fmt):\n        \"\"\"\n        Render a matplotlib animation object and return the corresponding data.\n        \"\"\"\n        (writer, _, anim_kwargs, extra_args) = ANIMATION_OPTS[fmt]\n        if extra_args != []:\n            anim_kwargs = dict(anim_kwargs, extra_args=extra_args)\n\n        if self.fps is not None: anim_kwargs['fps'] = max([int(self.fps), 1])\n        if self.dpi is not None: anim_kwargs['dpi'] = self.dpi\n        if not hasattr(anim, '_encoded_video'):\n            # Windows will throw PermissionError with auto-delete\n            with NamedTemporaryFile(suffix='.%s' % fmt, delete=False) as f:\n                anim.save(f.name, writer=writer, **anim_kwargs)\n                video = f.read()\n            f.close()\n            os.remove(f.name)\n        return video"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the tight bounding box for each figure once and reduce the number of required canvas draw calls from N + 1 to N + 1.", "response": "def _compute_bbox(self, fig, kw):\n        \"\"\"\n        Compute the tight bounding box for each figure once, reducing\n        number of required canvas draw calls from N*2 to N+1 as a\n        function of the number of frames.\n\n        Tight bounding box computing code here mirrors:\n        matplotlib.backend_bases.FigureCanvasBase.print_figure\n        as it hasn't been factored out as a function.\n        \"\"\"\n        fig_id = id(fig)\n        if kw['bbox_inches'] == 'tight':\n            if not fig_id in MPLRenderer.drawn:\n                fig.set_dpi(self.dpi)\n                fig.canvas.draw()\n                extra_artists = kw.pop(\"bbox_extra_artists\", [])\n                pad = mpl.rcParams['savefig.pad_inches']\n                bbox_inches = get_tight_bbox(fig, extra_artists, pad=pad)\n                MPLRenderer.drawn[fig_id] = bbox_inches\n                kw['bbox_inches'] = bbox_inches\n            else:\n                kw['bbox_inches'] = MPLRenderer.drawn[fig_id]\n        return kw"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the object from the backend", "response": "def load_nb(cls, inline=True):\n        \"\"\"\n        Initialize matplotlib backend\n        \"\"\"\n        import matplotlib.pyplot as plt\n        backend = plt.get_backend()\n        if backend not in ['agg', 'module://ipykernel.pylab.backend_inline']:\n            plt.switch_backend('agg')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef options_policy(skip_invalid, warn_on_skip):\n    settings = (Options.skip_invalid, Options.warn_on_skip)\n    (Options.skip_invalid, Options.warn_on_skip) = (skip_invalid, warn_on_skip)\n    yield\n    (Options.skip_invalid, Options.warn_on_skip) = settings", "response": "Context manager to temporarily set the skip_invalid and warn_on_skip class parameters on Options.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a fuzzy match message based on the OptionErrorMacro", "response": "def format_options_error(self):\n        \"\"\"\n        Return a fuzzy match message based on the OptionError\n        \"\"\"\n        allowed_keywords = self.allowed_keywords\n        target = allowed_keywords.target\n        matches = allowed_keywords.fuzzy_match(self.invalid_keyword)\n        if not matches:\n            matches = allowed_keywords.values\n            similarity = 'Possible'\n        else:\n            similarity = 'Similar'\n\n        loaded_backends = Store.loaded_backends()\n        target = 'for {0}'.format(target) if target else ''\n\n        if len(loaded_backends) == 1:\n            loaded=' in loaded backend {0!r}'.format(loaded_backends[0])\n        else:\n            backend_list = ', '.join(['%r'% b for b in loaded_backends[:-1]])\n            loaded=' in loaded backends {0} and {1!r}'.format(backend_list,\n                                                            loaded_backends[-1])\n\n        suggestion = (\"If you believe this keyword is correct, please make sure \"\n                      \"the backend has been imported or loaded with the \"\n                      \"hv.extension.\")\n\n        group = '{0} option'.format(self.group_name) if self.group_name else 'keyword'\n        msg=('Unexpected {group} {kw} {target}{loaded}.\\n\\n'\n             '{similarity} keywords in the currently active '\n             '{current_backend} renderer are: {matches}\\n\\n{suggestion}')\n        return msg.format(kw=\"'%s'\" % self.invalid_keyword,\n                          target=target,\n                          group=group,\n                          loaded=loaded, similarity=similarity,\n                          current_backend=repr(Store.current_backend),\n                          matches=matches,\n                          suggestion=suggestion)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef print_traceback(self):\n        traceback.print_exception(self.etype, self.value, self.traceback)", "response": "Print the traceback of the exception wrapped by the AbbreviatedException."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new Options object that is filtered by the specified list of keys.", "response": "def filtered(self, allowed):\n        \"\"\"\n        Return a new Options object that is filtered by the specified\n        list of keys. Mutating self.kwargs to filter is unsafe due to\n        the option expansion that occurs on initialization.\n        \"\"\"\n        kws = {k:v for k,v in self.kwargs.items() if k in allowed}\n        return self.__class__(key=self.key,\n                              allowed_keywords=self.allowed_keywords,\n                              merge_keywords=self.merge_keywords, **kws)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntruncates all contained Palette objects to a maximum number of samples and returns a new Options object containing the truncated or resampled Palettes.", "response": "def max_cycles(self, num):\n        \"\"\"\n        Truncates all contained Palette objects to a maximum number\n        of samples and returns a new Options object containing the\n        truncated or resampled Palettes.\n        \"\"\"\n        kwargs = {kw: (arg[num] if isinstance(arg, Palette) else arg)\n                  for kw, arg in self.kwargs.items()}\n        return self(max_cycles=num, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cyclic(self):\n        \"Returns True if the options cycle, otherwise False\"\n        return any(isinstance(val, Cycle) for val in self.kwargs.values())", "response": "Returns True if the options cycle otherwise False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmerges the Options object for the given group_name with the Options object passed in.", "response": "def _merge_options(self, identifier, group_name, options):\n        \"\"\"\n        Computes a merged Options object for the given group\n        name from the existing Options on the node and the\n        new Options which are passed in.\n        \"\"\"\n        if group_name not in self.groups:\n            raise KeyError(\"Group %s not defined on SettingTree\" % group_name)\n\n        if identifier in self.children:\n            current_node = self[identifier]\n            group_options = current_node.groups[group_name]\n        else:\n            #When creating a node (nothing to merge with) ensure it is empty\n            group_options = Options(group_name,\n                     allowed_keywords=self.groups[group_name].allowed_keywords)\n\n        override_kwargs = dict(options.kwargs)\n        old_allowed = group_options.allowed_keywords\n        override_kwargs['allowed_keywords'] = options.allowed_keywords + old_allowed\n\n        try:\n            return (group_options(**override_kwargs)\n                    if options.merge_keywords else Options(group_name, **override_kwargs))\n        except OptionError as e:\n            raise OptionError(e.invalid_keyword,\n                              e.allowed_keywords,\n                              group_name=group_name,\n                              path = self.path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find(self, path, mode='node'):\n        path = path.split('.') if isinstance(path, str) else list(path)\n        item = self\n\n        for child in path:\n            escaped_child = sanitize_identifier(child, escape=False)\n            matching_children = [c for c in item.children\n                                 if child.endswith(c) or escaped_child.endswith(c)]\n            matching_children = sorted(matching_children, key=lambda x: -len(x))\n            if matching_children:\n                item = item[matching_children[0]]\n            else:\n                continue\n        return item if mode == 'node' else item.path", "response": "Find the closest node or path to an arbitrary path that is\n        supplied down the tree from the given node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef closest(self, obj, group, defaults=True):\n        components = (obj.__class__.__name__,\n                      group_sanitizer(obj.group),\n                      label_sanitizer(obj.label))\n        target = '.'.join([c for c in components if c])\n        return self.find(components).options(group, target=target,\n                                             defaults=defaults)", "response": "This method returns the most appropriate Options object for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the complete Options object for the given node and group.", "response": "def options(self, group, target=None, defaults=True):\n        \"\"\"\n        Using inheritance up to the root, get the complete Options\n        object for the given node and the specified group.\n        \"\"\"\n        if target is None:\n            target = self.path\n        if self.groups.get(group, None) is None:\n            return None\n        if self.parent is None and target and (self is not Store.options()) and defaults:\n            root_name = self.__class__.__name__\n            replacement = root_name + ('' if len(target) == len(root_name) else '.')\n            option_key = target.replace(replacement,'')\n            match = Store.options().find(option_key)\n            if match is not Store.options():\n                return match.options(group)\n            else:\n                return Options()\n        elif self.parent is None:\n            return self.groups[group]\n\n        parent_opts = self.parent.options(group,target, defaults)\n        return Options(**dict(parent_opts.kwargs, **self.groups[group].kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strongest_match(cls, overlay, mode, backend=None):\n        match_strength = [(op.match_level(overlay), op) for op in cls.definitions\n                          if op.mode == mode and (not op.backends or backend in op.backends)]\n        matches = [(match[0], op, match[1]) for (match, op) in match_strength if match is not None]\n        if matches == []: return None\n        else:             return sorted(matches)[0]", "response": "Returns the strongest matching compositor operation\n        given an overlay."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an overlay and a list of ranges return a new overlay with the expanded elements.", "response": "def collapse_element(cls, overlay, ranges=None, mode='data', backend=None):\n        \"\"\"\n        Finds any applicable compositor and applies it.\n        \"\"\"\n        from .element import Element\n        from .overlay import Overlay, CompositeOverlay\n        unpack = False\n        if not isinstance(overlay, CompositeOverlay):\n            overlay = Overlay([overlay])\n            unpack = True\n\n        prev_ids = tuple()\n        processed = defaultdict(list)\n        while True:\n            match = cls.strongest_match(overlay, mode, backend)\n            if match is None:\n                if unpack and len(overlay) == 1:\n                    return overlay.values()[0]\n                return overlay\n            (_, applicable_op, (start, stop)) = match\n            if isinstance(overlay, Overlay):\n                values = overlay.values()\n                sliced = Overlay(values[start:stop])\n            else:\n                values = overlay.items()\n                sliced = overlay.clone(values[start:stop])\n            items = sliced.traverse(lambda x: x, [Element])\n            if applicable_op and all(el in processed[applicable_op] for el in items):\n                return overlay\n            result = applicable_op.apply(sliced, ranges, backend)\n            if applicable_op.group:\n                result = result.relabel(group=applicable_op.group)\n            if isinstance(overlay, Overlay):\n                result = [result]\n            else:\n                result = list(zip(sliced.keys(), [result]))\n            processed[applicable_op] += [el for r in result for el in r.traverse(lambda x: x, [Element])]\n            overlay = overlay.clone(values[:start]+result+values[stop:])\n\n            # Guard against infinite recursion for no-ops\n            spec_fn = lambda x: not isinstance(x, CompositeOverlay)\n            new_ids = tuple(overlay.traverse(lambda x: id(x), [spec_fn]))\n            if new_ids == prev_ids:\n                return overlay\n            prev_ids = new_ids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef collapse(cls, holomap, ranges=None, mode='data'):\n        # No potential compositors\n        if cls.definitions == []:\n            return holomap\n\n        # Apply compositors\n        clone = holomap.clone(shared_data=False)\n        data = zip(ranges[1], holomap.data.values()) if ranges else holomap.data.items()\n        for key, overlay in data:\n            clone[key] = cls.collapse_element(overlay, ranges, mode)\n        return clone", "response": "Given a map of Overlays apply all applicable compositors."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef map(cls, obj, mode='data', backend=None):\n        from .overlay import CompositeOverlay\n        element_compositors = [c for c in cls.definitions if len(c._pattern_spec) == 1]\n        overlay_compositors = [c for c in cls.definitions if len(c._pattern_spec) > 1]\n        if overlay_compositors:\n            obj = obj.map(lambda obj: cls.collapse_element(obj, mode=mode, backend=backend),\n                          [CompositeOverlay])\n        element_patterns = [c.pattern for c in element_compositors]\n        if element_compositors and obj.traverse(lambda x: x, element_patterns):\n            obj = obj.map(lambda obj: cls.collapse_element(obj, mode=mode, backend=backend),\n                          element_patterns)\n        return obj", "response": "Applies compositor operations to any HoloViews element or container\n        using the map method."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _slice_match_level(self, overlay_items):\n        level = 0\n        for spec, el in zip(self._pattern_spec, overlay_items):\n            if spec[0] != type(el).__name__:\n                return None\n            level += 1      # Types match\n            if len(spec) == 1: continue\n\n            group = [el.group, group_sanitizer(el.group, escape=False)]\n            if spec[1] in group: level += 1  # Values match\n            else:                     return None\n\n            if len(spec) == 3:\n                group = [el.label, label_sanitizer(el.label, escape=False)]\n                if (spec[2] in group):\n                    level += 1  # Labels match\n                else:\n                    return None\n        return level", "response": "Find the match strength for a list of overlay items that must be exactly the same length as the pattern specification."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match_level(self, overlay):\n        slice_width = len(self._pattern_spec)\n        if slice_width > len(overlay): return None\n\n        # Check all the possible slices and return the best matching one\n        best_lvl, match_slice = (0, None)\n        for i in range(len(overlay)-slice_width+1):\n            overlay_slice = overlay.values()[i:i+slice_width]\n            lvl = self._slice_match_level(overlay_slice)\n            if lvl is None: continue\n            if lvl > best_lvl:\n                best_lvl = lvl\n                match_slice = (i, i+slice_width)\n\n        return (best_lvl, match_slice) if best_lvl != 0 else None", "response": "Given an overlay return the match level and applicable slice\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply(self, value, input_ranges, backend=None):\n        from .overlay import CompositeOverlay\n        if backend is None: backend = Store.current_backend\n        kwargs = {k: v for k, v in self.kwargs.items() if k != 'output_type'}\n        if isinstance(value, CompositeOverlay) and len(value) == 1:\n            value = value.values()[0]\n            if self.transfer_parameters:\n                plot_opts = Store.lookup_options(backend, value, 'plot').kwargs\n                kwargs.update({k: v for k, v in plot_opts.items()\n                               if k in self.operation.params()})\n\n        transformed = self.operation(value, input_ranges=input_ranges, **kwargs)\n        if self.transfer_options:\n            Store.transfer_options(value, transformed, backend)\n        return transformed", "response": "Apply the compositor on the input with the given input ranges."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nusing this method to set the backend to run the switch hooks", "response": "def set_current_backend(cls, backend):\n        \"Use this method to set the backend to run the switch hooks\"\n        for hook in cls._backend_switch_hooks:\n            hook(backend)\n        cls.current_backend = backend"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the HoloViews tree from a file.", "response": "def load(cls, filename):\n        \"\"\"\n        Equivalent to pickle.load except that the HoloViews trees is\n        restored appropriately.\n        \"\"\"\n        cls.load_counter_offset = StoreOptions.id_offset()\n        val = pickle.load(filename)\n        cls.load_counter_offset = None\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef info(cls, obj, ansi=True, backend='matplotlib', visualization=True,\n             recursive=False, pattern=None, elements=[]):\n        \"\"\"\n        Show information about a particular object or component class\n        including the applicable style and plot options. Returns None if\n        the object is not parameterized.\n        \"\"\"\n        parameterized_object = isinstance(obj, param.Parameterized)\n        parameterized_class = (isinstance(obj,type)\n                               and  issubclass(obj,param.Parameterized))\n        info = None\n        if parameterized_object or parameterized_class:\n            info = InfoPrinter.info(obj, ansi=ansi, backend=backend,\n                                    visualization=visualization,\n                                    pattern=pattern, elements=elements)\n\n        if parameterized_object and recursive:\n            hierarchy = obj.traverse(lambda x: type(x))\n            listed = []\n            for c in hierarchy[1:]:\n                if c not in listed:\n                    inner_info = InfoPrinter.info(c, ansi=ansi, backend=backend,\n                                                  visualization=visualization,\n                                                  pattern=pattern)\n                    black = '\\x1b[1;30m%s\\x1b[0m' if ansi else '%s'\n                    info +=  '\\n\\n' + (black % inner_info)\n                    listed.append(c)\n        return info", "response": "Show information about a particular object or component class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive an object returns the corresponding customized option tree if a single custom tree is applicable.", "response": "def lookup(cls, backend, obj):\n        \"\"\"\n        Given an object, lookup the corresponding customized option\n        tree if a single custom tree is applicable.\n        \"\"\"\n        ids = set([el for el in obj.traverse(lambda x: x.id) if el is not None])\n        if len(ids) == 0:\n            raise Exception(\"Object does not own a custom options tree\")\n        elif len(ids) != 1:\n            idlist = \",\".join([str(el) for el in sorted(ids)])\n            raise Exception(\"Object contains elements combined across \"\n                            \"multiple custom trees (ids %s)\" % idlist)\n        return cls._custom_options[backend][list(ids)[0]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransfers options for all backends from one object to another.", "response": "def transfer_options(cls, obj, new_obj, backend=None):\n        \"\"\"\n        Transfers options for all backends from one object to another.\n        Drops any options defined in the supplied drop list.\n        \"\"\"\n        backend = cls.current_backend if backend is None else backend\n        type_name = type(new_obj).__name__\n        group = type_name if obj.group == type(obj).__name__ else obj.group\n        spec = '.'.join([s for s in (type_name, group, obj.label) if s])\n        options = []\n        for group in Options._option_groups:\n            opts = cls.lookup_options(backend, obj, group)\n            if opts and opts.kwargs: options.append(Options(group, **opts.kwargs))\n        if options:\n            StoreOptions.set_options(new_obj, {spec: options}, backend)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds style options to the current plot class.", "response": "def add_style_opts(cls, component, new_options, backend=None):\n        \"\"\"\n        Given a component such as an Element (e.g. Image, Curve) or a\n        container (e.g Layout) specify new style options to be\n        accepted by the corresponding plotting class.\n\n        Note: This is supplied for advanced users who know which\n        additional style keywords are appropriate for the\n        corresponding plotting class.\n        \"\"\"\n        backend = cls.current_backend if backend is None else backend\n        if component not in cls.registry[backend]:\n            raise ValueError(\"Component %r not registered to a plotting class\" % component)\n\n        if not isinstance(new_options, list) or not all(isinstance(el, str) for el in new_options):\n            raise ValueError(\"Please supply a list of style option keyword strings\")\n\n        with param.logging_level('CRITICAL'):\n            for option in new_options:\n                if option not in cls.registry[backend][component].style_opts:\n                    plot_class = cls.registry[backend][component]\n                    plot_class.style_opts = sorted(plot_class.style_opts+[option])\n        cls._options[backend][component.name] = Options('style', merge_keywords=True, allowed_keywords=new_options)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef register(cls, associations, backend, style_aliases={}):\n        if backend not in cls.registry:\n            cls.registry[backend] = {}\n        cls.registry[backend].update(associations)\n\n        groups = Options._option_groups\n        if backend not in cls._options:\n            cls._options[backend] = OptionTree([], groups=groups)\n        if backend not in cls._custom_options:\n            cls._custom_options[backend] = {}\n\n        for view_class, plot in cls.registry[backend].items():\n            expanded_opts = [opt for key in plot.style_opts\n                             for opt in style_aliases.get(key, [])]\n            style_opts = sorted(set(opt for opt in (expanded_opts + plot.style_opts)\n                                    if opt not in plot._disabled_opts))\n            plot_opts = [k for k in plot.params().keys() if k not in ['name']]\n\n            with param.logging_level('CRITICAL'):\n                plot.style_opts = style_opts\n\n            plot_opts =  Keywords(plot_opts,  target=view_class.__name__)\n            style_opts = Keywords(style_opts, target=view_class.__name__)\n\n            opt_groups = {'plot':   Options(allowed_keywords=plot_opts),\n                          'output': Options(allowed_keywords=Options._output_allowed_kws),\n                          'style': Options(allowed_keywords=style_opts),\n                          'norm':  Options(framewise=False, axiswise=False,\n                                           allowed_keywords=['framewise',\n                                                             'axiswise'])}\n\n            name = view_class.__name__\n            cls._options[backend][name] = opt_groups", "response": "Register the supplied dictionary of associations between\n        elements and plotting classes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset a display hook that will be applied to objects of type type objtype.", "response": "def set_display_hook(cls, group, objtype, hook):\n        \"\"\"\n        Specify a display hook that will be applied to objects of type\n        objtype. The group specifies the set to which the display hook\n        belongs, allowing the Store to compute the precedence within\n        each group.\n        \"\"\"\n        cls._display_hooks[group][objtype] = hook"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render(cls, obj):\n        class_hierarchy = inspect.getmro(type(obj))\n        hooks = []\n        for _, type_hooks in cls._display_hooks.items():\n            for cls in class_hierarchy:\n                if cls in type_hooks:\n                    hooks.append(type_hooks[cls])\n                    break\n\n        data, metadata = {}, {}\n        for hook in hooks:\n            ret = hook(obj)\n            if ret is None:\n                continue\n            d, md = ret\n            data.update(d)\n            metadata.update(md)\n        return data, metadata", "response": "Render the object to a dictionary of MIME types and metadata information."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop collecting OptionErrors recorded with the record_skipped_option method and return them.", "response": "def stop_recording_skipped(cls):\n        \"\"\"\n        Stop collecting OptionErrors recorded with the\n        record_skipped_option method and return them\n        \"\"\"\n        if cls._errors_recorded is None:\n            raise Exception('Cannot stop recording before it is started')\n        recorded = cls._errors_recorded[:]\n        cls._errors_recorded = None\n        return recorded"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an OptionTree convert it into the equivalent dictionary format.", "response": "def tree_to_dict(cls, tree):\n        \"\"\"\n        Given an OptionTree, convert it into the equivalent dictionary format.\n        \"\"\"\n        specs = {}\n        for k in tree.keys():\n            spec_key = '.'.join(k)\n            specs[spec_key] = {}\n            for grp in tree[k].groups:\n                kwargs = tree[k].groups[grp].kwargs\n                if kwargs:\n                    specs[spec_key][grp] = kwargs\n        return specs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive an object and a list of ids as captured with capture_ids restore the ids.", "response": "def restore_ids(cls, obj, ids):\n        \"\"\"\n        Given an list of ids as captured with capture_ids, restore the\n        ids. Note the structure of an object must not change between\n        the calls to capture_ids and restore_ids.\n        \"\"\"\n        ids = iter(ids)\n        obj.traverse(lambda o: setattr(o, 'id', next(ids)))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying the given option specs to the supplied options tree.", "response": "def apply_customizations(cls, spec, options):\n        \"\"\"\n        Apply the given option specs to the supplied options tree.\n        \"\"\"\n        for key in sorted(spec.keys()):\n            if isinstance(spec[key], (list, tuple)):\n                customization = {v.key:v for v in spec[key]}\n            else:\n                customization = {k:(Options(**v) if isinstance(v, dict) else v)\n                                 for k,v in spec[key].items()}\n\n            # Set the Keywords target on Options from the {type} part of the key.\n            customization = {k:v.keywords_target(key.split('.')[0])\n                             for k,v in customization.items()}\n            options[str(key)] = customization\n        return options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate a specification against the options tree.", "response": "def validate_spec(cls, spec, backends=None):\n        \"\"\"\n        Given a specification, validated it against the options tree for\n        the specified backends by raising OptionError for invalid\n        options. If backends is None, validates against all the\n        currently loaded backend.\n\n        Only useful when invalid keywords generate exceptions instead of\n        skipping i.e Options.skip_invalid is False.\n        \"\"\"\n        loaded_backends =  Store.loaded_backends() if backends is None else backends\n\n        error_info     = {}\n        backend_errors = defaultdict(set)\n        for backend in loaded_backends:\n            cls.start_recording_skipped()\n            with options_policy(skip_invalid=True, warn_on_skip=False):\n                options = OptionTree(items=Store.options(backend).data.items(),\n                                     groups=Store.options(backend).groups)\n                cls.apply_customizations(spec, options)\n\n            for error in cls.stop_recording_skipped():\n                error_key = (error.invalid_keyword,\n                             error.allowed_keywords.target,\n                             error.group_name)\n                error_info[error_key+(backend,)] = error.allowed_keywords\n                backend_errors[error_key].add(backend)\n\n\n        for ((keyword, target, group_name), backends) in backend_errors.items():\n            # If the keyword failed for the target across all loaded backends...\n            if set(backends) == set(loaded_backends):\n                key = (keyword, target, group_name, Store.current_backend)\n                raise OptionError(keyword,\n                                  group_name=group_name,\n                                  allowed_keywords=error_info[key])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an options validation error message for the specified options specification.", "response": "def validation_error_message(cls, spec, backends=None):\n        \"\"\"\n        Returns an options validation error message if there are any\n        invalid keywords. Otherwise returns None.\n        \"\"\"\n        try:\n            cls.validate_spec(spec, backends=backends)\n        except OptionError as e:\n            return e.format_options_error()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands compositor definition keys into group string Image expands to RGB. Image.", "response": "def expand_compositor_keys(cls, spec):\n        \"\"\"\n        Expands compositor definition keys into {type}.{group}\n        keys. For instance a compositor operation returning a group\n        string 'Image' of element type RGB expands to 'RGB.Image'.\n        \"\"\"\n        expanded_spec={}\n        applied_keys = []\n        compositor_defs = {el.group:el.output_type.__name__\n                           for el in Compositor.definitions}\n        for key, val in spec.items():\n            if key not in compositor_defs:\n                expanded_spec[key] = val\n            else:\n                # Send id to Overlays\n                applied_keys = ['Overlay']\n                type_name = compositor_defs[key]\n                expanded_spec[str(type_name+'.'+key)] = val\n        return expanded_spec, applied_keys"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_custom_trees(cls, obj, options=None):\n        clones, id_mapping = {}, []\n        obj_ids = cls.get_object_ids(obj)\n        offset = cls.id_offset()\n        obj_ids = [None] if len(obj_ids)==0 else obj_ids\n        for tree_id in obj_ids:\n            if tree_id is not None and tree_id in Store.custom_options():\n                original = Store.custom_options()[tree_id]\n                clone = OptionTree(items = original.items(),\n                                   groups = original.groups)\n                clones[tree_id + offset + 1] = clone\n                id_mapping.append((tree_id, tree_id + offset + 1))\n            else:\n                clone = OptionTree(groups=Store.options().groups)\n                clones[offset] = clone\n                id_mapping.append((tree_id, offset))\n\n           # Nodes needed to ensure allowed_keywords is respected\n            for k in Store.options():\n                if k in [(opt.split('.')[0],) for opt in options]:\n                    group = {grp:Options(\n                        allowed_keywords=opt.allowed_keywords)\n                             for (grp, opt) in\n                             Store.options()[k].groups.items()}\n                    clone[k] = group\n\n        return {k:cls.apply_customizations(options, t) if options else t\n                for k,t in clones.items()}, id_mapping", "response": "Create a set of custom subtree clones for the given object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef merge_options(cls, groups, options=None,**kwargs):\n        groups = set(groups)\n        if (options is not None and set(options.keys()) <= groups):\n            kwargs, options = options, None\n        elif (options is not None and any(k in groups for k in options)):\n              raise Exception(\"All keys must be a subset of %s\"\n                              % ', '.join(groups))\n\n        options = {} if (options is None) else dict(**options)\n        all_keys = set(k for d in kwargs.values() for k in d)\n        for spec_key in all_keys:\n            additions = {}\n            for k, d in kwargs.items():\n                if spec_key in d:\n                    kws = d[spec_key]\n                    additions.update({k:kws})\n            if spec_key not in options:\n                options[spec_key] = {}\n            for key in additions:\n                if key in options[spec_key]:\n                    options[spec_key][key].update(additions[key])\n                else:\n                    options[spec_key][key] = additions[key]\n        return options", "response": "Given a full options dictionary and options groups specified as a keywords return a full set of merged options."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state(cls, obj, state=None):\n        if state is None:\n            ids = cls.capture_ids(obj)\n            original_custom_keys = set(Store.custom_options().keys())\n            return (ids, original_custom_keys)\n        else:\n            (ids, original_custom_keys) = state\n            current_custom_keys = set(Store.custom_options().keys())\n            for key in current_custom_keys.difference(original_custom_keys):\n                del Store.custom_options()[key]\n                cls.restore_ids(obj, ids)", "response": "Method to capture and restore option state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing an appropriate offset for future id values given the set of ids currently defined across backends.", "response": "def id_offset(cls):\n        \"\"\"\n        Compute an appropriate offset for future id values given the set\n        of ids currently defined across backends.\n        \"\"\"\n        max_ids = []\n        for backend in Store.renderers.keys():\n            store_ids = Store.custom_options(backend=backend).keys()\n            max_id = max(store_ids)+1 if len(store_ids) > 0 else 0\n            max_ids.append(max_id)\n        # If no backends defined (e.g plotting not imported) return zero\n        return max(max_ids) if len(max_ids) else 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the current backend with the new ones and the custom trees.", "response": "def update_backends(cls, id_mapping, custom_trees, backend=None):\n        \"\"\"\n        Given the id_mapping from previous ids to new ids and the new\n        custom tree dictionary, update the current backend with the\n        supplied trees and update the keys in the remaining backends to\n        stay linked with the current object.\n        \"\"\"\n        backend = Store.current_backend if backend is None else backend\n        # Update the custom option entries for the current backend\n        Store.custom_options(backend=backend).update(custom_trees)\n\n        # Propagate option ids for non-selected backends\n        for b in Store.loaded_backends():\n            if b == backend:\n                continue\n            backend_trees = Store._custom_options[b]\n            for (old_id, new_id) in id_mapping:\n                tree = backend_trees.get(old_id, None)\n                if tree is not None:\n                    backend_trees[new_id] = tree"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if supplied object is a valid color spec.", "response": "def is_color(color):\n    \"\"\"\n    Checks if supplied object is a valid color spec.\n    \"\"\"\n    if not isinstance(color, basestring):\n        return False\n    elif RGB_HEX_REGEX.match(color):\n        return True\n    elif color in COLOR_ALIASES:\n        return True\n    elif color in cnames:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates a style and associated value.", "response": "def validate(style, value, vectorized=True):\n    \"\"\"\n    Validates a style and associated value.\n\n    Arguments\n    ---------\n    style: str\n       The style to validate (e.g. 'color', 'size' or 'marker')\n    value:\n       The style value to validate\n    vectorized: bool\n       Whether validator should allow vectorized setting\n\n    Returns\n    -------\n    valid: boolean or None\n       If validation is supported returns boolean, otherwise None\n    \"\"\"\n    validator = get_validator(style)\n    if validator is None:\n        return None\n    if isinstance(value, (np.ndarray, list)) and vectorized:\n        return all(validator(v) for v in value)\n    try:\n        valid = validator(value)\n        return False if valid == False else True\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering styles which are specific to a particular artist.", "response": "def filter_styles(style, group, other_groups, blacklist=[]):\n    \"\"\"\n    Filters styles which are specific to a particular artist, e.g.\n    for a GraphPlot this will filter options specific to the nodes and\n    edges.\n\n    Arguments\n    ---------\n    style: dict\n        Dictionary of styles and values\n    group: str\n        Group within the styles to filter for\n    other_groups: list\n        Other groups to filter out\n    blacklist: list (optional)\n        List of options to filter out\n\n    Returns\n    -------\n    filtered: dict\n        Filtered dictionary of styles\n    \"\"\"\n    group = group+'_'\n    filtered = {}\n    for k, v in style.items():\n        if (any(k.startswith(p) for p in other_groups)\n            or k.startswith(group) or k in blacklist):\n            continue\n        filtered[k] = v\n    for k, v in style.items():\n        if not k.startswith(group) or k in blacklist:\n            continue\n        filtered[k[len(group):]] = v\n    return filtered"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrap a formatting function or string in appropriate matplotlib formatter type.", "response": "def wrap_formatter(formatter):\n    \"\"\"\n    Wraps formatting function or string in\n    appropriate matplotlib formatter type.\n    \"\"\"\n    if isinstance(formatter, ticker.Formatter):\n        return formatter\n    elif callable(formatter):\n        args = [arg for arg in _getargspec(formatter).args\n                if arg != 'self']\n        wrapped = formatter\n        if len(args) == 1:\n            def wrapped(val, pos=None):\n                return formatter(val)\n        return ticker.FuncFormatter(wrapped)\n    elif isinstance(formatter, basestring):\n        if re.findall(r\"\\{(\\w+)\\}\", formatter):\n            return ticker.StrMethodFormatter(formatter)\n        else:\n            return ticker.FormatStrFormatter(formatter)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest whether two axes overlap vertically", "response": "def axis_overlap(ax1, ax2):\n    \"\"\"\n    Tests whether two axes overlap vertically\n    \"\"\"\n    b1, t1 = ax1.get_position().intervaly\n    b2, t2 = ax2.get_position().intervaly\n    return t1 > b2 and b1 < t2"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix_aspect(fig, nrows, ncols, title=None, extra_artists=[],\n               vspace=0.2, hspace=0.2):\n    \"\"\"\n    Calculate heights and widths of axes and adjust\n    the size of the figure to match the aspect.\n    \"\"\"\n    fig.canvas.draw()\n    w, h = fig.get_size_inches()\n\n    # Compute maximum height and width of each row and columns\n    rows = resolve_rows([[ax] for ax in fig.axes])\n    rs, cs = len(rows), max([len(r) for r in rows])\n    heights = [[] for i in range(cs)]\n    widths = [[] for i in range(rs)]\n    for r, row in enumerate(rows):\n        for c, ax in enumerate(row):\n            bbox = ax.get_tightbbox(fig.canvas.get_renderer())\n            heights[c].append(bbox.height)\n            widths[r].append(bbox.width)\n    height = (max([sum(c) for c in heights])) + nrows*vspace*fig.dpi\n    width = (max([sum(r) for r in widths])) + ncols*hspace*fig.dpi\n\n    # Compute aspect and set new size (in inches)\n    aspect = height/width\n    offset = 0\n    if title and title.get_text():\n        offset = title.get_window_extent().height/fig.dpi\n    fig.set_size_inches(w, (w*aspect)+offset)\n\n    # Redraw and adjust title position if defined\n    fig.canvas.draw()\n    if title and title.get_text():\n        extra_artists = [a for a in extra_artists\n                         if a is not title]\n        bbox = get_tight_bbox(fig, extra_artists)\n        top = bbox.intervaly[1]\n        if title and title.get_text():\n            title.set_y((top/(w*aspect)))", "response": "Fix the aspect of the figure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tight_bbox(fig, bbox_extra_artists=[], pad=None):\n    renderer = fig.canvas.get_renderer()\n    bbox_inches = fig.get_tightbbox(renderer)\n    bbox_artists = bbox_extra_artists[:]\n    bbox_artists += fig.get_default_bbox_extra_artists()\n    bbox_filtered = []\n    for a in bbox_artists:\n        bbox = a.get_window_extent(renderer)\n        if isinstance(bbox, tuple):\n            continue\n        if a.get_clip_on():\n            clip_box = a.get_clip_box()\n            if clip_box is not None:\n                bbox = Bbox.intersection(bbox, clip_box)\n            clip_path = a.get_clip_path()\n            if clip_path is not None and bbox is not None:\n                clip_path = clip_path.get_fully_transformed_path()\n                bbox = Bbox.intersection(bbox,\n                                         clip_path.get_extents())\n        if bbox is not None and (bbox.width != 0 or\n                                 bbox.height != 0):\n            bbox_filtered.append(bbox)\n    if bbox_filtered:\n        _bbox = Bbox.union(bbox_filtered)\n        trans = Affine2D().scale(1.0 / fig.dpi)\n        bbox_extra = TransformedBbox(_bbox, trans)\n        bbox_inches = Bbox.union([bbox_inches, bbox_extra])\n    return bbox_inches.padded(pad) if pad else bbox_inches", "response": "Compute a tight bounding box around all the artists in the figure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the array data from any Raster or Image type AttributeNames", "response": "def get_raster_array(image):\n    \"\"\"\n    Return the array data from any Raster or Image type\n    \"\"\"\n    if isinstance(image, RGB):\n        rgb = image.rgb\n        data = np.dstack([np.flipud(rgb.dimension_values(d, flat=False))\n                          for d in rgb.vdims])\n    else:\n        data = image.dimension_values(2, flat=False)\n        if type(image) is Raster:\n            data = data.T\n        else:\n            data = np.flipud(data)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ring_coding(array):\n    # The codes will be all \"LINETO\" commands, except for \"MOVETO\"s at the\n    # beginning of each subpath\n    n = len(array)\n    codes = np.ones(n, dtype=Path.code_type) * Path.LINETO\n    codes[0] = Path.MOVETO\n    codes[-1] = Path.CLOSEPOLY\n    return codes", "response": "Produces matplotlib Path codes for exterior and interior rings\n    of a polygon geometry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef polygons_to_path_patches(element):\n    paths = element.split(datatype='array', dimensions=element.kdims)\n    has_holes = isinstance(element, Polygons) and element.interface.has_holes(element)\n    holes = element.interface.holes(element) if has_holes else None\n    mpl_paths = []\n    for i, path in enumerate(paths):\n        splits = np.where(np.isnan(path[:, :2].astype('float')).sum(axis=1))[0]\n        arrays = np.split(path, splits+1) if len(splits) else [path]\n        subpath = []\n        for j, array in enumerate(arrays):\n            if j != (len(arrays)-1):\n                array = array[:-1]\n            if (array[0] != array[-1]).any():\n                array = np.append(array, array[:1], axis=0)\n            interiors = []\n            for interior in (holes[i][j] if has_holes else []):\n                if (interior[0] != interior[-1]).any():\n                    interior = np.append(interior, interior[:1], axis=0)\n                interiors.append(interior)\n            vertices = np.concatenate([array]+interiors)\n            codes = np.concatenate([ring_coding(array)]+\n                                   [ring_coding(h) for h in interiors])\n            subpath.append(PathPatch(Path(vertices, codes)))\n        mpl_paths.append(subpath)\n    return mpl_paths", "response": "Converts a list of Polygons into list of matplotlib. patches. PathPatch objects including any specified holes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef link_sources(self):\n        \"Returns potential Link or Stream sources.\"\n        if isinstance(self, GenericOverlayPlot):\n            zorders = []\n        elif self.batched:\n            zorders = list(range(self.zorder, self.zorder+len(self.hmap.last)))\n        else:\n            zorders = [self.zorder]\n\n        if isinstance(self, GenericOverlayPlot) and not self.batched:\n            sources = []\n        elif not self.static or isinstance(self.hmap, DynamicMap):\n            sources = [o for i, inputs in self.stream_sources.items()\n                       for o in inputs if i in zorders]\n        else:\n            sources = [self.hmap.last]\n        return sources", "response": "Returns potential Link or Stream sources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconstruct any callbacks for streams which have defined the plotted object as a source.", "response": "def _construct_callbacks(self):\n        \"\"\"\n        Initializes any callbacks for streams which have defined\n        the plotted object as a source.\n        \"\"\"\n        cb_classes = set()\n        registry = list(Stream.registry.items())\n        callbacks = Stream._callbacks['bokeh']\n        for source in self.link_sources:\n            streams = [\n                s for src, streams in registry for s in streams\n                if src is source or (src._plot_id is not None and\n                                     src._plot_id == source._plot_id)]\n            cb_classes |= {(callbacks[type(stream)], stream) for stream in streams\n                           if type(stream) in callbacks and stream.linked\n                           and stream.source is not None}\n        cbs = []\n        sorted_cbs = sorted(cb_classes, key=lambda x: id(x[0]))\n        for cb, group in groupby(sorted_cbs, lambda x: x[0]):\n            cb_streams = [s for _, s in group]\n            cbs.append(cb(self, cb_streams, source))\n        return cbs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npushes updated plot data via the Comm.", "response": "def push(self):\n        \"\"\"\n        Pushes updated plot data via the Comm.\n        \"\"\"\n        if self.renderer.mode == 'server':\n            return\n        if self.comm is None:\n            raise Exception('Renderer does not have a comm.')\n\n        if self._root and 'embedded' in self._root.tags:\n            # Allows external libraries to prevent comm updates\n            return\n\n        msg = self.renderer.diff(self, binary=True)\n        if msg is None:\n            return\n        self.comm.send(msg.header_json)\n        self.comm.send(msg.metadata_json)\n        self.comm.send(msg.content_json)\n        for header, payload in msg.buffers:\n            self.comm.send(json.dumps(header))\n            self.comm.send(buffers=[payload])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_root(self, root):\n        if root is None:\n            return\n        for plot in self.traverse(lambda x: x):\n            plot._root = root", "response": "Sets the root model on all subplots."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _init_datasource(self, data):\n        data = self._postprocess_data(data)\n        return ColumnDataSource(data=data)", "response": "Initializes a data source to be passed into the bokeh glyph."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply necessary type transformation to the data before it is set on a ColumnDataSource.", "response": "def _postprocess_data(self, data):\n        \"\"\"\n        Applies necessary type transformation to the data before\n        it is set on a ColumnDataSource.\n        \"\"\"\n        new_data = {}\n        for k, values in data.items():\n            values = decode_bytes(values) # Bytes need decoding to strings\n\n            # Certain datetime types need to be converted\n            if len(values) and isinstance(values[0], cftime_types):\n                if any(v.calendar not in _STANDARD_CALENDARS for v in values):\n                    self.param.warning(\n                        'Converting cftime.datetime from a non-standard '\n                        'calendar (%s) to a standard calendar for plotting. '\n                        'This may lead to subtle errors in formatting '\n                        'dates, for accurate tick formatting switch to '\n                        'the matplotlib backend.' % values[0].calendar)\n                values = cftime_to_timestamp(values, 'ms')\n            new_data[k] = values\n        return new_data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_datasource(self, source, data):\n        if not self.document:\n            return\n\n        data = self._postprocess_data(data)\n        empty = all(len(v) == 0 for v in data.values())\n        if (self.streaming and self.streaming[0].data is self.current_frame.data\n            and self._stream_data and not empty):\n            stream = self.streaming[0]\n            if stream._triggering:\n                data = {k: v[-stream._chunk_length:] for k, v in data.items()}\n                source.stream(data, stream.length)\n            return\n\n        if cds_column_replace(source, data):\n            source.data = data\n        else:\n            source.data.update(data)", "response": "Update the datasource with data for a new frame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating existing CustomJS callbacks with models that were replaced when compositing subplots into a CompositePlot.", "response": "def _update_callbacks(self, plot):\n        \"\"\"\n        Iterates over all subplots and updates existing CustomJS\n        callbacks with models that were replaced when compositing\n        subplots into a CompositePlot and sets the plot id to match\n        the root level bokeh model.\n        \"\"\"\n        subplots = self.traverse(lambda x: x, [GenericElementPlot])\n        merged_tools = {t: list(plot.select({'type': TOOL_TYPES[t]}))\n                        for t in self._merged_tools}\n        for subplot in subplots:\n            for cb in subplot.callbacks:\n                for c in cb.callbacks:\n                    for tool, objs in merged_tools.items():\n                        if tool in c.args and objs:\n                            c.args[tool] = objs[0]\n                    if self.top_level:\n                        c.code = c.code.replace('PLACEHOLDER_PLOT_ID', self.id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncleaning up references to the plot after the plot has been deleted.", "response": "def cleanup(self):\n        \"\"\"\n        Cleans up references to the plot after the plot has been\n        deleted. Traverses through all plots cleaning up Callbacks and\n        Stream subscribers.\n        \"\"\"\n        plots = self.traverse(lambda x: x, [BokehPlot])\n        for plot in plots:\n            if not isinstance(plot, (GenericCompositePlot, GenericElementPlot, GenericOverlayPlot)):\n                continue\n            streams = list(plot.streams)\n            plot.streams = []\n            plot._document = None\n\n            if plot.subplots:\n                plot.subplots.clear()\n\n            if isinstance(plot, GenericElementPlot):\n                for callback in plot.callbacks:\n                    streams += callback.streams\n                    callback.cleanup()\n\n            for stream in set(streams):\n                stream._subscribers = [\n                    (p, subscriber) for p, subscriber in stream._subscribers\n                    if get_method_owner(subscriber) not in plots\n                ]\n\n        if self.comm and self.root is self.handles.get('plot'):\n            self.comm.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting integer fontsizes to a string specifying fontsize in pt.", "response": "def _fontsize(self, key, label='fontsize', common=True):\n        \"\"\"\n        Converts integer fontsizes to a string specifying\n        fontsize in pt.\n        \"\"\"\n        size = super(BokehPlot, self)._fontsize(key, label, common)\n        return {k: v if isinstance(v, basestring) else '%spt' % v\n                for k, v in size.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sync_sources(self):\n        get_sources = lambda x: (id(x.current_frame.data), x)\n        filter_fn = lambda x: (x.shared_datasource and x.current_frame is not None and\n                               not isinstance(x.current_frame.data, np.ndarray)\n                               and 'source' in x.handles)\n        data_sources = self.traverse(get_sources, [filter_fn])\n        grouped_sources = groupby(sorted(data_sources, key=lambda x: x[0]), lambda x: x[0])\n        shared_sources = []\n        source_cols = {}\n        plots = []\n        for _, group in grouped_sources:\n            group = list(group)\n            if len(group) > 1:\n                source_data = {}\n                for _, plot in group:\n                    source_data.update(plot.handles['source'].data)\n                new_source = ColumnDataSource(source_data)\n                for _, plot in group:\n                    renderer = plot.handles.get('glyph_renderer')\n                    for callback in plot.callbacks:\n                        callback.reset()\n                    if renderer is None:\n                        continue\n                    elif 'data_source' in renderer.properties():\n                        renderer.update(data_source=new_source)\n                    else:\n                        renderer.update(source=new_source)\n                    if hasattr(renderer, 'view'):\n                        renderer.view.update(source=new_source)\n                    plot.handles['source'] = plot.handles['cds'] = new_source\n                    plots.append(plot)\n                shared_sources.append(new_source)\n                source_cols[id(new_source)] = [c for c in new_source.data]\n        for plot in plots:\n            if plot.hooks and plot.finalize_hooks:\n                self.param.warning(\n                    \"Supply either hooks or finalize_hooks not both; \"\n                    \"using hooks and ignoring finalize_hooks.\")\n            hooks = plot.hooks or plot.finalize_hooks\n            for hook in hooks:\n                hook(plot, plot.current_frame)\n            for callback in plot.callbacks:\n                callback.initialize(plot_id=self.id)\n        self.handles['shared_sources'] = shared_sources\n        self.handles['source_cols'] = source_cols", "response": "Syncs data sources between Elements which draw data sources from the same object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the internal state of the Plot to represent the given key tuple. Returns this state.", "response": "def update_frame(self, key, ranges=None):\n        \"\"\"\n        Update the internal state of the Plot to represent the given\n        key tuple (where integers represent frames). Returns this\n        state.\n        \"\"\"\n        ranges = self.compute_ranges(self.layout, key, ranges)\n        for coord in self.layout.keys(full_grid=True):\n            subplot = self.subplots.get(wrap_tuple(coord), None)\n            if subplot is not None:\n                subplot.update_frame(key, ranges)\n        title = self._get_title_div(key)\n        if title:\n            self.handles['title']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate all the subplots for the given layout.", "response": "def _create_subplots(self, layout, positions, layout_dimensions, ranges, num=0):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        subplots = {}\n        adjoint_clone = layout.clone(shared_data=False, id=layout.id)\n        main_plot = None\n        for pos in positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            element = layout.get(pos, None)\n            if element is None or not element.traverse(lambda x: x, [Element, Empty]):\n                continue\n            if not displayable(element):\n                element = collate(element)\n\n            subplot_opts = dict(adjoined=main_plot)\n            # Options common for any subplot\n            vtype = element.type if isinstance(element, HoloMap) else element.__class__\n            plot_type = Store.registry[self.renderer.backend].get(vtype, None)\n            plotopts = self.lookup_options(element, 'plot').options\n            side_opts = {}\n            if pos != 'main':\n                plot_type = AdjointLayoutPlot.registry.get(vtype, plot_type)\n                if pos == 'right':\n                    yaxis = 'right-bare' if plot_type and 'bare' in plot_type.yaxis else 'right'\n                    width = plot_type.width if plot_type else 0\n                    side_opts = dict(height=main_plot.height, yaxis=yaxis,\n                                     width=width, invert_axes=True,\n                                     labelled=['y'], xticks=1, xaxis=main_plot.xaxis)\n                else:\n                    xaxis = 'top-bare' if plot_type and 'bare' in plot_type.xaxis else 'top'\n                    height = plot_type.height if plot_type else 0\n                    side_opts = dict(width=main_plot.width, xaxis=xaxis,\n                                     height=height, labelled=['x'],\n                                     yticks=1, yaxis=main_plot.yaxis)\n\n            # Override the plotopts as required\n            # Customize plotopts depending on position.\n            plotopts = dict(side_opts, **plotopts)\n            plotopts.update(subplot_opts)\n\n            if vtype is Empty:\n                adjoint_clone[pos] = element\n                subplots[pos] = None\n                continue\n            elif plot_type is None:\n                self.param.warning(\n                    \"Bokeh plotting class for %s type not found, object \"\n                    \" will not be rendered.\" % vtype.__name__)\n                continue\n            num = num if len(self.coords) > 1 else 0\n            subplot = plot_type(element, keys=self.keys,\n                                dimensions=self.dimensions,\n                                layout_dimensions=layout_dimensions,\n                                ranges=ranges, subplot=True,\n                                uniform=self.uniform, layout_num=num,\n                                renderer=self.renderer,\n                                **dict({'shared_axes': self.shared_axes},\n                                       **plotopts))\n            subplots[pos] = subplot\n            if isinstance(plot_type, type) and issubclass(plot_type, GenericCompositePlot):\n                adjoint_clone[pos] = subplots[pos].layout\n            else:\n                adjoint_clone[pos] = subplots[pos].hmap\n            if pos == 'main':\n                main_plot = subplot\n\n        return subplots, adjoint_clone"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes an empty grid to position the plots on by expanding any AdjointLayouts into multiple rows and columns.", "response": "def _compute_grid(self):\n        \"\"\"\n        Computes an empty grid to position the plots on by expanding\n        any AdjointLayouts into multiple rows and columns.\n        \"\"\"\n        widths = []\n        for c in range(self.cols):\n            c_widths = []\n            for r in range(self.rows):\n                subplot = self.subplots.get((r, c), None)\n                nsubplots = 1 if subplot is None else len(subplot.layout)\n                c_widths.append(2 if nsubplots > 1 else 1)\n            widths.append(max(c_widths))\n\n        heights = []\n        for r in range(self.rows):\n            r_heights = []\n            for c in range(self.cols):\n                subplot = self.subplots.get((r, c), None)\n                nsubplots = 1 if subplot is None else len(subplot.layout)\n                r_heights.append(2 if nsubplots > 2 else 1)\n            heights.append(max(r_heights))\n\n        # Generate empty grid\n        rows = sum(heights)\n        cols = sum(widths)\n        grid = [[None]*cols for _ in range(rows)]\n\n        return grid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the internal state of the Plot to represent the given key tuple. Returns this state.", "response": "def update_frame(self, key, ranges=None):\n        \"\"\"\n        Update the internal state of the Plot to represent the given\n        key tuple (where integers represent frames). Returns this\n        state.\n        \"\"\"\n        ranges = self.compute_ranges(self.layout, key, ranges)\n        for r, c in self.coords:\n            subplot = self.subplots.get((r, c), None)\n            if subplot is not None:\n                subplot.update_frame(key, ranges)\n        title = self._get_title_div(key)\n        if title:\n            self.handles['title'] = title"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninitializes all the views contained in the AdjointLayout Object using axes appropriate to the layout configuration.", "response": "def initialize_plot(self, ranges=None, plots=[]):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        if plots is None: plots = []\n        adjoined_plots = []\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            subplot = self.subplots.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if subplot is None:\n                adjoined_plots.append(empty_plot(0, 0))\n            else:\n                passed_plots = plots + adjoined_plots\n                adjoined_plots.append(subplot.initialize_plot(ranges=ranges, plots=passed_plots))\n        self.drawn = True\n        if not adjoined_plots: adjoined_plots = [None]\n        return adjoined_plots"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_plot(self_or_cls, obj, doc=None, renderer=None, **kwargs):\n        if doc is None:\n            doc = Document() if self_or_cls.notebook_context else curdoc()\n\n        if self_or_cls.notebook_context:\n            curdoc().theme = self_or_cls.theme\n        doc.theme = self_or_cls.theme\n        plot = super(BokehRenderer, self_or_cls).get_plot(obj, renderer, **kwargs)\n        plot.document = doc\n        return plot", "response": "Given a HoloViews Viewable return a corresponding plot instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef app(self_or_cls, plot, show=False, new_window=False, websocket_origin=None, port=0):\n        if not isinstance(self_or_cls, BokehRenderer) or self_or_cls.mode != 'server':\n            renderer = self_or_cls.instance(mode='server')\n        else:\n            renderer = self_or_cls\n\n        def modify_doc(doc):\n            renderer(plot, doc=doc)\n        handler = FunctionHandler(modify_doc)\n        app = Application(handler)\n\n        if not show:\n            # If not showing and in notebook context return app\n            return app\n        elif self_or_cls.notebook_context and not new_window:\n            # If in notebook, show=True and no new window requested\n            # display app inline\n            if isinstance(websocket_origin, list):\n                if len(websocket_origin) > 1:\n                    raise ValueError('In the notebook only a single websocket origin '\n                                     'may be defined, which must match the URL of the '\n                                     'notebook server.')\n                websocket_origin = websocket_origin[0]\n            opts = dict(notebook_url=websocket_origin) if websocket_origin else {}\n            return bkshow(app, **opts)\n\n        # If app shown outside notebook or new_window requested\n        # start server and open in new browser tab\n        from tornado.ioloop import IOLoop\n        loop = IOLoop.current()\n        if websocket_origin and not isinstance(websocket_origin, list):\n            websocket_origin = [websocket_origin]\n        opts = dict(allow_websocket_origin=websocket_origin) if websocket_origin else {}\n        opts['io_loop'] = loop\n        server = Server({'/': app}, port=port, **opts)\n        def show_callback():\n            server.show('/')\n        server.io_loop.add_callback(show_callback)\n        server.start()\n\n        def sig_exit(*args, **kwargs):\n            loop.add_callback_from_signal(do_stop)\n\n        def do_stop(*args, **kwargs):\n            loop.stop()\n\n        signal.signal(signal.SIGINT, sig_exit)\n        try:\n            loop.start()\n        except RuntimeError:\n            pass\n        return server", "response": "Creates a bokeh application from a HoloViews object or a plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a bokeh. io. Document with the plot attached.", "response": "def server_doc(self_or_cls, obj, doc=None):\n        \"\"\"\n        Get a bokeh Document with the plot attached. May supply\n        an existing doc, otherwise bokeh.io.curdoc() is used to\n        attach the plot to the global document instance.\n        \"\"\"\n        if not isinstance(obj, (Plot, BokehServerWidgets)):\n            if not isinstance(self_or_cls, BokehRenderer) or self_or_cls.mode != 'server':\n                renderer = self_or_cls.instance(mode='server')\n            else:\n                renderer = self_or_cls\n            plot, _ =  renderer._validate(obj, 'auto')\n        else:\n            plot = obj\n\n        root = plot.state\n        if isinstance(plot, BokehServerWidgets):\n            plot = plot.plot\n\n        if doc is None:\n            doc = plot.document\n        else:\n            plot.document = doc\n\n        plot.traverse(lambda x: attach_periodic(x), [GenericElementPlot])\n        doc.add_root(root)\n        return doc"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the corresponding data for a given plot instance.", "response": "def _figure_data(self, plot, fmt='html', doc=None, as_script=False, **kwargs):\n        \"\"\"\n        Given a plot instance, an output format and an optional bokeh\n        document, return the corresponding data. If as_script is True,\n        the content will be split in an HTML and a JS component.\n        \"\"\"\n        model = plot.state\n        if doc is None:\n            doc = plot.document\n        else:\n            plot.document = doc\n\n        for m in model.references():\n            m._document = None\n\n        doc.theme = self.theme\n        doc.add_root(model)\n\n        # Bokeh raises warnings about duplicate tools and empty subplots\n        # but at the holoviews level these are not issues\n        logger = logging.getLogger(bokeh.core.validation.check.__file__)\n        logger.disabled = True\n\n        if fmt == 'png':\n            from bokeh.io.export import get_screenshot_as_png\n            img = get_screenshot_as_png(plot.state, None)\n            imgByteArr = BytesIO()\n            img.save(imgByteArr, format='PNG')\n            data = imgByteArr.getvalue()\n            if as_script:\n                b64 = base64.b64encode(data).decode(\"utf-8\")\n                (mime_type, tag) = MIME_TYPES[fmt], HTML_TAGS[fmt]\n                src = HTML_TAGS['base64'].format(mime_type=mime_type, b64=b64)\n                div = tag.format(src=src, mime_type=mime_type, css='')\n                js = ''\n        else:\n            try:\n                with silence_warnings(EMPTY_LAYOUT, MISSING_RENDERERS):\n                    js, div, _ = notebook_content(model)\n                html = NOTEBOOK_DIV.format(plot_script=js, plot_div=div)\n                data = encode_utf8(html)\n                doc.hold()\n            except:\n                logger.disabled = False\n                raise\n            logger.disabled = False\n\n        plot.document = doc\n        if as_script:\n            return div, js\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff(self, plot, binary=True):\n        events = list(plot.document._held_events)\n        if not events:\n            return None\n        msg = Protocol(\"1.0\").create(\"PATCH-DOC\", events, use_buffers=binary)\n        plot.document._held_events = []\n        return msg", "response": "Returns a json diff required to update an existing plot with the latest plot data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_options(cls, obj, percent_size):\n        obj = obj.last if isinstance(obj, HoloMap) else obj\n        plot = Store.registry[cls.backend].get(type(obj), None)\n        if not hasattr(plot, 'width') or not hasattr(plot, 'height'):\n            from .plot import BokehPlot\n            plot = BokehPlot\n        options = plot.lookup_options(obj, 'plot').options\n        width = options.get('width', plot.width)\n        height = options.get('height', plot.height)\n        if width is not None:\n            width = int(width)\n        if height is not None:\n            height = int(height)\n        return dict(options, **{'width': width, 'height': height})", "response": "Given a holoviews object and a percentage size apply heuristics\n        to compute a suitable figure size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading the bokeh notebook resources.", "response": "def load_nb(cls, inline=True):\n        \"\"\"\n        Loads the bokeh notebook resources.\n        \"\"\"\n        LOAD_MIME_TYPE = bokeh.io.notebook.LOAD_MIME_TYPE\n        bokeh.io.notebook.LOAD_MIME_TYPE = MIME_TYPES['jlab-hv-load']\n        load_notebook(hide_banner=True, resources=INLINE if inline else CDN)\n        bokeh.io.notebook.LOAD_MIME_TYPE = LOAD_MIME_TYPE\n        bokeh.io.notebook.curstate().output_notebook()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmerge two dictionary of option types and their values into one dictionary of all option types.", "response": "def merge_option_dicts(old_opts, new_opts):\n    \"\"\"\n    Update the old_opts option dictionary with the options defined in\n    new_opts. Instead of a shallow update as would be performed by calling\n    old_opts.update(new_opts), this updates the dictionaries of all option\n    types separately.\n\n    Given two dictionaries\n        old_opts = {'a': {'x': 'old', 'y': 'old'}}\n    and\n        new_opts = {'a': {'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    this returns a dictionary\n        {'a': {'x': 'old', 'y': 'new', 'z': 'new'}, 'b': {'k': 'new'}}\n    \"\"\"\n    merged = dict(old_opts)\n\n    for option_type, options in new_opts.items():\n        if option_type not in merged:\n            merged[option_type] = {}\n\n        merged[option_type].update(options)\n\n    return merged"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a collection of Option objects or partial option dictionaries merge them to a single dictionary.", "response": "def merge_options_to_dict(options):\n    \"\"\"\n    Given a collection of Option objects or partial option dictionaries,\n    merge everything to a single dictionary.\n    \"\"\"\n    merged_options = {}\n    for obj in options:\n        if isinstance(obj,dict):\n            new_opts = obj\n        else:\n            new_opts = {obj.key: obj.kwargs}\n\n        merged_options = merge_option_dicts(merged_options, new_opts)\n    return merged_options"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an object return a hash using HashableJSON.", "response": "def deephash(obj):\n    \"\"\"\n    Given an object, return a hash using HashableJSON. This hash is not\n    architecture, Python version or platform independent.\n    \"\"\"\n    try:\n        return hash(json.dumps(obj, cls=HashableJSON, sort_keys=True))\n    except:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tree_attribute(identifier):\n    if identifier[0].upper().isupper() is False and identifier[0] != '_':\n        return True\n    else:\n        return identifier[0].isupper()", "response": "Predicate that returns True for custom attributes added to AttrTrees that are not methods properties or internal attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an ArgSpec object for functions staticmethods instancemethods classmethods and partials.", "response": "def argspec(callable_obj):\n    \"\"\"\n    Returns an ArgSpec object for functions, staticmethods, instance\n    methods, classmethods and partials.\n\n    Note that the args list for instance and class methods are those as\n    seen by the user. In other words, the first argument which is\n    conventionally called 'self' or 'cls' is omitted in these cases.\n    \"\"\"\n    if (isinstance(callable_obj, type)\n        and issubclass(callable_obj, param.ParameterizedFunction)):\n        # Parameterized function.__call__ considered function in py3 but not py2\n        spec = _getargspec(callable_obj.__call__)\n        args = spec.args[1:]\n    elif inspect.isfunction(callable_obj):    # functions and staticmethods\n        spec = _getargspec(callable_obj)\n        args = spec.args\n    elif isinstance(callable_obj, partial): # partials\n        arglen = len(callable_obj.args)\n        spec =  _getargspec(callable_obj.func)\n        args = [arg for arg in spec.args[arglen:] if arg not in callable_obj.keywords]\n    elif inspect.ismethod(callable_obj):    # instance and class methods\n        spec = _getargspec(callable_obj)\n        args = spec.args[1:]\n    else:                                   # callable objects\n        return argspec(callable_obj.__call__)\n\n    return inspect.ArgSpec(args=args,\n                           varargs=spec.varargs,\n                           keywords=get_keywords(spec),\n                           defaults=spec.defaults)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_dynamic_argspec(callback, kdims, streams):\n    argspec = callback.argspec\n    name = callback.name\n    kdims = [kdim.name for kdim in kdims]\n    stream_params = stream_parameters(streams)\n    defaults = argspec.defaults if argspec.defaults else []\n    all_posargs = argspec.args[:-len(defaults)] if defaults else argspec.args\n    # Filter out any posargs for streams\n    posargs = [arg for arg in all_posargs if arg not in stream_params]\n    kwargs = argspec.args[-len(defaults):]\n\n    if argspec.keywords is None:\n        unassigned_streams = set(stream_params) - set(argspec.args)\n        if unassigned_streams:\n            unassigned = ','.join(unassigned_streams)\n            raise KeyError('Callable {name!r} missing keywords to '\n                           'accept stream parameters: {unassigned}'.format(name=name,\n                                                                    unassigned=unassigned))\n\n\n    if len(posargs) > len(kdims) + len(stream_params):\n        raise KeyError('Callable {name!r} accepts more positional arguments than '\n                       'there are kdims and stream parameters'.format(name=name))\n    if kdims == []:                  # Can be no posargs, stream kwargs already validated\n        return []\n    if set(kdims) == set(posargs):   # Posargs match exactly, can all be passed as kwargs\n        return kdims\n    elif len(posargs) == len(kdims): # Posargs match kdims length, supplying names\n        if argspec.args[:len(kdims)] != posargs:\n            raise KeyError('Unmatched positional kdim arguments only allowed at '\n                           'the start of the signature of {name!r}'.format(name=name))\n\n        return posargs\n    elif argspec.varargs:            # Posargs missing, passed to Callable directly\n        return None\n    elif set(posargs) - set(kdims):\n        raise KeyError('Callable {name!r} accepts more positional arguments {posargs} '\n                       'than there are key dimensions {kdims}'.format(name=name,\n                                                                      posargs=posargs,\n                                                                      kdims=kdims))\n    elif set(kdims).issubset(set(kwargs)): # Key dims can be supplied by keyword\n        return kdims\n    elif set(kdims).issubset(set(posargs+kwargs)):\n        return kdims\n    else:\n        raise KeyError('Callback {name!r} signature over {names} does not accommodate '\n                       'required kdims {kdims}'.format(name=name,\n                                                       names=list(set(posargs+kwargs)),\n                                                       kdims=kdims))", "response": "Utility used by DynamicMap to ensure the supplied callback has an anonymized signature."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef callable_name(callable_obj):\n    try:\n        if (isinstance(callable_obj, type)\n            and issubclass(callable_obj, param.ParameterizedFunction)):\n            return callable_obj.__name__\n        elif (isinstance(callable_obj, param.Parameterized)\n              and 'operation' in callable_obj.params()):\n            return callable_obj.operation.__name__\n        elif isinstance(callable_obj, partial):\n            return str(callable_obj)\n        elif inspect.isfunction(callable_obj):  # functions and staticmethods\n            return callable_obj.__name__\n        elif inspect.ismethod(callable_obj):    # instance and class methods\n            meth = callable_obj\n            if sys.version_info < (3,0):\n                owner =  meth.im_class if meth.im_self is None else meth.im_self\n            else:\n                owner =  meth.__self__\n            if meth.__name__ == '__call__':\n                return type(owner).__name__\n            return '.'.join([owner.__name__, meth.__name__])\n        elif isinstance(callable_obj, types.GeneratorType):\n            return callable_obj.__name__\n        else:\n            return type(callable_obj).__name__\n    except:\n        return str(callable_obj)", "response": "Returns a meaningful name identifying a callable or generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns a string such as capital delta into the shortened alphabetized version in this case simply Delta. Used as a transform in sanitize_identifier.", "response": "def capitalize_unicode_name(s):\n    \"\"\"\n    Turns a string such as 'capital delta' into the shortened,\n    capitalized version, in this case simply 'Delta'. Used as a\n    transform in sanitize_identifier.\n    \"\"\"\n    index = s.find('capital')\n    if index == -1: return s\n    tail = s[index:].replace('capital', '').strip()\n    tail = tail[0].upper() + tail[1:]\n    return s[:index] + tail"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef asarray(arraylike, strict=True):\n    if isinstance(arraylike, np.ndarray):\n        return arraylike\n    elif isinstance(arraylike, list):\n        return np.asarray(arraylike, dtype=object)\n    elif not isinstance(arraylike, np.ndarray) and isinstance(arraylike, arraylike_types):\n        return arraylike.values\n    elif hasattr(arraylike, '__array__'):\n        return np.asarray(arraylike)\n    elif strict:\n        raise ValueError('Could not convert %s type to array' % type(arraylike))\n    return arraylike", "response": "Converts arraylike objects to NumPy ndarray types. Errors if arraylike object is not arraylike."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef isnat(val):\n    if (isinstance(val, (np.datetime64, np.timedelta64)) or\n        (isinstance(val, np.ndarray) and val.dtype.kind == 'M')):\n        if numpy_version >= '1.13':\n            return np.isnat(val)\n        else:\n            return val.view('i8') == nat_as_integer\n    elif pd and val is pd.NaT:\n        return True\n    elif pd and isinstance(val, pandas_datetime_types+pandas_timedelta_types):\n        return pd.isna(val)\n    else:\n        return False", "response": "Checks if the value is a NaT."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isdatetime(value):\n    if isinstance(value, np.ndarray):\n        return (value.dtype.kind == \"M\" or\n                (value.dtype.kind == \"O\" and len(value) and\n                 isinstance(value[0], datetime_types)))\n    else:\n        return isinstance(value, datetime_types)", "response": "Determines if the given value is a datetime type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_minmax(lims, olims):\n    try:\n        limzip = zip(list(lims), list(olims), [np.nanmin, np.nanmax])\n        limits = tuple([float(fn([l, ol])) for l, ol, fn in limzip])\n    except:\n        limits = (np.NaN, np.NaN)\n    return limits", "response": "Given a set of limits and a set of items returns a tuple of min and max values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_range(values, soft_range=[]):\n    try:\n        values = np.array(values)\n        values = np.squeeze(values) if len(values.shape) > 1 else values\n        if len(soft_range):\n            values = np.concatenate([values, soft_range])\n        if values.dtype.kind == 'M':\n            return values.min(), values.max()\n        return np.nanmin(values), np.nanmax(values)\n    except:\n        try:\n            values = sorted(values)\n            return (values[0], values[-1])\n        except:\n            return (None, None)", "response": "Find the numerical min max of a set of values falling back to the first and last value in the sorted list of values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef max_range(ranges, combined=True):\n    try:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n            values = [tuple(np.NaN if v is None else v for v in r) for r in ranges]\n            if pd and any(isinstance(v, datetime_types) and not isinstance(v, cftime_types)\n                          for r in values for v in r):\n                converted = []\n                for l, h in values:\n                    if isinstance(l, datetime_types) and isinstance(h, datetime_types):\n                        l, h = (pd.Timestamp(l).to_datetime64(),\n                                pd.Timestamp(h).to_datetime64())\n                    converted.append((l, h))\n                values = converted\n\n            arr = np.array(values)\n            if not len(arr):\n                return np.NaN, np.NaN\n            elif arr.dtype.kind in 'OSU':\n                arr = list(python2sort([\n                    v for r in values for v in r\n                    if not is_nan(v) and v is not None]))\n                return arr[0], arr[-1]\n            elif arr.dtype.kind in 'M':\n                return ((arr.min(), arr.max()) if combined else\n                        (arr[:, 0].min(), arr[:, 1].min()))\n\n            if combined:\n                return (np.nanmin(arr), np.nanmax(arr))\n            else:\n                return (np.nanmin(arr[:, 0]), np.nanmax(arr[:, 1]))\n    except:\n        return (np.NaN, np.NaN)", "response": "Computes the maximal lower and upper bounds of a list of range tuples."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dimension_range(lower, upper, hard_range, soft_range, padding=None, log=False):\n    lower, upper = range_pad(lower, upper, padding, log)\n    lower = max_range([(lower, None), (soft_range[0], None)])[0]\n    upper = max_range([(None, upper), (None, soft_range[1])])[1]\n    dmin, dmax = hard_range\n    lower = lower if dmin is None or not isfinite(dmin) else dmin\n    upper = upper if dmax is None or not isfinite(dmax) else dmax\n    return lower, upper", "response": "Compute the range along a dimension by combining the data range\n    with the Dimension soft_range and range."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef max_extents(extents, zrange=False):\n    if zrange:\n        num = 6\n        inds = [(0, 3), (1, 4), (2, 5)]\n        extents = [e if len(e) == 6 else (e[0], e[1], None,\n                                          e[2], e[3], None)\n                   for e in extents]\n    else:\n        num = 4\n        inds = [(0, 2), (1, 3)]\n    arr = list(zip(*extents)) if extents else []\n    extents = [np.NaN] * num\n    if len(arr) == 0:\n        return extents\n    with warnings.catch_warnings():\n        warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n        for lidx, uidx in inds:\n            lower = [v for v in arr[lidx] if v is not None and not is_nan(v)]\n            upper = [v for v in arr[uidx] if v is not None and not is_nan(v)]\n            if lower and isinstance(lower[0], datetime_types):\n                extents[lidx] = np.min(lower)\n            elif any(isinstance(l, basestring) for l in lower):\n                extents[lidx] = np.sort(lower)[0]\n            elif lower:\n                extents[lidx] = np.nanmin(lower)\n            if upper and isinstance(upper[0], datetime_types):\n                extents[uidx] = np.max(upper)\n            elif any(isinstance(u, basestring) for u in upper):\n                extents[uidx] = np.sort(upper)[-1]\n            elif upper:\n                extents[uidx] = np.nanmax(upper)\n    return tuple(extents)", "response": "Computes the maximal extent in 2D and 3D space from a list of 4 - tuples or 6 - tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates alphanumeric labels of form A - Z AA - ZZ etc.", "response": "def int_to_alpha(n, upper=True):\n    \"Generates alphanumeric labels of form A-Z, AA-ZZ etc.\"\n    casenum = 65 if upper else 97\n    label = ''\n    count= 0\n    if n == 0: return str(chr(n + casenum))\n    while n >= 0:\n        mod, div = n % 26, n\n        for _ in range(count):\n            div //= 26\n        div %= 26\n        if count == 0:\n            val = mod\n        else:\n            val = div\n        label += str(chr(val + casenum))\n        count += 1\n        n -= 26**count\n    return label[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator that returns all non - duplicate elements in the input sequence.", "response": "def unique_iterator(seq):\n    \"\"\"\n    Returns an iterator containing all non-duplicate elements\n    in the input sequence.\n    \"\"\"\n    seen = set()\n    for item in seq:\n        if item not in seen:\n            seen.add(item)\n            yield item"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an array of unique values in the input order.", "response": "def unique_array(arr):\n    \"\"\"\n    Returns an array of unique values in the input order.\n\n    Args:\n       arr (np.ndarray or list): The array to compute unique values on\n\n    Returns:\n       A new array of unique values\n    \"\"\"\n    if not len(arr):\n        return np.asarray(arr)\n    elif pd:\n        if isinstance(arr, np.ndarray) and arr.dtype.kind not in 'MO':\n            # Avoid expensive unpacking if not potentially datetime\n            return pd.unique(arr)\n\n        values = []\n        for v in arr:\n            if (isinstance(v, datetime_types) and\n                not isinstance(v, cftime_types)):\n                v = pd.Timestamp(v).to_datetime64()\n            values.append(v)\n        return pd.unique(values)\n    else:\n        arr = np.asarray(arr)\n        _, uniq_inds = np.unique(arr, return_index=True)\n        return arr[np.sort(uniq_inds)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_spec(element, specification):\n    match_tuple = ()\n    match = specification.get((), {})\n    for spec in [type(element).__name__,\n                 group_sanitizer(element.group, escape=False),\n                 label_sanitizer(element.label, escape=False)]:\n        match_tuple += (spec,)\n        if match_tuple in specification:\n            match = specification[match_tuple]\n    return match", "response": "Matches the group. label specification of the supplied\n    element against the supplied specification dictionary returning the value of the best match."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of fully or partially overlapping dimensions by .", "response": "def merge_dimensions(dimensions_list):\n    \"\"\"\n    Merges lists of fully or partially overlapping dimensions by\n    merging their values.\n\n    >>> from holoviews import Dimension\n    >>> dim_list = [[Dimension('A', values=[1, 2, 3]), Dimension('B')],\n    ...             [Dimension('A', values=[2, 3, 4])]]\n    >>> dimensions = merge_dimensions(dim_list)\n    >>> dimensions\n    [Dimension('A'), Dimension('B')]\n    >>> dimensions[0].values\n    [1, 2, 3, 4]\n    \"\"\"\n    dvalues = defaultdict(list)\n    dimensions = []\n    for dims in dimensions_list:\n        for d in dims:\n            dvalues[d.name].append(d.values)\n            if d not in dimensions:\n                dimensions.append(d)\n    dvalues = {k: list(unique_iterator(itertools.chain(*vals)))\n               for k, vals in dvalues.items()}\n    return [d(values=dvalues.get(d.name, [])) for d in dimensions]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dimension_sort(odict, kdims, vdims, key_index):\n    sortkws = {}\n    ndims = len(kdims)\n    dimensions = kdims+vdims\n    indexes = [(dimensions[i], int(i not in range(ndims)),\n                    i if i in range(ndims) else i-ndims)\n                for i in key_index]\n    cached_values = {d.name: [None]+list(d.values) for d in dimensions}\n\n    if len(set(key_index)) != len(key_index):\n        raise ValueError(\"Cannot sort on duplicated dimensions\")\n    else:\n       sortkws['key'] = lambda x: tuple(cached_values[dim.name].index(x[t][d])\n                                        if dim.values else x[t][d]\n                                        for i, (dim, t, d) in enumerate(indexes))\n    if sys.version_info.major == 3:\n        return python2sort(odict.items(), **sortkws)\n    else:\n        return sorted(odict.items(), **sortkws)", "response": "Sort data by key using python tuple sorting semantics\n    or sorts in categorical order for any categorical Dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_cyclic(graph):\n    path = set()\n\n    def visit(vertex):\n        path.add(vertex)\n        for neighbour in graph.get(vertex, ()):\n            if neighbour in path or visit(neighbour):\n                return True\n        path.remove(vertex)\n        return False\n\n    return any(visit(v) for v in graph)", "response": "Returns True if the directed graph g has a cycle."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if graph contains only one to one mappings.", "response": "def one_to_one(graph, nodes):\n    \"\"\"\n    Return True if graph contains only one to one mappings. The\n    directed graph should be represented as a dictionary mapping of\n    edges for each node. Nodes should be passed a simple list.\n    \"\"\"\n    edges = itertools.chain.from_iterable(graph.values())\n    return len(graph) == len(nodes) and len(set(edges)) == len(nodes)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_overlay_spec(o, k, v):\n    k = wrap_tuple(k)\n    return ((type(v).__name__, v.group, v.label) + k if len(o.kdims) else\n            (type(v).__name__,) + k)", "response": "Gets the type. group. label + key spec from an Overlay."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds a global ordering for layers in a HoloMap of CompositeOverlay types.", "response": "def layer_sort(hmap):\n   \"\"\"\n   Find a global ordering for layers in a HoloMap of CompositeOverlay\n   types.\n   \"\"\"\n   orderings = {}\n   for o in hmap:\n      okeys = [get_overlay_spec(o, k, v) for k, v in o.data.items()]\n      if len(okeys) == 1 and not okeys[0] in orderings:\n         orderings[okeys[0]] = []\n      else:\n         orderings.update({k: [] if k == v else [v] for k, v in zip(okeys[1:], okeys)})\n   return [i for g in sort_topologically(orderings) for i in sorted(g)]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a global ordering of Layers into groups based on a slice of .", "response": "def layer_groups(ordering, length=2):\n   \"\"\"\n   Splits a global ordering of Layers into groups based on a slice of\n   the spec.  The grouping behavior can be modified by changing the\n   length of spec the entries are grouped by.\n   \"\"\"\n   group_orderings = defaultdict(list)\n   for el in ordering:\n      group_orderings[el[:length]].append(el)\n   return group_orderings"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive a list of key tuples to select groups them into sensible chunks.", "response": "def group_select(selects, length=None, depth=None):\n    \"\"\"\n    Given a list of key tuples to select, groups them into sensible\n    chunks to avoid duplicating indexing operations.\n    \"\"\"\n    if length == None and depth == None:\n        length = depth = len(selects[0])\n    getter = operator.itemgetter(depth-length)\n    if length > 1:\n        selects = sorted(selects, key=getter)\n        grouped_selects = defaultdict(dict)\n        for k, v in itertools.groupby(selects, getter):\n            grouped_selects[k] = group_select(list(v), length-1, depth)\n        return grouped_selects\n    else:\n        return list(selects)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iterative_select(obj, dimensions, selects, depth=None):\n    ndims = len(dimensions)\n    depth = depth if depth is not None else ndims\n    items = []\n    if isinstance(selects, dict):\n        for k, v in selects.items():\n            items += iterative_select(obj.select(**{dimensions[ndims-depth]: k}),\n                                      dimensions, v, depth-1)\n    else:\n        for s in selects:\n            items.append((s, obj.select(**{dimensions[-1]: s[-1]})))\n    return items", "response": "Takes the output of group_select selecting subgroups iteratively and returns a list of lists of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_file(folder, filename):\n    matches = []\n    if os.path.isabs(filename) and os.path.isfile(filename):\n        return filename\n    for root, _, filenames in os.walk(folder):\n        for fn in fnmatch.filter(filenames, filename):\n            matches.append(os.path.join(root, fn))\n    if not matches:\n        raise IOError('File %s could not be found' % filename)\n    return matches[-1]", "response": "Find a file given folder and filename."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether the supplied data is of DataFrame type.", "response": "def is_dataframe(data):\n    \"\"\"\n    Checks whether the supplied data is of DataFrame type.\n    \"\"\"\n    dd = None\n    if 'dask' in sys.modules and 'pandas' in sys.modules:\n        import dask.dataframe as dd\n    return((pd is not None and isinstance(data, pd.DataFrame)) or\n          (dd is not None and isinstance(data, dd.DataFrame)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_series(data):\n    dd = None\n    if 'dask' in sys.modules:\n        import dask.dataframe as dd\n    return((pd is not None and isinstance(data, pd.Series)) or\n          (dd is not None and isinstance(data, dd.Series)))", "response": "Checks whether the supplied data is of Series type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions to get the first non - auxiliary object label attribute from an NdMapping.", "response": "def get_ndmapping_label(ndmapping, attr):\n    \"\"\"\n    Function to get the first non-auxiliary object\n    label attribute from an NdMapping.\n    \"\"\"\n    label = None\n    els = itervalues(ndmapping.data)\n    while label is None:\n        try:\n            el = next(els)\n        except StopIteration:\n            return None\n        if not getattr(el, '_auxiliary_component', True):\n            label = getattr(el, attr)\n    if attr == 'group':\n        tp = type(el).__name__\n        if tp == label:\n            return None\n    return label"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a complete dictionary mapping between stream parameter names to their applicable renames excluding parameters listed in exclude_params.", "response": "def stream_name_mapping(stream, exclude_params=['name'], reverse=False):\n    \"\"\"\n    Return a complete dictionary mapping between stream parameter names\n    to their applicable renames, excluding parameters listed in\n    exclude_params.\n\n    If reverse is True, the mapping is from the renamed strings to the\n    original stream parameter names.\n    \"\"\"\n    filtered = [k for k in stream.params().keys() if k not in exclude_params]\n    mapping = {k:stream._rename.get(k,k) for k in filtered}\n    if reverse:\n        return {v:k for k,v in mapping.items()}\n    else:\n        return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef rename_stream_kwargs(stream, kwargs, reverse=False):\n    mapped_kwargs = {}\n    mapping = stream_name_mapping(stream, reverse=reverse)\n    for k,v in kwargs.items():\n        if k not in mapping:\n            msg = 'Could not map key {key} {direction} renamed equivalent'\n            direction = 'from' if reverse else 'to'\n            raise KeyError(msg.format(key=repr(k), direction=direction))\n        mapped_kwargs[mapping[k]] = v\n    return mapped_kwargs", "response": "Given a stream and a dictionary of parameter values map to\n    the corresponding dictionary where the keys are substituted with the corresponding string."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stream_parameters(streams, no_duplicates=True, exclude=['name']):\n    param_groups = []\n    for s in streams:\n        if not s.contents and isinstance(s.hashkey, dict):\n            param_groups.append(list(s.hashkey))\n        else:\n            param_groups.append(list(s.contents))\n    names = [name for group in param_groups for name in group]\n\n    if no_duplicates:\n        clashes = sorted(set([n for n in names if names.count(n) > 1]))\n        clash_streams = []\n        for s in streams:\n            for c in clashes:\n                if c in s.contents or (not s.contents and isinstance(s.hashkey, dict) and c in s.hashkey):\n                    clash_streams.append(s)\n        if clashes:\n            clashing = ', '.join([repr(c) for c in clash_streams[:-1]])\n            raise Exception('The supplied stream objects %s and %s '\n                            'clash on the following parameters: %r'\n                            % (clashing, clash_streams[-1], clashes))\n    return [name for name in names if name not in exclude]", "response": "Given a list of streams return a flat list of parameter name excluding those listed in the exclude list."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of stream parameters that have not been associated with any of the key dimensions.", "response": "def dimensionless_contents(streams, kdims, no_duplicates=True):\n    \"\"\"\n    Return a list of stream parameters that have not been associated\n    with any of the key dimensions.\n    \"\"\"\n    names = stream_parameters(streams, no_duplicates)\n    return [name for name in names if name not in kdims]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of dimensions that have not been associated with any streams.", "response": "def unbound_dimensions(streams, kdims, no_duplicates=True):\n    \"\"\"\n    Return a list of dimensions that have not been associated with\n    any streams.\n    \"\"\"\n    params = stream_parameters(streams, no_duplicates)\n    return [d for d in kdims if d not in params]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap the given tuple of streams into tuples with dimensioned stream values as appropriate.", "response": "def wrap_tuple_streams(unwrapped, kdims, streams):\n    \"\"\"\n    Fills in tuple keys with dimensioned stream values as appropriate.\n    \"\"\"\n    param_groups = [(s.contents.keys(), s) for s in streams]\n    pairs = [(name,s)  for (group, s) in param_groups for name in group]\n    substituted = []\n    for pos,el in enumerate(wrap_tuple(unwrapped)):\n        if el is None and pos < len(kdims):\n            matches = [(name,s) for (name,s) in pairs if name==kdims[pos].name]\n            if len(matches) == 1:\n                (name, stream) = matches[0]\n                el = stream.contents[name]\n        substituted.append(el)\n    return tuple(substituted)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef drop_streams(streams, kdims, keys):\n    stream_params = stream_parameters(streams)\n    inds, dims = zip(*[(ind, kdim) for ind, kdim in enumerate(kdims)\n                       if kdim not in stream_params])\n    get = operator.itemgetter(*inds) # itemgetter used for performance\n    keys = (get(k) for k in keys)\n    return dims, ([wrap_tuple(k) for k in keys] if len(inds) == 1 else list(keys))", "response": "Drop any dimensioned streams from the keys and kdims."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef itervalues(obj):\n    \"Get value iterator from dictionary for Python 2 and 3\"\n    return iter(obj.values()) if sys.version_info.major == 3 else obj.itervalues()", "response": "Get value iterator from dictionary for Python 2 and 3"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iterkeys(obj):\n    \"Get key iterator from dictionary for Python 2 and 3\"\n    return iter(obj.keys()) if sys.version_info.major == 3 else obj.iterkeys()", "response": "Get key iterator from dictionary for Python 2 and 3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a path from an existing object or a tuple of existing objects and a labelled object.", "response": "def get_path(item):\n    \"\"\"\n    Gets a path from an Labelled object or from a tuple of an existing\n    path and a labelled object. The path strings are sanitized and\n    capitalized.\n    \"\"\"\n    sanitizers = [group_sanitizer, label_sanitizer]\n    if isinstance(item, tuple):\n        path, item = item\n        if item.label:\n            if len(path) > 1 and item.label == path[1]:\n                path = path[:2]\n            else:\n                path = path[:1] + (item.label,)\n        else:\n            path = path[:1]\n    else:\n        path = (item.group, item.label) if item.label else (item.group,)\n    return tuple(capitalize(fn(p)) for (p, fn) in zip(path, sanitizers))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nallowing efficiently indexing into a cartesian product without expanding it.", "response": "def cross_index(values, index):\n    \"\"\"\n    Allows efficiently indexing into a cartesian product without\n    expanding it. The values should be defined as a list of iterables\n    making up the cartesian product and a linear index, returning\n    the cross product of the values at the supplied index.\n    \"\"\"\n    lengths = [len(v) for v in values]\n    length = np.product(lengths)\n    if index >= length:\n        raise IndexError('Index %d out of bounds for cross-product of size %d'\n                         % (index, length))\n    indexes = []\n    for i in range(1, len(values))[::-1]:\n        p = np.product(lengths[-i:])\n        indexes.append(index//p)\n        index -= indexes[-1] * p\n    indexes.append(index)\n    return tuple(v[i] for v, i in zip(values, indexes))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the indices of the lexicographical sorting of the supplied arrays.", "response": "def arglexsort(arrays):\n    \"\"\"\n    Returns the indices of the lexicographical sorting\n    order of the supplied arrays.\n    \"\"\"\n    dtypes = ','.join(array.dtype.str for array in arrays)\n    recarray = np.empty(len(arrays[0]), dtype=dtypes)\n    for i, array in enumerate(arrays):\n        recarray['f%s' % i] = array\n    return recarray.argsort()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dimensioned_streams(dmap):\n    dimensioned = []\n    for stream in dmap.streams:\n        stream_params = stream_parameters([stream])\n        if set([str(k) for k in dmap.kdims]) & set(stream_params):\n            dimensioned.append(stream)\n    return dimensioned", "response": "Given a DynamicMap return all streams that have any dimensioned\n    parameters i. e parameters also listed in the key dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nexpanding the coordinates along a dimension of the gridded dataset into an ND - array matching the dimensionality of the dataset.", "response": "def expand_grid_coords(dataset, dim):\n    \"\"\"\n    Expand the coordinates along a dimension of the gridded\n    dataset into an ND-array matching the dimensionality of\n    the dataset.\n    \"\"\"\n    arrays = [dataset.interface.coords(dataset, d.name, True)\n              for d in dataset.kdims]\n    idx = dataset.get_dimension_index(dim)\n    return cartesian_product(arrays, flat=False)[idx].T"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bound_range(vals, density, time_unit='us'):\n    if not len(vals):\n        return(np.nan, np.nan, density, False)\n    low, high = vals.min(), vals.max()\n    invert = False\n    if len(vals) > 1 and vals[0] > vals[1]:\n        invert = True\n    if not density:\n        with warnings.catch_warnings():\n            warnings.filterwarnings('ignore', r'invalid value encountered in double_scalars')\n            full_precision_density = compute_density(low, high, len(vals)-1)\n            density = round(full_precision_density, sys.float_info.dig)\n        if density == 0:\n            density = full_precision_density\n    if density == 0:\n        raise ValueError('Could not determine Image density, ensure it has a non-zero range.')\n    halfd = 0.5/density\n    if isinstance(low, datetime_types):\n        halfd = np.timedelta64(int(round(halfd)), time_unit)\n    return low-halfd, high+halfd, density, invert", "response": "Computes a bounding range of a number of samples from a number of samples and a density."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_regular_sampling(values, rtol=10e-6):\n    diffs = np.diff(values)\n    return (len(diffs) < 1) or abs(diffs.min()-diffs.max()) < abs(diffs.min()*rtol)", "response": "Validates regular sampling of a 1D array ensuring that the difference between the smallest and the smallest sampling step is at most rtol times the smallest sampling step."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compute_density(start, end, length, time_unit='us'):\n    if isinstance(start, int): start = float(start)\n    if isinstance(end, int): end = float(end)\n    diff = end-start\n    if isinstance(diff, timedelta_types):\n        if isinstance(diff, np.timedelta64):\n            diff = np.timedelta64(diff, time_unit).tolist()\n        tscale = 1./np.timedelta64(1, time_unit).tolist().total_seconds()\n        return (length/(diff.total_seconds()*tscale))\n    else:\n        return length/diff", "response": "Computes a density given the edges and number of samples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing a date range given a start date end date and a number of samples.", "response": "def date_range(start, end, length, time_unit='us'):\n    \"\"\"\n    Computes a date range given a start date, end date and the number\n    of samples.\n    \"\"\"\n    step = (1./compute_density(start, end, length, time_unit))\n    if pd and isinstance(start, pd.Timestamp):\n        start = start.to_datetime64()\n    step = np.timedelta64(int(round(step)), time_unit)\n    return start+step/2.+np.arange(length)*step"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dt_to_int(value, time_unit='us'):\n    if pd:\n        if isinstance(value, pd.Period):\n            value = value.to_timestamp()\n        if isinstance(value, pd.Timestamp):\n            try:\n                value = value.to_datetime64()\n            except:\n                value = np.datetime64(value.to_pydatetime())\n    elif isinstance(value, cftime_types):\n        return cftime_to_timestamp(value, time_unit)\n\n    if isinstance(value, dt.date):\n        value = dt.datetime(*value.timetuple()[:6])\n\n    # Handle datetime64 separately\n    if isinstance(value, np.datetime64):\n        try:\n            value = np.datetime64(value, 'ns')\n            tscale = (np.timedelta64(1, time_unit)/np.timedelta64(1, 'ns'))\n            return value.tolist()/tscale\n        except:\n            # If it can't handle ns precision fall back to datetime\n            value = value.tolist()\n\n    if time_unit == 'ns':\n        tscale = 1e9\n    else:\n        tscale = 1./np.timedelta64(1, time_unit).tolist().total_seconds()\n\n    try:\n        # Handle python3\n        return int(value.timestamp() * tscale)\n    except:\n        # Handle python2\n        return (time.mktime(value.timetuple()) + value.microsecond / 1e6) * tscale", "response": "Converts a datetime type to an integer with the supplied time unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cftime_to_timestamp(date, time_unit='us'):\n    import cftime\n    utime = cftime.utime('microseconds since 1970-01-01 00:00:00')\n    if time_unit == 'us':\n        tscale = 1\n    else:\n        tscale = (np.timedelta64(1, 'us')/np.timedelta64(1, time_unit))\n    return utime.date2num(date)*tscale", "response": "Converts cftime to timestamp since epoch in milliseconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a set of values returns the indices of each of those values in the source array.", "response": "def search_indices(values, source):\n    \"\"\"\n    Given a set of values returns the indices of each of those values\n    in the source array.\n    \"\"\"\n    orig_indices = source.argsort()\n    return orig_indices[np.searchsorted(source[orig_indices], values)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the edges as midpoints of the bin centers.", "response": "def compute_edges(edges):\n    \"\"\"\n    Computes edges as midpoints of the bin centers.  The first and\n    last boundaries are equidistant from the first and last midpoints\n    respectively.\n    \"\"\"\n    edges = np.asarray(edges)\n    if edges.dtype.kind == 'i':\n        edges = edges.astype('f')\n    midpoints = (edges[:-1] + edges[1:])/2.0\n    boundaries = (2*edges[0] - midpoints[0], 2*edges[-1] - midpoints[-1])\n    return np.concatenate([boundaries[:1], midpoints, boundaries[-1:]])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mimebundle_to_html(bundle):\n    if isinstance(bundle, tuple):\n        data, metadata = bundle\n    else:\n        data = bundle\n    html = data.get('text/html', '')\n    if 'application/javascript' in data:\n        js = data['application/javascript']\n        html += '\\n<script type=\"application/javascript\">{js}</script>'.format(js=js)\n    return html", "response": "Converts a MIME bundle into HTML."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a NumPy scalar to a regular python type.", "response": "def numpy_scalar_to_python(scalar):\n    \"\"\"\n    Converts a NumPy scalar to a regular python type.\n    \"\"\"\n    scalar_type = type(scalar)\n    if np.issubclass_(scalar_type, np.float_):\n        return float(scalar)\n    elif np.issubclass_(scalar_type, np.int_):\n        return int(scalar)\n    return scalar"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a match string returns the closest match.", "response": "def closest_match(match, specs, depth=0):\n    \"\"\"\n    Recursively iterates over type, group, label and overlay key,\n    finding the closest matching spec.\n    \"\"\"\n    new_specs = []\n    match_lengths = []\n    for i, spec in specs:\n        if spec[0] == match[0]:\n            new_specs.append((i, spec[1:]))\n        else:\n            if all(isinstance(s[0], basestring) for s in [spec, match]):\n                match_length = max(i for i in range(len(match[0]))\n                                   if match[0].startswith(spec[0][:i]))\n            elif is_number(match[0]) and is_number(spec[0]):\n                match_length = -abs(match[0]-spec[0])\n            else:\n                match_length = 0\n            match_lengths.append((i, match_length, spec[0]))\n\n    if len(new_specs) == 1:\n        return new_specs[0][0]\n    elif new_specs:\n        depth = depth+1\n        return closest_match(match[1:], new_specs, depth)\n    else:\n        if depth == 0 or not match_lengths:\n            return None\n        else:\n            return sorted(match_lengths, key=lambda x: -x[1])[0][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds new aliases as keyword arguments.", "response": "def add_aliases(self_or_cls, **kwargs):\n        \"\"\"\n        Conveniently add new aliases as keyword arguments. For instance\n        you can add a new alias with add_aliases(short='Longer string')\n        \"\"\"\n        self_or_cls.aliases.update({v:k for k,v in kwargs.items()})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_aliases(self_or_cls, aliases):\n        for k,v in self_or_cls.aliases.items():\n            if v in aliases:\n                self_or_cls.aliases.pop(k)", "response": "Removes a list of aliases from the self or class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prefixed(self, identifier, version):\n        invalid_starting = ['Mn', 'Mc', 'Nd', 'Pc']\n        if identifier.startswith('_'):  return True\n        return((identifier[0] in string.digits) if version==2\n               else (unicodedata.category(identifier[0]) in invalid_starting))", "response": "Returns True if the identifier should be prefixed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove diacritics and accents from the input leaving other unicode characters alone.", "response": "def remove_diacritics(self_or_cls, identifier):\n        \"\"\"\n        Remove diacritics and accents from the input leaving other\n        unicode characters alone.\"\"\"\n        chars = ''\n        for c in identifier:\n            replacement = unicodedata.normalize('NFKD', c).encode('ASCII', 'ignore')\n            if replacement != '':\n                chars += bytes_to_unicode(replacement)\n            else:\n                chars += c\n        return chars"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shortened_character_name(self_or_cls, c, eliminations=[], substitutions={}, transforms=[]):\n        name = unicodedata.name(c).lower()\n        # Filtering\n        for elim in eliminations:\n            name = name.replace(elim, '')\n        # Substitution\n        for i,o in substitutions.items():\n            name = name.replace(i, o)\n        for transform in transforms:\n            name = transform(name)\n        return ' '.join(name.strip().split()).replace(' ','_').replace('-','_')", "response": "Given a unicode character c return the shortened unicode name by applying the eliminations and substitutions and transforms."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstripping underscores to make sure the number is correct after join", "response": "def _process_underscores(self, tokens):\n        \"Strip underscores to make sure the number is correct after join\"\n        groups = [[str(''.join(el))] if b else list(el)\n                  for (b,el) in itertools.groupby(tokens, lambda k: k=='_')]\n        flattened = [el for group in groups for el in group]\n        processed = []\n        for token in flattened:\n            if token == '_':  continue\n            if token.startswith('_'):\n                token = str(token[1:])\n            if token.endswith('_'):\n                token = str(token[:-1])\n            processed.append(token)\n        return processed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sanitize(self, name, valid_fn):\n        \"Accumulate blocks of hex and separate blocks by underscores\"\n        invalid = {'\\a':'a','\\b':'b', '\\v':'v','\\f':'f','\\r':'r'}\n        for cc in filter(lambda el: el in name, invalid.keys()):\n            raise Exception(\"Please use a raw string or escape control code '\\%s'\"\n                            % invalid[cc])\n        sanitized, chars = [], ''\n        for split in name.split():\n            for c in split:\n                if valid_fn(c): chars += str(c) if c=='_' else c\n                else:\n                    short = self.shortened_character_name(c, self.eliminations,\n                                                         self.substitutions,\n                                                         self.transforms)\n                    sanitized.extend([chars] if chars else [])\n                    if short != '':\n                       sanitized.append(short)\n                    chars = ''\n            if chars:\n                sanitized.extend([chars])\n                chars=''\n        return self._process_underscores(sanitized + ([chars] if chars else []))", "response": "Accumulate blocks of hex and separate blocks by underscores"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef values(cls, dataset, dim, expanded=True, flat=True, compute=True):\n        dim_idx = dataset.get_dimension_index(dim)\n        if dim_idx in [0, 1]:\n            l, b, r, t = dataset.bounds.lbrt()\n            dim2, dim1 = dataset.data.shape[:2]\n            xdate, ydate = isinstance(l, util.datetime_types), isinstance(b, util.datetime_types)\n            if l == r or dim1 == 0:\n                xlin = np.full((dim1,), l, dtype=('datetime64[us]' if xdate else 'float'))\n            elif xdate:\n                xlin = util.date_range(l, r, dim1, dataset._time_unit)\n            else:\n                xstep = float(r - l)/dim1\n                xlin = np.linspace(l+(xstep/2.), r-(xstep/2.), dim1)\n            if b == t or dim2 == 0:\n                ylin = np.full((dim2,), b, dtype=('datetime64[us]' if ydate else 'float'))\n            elif ydate:\n                ylin = util.date_range(b, t, dim2, dataset._time_unit)\n            else:\n                ystep = float(t - b)/dim2\n                ylin = np.linspace(b+(ystep/2.), t-(ystep/2.), dim2)\n            if expanded:\n                values = np.meshgrid(ylin, xlin)[abs(dim_idx-1)]\n                return values.flatten() if flat else values.T\n            else:\n                return ylin if dim_idx else xlin\n        elif dataset.ndims <= dim_idx < len(dataset.dimensions()):\n            # Raster arrays are stored with different orientation\n            # than expanded column format, reorient before expanding\n            if dataset.data.ndim > 2:\n                data = dataset.data[:, :, dim_idx-dataset.ndims]\n            else:\n                data = dataset.data\n            data = np.flipud(data)\n            return data.T.flatten() if flat else data\n        else:\n            return None", "response": "Returns the values of the object in the specified dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nselecting the underlying numpy array in sheet coordinates.", "response": "def select(cls, dataset, selection_mask=None, **selection):\n        \"\"\"\n        Slice the underlying numpy array in sheet coordinates.\n        \"\"\"\n        selection = {k: slice(*sel) if isinstance(sel, tuple) else sel\n                     for k, sel in selection.items()}\n        coords = tuple(selection[kd.name] if kd.name in selection else slice(None)\n                       for kd in dataset.kdims)\n        if not any([isinstance(el, slice) for el in coords]):\n            return dataset.data[dataset.sheet2matrixidx(*coords)]\n\n        # Apply slices\n        xidx, yidx = coords\n        l, b, r, t = dataset.bounds.lbrt()\n        if isinstance(xidx, slice):\n            l = l if xidx.start is None else max(l, xidx.start)\n            r = r if xidx.stop is None else min(r, xidx.stop)\n        if isinstance(yidx, slice):\n            b = b if yidx.start is None else max(b, yidx.start)\n            t = t if yidx.stop is None else min(t, yidx.stop)\n        bounds = BoundingBox(points=((l, b), (r, t)))\n        slc = Slice(bounds, dataset)\n        return slc.submatrix(dataset.data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsamples the Raster along one or both of its dimensions returning a reduced dimensionality type which is either a ItemTable Curve or Scatter.", "response": "def sample(cls, dataset, samples=[]):\n        \"\"\"\n        Sample the Raster along one or both of its dimensions,\n        returning a reduced dimensionality type, which is either\n        a ItemTable, Curve or Scatter. If two dimension samples\n        and a new_xaxis is provided the sample will be the value\n        of the sampled unit indexed by the value in the new_xaxis\n        tuple.\n        \"\"\"\n        if len(samples[0]) == 1:\n            select = {dataset.kdims[0].name: [s[0] for s in samples]}\n            return tuple(dataset.select(**select).columns().values())\n        return [c+(dataset.data[dataset._coord2matrix(c)],) for c in samples]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a dataset object and data in the appropriate format for the interface return a simple scalar.", "response": "def unpack_scalar(cls, dataset, data):\n        \"\"\"\n        Given a dataset object and data in the appropriate format for\n        the interface, return a simple scalar.\n        \"\"\"\n        if np.isscalar(data) or len(data) != 1:\n            return data\n        key = list(data.keys())[0]\n\n        if len(data[key]) == 1 and key in dataset.vdims:\n            return data[key][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mandel(x, y, max_iters):\n    i = 0\n    c = complex(x,y)\n    z = 0.0j\n    for i in range(max_iters):\n        z = z*z + c\n        if (z.real*z.real + z.imag*z.imag) >= 4:\n            return i\n\n    return 255", "response": "Given the real and imaginary parts of a complex number x y determine if it is a candidate for membership in the Mandelbrot\n    set given a fixed number of iterations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef range(self, dim, data_range=True, dimension_range=True):\n        didx = self.get_dimension_index(dim)\n        dim = self.get_dimension(dim)\n        if didx == 1 and data_range and len(self):\n            mean = self.dimension_values(1)\n            neg_error = self.dimension_values(2)\n            if len(self.dimensions()) > 3:\n                pos_error = self.dimension_values(3)\n            else:\n                pos_error = neg_error\n            lower = np.nanmin(mean-neg_error)\n            upper = np.nanmax(mean+pos_error)\n            if not dimension_range:\n                return (lower, upper)\n            return util.dimension_range(lower, upper, dim.range, dim.soft_range)\n        return super(ErrorBars, self).range(dim, data_range)", "response": "Return the lower and upper bounds of values along the y - dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edges(self):\n        \"Property to access the Histogram edges provided for backward compatibility\"\n        if util.config.future_deprecations:\n            self.param.warning('Histogram.edges is deprecated in favor of '\n                               'common dimension_values method.')\n        return self.interface.coords(self, self.kdims[0], edges=True)", "response": "Property to access the Histogram edges provided for backward compatibility"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstack an NdOverlay of Area or Curve Elements by offsetting their baselines.", "response": "def stack(cls, areas):\n        \"\"\"\n        Stacks an (Nd)Overlay of Area or Curve Elements by offsetting\n        their baselines. To stack a HoloMap or DynamicMap use the map\n        method.\n        \"\"\"\n        if not len(areas):\n            return areas\n        baseline = np.zeros(len(areas.values()[0]))\n        stacked = areas.clone(shared_data=False)\n        vdims = [areas.values()[0].vdims[0], 'Baseline']\n        for k, area in areas.items():\n            x, y = (area.dimension_values(i) for i in range(2))\n            stacked[k] = area.clone((x, y+baseline, baseline), vdims=vdims,\n                                    new_type=Area)\n            baseline = baseline + y\n        return stacked"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncleans up references to the plot on the attached Stream subscribers.", "response": "def cleanup(self):\n        \"\"\"\n        Cleans up references to the plot on the attached Stream\n        subscribers.\n        \"\"\"\n        plots = self.traverse(lambda x: x, [Plot])\n        for plot in plots:\n            if not isinstance(plot, (GenericCompositePlot, GenericElementPlot, GenericOverlayPlot)):\n                continue\n            for stream in set(plot.streams):\n                stream._subscribers = [\n                    (p, subscriber) for p, subscriber in stream._subscribers\n                    if util.get_method_owner(subscriber) not in plots]\n        if self.comm:\n            self.comm.close()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef matches(self, spec):\n        if callable(spec) and not isinstance(spec, type): return spec(self)\n        elif isinstance(spec, type): return isinstance(self, spec)\n        else:\n            raise ValueError(\"Matching specs have to be either a type or a callable.\")", "response": "Matches a specification against the current Plot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntraverse any nested DimensionedPlot returning a list of all plots that match the specs.", "response": "def traverse(self, fn=None, specs=None, full_breadth=True):\n        \"\"\"\n        Traverses any nested DimensionedPlot returning a list\n        of all plots that match the specs. The specs should\n        be supplied as a list of either Plot types or callables,\n        which should return a boolean given the plot class.\n        \"\"\"\n        accumulator = []\n        matches = specs is None\n        if not matches:\n            for spec in specs:\n                matches = self.matches(spec)\n                if matches: break\n        if matches:\n            accumulator.append(fn(self) if fn else self)\n\n        # Assumes composite objects are iterables\n        if hasattr(self, 'subplots') and self.subplots:\n            for el in self.subplots.values():\n                if el is None:\n                    continue\n                accumulator += el.traverse(fn, specs, full_breadth)\n                if not full_breadth: break\n        return accumulator"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _frame_title(self, key, group_size=2, separator='\\n'):\n        if self.layout_dimensions is not None:\n            dimensions, key = zip(*self.layout_dimensions.items())\n        elif not self.dynamic and (not self.uniform or len(self) == 1) or self.subplot:\n            return ''\n        else:\n            key = key if isinstance(key, tuple) else (key,)\n            dimensions = self.dimensions\n        dimension_labels = [dim.pprint_value_string(k) for dim, k in\n                            zip(dimensions, key)]\n        groups = [', '.join(dimension_labels[i*group_size:(i+1)*group_size])\n                  for i in range(len(dimension_labels))]\n        return util.bytes_to_unicode(separator.join(g for g in groups if g))", "response": "Returns the formatted dimension group strings for a particular frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_ranges(self, obj, key, ranges):\n        all_table = all(isinstance(el, Table) for el in obj.traverse(lambda x: x, [Element]))\n        if obj is None or not self.normalize or all_table:\n            return OrderedDict()\n        # Get inherited ranges\n        ranges = self.ranges if ranges is None else dict(ranges)\n\n        # Get element identifiers from current object and resolve\n        # with selected normalization options\n        norm_opts = self._get_norm_opts(obj)\n\n        # Traverse displayed object if normalization applies\n        # at this level, and ranges for the group have not\n        # been supplied from a composite plot\n        return_fn = lambda x: x if isinstance(x, Element) else None\n        for group, (axiswise, framewise) in norm_opts.items():\n            elements = []\n            # Skip if ranges are cached or already computed by a\n            # higher-level container object.\n            framewise = framewise or self.dynamic or len(elements) == 1\n            if group in ranges and (not framewise or ranges is not self.ranges):\n                continue\n            elif not framewise: # Traverse to get all elements\n                elements = obj.traverse(return_fn, [group])\n            elif key is not None: # Traverse to get elements for each frame\n                frame = self._get_frame(key)\n                elements = [] if frame is None else frame.traverse(return_fn, [group])\n            # Only compute ranges if not axiswise on a composite plot\n            # or not framewise on a Overlay or ElementPlot\n            if (not (axiswise and not isinstance(obj, HoloMap)) or\n                (not framewise and isinstance(obj, HoloMap))):\n                self._compute_group_range(group, elements, ranges)\n        self.ranges.update(ranges)\n        return ranges", "response": "This method will traverse the object and compute the ranges for the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_norm_opts(self, obj):\n        norm_opts = {}\n\n        # Get all elements' type.group.label specs and ids\n        type_val_fn = lambda x: (x.id, (type(x).__name__, util.group_sanitizer(x.group, escape=False),\n                                        util.label_sanitizer(x.label, escape=False))) \\\n            if isinstance(x, Element) else None\n        element_specs = {(idspec[0], idspec[1]) for idspec in obj.traverse(type_val_fn)\n                         if idspec is not None}\n\n        # Group elements specs by ID and override normalization\n        # options sequentially\n        key_fn = lambda x: -1 if x[0] is None else x[0]\n        id_groups = groupby(sorted(element_specs, key=key_fn), key_fn)\n        for gid, element_spec_group in id_groups:\n            gid = None if gid == -1 else gid\n            group_specs = [el for _, el in element_spec_group]\n\n            backend = self.renderer.backend\n            optstree = Store.custom_options(\n                backend=backend).get(gid, Store.options(backend=backend))\n            # Get the normalization options for the current id\n            # and match against customizable elements\n            for opts in optstree:\n                path = tuple(opts.path.split('.')[1:])\n                applies = any(path == spec[:i] for spec in group_specs\n                              for i in range(1, 4))\n                if applies and 'norm' in opts.groups:\n                    nopts = opts['norm'].options\n                    if 'axiswise' in nopts or 'framewise' in nopts:\n                        norm_opts.update({path: (nopts.get('axiswise', False),\n                                                 nopts.get('framewise', False))})\n        element_specs = [spec for _, spec in element_specs]\n        norm_opts.update({spec: (False, False) for spec in element_specs\n                          if not any(spec[:i] in norm_opts.keys() for i in range(1, 4))})\n        return norm_opts", "response": "Get the normalization options for a LabelledData object by traversing the object and finding the appropriate elements and their ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntraverses the supplied object and return all options for the specified opt_type and specs.", "response": "def _traverse_options(cls, obj, opt_type, opts, specs=None, keyfn=None, defaults=True):\n        \"\"\"\n        Traverses the supplied object getting all options in opts for\n        the specified opt_type and specs. Also takes into account the\n        plotting class defaults for plot options. If a keyfn is\n        supplied the returned options will be grouped by the returned\n        keys.\n        \"\"\"\n        def lookup(x):\n            \"\"\"\n            Looks up options for object, including plot defaults.\n            keyfn determines returned key otherwise None key is used.\n            \"\"\"\n            options = cls.lookup_options(x, opt_type)\n            selected = {o: options.options[o]\n                        for o in opts if o in options.options}\n            if opt_type == 'plot' and defaults:\n                plot = Store.registry[cls.backend].get(type(x))\n                selected['defaults'] = {o: getattr(plot, o) for o in opts\n                                        if o not in selected and hasattr(plot, o)}\n            key = keyfn(x) if keyfn else None\n            return (key, selected)\n\n        # Traverse object and accumulate options by key\n        traversed = obj.traverse(lookup, specs)\n        options = defaultdict(lambda: defaultdict(list))\n        default_opts = defaultdict(lambda: defaultdict(list))\n        for key, opts in traversed:\n            defaults = opts.pop('defaults', {})\n            for opt, v in opts.items():\n                options[key][opt].append(v)\n            for opt, v in defaults.items():\n                default_opts[key][opt].append(v)\n\n        # Merge defaults into dictionary if not explicitly specified\n        for key, opts in default_opts.items():\n            for opt, v in opts.items():\n                if opt not in options[key]:\n                    options[key][opt] = v\n        return options if keyfn else options[None]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_projection(cls, obj):\n        isoverlay = lambda x: isinstance(x, CompositeOverlay)\n        element3d = obj.traverse(lambda x: x, [Element3D])\n        if element3d:\n            return '3d'\n        opts = cls._traverse_options(obj, 'plot', ['projection'],\n                                     [CompositeOverlay, Element],\n                                     keyfn=isoverlay)\n        from_overlay = not all(p is None for p in opts[True]['projection'])\n        projections = opts[from_overlay]['projection']\n        custom_projs = [p for p in projections if p is not None]\n        if len(set(custom_projs)) > 1:\n            raise Exception(\"An axis may only be assigned one projection type\")\n        return custom_projs[0] if custom_projs else None", "response": "Returns the appropriate projection for a nested object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrefreshing the current data for the current key and pushes the updated data if the Comm has associated Comm.", "response": "def refresh(self, **kwargs):\n        \"\"\"\n        Refreshes the plot by rerendering it and then pushing\n        the updated data if the plot has an associated Comm.\n        \"\"\"\n        traverse_setter(self, '_force', True)\n        key = self.current_key if self.current_key else self.keys[0]\n        dim_streams = [stream for stream in self.streams\n                       if any(c in self.dimensions for c in stream.contents)]\n        stream_params = stream_parameters(dim_streams)\n        key = tuple(None if d in stream_params else k\n                    for d, k in zip(self.dimensions, key))\n        stream_key = util.wrap_tuple_streams(key, self.dimensions, self.streams)\n\n        self._trigger_refresh(stream_key)\n        if self.comm is not None and self.top_level:\n            self.push()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _trigger_refresh(self, key):\n        \"Triggers update to a plot on a refresh event\"\n        # Update if not top-level, batched or an ElementPlot\n        if not self.top_level or isinstance(self, GenericElementPlot):\n            self.update(key)", "response": "Triggers update to a plot on a refresh event"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npush updated plot data via the Comm.", "response": "def push(self):\n        \"\"\"\n        Pushes updated plot data via the Comm.\n        \"\"\"\n        if self.comm is None:\n            raise Exception('Renderer does not have a comm.')\n        diff = self.renderer.diff(self)\n        self.comm.send(diff)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninitialize comm and attaches streams.", "response": "def init_comm(self):\n        \"\"\"\n        Initializes comm and attaches streams.\n        \"\"\"\n        if self.comm:\n            return self.comm\n        comm = None\n        if self.dynamic or self.renderer.widget_mode == 'live':\n            comm = self.renderer.comm_manager.get_server_comm()\n        return comm"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_zorder(self, overlay, key, el):\n        spec = util.get_overlay_spec(overlay, key, el)\n        return self.ordering.index(spec)", "response": "Returns the z - order of the element in the NdOverlay."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _execute_hooks(self, element):\n        if self.hooks and self.finalize_hooks:\n            self.param.warning(\n                \"Supply either hooks or finalize_hooks not both, \"\n                \"using hooks and ignoring finalize_hooks.\")\n        hooks = self.hooks or self.finalize_hooks\n        for hook in hooks:\n            try:\n                hook(self, element)\n            except Exception as e:\n                self.param.warning(\"Plotting hook %r could not be \"\n                                   \"applied:\\n\\n %s\" % (hook, e))", "response": "Executes the hooks that are applied to the given element."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the padding along the axes taking into account the aspect.", "response": "def get_padding(self, extents):\n        \"\"\"\n        Computes padding along the axes taking into account the plot aspect.\n        \"\"\"\n        (x0, y0, z0, x1, y1, z1) = extents\n        padding = 0 if self.overlaid else self.padding\n        xpad, ypad, zpad = get_axis_padding(padding)\n        if not self.overlaid and not self.batched:\n            xspan = x1-x0 if util.is_number(x0) and util.is_number(x1) else None\n            yspan = y1-y0 if util.is_number(y0) and util.is_number(y1) else None\n            aspect = self.get_aspect(xspan, yspan)\n            if aspect > 1:\n                xpad = tuple(xp/aspect for xp in xpad) if isinstance(xpad, tuple) else xpad/aspect\n            else:\n                ypad = tuple(yp*aspect for yp in ypad) if isinstance(ypad, tuple) else ypad*aspect\n        return xpad, ypad, zpad"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the extents for the axes from the current Element.", "response": "def get_extents(self, element, ranges, range_type='combined', xdim=None, ydim=None, zdim=None):\n        \"\"\"\n        Gets the extents for the axes from the current Element. The globally\n        computed ranges can optionally override the extents.\n\n        The extents are computed by combining the data ranges, extents\n        and dimension ranges. Each of these can be obtained individually\n        by setting the range_type to one of:\n\n        * 'data': Just the data ranges\n        * 'extents': Element.extents\n        * 'soft': Dimension.soft_range values\n        * 'hard': Dimension.range values\n\n        To obtain the combined range, which includes range padding the\n        default may be used:\n\n        * 'combined': All the range types combined and padding applied\n\n        This allows Overlay plots to obtain each range and combine them\n        appropriately for all the objects in the overlay.\n        \"\"\"\n        num = 6 if self.projection == '3d' else 4\n        if self.apply_extents and range_type in ('combined', 'extents'):\n            norm_opts = self.lookup_options(element, 'norm').options\n            if norm_opts.get('framewise', False) or self.dynamic:\n                extents = element.extents\n            else:\n                extent_list = self.hmap.traverse(lambda x: x.extents, [Element])\n                extents = util.max_extents(extent_list, self.projection == '3d')\n        else:\n            extents = (np.NaN,) * num\n\n        if range_type == 'extents':\n            return extents\n\n        if self.apply_ranges:\n            range_extents = self._get_range_extents(element, ranges, range_type, xdim, ydim, zdim)\n        else:\n            range_extents = (np.NaN,) * num\n\n        if getattr(self, 'shared_axes', False) and self.subplot:\n            combined = util.max_extents([range_extents, extents], self.projection == '3d')\n        else:\n            max_extent = []\n            for l1, l2 in zip(range_extents, extents):\n                if isfinite(l2):\n                    max_extent.append(l2)\n                else:\n                    max_extent.append(l1)\n            combined = tuple(max_extent)\n\n        if self.projection == '3d':\n            x0, y0, z0, x1, y1, z1 = combined\n        else:\n            x0, y0, x1, y1 = combined\n\n        x0, x1 = util.dimension_range(x0, x1, self.xlim, (None, None))\n        y0, y1 = util.dimension_range(y0, y1, self.ylim, (None, None))\n        if self.projection == '3d':\n            z0, z1 = util.dimension_range(z0, z1, self.zlim, (None, None))\n            return (x0, y0, z0, x1, y1, z1)\n        return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the appropriate mapwise or framewise compositor for the given HoloMap and apply the appropriate Compositor collapse operations.", "response": "def _apply_compositor(self, holomap, ranges=None, keys=None, dimensions=None):\n        \"\"\"\n        Given a HoloMap compute the appropriate (mapwise or framewise)\n        ranges in order to apply the Compositor collapse operations in\n        display mode (data collapse should already have happened).\n        \"\"\"\n        # Compute framewise normalization\n        defaultdim = holomap.ndims == 1 and holomap.kdims[0].name != 'Frame'\n\n        if keys and ranges and dimensions and not defaultdim:\n            dim_inds = [dimensions.index(d) for d in holomap.kdims]\n            sliced_keys = [tuple(k[i] for i in dim_inds) for k in keys]\n            frame_ranges = OrderedDict([(slckey, self.compute_ranges(holomap, key, ranges[key]))\n                                        for key, slckey in zip(keys, sliced_keys) if slckey in holomap.data.keys()])\n        else:\n            mapwise_ranges = self.compute_ranges(holomap, None, None)\n            frame_ranges = OrderedDict([(key, self.compute_ranges(holomap, key, mapwise_ranges))\n                                        for key in holomap.data.keys()])\n        ranges = frame_ranges.values()\n\n        return Compositor.collapse(holomap, (ranges, frame_ranges.keys()), mode='display')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating subplots for DynamicMap.", "response": "def _create_dynamic_subplots(self, key, items, ranges, **init_kwargs):\n        \"\"\"\n        Handles the creation of new subplots when a DynamicMap returns\n        a changing set of elements in an Overlay.\n        \"\"\"\n        length = self.style_grouping\n        group_fn = lambda x: (x.type.__name__, x.last.group, x.last.label)\n        for i, (k, obj) in enumerate(items):\n            vmap = self.hmap.clone([(key, obj)])\n            self.map_lengths[group_fn(vmap)[:length]] += 1\n            subplot = self._create_subplot(k, vmap, [], ranges)\n            if subplot is None:\n                continue\n            self.subplots[k] = subplot\n            subplot.initialize_plot(ranges, **init_kwargs)\n            subplot.update_frame(key, ranges, element=obj)\n            self.dynamic_subplots.append(subplot)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate existing subplots when the subplot has been assigned to plot an element that is not an exact match to the object .", "response": "def _update_subplot(self, subplot, spec):\n        \"\"\"\n        Updates existing subplots when the subplot has been assigned\n        to plot an element that is not an exact match to the object\n        it was initially assigned.\n        \"\"\"\n\n        # See if the precise spec has already been assigned a cyclic\n        # index otherwise generate a new one\n        if spec in self.cyclic_index_lookup:\n            cyclic_index = self.cyclic_index_lookup[spec]\n        else:\n            group_key = spec[:self.style_grouping]\n            self.group_counter[group_key] += 1\n            cyclic_index = self.group_counter[group_key]\n            self.cyclic_index_lookup[spec] = cyclic_index\n\n        subplot.cyclic_index = cyclic_index\n        if subplot.overlay_dims:\n            odim_key = util.wrap_tuple(spec[-1])\n            new_dims = zip(subplot.overlay_dims, odim_key)\n            subplot.overlay_dims = util.OrderedDict(new_dims)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the extents of all subplots and return them as a dict.", "response": "def _get_subplot_extents(self, overlay, ranges, range_type):\n        \"\"\"\n        Iterates over all subplots and collects the extents of each.\n        \"\"\"\n        if range_type == 'combined':\n            extents = {'extents': [], 'soft': [], 'hard': [], 'data': []}\n        else:\n            extents = {range_type: []}\n        items = overlay.items()\n        if self.batched and self.subplots:\n            subplot = list(self.subplots.values())[0]\n            subplots = [(k, subplot) for k in overlay.data.keys()]\n        else:\n            subplots = self.subplots.items()\n\n        for key, subplot in subplots:\n            found = False\n            if subplot is None:\n                continue\n            layer = overlay.data.get(key, None)\n            if isinstance(self.hmap, DynamicMap) and layer is None:\n                for _, layer in items:\n                    if isinstance(layer, subplot.hmap.type):\n                        found = True\n                        break\n                if not found:\n                    layer = None\n            if layer is None or not subplot.apply_ranges:\n                continue\n\n            if isinstance(layer, CompositeOverlay):\n                sp_ranges = ranges\n            else:\n                sp_ranges = util.match_spec(layer, ranges) if ranges else {}\n            for rt in extents:\n                extent = subplot.get_extents(layer, sp_ranges, range_type=rt)\n                extents[rt].append(extent)\n        return extents"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_frame(self, key):\n        cached = self.current_key is None\n        layout_frame = self.layout.clone(shared_data=False)\n        if key == self.current_key and not self._force:\n            return self.current_frame\n        else:\n            self.current_key = key\n\n        key_map = dict(zip([d.name for d in self.dimensions], key))\n        for path, item in self.layout.items():\n            frame = get_nested_plot_frame(item, key_map, cached)\n            if frame is not None:\n                layout_frame[path] = frame\n        traverse_setter(self, '_force', False)\n\n        self.current_frame = layout_frame\n        return layout_frame", "response": "Returns a clone of the Layout with the nth - frame for each Arc element."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstrips out any leading / training commas from the token", "response": "def _strip_commas(cls, kw):\n        \"Strip out any leading/training commas from the token\"\n        kw = kw[:-1] if kw[-1]==',' else kw\n        return kw[1:] if kw[0]==',' else kw"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncollect the tokens from a nested parse result.", "response": "def collect_tokens(cls, parseresult, mode):\n        \"\"\"\n        Collect the tokens from a (potentially) nested parse result.\n        \"\"\"\n        inner = '(%s)' if mode=='parens' else '[%s]'\n        if parseresult is None: return []\n        tokens = []\n        for token in parseresult.asList():\n            # If value is a tuple, the token will be a list\n            if isinstance(token, list):\n                token = cls.recurse_token(token, inner)\n                tokens[-1] = tokens[-1] + token\n            else:\n                if token.strip() == ',': continue\n                tokens.append(cls._strip_commas(token))\n        return tokens"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef todict(cls, parseresult, mode='parens', ns={}):\n        grouped, kwargs = [], {}\n        tokens = cls.collect_tokens(parseresult, mode)\n        # Group tokens without '=' and append to last token containing '='\n        for group in groupby(tokens, lambda el: '=' in el):\n            (val, items) = group\n            if val is True:\n                grouped += list(items)\n            if val is False:\n                elements =list(items)\n                # Assume anything before ) or } can be joined with commas\n                # (e.g tuples with spaces in them)\n                joiner=',' if any(((')' in el) or ('}' in el))\n                                  for el in elements) else ''\n                grouped[-1] += joiner + joiner.join(elements)\n\n        for keyword in grouped:\n            # Tuple ('a', 3) becomes (,'a',3) and '(,' is never valid\n            # Same for some of the other joining errors corrected here\n            for (fst,snd) in [('(,', '('), ('{,', '{'), ('=,','='),\n                              (',:',':'), (':,', ':'), (',,', ','),\n                              (',.', '.')]:\n                keyword = keyword.replace(fst, snd)\n            try:\n                kwargs.update(eval('dict(%s)' % keyword,\n                                   dict(cls.namespace, **ns)))\n            except:\n                if cls.abort_on_eval_failure:\n                    raise SyntaxError(\"Could not evaluate keyword: %r\"\n                                      % keyword)\n                msg = \"Ignoring keyword pair that fails to evaluate: '%s'\"\n                parsewarning.warning(msg % keyword)\n\n        return kwargs", "response": "Helper function to return dictionary given the parse results from a pyparsing. nestedExpr object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess a normalization parse group and return the appropriate dictionary of the appropriate values for the plotting option.", "response": "def process_normalization(cls, parse_group):\n        \"\"\"\n        Given a normalization parse group (i.e. the contents of the\n        braces), validate the option list and compute the appropriate\n        integer value for the normalization plotting option.\n        \"\"\"\n        if ('norm_options' not in parse_group): return None\n        opts = parse_group['norm_options'][0].asList()\n        if opts == []: return None\n\n        options = ['+framewise', '-framewise', '+axiswise', '-axiswise']\n\n        for normopt in options:\n            if opts.count(normopt) > 1:\n                raise SyntaxError(\"Normalization specification must not\"\n                                  \" contain repeated %r\" % normopt)\n\n        if not all(opt in options for opt in opts):\n            raise SyntaxError(\"Normalization option not one of %s\"\n                              % \", \".join(options))\n        excluded = [('+framewise', '-framewise'), ('+axiswise', '-axiswise')]\n        for pair in excluded:\n            if all(exclude in opts for exclude in pair):\n                raise SyntaxError(\"Normalization specification cannot\"\n                                  \" contain both %s and %s\" % (pair[0], pair[1]))\n\n        # If unspecified, default is -axiswise and -framewise\n        if len(opts) == 1 and opts[0].endswith('framewise'):\n            axiswise = False\n            framewise = True if '+framewise' in opts else False\n        elif len(opts) == 1 and opts[0].endswith('axiswise'):\n            framewise = False\n            axiswise = True if '+axiswise' in opts else False\n        else:\n            axiswise = True if '+axiswise' in opts else False\n            framewise = True if '+framewise' in opts else False\n\n        return dict(axiswise=axiswise,\n                    framewise=framewise)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _group_paths_without_options(cls, line_parse_result):\n        active_pathspecs = set()\n        for group in line_parse_result:\n            active_pathspecs.add(group['pathspec'])\n\n            has_options = (\n                'norm_options' in group or\n                'plot_options' in group or\n                'style_options' in group\n            )\n            if has_options:\n                yield active_pathspecs, group\n                active_pathspecs = set()\n\n        if active_pathspecs:\n            yield active_pathspecs, {}", "response": "Given a parsed options specification as a list of groups yield a set of active pathspecs and group that has options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef apply_deprecations(cls, path):\n        \"Convert any potentially deprecated paths and issue appropriate warnings\"\n        split = path.split('.')\n        msg = 'Element {old} deprecated. Use {new} instead.'\n        for old, new in cls.deprecations:\n            if split[0] == old:\n                parsewarning.warning(msg.format(old=old, new=new))\n                return '.'.join([new] + split[1:])\n        return path", "response": "Convert any potentially deprecated paths and issue appropriate warnings"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse(cls, line, ns={}):\n        parses  = [p for p in cls.opts_spec.scanString(line)]\n        if len(parses) != 1:\n            raise SyntaxError(\"Invalid specification syntax.\")\n        else:\n            e = parses[0][2]\n            processed = line[:e]\n            if (processed.strip() != line.strip()):\n                raise SyntaxError(\"Failed to parse remainder of string: %r\" % line[e:])\n\n        grouped_paths = cls._group_paths_without_options(cls.opts_spec.parseString(line))\n        parse = {}\n        for pathspecs, group in grouped_paths:\n            options = {}\n\n            normalization = cls.process_normalization(group)\n            if normalization is not None:\n                options['norm'] = normalization\n\n            if 'plot_options' in group:\n                plotopts =  group['plot_options'][0]\n                opts = cls.todict(plotopts, 'brackets', ns=ns)\n                options['plot'] = {cls.aliases.get(k,k):v for k,v in opts.items()}\n\n            if 'style_options' in group:\n                styleopts = group['style_options'][0]\n                opts = cls.todict(styleopts, 'parens', ns=ns)\n                options['style'] = {cls.aliases.get(k,k):v for k,v in opts.items()}\n\n            for pathspec in pathspecs:\n                parse[pathspec] = merge_option_dicts(parse.get(pathspec, {}), options)\n\n        return {\n            cls.apply_deprecations(path): {\n                option_type: Options(**option_pairs)\n                for option_type, option_pairs in options.items()\n            }\n            for path, options in parse.items()\n        }", "response": "Parse an options specification returning a dictionary with the keys path and options and values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_options(cls, line, ns={}):\n        parsed = cls.parse(line, ns=ns)\n        options_list = []\n        for spec in sorted(parsed.keys()):\n            options = parsed[spec]\n            merged = {}\n            for group in options.values():\n                merged = dict(group.kwargs, **merged)\n            options_list.append(Options(spec, **merged))\n        return options_list", "response": "Similar to parse but returns a list of Options objects instead of the dictionary format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing compositor specifications returning a list of Compositors", "response": "def parse(cls, line, ns={}):\n        \"\"\"\n        Parse compositor specifications, returning a list Compositors\n        \"\"\"\n        definitions = []\n        parses  = [p for p in cls.compositor_spec.scanString(line)]\n        if len(parses) != 1:\n            raise SyntaxError(\"Invalid specification syntax.\")\n        else:\n            e = parses[0][2]\n            processed = line[:e]\n            if (processed.strip() != line.strip()):\n                raise SyntaxError(\"Failed to parse remainder of string: %r\" % line[e:])\n\n        opmap = {op.__name__:op for op in Compositor.operations}\n        for group in cls.compositor_spec.parseString(line):\n\n            if ('mode' not in group) or group['mode'] not in ['data', 'display']:\n                raise SyntaxError(\"Either data or display mode must be specified.\")\n            mode = group['mode']\n\n            kwargs = {}\n            operation = opmap[group['op']]\n            spec = ' '.join(group['spec'].asList()[0])\n\n            if  group['op'] not in opmap:\n                raise SyntaxError(\"Operation %s not available for use with compositors.\"\n                                  % group['op'])\n            if  'op_settings' in group:\n                kwargs = cls.todict(group['op_settings'][0], 'brackets', ns=ns)\n\n            definition = Compositor(str(spec), operation, str(group['value']), mode, **kwargs)\n            definitions.append(definition)\n        return definitions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_ranges(self, element, key):\n        keys = self.p['keys']\n        ranges = self.p['ranges']\n\n        if ranges == {}:\n            return {d.name: element.range(d.name, self.data_range)\n                    for d in element.dimensions()}\n        if keys is None:\n            specs = ranges\n        elif keys and not isinstance(ranges, list):\n            raise ValueError(\"Key list specified but ranges parameter\"\n                             \" not specified as a list.\")\n        elif len(keys) == len(ranges):\n            # Unpack any 1-tuple keys\n            try:\n                index = keys.index(key)\n                specs = ranges[index]\n            except:\n                raise KeyError(\"Could not match element key to defined keys\")\n        else:\n            raise ValueError(\"Key list length must match length of supplied ranges\")\n\n        return match_spec(element, specs)", "response": "Method to get the appropriate normalization range dictionary for a key and element."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a function to all ViewableElements.", "response": "def aggregate(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n        \"\"\"Applies a aggregate function to all ViewableElements.\n\n        See :py:meth:`Dimensioned.opts` and :py:meth:`Apply.__call__`\n        for more information.\n        \"\"\"\n        kwargs['_method_args'] = (dimensions, function, spreadfn)\n        return self.__call__('aggregate', **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reduce(self, dimensions=[], function=None, spreadfn=None, **kwargs):\n        kwargs['_method_args'] = (dimensions, function, spreadfn)\n        return self.__call__('reduce', **kwargs)", "response": "Applies a reduce function to all ViewableElements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef replace_dimensions(cls, dimensions, overrides):\n        from .dimension import Dimension\n        \n        replaced = []\n        for d in dimensions:\n            if d.name in overrides:\n                override = overrides[d.name]\n            elif d.label in overrides:\n                override = overrides[d.label]\n            else:\n                override = None\n\n            if override is None:\n                replaced.append(d)\n            elif isinstance(override, (util.basestring, tuple)):\n                replaced.append(d.clone(override))\n            elif isinstance(override, Dimension):\n                replaced.append(override)\n            elif isinstance(override, dict):\n                replaced.append(d.clone(override.get('name',None),\n                                        **{k:v for k,v in override.items() if k != 'name'}))\n            else:\n                raise ValueError('Dimension can only be overridden '\n                                 'with another dimension or a dictionary '\n                                 'of attributes')\n        return replaced", "response": "Replaces dimensions in list with dictionary of overrides."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a filtered version of the DynamicMap cache leaving only those keys consistently with the newly specified values", "response": "def _filter_cache(self, dmap, kdims):\n        \"\"\"\n        Returns a filtered version of the DynamicMap cache leaving only\n        keys consistently with the newly specified values\n        \"\"\"\n        filtered = []\n        for key, value in dmap.data.items():\n            if not any(kd.values and v not in kd.values for kd, v in zip(kdims, key)):\n                filtered.append((key, value))\n        return filtered"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, group=None, backend=None):\n        from .options import Store, Options\n        keywords = {}\n        groups = Options._option_groups if group is None else [group]\n        backend = backend if backend else Store.current_backend\n        for group in groups:\n            optsobj = Store.lookup_options(backend, self._obj, group)\n            keywords = dict(keywords, **optsobj.kwargs)\n        return Options(**keywords)", "response": "Returns the corresponding Options object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef info(self, show_defaults=False):\n        pprinter = PrettyPrinter(show_options=True, show_defaults=show_defaults)\n        print(pprinter.pprint(self._obj))", "response": "Prints a repr of the object including any applied options."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cast(cls, datasets, datatype=None, cast_type=None):\n        datatype = datatype or cls.datatype\n        cast = []\n        for ds in datasets:\n            if cast_type is not None or ds.interface.datatype != datatype:\n                ds = ds.clone(ds, datatype=[datatype], new_type=cast_type)\n            cast.append(ds)\n        return cast", "response": "Given a list of Dataset objects and a datatype returns a list of Dataset objects that are of the specified datatype."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef indexed(cls, dataset, selection):\n        selected = list(selection.keys())\n        all_scalar = all((not isinstance(sel, (tuple, slice, set, list))\n                          and not callable(sel)) for sel in selection.values())\n        all_kdims = all(d in selected for d in dataset.kdims)\n        return all_scalar and all_kdims", "response": "Returns whether a scalar value has been indexed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_extents(self, element, ranges, range_type='combined'):\n        if self.batched:\n            overlay = self.current_frame\n            element = Bars(overlay.table(), kdims=element.kdims+overlay.kdims,\n                           vdims=element.vdims)\n            for kd in overlay.kdims:\n                ranges[kd.name]['combined'] = overlay.range(kd)\n\n        xdim = element.kdims[0]\n        ydim = element.vdims[0]\n\n        # Compute stack heights\n        if self.stacked or self.stack_index:\n            ds = Dataset(element)\n            pos_range = ds.select(**{ydim.name: (0, None)}).aggregate(xdim, function=np.sum).range(ydim)\n            neg_range = ds.select(**{ydim.name: (None, 0)}).aggregate(xdim, function=np.sum).range(ydim)\n            y0, y1 = util.max_range([pos_range, neg_range])\n        else:\n            y0, y1 = ranges[ydim.name]['combined']\n\n        padding = 0 if self.overlaid else self.padding\n        _, ypad, _ = get_axis_padding(padding)\n        y0, y1 = util.range_pad(y0, y1, ypad, self.logy)\n\n        # Set y-baseline\n        if y0 < 0:\n            y1 = max([y1, 0])\n        elif self.logy:\n            y0 = (ydim.range[0] or (10**(np.log10(y1)-2)) if y1 else 0.01)\n        else:\n            y0 = 0\n\n        # Ensure x-axis is picked up as categorical\n        nx = len(element.dimension_values(0, False))\n        return (-0.5, y0, nx-0.5, y1)", "response": "Compute the extents of the current bar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a dataset object and data in the appropriate format for the interface return a simple scalar.", "response": "def unpack_scalar(cls, dataset, data):\n        \"\"\"\n        Given a dataset object and data in the appropriate format for\n        the interface, return a simple scalar.\n        \"\"\"\n        if (len(data.data_vars) == 1 and\n            len(data[dataset.vdims[0].name].shape) == 0):\n            return data[dataset.vdims[0].name].item()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the data elements are of a certain type.", "response": "def _item_check(self, dim_vals, data):\n        \"\"\"\n        Applies optional checks to individual data elements before\n        they are inserted ensuring that they are of a certain\n        type. Subclassed may implement further element restrictions.\n        \"\"\"\n        if not self._check_items:\n            return\n        elif self.data_type is not None and not isinstance(data, self.data_type):\n            if isinstance(self.data_type, tuple):\n                data_type = tuple(dt.__name__ for dt in self.data_type)\n            else:\n                data_type = self.data_type.__name__\n            raise TypeError('{slf} does not accept {data} type, data elements have '\n                            'to be a {restr}.'.format(slf=type(self).__name__,\n                                                      data=type(data).__name__,\n                                                      restr=data_type))\n        elif not len(dim_vals) == self.ndims:\n            raise KeyError('The data contains keys of length %d, but the kdims '\n                           'only declare %d dimensions. Ensure that the number '\n                           'of kdims match the length of the keys in your data.'\n                           % (len(dim_vals), self.ndims))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds item to the data structure and applies dimension types and ensuring that the key values conforms to Dimension type and values.", "response": "def _add_item(self, dim_vals, data, sort=True, update=True):\n        \"\"\"\n        Adds item to the data, applying dimension types and ensuring\n        key conforms to Dimension type and values.\n        \"\"\"\n        sort = sort and self.sort\n        if not isinstance(dim_vals, tuple):\n            dim_vals = (dim_vals,)\n\n        self._item_check(dim_vals, data)\n\n        # Apply dimension types\n        dim_types = zip([kd.type for kd in self.kdims], dim_vals)\n        dim_vals = tuple(v if None in [t, v] else t(v) for t, v in dim_types)\n        valid_vals = zip(self.kdims, dim_vals)\n\n        for dim, val in valid_vals:\n            if dim.values and val is not None and val not in dim.values:\n                raise KeyError('%s dimension value %s not in'\n                               ' specified dimension values.' % (dim, repr(val)))\n\n        # Updates nested data structures rather than simply overriding them.\n        if (update and (dim_vals in self.data)\n            and isinstance(self.data[dim_vals], (MultiDimensionalMapping, OrderedDict))):\n            self.data[dim_vals].update(data)\n        else:\n            self.data[dim_vals] = data\n\n        if sort:\n            self._resort()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _apply_key_type(self, keys):\n        typed_key = ()\n        for dim, key in zip(self.kdims, keys):\n            key_type = dim.type\n            if key_type is None:\n                typed_key += (key,)\n            elif isinstance(key, slice):\n                sl_vals = [key.start, key.stop, key.step]\n                typed_key += (slice(*[key_type(el) if el is not None else None\n                                      for el in sl_vals]),)\n            elif key is Ellipsis:\n                typed_key += (key,)\n            elif isinstance(key, list):\n                typed_key += ([key_type(k) for k in key],)\n            else:\n                typed_key += (key_type(key),)\n        return typed_key", "response": "Applies the type to the supplied key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _split_index(self, key):\n        if not isinstance(key, tuple):\n            key = (key,)\n        elif key == ():\n            return (), ()\n\n        if key[0] is Ellipsis:\n            num_pad = self.ndims - len(key) + 1\n            key = (slice(None),) * num_pad + key[1:]\n        elif len(key) < self.ndims:\n            num_pad = self.ndims - len(key)\n            key = key + (slice(None),) * num_pad\n\n        map_slice = key[:self.ndims]\n        if self._check_key_type:\n            map_slice = self._apply_key_type(map_slice)\n        if len(key) == self.ndims:\n            return map_slice, ()\n        else:\n            return map_slice, key[self.ndims:]", "response": "Splits the index into key and deep dimension groups."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dataslice(self, data, indices):\n        if self._deep_indexable and isinstance(data, Dimensioned) and indices:\n            return data[indices]\n        elif len(indices) > 0:\n            self.param.warning('Cannot index into data element, extra data'\n                               ' indices ignored.')\n        return data", "response": "Slice the data element if deep indexable."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone(self, data=None, shared_data=True, *args, **overrides):\n        with item_check(not shared_data and self._check_items):\n            return super(MultiDimensionalMapping, self).clone(data, shared_data,\n                                                              *args, **overrides)", "response": "Clones the object overriding data and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngrouping object by one or more dimensions returning a dictionary containing the groups.", "response": "def groupby(self, dimensions, container_type=None, group_type=None, **kwargs):\n        \"\"\"Groups object by one or more dimensions\n\n        Applies groupby operation over the specified dimensions\n        returning an object of type container_type (expected to be\n        dictionary-like) containing the groups.\n\n        Args:\n            dimensions: Dimension(s) to group by\n            container_type: Type to cast group container to\n            group_type: Type to cast each group to\n            dynamic: Whether to return a DynamicMap\n            **kwargs: Keyword arguments to pass to each group\n\n        Returns:\n            Returns object of supplied container_type containing the\n            groups. If dynamic=True returns a DynamicMap instead.\n        \"\"\"\n        if self.ndims == 1:\n            self.param.warning('Cannot split Map with only one dimension.')\n            return self\n        elif not isinstance(dimensions, list):\n            dimensions = [dimensions]\n        container_type = container_type if container_type else type(self)\n        group_type = group_type if group_type else type(self)\n        dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        with item_check(False):\n            return util.ndmapping_groupby(self, dimensions, container_type,\n                                          group_type, sort=True, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a dimension and its values to the object.", "response": "def add_dimension(self, dimension, dim_pos, dim_val, vdim=False, **kwargs):\n        \"\"\"Adds a dimension and its values to the object\n\n        Requires the dimension name or object, the desired position in\n        the key dimensions and a key value scalar or sequence of the\n        same length as the existing keys.\n\n        Args:\n            dimension: Dimension or dimension spec to add\n            dim_pos (int) Integer index to insert dimension at\n            dim_val (scalar or ndarray): Dimension value(s) to add\n            vdim: Disabled, this type does not have value dimensions\n            **kwargs: Keyword arguments passed to the cloned element\n\n        Returns:\n            Cloned object containing the new dimension\n        \"\"\"\n        dimension = asdim(dimension)\n\n        if dimension in self.dimensions():\n            raise Exception('{dim} dimension already defined'.format(dim=dimension.name))\n\n        if vdim and self._deep_indexable:\n            raise Exception('Cannot add value dimension to object that is deep indexable')\n\n        if vdim:\n            dims = self.vdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(vdims=dims)\n            dim_pos += self.ndims\n        else:\n            dims = self.kdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(kdims=dims)\n\n        if isinstance(dim_val, basestring) or not hasattr(dim_val, '__iter__'):\n            dim_val = cycle([dim_val])\n        else:\n            if not len(dim_val) == len(self):\n                raise ValueError(\"Added dimension values must be same length\"\n                                 \"as existing keys.\")\n\n        items = OrderedDict()\n        for dval, (key, val) in zip(dim_val, self.data.items()):\n            if vdim:\n                new_val = list(val)\n                new_val.insert(dim_pos, dval)\n                items[key] = tuple(new_val)\n            else:\n                new_key = list(key)\n                new_key.insert(dim_pos, dval)\n                items[tuple(new_key)] = val\n\n        return self.clone(items, **dict(dimensions, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndrops dimension from keys", "response": "def drop_dimension(self, dimensions):\n        \"\"\"Drops dimension(s) from keys\n\n        Args:\n            dimensions: Dimension(s) to drop\n\n        Returns:\n            Clone of object with with dropped dimension(s)\n        \"\"\"\n        dimensions = [dimensions] if np.isscalar(dimensions) else dimensions\n        dims = [d for d in self.kdims if d not in dimensions]\n        dim_inds = [self.get_dimension_index(d) for d in dims]\n        key_getter = itemgetter(*dim_inds)\n        return self.clone([(key_getter(k), v) for k, v in self.data.items()],\n                          kdims=dims)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the values along the requested dimension.", "response": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        dimension = self.get_dimension(dimension, strict=True)\n        if dimension in self.kdims:\n            return np.array([k[self.get_dimension_index(dimension)] for k in self.data.keys()])\n        if dimension in self.dimensions():\n            values = [el.dimension_values(dimension, expanded, flat) for el in self\n                      if dimension in el.dimensions()]\n            vals = np.concatenate(values)\n            return vals if expanded else util.unique_array(vals)\n        else:\n            return super(MultiDimensionalMapping, self).dimension_values(dimension, expanded, flat)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new object with a reduced set of key dimensions.", "response": "def reindex(self, kdims=[], force=False):\n        \"\"\"Reindexes object dropping static or supplied kdims\n\n        Creates a new object with a reordered or reduced set of key\n        dimensions. By default drops all non-varying key dimensions.\n\n        Reducing the number of key dimensions will discard information\n        from the keys. All data values are accessible in the newly\n        created object as the new labels must be sufficient to address\n        each value uniquely.\n\n        Args:\n            kdims (optional): New list of key dimensions after reindexing\n            force (bool, optional): Whether to drop non-unique items\n\n        Returns:\n            Reindexed object\n        \"\"\"\n        old_kdims = [d.name for d in self.kdims]\n        if not isinstance(kdims, list):\n            kdims = [kdims]\n        elif not len(kdims):\n            kdims = [d for d in old_kdims\n                     if not len(set(self.dimension_values(d))) == 1]\n        indices = [self.get_dimension_index(el) for el in kdims]\n\n        keys = [tuple(k[i] for i in indices) for k in self.data.keys()]\n        reindexed_items = OrderedDict(\n            (k, v) for (k, v) in zip(keys, self.data.values()))\n        reduced_dims = set([d.name for d in self.kdims]).difference(kdims)\n        dimensions = [self.get_dimension(d) for d in kdims\n                      if d not in reduced_dims]\n\n        if len(set(keys)) != len(keys) and not force:\n            raise Exception(\"Given dimension labels not sufficient\"\n                            \"to address all values uniquely\")\n\n        if len(keys):\n            cdims = {self.get_dimension(d): self.dimension_values(d)[0] for d in reduced_dims}\n        else:\n            cdims = {}\n        with item_check(indices == sorted(indices)):\n            return self.clone(reindexed_items, kdims=dimensions,\n                              cdims=cdims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef info(self):\n        if (len(self.values()) > 0):\n            info_str = self.__class__.__name__ +\\\n                       \" containing %d items of type %s\\n\" % (len(self.keys()),\n                                                              type(self.values()[0]).__name__)\n        else:\n            info_str = self.__class__.__name__ + \" containing no items\\n\"\n        info_str += ('-' * (len(info_str)-1)) + \"\\n\\n\"\n        aliases = {v: k for k, v in self._dim_aliases.items()}\n        for group in self._dim_groups:\n            dimensions = getattr(self, group)\n            if dimensions:\n                group = aliases[group].split('_')[0]\n                info_str += '%s Dimensions: \\n' % group.capitalize()\n            for d in dimensions:\n                dmin, dmax = self.range(d.name)\n                if d.value_format:\n                    dmin, dmax = d.value_format(dmin), d.value_format(dmax)\n                info_str += '\\t %s: %s...%s \\n' % (d.pprint_label, dmin, dmax)\n        print(info_str)", "response": "Prints information about the Dimensioned object including the\nAttributeNames number and type of objects contained within it and information about its dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self, other):\n        if isinstance(other, NdMapping):\n            dims = [d for d in other.kdims if d not in self.kdims]\n            if len(dims) == other.ndims:\n                raise KeyError(\"Cannot update with NdMapping that has\"\n                               \" a different set of key dimensions.\")\n            elif dims:\n                other = other.drop_dimension(dims)\n            other = other.data\n        for key, data in other.items():\n            self._add_item(key, data, sort=False)\n        if self.sort:\n            self._resort()", "response": "Merges other item with this object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the keys of all the elements.", "response": "def keys(self):\n        \" Returns the keys of all the elements.\"\n        if self.ndims == 1:\n            return [k[0] for k in self.data.keys()]\n        else:\n            return list(self.data.keys())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning all elements as a list in ( key value ) format.", "response": "def items(self):\n        \"Returns all elements as a list in (key,value) format.\"\n        return list(zip(list(self.keys()), list(self.values())))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef table(self, datatype=None, **kwargs):\n        if config.future_deprecations:\n            self.param.warning(\"The table method is deprecated and should no \"\n                               \"longer be used. If using a HoloMap use \"\n                               \"HoloMap.collapse() instead to return a Dataset.\")\n\n        from .data.interface import Interface\n        from ..element.tabular import Table\n        new_data = [(key, value.table(datatype=datatype, **kwargs))\n                    for key, value in self.data.items()]\n        tables = self.clone(new_data)\n        return Interface.concatenate(tables, new_type=Table)", "response": "Convert an MultiDimensionalMapping of ArcGIS elements to a Table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a MultiDimensionalMapping to a pandas DataFrame.", "response": "def dframe(self):\n        \"\"\"\n        Deprecated method to convert a MultiDimensionalMapping to\n        a pandas DataFrame. Conversion to a dataframe now only\n        supported by specific subclasses such as UniformNdMapping\n        types.\n        \"\"\"\n        self.param.warning(\"The MultiDimensionalMapping.dframe method is \"\n                           \"deprecated and should no longer be used. \"\n                           \"Use a more specific subclass which does support \"\n                           \"the dframe method instead, e.g. a HoloMap.\")\n        try:\n            import pandas\n        except ImportError:\n            raise Exception(\"Cannot build a DataFrame without the pandas library.\")\n        labels = self.dimensions('key', True) + [self.group]\n        return pandas.DataFrame(\n            [dict(zip(labels, k + (v,))) for (k, v) in self.data.items()])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _expand_slice(self, indices):\n        keys = list(self.data.keys())\n        expanded = []\n        for idx, ind in enumerate(indices):\n            if isinstance(ind, slice) and ind.step is not None:\n                dim_ind = slice(ind.start, ind.stop)\n                if dim_ind == slice(None):\n                    condition = self._all_condition()\n                elif dim_ind.start is None:\n                    condition = self._upto_condition(dim_ind)\n                elif dim_ind.stop is None:\n                    condition = self._from_condition(dim_ind)\n                else:\n                    condition = self._range_condition(dim_ind)\n                dim_vals = unique_iterator(k[idx] for k in keys)\n                expanded.append(set([k for k in dim_vals if condition(k)][::int(ind.step)]))\n            else:\n                expanded.append(ind)\n        return tuple(expanded)", "response": "Expands slices containing steps into a list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_conditions(self, map_slice):\n        conditions = []\n        for dim, dim_slice in zip(self.kdims, map_slice):\n            if isinstance(dim_slice, slice):\n                start, stop = dim_slice.start, dim_slice.stop\n                if dim.values:\n                    values = dim.values\n                    dim_slice = slice(None if start is None else values.index(start),\n                                      None if stop is None else values.index(stop))\n                if dim_slice == slice(None):\n                    conditions.append(self._all_condition())\n                elif start is None:\n                    conditions.append(self._upto_condition(dim_slice))\n                elif stop is None:\n                    conditions.append(self._from_condition(dim_slice))\n                else:\n                    conditions.append(self._range_condition(dim_slice))\n            elif isinstance(dim_slice, (set, list)):\n                if dim.values:\n                    dim_slice = [dim.values.index(dim_val)\n                                 for dim_val in dim_slice]\n                conditions.append(self._values_condition(dim_slice))\n            elif dim_slice is Ellipsis:\n                conditions.append(self._all_condition())\n            elif callable(dim_slice):\n                conditions.append(dim_slice)\n            elif isinstance(dim_slice, (tuple)):\n                raise IndexError(\"Keys may only be selected with sets or lists, not tuples.\")\n            else:\n                if dim.values:\n                    dim_slice = dim.values.index(dim_slice)\n                conditions.append(self._value_condition(dim_slice))\n        return conditions", "response": "Generates the list of conditions used for slicing the data structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclone the object overriding data and parameters.", "response": "def clone(self, data=None, shared_data=True, new_type=None, link=True,\n              *args, **overrides):\n        \"\"\"Clones the object, overriding data and parameters.\n\n        Args:\n            data: New data replacing the existing data\n            shared_data (bool, optional): Whether to use existing data\n            new_type (optional): Type to cast object to\n            link (bool, optional): Whether clone should be linked\n                Determines whether Streams and Links attached to\n                original object will be inherited.\n            *args: Additional arguments to pass to constructor\n            **overrides: New keyword arguments to pass to constructor\n\n        Returns:\n            Cloned object\n        \"\"\"\n        settings = dict(self.get_param_values())\n        if settings.get('group', None) != self._group:\n            settings.pop('group')\n        if settings.get('label', None) != self._label:\n            settings.pop('label')\n        if new_type is None:\n            clone_type = self.__class__\n        else:\n            clone_type = new_type\n            new_params = new_type.params()\n            settings = {k: v for k, v in settings.items()\n                      if k in new_params}\n        settings = dict(settings, **overrides)\n        if 'id' not in settings and new_type in [type(self), None]:\n            settings['id'] = self.id\n\n        if data is None and shared_data:\n            data = self.data\n            if link:\n                settings['plot_id'] = self._plot_id\n        # Apply name mangling for __ attribute\n        pos_args = getattr(self, '_' + type(self).__name__ + '__pos_params', [])\n        with item_check(not shared_data and self._check_items):\n            return clone_type(data, *args, **{k:v for k,v in settings.items()\n                                              if k not in pos_args})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dframe(self, dimensions=None, multi_index=False):\n        import pandas as pd\n        if dimensions is None:\n            outer_dimensions = self.kdims\n            inner_dimensions = None\n        else:\n            outer_dimensions = [self.get_dimension(d) for d in dimensions\n                                if d in self.kdims]\n            inner_dimensions = [d for d in dimensions\n                                if d not in outer_dimensions]\n        inds = [(d, self.get_dimension_index(d)) for d in outer_dimensions]\n\n        dframes = []\n        for key, element in self.data.items():\n            df = element.dframe(inner_dimensions, multi_index)\n            names = [d.name for d in outer_dimensions]\n            key_dims = [(d.name, key[i]) for d, i in inds]\n            if multi_index:\n                length = len(df)\n                indexes = [[v]*length for _, v in key_dims]\n                if df.index.names != [None]:\n                    indexes += [df.index]\n                    names += list(df.index.names)\n                df = df.set_index(indexes)\n                df.index.names = names\n            else:\n                for dim, val in key_dims:\n                    dimn = 1\n                    while dim in df:\n                        dim = dim+'_%d' % dimn\n                        if dim in df:\n                            dimn += 1\n                    df.insert(0, dim, val)\n            dframes.append(df)\n        return pd.concat(dframes)", "response": "Convert dimension values to DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef group(self):\n        \"Group inherited from items\"\n        if self._group:\n            return self._group\n        group =  get_ndmapping_label(self, 'group') if len(self) else None\n        if group is None:\n            return type(self).__name__\n        return group", "response": "Group inherited from items"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef label(self):\n        \"Label inherited from items\"\n        if self._label:\n            return self._label\n        else:\n            if len(self):\n                label = get_ndmapping_label(self, 'label')\n                return '' if label is None else label\n            else:\n                return ''", "response": "Label inherited from items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef type(self):\n        \"The type of elements stored in the mapping.\"\n        if self._type is None and len(self):\n            self._type = self.values()[0].__class__\n        return self._type", "response": "The type of elements stored in the mapping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsnap coordinate to closest coordinate in Dataset", "response": "def closest(self, coords=[], **kwargs):\n        \"\"\"Snaps coordinate(s) to closest coordinate in Dataset\n\n        Args:\n            coords: List of coordinates expressed as tuples\n            **kwargs: Coordinates defined as keyword pairs\n\n        Returns:\n            List of tuples of the snapped coordinates\n\n        Raises:\n            NotImplementedError: Raised if snapping is not supported\n        \"\"\"\n        if self.ndims > 1:\n            raise NotImplementedError(\"Closest method currently only \"\n                                      \"implemented for 1D Elements\")\n\n        if kwargs:\n            if len(kwargs) > 1:\n                raise NotImplementedError(\"Closest method currently only \"\n                                          \"supports 1D indexes\")\n            samples = list(kwargs.values())[0]\n            coords = samples if isinstance(samples, list) else [samples]\n\n        xs = self.dimension_values(0)\n        if xs.dtype.kind in 'SO':\n            raise NotImplementedError(\"Closest only supported for numeric types\")\n        idxs = [np.argmin(np.abs(xs-coord)) for coord in coords]\n        return [xs[idx] for idx in idxs]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort(self, by=None, reverse=False):\n        if by is None:\n            by = self.kdims\n        elif not isinstance(by, list):\n            by = [by]\n        sorted_columns = self.interface.sort(self, by, reverse)\n        return self.clone(sorted_columns)", "response": "Sorts the data by the values along the supplied dimensions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the lower and upper bounds of values along a dimension.", "response": "def range(self, dim, data_range=True, dimension_range=True):\n        \"\"\"Return the lower and upper bounds of values along dimension.\n\n        Args:\n            dimension: The dimension to compute the range on.\n            data_range (bool): Compute range from data values\n            dimension_range (bool): Include Dimension ranges\n                Whether to include Dimension range and soft_range\n                in range calculation\n\n        Returns:\n            Tuple containing the lower and upper bound\n        \"\"\"\n        dim = self.get_dimension(dim)\n\n        if dim is None or (not data_range and not dimension_range):\n            return (None, None)\n        elif all(util.isfinite(v) for v in dim.range) and dimension_range:\n            return dim.range\n        elif dim in self.dimensions() and data_range and bool(self):\n            lower, upper = self.interface.range(self, dim)\n        else:\n            lower, upper = (np.NaN, np.NaN)\n        if not dimension_range:\n            return lower, upper\n        return util.dimension_range(lower, upper, dim.range, dim.soft_range)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_dimension(self, dimension, dim_pos, dim_val, vdim=False, **kwargs):\n        if isinstance(dimension, (util.basestring, tuple)):\n            dimension = Dimension(dimension)\n\n        if dimension.name in self.kdims:\n            raise Exception('{dim} dimension already defined'.format(dim=dimension.name))\n\n        if vdim:\n            dims = self.vdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(vdims=dims)\n            dim_pos += self.ndims\n        else:\n            dims = self.kdims[:]\n            dims.insert(dim_pos, dimension)\n            dimensions = dict(kdims=dims)\n\n        if issubclass(self.interface, ArrayInterface) and np.asarray(dim_val).dtype != self.data.dtype:\n            element = self.clone(datatype=[default_datatype])\n            data = element.interface.add_dimension(element, dimension, dim_pos, dim_val, vdim)\n        else:\n            data = self.interface.add_dimension(self, dimension, dim_pos, dim_val, vdim)\n        return self.clone(data, **dimensions)", "response": "Adds a dimension and its values to the Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, selection_specs=None, **selection):\n        if selection_specs is not None and not isinstance(selection_specs, (list, tuple)):\n            selection_specs = [selection_specs]\n        selection = {dim: sel for dim, sel in selection.items()\n                     if dim in self.dimensions()+['selection_mask']}\n        if (selection_specs and not any(self.matches(sp) for sp in selection_specs)\n            or not selection):\n            return self\n\n        data = self.interface.select(self, **selection)\n\n        if np.isscalar(data):\n            return data\n        else:\n            return self.clone(data)", "response": "Applies a selection to the object using the specified selection_specs."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsamples values at supplied coordinates returning a new object containing the selected samples.", "response": "def sample(self, samples=[], bounds=None, closest=True, **kwargs):\n        \"\"\"Samples values at supplied coordinates.\n\n        Allows sampling of element with a list of coordinates matching\n        the key dimensions, returning a new object containing just the\n        selected samples. Supports multiple signatures:\n\n        Sampling with a list of coordinates, e.g.:\n\n            ds.sample([(0, 0), (0.1, 0.2), ...])\n\n        Sampling a range or grid of coordinates, e.g.:\n\n            1D: ds.sample(3)\n            2D: ds.sample((3, 3))\n\n        Sampling by keyword, e.g.:\n\n            ds.sample(x=0)\n\n        Args:\n            samples: List of nd-coordinates to sample\n            bounds: Bounds of the region to sample\n                Defined as two-tuple for 1D sampling and four-tuple\n                for 2D sampling.\n            closest: Whether to snap to closest coordinates\n            **kwargs: Coordinates specified as keyword pairs\n                Keywords of dimensions and scalar coordinates\n\n        Returns:\n            Element containing the sampled coordinates\n        \"\"\"\n        if kwargs and samples != []:\n            raise Exception('Supply explicit list of samples or kwargs, not both.')\n        elif kwargs:\n            sample = [slice(None) for _ in range(self.ndims)]\n            for dim, val in kwargs.items():\n                sample[self.get_dimension_index(dim)] = val\n            samples = [tuple(sample)]\n        elif isinstance(samples, tuple) or util.isscalar(samples):\n            if self.ndims == 1:\n                xlim = self.range(0)\n                lower, upper = (xlim[0], xlim[1]) if bounds is None else bounds\n                edges = np.linspace(lower, upper, samples+1)\n                linsamples = [(l+u)/2.0 for l,u in zip(edges[:-1], edges[1:])]\n            elif self.ndims == 2:\n                (rows, cols) = samples\n                if bounds:\n                    (l,b,r,t) = bounds\n                else:\n                    l, r = self.range(0)\n                    b, t = self.range(1)\n\n                xedges = np.linspace(l, r, cols+1)\n                yedges = np.linspace(b, t, rows+1)\n                xsamples = [(lx+ux)/2.0 for lx,ux in zip(xedges[:-1], xedges[1:])]\n                ysamples = [(ly+uy)/2.0 for ly,uy in zip(yedges[:-1], yedges[1:])]\n\n                Y,X = np.meshgrid(ysamples, xsamples)\n                linsamples = list(zip(X.flat, Y.flat))\n            else:\n                raise NotImplementedError(\"Regular sampling not implemented \"\n                                          \"for elements with more than two dimensions.\")\n            samples = list(util.unique_iterator(self.closest(linsamples)))\n\n        # Note: Special handling sampling of gridded 2D data as Curve\n        # may be replaced with more general handling\n        # see https://github.com/ioam/holoviews/issues/1173\n        from ...element import Table, Curve\n        if len(samples) == 1:\n            sel = {kd.name: s for kd, s in zip(self.kdims, samples[0])}\n            dims = [kd for kd, v in sel.items() if not np.isscalar(v)]\n            selection = self.select(**sel)\n\n            # If a 1D cross-section of 2D space return Curve\n            if self.interface.gridded and self.ndims == 2 and len(dims) == 1:\n                new_type = Curve\n                kdims = [self.get_dimension(kd) for kd in dims]\n            else:\n                new_type = Table\n                kdims = self.kdims\n\n            if np.isscalar(selection):\n                selection = [samples[0]+(selection,)]\n            else:\n                reindexed = selection.clone(new_type=Dataset).reindex(kdims)\n                selection = tuple(reindexed.columns(kdims+self.vdims).values())\n\n            datatype = list(util.unique_iterator(self.datatype+['dataframe', 'dict']))\n            return self.clone(selection, kdims=kdims, new_type=new_type,\n                              datatype=datatype)\n\n        lens = set(len(util.wrap_tuple(s)) for s in samples)\n        if len(lens) > 1:\n            raise IndexError('Sample coordinates must all be of the same length.')\n\n        if closest:\n            try:\n                samples = self.closest(samples)\n            except NotImplementedError:\n                pass\n        samples = [util.wrap_tuple(s) for s in samples]\n        return self.clone(self.interface.sample(self, samples), new_type=Table)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies reduction along the specified key dimensions and returns a new Dataset with the reduced values.", "response": "def reduce(self, dimensions=[], function=None, spreadfn=None, **reductions):\n        \"\"\"Applies reduction along the specified dimension(s).\n\n        Allows reducing the values along one or more key dimension\n        with the supplied function. Supports two signatures:\n\n        Reducing with a list of dimensions, e.g.:\n\n            ds.reduce(['x'], np.mean)\n\n        Defining a reduction using keywords, e.g.:\n\n            ds.reduce(x=np.mean)\n\n        Args:\n            dimensions: Dimension(s) to apply reduction on\n                Defaults to all key dimensions\n            function: Reduction operation to apply, e.g. numpy.mean\n            spreadfn: Secondary reduction to compute value spread\n                Useful for computing a confidence interval, spread, or\n                standard deviation.\n            **reductions: Keyword argument defining reduction\n                Allows reduction to be defined as keyword pair of\n                dimension and function\n\n        Returns:\n            The Dataset after reductions have been applied.\n        \"\"\"\n        if any(dim in self.vdims for dim in dimensions):\n            raise Exception(\"Reduce cannot be applied to value dimensions\")\n        function, dims = self._reduce_map(dimensions, function, reductions)\n        dims = [d for d in self.kdims if d not in dims]\n        return self.aggregate(dims, function, spreadfn)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef aggregate(self, dimensions=None, function=None, spreadfn=None, **kwargs):\n        if function is None:\n            raise ValueError(\"The aggregate method requires a function to be specified\")\n        if dimensions is None: dimensions = self.kdims\n        elif not isinstance(dimensions, list): dimensions = [dimensions]\n        kdims = [self.get_dimension(d, strict=True) for d in dimensions]\n        if not len(self):\n            if spreadfn:\n                spread_name = spreadfn.__name__\n                vdims = [d for vd in self.vdims for d in [vd, vd('_'.join([vd.name, spread_name]))]]\n            else:\n                vdims = self.vdims\n            return self.clone([], kdims=kdims, vdims=vdims)\n\n        vdims = self.vdims\n        aggregated, dropped = self.interface.aggregate(self, kdims, function, **kwargs)\n        aggregated = self.interface.unpack_scalar(self, aggregated)\n        vdims = [vd for vd in vdims if vd not in dropped]\n\n        ndims = len(dimensions)\n        min_d, max_d = self.params('kdims').bounds\n        generic_type = (min_d is not None and ndims < min_d) or (max_d is not None and ndims > max_d)\n\n        if spreadfn:\n            error, _ = self.interface.aggregate(self, dimensions, spreadfn)\n            spread_name = spreadfn.__name__\n            ndims = len(vdims)\n            error = self.clone(error, kdims=kdims, new_type=Dataset)\n            combined = self.clone(aggregated, kdims=kdims, new_type=Dataset)\n            for i, d in enumerate(vdims):\n                dim = d('_'.join([d.name, spread_name]))\n                dvals = error.dimension_values(d, flat=False)\n                combined = combined.add_dimension(dim, ndims+i, dvals, True)\n            return combined.clone(new_type=Dataset if generic_type else type(self))\n\n        if np.isscalar(aggregated):\n            return aggregated\n        else:\n            try:\n                # Should be checking the dimensions declared on the element are compatible\n                return self.clone(aggregated, kdims=kdims, vdims=vdims)\n            except:\n                datatype = self.params('datatype').default\n                return self.clone(aggregated, kdims=kdims, vdims=vdims,\n                                  new_type=Dataset if generic_type else None,\n                                  datatype=datatype)", "response": "Aggregates data over the supplied key dimensions with the defined function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngroup object by one or more dimensions returning a dictionary containing the groups.", "response": "def groupby(self, dimensions=[], container_type=HoloMap, group_type=None,\n                dynamic=False, **kwargs):\n        \"\"\"Groups object by one or more dimensions\n\n        Applies groupby operation over the specified dimensions\n        returning an object of type container_type (expected to be\n        dictionary-like) containing the groups.\n\n        Args:\n            dimensions: Dimension(s) to group by\n            container_type: Type to cast group container to\n            group_type: Type to cast each group to\n            dynamic: Whether to return a DynamicMap\n            **kwargs: Keyword arguments to pass to each group\n\n        Returns:\n            Returns object of supplied container_type containing the\n            groups. If dynamic=True returns a DynamicMap instead.\n        \"\"\"\n        if not isinstance(dimensions, list): dimensions = [dimensions]\n        if not len(dimensions): dimensions = self.dimensions('key', True)\n        if group_type is None: group_type = type(self)\n\n        dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        dim_names = [d.name for d in dimensions]\n\n        if dynamic:\n            group_dims = [kd for kd in self.kdims if kd not in dimensions]\n            kdims = [self.get_dimension(d) for d in kwargs.pop('kdims', group_dims)]\n            drop_dim = len(group_dims) != len(kdims)\n            group_kwargs = dict(util.get_param_values(self), kdims=kdims)\n            group_kwargs.update(kwargs)\n            def load_subset(*args):\n                constraint = dict(zip(dim_names, args))\n                group = self.select(**constraint)\n                if np.isscalar(group):\n                    return group_type(([group],), group=self.group,\n                                      label=self.label, vdims=self.vdims)\n                data = group.reindex(kdims)\n                if drop_dim and self.interface.gridded:\n                    data = data.columns()\n                return group_type(data, **group_kwargs)\n            dynamic_dims = [d(values=list(self.interface.values(self, d.name, False)))\n                            for d in dimensions]\n            return DynamicMap(load_subset, kdims=dynamic_dims)\n\n        return self.interface.groupby(self, dim_names, container_type,\n                                      group_type, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the values along the requested dimension.", "response": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        dim = self.get_dimension(dimension, strict=True)\n        return self.interface.values(self, dim, expanded, flat)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_dimension_type(self, dim):\n        dim_obj = self.get_dimension(dim)\n        if dim_obj and dim_obj.type is not None:\n            return dim_obj.type\n        return self.interface.dimension_type(self, dim_obj)", "response": "Returns the type of the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dframe(self, dimensions=None, multi_index=False):\n        if dimensions is None:\n            dimensions = [d.name for d in self.dimensions()]\n        else:\n            dimensions = [self.get_dimension(d, strict=True).name for d in dimensions]\n        df = self.interface.dframe(self, dimensions)\n        if multi_index:\n            df = df.set_index([d for d in dimensions if d in self.kdims])\n        return df", "response": "Convert dimension values to DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef columns(self, dimensions=None):\n        if dimensions is None:\n            dimensions = self.dimensions()\n        else:\n            dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        return OrderedDict([(d.name, self.dimension_values(d)) for d in dimensions])", "response": "Convert dimension values to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncloning the object overriding data and parameters.", "response": "def clone(self, data=None, shared_data=True, new_type=None, *args, **overrides):\n        \"\"\"Clones the object, overriding data and parameters.\n\n        Args:\n            data: New data replacing the existing data\n            shared_data (bool, optional): Whether to use existing data\n            new_type (optional): Type to cast object to\n            *args: Additional arguments to pass to constructor\n            **overrides: New keyword arguments to pass to constructor\n\n        Returns:\n            Cloned object\n        \"\"\"\n        if 'datatype' not in overrides:\n            datatypes = [self.interface.datatype] + self.datatype\n            overrides['datatype'] = list(util.unique_iterator(datatypes))\n        return super(Dataset, self).clone(data, shared_data, new_type, *args, **overrides)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _rc_context(rcparams):\n    deprecated = ['text.latex.unicode', 'examples.directory']\n    old_rcparams = {k: mpl.rcParams[k] for k in mpl.rcParams.keys()\n                    if mpl_version < '3.0' or k not in deprecated}\n    mpl.rcParams.clear()\n    mpl.rcParams.update(dict(old_rcparams, **rcparams))\n    try:\n        yield\n    finally:\n        mpl.rcParams.clear()\n        mpl.rcParams.update(old_rcparams)", "response": "Context manager that temporarily overrides the pyplot rcParams. rcParams."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef mpl_rc_context(f):\n    def wrapper(self, *args, **kwargs):\n        with _rc_context(self.fig_rcparams):\n            return f(self, *args, **kwargs)\n    return wrapper", "response": "Decorator for MPLPlot methods applying matplotlib rc params while when method is called."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an axis which may need to be initialized from", "response": "def _init_axis(self, fig, axis):\n        \"\"\"\n        Return an axis which may need to be initialized from\n        a new figure.\n        \"\"\"\n        if not fig and self._create_fig:\n            fig = plt.figure()\n            l, b, r, t = self.fig_bounds\n            inches = self.fig_inches\n            fig.subplots_adjust(left=l, bottom=b, right=r, top=t)\n            fig.patch.set_alpha(self.fig_alpha)\n            if isinstance(inches, (tuple, list)):\n                inches = list(inches)\n                if inches[0] is None:\n                    inches[0] = inches[1]\n                elif inches[1] is None:\n                    inches[1] = inches[0]\n                fig.set_size_inches(list(inches))\n            else:\n                fig.set_size_inches([inches, inches])\n            axis = fig.add_subplot(111, projection=self.projection)\n            axis.set_aspect('auto')\n        return fig, axis"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _finalize_axis(self, key):\n        if 'title' in self.handles:\n            self.handles['title'].set_visible(self.show_title)\n\n        self.drawn = True\n        if self.subplot:\n            return self.handles['axis']\n        else:\n            fig = self.handles['fig']\n            if not getattr(self, 'overlaid', False) and self._close_figures:\n                plt.close(fig)\n            return fig", "response": "Finalize the axis and plot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef anim(self, start=0, stop=None, fps=30):\n        figure = self.state or self.initialize_plot()\n        anim = animation.FuncAnimation(figure, self.update_frame,\n                                       frames=self.keys,\n                                       interval = 1000.0/fps)\n        # Close the figure handle\n        if self._close_figures: plt.close(figure)\n        return anim", "response": "Method to return a matplotlib animation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef initialize_plot(self, ranges=None):\n        for pos in self.view_positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = self.layout.get(pos, None)\n            subplot = self.subplots.get(pos, None)\n            ax = self.subaxes.get(pos, None)\n            # If no view object or empty position, disable the axis\n            if None in [view, pos, subplot]:\n                ax.set_axis_off()\n                continue\n            subplot.initialize_plot(ranges=ranges)\n\n        self.adjust_positions()\n        self.drawn = True", "response": "Initialize all the views contained in the AdjointLayout Object using axes\n            appropriate to the layout configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadjusting the positions of the axes of the current figure to match the layout positions of the main plot.", "response": "def adjust_positions(self, redraw=True):\n        \"\"\"\n        Make adjustments to the positions of subplots (if available)\n        relative to the main plot axes as required.\n\n        This method is called by LayoutPlot after an initial pass\n        used to position all the Layouts together. This method allows\n        LayoutPlots to make final adjustments to the axis positions.\n        \"\"\"\n        checks = [self.view_positions, self.subaxes, self.subplots]\n        right = all('right' in check for check in checks)\n        top = all('top' in check for check in checks)\n        if not 'main' in self.subplots or not (top or right):\n            return\n        if redraw:\n            self.handles['fig'].canvas.draw()\n        main_ax = self.subplots['main'].handles['axis']\n        bbox = main_ax.get_position()\n        if right:\n            ax = self.subaxes['right']\n            subplot = self.subplots['right']\n            if isinstance(subplot, AdjoinedPlot):\n                subplot_size = subplot.subplot_size\n                border_size = subplot.border_size\n            else:\n                subplot_size = 0.25\n                border_size = 0.25\n            ax.set_position([bbox.x1 + bbox.width * border_size,\n                             bbox.y0,\n                             bbox.width * subplot_size, bbox.height])\n            if isinstance(subplot, GridPlot):\n                ax.set_aspect('equal')\n        if top:\n            ax = self.subaxes['top']\n            subplot = self.subplots['top']\n            if isinstance(subplot, AdjoinedPlot):\n                subplot_size = subplot.subplot_size\n                border_size = subplot.border_size\n            else:\n                subplot_size = 0.25\n                border_size = 0.25\n            ax.set_position([bbox.x0,\n                             bbox.y1 + bbox.height * border_size,\n                             bbox.width, bbox.height * subplot_size])\n            if isinstance(subplot, GridPlot):\n                ax.set_aspect('equal')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_gridspec(self, layout):\n        layout_items = layout.grid_items()\n        layout_dimensions = layout.kdims if isinstance(layout, NdLayout) else None\n\n        layouts = {}\n        col_widthratios, row_heightratios = {}, {}\n        for (r, c) in self.coords:\n            # Get view at layout position and wrap in AdjointLayout\n            _, view = layout_items.get((c, r) if self.transpose else (r, c), (None, None))\n            if isinstance(view, NdLayout):\n                raise SkipRendering(\"Cannot render NdLayout nested inside a Layout\")\n            layout_view = view if isinstance(view, AdjointLayout) else AdjointLayout([view])\n            layouts[(r, c)] = layout_view\n\n            # Compute shape of AdjointLayout element\n            layout_lens = {1:'Single', 2:'Dual', 3:'Triple'}\n            layout_type = layout_lens[len(layout_view)]\n\n            # Get aspects\n            main = layout_view.main\n            main = main.last if isinstance(main, HoloMap) else main\n            main_options = self.lookup_options(main, 'plot').options if main else {}\n            if main and not isinstance(main_options.get('aspect', 1), basestring):\n                main_aspect = np.nan if isinstance(main, Empty) else main_options.get('aspect', 1)\n                main_aspect = self.aspect_weight*main_aspect + 1-self.aspect_weight\n            else:\n                main_aspect = np.nan\n\n            if layout_type in ['Dual', 'Triple']:\n                el = layout_view.get('right', None)\n                eltype = type(el)\n                if el and eltype in MPLPlot.sideplots:\n                    plot_type = MPLPlot.sideplots[type(el)]\n                    ratio = 0.6*(plot_type.subplot_size+plot_type.border_size)\n                    width_ratios = [4, 4*ratio]\n                else:\n                    width_ratios = [4, 1]\n            else:\n                width_ratios = [4]\n\n            inv_aspect = 1./main_aspect if main_aspect else np.NaN\n            if layout_type in ['Embedded Dual', 'Triple']:\n                el = layout_view.get('top', None)\n                eltype = type(el)\n                if el and eltype in MPLPlot.sideplots:\n                    plot_type = MPLPlot.sideplots[type(el)]\n                    ratio = 0.6*(plot_type.subplot_size+plot_type.border_size)\n                    height_ratios = [4*ratio, 4]\n                else:\n                    height_ratios = [1, 4]\n            else:\n                height_ratios = [4]\n\n            if not isinstance(main_aspect, (basestring, type(None))):\n                width_ratios = [wratio * main_aspect for wratio in width_ratios]\n                height_ratios = [hratio * inv_aspect for hratio in height_ratios]\n            layout_shape = (len(width_ratios), len(height_ratios))\n\n            # For each row and column record the width and height ratios\n            # of the LayoutPlot with the most horizontal or vertical splits\n            # and largest aspect\n            prev_heights = row_heightratios.get(r, (0, []))\n            if layout_shape[1] > prev_heights[0]:\n                row_heightratios[r] = [layout_shape[1], prev_heights[1]]\n            row_heightratios[r][1].append(height_ratios)\n\n            prev_widths = col_widthratios.get(c, (0, []))\n            if layout_shape[0] > prev_widths[0]:\n                col_widthratios[c] = (layout_shape[0], prev_widths[1])\n            col_widthratios[c][1].append(width_ratios)\n\n\n        col_splits = [v[0] for __, v in sorted(col_widthratios.items())]\n        row_splits = [v[0] for ___, v in sorted(row_heightratios.items())]\n\n        widths = np.array([r for col in col_widthratios.values()\n                           for ratios in col[1] for r in ratios])/4\n\n        wr_unnormalized = compute_ratios(col_widthratios, False)\n        hr_list = compute_ratios(row_heightratios)\n        wr_list = compute_ratios(col_widthratios)\n\n        # Compute the number of rows and cols\n        cols, rows = len(wr_list), len(hr_list)\n\n\n        wr_list = [r if np.isfinite(r) else 1 for r in wr_list]\n        hr_list = [r if np.isfinite(r) else 1 for r in hr_list]\n\n        width = sum([r if np.isfinite(r) else 1 for r in wr_list])\n        yscale = width/sum([(1/v)*4 if np.isfinite(v) else 4 for v in wr_unnormalized])\n        if self.absolute_scaling:\n            width = width*np.nanmax(widths)\n\n        xinches, yinches = None, None\n        if not isinstance(self.fig_inches, (tuple, list)):\n            xinches = self.fig_inches * width\n            yinches = xinches/yscale\n        elif self.fig_inches[0] is None:\n            xinches = self.fig_inches[1] * yscale\n            yinches = self.fig_inches[1]\n        elif self.fig_inches[1] is None:\n            xinches = self.fig_inches[0]\n            yinches = self.fig_inches[0] / yscale\n        if xinches and yinches:\n            self.handles['fig'].set_size_inches([xinches, yinches])\n\n        self.gs = gridspec.GridSpec(rows, cols,\n                                    width_ratios=wr_list,\n                                    height_ratios=hr_list,\n                                    wspace=self.hspace,\n                                    hspace=self.vspace)\n\n        # Situate all the Layouts in the grid and compute the gridspec\n        # indices for all the axes required by each LayoutPlot.\n        gidx = 0\n        layout_count = 0\n        tight = self.tight\n        collapsed_layout = layout.clone(shared_data=False, id=layout.id)\n        frame_ranges = self.compute_ranges(layout, None, None)\n        keys = self.keys[:1] if self.dynamic else self.keys\n        frame_ranges = OrderedDict([(key, self.compute_ranges(layout, key, frame_ranges))\n                                    for key in keys])\n        layout_subplots, layout_axes = {}, {}\n        for r, c in self.coords:\n            # Compute the layout type from shape\n            wsplits = col_splits[c]\n            hsplits = row_splits[r]\n            if (wsplits, hsplits) == (1,1):\n                layout_type = 'Single'\n            elif (wsplits, hsplits) == (2,1):\n                layout_type = 'Dual'\n            elif (wsplits, hsplits) == (1,2):\n                layout_type = 'Embedded Dual'\n            elif (wsplits, hsplits) == (2,2):\n                layout_type = 'Triple'\n\n            # Get the AdjoinLayout at the specified coordinate\n            view = layouts[(r, c)]\n            positions = AdjointLayoutPlot.layout_dict[layout_type]\n\n            # Create temporary subplots to get projections types\n            # to create the correct subaxes for all plots in the layout\n            _, _, projs = self._create_subplots(layouts[(r, c)], positions,\n                                                None, frame_ranges, create=False)\n            gidx, gsinds = self.grid_situate(gidx, layout_type, cols)\n\n            layout_key, _ = layout_items.get((r, c), (None, None))\n            if isinstance(layout, NdLayout) and layout_key:\n                layout_dimensions = OrderedDict(zip(layout_dimensions, layout_key))\n\n            # Generate the axes and create the subplots with the appropriate\n            # axis objects, handling any Empty objects.\n            obj = layouts[(r, c)]\n            empty = isinstance(obj.main, Empty)\n            if view.main is None:\n                continue\n            elif empty:\n                obj = AdjointLayout([])\n            elif not view.traverse(lambda x: x, [Element]):\n                self.param.warning('%s is empty, skipping subplot.' % obj.main)\n                continue\n            elif self.transpose:\n                layout_count = (c*self.rows+(r+1))\n            else:\n                layout_count += 1\n            subaxes = [plt.subplot(self.gs[ind], projection=proj)\n                       for ind, proj in zip(gsinds, projs)]\n            subplot_data = self._create_subplots(obj, positions,\n                                                 layout_dimensions, frame_ranges,\n                                                 dict(zip(positions, subaxes)),\n                                                 num=0 if empty else layout_count)\n            subplots, adjoint_layout, _ = subplot_data\n            layout_axes[(r, c)] = subaxes\n\n            # Generate the AdjointLayoutsPlot which will coordinate\n            # plotting of AdjointLayouts in the larger grid\n            plotopts = self.lookup_options(view, 'plot').options\n            layout_plot = AdjointLayoutPlot(adjoint_layout, layout_type, subaxes, subplots,\n                                            fig=self.handles['fig'], **plotopts)\n            layout_subplots[(r, c)] = layout_plot\n            tight = not any(type(p) is GridPlot for p in layout_plot.subplots.values()) and tight\n            if layout_key:\n                collapsed_layout[layout_key] = adjoint_layout\n\n        # Apply tight layout if enabled and incompatible\n        # GridPlot isn't present.\n        if tight:\n            if isinstance(self.tight_padding, (tuple, list)):\n                wpad, hpad = self.tight_padding\n                padding = dict(w_pad=wpad, h_pad=hpad)\n            else:\n                padding = dict(w_pad=self.tight_padding, h_pad=self.tight_padding)\n            self.gs.tight_layout(self.handles['fig'], rect=self.fig_bounds, **padding)\n\n        return layout_subplots, layout_axes, collapsed_layout", "response": "Compute the gridspec for each cell and each cell in the GridSpace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsituating the current AdjointLayoutPlot in a LayoutPlot. The LayoutPlot specifies a layout_type into which the AdjointLayoutPlot must be embedded. This enclosing layout is guaranteed to have enough cells to display all the views. Based on this enforced layout format, a starting index supplied by LayoutPlot (indexing into a large gridspec arrangement) is updated to the appropriate embedded value. It will also return a list of gridspec indices associated with the all the required layout axes.", "response": "def grid_situate(self, current_idx, layout_type, subgrid_width):\n        \"\"\"\n        Situate the current AdjointLayoutPlot in a LayoutPlot. The\n        LayoutPlot specifies a layout_type into which the AdjointLayoutPlot\n        must be embedded. This enclosing layout is guaranteed to have\n        enough cells to display all the views.\n\n        Based on this enforced layout format, a starting index\n        supplied by LayoutPlot (indexing into a large gridspec\n        arrangement) is updated to the appropriate embedded value. It\n        will also return a list of gridspec indices associated with\n        the all the required layout axes.\n        \"\"\"\n        # Set the layout configuration as situated in a NdLayout\n\n        if layout_type == 'Single':\n            start, inds = current_idx+1, [current_idx]\n        elif layout_type == 'Dual':\n            start, inds = current_idx+2, [current_idx, current_idx+1]\n\n        bottom_idx = current_idx + subgrid_width\n        if layout_type == 'Embedded Dual':\n            bottom = ((current_idx+1) % subgrid_width) == 0\n            grid_idx = (bottom_idx if bottom else current_idx)+1\n            start, inds = grid_idx, [current_idx, bottom_idx]\n        elif layout_type == 'Triple':\n            bottom = ((current_idx+2) % subgrid_width) == 0\n            grid_idx = (bottom_idx if bottom else current_idx) + 2\n            start, inds = grid_idx, [current_idx, current_idx+1,\n                              bottom_idx, bottom_idx+1]\n\n        return start, inds"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a list of subplots for the AdjointLayout object.", "response": "def _create_subplots(self, layout, positions, layout_dimensions, ranges, axes={}, num=1, create=True):\n        \"\"\"\n        Plot all the views contained in the AdjointLayout Object using axes\n        appropriate to the layout configuration. All the axes are\n        supplied by LayoutPlot - the purpose of the call is to\n        invoke subplots with correct options and styles and hide any\n        empty axes as necessary.\n        \"\"\"\n        subplots = {}\n        projections = []\n        adjoint_clone = layout.clone(shared_data=False, id=layout.id)\n        subplot_opts = dict(show_title=False, adjoined=layout)\n        for pos in positions:\n            # Pos will be one of 'main', 'top' or 'right' or None\n            view = layout.get(pos, None)\n            if not displayable(view):\n                view = collate(view)\n            ax = axes.get(pos, None)\n            if view is None or not view.traverse(lambda x: x, [Element]):\n                projections.append(None)\n                continue\n\n            # Determine projection type for plot\n            projections.append(self._get_projection(view))\n\n            if not create:\n                continue\n\n            # Customize plotopts depending on position.\n            plotopts = self.lookup_options(view, 'plot').options\n\n            # Options common for any subplot\n            override_opts = {}\n            sublabel_opts = {}\n            if pos == 'main':\n                own_params = self.get_param_values(onlychanged=True)\n                sublabel_opts = {k: v for k, v in own_params\n                                 if 'sublabel_' in k}\n            elif pos == 'right':\n                right_opts = dict(invert_axes=True,\n                                  xaxis=None)\n                override_opts = dict(subplot_opts, **right_opts)\n            elif pos == 'top':\n                top_opts = dict(yaxis=None)\n                override_opts = dict(subplot_opts, **top_opts)\n\n            # Override the plotopts as required\n            plotopts = dict(sublabel_opts, **plotopts)\n            plotopts.update(override_opts, fig=self.handles['fig'])\n            vtype = view.type if isinstance(view, HoloMap) else view.__class__\n            if isinstance(view, GridSpace):\n                plotopts['create_axes'] = ax is not None\n            plot_type = Store.registry['matplotlib'][vtype]\n            if pos != 'main' and vtype in MPLPlot.sideplots:\n                plot_type = MPLPlot.sideplots[vtype]\n            num = num if len(self.coords) > 1 else 0\n            subplots[pos] = plot_type(view, axis=ax, keys=self.keys,\n                                      dimensions=self.dimensions,\n                                      layout_dimensions=layout_dimensions,\n                                      ranges=ranges, subplot=True,\n                                      uniform=self.uniform, layout_num=num,\n                                      renderer=self.renderer, **plotopts)\n            if isinstance(view, (Element, HoloMap, Collator, CompositeOverlay)):\n                adjoint_clone[pos] = subplots[pos].hmap\n            else:\n                adjoint_clone[pos] = subplots[pos].layout\n        return subplots, adjoint_clone, projections"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef triggering_streams(streams):\n    for stream in streams:\n        stream._triggering = True\n    try:\n        yield\n    except:\n        raise\n    finally:\n        for stream in streams:\n            stream._triggering = False", "response": "Temporarily declares the streams as being in a triggered state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef trigger(cls, streams):\n        # Union of stream contents\n        items = [stream.contents.items() for stream in set(streams)]\n        union = [kv for kvs in items for kv in kvs]\n        klist = [k for k, _ in union]\n        key_clashes = set([k for k in klist if klist.count(k) > 1])\n        if key_clashes:\n            clashes = []\n            dicts = [dict(kvs) for kvs in items]\n            for clash in key_clashes:\n                values = set(d[clash] for d in dicts if clash in d)\n                if len(values) > 1:\n                    clashes.append((clash, values))\n            if clashes:\n                msg = ', '.join(['%r has values %r' % (k, v) for k, v in clashes])\n                print('Parameter value clashes where %s' % msg)\n\n        # Group subscribers by precedence while keeping the ordering\n        # within each group\n        subscriber_precedence = defaultdict(list)\n        for stream in streams:\n            stream._on_trigger()\n            for precedence, subscriber in stream._subscribers:\n                subscriber_precedence[precedence].append(subscriber)\n        sorted_subscribers = sorted(subscriber_precedence.items(), key=lambda x: x[0])\n        subscribers = util.unique_iterator([s for _, subscribers in sorted_subscribers\n                                            for s in subscribers])\n\n        with triggering_streams(streams):\n            for subscriber in subscribers:\n                subscriber(**dict(union))\n\n        for stream in streams:\n            with util.disable_constant(stream):\n                if stream.transient:\n                    stream.reset()", "response": "Trigger all the parameters of a list of streams."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing a list of streams promoting Parameterized objects and ParamMethod objects to Param based streams.", "response": "def _process_streams(cls, streams):\n        \"\"\"\n        Processes a list of streams promoting Parameterized objects and\n        methods to Param based streams.\n        \"\"\"\n        parameterizeds = defaultdict(set)\n        valid, invalid = [], []\n        for s in streams:\n            if isinstance(s, Stream):\n                pass\n            elif isinstance(s, param.Parameter):\n                s = Params(s.owner, [s.name])\n            elif isinstance(s, param.Parameterized):\n                s = Params(s)\n            elif util.is_param_method(s):\n                if not hasattr(s, \"_dinfo\"):\n                    continue\n                s = ParamMethod(s)\n            else:\n                invalid.append(s)\n                continue\n            if isinstance(s, Params):\n                pid = id(s.parameterized)\n                overlap = (set(s.parameters) & parameterizeds[pid])\n                if overlap:\n                    pname = type(s.parameterized).__name__\n                    param.main.param.warning(\n                        'The %s parameter(s) on the %s object have '\n                        'already been supplied in another stream. '\n                        'Ensure that the supplied streams only specify '\n                        'each parameter once, otherwise multiple '\n                        'events will be triggered when the parameter '\n                        'changes.' % (sorted(overlap), pname))\n                parameterizeds[pid] |= set(s.parameters)\n            valid.append(s)\n        return valid, invalid"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subscribers(self):\n        return [s for p, s in sorted(self._subscribers, key=lambda x: x[0])]", "response": "Property returning the subscriber list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clear(self, policy='all'):\n        policies = ['all', 'user', 'internal']\n        if policy not in policies:\n            raise ValueError('Policy for clearing subscribers must be one of %s' % policies)\n        if policy == 'all':\n            remaining = []\n        elif policy == 'user':\n            remaining = [(p, s) for (p, s) in self._subscribers if p > 1]\n        else:\n            remaining = [(p, s) for (p, s) in self._subscribers if p <= 1]\n        self._subscribers = remaining", "response": "Clear all subscribers registered to this stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreset the internal state of the internal state of the internal state of the internal state of the internal state.", "response": "def reset(self):\n        \"\"\"\n        Resets stream parameters to their defaults.\n        \"\"\"\n        with util.disable_constant(self):\n            for k, p in self.params().items():\n                if k != 'name':\n                    setattr(self, k, p.default)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd a callable subscriber to this stream.", "response": "def add_subscriber(self, subscriber, precedence=0):\n        \"\"\"\n        Register a callable subscriber to this stream which will be\n        invoked either when event is called or when this stream is\n        passed to the trigger classmethod.\n\n        Precedence allows the subscriber ordering to be\n        controlled. Users should only add subscribers with precedence\n        between zero and one while HoloViews itself reserves the use of\n        higher precedence values. Subscribers with high precedence are\n        invoked later than ones with low precedence.\n        \"\"\"\n        if not callable(subscriber):\n            raise TypeError('Subscriber must be a callable.')\n        self._subscribers.append((precedence, subscriber))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new stream instance with the specified name mapping.", "response": "def rename(self, **mapping):\n        \"\"\"\n        The rename method allows stream parameters to be allocated to\n        new names to avoid clashes with other stream parameters of the\n        same name. Returns a new clone of the stream instance with the\n        specified name mapping.\n        \"\"\"\n        params = {k: v for k, v in self.get_param_values() if k != 'name'}\n        return self.__class__(rename=mapping,\n                              source=(self._source() if self._source else None),\n                              linked=self.linked, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_stream_parameters(self, **kwargs):\n        with util.disable_constant(self):\n            self.param.set_param(**kwargs)", "response": "Sets the stream parameters which are expected to be declared\n            constant."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, **kwargs):\n        self._set_stream_parameters(**kwargs)\n        transformed = self.transform()\n        if transformed:\n            self._set_stream_parameters(**transformed)", "response": "This method updates the stream parameters with the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify(self, x):\n        if type(x) != type(self.data):\n            raise TypeError(\"Input expected to be of type %s, got %s.\" %\n                            (type(self.data).__name__, type(x).__name__))\n        elif isinstance(x, np.ndarray):\n            if x.ndim != 2:\n                raise ValueError('Streamed array data must be two-dimensional')\n            elif x.shape[1] != self.data.shape[1]:\n                raise ValueError(\"Streamed array data expeced to have %d columns, \"\n                                 \"got %d.\" % (self.data.shape[1], x.shape[1]))\n        elif util.pd and isinstance(x, util.pd.DataFrame) and list(x.columns) != list(self.data.columns):\n            raise IndexError(\"Input expected to have columns %s, got %s\" %\n                             (list(self.data.columns), list(x.columns)))\n        elif isinstance(x, dict):\n            if any(c not in x for c in self.data):\n                raise IndexError(\"Input expected to have columns %s, got %s\" %\n                                 (sorted(self.data.keys()), sorted(x.keys())))\n            elif len(set(len(v) for v in x.values())) > 1:\n                raise ValueError(\"Input columns expected to have the \"\n                                 \"same number of rows.\")", "response": "Verify consistency of dataframes that pass through this stream."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clear(self):\n        \"Clears the data in the stream\"\n        if isinstance(self.data, np.ndarray):\n            data = self.data[:, :0]\n        elif util.pd and isinstance(self.data, util.pd.DataFrame):\n            data = self.data.iloc[:0]\n        elif isinstance(self.data, dict):\n            data = {k: v[:0] for k, v in self.data.items()}\n        with util.disable_constant(self):\n            self.data = data\n        self.send(data)", "response": "Clears the data in the stream"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconcatenate and slice the accepted data types to the defined length.", "response": "def _concat(self, data):\n        \"\"\"\n        Concatenate and slice the accepted data types to the defined\n        length.\n        \"\"\"\n        if isinstance(data, np.ndarray):\n            data_length = len(data)\n            if data_length < self.length:\n                prev_chunk = self.data[-(self.length-data_length):]\n                data = np.concatenate([prev_chunk, data])\n            elif data_length > self.length:\n                data = data[-self.length:]\n        elif util.pd and isinstance(data, util.pd.DataFrame):\n            data_length = len(data)\n            if data_length < self.length:\n                prev_chunk = self.data.iloc[-(self.length-data_length):]\n                data = util.pd.concat([prev_chunk, data])\n            elif data_length > self.length:\n                data = data.iloc[-self.length:]\n        elif isinstance(data, dict) and data:\n            data_length = len(list(data.values())[0])\n            new_data = {}\n            for k, v in data.items():\n                if data_length < self.length:\n                    prev_chunk = self.data[k][-(self.length-data_length):]\n                    new_data[k] = np.concatenate([prev_chunk, v])\n                elif data_length > self.length:\n                    new_data[k] = v[-self.length:]\n                else:\n                    new_data[k] = v\n            data = new_data\n        self._chunk_length = data_length\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n        data = kwargs.get('data')\n        if data is not None:\n            if (util.pd and isinstance(data, util.pd.DataFrame) and\n                list(data.columns) != list(self.data.columns) and self._index):\n                data = data.reset_index()\n            self.verify(data)\n            kwargs['data'] = self._concat(data)\n            self._count += 1\n        super(Buffer, self).update(**kwargs)", "response": "Overwrites update to concatenate streamed data up to defined length."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of Params streams given a dictionary of parameters AttributeNames", "response": "def from_params(cls, params):\n        \"\"\"Returns Params streams given a dictionary of parameters\n\n        Args:\n            params (dict): Dictionary of parameters\n\n        Returns:\n            List of Params streams\n        \"\"\"\n        key_fn = lambda x: id(x[1].owner)\n        streams = []\n        for _, group in groupby(sorted(params.items(), key=key_fn), key_fn):\n            group = list(group)\n            inst = [p.owner for _, p in group][0]\n            if not isinstance(inst, param.Parameterized):\n                continue\n            names = [p.name for _, p in group]\n            rename = {p.name: n for n, p in group}\n            streams.append(cls(inst, names, rename=rename))\n        return streams"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef select(self, selection_specs=None, **kwargs):\n        return super(Element2D, self).select(selection_specs, **kwargs)", "response": "Selects the elements in the current region based on selection_specs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clone(self, *args, **overrides):\n        link = overrides.pop('link', True)\n        settings = dict(self.get_param_values(), **overrides)\n        if 'id' not in settings:\n            settings['id'] = self.id\n        if not args and link:\n            settings['plot_id'] = self._plot_id\n\n        pos_args = getattr(self, '_' + type(self).__name__ + '__pos_params', [])\n        return self.__class__(*(settings[n] for n in pos_args),\n                              **{k:v for k,v in settings.items()\n                                 if k not in pos_args})", "response": "Returns a clone of the object with matching parameter values\n        containing the specified args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert_timestamp(timestamp):\n    datetime = dt.datetime.utcfromtimestamp(timestamp/1000.)\n    return np.datetime64(datetime.replace(tzinfo=None))", "response": "Converts a bokehJS timestamp to a datetime64."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode_bytes(array):\n    if (sys.version_info.major == 2 or not len(array) or\n        (isinstance(array, np.ndarray) and array.dtype.kind != 'O')):\n        return array\n    decoded = [v.decode('utf-8') if isinstance(v, bytes) else v for v in array]\n    if isinstance(array, np.ndarray):\n        return np.asarray(decoded)\n    elif isinstance(array, tuple):\n        return tuple(decoded)\n    return decoded", "response": "Decodes an array list or tuple of bytestrings to avoid python 3\n    bokeh serialization errors\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of lists of lists of empty plots with empty plots.", "response": "def layout_padding(plots, renderer):\n    \"\"\"\n    Pads Nones in a list of lists of plots with empty plots.\n    \"\"\"\n    widths, heights = defaultdict(int), defaultdict(int)\n    for r, row in enumerate(plots):\n        for c, p in enumerate(row):\n            if p is not None:\n                width, height = renderer.get_size(p)\n                widths[c] = max(widths[c], width)\n                heights[r] = max(heights[r], height)\n\n    expanded_plots = []\n    for r, row in enumerate(plots):\n        expanded_plots.append([])\n        for c, p in enumerate(row):\n            if p is None:\n                p = empty_plot(widths[c], heights[r])\n            elif hasattr(p, 'plot_width') and p.plot_width == 0 and p.plot_height == 0:\n                p.plot_width = widths[c]\n                p.plot_height = heights[r]\n            expanded_plots[r].append(p)\n    return expanded_plots"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the size of a bokeh plot.", "response": "def compute_plot_size(plot):\n    \"\"\"\n    Computes the size of bokeh models that make up a layout such as\n    figures, rows, columns, widgetboxes and Plot.\n    \"\"\"\n    if isinstance(plot, GridBox):\n        ndmapping = NdMapping({(x, y): fig for fig, y, x in plot.children}, kdims=['x', 'y'])\n        cols = ndmapping.groupby('x')\n        rows = ndmapping.groupby('y')\n        width = sum([max([compute_plot_size(f)[0] for f in col]) for col in cols])\n        height = sum([max([compute_plot_size(f)[1] for f in row]) for row in rows])\n        return width, height\n    elif isinstance(plot, (Div, ToolbarBox)):\n        # Cannot compute size for Div or ToolbarBox\n        return 0, 0\n    elif isinstance(plot, (Row, Column, WidgetBox, Tabs)):\n        if not plot.children: return 0, 0\n        if isinstance(plot, Row) or (isinstance(plot, ToolbarBox) and plot.toolbar_location not in ['right', 'left']):\n            w_agg, h_agg = (np.sum, np.max)\n        elif isinstance(plot, Tabs):\n            w_agg, h_agg = (np.max, np.max)\n        else:\n            w_agg, h_agg = (np.max, np.sum)\n        widths, heights = zip(*[compute_plot_size(child) for child in plot.children])\n        return w_agg(widths), h_agg(heights)\n    elif isinstance(plot, (Figure, Chart)):\n        if plot.plot_width:\n            width = plot.plot_width\n        else:\n            width = plot.frame_width + plot.min_border_right + plot.min_border_left\n        if plot.plot_height:\n            height = plot.plot_height\n        else:\n            height = plot.frame_height + plot.min_border_bottom + plot.min_border_top\n        return width, height\n    elif isinstance(plot, (Plot, DataTable, Spacer)):\n        return plot.width, plot.height\n    else:\n        return 0, 0"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a fontsize to a pixel value", "response": "def font_size_to_pixels(size):\n    \"\"\"\n    Convert a fontsize to a pixel value\n    \"\"\"\n    if size is None or not isinstance(size, basestring):\n        return\n    conversions = {'em': 16, 'pt': 16/12.}\n    val = re.findall('\\d+', size)\n    unit = re.findall('[a-z]+', size)\n    if (val and not unit) or (val and unit[0] == 'px'):\n        return int(val[0])\n    elif val and unit[0] in conversions:\n        return (int(int(val[0]) * conversions[unit[0]]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef hsv_to_rgb(hsv):\n    h, s, v = (hsv[..., i] for i in range(3))\n    shape = h.shape\n    i = np.int_(h*6.)\n    f = h*6.-i\n\n    q = f\n    t = 1.-f\n    i = np.ravel(i)\n    f = np.ravel(f)\n    i%=6\n\n    t = np.ravel(t)\n    q = np.ravel(q)\n    s = np.ravel(s)\n    v = np.ravel(v)\n\n    clist = (1-s*np.vstack([np.zeros_like(f),np.ones_like(f),q,t]))*v\n\n    #0:v 1:p 2:q 3:t\n    order = np.array([[0,3,1],[2,0,1],[1,0,3],[1,2,0],[3,1,0],[0,1,2]])\n    rgb = clist[order[i], np.arange(np.prod(shape))[:,None]]\n\n    return rgb.reshape(shape+(3,))", "response": "Vectorized HSV to RGB conversion adapted from :\n    http://stackoverflow. com/questions / 24852345 / hsv - to - rgb - color - conversion"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the width of a model and sets up appropriate padding for Tabs and DataTable types.", "response": "def pad_width(model, table_padding=0.85, tabs_padding=1.2):\n    \"\"\"\n    Computes the width of a model and sets up appropriate padding\n    for Tabs and DataTable types.\n    \"\"\"\n    if isinstance(model, Row):\n        vals = [pad_width(child) for child in model.children]\n        width = np.max([v for v in vals if v is not None])\n    elif isinstance(model, Column):\n        vals = [pad_width(child) for child in model.children]\n        width = np.sum([v for v in vals if v is not None])\n    elif isinstance(model, Tabs):\n        vals = [pad_width(t) for t in model.tabs]\n        width = np.max([v for v in vals if v is not None])\n        for model in model.tabs:\n            model.width = width\n            width = int(tabs_padding*width)\n    elif isinstance(model, DataTable):\n        width = model.width\n        model.width = int(table_padding*width)\n    elif isinstance(model, (WidgetBox, Div)):\n        width = model.width\n    elif model:\n        width = model.plot_width\n    else:\n        width = 0\n    return width"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pad_plots(plots):\n    widths = []\n    for row in plots:\n        row_widths = []\n        for p in row:\n            width = pad_width(p)\n            row_widths.append(width)\n        widths.append(row_widths)\n    plots = [[WidgetBox(p, width=w) if isinstance(p, (DataTable, Tabs)) else p\n              for p, w in zip(row, ws)] for row, ws in zip(plots, widths)]\n    return plots", "response": "Takes a list of bokeh plots and returns a list of bokeh plots with appropriate padding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef filter_toolboxes(plots):\n    if isinstance(plots, list):\n        plots = [filter_toolboxes(plot) for plot in plots]\n    elif hasattr(plots, 'children'):\n        plots.children = [filter_toolboxes(child) for child in plots.children\n                          if not isinstance(child, ToolbarBox)]\n    return plots", "response": "Filters out toolboxes out of a list of plots to be able to compose\n    them into a larger plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef py2js_tickformatter(formatter, msg=''):\n    try:\n        from flexx.pyscript import py2js\n    except ImportError:\n        param.main.param.warning(\n            msg+'Ensure Flexx is installed (\"conda install -c bokeh flexx\" '\n            'or \"pip install flexx\")')\n        return\n    try:\n        jscode = py2js(formatter, 'formatter')\n    except Exception as e:\n        error = 'Pyscript raised an error: {0}'.format(e)\n        error = error.replace('%', '%%')\n        param.main.param.warning(msg+error)\n        return\n\n    args = _getargspec(formatter).args\n    arg_define = 'var %s = tick;' % args[0] if args else ''\n    return_js = 'return formatter();\\n'\n    jsfunc = '\\n'.join([arg_define, jscode, return_js])\n    match = re.search('(formatter \\= function \\(.*\\))', jsfunc )\n    return jsfunc[:match.start()] + 'formatter = function ()' + jsfunc[match.end():]", "response": "Returns a sequence of JavaScript code that can be used to compile a python tick formatter to JavaScript code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_tab_title(key, frame, overlay):\n    if isinstance(overlay, Overlay):\n        if frame is not None:\n            title = []\n            if frame.label:\n                title.append(frame.label)\n                if frame.group != frame.params('group').default:\n                    title.append(frame.group)\n            else:\n                title.append(frame.group)\n        else:\n            title = key\n        title = ' '.join(title)\n    else:\n        title = ' | '.join([d.pprint_value_string(k) for d, k in\n                            zip(overlay.kdims, key)])\n    return title", "response": "Computes a title for bokeh tabs from the key in the overlay and the frame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering the data and mapping for a ColumnDataSource and replaces any columns with repeating values with scalar types.", "response": "def filter_batched_data(data, mapping):\n    \"\"\"\n    Iterates over the data and mapping for a ColumnDataSource and\n    replaces columns with repeating values with a scalar. This is\n    purely and optimization for scalar types.\n    \"\"\"\n    for k, v in list(mapping.items()):\n        if isinstance(v, dict) and 'field' in v:\n            if 'transform' in v:\n                continue\n            v = v['field']\n        elif not isinstance(v, basestring):\n            continue\n        values = data[v]\n        try:\n            if len(unique_array(values)) == 1:\n                mapping[k] = values[0]\n                del data[v]\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cds_column_replace(source, data):\n    current_length = [len(v) for v in source.data.values() if isinstance(v, (list, np.ndarray))]\n    new_length = [len(v) for v in data.values() if isinstance(v, (list, np.ndarray))]\n    untouched = [k for k in source.data if k not in data]\n    return bool(untouched and current_length and new_length and current_length[0] != new_length[0])", "response": "Determines if the CDS. data requires a full replacement or simply\n    needs to be updated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recursive_model_update(model, props):\n    updates = {}\n    valid_properties = model.properties_with_values()\n    for k, v in props.items():\n        if isinstance(v, Model):\n            nested_model = getattr(model, k)\n            if type(v) is type(nested_model):\n                nested_props = v.properties_with_values(include_defaults=False)\n                recursive_model_update(nested_model, nested_props)\n            else:\n                setattr(model, k, v)\n        elif k in valid_properties and v != valid_properties[k]:\n            updates[k] = v\n    model.update(**updates)", "response": "Recursively updates the model with the properties of the given properties."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_shared_sources(f):\n    def wrapper(self, *args, **kwargs):\n        source_cols = self.handles.get('source_cols', {})\n        shared_sources = self.handles.get('shared_sources', [])\n        for source in shared_sources:\n            source.data.clear()\n            if self.document and self.document._held_events:\n                self.document._held_events = self.document._held_events[:-1]\n\n        ret = f(self, *args, **kwargs)\n\n        for source in shared_sources:\n            expected = source_cols[id(source)]\n            found = [c for c in expected if c in source.data]\n            empty = np.full_like(source.data[found[0]], np.NaN) if found else []\n            patch = {c: empty for c in expected if c not in source.data}\n            source.data.update(patch)\n        return ret\n    return wrapper", "response": "Decorator to update the data sources shared between multiple plots."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef categorize_array(array, dim):\n    return np.array([dim.pprint_value(x) for x in array])", "response": "Converts an array of values to categorical alphabetical order."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattaching periodic to all streams on the object.", "response": "def attach_periodic(plot):\n    \"\"\"\n    Attaches plot refresh to all streams on the object.\n    \"\"\"\n    def append_refresh(dmap):\n        for dmap in get_nested_dmaps(dmap):\n            dmap.periodic._periodic_util = periodic(plot.document)\n    return plot.hmap.traverse(append_refresh, [DynamicMap])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a date object to an integer", "response": "def date_to_integer(date):\n    \"\"\"Converts support date types to milliseconds since epoch\n\n    Attempts highest precision conversion of different datetime\n    formats to milliseconds since the epoch (1970-01-01 00:00:00).\n    If datetime is a cftime with a non-standard calendar the\n    caveats described in hv.core.util.cftime_to_timestamp apply.\n\n    Args:\n        date: Date- or datetime-like object\n\n    Returns:\n        Milliseconds since 1970-01-01 00:00:00\n    \"\"\"\n    if pd and isinstance(date, pd.Timestamp):\n        try:\n            date = date.to_datetime64()\n        except:\n            date = date.to_datetime()\n\n    if isinstance(date, np.datetime64):\n        return date.astype('datetime64[ms]').astype(float)\n    elif isinstance(date, cftime_types):\n        return cftime_to_timestamp(date, 'ms')\n\n    if hasattr(date, 'timetuple'):\n        dt_int = calendar.timegm(date.timetuple())*1000\n    else:\n        raise ValueError('Datetime type not recognized')\n    return dt_int"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a set of glyph handles in a random order.", "response": "def glyph_order(keys, draw_order=[]):\n    \"\"\"\n    Orders a set of glyph handles using regular sort and an explicit\n    sort order. The explicit draw order must take the form of a list\n    of glyph names while the keys should be glyph names with a custom\n    suffix. The draw order may only match subset of the keys and any\n    matched items will take precedence over other entries.\n    \"\"\"\n    keys = sorted(keys)\n    def order_fn(glyph):\n        matches = [item for item in draw_order if glyph.startswith(item)]\n        return ((draw_order.index(matches[0]), glyph) if matches else\n                (1e9+keys.index(glyph), glyph))\n    return sorted(keys, key=order_fn)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates line paths for a quadmesh given 2D arrays of X and Y.", "response": "def colormesh(X, Y):\n    \"\"\"\n    Generates line paths for a quadmesh given 2D arrays of X and Y\n    coordinates.\n    \"\"\"\n    X1 = X[0:-1, 0:-1].ravel()\n    Y1 = Y[0:-1, 0:-1].ravel()\n    X2 = X[1:, 0:-1].ravel()\n    Y2 = Y[1:, 0:-1].ravel()\n    X3 = X[1:, 1:].ravel()\n    Y3 = Y[1:, 1:].ravel()\n    X4 = X[0:-1, 1:].ravel()\n    Y4 = Y[0:-1, 1:].ravel()\n\n    X = np.column_stack([X1, X2, X3, X4, X1])\n    Y = np.column_stack([Y1, Y2, Y3, Y4, Y1])\n    return X, Y"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpands polygon data which contains holes to a bokeh multi_polygons representation.", "response": "def multi_polygons_data(element):\n    \"\"\"\n    Expands polygon data which contains holes to a bokeh multi_polygons\n    representation. Multi-polygons split by nans are expanded and the\n    correct list of holes is assigned to each sub-polygon.\n    \"\"\"\n    paths = element.split(datatype='array', dimensions=element.kdims)\n    xs, ys = ([path[:, idx] for path in paths] for idx in (0, 1))\n    holes = element.holes()\n    xsh, ysh = [], []\n    for x, y, multi_hole in zip(xs, ys, holes):\n        xhs = [[h[:, 0] for h in hole] for hole in multi_hole]\n        yhs = [[h[:, 1] for h in hole] for hole in multi_hole]\n        array = np.column_stack([x, y])\n        splits = np.where(np.isnan(array[:, :2].astype('float')).sum(axis=1))[0]\n        arrays = np.split(array, splits+1) if len(splits) else [array]\n        multi_xs, multi_ys = [], []\n        for i, (path, hx, hy) in enumerate(zip(arrays, xhs, yhs)):\n            if i != (len(arrays)-1):\n                path = path[:-1]\n            multi_xs.append([path[:, 0]]+hx)\n            multi_ys.append([path[:, 1]]+hy)\n        xsh.append(multi_xs)\n        ysh.append(multi_ys)\n    return xsh, ysh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef match_dim_specs(specs1, specs2):\n    if (specs1 is None or specs2 is None) or (len(specs1) != len(specs2)):\n        return False\n    for spec1, spec2 in zip(specs1, specs2):\n        for s1, s2 in zip(spec1, spec2):\n            if s1 is None or s2 is None:\n                continue\n            if s1 != s2:\n                return False\n    return True", "response": "Matches dimension specs used to link axes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_glyph(self, plot, mapping, properties):\n        ret = super(SideHistogramPlot, self)._init_glyph(plot, mapping, properties)\n        if not 'field' in mapping.get('fill_color', {}):\n            return ret\n        dim = mapping['fill_color']['field']\n        sources = self.adjoined.traverse(lambda x: (x.handles.get('color_dim'),\n                                                     x.handles.get('source')))\n        sources = [src for cdim, src in sources if cdim == dim]\n        tools = [t for t in self.handles['plot'].tools\n                 if isinstance(t, BoxSelectTool)]\n        if not tools or not sources:\n            return\n        box_select, main_source = tools[0], sources[0]\n        handles = {'color_mapper': self.handles['color_mapper'],\n                   'source': self.handles['source'],\n                   'cds': self.handles['source'],\n                   'main_source': main_source}\n        axis = 'y' if self.invert_axes else 'x'\n        callback = self._callback.format(axis=axis)\n        if box_select.callback:\n            box_select.callback.code += callback\n            box_select.callback.args.update(handles)\n        else:\n            box_select.callback = CustomJS(args=handles, code=callback)\n        return ret", "response": "Initialize a Bokeh glyph."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitializes a glyph object.", "response": "def _init_glyph(self, plot, mapping, properties):\n        \"\"\"\n        Returns a Bokeh glyph object.\n        \"\"\"\n        properties.pop('legend', None)\n        for prop in ['color', 'alpha']:\n            if prop not in properties:\n                continue\n            pval = properties.pop(prop)\n            line_prop = 'line_%s' % prop\n            fill_prop = 'fill_%s' % prop\n            if line_prop not in properties:\n                properties[line_prop] = pval\n            if fill_prop not in properties and fill_prop in self.style_opts:\n                properties[fill_prop] = pval\n        properties = mpl_to_bokeh(properties)\n        plot_method = self._plot_methods['single']\n        glyph = plot_method(**dict(properties, **mapping))\n        plot.add_layout(glyph)\n        return None, glyph"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits the area plots at nans and returns x - and y - coordinates for each area separated by nans.", "response": "def _split_area(self, xs, lower, upper):\n        \"\"\"\n        Splits area plots at nans and returns x- and y-coordinates for\n        each area separated by nans.\n        \"\"\"\n        xnan = np.array([np.datetime64('nat') if xs.dtype.kind == 'M' else np.nan])\n        ynan = np.array([np.datetime64('nat') if lower.dtype.kind == 'M' else np.nan])\n        split = np.where(~isfinite(xs) | ~isfinite(lower) | ~isfinite(upper))[0]\n        xvals = np.split(xs, split)\n        lower = np.split(lower, split)\n        upper = np.split(upper, split)\n        band_x, band_y = [], []\n        for i, (x, l, u) in enumerate(zip(xvals, lower, upper)):\n            if i:\n                x, l, u = x[1:], l[1:], u[1:]\n            if not len(x):\n                continue\n            band_x += [np.append(x, x[::-1]), xnan]\n            band_y += [np.append(l, u[::-1]), ynan]\n        if len(band_x):\n            xs = np.concatenate(band_x[:-1])\n            ys = np.concatenate(band_y[:-1])\n            return xs, ys\n        return [], []"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the extents of the current bar.", "response": "def get_extents(self, element, ranges, range_type='combined'):\n        \"\"\"\n        Make adjustments to plot extents by computing\n        stacked bar heights, adjusting the bar baseline\n        and forcing the x-axis to be categorical.\n        \"\"\"\n        if self.batched:\n            overlay = self.current_frame\n            element = Bars(overlay.table(), kdims=element.kdims+overlay.kdims,\n                           vdims=element.vdims)\n            for kd in overlay.kdims:\n                ranges[kd.name]['combined'] = overlay.range(kd)\n\n        extents = super(BarPlot, self).get_extents(element, ranges, range_type)\n        xdim = element.kdims[0]\n        ydim = element.vdims[0]\n\n        # Compute stack heights\n        if self.stacked or self.stack_index:\n            ds = Dataset(element)\n            pos_range = ds.select(**{ydim.name: (0, None)}).aggregate(xdim, function=np.sum).range(ydim)\n            neg_range = ds.select(**{ydim.name: (None, 0)}).aggregate(xdim, function=np.sum).range(ydim)\n            y0, y1 = max_range([pos_range, neg_range])\n        else:\n            y0, y1 = ranges[ydim.name]['combined']\n\n        padding = 0 if self.overlaid else self.padding\n        _, ypad, _ = get_axis_padding(padding)\n        y0, y1 = range_pad(y0, y1, ypad, self.logy)\n\n        # Set y-baseline\n        if y0 < 0:\n            y1 = max([y1, 0])\n        elif self.logy:\n            y0 = (ydim.range[0] or (10**(np.log10(y1)-2)) if y1 else 0.01)\n        else:\n            y0 = 0\n\n        # Ensure x-axis is picked up as categorical\n        x0 = xdim.pprint_value(extents[0])\n        x1 = xdim.pprint_value(extents[2])\n        return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget factors for categorical axes.", "response": "def _get_factors(self, element):\n        \"\"\"\n        Get factors for categorical axes.\n        \"\"\"\n        gdim = None\n        sdim = None\n        if element.ndims == 1:\n            pass\n        elif not (self.stacked or self.stack_index):\n            gdim = element.get_dimension(1)\n        else:\n            sdim = element.get_dimension(1)\n\n        xdim, ydim = element.dimensions()[:2]\n        xvals = element.dimension_values(0, False)\n        xvals = [x if xvals.dtype.kind in 'SU' else xdim.pprint_value(x)\n                 for x in xvals]\n        if gdim and not sdim:\n            gvals = element.dimension_values(gdim, False)\n            xvals = sorted([(x, g) for x in xvals for g in gvals])\n            is_str = gvals.dtype.kind in 'SU'\n            xvals = [(x, g if is_str else gdim.pprint_value(g)) for (x, g) in xvals]\n        coords = xvals, []\n        if self.invert_axes: coords = coords[::-1]\n        return coords"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the bottom and top of a stack.", "response": "def get_stack(self, xvals, yvals, baselines, sign='positive'):\n        \"\"\"\n        Iterates over a x- and y-values in a stack layer\n        and appropriately offsets the layer on top of the\n        previous layer.\n        \"\"\"\n        bottoms, tops = [], []\n        for x, y in zip(xvals, yvals):\n            baseline = baselines[x][sign]\n            if sign == 'positive':\n                bottom = baseline\n                top = bottom+y\n                baseline = top\n            else:\n                top = baseline\n                bottom = top+y\n                baseline = bottom\n            baselines[x][sign] = baseline\n            bottoms.append(bottom)\n            tops.append(top)\n        return bottoms, tops"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef param_aliases(d):\n    for old, new in ALIASES.items():\n        old_param = '_%s_param_value' % old\n        new_param = '_%s_param_value' % new\n        if old_param in d:\n            d[new_param] = d.pop(old_param)\n    return d", "response": "A function to replace the aliases of the parameters in a dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef asdim(dimension):\n    if isinstance(dimension, Dimension):\n        return dimension\n    elif isinstance(dimension, (tuple, dict, basestring)):\n        return Dimension(dimension)\n    else:\n        raise ValueError('%s type could not be interpreted as Dimension. '\n                         'Dimensions must be declared as a string, tuple, '\n                         'dictionary or Dimension type.')", "response": "Convert the input to a Dimension."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the name of the Dimension or what would be the name if the Dimension is a string tuple or dict.", "response": "def dimension_name(dimension):\n    \"\"\"Return the Dimension.name for a dimension-like object.\n\n    Args:\n        dimension: Dimension or dimension string, tuple or dict\n\n    Returns:\n        The name of the Dimension or what would be the name if the\n        input as converted to a Dimension.\n    \"\"\"\n    if isinstance(dimension, Dimension):\n        return dimension.name\n    elif isinstance(dimension, basestring):\n        return dimension\n    elif isinstance(dimension, tuple):\n        return dimension[0]\n    elif isinstance(dimension, dict):\n        return dimension['name']\n    elif dimension is None:\n        return None\n    else:\n        raise ValueError('%s type could not be interpreted as Dimension. '\n                         'Dimensions must be declared as a string, tuple, '\n                         'dictionary or Dimension type.'\n                         % type(dimension).__name__)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_dimensions(kdims, vdims):\n    dimensions = {}\n    for group, dims in [('kdims', kdims), ('vdims', vdims)]:\n        if dims is None:\n            continue\n        elif isinstance(dims, (tuple, basestring, Dimension, dict)):\n            dims = [dims]\n        elif not isinstance(dims, list):\n            raise ValueError(\"%s argument expects a Dimension or list of dimensions, \"\n                             \"specified as tuples, strings, dictionaries or Dimension \"\n                             \"instances, not a %s type. Ensure you passed the data as the \"\n                             \"first argument.\" % (group, type(dims).__name__))\n        for dim in dims:\n            if not isinstance(dim, (tuple, basestring, Dimension, dict)):\n                raise ValueError('Dimensions must be defined as a tuple, '\n                                 'string, dictionary or Dimension instance, '\n                                 'found a %s type.' % type(dim).__name__)\n        dimensions[group] = [asdim(d) for d in dims]\n    return dimensions", "response": "Converts kdims and vdims to Dimension objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclone the Dimension with new parameters except for the supplied explicit overrides Returns a new Dimension object with new parameters except for the supplied explicit overrides", "response": "def clone(self, spec=None, **overrides):\n        \"\"\"Clones the Dimension with new parameters\n\n        Derive a new Dimension that inherits existing parameters\n        except for the supplied, explicit overrides\n\n        Args:\n            spec (tuple, optional): Dimension tuple specification\n            **overrides: Dimension parameter overrides\n\n        Returns:\n            Cloned Dimension object\n        \"\"\"\n        settings = dict(self.get_param_values(), **overrides)\n\n        if spec is None:\n            spec = (self.name, overrides.get('label', self.label))\n        if 'label' in overrides and isinstance(spec, basestring) :\n            spec = (spec, overrides['label'])\n        elif 'label' in overrides and isinstance(spec, tuple) :\n            if overrides['label'] != spec[1]:\n                self.param.warning(\n                    'Using label as supplied by keyword ({!r}), ignoring '\n                    'tuple value {!r}'.format(overrides['label'], spec[1]))\n            spec = (spec[0],  overrides['label'])\n        return self.__class__(spec, **{k:v for k,v in settings.items()\n                                       if k not in ['name', 'label']})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pprint_value(self, value):\n        own_type = type(value) if self.type is None else self.type\n        formatter = (self.value_format if self.value_format\n                     else self.type_formatters.get(own_type))\n        if formatter:\n            if callable(formatter):\n                return formatter(value)\n            elif isinstance(formatter, basestring):\n                if isinstance(value, (dt.datetime, dt.date)):\n                    return value.strftime(formatter)\n                elif isinstance(value, np.datetime64):\n                    return util.dt64_to_dt(value).strftime(formatter)\n                elif re.findall(r\"\\{(\\w+)\\}\", formatter):\n                    return formatter.format(value)\n                else:\n                    return formatter % value\n        return unicode(bytes_to_unicode(value))", "response": "Applies the applicable formatter to the value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle tracking and cleanup of custom ids.", "response": "def id(self, opts_id):\n        \"\"\"Handles tracking and cleanup of custom ids.\"\"\"\n        old_id = self._id\n        self._id = opts_id\n        if old_id is not None:\n            cleanup_custom_options(old_id)\n        if opts_id is not None and opts_id != old_id:\n            if opts_id not in Store._weakrefs:\n                Store._weakrefs[opts_id] = []\n            ref = weakref.ref(self, partial(cleanup_custom_options, opts_id))\n            Store._weakrefs[opts_id].append(ref)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncloning the object overriding data and parameters.", "response": "def clone(self, data=None, shared_data=True, new_type=None, link=True,\n              *args, **overrides):\n        \"\"\"Clones the object, overriding data and parameters.\n\n        Args:\n            data: New data replacing the existing data\n            shared_data (bool, optional): Whether to use existing data\n            new_type (optional): Type to cast object to\n            link (bool, optional): Whether clone should be linked\n                Determines whether Streams and Links attached to\n                original object will be inherited.\n            *args: Additional arguments to pass to constructor\n            **overrides: New keyword arguments to pass to constructor\n\n        Returns:\n            Cloned object\n        \"\"\"\n        params = dict(self.get_param_values())\n        if new_type is None:\n            clone_type = self.__class__\n        else:\n            clone_type = new_type\n            new_params = new_type.params()\n            params = {k: v for k, v in params.items()\n                      if k in new_params}\n            if params.get('group') == self.params()['group'].default:\n                params.pop('group')\n        settings = dict(params, **overrides)\n        if 'id' not in settings:\n            settings['id'] = self.id\n\n        if data is None and shared_data:\n            data = self.data\n            if link:\n                settings['plot_id'] = self._plot_id\n        # Apply name mangling for __ attribute\n        pos_args = getattr(self, '_' + type(self).__name__ + '__pos_params', [])\n        return clone_type(data, *args, **{k:v for k,v in settings.items()\n                                          if k not in pos_args})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef relabel(self, label=None, group=None, depth=0):\n        new_data = self.data\n        if (depth > 0) and getattr(self, '_deep_indexable', False):\n            new_data = []\n            for k, v in self.data.items():\n                relabelled = v.relabel(group=group, label=label, depth=depth-1)\n                new_data.append((k, relabelled))\n        keywords = [('label', label), ('group', group)]\n        kwargs = {k: v for k, v in keywords if v is not None}\n        return self.clone(new_data, **kwargs)", "response": "Clone object and apply new label and group and depth."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine if the object matches the given object.", "response": "def matches(self, spec):\n        \"\"\"Whether the spec applies to this object.\n\n        Args:\n            spec: A function, spec or type to check for a match\n                * A 'type[[.group].label]' string which is compared\n                  against the type, group and label of this object\n                * A function which is given the object and returns\n                  a boolean.\n                * An object type matched using isinstance.\n\n        Returns:\n            bool: Whether the spec matched this object.\n        \"\"\"\n        if callable(spec) and not isinstance(spec, type): return spec(self)\n        elif isinstance(spec, type): return isinstance(self, spec)\n        specification = (self.__class__.__name__, self.group, self.label)\n        split_spec = tuple(spec.split('.')) if not isinstance(spec, tuple) else spec\n        split_spec, nocompare = zip(*((None, True) if s == '*' or s is None else (s, False)\n                                    for s in split_spec))\n        if all(nocompare): return True\n        match_fn = itemgetter(*(idx for idx, nc in enumerate(nocompare) if not nc))\n        self_spec = match_fn(split_spec)\n        unescaped_match = match_fn(specification[:len(split_spec)]) == self_spec\n        if unescaped_match: return True\n        sanitizers = [util.sanitize_identifier, util.group_sanitizer, util.label_sanitizer]\n        identifier_specification = tuple(fn(ident, escape=False)\n                                         for ident, fn in zip(specification, sanitizers))\n        identifier_match = match_fn(identifier_specification[:len(split_spec)]) == self_spec\n        return identifier_match"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntraversing the set of children of the object returning matching items", "response": "def traverse(self, fn=None, specs=None, full_breadth=True):\n        \"\"\"Traverses object returning matching items\n\n        Traverses the set of children of the object, collecting the\n        all objects matching the defined specs. Each object can be\n        processed with the supplied function.\n\n        Args:\n            fn (function, optional): Function applied to matched objects\n            specs: List of specs to match\n                Specs must be types, functions or type[.group][.label]\n                specs to select objects to return, by default applies\n                to all objects.\n            full_breadth: Whether to traverse all objects\n                Whether to traverse the full set of objects on each\n                container or only the first.\n\n        Returns:\n            list: List of objects that matched\n        \"\"\"\n        if fn is None:\n            fn = lambda x: x\n        if specs is not None and not isinstance(specs, (list, set, tuple)):\n            specs = [specs]\n        accumulator = []\n        matches = specs is None\n        if not matches:\n            for spec in specs:\n                matches = self.matches(spec)\n                if matches: break\n        if matches:\n            accumulator.append(fn(self))\n\n        # Assumes composite objects are iterables\n        if self._deep_indexable:\n            for el in self:\n                if el is None:\n                    continue\n                accumulator += el.traverse(fn, specs, full_breadth)\n                if not full_breadth: break\n        return accumulator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef map(self, map_fn, specs=None, clone=True):\n        if specs is not None and not isinstance(specs, (list, set, tuple)):\n            specs = [specs]\n        applies = specs is None or any(self.matches(spec) for spec in specs)\n\n        if self._deep_indexable:\n            deep_mapped = self.clone(shared_data=False) if clone else self\n            for k, v in self.items():\n                new_val = v.map(map_fn, specs, clone)\n                if new_val is not None:\n                    deep_mapped[k] = new_val\n            if applies: deep_mapped = map_fn(deep_mapped)\n            return deep_mapped\n        else:\n            return map_fn(self) if applies else self", "response": "Map a function to all objects matching the specs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates key dimension input Returns kdims if no dimensions are specified", "response": "def _valid_dimensions(self, dimensions):\n        \"\"\"Validates key dimension input\n\n        Returns kdims if no dimensions are specified\"\"\"\n        if dimensions is None:\n            dimensions = self.kdims\n        elif not isinstance(dimensions, list):\n            dimensions = [dimensions]\n\n        valid_dimensions = []\n        for dim in dimensions:\n            if isinstance(dim, Dimension): dim = dim.name\n            if dim not in self.kdims:\n                raise Exception(\"Supplied dimensions %s not found.\" % dim)\n            valid_dimensions.append(dim)\n        return valid_dimensions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dimensions(self, selection='all', label=False):\n        if label in ['name', True]:\n            label = 'short'\n        elif label == 'label':\n            label = 'long'\n        elif label:\n            raise ValueError(\"label needs to be one of True, False, 'name' or 'label'\")\n\n        lambdas = {'k': (lambda x: x.kdims, {'full_breadth': False}),\n                   'v': (lambda x: x.vdims, {}),\n                   'c': (lambda x: x.cdims, {})}\n        aliases = {'key': 'k', 'value': 'v', 'constant': 'c'}\n        if selection in ['all', 'ranges']:\n            groups = [d for d in self._dim_groups if d != 'cdims']\n            dims = [dim for group in groups\n                    for dim in getattr(self, group)]\n        elif isinstance(selection, list):\n            dims =  [dim for group in selection\n                     for dim in getattr(self, '%sdims' % aliases.get(group))]\n        elif aliases.get(selection) in lambdas:\n            selection = aliases.get(selection, selection)\n            lmbd, kwargs = lambdas[selection]\n            key_traversal = self.traverse(lmbd, **kwargs)\n            dims = [dim for keydims in key_traversal for dim in keydims]\n        else:\n            raise KeyError(\"Invalid selection %r, valid selections include\"\n                           \"'all', 'value' and 'key' dimensions\" % repr(selection))\n        return [(dim.label if label == 'long' else dim.name)\n                if label else dim for dim in dims]", "response": "Returns the available dimensions on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_dimension(self, dimension, default=None, strict=False):\n        if dimension is not None and not isinstance(dimension, (int, basestring, Dimension)):\n            raise TypeError('Dimension lookup supports int, string, '\n                            'and Dimension instances, cannot lookup '\n                            'Dimensions using %s type.' % type(dimension).__name__)\n        all_dims = self.dimensions()\n        if isinstance(dimension, int):\n            if 0 <= dimension < len(all_dims):\n                return all_dims[dimension]\n            elif strict:\n                raise KeyError(\"Dimension %r not found\" % dimension)\n            else:\n                return default\n        dimension = dimension_name(dimension)\n        name_map = {dim.name: dim for dim in all_dims}\n        name_map.update({dim.label: dim for dim in all_dims})\n        name_map.update({util.dimension_sanitizer(dim.name): dim for dim in all_dims})\n        if strict and dimension not in name_map:\n            raise KeyError(\"Dimension %r not found.\" % dimension)\n        else:\n            return name_map.get(dimension, default)", "response": "Get a Dimension object by name or index."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the index of the requested dimension.", "response": "def get_dimension_index(self, dimension):\n        \"\"\"Get the index of the requested dimension.\n\n        Args:\n            dimension: Dimension to look up by name or by index\n\n        Returns:\n            Integer index of the requested dimension\n        \"\"\"\n        if isinstance(dimension, int):\n            if (dimension < (self.ndims + len(self.vdims)) or\n                dimension < len(self.dimensions())):\n                return dimension\n            else:\n                return IndexError('Dimension index out of bounds')\n        dim = dimension_name(dimension)\n        try:\n            dimensions = self.kdims+self.vdims\n            return [i for i, d in enumerate(dimensions) if d == dim][0]\n        except IndexError:\n            raise Exception(\"Dimension %s not found in %s.\" %\n                            (dim, self.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the type of the requested dimension.", "response": "def get_dimension_type(self, dim):\n        \"\"\"Get the type of the requested dimension.\n\n        Type is determined by Dimension.type attribute or common\n        type of the dimension values, otherwise None.\n\n        Args:\n            dimension: Dimension to look up by name or by index\n\n        Returns:\n            Declared type of values along the dimension\n        \"\"\"\n        dim_obj = self.get_dimension(dim)\n        if dim_obj and dim_obj.type is not None:\n            return dim_obj.type\n        dim_vals = [type(v) for v in self.dimension_values(dim)]\n        if len(set(dim_vals)) == 1:\n            return dim_vals[0]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies a selection to the object using the key - value pairs specified by selection_specs.", "response": "def select(self, selection_specs=None, **kwargs):\n        \"\"\"Applies selection by dimension name\n\n        Applies a selection along the dimensions of the object using\n        keyword arguments. The selection may be narrowed to certain\n        objects using selection_specs. For container objects the\n        selection will be applied to all children as well.\n\n        Selections may select a specific value, slice or set of values:\n\n        * value: Scalar values will select rows along with an exact\n                 match, e.g.:\n\n            ds.select(x=3)\n\n        * slice: Slices may be declared as tuples of the upper and\n                 lower bound, e.g.:\n\n            ds.select(x=(0, 3))\n\n        * values: A list of values may be selected using a list or\n                  set, e.g.:\n\n            ds.select(x=[0, 1, 2])\n\n        Args:\n            selection_specs: List of specs to match on\n                A list of types, functions, or type[.group][.label]\n                strings specifying which objects to apply the\n                selection on.\n            **selection: Dictionary declaring selections by dimension\n                Selections can be scalar values, tuple ranges, lists\n                of discrete values and boolean arrays\n\n        Returns:\n            Returns an Dimensioned object containing the selected data\n            or a scalar if a single value was selected\n        \"\"\"\n        if selection_specs is not None and not isinstance(selection_specs, (list, tuple)):\n            selection_specs = [selection_specs]\n\n        # Apply all indexes applying on this object\n        vdims = self.vdims+['value'] if self.vdims else []\n        kdims = self.kdims\n        local_kwargs = {k: v for k, v in kwargs.items()\n                        if k in kdims+vdims}\n\n        # Check selection_spec applies\n        if selection_specs is not None:\n            if not isinstance(selection_specs, (list, tuple)):\n                selection_specs = [selection_specs]\n            matches = any(self.matches(spec)\n                          for spec in selection_specs)\n        else:\n            matches = True\n\n        # Apply selection to self\n        if local_kwargs and matches:\n            ndims = self.ndims\n            if any(d in self.vdims for d in kwargs):\n                ndims = len(self.kdims+self.vdims)\n            select = [slice(None) for _ in range(ndims)]\n            for dim, val in local_kwargs.items():\n                if dim == 'value':\n                    select += [val]\n                else:\n                    if isinstance(val, tuple): val = slice(*val)\n                    select[self.get_dimension_index(dim)] = val\n            if self._deep_indexable:\n                selection = self.get(tuple(select), None)\n                if selection is None:\n                    selection = self.clone(shared_data=False)\n            else:\n                selection = self[tuple(select)]\n        else:\n            selection = self\n\n        if not isinstance(selection, Dimensioned):\n            return selection\n        elif type(selection) is not type(self) and isinstance(selection, Dimensioned):\n            # Apply the selection on the selected object of a different type\n            dimensions = selection.dimensions() + ['value']\n            if any(kw in dimensions for kw in kwargs):\n                selection = selection.select(selection_specs, **kwargs)\n        elif isinstance(selection, Dimensioned) and selection._deep_indexable:\n            # Apply the deep selection on each item in local selection\n            items = []\n            for k, v in selection.items():\n                dimensions = v.dimensions() + ['value']\n                if any(kw in dimensions for kw in kwargs):\n                    items.append((k, v.select(selection_specs, **kwargs)))\n                else:\n                    items.append((k, v))\n            selection = selection.clone(items)\n        return selection"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the values along the requested dimension.", "response": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        val = self._cached_constants.get(dimension, None)\n        if val:\n            return np.array([val])\n        else:\n            raise Exception(\"Dimension %s not found in %s.\" %\n                            (dimension, self.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range(self, dimension, data_range=True, dimension_range=True):\n        dimension = self.get_dimension(dimension)\n        if dimension is None or (not data_range and not dimension_range):\n            return (None, None)\n        elif all(util.isfinite(v) for v in dimension.range) and dimension_range:\n            return dimension.range\n        elif data_range:\n            if dimension in self.kdims+self.vdims:\n                dim_vals = self.dimension_values(dimension.name)\n                lower, upper = util.find_range(dim_vals)\n            else:\n                dname = dimension.name\n                match_fn = lambda x: dname in x.kdims + x.vdims\n                range_fn = lambda x: x.range(dname)\n                ranges = self.traverse(range_fn, [match_fn])\n                lower, upper = util.max_range(ranges)\n        else:\n            lower, upper = (np.NaN, np.NaN)\n        if not dimension_range:\n            return lower, upper\n        return util.dimension_range(lower, upper, dimension.range, dimension.soft_range)", "response": "Return the lower and upper bounds of values along a dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\napplies simplified option definition returning a new object.", "response": "def options(self, *args, **kwargs):\n        \"\"\"Applies simplified option definition returning a new object.\n\n        Applies options on an object or nested group of objects in a\n        flat format returning a new object with the options\n        applied. If the options are to be set directly on the object a\n        simple format may be used, e.g.:\n\n            obj.options(cmap='viridis', show_title=False)\n\n        If the object is nested the options must be qualified using\n        a type[.group][.label] specification, e.g.:\n\n            obj.options('Image', cmap='viridis', show_title=False)\n\n        or using:\n\n            obj.options({'Image': dict(cmap='viridis', show_title=False)})\n\n        Identical to the .opts method but returns a clone of the object\n        by default.\n\n        Args:\n            *args: Sets of options to apply to object\n                Supports a number of formats including lists of Options\n                objects, a type[.group][.label] followed by a set of\n                keyword options to apply and a dictionary indexed by\n                type[.group][.label] specs.\n            backend (optional): Backend to apply options to\n                Defaults to current selected backend\n            clone (bool, optional): Whether to clone object\n                Options can be applied inplace with clone=False\n            **kwargs: Keywords of options\n                Set of options to apply to the object\n\n        Returns:\n            Returns the cloned object with the options applied\n        \"\"\"\n        backend = kwargs.get('backend', None)\n        clone = kwargs.pop('clone', True)\n\n        if len(args) == 0 and len(kwargs)==0:\n            options = None\n        elif args and isinstance(args[0], basestring):\n            options = {args[0]: kwargs}\n        elif args and isinstance(args[0], list):\n            if kwargs:\n                raise ValueError('Please specify a list of option objects, or kwargs, but not both')\n            options = args[0]\n        elif args and [k for k in kwargs.keys() if k != 'backend']:\n            raise ValueError(\"Options must be defined in one of two formats. \"\n                             \"Either supply keywords defining the options for \"\n                             \"the current object, e.g. obj.options(cmap='viridis'), \"\n                             \"or explicitly define the type, e.g. \"\n                             \"obj.options({'Image': {'cmap': 'viridis'}}). \"\n                             \"Supplying both formats is not supported.\")\n        elif args and all(isinstance(el, dict) for el in args):\n            if len(args) > 1:\n                self.warning('Only a single dictionary can be passed '\n                             'as a positional argument. Only processing '\n                             'the first dictionary')\n            options = [Options(spec, **kws) for spec,kws in args[0].items()]\n        elif args:\n            options = list(args)\n        elif kwargs:\n            options = {type(self).__name__: kwargs}\n\n        from ..util import opts\n        if options is None:\n            expanded_backends = [(backend, {})]\n        elif isinstance(options, list): # assuming a flat list of Options objects\n            expanded_backends = opts._expand_by_backend(options, backend)\n        else:\n            expanded_backends = [(backend, opts._expand_options(options, backend))]\n\n        obj = self\n        for backend, expanded in expanded_backends:\n            obj = obj.opts._dispatch_opts(expanded, backend=backend, clone=clone)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeprecating method to construct tree from list of objects", "response": "def from_values(cls, vals):\n        \"Deprecated method to construct tree from list of objects\"\n        if util.config.future_deprecations:\n            param.main.param.warning(\"%s.from_values is deprecated, the %s \"\n                                     \"constructor may now be used directly.\")\n        return cls(items=cls._process_items(vals))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess list of items assigning unique paths to each.", "response": "def _process_items(cls, vals):\n        \"Processes list of items assigning unique paths to each.\"\n        if type(vals) is cls:\n            return vals.data\n        elif not isinstance(vals, (list, tuple)):\n            vals = [vals]\n        items = []\n        counts = defaultdict(lambda: 1)\n        cls._unpack_paths(vals, items, counts)\n        items = cls._deduplicate_items(items)\n        return items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unpack_paths(cls, objs, items, counts):\n        if type(objs) is cls:\n            objs = objs.items()\n        for item in objs:\n            path, obj = item if isinstance(item, tuple) else (None, item)\n            if type(obj) is cls:\n                cls._unpack_paths(obj, items, counts)\n                continue\n            new = path is None or len(path) == 1\n            path = util.get_path(item) if new else path\n            new_path = util.make_path_unique(path, counts, new)\n            items.append((new_path, obj))", "response": "Recursively unpacks lists and ViewableTree - like objects accumulating\n            into the supplied list of items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the values along the requested dimension.", "response": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Concatenates values on all nodes with requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n                Whether to return the expanded values, behavior depends\n                on the type of data:\n                  * Columnar: If false returns unique values\n                  * Geometry: If false returns scalar values per geometry\n                  * Gridded: If false returns 1D coordinates\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        dimension = self.get_dimension(dimension, strict=True).name\n        all_dims = self.traverse(lambda x: [d.name for d in x.dimensions()])\n        if dimension in chain.from_iterable(all_dims):\n            values = [el.dimension_values(dimension) for el in self\n                      if dimension in el.dimensions(label=True)]\n            vals = np.concatenate(values)\n            return vals if expanded else util.unique_array(vals)\n        else:\n            return super(ViewableTree, self).dimension_values(\n                dimension, expanded, flat)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a new set of items in the specified group.", "response": "def regroup(self, group):\n        \"\"\"Deprecated method to apply new group to items.\n\n        Equivalent functionality possible using:\n\n            ViewableTree(tree.relabel(group='Group').values())\n        \"\"\"\n        if util.config.future_deprecations:\n            self.param.warning('%s.regroup is deprecated, use relabel '\n                               'method with a group argument instead.'\n                               % type(self).__name__)\n        new_items = [el.relabel(group=group) for el in self.data.values()]\n        return reduce(lambda x,y: x+y, new_items)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _finalize_axis(self, key, element=None, title=None, dimensions=None, ranges=None, xticks=None,\n                       yticks=None, zticks=None, xlabel=None, ylabel=None, zlabel=None):\n        \"\"\"\n        Applies all the axis settings before the axis or figure is returned.\n        Only plots with zorder 0 get to apply their settings.\n\n        When the number of the frame is supplied as n, this method looks\n        up and computes the appropriate title, axis labels and axis bounds.\n        \"\"\"\n        if element is None:\n            element = self._get_frame(key)\n        self.current_frame = element\n        if not dimensions and element and not self.subplots:\n            el = element.traverse(lambda x: x, [Element])\n            if el:\n                el = el[0]\n                dimensions = el.nodes.dimensions() if isinstance(el, Graph) else el.dimensions()\n        axis = self.handles['axis']\n\n        subplots = list(self.subplots.values()) if self.subplots else []\n        if self.zorder == 0 and key is not None:\n            if self.bgcolor:\n                if mpl_version <= '1.5.9':\n                    axis.set_axis_bgcolor(self.bgcolor)\n                else:\n                    axis.set_facecolor(self.bgcolor)\n\n            # Apply title\n            title = self._format_title(key)\n            if self.show_title and title is not None:\n                fontsize = self._fontsize('title')\n                if 'title' in self.handles:\n                    self.handles['title'].set_text(title)\n                else:\n                    self.handles['title'] = axis.set_title(title, **fontsize)\n\n            # Apply subplot label\n            self._subplot_label(axis)\n\n            # Apply axis options if axes are enabled\n            if element is not None and not any(not sp._has_axes for sp in [self] + subplots):\n                # Set axis labels\n                if dimensions:\n                    self._set_labels(axis, dimensions, xlabel, ylabel, zlabel)\n                else:\n                    if self.xlabel is not None:\n                        axis.set_xlabel(self.xlabel)\n                    if self.ylabel is not None:\n                        axis.set_ylabel(self.ylabel)\n                    if self.zlabel is not None and hasattr(axis, 'set_zlabel'):\n                        axis.set_zlabel(self.zlabel)\n\n                if not subplots:\n                    legend = axis.get_legend()\n                    if legend:\n                        legend.set_visible(self.show_legend)\n                        self.handles[\"bbox_extra_artists\"] += [legend]\n                    axis.xaxis.grid(self.show_grid)\n                    axis.yaxis.grid(self.show_grid)\n\n                # Apply log axes\n                if self.logx:\n                    axis.set_xscale('log')\n                if self.logy:\n                    axis.set_yscale('log')\n\n                if not self.projection == '3d':\n                    self._set_axis_position(axis, 'x', self.xaxis)\n                    self._set_axis_position(axis, 'y', self.yaxis)\n\n                # Apply ticks\n                if self.apply_ticks:\n                    self._finalize_ticks(axis, dimensions, xticks, yticks, zticks)\n\n                # Set axes limits\n                self._set_axis_limits(axis, element, subplots, ranges)\n\n            # Apply aspects\n            if self.aspect is not None and self.projection != 'polar' and not self.adjoined:\n                self._set_aspect(axis, self.aspect)\n\n        if not subplots and not self.drawn:\n            self._finalize_artist(element)\n\n        self._execute_hooks(element)\n        return super(ElementPlot, self)._finalize_axis(key)", "response": "Finalizes the axis with the given key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfinalize the ticks on the axes based on the supplied ticks and Elements.", "response": "def _finalize_ticks(self, axis, dimensions, xticks, yticks, zticks):\n        \"\"\"\n        Finalizes the ticks on the axes based on the supplied ticks\n        and Elements. Sets the axes position as well as tick positions,\n        labels and fontsize.\n        \"\"\"\n        ndims = len(dimensions) if dimensions else 0\n        xdim = dimensions[0] if ndims else None\n        ydim = dimensions[1] if ndims > 1 else None\n\n        # Tick formatting\n        if xdim:\n            self._set_axis_formatter(axis.xaxis, xdim, self.xformatter)\n        if ydim:\n            self._set_axis_formatter(axis.yaxis, ydim, self.yformatter)\n        if self.projection == '3d':\n            zdim = dimensions[2] if ndims > 2 else None\n            if zdim or self.zformatter is not None:\n                self._set_axis_formatter(axis.zaxis, zdim, self.zformatter)\n\n        xticks = xticks if xticks else self.xticks\n        self._set_axis_ticks(axis.xaxis, xticks, log=self.logx,\n                             rotation=self.xrotation)\n\n        yticks = yticks if yticks else self.yticks\n        self._set_axis_ticks(axis.yaxis, yticks, log=self.logy,\n                             rotation=self.yrotation)\n\n        if self.projection == '3d':\n            zticks = zticks if zticks else self.zticks\n            self._set_axis_ticks(axis.zaxis, zticks, log=self.logz,\n                                 rotation=self.zrotation)\n\n        axes_str = 'xy'\n        axes_list = [axis.xaxis, axis.yaxis]\n\n        if hasattr(axis, 'zaxis'):\n            axes_str += 'z'\n            axes_list.append(axis.zaxis)\n\n        for ax, ax_obj in zip(axes_str, axes_list):\n            tick_fontsize = self._fontsize('%sticks' % ax,'labelsize',common=False)\n            if tick_fontsize: ax_obj.set_tick_params(**tick_fontsize)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the labels of the specified axes using the supplied list of dimensions.", "response": "def _set_labels(self, axes, dimensions, xlabel=None, ylabel=None, zlabel=None):\n        \"\"\"\n        Sets the labels of the axes using the supplied list of dimensions.\n        Optionally explicit labels may be supplied to override the dimension\n        label.\n        \"\"\"\n        xlabel, ylabel, zlabel = self._get_axis_labels(dimensions, xlabel, ylabel, zlabel)\n        if self.invert_axes:\n            xlabel, ylabel = ylabel, xlabel\n        if xlabel and self.xaxis and 'x' in self.labelled:\n            axes.set_xlabel(xlabel, **self._fontsize('xlabel'))\n        if ylabel and self.yaxis and 'y' in self.labelled:\n            axes.set_ylabel(ylabel, **self._fontsize('ylabel'))\n        if zlabel and self.zaxis and 'z' in self.labelled:\n            axes.set_zlabel(zlabel, **self._fontsize('zlabel'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset axis formatter based on dimension formatter.", "response": "def _set_axis_formatter(self, axis, dim, formatter):\n        \"\"\"\n        Set axis formatter based on dimension formatter.\n        \"\"\"\n        if isinstance(dim, list): dim = dim[0]\n        if formatter is not None:\n            pass\n        elif dim.value_format:\n            formatter = dim.value_format\n        elif dim.type in dim.type_formatters:\n            formatter = dim.type_formatters[dim.type]\n        if formatter:\n            axis.set_major_formatter(wrap_formatter(formatter))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the aspect ratio of the plotted object.", "response": "def get_aspect(self, xspan, yspan):\n        \"\"\"\n        Computes the aspect ratio of the plot\n        \"\"\"\n        if isinstance(self.aspect, (int, float)):\n            return self.aspect\n        elif self.aspect == 'square':\n            return 1\n        elif self.aspect == 'equal':\n            return xspan/yspan\n        return 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the aspect on the axes based on the aspect setting.", "response": "def _set_aspect(self, axes, aspect):\n        \"\"\"\n        Set the aspect on the axes based on the aspect setting.\n        \"\"\"\n        if ((isinstance(aspect, util.basestring) and aspect != 'square') or\n            self.data_aspect):\n            data_ratio = self.data_aspect or aspect\n        else:\n            (x0, x1), (y0, y1) = axes.get_xlim(), axes.get_ylim()\n            xsize = np.log(x1) - np.log(x0) if self.logx else x1-x0\n            ysize = np.log(y1) - np.log(y0) if self.logy else y1-y0\n            xsize = max(abs(xsize), 1e-30)\n            ysize = max(abs(ysize), 1e-30)\n            data_ratio = 1./(ysize/xsize)\n            if aspect != 'square':\n                data_ratio = data_ratio/aspect\n        axes.set_aspect(data_ratio)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _set_axis_limits(self, axis, view, subplots, ranges):\n        # Extents\n        extents = self.get_extents(view, ranges)\n        if not extents or self.overlaid:\n            axis.autoscale_view(scalex=True, scaley=True)\n            return\n\n        valid_lim = lambda c: util.isnumeric(c) and not np.isnan(c)\n        coords = [coord if np.isreal(coord) or isinstance(coord, np.datetime64) else np.NaN for coord in extents]\n        coords = [date2num(util.dt64_to_dt(c)) if isinstance(c, np.datetime64) else c\n                  for c in coords]\n        if self.projection == '3d' or len(extents) == 6:\n            l, b, zmin, r, t, zmax = coords\n            if self.invert_zaxis or any(p.invert_zaxis for p in subplots):\n                zmin, zmax = zmax, zmin\n            if zmin != zmax:\n                if valid_lim(zmin):\n                    axis.set_zlim(bottom=zmin)\n                if valid_lim(zmax):\n                    axis.set_zlim(top=zmax)\n        else:\n            l, b, r, t = coords\n\n        if self.invert_axes:\n            l, b, r, t = b, l, t, r\n\n        invertx = self.invert_xaxis or any(p.invert_xaxis for p in subplots)\n        xlim, scalex = self._compute_limits(l, r, self.logx, invertx, 'left', 'right')\n        inverty = self.invert_yaxis or any(p.invert_yaxis for p in subplots)\n        ylim, scaley =  self._compute_limits(b, t, self.logy, inverty, 'bottom', 'top')\n        if xlim:\n            axis.set_xlim(**xlim)\n        if ylim:\n            axis.set_ylim(**ylim)\n        axis.autoscale_view(scalex=scalex, scaley=scaley)", "response": "Compute extents for current view and apply as axis limits"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _set_axis_position(self, axes, axis, option):\n        positions = {'x': ['bottom', 'top'], 'y': ['left', 'right']}[axis]\n        axis = axes.xaxis if axis == 'x' else axes.yaxis\n        if option in [None, False]:\n            axis.set_visible(False)\n            for pos in positions:\n                axes.spines[pos].set_visible(False)\n        else:\n            if option is True:\n                option = positions[0]\n            if 'bare' in option:\n                axis.set_ticklabels([])\n                axis.set_label_text('')\n            if option != 'bare':\n                option = option.split('-')[0]\n                axis.set_ticks_position(option)\n                axis.set_label_position(option)\n        if not self.overlaid and not self.show_frame and self.projection != 'polar':\n            pos = (positions[1] if (option and (option == 'bare' or positions[0] in option))\n                   else positions[0])\n            axes.spines[pos].set_visible(False)", "response": "Set the position and visibility of the axis by supplying the axes object the axis and the option to specify the position and visibility of the axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_axis_ticks(self, axis, ticks, log=False, rotation=0):\n        if isinstance(ticks, (list, tuple)) and all(isinstance(l, list) for l in ticks):\n            axis.set_ticks(ticks[0])\n            axis.set_ticklabels(ticks[1])\n        elif isinstance(ticks, ticker.Locator):\n            axis.set_major_locator(ticks)\n        elif not ticks and ticks is not None:\n            axis.set_ticks([])\n        elif isinstance(ticks, int):\n            if log:\n                locator = ticker.LogLocator(numticks=ticks,\n                                            subs=range(1,10))\n            else:\n                locator = ticker.MaxNLocator(ticks)\n            axis.set_major_locator(locator)\n        elif isinstance(ticks, (list, tuple)):\n            labels = None\n            if all(isinstance(t, tuple) for t in ticks):\n                ticks, labels = zip(*ticks)\n            axis.set_ticks(ticks)\n            if labels:\n                axis.set_ticklabels(labels)\n        for tick in axis.get_ticklabels():\n            tick.set_rotation(rotation)", "response": "Allows setting the ticks for a particular axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the plot with the given key and ranges.", "response": "def update_frame(self, key, ranges=None, element=None):\n        \"\"\"\n        Set the plot(s) to the given frame number.  Operates by\n        manipulating the matplotlib objects held in the self._handles\n        dictionary.\n\n        If n is greater than the number of available frames, update\n        using the last available frame.\n        \"\"\"\n        reused = isinstance(self.hmap, DynamicMap) and self.overlaid\n        if not reused and element is None:\n            element = self._get_frame(key)\n        elif element is not None:\n            self.current_key = key\n            self.current_frame = element\n\n        if element is not None:\n            self.param.set_param(**self.lookup_options(element, 'plot').options)\n        axis = self.handles['axis']\n\n        axes_visible = element is not None or self.overlaid\n        axis.xaxis.set_visible(axes_visible and self.xaxis)\n        axis.yaxis.set_visible(axes_visible and self.yaxis)\n        axis.patch.set_alpha(np.min([int(axes_visible), 1]))\n\n        for hname, handle in self.handles.items():\n            hideable = hasattr(handle, 'set_visible')\n            if hname not in ['axis', 'fig'] and hideable:\n                handle.set_visible(element is not None)\n        if element is None:\n            return\n\n        ranges = self.compute_ranges(self.hmap, key, ranges)\n        ranges = util.match_spec(element, ranges)\n\n        max_cycles = self.style._max_cycles\n        style = self.lookup_options(element, 'style')\n        self.style = style.max_cycles(max_cycles) if max_cycles else style\n\n        label = element.label if self.show_legend else ''\n        style = dict(label=label, zorder=self.zorder, **self.style[self.cyclic_index])\n        axis_kwargs = self.update_handles(key, axis, element, ranges, style)\n        self._finalize_axis(key, element=element, ranges=ranges,\n                            **(axis_kwargs if axis_kwargs else {}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_artists(self, ax, plot_args, plot_kwargs):\n        plot_method = self._plot_methods.get('batched' if self.batched else 'single')\n        plot_fn = getattr(ax, plot_method)\n        artist = plot_fn(*plot_args, **plot_kwargs)\n        return {'artist': artist[0] if isinstance(artist, list) and\n                len(artist) == 1 else artist}", "response": "Initializes the artist based on the plot method declared on the plot method declared on the plot."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the elements of the plot.", "response": "def update_handles(self, key, axis, element, ranges, style):\n        \"\"\"\n        Update the elements of the plot.\n        \"\"\"\n        self.teardown_handles()\n        plot_data, plot_kwargs, axis_kwargs = self.get_data(element, ranges, style)\n\n        with abbreviated_exception():\n            handles = self.init_artists(axis, plot_data, plot_kwargs)\n        self.handles.update(handles)\n        return axis_kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nnormalize kwargs to be passed to matplotlib plot function.", "response": "def _norm_kwargs(self, element, ranges, opts, vdim, values=None, prefix=''):\n        \"\"\"\n        Returns valid color normalization kwargs\n        to be passed to matplotlib plot function.\n        \"\"\"\n        dim_name = dim_range_key(vdim)\n        if values is None:\n            if isinstance(vdim, dim):\n                values = vdim.apply(element, flat=True)\n            else:\n                expanded = not (\n                    isinstance(element, Dataset) and\n                    element.interface.multi and\n                    (getattr(element, 'level', None) is not None or\n                     element.interface.isscalar(element, vdim.name))\n                )\n                values = np.asarray(element.dimension_values(vdim, expanded=expanded))\n\n        # Store dimension being colormapped for colorbars\n        if prefix+'color_dim' not in self.handles:\n            self.handles[prefix+'color_dim'] = vdim\n\n        clim = opts.pop(prefix+'clims', None)\n\n        # check if there's an actual value (not np.nan)\n        if clim is None and util.isfinite(self.clim).all():\n            clim = self.clim\n\n        if clim is None:\n            if not len(values):\n                clim = (0, 0)\n                categorical = False\n            elif values.dtype.kind in 'uif':\n                if dim_name in ranges:\n                    clim = ranges[dim_name]['combined']\n                elif isinstance(vdim, dim):\n                    if values.dtype.kind == 'M':\n                        clim = values.min(), values.max()\n                    elif len(values) == 0:\n                        clim = np.NaN, np.NaN\n                    else:\n                        try:\n                            with warnings.catch_warnings():\n                                warnings.filterwarnings('ignore', r'All-NaN (slice|axis) encountered')\n                                clim = (np.nanmin(values), np.nanmax(values))\n                        except:\n                            clim = np.NaN, np.NaN\n                else:\n                    clim = element.range(vdim)\n                if self.logz:\n                    # Lower clim must be >0 when logz=True\n                    # Choose the maximum between the lowest non-zero value\n                    # and the overall range\n                    if clim[0] == 0:\n                        clim = (values[values!=0].min(), clim[1])\n                if self.symmetric:\n                    clim = -np.abs(clim).max(), np.abs(clim).max()\n                categorical = False\n            else:\n                range_key = dim_range_key(vdim)\n                if range_key in ranges and 'factors' in ranges[range_key]:\n                    factors = ranges[range_key]['factors']\n                else:\n                    factors = util.unique_array(values)\n                clim = (0, len(factors)-1)\n                categorical = True\n        else:\n            categorical = values.dtype.kind not in 'uif'\n\n        if self.logz:\n            if self.symmetric:\n                norm = mpl_colors.SymLogNorm(vmin=clim[0], vmax=clim[1],\n                                             linthresh=clim[1]/np.e)\n            else:\n                norm = mpl_colors.LogNorm(vmin=clim[0], vmax=clim[1])\n            opts[prefix+'norm'] = norm\n        opts[prefix+'vmin'] = clim[0]\n        opts[prefix+'vmax'] = clim[1]\n\n        cmap = opts.get(prefix+'cmap', opts.get('cmap', 'viridis'))\n        if values.dtype.kind not in 'OSUM':\n            ncolors = None\n            if isinstance(self.color_levels, int):\n                ncolors = self.color_levels\n            elif isinstance(self.color_levels, list):\n                ncolors = len(self.color_levels) - 1\n                if isinstance(cmap, list) and len(cmap) != ncolors:\n                    raise ValueError('The number of colors in the colormap '\n                                     'must match the intervals defined in the '\n                                     'color_levels, expected %d colors found %d.'\n                                     % (ncolors, len(cmap)))\n            try:\n                el_min, el_max = np.nanmin(values), np.nanmax(values)\n            except ValueError:\n                el_min, el_max = -np.inf, np.inf\n        else:\n            ncolors = clim[-1]+1\n            el_min, el_max = -np.inf, np.inf\n        vmin = -np.inf if opts[prefix+'vmin'] is None else opts[prefix+'vmin']\n        vmax = np.inf if opts[prefix+'vmax'] is None else opts[prefix+'vmax']\n        if el_min < vmin and el_max > vmax:\n            self._cbar_extend = 'both'\n        elif el_min < vmin:\n            self._cbar_extend = 'min'\n        elif el_max > vmax:\n            self._cbar_extend = 'max'\n\n        # Define special out-of-range colors on colormap\n        colors = {}\n        for k, val in self.clipping_colors.items():\n            if val == 'transparent':\n                colors[k] = {'color': 'w', 'alpha': 0}\n            elif isinstance(val, tuple):\n                colors[k] = {'color': val[:3],\n                             'alpha': val[3] if len(val) > 3 else 1}\n            elif isinstance(val, util.basestring):\n                color = val\n                alpha = 1\n                if color.startswith('#') and len(color) == 9:\n                    alpha = int(color[-2:], 16)/255.\n                    color = color[:-2]\n                colors[k] = {'color': color, 'alpha': alpha}\n\n        if not isinstance(cmap, mpl_colors.Colormap):\n            if isinstance(cmap, dict):\n                factors = util.unique_array(values)\n                palette = [cmap.get(f, colors.get('NaN', {'color': self._default_nan})['color'])\n                           for f in factors]\n            else:\n                palette = process_cmap(cmap, ncolors, categorical=categorical)\n                if isinstance(self.color_levels, list):\n                    palette, (vmin, vmax) = color_intervals(palette, self.color_levels, clip=(vmin, vmax))\n            cmap = mpl_colors.ListedColormap(palette)\n        if 'max' in colors: cmap.set_over(**colors['max'])\n        if 'min' in colors: cmap.set_under(**colors['min'])\n        if 'NaN' in colors: cmap.set_bad(**colors['NaN'])\n        opts[prefix+'cmap'] = cmap"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _adjust_legend(self, overlay, axis):\n        legend_data = []\n        dimensions = overlay.kdims\n        title = ', '.join([d.name for d in dimensions])\n        for key, subplot in self.subplots.items():\n            element = overlay.data.get(key, False)\n            if not subplot.show_legend or not element: continue\n            title = ', '.join([d.name for d in dimensions])\n            handle = subplot.traverse(lambda p: p.handles['artist'],\n                                      [lambda p: 'artist' in p.handles])\n            if isinstance(overlay, NdOverlay):\n                key = (dim.pprint_value(k) for k, dim in zip(key, dimensions))\n                label = ','.join([str(k) + dim.unit if dim.unit else str(k) for dim, k in\n                                  zip(dimensions, key)])\n                if handle:\n                    legend_data.append((handle, label))\n            else:\n                if isinstance(subplot, OverlayPlot):\n                    legend_data += subplot.handles.get('legend_data', {}).items()\n                elif element.label and handle:\n                    legend_data.append((handle, element.label))\n        all_handles, all_labels = list(zip(*legend_data)) if legend_data else ([], [])\n        data = OrderedDict()\n        used_labels = []\n        for handle, label in zip(all_handles, all_labels):\n            # Ensure that artists with multiple handles are supported\n            if isinstance(handle, list): handle = tuple(handle)\n            if handle and (handle not in data) and label and label not in used_labels:\n                data[handle] = label\n                used_labels.append(label)\n        if (not len(set(data.values())) > 0) or not self.show_legend:\n            legend = axis.get_legend()\n            if legend:\n                legend.set_visible(False)\n        else:\n            leg_spec = self.legend_specs[self.legend_position]\n            if self.legend_cols: leg_spec['ncol'] = self.legend_cols\n            leg = axis.legend(list(data.keys()), list(data.values()),\n                              title=title, scatterpoints=1,\n                              **dict(leg_spec, **self._fontsize('legend')))\n            title_fontsize = self._fontsize('legend_title')\n            if title_fontsize:\n                leg.get_title().set_fontsize(title_fontsize['fontsize'])\n            frame = leg.get_frame()\n            frame.set_facecolor('1.0')\n            frame.set_edgecolor('0.0')\n            frame.set_linewidth('1.0')\n            leg.set_zorder(10e6)\n            self.handles['legend'] = leg\n            self.handles['bbox_extra_artists'].append(leg)\n        self.handles['legend_data'] = data", "response": "Adjusts the legend handles and labels for all subplots and sets up the legend"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_overlay_label(cls, overlay, default_label=''):\n        if all(el.label==overlay.get(0).label for el in overlay):\n            return overlay.get(0).label\n        else:\n            return default_label", "response": "Returns a label if all the elements of an overlay agree on a\n        consistent label otherwise returns the default label."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_overlay_bounds(cls, overlay):\n        if all(el.bounds==overlay.get(0).bounds for el in overlay):\n            return overlay.get(0).bounds\n        else:\n            raise ValueError(\"Extents across the overlay are inconsistent\")", "response": "Returns the extents of the elements of an overlay that are consistent with the ones in the other overlay."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _apply(self, element, key=None):\n        kwargs = {}\n        for hook in self._preprocess_hooks:\n            kwargs.update(hook(self, element))\n        ret = self._process(element, key)\n        for hook in self._postprocess_hooks:\n            ret = hook(self, ret, **kwargs)\n        return ret", "response": "Applies the operation to the element executing any pre - and - post - processor hooks if defined."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses an element and return the corresponding object.", "response": "def process_element(self, element, key, **params):\n        \"\"\"\n        The process_element method allows a single element to be\n        operated on given an externally supplied key.\n        \"\"\"\n        self.p = param.ParamOverrides(self, params)\n        return self._apply(element, key)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef uniform(obj):\n    from .spaces import HoloMap\n    dim_groups = obj.traverse(lambda x: tuple(x.kdims),\n                              (HoloMap,))\n    if dim_groups:\n        dgroups = [frozenset(d.name for d in dg) for dg in dim_groups]\n        return all(g1 <= g2 or g1 >= g2 for g1 in dgroups for g2 in dgroups)\n    return True", "response": "Returns True if the object contains only the common dimension keys."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all common dimension keys in the object including subsets of dimensions.", "response": "def unique_dimkeys(obj, default_dim='Frame'):\n    \"\"\"\n    Finds all common dimension keys in the object including subsets of\n    dimensions. If there are is no common subset of dimensions, None\n    is returned.\n\n    Returns the list of dimensions followed by the list of unique\n    keys.\n    \"\"\"\n    from .ndmapping import NdMapping, item_check\n    from .spaces import HoloMap\n    key_dims = obj.traverse(lambda x: (tuple(x.kdims),\n                                       list(x.data.keys())), (HoloMap,))\n    if not key_dims:\n        return [Dimension(default_dim)], [(0,)]\n    dim_groups, keys = zip(*sorted(key_dims, key=lambda x: -len(x[0])))\n    dgroups = [frozenset(d.name for d in dg) for dg in dim_groups]\n    subset = all(g1 <= g2 or g1 >= g2 for g1 in dgroups for g2 in dgroups)\n    # Find unique keys\n    if subset:\n        dims = merge_dimensions(dim_groups)\n        all_dims = sorted(dims, key=lambda x: dim_groups[0].index(x))\n    else:\n        # Handle condition when HoloMap/DynamicMap dimensions do not overlap\n        hmaps = obj.traverse(lambda x: x, ['HoloMap'])\n        if hmaps:\n            raise ValueError('When combining HoloMaps into a composite plot '\n                             'their dimensions must be subsets of each other.')\n        dimensions = merge_dimensions(dim_groups)\n        dim_keys = {}\n        for dims, keys in key_dims:\n            for key in keys:\n                for d, k in zip(dims, key):\n                    dim_keys[d.name] = k\n        if dim_keys:\n            keys = [tuple(dim_keys.get(dim.name) for dim in dimensions)]\n        else:\n            keys = []\n        return merge_dimensions(dim_groups), keys\n\n    ndims = len(all_dims)\n    unique_keys = []\n    for group, keys in zip(dim_groups, keys):\n        dim_idxs = [all_dims.index(dim) for dim in group]\n        for key in keys:\n            padded_key = create_ndkey(ndims, dim_idxs, key)\n            matches = [item for item in unique_keys\n                       if padded_key == tuple(k if k is None else i\n                                              for i, k in zip(item, padded_key))]\n            if not matches:\n                unique_keys.append(padded_key)\n\n    with item_check(False):\n        sorted_keys = NdMapping({key: None for key in unique_keys},\n                                kdims=all_dims).data.keys()\n    return all_dims, list(sorted_keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of n - 1 mappings between two sets consecutive dimensions.", "response": "def hierarchical(keys):\n    \"\"\"\n    Iterates over dimension values in keys, taking two sets\n    of dimension values at a time to determine whether two\n    consecutive dimensions have a one-to-many relationship.\n    If they do a mapping between the first and second dimension\n    values is returned. Returns a list of n-1 mappings, between\n    consecutive dimensions.\n    \"\"\"\n    ndims = len(keys[0])\n    if ndims <= 1:\n        return True\n    dim_vals = list(zip(*keys))\n    combinations = (zip(*dim_vals[i:i+2])\n                    for i in range(ndims-1))\n    hierarchies = []\n    for combination in combinations:\n        hierarchy = True\n        store1 = defaultdict(list)\n        store2 = defaultdict(list)\n        for v1, v2 in combination:\n            if v2 not in store2[v1]:\n                store2[v1].append(v2)\n            previous = store1[v2]\n            if previous and previous[0] != v1:\n                hierarchy = False\n                break\n            if v1 not in store1[v2]:\n                store1[v2].append(v1)\n        hierarchies.append(store2 if hierarchy else {})\n    return hierarchies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninfer interval breaks from a given coordinate.", "response": "def _infer_interval_breaks(cls, coord, axis=0):\n        \"\"\"\n        >>> GridInterface._infer_interval_breaks(np.arange(5))\n        array([-0.5,  0.5,  1.5,  2.5,  3.5,  4.5])\n        >>> GridInterface._infer_interval_breaks([[0, 1], [3, 4]], axis=1)\n        array([[-0.5,  0.5,  1.5],\n               [ 2.5,  3.5,  4.5]])\n        \"\"\"\n        coord = np.asarray(coord)\n        if sys.version_info.major == 2 and len(coord) and isinstance(coord[0], (dt.datetime, dt.date)):\n            # np.diff does not work on datetimes in python 2\n            coord = coord.astype('datetime64')\n        if len(coord) == 0:\n            return np.array([], dtype=coord.dtype)\n        deltas = 0.5 * np.diff(coord, axis=axis)\n        first = np.take(coord, [0], axis=axis) - np.take(deltas, [0], axis=axis)\n        last = np.take(coord, [-1], axis=axis) + np.take(deltas, [-1], axis=axis)\n        trim_last = tuple(slice(None, -1) if n == axis else slice(None)\n                          for n in range(coord.ndim))\n        return np.concatenate([first, coord[trim_last] + deltas, last], axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coords(cls, dataset, dim, ordered=False, expanded=False, edges=False):\n        dim = dataset.get_dimension(dim, strict=True)\n        irregular = cls.irregular(dataset, dim)\n        if irregular or expanded:\n            if irregular:\n                data = dataset.data[dim.name]\n            else:\n                data = util.expand_grid_coords(dataset, dim)\n            if edges and data.shape == dataset.data[dataset.vdims[0].name].shape:\n                data = cls._infer_interval_breaks(data, axis=1)\n                data = cls._infer_interval_breaks(data, axis=0)\n            return data\n\n        data = dataset.data[dim.name]\n        if ordered and np.all(data[1:] < data[:-1]):\n            data = data[::-1]\n        shape = cls.shape(dataset, True)\n        if dim in dataset.kdims:\n            idx = dataset.get_dimension_index(dim)\n            isedges = (dim in dataset.kdims and len(shape) == dataset.ndims\n                       and len(data) == (shape[dataset.ndims-idx-1]+1))\n        else:\n            isedges = False\n        if edges and not isedges:\n            data = cls._infer_interval_breaks(data)\n        elif not edges and isedges:\n            data = data[:-1] + np.diff(data)/2.\n        return data", "response": "Returns the coordinates along a dimension."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef canonicalize(cls, dataset, data, data_coords=None, virtual_coords=[]):\n        if data_coords is None:\n            data_coords = dataset.dimensions('key', label='name')[::-1]\n\n        # Transpose data\n        dims = [name for name in data_coords\n                if isinstance(cls.coords(dataset, name), get_array_types())]\n        dropped = [dims.index(d) for d in dims\n                   if d not in dataset.kdims+virtual_coords]\n        if dropped:\n            data = np.squeeze(data, axis=tuple(dropped))\n\n        if not any(cls.irregular(dataset, d) for d in dataset.kdims):\n            inds = [dims.index(kd.name) for kd in dataset.kdims]\n            inds = [i - sum([1 for d in dropped if i>=d]) for i in inds]\n            if inds:\n                data = data.transpose(inds[::-1])\n\n        # Reorient data\n        invert = False\n        slices = []\n        for d in dataset.kdims[::-1]:\n            coords = cls.coords(dataset, d)\n            if np.all(coords[1:] < coords[:-1]) and not coords.ndim > 1:\n                slices.append(slice(None, None, -1))\n                invert = True\n            else:\n                slices.append(slice(None))\n        data = data[tuple(slices)] if invert else data\n\n        # Allow lower dimensional views into data\n        if len(dataset.kdims) < 2:\n            data = data.flatten()\n        return data", "response": "Canonicalize takes an array of values as input and reorients it to match the canonical format expected by the plotting functions."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample(cls, dataset, samples=[]):\n        ndims = dataset.ndims\n        dimensions = dataset.dimensions(label='name')\n        arrays = [dataset.data[vdim.name] for vdim in dataset.vdims]\n        data = defaultdict(list)\n\n        for sample in samples:\n            if np.isscalar(sample): sample = [sample]\n            if len(sample) != ndims:\n                sample = [sample[i] if i < len(sample) else None\n                          for i in range(ndims)]\n            sampled, int_inds = [], []\n            for d, ind in zip(dimensions, sample):\n                cdata = dataset.data[d]\n                mask = cls.key_select_mask(dataset, cdata, ind)\n                inds = np.arange(len(cdata)) if mask is None else np.argwhere(mask)\n                int_inds.append(inds)\n                sampled.append(cdata[mask])\n            for d, arr in zip(dimensions, np.meshgrid(*sampled)):\n                data[d].append(arr)\n            for vdim, array in zip(dataset.vdims, arrays):\n                da = dask_array_module()\n                flat_index = np.ravel_multi_index(tuple(int_inds)[::-1], array.shape)\n                if da and isinstance(array, da.Array):\n                    data[vdim.name].append(array.flatten().vindex[tuple(flat_index)])\n                else:\n                    data[vdim.name].append(array.flat[flat_index])\n        concatenated = {d: np.concatenate(arrays).flatten() for d, arrays in data.items()}\n        return concatenated", "response": "Samples the gridded data into dataset of samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating datasource with data for a new frame.", "response": "def _update_datasource(self, source, data):\n        \"\"\"\n        Update datasource with data for a new frame.\n        \"\"\"\n        if isinstance(source, ColumnDataSource):\n            if self.handles['static_source']:\n                source.trigger('data', source.data, data)\n            else:\n                source.data.update(data)\n        else:\n            source.graph_layout = data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_filled_edges(self, renderer, properties, edge_mapping):\n        \"Replace edge renderer with filled renderer\"\n        glyph_model = Patches if self.filled else Bezier\n        allowed_properties = glyph_model.properties()\n        for glyph_type in ('', 'selection_', 'nonselection_', 'hover_', 'muted_'):\n            glyph = getattr(renderer.edge_renderer, glyph_type+'glyph', None)\n            if glyph is None:\n                continue\n            group_properties = dict(properties)\n            props = self._process_properties(self.edge_glyph, group_properties, {})\n            filtered = self._filter_properties(props, glyph_type, allowed_properties)\n            new_glyph = glyph_model(**dict(filtered, **edge_mapping))\n            setattr(renderer.edge_renderer, glyph_type+'glyph', new_glyph)", "response": "Replace edge renderer with filled renderer"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the args and kwargs for the GraphRenderer", "response": "def _get_graph_properties(self, plot, element, data, mapping, ranges, style):\n        \"Computes the args and kwargs for the GraphRenderer\"\n        sources = []\n        properties, mappings = {}, {}\n\n        for key in ('scatter_1', self.edge_glyph):\n            gdata = data.pop(key, {})\n            group_style = dict(style)\n            style_group = self._style_groups.get('_'.join(key.split('_')[:-1]))\n\n            with abbreviated_exception():\n                group_style = self._apply_transforms(element, gdata, ranges, group_style, style_group)\n\n            # Get source\n            source = self._init_datasource(gdata)\n            self.handles[key+'_source'] = source\n            sources.append(source)\n\n            # Get style\n            others = [sg for sg in self._style_groups.values() if sg != style_group]\n            glyph_props = self._glyph_properties(\n                plot, element, source, ranges, group_style, style_group)\n            for k, p in glyph_props.items():\n                if any(k.startswith(o) for o in others):\n                    continue\n                properties[k] = p\n            mappings.update(mapping.pop(key, {}))\n        properties = {p: v for p, v in properties.items() if p not in ('legend', 'source')}\n        properties.update(mappings)\n\n        # Initialize graph layout\n        layout = data.pop('layout', {})\n        layout = StaticLayoutProvider(graph_layout=layout)\n        self.handles['layout_source'] = layout\n\n        return tuple(sources+[layout]), properties"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreorder renderers based on the defined draw order", "response": "def _reorder_renderers(self, plot, renderer, mapping):\n        \"Reorders renderers based on the defined draw order\"\n        renderers = dict({r: self.handles[r+'_glyph_renderer']\n                          for r in mapping}, graph=renderer)\n        other = [r for r in plot.renderers if r not in renderers.values()]\n        graph_renderers = [renderers[k] for k in self._draw_order if k in renderers]\n        plot.renderers = other + graph_renderers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the extents of the current Chord.", "response": "def get_extents(self, element, ranges, range_type='combined'):\n        \"\"\"\n        A Chord plot is always drawn on a unit circle.\n        \"\"\"\n        xdim, ydim = element.nodes.kdims[:2]\n        if range_type not in ('combined', 'data', 'extents'):\n            return xdim.range[0], ydim.range[0], xdim.range[1], ydim.range[1]\n        no_labels = (element.nodes.get_dimension(self.label_index) is None and\n                     self.labels is None)\n        rng = 1.1 if no_labels else 1.4\n        x0, x1 = max_range([xdim.range, (-rng, rng)])\n        y0, y1 = max_range([ydim.range, (-rng, rng)])\n        return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstrings sanitizer to avoid problematic characters in filenames.", "response": "def sanitizer(name, replacements=[(':','_'), ('/','_'), ('\\\\','_')]):\n    \"\"\"\n    String sanitizer to avoid problematic characters in filenames.\n    \"\"\"\n    for old,new in replacements:\n        name = name.replace(old,new)\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encode(cls, entry):\n        (data, info) = entry\n        if info['mime_type'] in cls.utf8_mime_types:\n            return data.encode('utf-8')\n        else:\n            return data", "response": "This method encodes the data of the entry in the appropriate encoding based on the mime - type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the file extension if not already present", "response": "def _filename(self_or_cls, filename):\n        \"Add the file extension if not already present\"\n        if not filename.endswith(self_or_cls.file_ext):\n            return '%s.%s' % (filename, self_or_cls.file_ext)\n        else:\n            return filename"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _merge_metadata(self_or_cls, obj, fn, *dicts):\n        merged = dict([(k,v) for d in dicts for (k,v) in d.items()])\n        return dict(merged, **fn(obj)) if fn else merged", "response": "Merges the supplied metadata info dictionary into the object s metadata info dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a list of file paths return a set of Collators that can be loaded from a given set of files.", "response": "def collect(self_or_cls, files, drop=[], metadata=True):\n        \"\"\"\n        Given a list or NdMapping type containing file paths return a\n        Layout of Collators, which can be called to load a given set\n        of files using the current Importer.\n\n        If supplied as a list each file is expected to disambiguate\n        itself with contained metadata. If an NdMapping type is\n        supplied additional key dimensions may be supplied as long as\n        they do not clash with the file metadata. Any key dimension\n        may be dropped by name by supplying a drop argument.\n        \"\"\"\n        aslist = not isinstance(files, (NdMapping, Element))\n        if isinstance(files, Element):\n            files = Collator(files)\n            file_kdims = files.kdims\n        else:\n            file_kdims = files.kdims\n        drop_extra = files.drop if isinstance(files, Collator) else []\n\n        mdata_dims = []\n        if metadata:\n            fnames = [fname[0] if isinstance(fname, tuple) else fname\n                      for fname in files.values()]\n            mdata_dims = {kdim for fname in fnames\n                          for kdim in self_or_cls.key(fname).keys()}\n        file_dims = set(files.dimensions('key', label=True))\n        added_dims = set(mdata_dims) - file_dims\n        overlap_dims = file_dims & set(mdata_dims)\n        kwargs = dict(kdims=file_kdims + sorted(added_dims),\n                      vdims=['filename', 'entries'],\n                      value_transform=self_or_cls.loader,\n                      drop=drop_extra + drop)\n        layout_data = defaultdict(lambda: Collator(None, **kwargs))\n\n        for key, fname in files.data.items():\n            fname = fname[0] if isinstance(fname, tuple) else fname\n            mdata = self_or_cls.key(fname) if metadata else {}\n            for odim in overlap_dims:\n                kval = key[files.get_dimension_index(odim)]\n                if kval != mdata[odim]:\n                    raise KeyError(\"Metadata supplies inconsistent \"\n                                   \"value for dimension %s\" % odim)\n            mkey = tuple(mdata.get(d, None) for d in added_dims)\n            key = mkey if aslist else key + mkey\n            if isinstance(fname, tuple) and len(fname) == 1:\n                (fname,) = fname\n            for entry in self_or_cls.entries(fname):\n                layout_data[entry][key] = (fname, [entry])\n        return Layout(layout_data.items())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_fields(cls, formatter):\n        \"Returns the format fields otherwise raise exception\"\n        if formatter is None: return []\n        try:\n            parse = list(string.Formatter().parse(formatter))\n            return  set(f for f in list(zip(*parse))[1] if f is not None)\n        except:\n            raise SyntaxError(\"Could not parse formatter %r\" % formatter)", "response": "Returns the format fields otherwise raise exception"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an entry to the archive.", "response": "def add(self, obj=None, filename=None, data=None, info={}, **kwargs):\n        \"\"\"\n        If a filename is supplied, it will be used. Otherwise, a\n        filename will be generated from the supplied object. Note that\n        if the explicit filename uses the {timestamp} field, it will\n        be formatted upon export.\n\n        The data to be archived is either supplied explicitly as\n        'data' or automatically rendered from the object.\n        \"\"\"\n        if [filename, obj] == [None, None]:\n            raise Exception(\"Either filename or a HoloViews object is \"\n                            \"needed to create an entry in the archive.\")\n        elif obj is None and not self.parse_fields(filename).issubset({'timestamp'}):\n            raise Exception(\"Only the {timestamp} formatter may be used unless an object is supplied.\")\n        elif [obj, data] == [None, None]:\n            raise Exception(\"Either an object or explicit data must be \"\n                            \"supplied to create an entry in the archive.\")\n        elif data and 'mime_type' not in info:\n            raise Exception(\"The mime-type must be supplied in the info dictionary \"\n                            \"when supplying data directly\")\n\n        self._validate_formatters()\n\n        entries = []\n        if data is None:\n            for exporter in self.exporters:\n                rendered = exporter(obj)\n                if rendered is None: continue\n                (data, new_info) = rendered\n                info = dict(info, **new_info)\n                entries.append((data, info))\n        else:\n            entries.append((data, info))\n\n        for (data, info) in entries:\n            self._add_content(obj, data, info, filename=filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind a unique name for a new file or key where existing is not None.", "response": "def _unique_name(self, basename, ext, existing, force=False):\n        \"\"\"\n        Find a unique basename for a new file/key where existing is\n        either a list of (basename, ext) pairs or an absolute path to\n        a directory.\n\n        By default, uniqueness is enforced depending on the state of\n        the unique_name parameter (for export names). If force is\n        True, this parameter is ignored and uniqueness is guaranteed.\n        \"\"\"\n        skip = False if force else (not self.unique_name)\n        if skip: return (basename, ext)\n        ext = '' if ext is None else ext\n        if isinstance(existing, str):\n            split = [os.path.splitext(el)\n                     for el in os.listdir(os.path.abspath(existing))]\n            existing = [(n, ex if not ex else ex[1:]) for (n, ex) in split]\n        new_name, counter = basename, 1\n        while (new_name, ext) in existing:\n            new_name = basename+'-'+str(counter)\n            counter += 1\n        return (sanitizer(new_name), ext)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, timestamp=None, info={}):\n        tval = tuple(time.localtime()) if timestamp is None else timestamp\n        tstamp = time.strftime(self.timestamp_format, tval)\n\n        info = dict(info, timestamp=tstamp)\n        export_name = self._format(self.export_name, info)\n        files = [((self._format(base, info), ext), val)\n                 for ((base, ext), val) in self._files.items()]\n        root = os.path.abspath(self.root)\n        # Make directory and populate if multiple files and not packed\n        if len(self) > 1 and not self.pack:\n            self._directory_archive(export_name, files, root)\n        elif len(files) == 1:\n            self._single_file_archive(export_name, files, root)\n        elif self.archive_format == 'zip':\n            self._zip_archive(export_name, files, root)\n        elif self.archive_format == 'tar':\n            self._tar_archive(export_name, files, root)\n        if self.flush_archive:\n            self._files = OrderedDict()", "response": "Export the archive directory or file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contents(self, maxlen=70):\n        \"Print the current (unexported) contents of the archive\"\n        lines = []\n        if len(self._files) == 0:\n            print(\"Empty %s\" % self.__class__.__name__)\n            return\n\n        fnames = [self._truncate_name(maxlen=maxlen, *k) for k in self._files]\n        max_len = max([len(f) for f in fnames])\n        for name,v in zip(fnames, self._files.values()):\n            mime_type = v[1].get('mime_type', 'no mime type')\n            lines.append('%s : %s' % (name.ljust(max_len), mime_type))\n        print('\\n'.join(lines))", "response": "Print the current ( unexported ) contents of the archive"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of filename entries currently in the archive", "response": "def listing(self):\n        \"Return a list of filename entries currently in the archive\"\n        return ['.'.join([f,ext]) if ext else f for (f,ext) in self._files.keys()]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __equalize_densities(self,nominal_bounds,nominal_density):\n        left,bottom,right,top = nominal_bounds.lbrt()\n        width = right-left; height = top-bottom\n        center_y = bottom + height/2.0\n        # True density is not equal to the nominal_density when\n        # nominal_density*(right-left) is not an integer.\n        true_density = int(nominal_density*(width))/float(width)\n\n        n_cells = round(height*true_density,0)\n        adjusted_half_height = n_cells/true_density/2.0\n\n        return (BoundingBox(points=((left, center_y-adjusted_half_height),\n                                    (right, center_y+adjusted_half_height))),\n                true_density)", "response": "Calculate the true density along x and adjust the top and bottom bounds so that the true density along y will be equal."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a point x y in Sheet coordinates to continuous matrix coordinates.", "response": "def sheet2matrix(self,x,y):\n        \"\"\"\n        Convert a point (x,y) in Sheet coordinates to continuous\n        matrix coordinates.\n\n        Returns (float_row,float_col), where float_row corresponds to\n        y, and float_col to x.\n\n        Valid for scalar or array x and y.\n\n        Note about Bounds For a Sheet with\n        BoundingBox(points=((-0.5,-0.5),(0.5,0.5))) and density=3,\n        x=-0.5 corresponds to float_col=0.0 and x=0.5 corresponds to\n        float_col=3.0.  float_col=3.0 is not inside the matrix\n        representing this Sheet, which has the three columns\n        (0,1,2). That is, x=-0.5 is inside the BoundingBox but x=0.5\n        is outside. Similarly, y=0.5 is inside (at row 0) but y=-0.5\n        is outside (at row 3) (it's the other way round for y because\n        the matrix row index increases as y decreases).\n        \"\"\"\n        # First translate to (left,top), which is [0,0] in the matrix,\n        # then scale to the size of the matrix. The y coordinate needs\n        # to be flipped, because the points are moving down in the\n        # sheet as the y index increases in the matrix.\n        xdensity = self.__xdensity\n        if ((isinstance(x, np.ndarray) and x.dtype.kind == 'M') or\n            isinstance(x, datetime_types)):\n            xdensity = np.timedelta64(int(round(1./xdensity)), self._time_unit)\n            float_col = (x-self.lbrt[0]) / xdensity\n        else:\n            float_col = (x-self.lbrt[0]) * xdensity\n\n        ydensity = self.__ydensity\n        if ((isinstance(y, np.ndarray) and y.dtype.kind == 'M') or\n            isinstance(y, datetime_types)):\n            ydensity = np.timedelta64(int(round(1./ydensity)), self._time_unit)\n            float_row = (self.lbrt[3]-y) / ydensity\n        else:\n            float_row = (self.lbrt[3]-y) * ydensity\n\n        return float_row, float_col"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a point x y in sheet coordinates to the integer row and column index of the matrix cell in which that point falls.", "response": "def sheet2matrixidx(self,x,y):\n        \"\"\"\n        Convert a point (x,y) in sheet coordinates to the integer row\n        and column index of the matrix cell in which that point falls,\n        given a bounds and density.  Returns (row,column).\n\n        Note that if coordinates along the right or bottom boundary\n        are passed into this function, the returned matrix coordinate\n        of the boundary will be just outside the matrix, because the\n        right and bottom boundaries are exclusive.\n\n        Valid for scalar or array x and y.\n        \"\"\"\n        r,c = self.sheet2matrix(x,y)\n        r = np.floor(r)\n        c = np.floor(c)\n\n        if hasattr(r,'astype'):\n            return r.astype(int), c.astype(int)\n        else:\n            return int(r),int(c)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef matrix2sheet(self,float_row,float_col):\n        xoffset = float_col*self.__xstep\n        if isinstance(self.lbrt[0], datetime_types):\n            xoffset = np.timedelta64(int(round(xoffset)), self._time_unit)\n        x = self.lbrt[0] + xoffset\n        yoffset = float_row*self.__ystep\n        if isinstance(self.lbrt[3], datetime_types):\n            yoffset = np.timedelta64(int(round(yoffset)), self._time_unit)\n        y = self.lbrt[3] - yoffset\n        return x, y", "response": "Convert a floating - point location to its corresponding location x y."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning x y coordinates where x and y are the floating point coordinates of the center of the given matrix cell row and col.", "response": "def matrixidx2sheet(self,row,col):\n        \"\"\"\n        Return (x,y) where x and y are the floating point coordinates\n        of the *center* of the given matrix cell (row,col). If the\n        matrix cell represents a 0.2 by 0.2 region, then the center\n        location returned would be 0.1,0.1.\n\n        NOTE: This is NOT the strict mathematical inverse of\n        sheet2matrixidx(), because sheet2matrixidx() discards all but\n        the integer portion of the continuous matrix coordinate.\n\n        Valid only for scalar or array row and col.\n        \"\"\"\n        x,y = self.matrix2sheet((row+0.5), (col+0.5))\n\n        # Rounding allows easier comparison with user specified values\n        if not isinstance(x, datetime_types):\n            x = np.around(x,10)\n        if not isinstance(y, datetime_types):\n            y = np.around(y,10)\n        return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive arbitrary sheet coordinates return the cell center of the closest unit.", "response": "def closest_cell_center(self,x,y):\n        \"\"\"\n        Given arbitrary sheet coordinates, return the sheet coordinates\n        of the center of the closest unit.\n        \"\"\"\n        return self.matrixidx2sheet(*self.sheet2matrixidx(x,y))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding the indices of a slice within an array of size sizefrom a given coordinate.", "response": "def findinputslice(coord, sliceshape, sheetshape):\n        \"\"\"\n        Gets the matrix indices of a slice within an array of size\n        sheetshape from a sliceshape, positioned at coord.\n        \"\"\"\n        center_row, center_col = coord\n        n_rows, n_cols = sliceshape\n        sheet_rows, sheet_cols = sheetshape\n\n        c1 = -min(0, center_col-n_cols/2)  # assuming odd shape (n_cols/2)\n        r1 = -min(0, center_row-n_rows/2)  # top and bottom\n        c2 = -max(-n_cols, center_col-sheet_cols-n_cols/2)\n        r2 = -max(-n_rows, center_row-sheet_rows-n_rows/2)\n\n        return (r1, r2, c1, c2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef positionlesscrop(self,x,y,sheet_coord_system):\n        slice_inds = self.findinputslice(\n            sheet_coord_system.sheet2matrixidx(x,y),\n            self.shape_on_sheet(), sheet_coord_system.shape)\n\n        self.set(slice_inds)", "response": "Positionless crop the ConnectionField to the given location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nposition the image to the given coordinates and store the result in the bounds attribute.", "response": "def positionedcrop(self,x,y,sheet_coord_system):\n        \"\"\"\n        Offset the bounds_template to this cf's location and store the\n        result in the 'bounds' attribute.\n\n        Also stores the input_sheet_slice for access by C.\n        \"\"\"\n        cf_row,cf_col = sheet_coord_system.sheet2matrixidx(x,y)\n        bounds_x,bounds_y=self.compute_bounds(sheet_coord_system).centroid()\n\n        b_row,b_col=sheet_coord_system.sheet2matrixidx(bounds_x,bounds_y)\n\n        row_offset = cf_row-b_row\n        col_offset = cf_col-b_col\n        self.translate(row_offset,col_offset)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef crop_to_sheet(self,sheet_coord_system):\n        \"Crop the slice to the SheetCoordinateSystem's bounds.\"\n        maxrow,maxcol = sheet_coord_system.shape\n\n        self[0] = max(0,self[0])\n        self[1] = min(maxrow,self[1])\n        self[2] = max(0,self[2])\n        self[3] = min(maxcol,self[3])", "response": "Crop the slice to the SheetCoordinateSystem s bounds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _createoddslicespec(bounds,scs,min_matrix_radius):\n        bounds_xcenter,bounds_ycenter=bounds.centroid()\n        sheet_rows,sheet_cols = scs.shape\n\n        center_row,center_col = sheet_rows/2,sheet_cols/2\n        unit_xcenter,unit_ycenter=scs.matrixidx2sheet(center_row,\n                                                      center_col)\n\n        bounds.translate(unit_xcenter-bounds_xcenter,\n                         unit_ycenter-bounds_ycenter)\n\n        r1,r2,c1,c2 = Slice._boundsspec2slicespec(bounds.lbrt(),scs)\n\n        xrad=max(c2-center_col-1,min_matrix_radius)\n        yrad=max(r2-center_row-1,min_matrix_radius)\n\n        r2=center_row+yrad+1\n        c2=center_col+xrad+1\n        r1=center_row-yrad\n        c1=center_col-xrad\n        return (r1,r2,c1,c2)", "response": "Create a slice that best approximates the bounds."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _boundsspec2slicespec(boundsspec,scs):\n        l,b,r,t = boundsspec\n\n        t_m,l_m = scs.sheet2matrix(l,t)\n        b_m,r_m = scs.sheet2matrix(r,b)\n\n        l_idx = int(np.ceil(l_m-0.5))\n        t_idx = int(np.ceil(t_m-0.5))\n        # CBENHANCEMENT: Python 2.6's math.trunc()?\n        r_idx = int(np.floor(r_m+0.5))\n        b_idx = int(np.floor(b_m+0.5))\n\n        return t_idx,b_idx,l_idx,r_idx", "response": "Convert an iterable boundsspec into a Slice specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an iterable slicespec into a BoundingRegion specification.", "response": "def _slicespec2boundsspec(slicespec,scs):\n        \"\"\"\n        Convert an iterable slicespec (supplying r1,r2,c1,c2 of a\n        Slice) into a BoundingRegion specification.\n\n        Exact inverse of _boundsspec2slicespec().\n        \"\"\"\n        r1,r2,c1,c2 = slicespec\n\n        left,bottom = scs.matrix2sheet(r2,c1)\n        right, top  = scs.matrix2sheet(r1,c2)\n\n        return ((left,bottom),(right,top))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the extents of the current Chord.", "response": "def get_extents(self, element, ranges, range_type='combined'):\n        \"\"\"\n        A Chord plot is always drawn on a unit circle.\n        \"\"\"\n        if range_type == 'extents':\n            return element.nodes.extents\n        xdim, ydim = element.nodes.kdims[:2]\n        xpad = .05 if self.label_index is None else 0.25\n        x0, x1 = ranges[xdim.name][range_type]\n        y0, y1 = ranges[ydim.name][range_type]\n        xdiff = (x1-x0)\n        ydiff = (y1-y0)\n        if self.label_position == 'right':\n            x0, x1 = x0-(0.05*xdiff), x1+xpad*xdiff\n        else:\n            x0, x1 = x0-xpad*xdiff, x1+(0.05*xdiff)\n        x0, x1 = max_range([xdim.range, (x0, x1)])\n        y0, y1 = max_range([ydim.range, (y0-(0.05*ydiff), y1+(0.05*ydiff))])\n        return (x0, y0, x1, y1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a dataset object and data in the appropriate format for the interface return a simple scalar.", "response": "def unpack_scalar(cls, dataset, data):\n        \"\"\"\n        Given a dataset object and data in the appropriate format for\n        the interface, return a simple scalar.\n        \"\"\"\n        if len(data) != 1 or len(data.columns) > 1:\n            return data\n        return data.iat[0,0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the data of a Dataset as a dataframe avoiding copying", "response": "def as_dframe(cls, dataset):\n        \"\"\"\n        Returns the data of a Dataset as a dataframe avoiding copying\n        if it already a dataframe type.\n        \"\"\"\n        if issubclass(dataset.interface, PandasInterface):\n            return dataset.data\n        else:\n            return dataset.dframe()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nescaping a list of values to a string converting to Formula unicode for safety.", "response": "def escape_vals(vals, escape_numerics=True):\n    \"\"\"\n    Escapes a list of values to a string, converting to\n    unicode for safety.\n    \"\"\"\n    # Ints formatted as floats to disambiguate with counter mode\n    ints, floats = \"%.1f\", \"%.10f\"\n\n    escaped = []\n    for v in vals:\n        if isinstance(v, np.timedelta64):\n            v = \"'\"+str(v)+\"'\"\n        elif isinstance(v, np.datetime64):\n            v = \"'\"+str(v.astype('datetime64[ns]'))+\"'\"\n        elif not isnumeric(v):\n            v = \"'\"+unicode(bytes_to_unicode(v))+\"'\"\n        else:\n            if v % 1 == 0:\n                v = ints % v\n            else:\n                v = (floats % v)[:-1]\n            if escape_numerics:\n                v = \"'\"+v+\"'\"\n        escaped.append(v)\n    return escaped"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_json(self, frames):\n        if self.json_save_path is None: return\n        path = os.path.join(self.json_save_path, '%s.json' % self.id)\n        if not os.path.isdir(self.json_save_path):\n            os.mkdir(self.json_save_path)\n        with open(path, 'w') as f:\n            json.dump(frames, f)\n        self.json_data = frames", "response": "Saves the given frames into a json file at the specified json_path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave the object to file.", "response": "def save(obj, filename, fmt='auto', backend=None, **kwargs):\n    \"\"\"\n    Saves the supplied object to file.\n\n    The available output formats depend on the backend being used. By\n    default and if the filename is a string the output format will be\n    inferred from the file extension. Otherwise an explicit format\n    will need to be specified. For ambiguous file extensions such as\n    html it may be necessary to specify an explicit fmt to override\n    the default, e.g. in the case of 'html' output the widgets will\n    default to fmt='widgets', which may be changed to scrubber widgets\n    using fmt='scrubber'.\n\n    Arguments\n    ---------\n    obj: HoloViews object\n        The HoloViews object to save to file\n    filename: string or IO object\n        The filename or BytesIO/StringIO object to save to\n    fmt: string\n        The format to save the object as, e.g. png, svg, html, or gif\n        and if widgets are desired either 'widgets' or 'scrubber'\n    backend: string\n        A valid HoloViews rendering backend, e.g. bokeh or matplotlib\n    **kwargs: dict\n        Additional keyword arguments passed to the renderer,\n        e.g. fps for animations\n    \"\"\"\n    backend = backend or Store.current_backend\n    renderer_obj = renderer(backend)\n    if kwargs:\n        renderer_obj = renderer_obj.instance(**kwargs)\n    if Path is not None and isinstance(filename, Path):\n        filename = str(filename.absolute())\n    if isinstance(filename, basestring):\n        supported = [mfmt for tformats in renderer_obj.mode_formats.values()\n                     for mformats in tformats.values() for mfmt in mformats]\n        formats = filename.split('.')\n        if fmt == 'auto' and formats and formats[-1] != 'html':\n            fmt = formats[-1]\n        if formats[-1] in supported:\n            filename = '.'.join(formats[:-1])\n    return renderer_obj.save(obj, filename, fmt=fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render(obj, backend=None, **kwargs):\n    backend = backend or Store.current_backend\n    renderer_obj = renderer(backend)\n    if kwargs:\n        renderer_obj = renderer_obj.instance(**kwargs)\n    plot = renderer_obj.get_plot(obj)\n    if backend == 'matplotlib' and len(plot) > 1:\n        return plot.anim(fps=renderer_obj.fps)\n    return renderer_obj.get_plot(obj).state", "response": "Renders the object to the corresponding object in the specified backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _group_kwargs_to_options(cls, obj, kwargs):\n        \"Format option group kwargs into canonical options format\"\n        groups = Options._option_groups\n        if set(kwargs.keys()) - set(groups):\n            raise Exception(\"Keyword options %s must be one of  %s\" % (groups,\n                            ','.join(repr(g) for g in groups)))\n        elif not all(isinstance(v, dict) for v in kwargs.values()):\n            raise Exception(\"The %s options must be specified using dictionary groups\" %\n                            ','.join(repr(k) for k in kwargs.keys()))\n\n        # Check whether the user is specifying targets (such as 'Image.Foo')\n        targets = [grp and all(k[0].isupper() for k in grp) for grp in kwargs.values()]\n        if any(targets) and not all(targets):\n            raise Exception(\"Cannot mix target specification keys such as 'Image' with non-target keywords.\")\n        elif not any(targets):\n            # Not targets specified - add current object as target\n            sanitized_group = util.group_sanitizer(obj.group)\n            if obj.label:\n                identifier = ('%s.%s.%s' % (\n                    obj.__class__.__name__, sanitized_group,\n                    util.label_sanitizer(obj.label)))\n            elif  sanitized_group != obj.__class__.__name__:\n                identifier = '%s.%s' % (obj.__class__.__name__, sanitized_group)\n            else:\n                identifier = obj.__class__.__name__\n\n            options = {identifier:{grp:kws for (grp,kws) in kwargs.items()}}\n        else:\n            dfltdict = defaultdict(dict)\n            for grp, entries in kwargs.items():\n                for identifier, kws in entries.items():\n                    dfltdict[identifier][grp] = kws\n            options = dict(dfltdict)\n        return options", "response": "Format option group kwargs into canonical options format"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the groups to a single specified backend", "response": "def _apply_groups_to_backend(cls, obj, options, backend, clone):\n        \"Apply the groups to a single specified backend\"\n        obj_handle = obj\n        if options is None:\n            if clone:\n                obj_handle = obj.map(lambda x: x.clone(id=None))\n            else:\n                obj.map(lambda x: setattr(x, 'id', None))\n        elif clone:\n            obj_handle = obj.map(lambda x: x.clone(id=x.id))\n\n        return StoreOptions.set_options(obj_handle, options, backend=backend)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _grouped_backends(cls, options, backend):\n        \"Group options by backend and filter out output group appropriately\"\n\n        if options is None:\n            return [(backend or Store.current_backend, options)]\n        dfltdict = defaultdict(dict)\n        for spec, groups in options.items():\n            if 'output' not in groups.keys() or len(groups['output'])==0:\n                dfltdict[backend or Store.current_backend][spec.strip()] = groups\n            elif set(groups['output'].keys()) - set(['backend']):\n                dfltdict[groups['output']['backend']][spec.strip()] = groups\n            elif ['backend'] == list(groups['output'].keys()):\n                filtered = {k:v for k,v in groups.items() if k != 'output'}\n                dfltdict[groups['output']['backend']][spec.strip()] = filtered\n            else:\n                raise Exception('The output options group must have the backend keyword')\n\n        return [(bk, bk_opts) for (bk, bk_opts) in dfltdict.items()]", "response": "Group options by backend and filter out output group appropriately"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_groups(cls, obj, options=None, backend=None, clone=True, **kwargs):\n        if isinstance(options, basestring):\n            from ..util.parser import OptsSpec\n            try:\n                options = OptsSpec.parse(options)\n            except SyntaxError:\n                options = OptsSpec.parse(\n                    '{clsname} {options}'.format(clsname=obj.__class__.__name__,\n                                                 options=options))\n        if kwargs:\n            options = cls._group_kwargs_to_options(obj, kwargs)\n\n        for backend, backend_opts in cls._grouped_backends(options, backend):\n            obj = cls._apply_groups_to_backend(obj, backend_opts, backend, clone)\n        return obj", "response": "Applies nested options definition grouped by type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndeprecate not expected to be used by any current code", "response": "def _cellmagic(cls, options, obj, strict=False):\n        \"Deprecated, not expected to be used by any current code\"\n        options, failure = cls._process_magic(options, strict)\n        if failure: return obj\n        if not isinstance(obj, Dimensioned):\n            return obj\n        else:\n            return StoreOptions.set_options(obj, options)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndeprecate not expected to be used by any current code", "response": "def _linemagic(cls, options, strict=False, backend=None):\n        \"Deprecated, not expected to be used by any current code\"\n        backends = None if backend is None else [backend]\n        options, failure = cls._process_magic(options, strict, backends=backends)\n        if failure: return\n        with options_policy(skip_invalid=True, warn_on_skip=False):\n            StoreOptions.apply_customizations(options, Store.options(backend=backend))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting default options for a session.", "response": "def defaults(cls, *options, **kwargs):\n        \"\"\"Set default options for a session.\n\n        Set default options for a session. whether in a Python script or\n        a Jupyter notebook.\n\n        Args:\n           *options: Option objects used to specify the defaults.\n           backend:  The plotting extension the options apply to\n        \"\"\"\n        if kwargs and len(kwargs) != 1 and list(kwargs.keys())[0] != 'backend':\n            raise Exception('opts.defaults only accepts \"backend\" keyword argument')\n\n        cls._linemagic(cls._expand_options(merge_options_to_dict(options)), backend=kwargs.get('backend'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a list of flat Option objects which may or may not have backend in their kwargs return a list of grouped backend", "response": "def _expand_by_backend(cls, options, backend):\n        \"\"\"\n        Given a list of flat Option objects which may or may not have\n        'backend' in their kwargs, return a list of grouped backend\n        \"\"\"\n        groups = defaultdict(list)\n        used_fallback = False\n        for obj in options:\n            if 'backend' in obj.kwargs:\n                opts_backend = obj.kwargs['backend']\n            elif backend is None:\n                opts_backend = Store.current_backend\n                obj.kwargs['backend']= opts_backend\n            else:\n                opts_backend = backend\n                obj.kwargs['backend'] = opts_backend\n                used_fallback = True\n            groups[opts_backend].append(obj)\n\n        if backend and not used_fallback:\n            cls.param.warning(\"All supplied Options objects already define a backend, \"\n                              \"backend override %r will be ignored.\" % backend)\n\n        return [(bk, cls._expand_options(o, bk)) for (bk, o) in groups.items()]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate and expands a dictionary of options indexed by type [ group ] [. label ] keys into separate style plot norm and output options.", "response": "def _expand_options(cls, options, backend=None):\n        \"\"\"\n        Validates and expands a dictionaries of options indexed by\n        type[.group][.label] keys into separate style, plot, norm and\n        output options.\n\n            opts._expand_options({'Image': dict(cmap='viridis', show_title=False)})\n\n        returns\n\n            {'Image': {'plot': dict(show_title=False), 'style': dict(cmap='viridis')}}\n        \"\"\"\n        current_backend = Store.current_backend\n        try:\n            backend_options = Store.options(backend=backend or current_backend)\n        except KeyError as e:\n            raise Exception('The %s backend is not loaded. Please load the backend using hv.extension.' % str(e))\n        expanded = {}\n        if isinstance(options, list):\n            options = merge_options_to_dict(options)\n\n        for objspec, options in options.items():\n            objtype = objspec.split('.')[0]\n            if objtype not in backend_options:\n                raise ValueError('%s type not found, could not apply options.'\n                                 % objtype)\n            obj_options = backend_options[objtype]\n            expanded[objspec] = {g: {} for g in obj_options.groups}\n            for opt, value in options.items():\n                found = False\n                valid_options = []\n                for g, group_opts in sorted(obj_options.groups.items()):\n                    if opt in group_opts.allowed_keywords:\n                        expanded[objspec][g][opt] = value\n                        found = True\n                        break\n                    valid_options += group_opts.allowed_keywords\n                if found: continue\n                cls._options_error(opt, objtype, backend, valid_options)\n        return expanded"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an error message for an invalid option suggesting similar options through fuzzy matching.", "response": "def _options_error(cls, opt, objtype, backend, valid_options):\n        \"\"\"\n        Generates an error message for an invalid option suggesting\n        similar options through fuzzy matching.\n        \"\"\"\n        current_backend = Store.current_backend\n        loaded_backends = Store.loaded_backends()\n        kws = Keywords(values=valid_options)\n        matches = sorted(kws.fuzzy_match(opt))\n        if backend is not None:\n            if matches:\n                raise ValueError('Unexpected option %r for %s type '\n                                 'when using the %r extension. Similar '\n                                 'options are: %s.' %\n                                 (opt, objtype, backend, matches))\n            else:\n                raise ValueError('Unexpected option %r for %s type '\n                                 'when using the %r extension. No '\n                                 'similar options founds.' %\n                                 (opt, objtype, backend))\n\n        # Check option is invalid for all backends\n        found = []\n        for lb in [b for b in loaded_backends if b != backend]:\n            lb_options = Store.options(backend=lb).get(objtype)\n            if lb_options is None:\n                continue\n            for g, group_opts in lb_options.groups.items():\n                if opt in group_opts.allowed_keywords:\n                    found.append(lb)\n        if found:\n            param.main.param.warning(\n                'Option %r for %s type not valid for selected '\n                'backend (%r). Option only applies to following '\n                'backends: %r' % (opt, objtype, current_backend, found))\n            return\n\n        if matches:\n            raise ValueError('Unexpected option %r for %s type '\n                             'across all extensions. Similar options '\n                             'for current extension (%r) are: %s.' %\n                             (opt, objtype, current_backend, matches))\n        else:\n            raise ValueError('Unexpected option %r for %s type '\n                             'across all extensions. No similar options '\n                             'found.' % (opt, objtype))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _builder_reprs(cls, options, namespace=None, ns=None):\n        if isinstance(options, basestring):\n            from .parser import OptsSpec\n            if ns is None:\n                try:     ns = get_ipython().user_ns  # noqa\n                except:  ns = globals()\n            options = options.replace('%%opts','').replace('%opts','')\n            options = OptsSpec.parse_options(options, ns=ns)\n\n\n        reprs = []\n        ns = '{namespace}.'.format(namespace=namespace) if namespace else ''\n        for option in options:\n            kws = ', '.join('%s=%r' % (k,option.kwargs[k]) for k in sorted(option.kwargs))\n            if '.' in option.key:\n                element = option.key.split('.')[0]\n                spec = repr('.'.join(option.key.split('.')[1:])) + ', '\n            else:\n                element = option.key\n                spec = ''\n\n            opts_format = '{ns}opts.{element}({spec}{kws})'\n            reprs.append(opts_format.format(ns=ns, spec=spec, kws=kws, element=element))\n        return reprs", "response": "Given a list of Option objects or an %opts or %%opts magic string return a list of corresponding option builder reprs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _element_keywords(cls, backend, elements=None):\n        \"Returns a dictionary of element names to allowed keywords\"\n        if backend not in Store.loaded_backends():\n            return {}\n\n        mapping = {}\n        backend_options = Store.options(backend)\n        elements = elements if elements is not None else backend_options.keys()\n        for element in elements:\n            if '.' in element: continue\n            element = element if isinstance(element, tuple) else (element,)\n            element_keywords = []\n            options = backend_options['.'.join(element)]\n            for group in Options._option_groups:\n                element_keywords.extend(options[group].allowed_keywords)\n\n            mapping[element[0]] = element_keywords\n        return mapping", "response": "Returns a dictionary of element names to allowed keywords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a list of streams that are attached to the input DynamicMap.", "response": "def _get_streams(self, map_obj, watch=True):\n        \"\"\"\n        Generates a list of streams to attach to the returned DynamicMap.\n        If the input is a DynamicMap any streams that are supplying values\n        for the key dimension of the input are inherited. And the list\n        of supplied stream classes and instances are processed and\n        added to the list.\n        \"\"\"\n        streams = []\n        for stream in self.p.streams:\n            if inspect.isclass(stream) and issubclass(stream, Stream):\n                stream = stream()\n            elif not (isinstance(stream, Stream) or util.is_param_method(stream)):\n                raise ValueError('Streams must be Stream classes or instances, found %s type' %\n                                 type(stream).__name__)\n            if isinstance(self.p.operation, Operation):\n                updates = {k: self.p.operation.p.get(k) for k, v in stream.contents.items()\n                           if v is None and k in self.p.operation.p}\n                if updates:\n                    reverse = {v: k for k, v in stream._rename.items()}\n                    stream.update(**{reverse.get(k, k): v for k, v in updates.items()})\n            streams.append(stream)\n\n        params = {k: v for k, v in self.p.kwargs.items() if isinstance(v, param.Parameter)\n                  and isinstance(v.owner, param.Parameterized)}\n        streams += Params.from_params(params)\n\n        # Inherit dimensioned streams\n        if isinstance(map_obj, DynamicMap):\n            dim_streams = util.dimensioned_streams(map_obj)\n            streams = list(util.unique_iterator(streams + dim_streams))\n\n        # If callback is a parameterized method and watch is disabled add as stream\n        has_dependencies = util.is_param_method(self.p.operation, has_deps=True)\n        if has_dependencies and watch:\n            streams.append(self.p.operation)\n\n        # Add any keyword arguments which are parameterized methods\n        # with dependencies as streams\n        for value in self.p.kwargs.values():\n            if util.is_param_method(value, has_deps=True):\n                streams.append(value)\n\n        valid, invalid = Stream._process_streams(streams)\n        if invalid:\n            msg = ('The supplied streams list contains objects that '\n                   'are not Stream instances: {objs}')\n            raise TypeError(msg.format(objs = ', '.join('%r' % el for el in invalid)))\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _eval_kwargs(self):\n        evaled_kwargs = {}\n        for k, v in self.p.kwargs.items():\n            if util.is_param_method(v):\n                v = v()\n            evaled_kwargs[k] = v\n        return evaled_kwargs", "response": "Evaluates any parameterized methods in the kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate function to dynamically apply the operation.", "response": "def _dynamic_operation(self, map_obj):\n        \"\"\"\n        Generate function to dynamically apply the operation.\n        Wraps an existing HoloMap or DynamicMap.\n        \"\"\"\n        if not isinstance(map_obj, DynamicMap):\n            def dynamic_operation(*key, **kwargs):\n                kwargs = dict(self._eval_kwargs(), **kwargs)\n                obj = map_obj[key] if isinstance(map_obj, HoloMap) else map_obj\n                return self._process(obj, key, kwargs)\n        else:\n            def dynamic_operation(*key, **kwargs):\n                kwargs = dict(self._eval_kwargs(), **kwargs)\n                return self._process(map_obj[key], key, kwargs)\n        if isinstance(self.p.operation, Operation):\n            return OperationCallable(dynamic_operation, inputs=[map_obj],\n                                     link_inputs=self.p.link_inputs,\n                                     operation=self.p.operation)\n        else:\n            return Callable(dynamic_operation, inputs=[map_obj],\n                            link_inputs=self.p.link_inputs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a DynamicMap from a HoloMap and a dynamic callback function creating an equivalent DynamicMap from the HoloMap.", "response": "def _make_dynamic(self, hmap, dynamic_fn, streams):\n        \"\"\"\n        Accepts a HoloMap and a dynamic callback function creating\n        an equivalent DynamicMap from the HoloMap.\n        \"\"\"\n        if isinstance(hmap, ViewableElement):\n            return DynamicMap(dynamic_fn, streams=streams)\n        dim_values = zip(*hmap.data.keys())\n        params = util.get_param_values(hmap)\n        kdims = [d(values=list(util.unique_iterator(values))) for d, values in\n                 zip(hmap.kdims, dim_values)]\n        return DynamicMap(dynamic_fn, streams=streams, **dict(params, kdims=kdims))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _trigger_refresh(self, key):\n        \"Triggers update to a plot on a refresh event\"\n        if self.top_level:\n            self.update(key)\n        else:\n            self.current_key = None\n            self.current_frame = None", "response": "Triggers update to a plot on a refresh event"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef norm(values, min=None, max=None):\n    min = np.min(values) if min is None else min\n    max = np.max(values) if max is None else max\n    return (values - min) / (max-min)", "response": "Unity - based normalization to scale data into 0 - 1 range."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbinning data into declared bins", "response": "def bin(values, bins, labels=None):\n    \"\"\"Bins data into declared bins\n\n    Bins data into declared bins. By default each bin is labelled\n    with bin center values but an explicit list of bin labels may be\n    defined.\n\n    Args:\n        values: Array of values to be binned\n        bins: List or array containing the bin boundaries\n        labels: List of labels to assign to each bin\n            If the bins are length N the labels should be length N-1\n\n    Returns:\n        Array of binned values\n    \"\"\"\n    bins = np.asarray(bins)\n    if labels is None:\n        labels = (bins[:-1] + np.diff(bins)/2.)\n    else:\n        labels = np.asarray(labels)\n    dtype = 'float' if labels.dtype.kind == 'f' else 'O'\n    binned = np.full_like(values, (np.nan if dtype == 'f' else None), dtype=dtype)\n    for lower, upper, label in zip(bins[:-1], bins[1:], labels):\n        condition = (values > lower) & (values <= upper)\n        binned[np.where(condition)[0]] = label\n    return binned"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef categorize(values, categories, default=None):\n    uniq_cats = list(unique_iterator(values))\n    cats = []\n    for c in values:\n        if isinstance(categories, list):\n            cat_ind = uniq_cats.index(c)\n            if cat_ind < len(categories):\n                cat = categories[cat_ind]\n            else:\n                cat = default\n        else:\n            cat = categories.get(c, default)\n        cats.append(cat)\n    return np.asarray(cats)", "response": "Maps discrete values to supplied categories."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbin continuous values. Bins continuous using the provided bins and assigns labels either computed from each bins center point or from the supplied labels. Args: bins: List or array containing the bin boundaries labels: List of labels to assign to each bin If the bins are length N the labels should be length N-1", "response": "def bin(self, bins, labels=None):\n        \"\"\"Bins continuous values.\n\n        Bins continuous using the provided bins and assigns labels\n        either computed from each bins center point or from the\n        supplied labels.\n\n        Args:\n            bins: List or array containing the bin boundaries\n            labels: List of labels to assign to each bin\n                If the bins are length N the labels should be length N-1\n        \"\"\"\n        return dim(self, bin, bins, labels=labels)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreplaces discrete values with supplied categories", "response": "def categorize(self, categories, default=None):\n        \"\"\"Replaces discrete values with supplied categories\n\n        Replaces discrete values in input array into a fixed set of\n        categories defined either as a list or dictionary.\n\n        Args:\n            categories: List or dict of categories to map inputs to\n            default: Default value to assign if value not in categories\n        \"\"\"\n        return dim(self, categorize, categories=categories, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef norm(self, limits=None):\n        kwargs = {}\n        if limits is not None:\n            kwargs = {'min': limits[0], 'max': limits[1]}\n        return dim(self, norm, **kwargs)", "response": "Unity - based normalization to scale data into 0 - 1 range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine whether this dim transform can be applied to the dataset i. e. whether all referenced dimensions can be resolved.", "response": "def applies(self, dataset):\n        \"\"\"\n        Determines whether the dim transform can be applied to the\n        Dataset, i.e. whether all referenced dimensions can be\n        resolved.\n        \"\"\"\n        if isinstance(self.dimension, dim):\n            applies = self.dimension.applies(dataset)\n        else:\n            applies = dataset.get_dimension(self.dimension) is not None\n            if isinstance(dataset, Graph) and not applies:\n                applies = dataset.nodes.get_dimension(self.dimension) is not None\n        for op in self.ops:\n            args = op.get('args')\n            if not args:\n                continue\n            for arg in args:\n                if isinstance(arg, dim):\n                    applies &= arg.applies(dataset)\n        return applies"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nevaluating the transform on the supplied dataset.", "response": "def apply(self, dataset, flat=False, expanded=None, ranges={}, all_values=False):\n        \"\"\"Evaluates the transform on the supplied dataset.\n\n        Args:\n            dataset: Dataset object to evaluate the expression on\n            flat: Whether to flatten the returned array\n            expanded: Whether to use the expanded expand values\n            ranges: Dictionary for ranges for normalization\n            all_values: Whether to evaluate on all values\n               Whether to evaluate on all available values, for some\n               element types, such as Graphs, this may include values\n               not included in the referenced column\n\n        Returns:\n            values: NumPy array computed by evaluating the expression\n        \"\"\"\n        dimension = self.dimension\n        if expanded is None:\n            expanded = not ((dataset.interface.gridded and dimension in dataset.kdims) or\n                            (dataset.interface.multi and dataset.interface.isscalar(dataset, dimension)))\n        if isinstance(dataset, Graph):\n            if dimension in dataset.kdims and all_values:\n                dimension = dataset.nodes.kdims[2]\n            dataset = dataset if dimension in dataset else dataset.nodes\n        data = dataset.dimension_values(dimension, expanded=expanded, flat=flat)\n        for o in self.ops:\n            args = o['args']\n            fn_args = [data]\n            for arg in args:\n                if isinstance(arg, dim):\n                    arg = arg.apply(dataset, flat, expanded, ranges, all_values)\n                fn_args.append(arg)\n            args = tuple(fn_args[::-1] if o['reverse'] else fn_args)\n            eldim = dataset.get_dimension(dimension)\n            drange = ranges.get(eldim.name, {})\n            drange = drange.get('combined', drange)\n            kwargs = o['kwargs']\n            if o['fn'] is norm and drange != {} and not ('min' in kwargs and 'max' in kwargs):\n                data = o['fn'](data, *drange)\n            else:\n                data = o['fn'](*args, **kwargs)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _compute_quads(self, element, data, mapping):\n        quad_mapping = {'left': 'x0', 'right': 'x1', 'bottom': 'y0', 'top': 'y1'}\n        quad_data = dict(data['scatter_1'])\n        quad_data.update({'x0': [], 'x1': [], 'y0': [], 'y1': []})\n        for node in element._sankey['nodes']:\n            quad_data['x0'].append(node['x0'])\n            quad_data['y0'].append(node['y0'])\n            quad_data['x1'].append(node['x1'])\n            quad_data['y1'].append(node['y1'])\n            data['scatter_1'].update(quad_data)\n        data['quad_1'] = data['scatter_1']\n        mapping['quad_1'] = quad_mapping", "response": "Computes the node quad glyph data. x\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_labels(self, element, data, mapping):\n        if element.vdims:\n            edges = Dataset(element)[element[element.vdims[0].name]>0]\n            nodes = list(np.unique([edges.dimension_values(i) for i in range(2)]))\n            nodes = element.nodes.select(**{element.nodes.kdims[2].name: nodes})\n        else:\n            nodes = element\n\n        label_dim = nodes.get_dimension(self.label_index)\n        labels = self.labels\n        if label_dim and labels:\n            if self.label_index not in [2, None]:\n                self.param.warning(\n                    \"Cannot declare style mapping for 'labels' option \"\n                    \"and declare a label_index; ignoring the label_index.\")\n        elif label_dim:\n            labels = label_dim\n        if isinstance(labels, basestring):\n            labels = element.nodes.get_dimension(labels)\n\n        if labels is None:\n            text = []\n        if isinstance(labels, dim):\n            text = labels.apply(element, flat=True)\n        else:\n            text = element.nodes.dimension_values(labels)\n            text = [labels.pprint_value(v) for v in text]\n\n        value_dim = element.vdims[0]\n        text_labels = []\n        for i, node in enumerate(element._sankey['nodes']):\n            if len(text):\n                label = text[i]\n            else:\n                label = ''\n            if self.show_values:\n                value = value_dim.pprint_value(node['value'])\n                if label:\n                    label = '%s - %s' % (label, value)\n                else:\n                    label = value\n            if value_dim.unit:\n                label += ' %s' % value_dim.unit\n            if label:\n                text_labels.append(label)\n\n        ys = nodes.dimension_values(1)\n        nodes = element._sankey['nodes']\n        if nodes:\n            offset = (nodes[0]['x1']-nodes[0]['x0'])/4.\n        else:\n            offset = 0\n        if self.label_position == 'right':\n            xs = np.array([node['x1'] for node in nodes])+offset\n        else:\n            xs = np.array([node['x0'] for node in nodes])-offset\n        data['text_1'] = dict(x=xs, y=ys, text=[str(l) for l in text_labels])\n        align = 'left' if self.label_position == 'right' else 'right'\n        mapping['text_1'] = dict(text='text', x='x', y='y', text_baseline='middle', text_align=align)", "response": "Compute the labels for the nodes and adds them to the data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _patch_hover(self, element, data):\n        if not (self.inspection_policy == 'edges' and 'hover' in self.handles):\n            return\n        lidx = element.nodes.get_dimension(self.label_index)\n        src, tgt = [dimension_sanitizer(kd.name) for kd in element.kdims[:2]]\n        if src == 'start': src += '_values'\n        if tgt == 'end':   tgt += '_values'\n        lookup = dict(zip(*(element.nodes.dimension_values(d) for d in (2, lidx))))\n        src_vals = data['patches_1'][src]\n        tgt_vals = data['patches_1'][tgt]\n        data['patches_1'][src] = [lookup.get(v, v) for v in src_vals]\n        data['patches_1'][tgt] = [lookup.get(v, v) for v in tgt_vals]", "response": "Patch hover data with label_index data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting a style by name.", "response": "def set_style(key):\n    \"\"\"\n    Select a style by name, e.g. set_style('default'). To revert to the\n    previous style use the key 'unset' or False.\n    \"\"\"\n    if key is None:\n        return\n    elif not key or key in ['unset', 'backup']:\n        if 'backup' in styles:\n            plt.rcParams.update(styles['backup'])\n        else:\n            raise Exception('No style backed up to restore')\n    elif key not in styles:\n        raise KeyError('%r not in available styles.')\n    else:\n        path = os.path.join(os.path.dirname(__file__), styles[key])\n        new_style = rc_params_from_file(path, use_default_template=False)\n        styles['backup'] = dict(plt.rcParams)\n\n        plt.rcParams.update(new_style)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef regexp_filter(self_or_cls, pattern):\n        def inner_filter(name, p):\n            name_match = re.search(pattern,name)\n            if name_match is not None:\n                return True\n            doc_match = re.search(pattern,p.doc)\n            if doc_match is not None:\n                return True\n            return False\n        return inner_filter", "response": "Builds a parameter filter using the supplied regular expression pattern"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget parameter information from the supplied class or object.", "response": "def get_parameter_info(cls, obj, ansi=False,  show_values=True,\n                           pattern=None, max_col_len=40):\n        \"\"\"\n        Get parameter information from the supplied class or object.\n        \"\"\"\n        if cls.ppager is None: return ''\n        if pattern is not None:\n            obj = ParamFilter(obj, ParamFilter.regexp_filter(pattern))\n            if len(obj.params()) <=1:\n                return None\n        param_info = cls.ppager.get_param_info(obj)\n        param_list = cls.ppager.param_docstrings(param_info)\n        if not show_values:\n            retval = cls.ansi_escape.sub('', param_list) if not ansi else param_list\n            return cls.highlight(pattern, retval)\n        else:\n            info = cls.ppager(obj)\n            if ansi is False:\n                info = cls.ansi_escape.sub('', info)\n            return cls.highlight(pattern, info)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn the supplied heading text into a suitable heading withan ansi and color.", "response": "def heading(cls, heading_text, char='=', level=0, ansi=False):\n        \"\"\"\n        Turn the supplied heading text into a suitable heading with\n        optional underline and color.\n        \"\"\"\n        heading_color = cls.headings[level] if ansi else '%s'\n        if char is None:\n            return heading_color % '%s\\n' % heading_text\n        else:\n            heading_ul = char*len(heading_text)\n            return heading_color % '%s\\n%s\\n%s' % (heading_ul, heading_text, heading_ul)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow information about an object in the given category.", "response": "def info(cls, obj, ansi=False, backend='matplotlib', visualization=True,\n             pattern=None, elements=[]):\n        \"\"\"\n        Show information about an object in the given category. ANSI\n        color codes may be enabled or disabled.\n        \"\"\"\n        cls.elements = elements\n        ansi_escape = re.compile(r'\\x1b[^m]*m')\n\n        isclass = isinstance(obj, type)\n        name = obj.__name__ if isclass  else obj.__class__.__name__\n        backend_registry = cls.store.registry.get(backend, {})\n        plot_class = backend_registry.get(obj if isclass else type(obj), None)\n        # Special case to handle PlotSelectors\n        if hasattr(plot_class, 'plot_classes'):\n            plot_class =  list(plot_class.plot_classes.values())[0]\n\n\n        if visualization is False or plot_class is None:\n            if pattern is not None:\n                obj = ParamFilter(obj, ParamFilter.regexp_filter(pattern))\n                if len(obj.params()) <=1:\n                    return ('No %r parameters found matching specified pattern %r'\n                            % (name, pattern))\n            info = param.ipython.ParamPager()(obj)\n            if ansi is False:\n                info = ansi_escape.sub('', info)\n            return cls.highlight(pattern, info)\n\n        heading = name if isclass else '{name}: {group} {label}'.format(name=name,\n                                                                        group=obj.group,\n                                                                        label=obj.label)\n        prefix = heading\n        lines = [prefix, cls.object_info(obj, name, backend=backend, ansi=ansi)]\n\n        if not isclass:\n            lines += ['', cls.target_info(obj, ansi=ansi)]\n        if plot_class is not None:\n            lines += ['', cls.options_info(plot_class, ansi, pattern=pattern)]\n        return \"\\n\".join(lines)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the type. group. label dotted information", "response": "def component_type(cls_or_slf, node):\n        \"Return the type.group.label dotted information\"\n        if node is None: return ''\n        return cls_or_slf.type_formatter.format(type=str(type(node).__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef node_info(cls_or_slf, node, attrpath, attrpaths, siblings, level, value_dims):\n        opts = None\n        if hasattr(node, 'children'):\n            (lvl, lines) = (level, [(level, cls_or_slf.component_type(node))])\n            opts = cls_or_slf.option_info(node)\n        elif hasattr(node, 'main'):\n            (lvl, lines) = cls_or_slf.adjointlayout_info(node, siblings, level, value_dims)\n        elif getattr(node, '_deep_indexable', False):\n            (lvl, lines) = cls_or_slf.ndmapping_info(node, siblings, level, value_dims)\n        elif hasattr(node, 'unit_format'):\n            (lvl, lines) = level, [(level, repr(node))]\n        else:\n            (lvl, lines) = cls_or_slf.element_info(node, siblings, level, value_dims)\n            opts = cls_or_slf.option_info(node)\n\n        # The attribute indexing path acts as a prefix (if applicable)\n        if attrpath is not None:\n            padding = cls_or_slf.padding(attrpaths)\n            (fst_lvl, fst_line) = lines[0]\n            line = '.'+attrpath.ljust(padding) +' ' + fst_line\n            lines[0] = (fst_lvl, line)\n        else:\n            fst_lvl = level\n\n        if cls_or_slf.show_options and opts and opts.kwargs:\n            lines += [(fst_lvl, l) for l in cls_or_slf.format_options(opts)]\n        return (lvl, lines)", "response": "Given a node return relevant information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef element_info(cls_or_slf, node, siblings, level, value_dims):\n        info = cls_or_slf.component_type(node)\n        if len(node.kdims) >= 1:\n            info += cls_or_slf.tab + '[%s]' % ','.join(d.name for d in node.kdims)\n        if value_dims and len(node.vdims) >= 1:\n            info += cls_or_slf.tab + '(%s)' % ','.join(d.name for d in node.vdims)\n        return level, [(level, info)]", "response": "Return the information summary for an Element. This consists of the dotted name followed by an value dimension names."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a Path type containing a single NaN separated path into a list of subpaths.", "response": "def split_path(path):\n    \"\"\"\n    Split a Path type containing a single NaN separated path into\n    multiple subpaths.\n    \"\"\"\n    path = path.split(0, 1)[0]\n    values = path.dimension_values(0)\n    splits = np.concatenate([[0], np.where(np.isnan(values))[0]+1, [None]])\n    subpaths = []\n    data = PandasInterface.as_dframe(path) if pd else path.array()\n    for i in range(len(splits)-1):\n        end = splits[i+1]\n        slc = slice(splits[i], None if end is None else end-1)\n        subpath = data.iloc[slc] if pd else data[slc]\n        if len(subpath):\n            subpaths.append(subpath)\n    return subpaths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_slice_bounds(slices, scs, shape):\n    xidx, yidx = slices\n    ys, xs = shape\n    l, b, r, t = scs.bounds.lbrt()\n    xdensity, ydensity = scs.xdensity, scs.ydensity\n    xunit = (1./xdensity)\n    yunit = (1./ydensity)\n    if isinstance(l, datetime_types):\n        xunit = np.timedelta64(int(round(xunit)), scs._time_unit)\n    if isinstance(b, datetime_types):\n        yunit = np.timedelta64(int(round(yunit)), scs._time_unit)\n    if isinstance(xidx, slice):\n        l = l if xidx.start is None else max(l, xidx.start)\n        r = r if xidx.stop is None else min(r, xidx.stop)\n    if isinstance(yidx, slice):\n        b = b if yidx.start is None else max(b, yidx.start)\n        t = t if yidx.stop is None else min(t, yidx.stop)\n    bounds = BoundingBox(points=((l, b), (r, t)))\n\n    # Apply new bounds\n    slc = Slice(bounds, scs)\n\n    # Apply scalar and list indices\n    l, b, r, t = slc.compute_bounds(scs).lbrt()\n    if not isinstance(xidx, slice):\n        if not isinstance(xidx, (list, set)): xidx = [xidx]\n        if len(xidx) > 1:\n            xdensity = xdensity*(float(len(xidx))/xs)\n        ls, rs = [], []\n        for idx in xidx:\n            xc, _ = scs.closest_cell_center(idx, b)\n            ls.append(xc-xunit/2)\n            rs.append(xc+xunit/2)\n        l, r = np.min(ls), np.max(rs)\n    elif not isinstance(yidx, slice):\n        if not isinstance(yidx, (set, list)): yidx = [yidx]\n        if len(yidx) > 1:\n            ydensity = ydensity*(float(len(yidx))/ys)\n        bs, ts = [], []\n        for idx in yidx:\n            _, yc = scs.closest_cell_center(l, idx)\n            bs.append(yc-yunit/2)\n            ts.append(yc+yunit/2)\n        b, t = np.min(bs), np.max(ts)\n    return BoundingBox(points=((l, b), (r, t)))", "response": "Given a 2D selection consisting of slices and shape of the array returns a new BoundingBox representing the sliced region."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef circular_layout(nodes):\n    N = len(nodes)\n    if not N:\n        return ([], [], [])\n    circ = np.pi/N*np.arange(N)*2\n    x = np.cos(circ)\n    y = np.sin(circ)\n    return (x, y, nodes)", "response": "Returns a list of nodes on a circle and add node index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes a quadratic bezier spline given start and end coordinate and two control points.", "response": "def quadratic_bezier(start, end, c0=(0, 0), c1=(0, 0), steps=50):\n    \"\"\"\n    Compute quadratic bezier spline given start and end coordinate and\n    two control points.\n    \"\"\"\n    steps = np.linspace(0, 1, steps)\n    sx, sy = start\n    ex, ey = end\n    cx0, cy0 = c0\n    cx1, cy1 = c1\n    xs = ((1-steps)**3*sx + 3*((1-steps)**2)*steps*cx0 +\n          3*(1-steps)*steps**2*cx1 + steps**3*ex)\n    ys = ((1-steps)**3*sy + 3*((1-steps)**2)*steps*cy0 +\n          3*(1-steps)*steps**2*cy1 + steps**3*ey)\n    return np.column_stack([xs, ys])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a Graph element containing abstract edges compute edge segments directly connecting the source and target nodes.", "response": "def connect_edges_pd(graph):\n    \"\"\"\n    Given a Graph element containing abstract edges compute edge\n    segments directly connecting the source and target nodes. This\n    operation depends on pandas and is a lot faster than the pure\n    NumPy equivalent.\n    \"\"\"\n    edges = graph.dframe()\n    edges.index.name = 'graph_edge_index'\n    edges = edges.reset_index()\n    nodes = graph.nodes.dframe()\n    src, tgt = graph.kdims\n    x, y, idx = graph.nodes.kdims[:3]\n\n    df = pd.merge(edges, nodes, left_on=[src.name], right_on=[idx.name])\n    df = df.rename(columns={x.name: 'src_x', y.name: 'src_y'})\n\n    df = pd.merge(df, nodes, left_on=[tgt.name], right_on=[idx.name])\n    df = df.rename(columns={x.name: 'dst_x', y.name: 'dst_y'})\n    df = df.sort_values('graph_edge_index').drop(['graph_edge_index'], axis=1)\n\n    edge_segments = []\n    for i, edge in df.iterrows():\n        start = edge['src_x'], edge['src_y']\n        end = edge['dst_x'], edge['dst_y']\n        edge_segments.append(np.array([start, end]))\n    return edge_segments"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect_edges(graph):\n    paths = []\n    for start, end in graph.array(graph.kdims):\n        start_ds = graph.nodes[:, :, start]\n        end_ds = graph.nodes[:, :, end]\n        if not len(start_ds) or not len(end_ds):\n            raise ValueError('Could not find node positions for all edges')\n        start = start_ds.array(start_ds.kdims[:2])\n        end = end_ds.array(end_ds.kdims[:2])\n        paths.append(np.array([start[0], end[0]]))\n    return paths", "response": "Given a Graph element containing abstract edges compute edgeTrees of the source and target nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the coordinates of the 2D aggregate and maintain the correct order.", "response": "def _get_coords(self, obj):\n        \"\"\"\n        Get the coordinates of the 2D aggregate, maintaining the correct\n        sorting order.\n        \"\"\"\n        xdim, ydim = obj.dimensions(label=True)[:2]\n        xcoords = obj.dimension_values(xdim, False)\n        ycoords = obj.dimension_values(ydim, False)\n\n        # Determine global orderings of y-values using topological sort\n        grouped = obj.groupby(xdim, container_type=OrderedDict,\n                              group_type=Dataset).values()\n        orderings = OrderedDict()\n        sort = True\n        for group in grouped:\n            vals = group.dimension_values(ydim, False)\n            if len(vals) == 1:\n                orderings[vals[0]] = [vals[0]]\n            else:\n                for i in range(len(vals)-1):\n                    p1, p2 = vals[i:i+2]\n                    orderings[p1] = [p2]\n            if sort:\n                if vals.dtype.kind in ('i', 'f'):\n                    sort = (np.diff(vals)>=0).all()\n                else:\n                    sort = np.array_equal(np.sort(vals), vals)\n        if sort or one_to_one(orderings, ycoords):\n            ycoords = np.sort(ycoords)\n        elif not is_cyclic(orderings):\n            coords = list(itertools.chain(*sort_topologically(orderings)))\n            ycoords = coords if len(coords) == len(ycoords) else np.sort(ycoords)\n        return xcoords, ycoords"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _aggregate_dataset(self, obj, xcoords, ycoords):\n        dim_labels = obj.dimensions(label=True)\n        vdims = obj.dimensions()[2:]\n        xdim, ydim = dim_labels[:2]\n        shape = (len(ycoords), len(xcoords))\n        nsamples = np.product(shape)\n        grid_data = {xdim: xcoords, ydim: ycoords}\n\n        ys, xs = cartesian_product([ycoords, xcoords], copy=True)\n        data = {xdim: xs, ydim: ys}\n        for vdim in vdims:\n            values = np.empty(nsamples)\n            values[:] = np.NaN\n            data[vdim.name] = values\n        dtype = default_datatype\n        dense_data = Dataset(data, kdims=obj.kdims, vdims=obj.vdims, datatype=[dtype])\n        concat_data = obj.interface.concatenate([dense_data, obj], datatype=dtype)\n        reindexed = concat_data.reindex([xdim, ydim], vdims)\n        if not reindexed:\n            agg = reindexed\n        elif pd:\n            df = PandasInterface.as_dframe(reindexed)\n            df = df.groupby([xdim, ydim], sort=False).first().reset_index()\n            agg = reindexed.clone(df)\n        else:\n            agg = reindexed.aggregate([xdim, ydim], reduce_fn)\n\n        # Convert data to a gridded dataset\n        for vdim in vdims:\n            grid_data[vdim.name] = agg.dimension_values(vdim).reshape(shape)\n        return agg.clone(grid_data, kdims=[xdim, ydim], vdims=vdims,\n                         datatype=self.p.datatype)", "response": "Generates a gridded Dataset from a column - based dataset and a list of xcoords and ycoords\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process(self, obj, key=None):\n        if isinstance(obj, Dataset) and obj.interface.gridded:\n            return obj\n        elif obj.ndims > 2:\n            raise ValueError(\"Cannot aggregate more than two dimensions\")\n        elif len(obj.dimensions()) < 3:\n            raise ValueError(\"Must have at two dimensions to aggregate over\"\n                             \"and one value dimension to aggregate on.\")\n\n        dtype = 'dataframe' if pd else 'dictionary'\n        obj = Dataset(obj, datatype=[dtype])\n        xcoords, ycoords = self._get_coords(obj)\n        return self._aggregate_dataset(obj, xcoords, ycoords)", "response": "Returns a 2D categorical 2D aggregate by inserting NaNs at all\n        cross - product locations that do not already have a value assigned."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a new plot object with the last available frame.", "response": "def initialize_plot(self, ranges=None, plot=None, plots=None, source=None):\n        \"\"\"\n        Initializes a new plot object with the last available frame.\n        \"\"\"\n        # Get element key and ranges for frame\n        element = self.hmap.last\n        key = self.keys[-1]\n        self.current_frame = element\n        self.current_key = key\n\n        style = self.lookup_options(element, 'style')[self.cyclic_index]\n        data, _, style = self.get_data(element, ranges, style)\n        if source is None:\n            source = self._init_datasource(data)\n        self.handles['source'] = self.handles['cds'] = source\n        self.handles['selected'] = source.selected\n\n        columns = self._get_columns(element, data)\n        style['reorderable'] = False\n        table = DataTable(source=source, columns=columns, height=self.height,\n                          width=self.width, **style)\n        self.handles['table'] = table\n        self.handles['glyph_renderer'] = table\n        self._execute_hooks(element)\n        self.drawn = True\n\n        title = self._get_title_div(self.keys[-1], '10pt')\n        if title:\n            plot = Column(title, table)\n            self.handles['title'] = title\n        else:\n            plot = table\n        self.handles['plot'] = plot\n\n        for cb in self.callbacks:\n            cb.initialize()\n        return plot"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating an existing plot with data corresponding to the key.", "response": "def update_frame(self, key, ranges=None, plot=None):\n        \"\"\"\n        Updates an existing plot with data corresponding\n        to the key.\n        \"\"\"\n        element = self._get_frame(key)\n        self._get_title_div(key, '12pt')\n\n        # Cache frame object id to skip updating data if unchanged\n        previous_id = self.handles.get('previous_id', None)\n        current_id = element._plot_id\n        self.handles['previous_id'] = current_id\n        self.static_source = (self.dynamic and (current_id == previous_id))\n        if (element is None or (not self.dynamic and self.static) or\n            (self.streaming and self.streaming[0].data is self.current_frame.data\n             and not self.streaming[0]._triggering) or self.static_source):\n            return\n        source = self.handles['source']\n        style = self.lookup_options(element, 'style')[self.cyclic_index]\n        data, _, style = self.get_data(element, ranges, style)\n        columns = self._get_columns(element, data)\n        self.handles['table'].columns = columns\n        self._update_datasource(source, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the values of a particular dimension.", "response": "def dimension_values(self, dim, expanded=True, flat=True):\n        \"\"\"\n        The set of samples available along a particular dimension.\n        \"\"\"\n        dim_idx = self.get_dimension_index(dim)\n        if not expanded and dim_idx == 0:\n            return np.array(range(self.data.shape[1]))\n        elif not expanded and dim_idx == 1:\n            return np.array(range(self.data.shape[0]))\n        elif dim_idx in [0, 1]:\n            values = np.mgrid[0:self.data.shape[1], 0:self.data.shape[0]][dim_idx]\n            return values.flatten() if flat else values\n        elif dim_idx == 2:\n            arr = self.data.T\n            return arr.flatten() if flat else arr\n        else:\n            return super(Raster, self).dimension_values(dim)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sample(self, samples=[], **sample_values):\n        if isinstance(samples, tuple):\n            X, Y = samples\n            samples = zip(X, Y)\n\n        params = dict(self.get_param_values(onlychanged=True),\n                      vdims=self.vdims)\n        params.pop('extents', None)\n        params.pop('bounds', None)\n        if len(sample_values) == self.ndims or len(samples):\n            if not len(samples):\n                samples = zip(*[c if isinstance(c, list) else [c] for _, c in\n                               sorted([(self.get_dimension_index(k), v) for k, v in\n                                       sample_values.items()])])\n            table_data = [c+(self._zdata[self._coord2matrix(c)],)\n                          for c in samples]\n            params['kdims'] = self.kdims\n            return Table(table_data, **params)\n        else:\n            dimension, sample_coord = list(sample_values.items())[0]\n            if isinstance(sample_coord, slice):\n                raise ValueError(\n                    'Raster sampling requires coordinates not slices,'\n                    'use regular slicing syntax.')\n            # Indices inverted for indexing\n            sample_ind = self.get_dimension_index(dimension)\n            if sample_ind is None:\n                raise Exception(\"Dimension %s not found during sampling\" % dimension)\n            other_dimension = [d for i, d in enumerate(self.kdims) if\n                               i != sample_ind]\n\n            # Generate sample slice\n            sample = [slice(None) for i in range(self.ndims)]\n            coord_fn = (lambda v: (v, 0)) if not sample_ind else (lambda v: (0, v))\n            sample[sample_ind] = self._coord2matrix(coord_fn(sample_coord))[abs(sample_ind-1)]\n\n            # Sample data\n            x_vals = self.dimension_values(other_dimension[0].name, False)\n            ydata = self._zdata[tuple(sample[::-1])]\n            if hasattr(self, 'bounds') and sample_ind == 0: ydata = ydata[::-1]\n            data = list(zip(x_vals, ydata))\n            params['kdims'] = other_dimension\n            return Curve(data, **params)", "response": "Sample the Raster along one or both of its dimensions returning a reduced dimensionality type a ItemTable Curve or Scatter."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreduces the Raster using functions provided via the reduce_map keyword arguments.", "response": "def reduce(self, dimensions=None, function=None, **reduce_map):\n        \"\"\"\n        Reduces the Raster using functions provided via the\n        kwargs, where the keyword is the dimension to be reduced.\n        Optionally a label_prefix can be provided to prepend to\n        the result Element label.\n        \"\"\"\n        function, dims = self._reduce_map(dimensions, function, reduce_map)\n        if len(dims) == self.ndims:\n            if isinstance(function, np.ufunc):\n                return function.reduce(self.data, axis=None)\n            else:\n                return function(self.data)\n        else:\n            dimension = dims[0]\n            other_dimension = [d for d in self.kdims if d.name != dimension]\n            oidx = self.get_dimension_index(other_dimension[0])\n            x_vals = self.dimension_values(other_dimension[0].name, False)\n            reduced = function(self._zdata, axis=oidx)\n            if oidx and hasattr(self, 'bounds'):\n                reduced = reduced[::-1]\n            data = zip(x_vals, reduced)\n            params = dict(dict(self.get_param_values(onlychanged=True)),\n                          kdims=other_dimension, vdims=self.vdims)\n            params.pop('bounds', None)\n            params.pop('extents', None)\n            return Table(data, **params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a clone of the object with matching parameter values containing the specified args and kwargs.", "response": "def clone(self, data=None, shared_data=True, new_type=None, link=True,\n              *args, **overrides):\n        \"\"\"\n        Returns a clone of the object with matching parameter values\n        containing the specified args and kwargs.\n\n        If shared_data is set to True and no data explicitly supplied,\n        the clone will share data with the original. May also supply\n        a new_type, which will inherit all shared parameters.\n        \"\"\"\n        if data is None and (new_type is None or issubclass(new_type, Image)):\n            sheet_params = dict(bounds=self.bounds, xdensity=self.xdensity,\n                                ydensity=self.ydensity)\n            overrides = dict(sheet_params, **overrides)\n        return super(Image, self).clone(data, shared_data, new_type, link,\n                                        *args, **overrides)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef select(self, selection_specs=None, **selection):\n        if selection_specs and not any(self.matches(sp) for sp in selection_specs):\n            return self\n\n        selection = {self.get_dimension(k).name: slice(*sel) if isinstance(sel, tuple) else sel\n                     for k, sel in selection.items() if k in self.kdims}\n        coords = tuple(selection[kd.name] if kd.name in selection else slice(None)\n                       for kd in self.kdims)\n\n        shape = self.interface.shape(self, gridded=True)\n        if any([isinstance(el, slice) for el in coords]):\n            bounds = compute_slice_bounds(coords, self, shape[:2])\n\n            xdim, ydim = self.kdims\n            l, b, r, t = bounds.lbrt()\n\n            # Situate resampled region into overall slice\n            y0, y1, x0, x1 = Slice(bounds, self)\n            y0, y1 = shape[0]-y1, shape[0]-y0\n            selection = (slice(y0, y1), slice(x0, x1))\n            sliced = True\n        else:\n            y, x = self.sheet2matrixidx(coords[0], coords[1])\n            y = shape[0]-y-1\n            selection = (y, x)\n            sliced = False\n\n        datatype = list(util.unique_iterator([self.interface.datatype]+self.datatype))\n        data = self.interface.ndloc(self, selection)\n        if not sliced:\n            if np.isscalar(data):\n                return data\n            elif isinstance(data, tuple):\n                data = data[self.ndims:]\n            return self.clone(data, kdims=[], new_type=Dataset,\n                              datatype=datatype)\n        else:\n            return self.clone(data, xdensity=self.xdensity, datatype=datatype,\n                              ydensity=self.ydensity, bounds=bounds)", "response": "Selects the data from the current object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sample(self, samples=[], **kwargs):\n        kwargs = {k: v for k, v in kwargs.items() if k != 'closest'}\n        if kwargs and samples:\n            raise Exception('Supply explicit list of samples or kwargs, not both.')\n        elif kwargs:\n            sample = [slice(None) for _ in range(self.ndims)]\n            for dim, val in kwargs.items():\n                sample[self.get_dimension_index(dim)] = val\n            samples = [tuple(sample)]\n\n        # If a 1D cross-section of 2D space return Curve\n        shape = self.interface.shape(self, gridded=True)\n        if len(samples) == 1:\n            dims = [kd for kd, v in zip(self.kdims, samples[0])\n                    if not (np.isscalar(v) or isinstance(v, util.datetime_types))]\n            if len(dims) == 1:\n                kdims = [self.get_dimension(kd) for kd in dims]\n                sample = tuple(np.datetime64(s) if isinstance(s, util.datetime_types) else s\n                               for s in samples[0])\n                sel = {kd.name: s for kd, s in zip(self.kdims, sample)}\n                dims = [kd for kd, v in sel.items() if not np.isscalar(v)]\n                selection = self.select(**sel)\n                selection = tuple(selection.columns(kdims+self.vdims).values())\n                datatype = list(util.unique_iterator(self.datatype+['dataframe', 'dict']))\n                return self.clone(selection, kdims=kdims, new_type=Curve,\n                                  datatype=datatype)\n            else:\n                kdims = self.kdims\n        else:\n            kdims = self.kdims\n\n        xs, ys = zip(*samples)\n        if isinstance(xs[0], util.datetime_types):\n            xs = np.array(xs).astype(np.datetime64)\n        if isinstance(ys[0], util.datetime_types):\n            ys = np.array(ys).astype(np.datetime64)\n        yidx, xidx = self.sheet2matrixidx(np.array(xs), np.array(ys))\n        yidx = shape[0]-yidx-1\n\n        # Detect out-of-bounds indices\n        out_of_bounds= (yidx<0) | (xidx<0) | (yidx>=shape[0]) | (xidx>=shape[1])\n        if out_of_bounds.any():\n            coords = [samples[idx] for idx in np.where(out_of_bounds)[0]]\n            raise IndexError('Coordinate(s) %s out of bounds for %s with bounds %s' %\n                             (coords, type(self).__name__, self.bounds.lbrt()))\n\n        data = self.interface.ndloc(self, (yidx, xidx))\n        return self.clone(data, new_type=Table, datatype=['dataframe', 'dictionary'])", "response": "Returns a new object containing the selected samples."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef closest(self, coords=[], **kwargs):\n        if kwargs and coords:\n            raise ValueError(\"Specify coordinate using as either a list \"\n                             \"keyword arguments not both\")\n        if kwargs:\n            coords = []\n            getter = []\n            for k, v in kwargs.items():\n                idx = self.get_dimension_index(k)\n                if np.isscalar(v):\n                    coords.append((0, v) if idx else (v, 0))\n                else:\n                    if isinstance(v, list):\n                        coords = [(0, c) if idx else (c, 0) for c in v]\n                    if len(coords) not in [0, len(v)]:\n                        raise ValueError(\"Length of samples must match\")\n                    elif len(coords):\n                        coords = [(t[abs(idx-1)], c) if idx else (c, t[abs(idx-1)])\n                                  for c, t in zip(v, coords)]\n                getter.append(idx)\n        else:\n            getter = [0, 1]\n        getter = itemgetter(*sorted(getter))\n        if len(coords) == 1:\n            coords = coords[0]\n        if isinstance(coords, tuple):\n            return getter(self.closest_cell_center(*coords))\n        else:\n            return [getter(self.closest_cell_center(*el)) for el in coords]", "response": "Given a single coordinate or multiple coordinates as\n        a tuple or list of tuples or keyword arguments matching\n        will find the closest actual x and y coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading an image from a PNG file.", "response": "def load_image(cls, filename, height=1, array=False, bounds=None, bare=False, **kwargs):\n        \"\"\"\n        Returns an raster element or raw numpy array from a PNG image\n        file, using matplotlib.\n\n        The specified height determines the bounds of the raster\n        object in sheet coordinates: by default the height is 1 unit\n        with the width scaled appropriately by the image aspect ratio.\n\n        Note that as PNG images are encoded as RGBA, the red component\n        maps to the first channel, the green component maps to the\n        second component etc. For RGB elements, this mapping is\n        trivial but may be important for subclasses e.g. for HSV\n        elements.\n\n        Setting bare=True will apply options disabling axis labels\n        displaying just the bare image. Any additional keyword\n        arguments will be passed to the Image object.\n        \"\"\"\n        try:\n            from matplotlib import pyplot as plt\n        except:\n            raise ImportError(\"RGB.load_image requires matplotlib.\")\n\n        data = plt.imread(filename)\n        if array:  return data\n\n        (h, w, _) = data.shape\n        if bounds is None:\n            f = float(height) / h\n            xoffset, yoffset = w*f/2, h*f/2\n            bounds=(-xoffset, -yoffset, xoffset, yoffset)\n        rgb = cls(data, bounds=bounds, **kwargs)\n        if bare: rgb = rgb(plot=dict(xaxis=None, yaxis=None))\n        return rgb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert from HSV to RGB.", "response": "def rgb(self):\n        \"\"\"\n        Conversion from HSV to RGB.\n        \"\"\"\n        coords = tuple(self.dimension_values(d, expanded=False)\n                       for d in self.kdims)\n        data = [self.dimension_values(d, flat=False)\n                for d in self.vdims]\n\n        hsv = self.hsv_to_rgb(*data[:3])\n        if len(self.vdims) == 4:\n            hsv += (data[3],)\n\n        params = util.get_param_values(self)\n        del params['vdims']\n        return RGB(coords+hsv, bounds=self.bounds,\n                   xdensity=self.xdensity, ydensity=self.ydensity,\n                   **params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef trimesh(self):\n        # Generate vertices\n        xs = self.interface.coords(self, 0, edges=True)\n        ys = self.interface.coords(self, 1, edges=True)\n\n        if xs.ndim == 1:\n            if np.all(xs[1:] < xs[:-1]):\n                xs = xs[::-1]\n            if np.all(ys[1:] < ys[:-1]):\n                ys = ys[::-1]\n            xs, ys = (np.tile(xs[:, np.newaxis], len(ys)).T,\n                      np.tile(ys[:, np.newaxis], len(xs)))\n        vertices = (xs.T.flatten(), ys.T.flatten())\n\n        # Generate triangle simplexes\n        shape = self.dimension_values(2, flat=False).shape\n        s0 = shape[0]\n        t1 = np.arange(np.product(shape))\n        js = (t1//s0)\n        t1s = js*(s0+1)+t1%s0\n        t2s = t1s+1\n        t3s = (js+1)*(s0+1)+t1%s0\n        t4s = t2s\n        t5s = t3s\n        t6s = t3s+1\n        t1 = np.concatenate([t1s, t6s])\n        t2 = np.concatenate([t2s, t5s])\n        t3 = np.concatenate([t3s, t4s])\n        ts = (t1, t2, t3)\n        for vd in self.vdims:\n            zs = self.dimension_values(vd)\n            ts = ts + (np.concatenate([zs, zs]),)\n\n        # Construct TriMesh\n        params = util.get_param_values(self)\n        params['kdims'] = params['kdims'] + TriMesh.node_type.kdims[2:]\n        nodes = TriMesh.node_type(vertices+(np.arange(len(vertices[0])),),\n                                  **{k: v for k, v in params.items()\n                                     if k != 'vdims'})\n        return TriMesh(((ts,), nodes), **{k: v for k, v in params.items()\n                                          if k != 'kdims'})", "response": "Converts a QuadMesh into a TriMesh."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef completions_sorting_key(cls, word):\n        \"Fixed version of IPyton.completer.completions_sorting_key\"\n        prio1, prio2 = 0, 0\n        if word.startswith('__'):  prio1 = 2\n        elif word.startswith('_'): prio1 = 1\n        if word.endswith('='):     prio1 = -1\n        if word.startswith('%%'):\n            if not \"%\" in word[2:]:\n                word = word[2:];   prio2 = 2\n        elif word.startswith('%'):\n            if not \"%\" in word[1:]:\n                word = word[1:];   prio2 = 1\n        return prio1, word, prio2", "response": "Fixed version of IPyton. completer. completions_sorting_key"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfind the list of resources from the keyword arguments and pops them out of the params dictionary.", "response": "def _get_resources(self, args, params):\n        \"\"\"\n        Finds the list of resources from the keyword parameters and pops\n        them out of the params dictionary.\n        \"\"\"\n        resources = []\n        disabled = []\n        for resource in ['holoviews'] + list(Store.renderers.keys()):\n            if resource in args:\n                resources.append(resource)\n\n            if resource in params:\n                setting = params.pop(resource)\n                if setting is True and resource != 'matplotlib':\n                    if resource not in resources:\n                        resources.append(resource)\n                if setting is False:\n                    disabled.append(resource)\n\n        unmatched_args = set(args) - set(resources)\n        if unmatched_args:\n            display(HTML('<b>Warning:</b> Unrecognized resources %s'\n                         % ', '.join(unmatched_args)))\n\n        resources = [r for r in resources if r not in disabled]\n        if ('holoviews' not in disabled) and ('holoviews' not in resources):\n            resources = ['holoviews'] + resources\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload HoloViews JS and CSS.", "response": "def load_hvjs(cls, logo=False, bokeh_logo=False, mpl_logo=False, plotly_logo=False,\n                  JS=True, message='HoloViewsJS successfully loaded.'):\n        \"\"\"\n        Displays javascript and CSS to initialize HoloViews widgets.\n        \"\"\"\n        import jinja2\n        # Evaluate load_notebook.html template with widgetjs code\n        if JS:\n            widgetjs, widgetcss = Renderer.html_assets(extras=False, backends=[], script=True)\n        else:\n            widgetjs, widgetcss = '', ''\n\n        # Add classic notebook MIME renderer\n        widgetjs += nb_mime_js\n\n        templateLoader = jinja2.FileSystemLoader(os.path.dirname(os.path.abspath(__file__)))\n        jinjaEnv = jinja2.Environment(loader=templateLoader)\n        template = jinjaEnv.get_template('load_notebook.html')\n        html = template.render({'widgetcss':   widgetcss,\n                                'logo':        logo,\n                                'bokeh_logo':  bokeh_logo,\n                                'mpl_logo':    mpl_logo,\n                                'plotly_logo': plotly_logo,\n                                'message':     message})\n        publish_display_data(data={'text/html': html})\n\n        # Vanilla JS mime type is only consumed by classic notebook\n        # Custom mime type is only consumed by JupyterLab\n        if JS:\n            mimebundle = {\n                MIME_TYPES['js']           : widgetjs,\n                MIME_TYPES['jlab-hv-load'] : widgetjs\n            }\n            if os.environ.get('HV_DOC_HTML', False):\n                mimebundle = {'text/html': mimebundle_to_html(mimebundle)}\n            publish_display_data(data=mimebundle)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tab_completion_docstring(self_or_cls):\n        elements = ['%s=Boolean' %k for k in list(Store.renderers.keys())]\n        for name, p in self_or_cls.params().items():\n            param_type = p.__class__.__name__\n            elements.append(\"%s=%s\" % (name, param_type))\n\n        return \"params(%s)\" % ', '.join(['holoviews=Boolean'] + elements)", "response": "Generates a docstring that can be used to enable tab - completion of resources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the dictionary of valid completions", "response": "def setup_completer(cls):\n        \"Get the dictionary of valid completions\"\n        try:\n            for element in Store.options().keys():\n                options = Store.options()['.'.join(element)]\n                plotkws = options['plot'].allowed_keywords\n                stylekws = options['style'].allowed_keywords\n                dotted = '.'.join(element)\n                cls._completions[dotted] = (plotkws, stylekws if stylekws else [])\n        except KeyError:\n            pass\n        return cls._completions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dotted_completion(cls, line, sorted_keys, compositor_defs):\n        completion_key, suggestions = None, []\n        tokens = [t for t in reversed(line.replace('.', ' ').split())]\n        for i, token in enumerate(tokens):\n            key_checks =[]\n            if i >= 0:  # Undotted key\n                key_checks.append(tokens[i])\n            if i >= 1:  # Single dotted key\n                key_checks.append('.'.join([key_checks[-1], tokens[i-1]]))\n            if i >= 2:  # Double dotted key\n                key_checks.append('.'.join([key_checks[-1], tokens[i-2]]))\n            # Check for longest potential dotted match first\n            for key in reversed(key_checks):\n                if key in sorted_keys:\n                    completion_key = key\n                    depth = completion_key.count('.')\n                    suggestions = [k.split('.')[depth+1] for k in sorted_keys\n                                   if k.startswith(completion_key+'.')]\n                    return completion_key, suggestions\n            # Attempting to match compositor definitions\n            if token in compositor_defs:\n                completion_key = compositor_defs[token]\n                break\n        return completion_key, suggestions", "response": "Given a dotted line of text return the completion key and suggestions for further completion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef option_completer(cls, k,v):\n        \"Tab completion hook for the %%opts cell magic.\"\n        line = v.text_until_cursor\n        completions = cls.setup_completer()\n        compositor_defs = {el.group:el.output_type.__name__\n                           for el in Compositor.definitions if el.group}\n        return cls.line_completer(line, completions, compositor_defs)", "response": "Tab completion hook for the %%opts cell magic."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef link(self):\n        if self.source in self.registry:\n            links = self.registry[self.source]\n            params = {\n                k: v for k, v in self.get_param_values() if k != 'name'}\n            for link in links:\n                link_params = {\n                    k: v for k, v in link.get_param_values() if k != 'name'}\n                if (type(link) is type(self) and link.source is self.source\n                    and link.target is self.target and params == link_params):\n                    return\n            self.registry[self.source].append(self)\n        else:\n            self.registry[self.source] = [self]", "response": "Registers the Link in the Link registry"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_factors(self, element):\n        if not element.kdims:\n            xfactors, yfactors = [element.label], []\n        else:\n            factors = [key for key in element.groupby(element.kdims).data.keys()]\n            if element.ndims > 1:\n                factors = sorted(factors)\n            factors = [tuple(d.pprint_value(k) for d, k in zip(element.kdims, key))\n                       for key in factors]\n            factors = [f[0] if len(f) == 1 else f for f in factors]\n            xfactors, yfactors = factors, []\n        return (yfactors, xfactors) if self.invert_axes else (xfactors, yfactors)", "response": "Get factors for categorical axes."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of map elements from given order to bins of start and end values for radius or angle dimension.", "response": "def _get_bins(self, kind, order, reverse=False):\n        \"\"\"\n        Map elements from given `order` array to bins of start and end values\n        for radius or angle dimension.\n        \"\"\"\n\n        if kind == \"radius\":\n            start = self.max_radius * self.radius_inner\n            end = self.max_radius\n\n        elif kind == \"angle\":\n            start = self.start_angle\n            end = self.start_angle + 2 * np.pi\n\n        bounds = np.linspace(start, end, len(order) + 1)\n        bins = np.vstack([bounds[:-1], bounds[1:]]).T\n\n        if reverse:\n            bins = bins[::-1]\n\n        return dict(zip(order, bins))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_bounds(mapper, values):\n\n        array = np.array([mapper.get(x) for x in values])\n        return array[:, 0], array[:, 1]", "response": "Extract first and second values from tuples of mapped bins."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverride the default postprocessing for the hover tool.", "response": "def _postprocess_hover(self, renderer, source):\n        \"\"\"\n        Limit hover tool to annular wedges only.\n        \"\"\"\n\n        if isinstance(renderer.glyph, AnnularWedge):\n            super(RadialHeatMapPlot, self)._postprocess_hover(renderer, source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsupplies custom static extents because radial heatmaps always have the same boundaries.", "response": "def get_extents(self, view, ranges, range_type='combined'):\n        \"\"\"Supply custom, static extents because radial heatmaps always have\n        the same boundaries.\n        \"\"\"\n        if range_type not in ('data', 'combined'):\n            return (None,)*4\n        lower = -self.radius_outer\n        upper = 2 * self.max_radius + self.radius_outer\n        return (lower, lower, upper, upper)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_default_mapping(self, z, cmapper):\n\n        map_annular = dict(x=self.max_radius, y=self.max_radius,\n                           inner_radius=\"inner_radius\",\n                           outer_radius=\"outer_radius\",\n                           start_angle=\"start_angle\",\n                           end_angle=\"end_angle\",\n                           fill_color={'field': z, 'transform': cmapper})\n\n        map_seg_label = dict(x=\"x\", y=\"y\", text=\"text\",\n                             angle=\"angle\", text_align=\"center\")\n\n        map_ann_label = dict(x=\"x\", y=\"y\", text=\"text\",\n                             angle=\"angle\", text_align=\"center\",\n                             text_baseline=\"bottom\")\n\n        map_xmarks = dict(xs=\"xs\", ys=\"ys\")\n\n        map_ymarks = dict(x= self.max_radius, y=self.max_radius,\n                          start_angle=0, end_angle=2*np.pi, radius=\"radius\")\n\n        return {'annular_wedge_1': map_annular,\n                'text_1': map_seg_label,\n                'text_2': map_ann_label,\n                'multi_line_1': map_xmarks,\n                'arc_1': map_ymarks}", "response": "Create dictionary containing default ColumnDataSource glyph to data\n        mappings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pprint(self, element, dim_label, vals):\n\n        if vals.dtype.kind not in 'SU':\n            dim = element.gridded.get_dimension(dim_label)\n            return [dim.pprint_value(v) for v in vals]\n\n        return vals", "response": "Helper function to convert values to corresponding dimension type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_tick_mapping(self, kind, order, bins):\n\n        if kind == \"angle\":\n            ticks = self.xticks\n            reverse = True\n        elif kind == \"radius\":\n            ticks = self.yticks\n            reverse = False\n\n        if callable(ticks):\n            text_nth = [x for x in order if ticks(x)]\n\n        elif isinstance(ticks, (tuple, list)):\n            bins = self._get_bins(kind, ticks, reverse)\n            text_nth = ticks\n\n        elif ticks:\n            nth_label = np.ceil(len(order) / float(ticks)).astype(int)\n            text_nth = order[::nth_label]\n\n        return {x: bins[x] for x in text_nth}", "response": "Helper function to compute tick mappings based on ticks and bins."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate ColumnDataSource dictionary for segment labels.", "response": "def _get_seg_labels_data(self, order_seg, bins_seg):\n        \"\"\"Generate ColumnDataSource dictionary for segment labels.\n\n        \"\"\"\n\n        if self.xticks is None:\n            return dict(x=[], y=[], text=[], angle=[])\n\n        mapping = self._compute_tick_mapping(\"angle\", order_seg, bins_seg)\n\n        values = [(text, ((end - start) / 2) + start)\n                  for text, (start, end) in mapping.items()]\n\n        labels, radiant = zip(*values)\n        radiant = np.array(radiant)\n\n        y_coord = np.sin(radiant) * self.max_radius + self.max_radius\n        x_coord = np.cos(radiant) * self.max_radius + self.max_radius\n\n        return dict(x=x_coord,\n                    y=y_coord,\n                    text=labels,\n                    angle=1.5 * np.pi + radiant)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_ann_labels_data(self, order_ann, bins_ann):\n\n        if self.yticks is None:\n            return dict(x=[], y=[], text=[], angle=[])\n\n        mapping = self._compute_tick_mapping(\"radius\", order_ann, bins_ann)\n        values = [(label, radius[0]) for label, radius in mapping.items()]\n\n        labels, radius = zip(*values)\n        radius = np.array(radius)\n\n        y_coord = np.sin(np.deg2rad(self.yrotation)) * radius + self.max_radius\n        x_coord = np.cos(np.deg2rad(self.yrotation)) * radius + self.max_radius\n\n        return dict(x=x_coord,\n                    y=y_coord,\n                    text=labels,\n                    angle=[0]*len(labels))", "response": "Generate ColumnDataSource dictionary for annular labels."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_markers(marks, order, bins):\n\n        if callable(marks):\n            markers = [x for x in order if marks(x)]\n        elif isinstance(marks, list):\n            markers = [order[x] for x in marks]\n        elif isinstance(marks, tuple):\n            markers = marks\n        else:\n            nth_mark = np.ceil(len(order) / marks).astype(int)\n            markers = order[::nth_mark]\n\n        return np.array([bins[x][1] for x in markers])", "response": "Helper function to get marker positions depending on mark type."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_xmarks_data(self, order_seg, bins_seg):\n\n        if not self.xmarks:\n            return dict(xs=[], ys=[])\n\n        angles = self._get_markers(self.xmarks, order_seg, bins_seg)\n\n        inner = self.max_radius * self.radius_inner\n        outer = self.max_radius\n\n        y_start = np.sin(angles) * inner + self.max_radius\n        y_end = np.sin(angles) * outer + self.max_radius\n\n        x_start = np.cos(angles) * inner + self.max_radius\n        x_end = np.cos(angles) * outer + self.max_radius\n\n        xs = zip(x_start, x_end)\n        ys = zip(y_start, y_end)\n\n        return dict(xs=list(xs), ys=list(ys))", "response": "Generate ColumnDataSource dictionary for segment separation lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates ColumnDataSource dictionary for segment separation lines.", "response": "def _get_ymarks_data(self, order_ann, bins_ann):\n        \"\"\"Generate ColumnDataSource dictionary for segment separation lines.\n\n        \"\"\"\n\n        if not self.ymarks:\n            return dict(radius=[])\n\n        radius = self._get_markers(self.ymarks, order_ann, bins_ann)\n\n        return dict(radius=radius)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge(cls, trees):\n        first = trees[0]\n        for tree in trees:\n            first.update(tree)\n        return first", "response": "Merge a collection of AttrTree objects into a single AttrTree object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path(self):\n        \"Returns the path up to the root for the current node.\"\n        if self.parent:\n            return '.'.join([self.parent.path, str(self.identifier)])\n        else:\n            return self.identifier if self.identifier else self.__class__.__name__", "response": "Returns the path up to the root for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the contents of the current AttrTree with the contents of the second AttrTree.", "response": "def update(self, other):\n        \"\"\"\n        Updated the contents of the current AttrTree with the\n        contents of a second AttrTree.\n        \"\"\"\n        if not isinstance(other, AttrTree):\n            raise Exception('Can only update with another AttrTree type.')\n        fixed_status = (self.fixed, other.fixed)\n        (self.fixed, other.fixed) = (False, False)\n        for identifier, element in other.items():\n            if identifier not in self.data:\n                self[identifier] = element\n            else:\n                self[identifier].update(element)\n        (self.fixed, other.fixed) = fixed_status"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the given value at the supplied path.", "response": "def set_path(self, path, val):\n        \"\"\"\n        Set the given value at the supplied path where path is either\n        a tuple of strings or a string in A.B.C format.\n        \"\"\"\n        path = tuple(path.split('.')) if isinstance(path , str) else tuple(path)\n\n        disallowed = [p for p in path if not type(self)._sanitizer.allowable(p)]\n        if any(disallowed):\n            raise Exception(\"Attribute strings in path elements cannot be \"\n                            \"correctly escaped : %s\" % ','.join(repr(el) for el in disallowed))\n        if len(path) > 1:\n            attrtree = self.__getattr__(path[0])\n            attrtree.set_path(path[1:], val)\n        else:\n            self.__setattr__(path[0], val)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter(self, path_filters):\n        if not path_filters: return self\n\n        # Convert string path filters\n        path_filters = [tuple(pf.split('.')) if not isinstance(pf, tuple)\n                        else pf for pf in path_filters]\n\n        # Search for substring matches between paths and path filters\n        new_attrtree = self.__class__()\n        for path, item in self.data.items():\n            if any([all([subpath in path for subpath in pf]) for pf in path_filters]):\n                new_attrtree.set_path(path, item)\n\n        return new_attrtree", "response": "Filters the loaded AttrTree using the supplied path_filters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _propagate(self, path, val):\n        if val == '_DELETE':\n            if path in self.data:\n                del self.data[path]\n            else:\n                items = [(key, v) for key, v in self.data.items()\n                         if not all(k==p for k, p in zip(key, path))]\n                self.data = OrderedDict(items)\n        else:\n            self.data[path] = val\n        if self.parent is not None:\n            self.parent._propagate((self.identifier,)+path, val)", "response": "Propagate the value up to the root node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, identifier, default=None):\n        split_label = (tuple(identifier.split('.'))\n                       if isinstance(identifier, str) else tuple(identifier))\n        if len(split_label) == 1:\n            identifier = split_label[0]\n            return self.__dict__.get(identifier, default)\n        path_item = self\n        for identifier in split_label:\n            if path_item == default or path_item is None:\n                return default\n            path_item = path_item.get(identifier, default)\n        return path_item", "response": "Get a node of the AttrTree using its path string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pop(self, identifier, default=None):\n        if identifier in self.children:\n            item = self[identifier]\n            self.__delitem__(identifier)\n            return item\n        else:\n            return default", "response": "Pop a node from the AttrTree using its path string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _filter_msg(self, msg, ids):\n        filtered_msg = {}\n        for k, v in msg.items():\n            if isinstance(v, dict) and 'id' in v:\n                if v['id'] in ids:\n                    filtered_msg[k] = v['value']\n            else:\n                filtered_msg[k] = v\n        return filtered_msg", "response": "Filter out the event values that do not originate from the plotting\n        ids associated with a particular stream using their\n        ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all requested plotting handles and cache them along with the IDs of the models that will be attached to the corresponding stream.", "response": "def _init_plot_handles(self):\n        \"\"\"\n        Find all requested plotting handles and cache them along\n        with the IDs of the models the callbacks will be attached to.\n        \"\"\"\n        plots = [self.plot]\n        if self.plot.subplots:\n            plots += list(self.plot.subplots.values())\n\n        handles = {}\n        for plot in plots:\n            for k, v in plot.handles.items():\n                handles[k] = v\n        self.plot_handles = handles\n\n        requested = {}\n        for h in self.models+self.extra_models:\n            if h in self.plot_handles:\n                requested[h] = handles[h]\n            elif h in self.extra_models:\n                print(\"Warning %s could not find the %s model. \"\n                      \"The corresponding stream may not work.\"\n                      % (type(self).__name__, h))\n        self.handle_ids.update(self._get_stream_handle_ids(requested))\n\n        return requested"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_stream_handle_ids(self, handles):\n        stream_handle_ids = defaultdict(dict)\n        for stream in self.streams:\n            for h in self.models:\n                if h in handles:\n                    handle_id = handles[h].ref['id']\n                    stream_handle_ids[stream][h] = handle_id\n        return stream_handle_ids", "response": "Gather the ids of the plotting handles attached to this callback"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating JS code to look up attributes on JS objects from a dictionary of attributes specification dictionary.", "response": "def attributes_js(cls, attributes):\n        \"\"\"\n        Generates JS code to look up attributes on JS objects from\n        an attributes specification dictionary. If the specification\n        references a plotting particular plotting handle it will also\n        generate JS code to get the ID of the object.\n\n        Simple example (when referencing cb_data or cb_obj):\n\n        Input  : {'x': 'cb_data.geometry.x'}\n\n        Output : data['x'] = cb_data['geometry']['x']\n\n        Example referencing plot handle:\n\n        Input  : {'x0': 'x_range.attributes.start'}\n\n        Output : if ((x_range !== undefined)) {\n                    data['x0'] = {id: x_range['id'], value: x_range['attributes']['start']}\n                 }\n        \"\"\"\n        assign_template = '{assign}{{id: {obj_name}[\"id\"], value: {obj_name}{attr_getters}}};\\n'\n        conditional_template = 'if (({obj_name} != undefined)) {{ {assign} }}'\n        code = ''\n        for key, attr_path in sorted(attributes.items()):\n            data_assign = 'data[\"{key}\"] = '.format(key=key)\n            attrs = attr_path.split('.')\n            obj_name = attrs[0]\n            attr_getters = ''.join(['[\"{attr}\"]'.format(attr=attr)\n                                    for attr in attrs[1:]])\n            if obj_name not in ['cb_obj', 'cb_data']:\n                assign_str = assign_template.format(\n                    assign=data_assign, obj_name=obj_name, attr_getters=attr_getters\n                )\n                code += conditional_template.format(\n                    obj_name=obj_name, assign=assign_str\n                )\n            else:\n                assign_str = ''.join([data_assign, obj_name, attr_getters, ';\\n'])\n                code += assign_str\n        return code"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates a CustomJS callback that will send the requested data to python", "response": "def get_customjs(self, references, plot_id=None):\n        \"\"\"\n        Creates a CustomJS callback that will send the requested\n        attributes back to python.\n        \"\"\"\n        # Generate callback JS code to get all the requested data\n        if plot_id is None:\n            plot_id = self.plot.id or 'PLACEHOLDER_PLOT_ID'\n        self_callback = self.js_callback.format(comm_id=self.comm.id,\n                                                timeout=self.timeout,\n                                                debounce=self.debounce,\n                                                plot_id=plot_id)\n\n        attributes = self.attributes_js(self.attributes)\n        conditions = [\"%s\" % cond for cond in self.skip]\n        conditional = ''\n        if conditions:\n            conditional = 'if (%s) { return };\\n' % (' || '.join(conditions))\n        data = \"var data = {};\\n\"\n        code = conditional + data + attributes + self.code + self_callback\n        return CustomJS(args=references, code=code)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_customjs_callback(self, js_callback, handle):\n        if self.on_events:\n            for event in self.on_events:\n                handle.js_on_event(event, js_callback)\n        if self.on_changes:\n            for change in self.on_changes:\n                handle.js_on_change(change, js_callback)\n        elif hasattr(handle, 'callback'):\n            handle.callback = js_callback", "response": "Sets a customJS callback for the given handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resolve_attr_spec(cls, spec, cb_obj, model=None):\n        if not cb_obj:\n            raise Exception('Bokeh plot attribute %s could not be found' % spec)\n        if model is None:\n            model = cb_obj\n        spec = spec.split('.')\n        resolved = cb_obj\n        for p in spec[1:]:\n            if p == 'attributes':\n                continue\n            if isinstance(resolved, dict):\n                resolved = resolved.get(p)\n            else:\n                resolved = getattr(resolved, p, None)\n        return {'id': model.ref['id'], 'value': resolved}", "response": "Resolves a callback attribute specification looking up the corresponding attribute up on the cb_obj which should be a bokeh model."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses change events adding timeout to process multiple concerted value change at once.", "response": "def on_change(self, attr, old, new):\n        \"\"\"\n        Process change events adding timeout to process multiple concerted\n        value change at once rather than firing off multiple plot updates.\n        \"\"\"\n        self._queue.append((attr, old, new))\n        if not self._active and self.plot.document:\n            self.plot.document.add_timeout_callback(self.process_on_change, 50)\n            self._active = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprocesses bokeh UIEvents adding timeout to process multiple concerted value change at once.", "response": "def on_event(self, event):\n        \"\"\"\n        Process bokeh UIEvents adding timeout to process multiple concerted\n        value change at once rather than firing off multiple plot updates.\n        \"\"\"\n        self._queue.append((event))\n        if not self._active and self.plot.document:\n            self.plot.document.add_timeout_callback(self.process_on_event, 50)\n            self._active = True"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process_on_event(self):\n        if not self._queue:\n            self._active = False\n            return\n        # Get unique event types in the queue\n        events = list(OrderedDict([(event.event_name, event)\n                                   for event in self._queue]).values())\n        self._queue = []\n\n        # Process event types\n        for event in events:\n            msg = {}\n            for attr, path in self.attributes.items():\n                model_obj = self.plot_handles.get(self.models[0])\n                msg[attr] = self.resolve_attr_spec(path, event, model_obj)\n            self.on_msg(msg)\n        self.plot.document.add_timeout_callback(self.process_on_event, 50)", "response": "Process the queue of events and trigger the appropriate callback."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset up the callback function for bokeh server interactions.", "response": "def set_server_callback(self, handle):\n        \"\"\"\n        Set up on_change events for bokeh server interactions.\n        \"\"\"\n        if self.on_events:\n            for event in self.on_events:\n                handle.on_event(event, self.on_event)\n        if self.on_changes:\n            for change in self.on_changes:\n                if change in ['patching', 'streaming']:\n                    # Patch and stream events do not need handling on server\n                    continue\n                handle.on_change(change, self.on_change)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_out_of_bounds(self, value, start, end):\n        \"Clips out of bounds values\"\n        if isinstance(value, np.datetime64):\n            v = dt64_to_dt(value)\n            if isinstance(start, (int, float)):\n                start = convert_timestamp(start)\n            if isinstance(end, (int, float)):\n                end = convert_timestamp(end)\n            s, e = start, end\n            if isinstance(s, np.datetime64):\n                s = dt64_to_dt(s)\n            if isinstance(e, np.datetime64):\n                e = dt64_to_dt(e)\n        else:\n            v, s, e = value, start, end\n\n        if v < s:\n            value = start\n        elif v > e:\n            value = end\n\n        return value", "response": "Clips out of bounds values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds any Links on the supplied plot and returns a list of tuples containing the source and target ones.", "response": "def find_links(cls, root_plot):\n        \"\"\"\n        Traverses the supplied plot and searches for any Links on\n        the plotted objects.\n        \"\"\"\n        plot_fn = lambda x: isinstance(x, GenericElementPlot) and not isinstance(x, GenericOverlayPlot)\n        plots = root_plot.traverse(lambda x: x, [plot_fn])\n        potentials = [cls.find_link(plot) for plot in plots]\n        source_links = [p for p in potentials if p is not None]\n        found = []\n        for plot, links in source_links:\n            for link in links:\n                if not link._requires_target:\n                    # If link has no target don't look further\n                    found.append((link, plot, None))\n                    continue\n                potentials = [cls.find_link(p, link) for p in plots]\n                tgt_links = [p for p in potentials if p is not None]\n                if tgt_links:\n                    found.append((link, plot, tgt_links[0][0]))\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching a GenericElementPlot for a Link.", "response": "def find_link(cls, plot, link=None):\n        \"\"\"\n        Searches a GenericElementPlot for a Link.\n        \"\"\"\n        registry = Link.registry.items()\n        for source in plot.link_sources:\n            if link is None:\n                links = [\n                    l for src, links in registry for l in links\n                    if src is source or (src._plot_id is not None and\n                                         src._plot_id == source._plot_id)]\n                if links:\n                    return (plot, links)\n            else:\n                if ((link.target is source) or\n                    (link.target is not None and\n                     link.target._plot_id is not None and\n                     link.target._plot_id == source._plot_id)):\n                    return (plot, [link])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the values along the requested dimension.", "response": "def dimension_values(self, dimension, expanded=True, flat=True):\n        \"\"\"Return the values along the requested dimension.\n\n        Args:\n            dimension: The dimension to return values for\n            expanded (bool, optional): Whether to expand values\n            flat (bool, optional): Whether to flatten array\n\n        Returns:\n            NumPy array of values along the requested dimension\n        \"\"\"\n        index = self.get_dimension_index(dimension)\n        if index == 0:\n            return np.array([self.data if np.isscalar(self.data) else self.data[index]])\n        elif index == 1:\n            return [] if np.isscalar(self.data) else np.array([self.data[1]])\n        else:\n            return super(Annotation, self).dimension_values(dimension)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef clone(self, data=None, shared_data=True, new_type=None, *args, **overrides):\n        return Element2D.clone(self, data, shared_data, new_type,\n                               *args, **overrides)", "response": "Clones the object overriding data and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dimension_values(self, dimension, expanded=True, flat=True):\n        index = self.get_dimension_index(dimension)\n        if index in [0, 1]:\n            return np.array([point[index] for point in self.data[0]])\n        else:\n            return super(Spline, self).dimension_values(dimension)", "response": "Return the values along the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef dimension_values(self, dimension, expanded=True, flat=True):\n        index = self.get_dimension_index(dimension)\n        if index == 0:\n            return np.array([self.x])\n        elif index == 1:\n            return np.array([self.y])\n        else:\n            return super(Arrow, self).dimension_values(dimension)", "response": "Return the values along the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the formatted cell value for the given row and column indices.", "response": "def pprint_cell(self, row, col):\n        \"\"\"\n        Get the formatted cell value for the given row and column indices.\n        \"\"\"\n        if col > 2:\n            raise Exception(\"Only two columns available in a ItemTable.\")\n        elif row >= self.rows:\n            raise Exception(\"Maximum row index is %d\" % self.rows-1)\n        elif col == 0:\n            return self.dimensions('value')[row].pprint_label\n        else:\n            dim = self.get_dimension(row)\n            heading = self.vdims[row]\n            return dim.pprint_value(self.data.get(heading.name, np.NaN))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef values(self):\n        self.param.warning('ItemTable values method is deprecated.')\n        return tuple(self.data.get(d.name, np.NaN)\n                     for d in self.vdims)", "response": "Returns a tuple of the values of the ItemTable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nhook to process the object currently being displayed.", "response": "def process_object(obj):\n    \"Hook to process the object currently being displayed.\"\n    invalid_options = OptsMagic.process_element(obj)\n    if invalid_options: return invalid_options\n    OutputMagic.info(obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn plot renderer and format for single frame export.", "response": "def single_frame_plot(obj):\n    \"\"\"\n    Returns plot, renderer and format for single frame export.\n    \"\"\"\n    obj = Layout.from_values(obj) if isinstance(obj, AdjointLayout) else obj\n\n    backend = Store.current_backend\n    renderer = Store.renderers[backend]\n\n    plot_cls = renderer.plotting_class(obj)\n    plot = plot_cls(obj, **renderer.plot_options(obj, renderer.size))\n    fmt = renderer.params('fig').objects[0] if renderer.fig == 'auto' else renderer.fig\n    return plot, renderer, fmt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef middle_frame(obj):\n    \"Only display the (approximately) middle frame of an animated plot\"\n    plot, renderer, fmt = single_frame_plot(obj)\n    middle_frame = int(len(plot) / 2)\n    plot.update(middle_frame)\n    return {'text/html': renderer.html(plot, fmt)}", "response": "Only display the ( approximimately ) middle frame of an animated plot"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef display(obj, raw_output=False, **kwargs):\n    if not Store.loaded_backends() and isinstance(obj, Dimensioned):\n        raise RuntimeError('To use display on a HoloViews object ensure '\n                           'a backend is loaded using the holoviews '\n                           'extension.')\n\n    raw = True\n    if isinstance(obj, GridSpace):\n        with option_state(obj):\n            output = grid_display(obj)\n    elif isinstance(obj, (CompositeOverlay, ViewableElement)):\n        with option_state(obj):\n            output = element_display(obj)\n    elif isinstance(obj, (Layout, NdLayout, AdjointLayout)):\n        with option_state(obj):\n            output = layout_display(obj)\n    elif isinstance(obj, (HoloMap, DynamicMap)):\n        with option_state(obj):\n            output = map_display(obj)\n    elif isinstance(obj, Plot):\n        output = render(obj)\n    else:\n        output = obj\n        raw = kwargs.pop('raw', False)\n\n    if raw_output:\n        return output\n    elif isinstance(output, tuple):\n        data, metadata = output\n    else:\n        data, metadata = output, {}\n    return IPython.display.display(data, raw=raw, metadata=metadata, **kwargs)", "response": "Renders any HoloViews object to HTML and displays it using IPython display."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_display(element, max_frames, fmt):\n    if fmt not in Store.display_formats:\n        return None\n    info = process_object(element)\n    if info:\n        display(HTML(info))\n        return\n\n    backend = Store.current_backend\n    if type(element) not in Store.registry[backend]:\n        return None\n    renderer = Store.renderers[backend]\n    plot = renderer.get_plot(element)\n\n    # Current renderer does not support the image format\n    if fmt not in renderer.params('fig').objects:\n        return None\n\n    data, info = renderer(plot, fmt=fmt)\n    return {info['mime_type']: data}, {}", "response": "Display an image element in a specific format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmap elements from given order array to ticks.", "response": "def _map_order_to_ticks(start, end, order, reverse=False):\n        \"\"\"Map elements from given `order` array to bins ranging from `start`\n        to `end`.\n        \"\"\"\n        size = len(order)\n        bounds = np.linspace(start, end, size + 1)\n        if reverse:\n            bounds = bounds[::-1]\n        mapping = list(zip(bounds[:-1]%(np.pi*2), order))\n        return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute x and y positions for separation lines for given angles.", "response": "def _compute_separations(inner, outer, angles):\n        \"\"\"Compute x and y positions for separation lines for given angles.\n\n        \"\"\"\n        return [np.array([[a, inner], [a, outer]]) for a in angles]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef range(self, dim, data_range=True, dimension_range=True):\n        iskdim = self.get_dimension(dim) not in self.vdims\n        return super(StatisticsElement, self).range(dim, iskdim, dimension_range)", "response": "Return the lower and upper bounds of values along dimension."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dimension_values(self, dim, expanded=True, flat=True):\n        dim = self.get_dimension(dim, strict=True)\n        if dim in self.vdims:\n            return np.full(len(self), np.NaN)\n        return self.interface.values(self, dim, expanded, flat)", "response": "Returns the values along the requested dimension."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the type of the requested dimension.", "response": "def get_dimension_type(self, dim):\n        \"\"\"Get the type of the requested dimension.\n\n        Type is determined by Dimension.type attribute or common\n        type of the dimension values, otherwise None.\n\n        Args:\n            dimension: Dimension to look up by name or by index\n\n        Returns:\n            Declared type of values along the dimension\n        \"\"\"\n        dim = self.get_dimension(dim)\n        if dim is None:\n            return None\n        elif dim.type is not None:\n            return dim.type\n        elif dim in self.vdims:\n            return np.float64\n        return self.interface.dimension_type(self, dim)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting dimension values to DataFrame.", "response": "def dframe(self, dimensions=None, multi_index=False):\n        \"\"\"Convert dimension values to DataFrame.\n\n        Returns a pandas dataframe of columns along each dimension,\n        either completely flat or indexed by key dimensions.\n\n        Args:\n            dimensions: Dimensions to return as columns\n            multi_index: Convert key dimensions to (multi-)index\n\n        Returns:\n            DataFrame of columns corresponding to each dimension\n        \"\"\"\n        if dimensions:\n            dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        else:\n            dimensions = self.kdims\n        vdims = [d for d in dimensions if d in self.vdims]\n        if vdims:\n            raise ValueError('%s element does not hold data for value '\n                             'dimensions. Could not return data for %s '\n                             'dimension(s).' %\n                             (type(self).__name__, ', '.join([d.name for d in vdims])))\n        return super(StatisticsElement, self).dframe(dimensions, False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting dimension values to a dictionary.", "response": "def columns(self, dimensions=None):\n        \"\"\"Convert dimension values to a dictionary.\n\n        Returns a dictionary of column arrays along each dimension\n        of the element.\n\n        Args:\n            dimensions: Dimensions to return as columns\n\n        Returns:\n            Dictionary of arrays for each dimension\n        \"\"\"\n        if dimensions is None:\n            dimensions = self.kdims\n        else:\n            dimensions = [self.get_dimension(d, strict=True) for d in dimensions]\n        vdims = [d for d in dimensions if d in self.vdims]\n        if vdims:\n            raise ValueError('%s element does not hold data for value '\n                             'dimensions. Could not return data for %s '\n                             'dimension(s).' %\n                             (type(self).__name__, ', '.join([d.name for d in vdims])))\n        return OrderedDict([(d.name, self.dimension_values(d)) for d in dimensions])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recv_msg(self):\n        '''message receive routine for UDP link'''\n        self.pre_message()\n        s = self.recv()\n        if len(s) > 0:\n            if self.first_byte:\n                self.auto_mavlink_version(s)\n\n        m = self.mav.parse_char(s)\n        if m is not None:\n            self.post_message(m)\n\n        return m", "response": "receive a message from the link"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef condition_yaw(heading, relative=False):\n    if relative:\n        is_relative = 1 #yaw relative to direction of travel\n    else:\n        is_relative = 0 #yaw is an absolute angle\n    # create the CONDITION_YAW command using command_long_encode()\n    msg = vehicle.message_factory.command_long_encode(\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_CMD_CONDITION_YAW, #command\n        0, #confirmation\n        heading,    # param 1, yaw in degrees\n        0,          # param 2, yaw speed deg/s\n        1,          # param 3, direction -1 ccw, 1 cw\n        is_relative, # param 4, relative offset 1, absolute angle 0\n        0, 0, 0)    # param 5 ~ 7 not used\n    # send command to vehicle\n    vehicle.send_mavlink(msg)", "response": "set yaw to the current heading in degrees"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_roi(location):\n    # create the MAV_CMD_DO_SET_ROI command\n    msg = vehicle.message_factory.command_long_encode(\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_CMD_DO_SET_ROI, #command\n        0, #confirmation\n        0, 0, 0, 0, #params 1-4\n        location.lat,\n        location.lon,\n        location.alt\n        )\n    # send command to vehicle\n    vehicle.send_mavlink(msg)", "response": "Set the ROI of the vehicle at a given location"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_location_metres(original_location, dNorth, dEast):\n    earth_radius = 6378137.0 #Radius of \"spherical\" earth\n    #Coordinate offsets in radians\n    dLat = dNorth/earth_radius\n    dLon = dEast/(earth_radius*math.cos(math.pi*original_location.lat/180))\n\n    #New position in decimal degrees\n    newlat = original_location.lat + (dLat * 180/math.pi)\n    newlon = original_location.lon + (dLon * 180/math.pi)\n    if type(original_location) is LocationGlobal:\n        targetlocation=LocationGlobal(newlat, newlon,original_location.alt)\n    elif type(original_location) is LocationGlobalRelative:\n        targetlocation=LocationGlobalRelative(newlat, newlon,original_location.alt)\n    else:\n        raise Exception(\"Invalid Location object passed\")\n        \n    return targetlocation;", "response": "Returns a LocationGlobal object containing the latitude and longitude dNorth and dEast metres from the \n    specified original_location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_distance_metres(aLocation1, aLocation2):\n    dlat = aLocation2.lat - aLocation1.lat\n    dlong = aLocation2.lon - aLocation1.lon\n    return math.sqrt((dlat*dlat) + (dlong*dlong)) * 1.113195e5", "response": "Returns the ground distance in metres between two LocationGlobal objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the bearing between two LocationGlobal objects passed as parameters.", "response": "def get_bearing(aLocation1, aLocation2):\n    \"\"\"\n    Returns the bearing between the two LocationGlobal objects passed as parameters.\n\n    This method is an approximation, and may not be accurate over large distances and close to the \n    earth's poles. It comes from the ArduPilot test code: \n    https://github.com/diydrones/ardupilot/blob/master/Tools/autotest/common.py\n    \"\"\"\t\n    off_x = aLocation2.lon - aLocation1.lon\n    off_y = aLocation2.lat - aLocation1.lat\n    bearing = 90.00 + math.atan2(-off_y, off_x) * 57.2957795\n    if bearing < 0:\n        bearing += 360.00\n    return bearing;"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef goto_position_target_global_int(aLocation):\n    msg = vehicle.message_factory.set_position_target_global_int_encode(\n        0,       # time_boot_ms (not used)\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT_INT, # frame\n        0b0000111111111000, # type_mask (only speeds enabled)\n        aLocation.lat*1e7, # lat_int - X Position in WGS84 frame in 1e7 * meters\n        aLocation.lon*1e7, # lon_int - Y Position in WGS84 frame in 1e7 * meters\n        aLocation.alt, # alt - Altitude in meters in AMSL altitude, not WGS84 if absolute or relative, above terrain if GLOBAL_TERRAIN_ALT_INT\n        0, # X velocity in NED frame in m/s\n        0, # Y velocity in NED frame in m/s\n        0, # Z velocity in NED frame in m/s\n        0, 0, 0, # afx, afy, afz acceleration (not supported yet, ignored in GCS_Mavlink)\n        0, 0)    # yaw, yaw_rate (not supported yet, ignored in GCS_Mavlink) \n    # send command to vehicle\n    vehicle.send_mavlink(msg)", "response": "Request the vehicle fly to a specified LocationGlobal."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef goto_position_target_local_ned(north, east, down):\n    msg = vehicle.message_factory.set_position_target_local_ned_encode(\n        0,       # time_boot_ms (not used)\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_FRAME_LOCAL_NED, # frame\n        0b0000111111111000, # type_mask (only positions enabled)\n        north, east, down, # x, y, z positions (or North, East, Down in the MAV_FRAME_BODY_NED frame\n        0, 0, 0, # x, y, z velocity in m/s  (not used)\n        0, 0, 0, # x, y, z acceleration (not supported yet, ignored in GCS_Mavlink)\n        0, 0)    # yaw, yaw_rate (not supported yet, ignored in GCS_Mavlink) \n    # send command to vehicle\n    vehicle.send_mavlink(msg)", "response": "Request the vehicle fly to a specified location in the North East Down frame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef goto(dNorth, dEast, gotoFunction=vehicle.simple_goto):\n    \n    currentLocation = vehicle.location.global_relative_frame\n    targetLocation = get_location_metres(currentLocation, dNorth, dEast)\n    targetDistance = get_distance_metres(currentLocation, targetLocation)\n    gotoFunction(targetLocation)\n    \n    #print \"DEBUG: targetLocation: %s\" % targetLocation\n    #print \"DEBUG: targetLocation: %s\" % targetDistance\n\n    while vehicle.mode.name==\"GUIDED\": #Stop action if we are no longer in guided mode.\n        #print \"DEBUG: mode: %s\" % vehicle.mode.name\n        remainingDistance=get_distance_metres(vehicle.location.global_relative_frame, targetLocation)\n        print(\"Distance to target: \", remainingDistance)\n        if remainingDistance<=targetDistance*0.01: #Just below target, in case of undershoot.\n            print(\"Reached target\")\n            break;\n        time.sleep(2)", "response": "Moves the vehicle to a position dNorth metres North and dEast metres East of the current position."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_ned_velocity(velocity_x, velocity_y, velocity_z, duration):\n    msg = vehicle.message_factory.set_position_target_local_ned_encode(\n        0,       # time_boot_ms (not used)\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_FRAME_LOCAL_NED, # frame\n        0b0000111111000111, # type_mask (only speeds enabled)\n        0, 0, 0, # x, y, z positions (not used)\n        velocity_x, velocity_y, velocity_z, # x, y, z velocity in m/s\n        0, 0, 0, # x, y, z acceleration (not supported yet, ignored in GCS_Mavlink)\n        0, 0)    # yaw, yaw_rate (not supported yet, ignored in GCS_Mavlink) \n\n    # send command to vehicle on 1 Hz cycle\n    for x in range(0,duration):\n        vehicle.send_mavlink(msg)\n        time.sleep(1)", "response": "Send NED velocity to vehicle on 1 Hz cycle."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a global velocity message to the vehicle", "response": "def send_global_velocity(velocity_x, velocity_y, velocity_z, duration):\n    \"\"\"\n    Move vehicle in direction based on specified velocity vectors.\n\n    This uses the SET_POSITION_TARGET_GLOBAL_INT command with type mask enabling only \n    velocity components \n    (http://dev.ardupilot.com/wiki/copter-commands-in-guided-mode/#set_position_target_global_int).\n    \n    Note that from AC3.3 the message should be re-sent every second (after about 3 seconds\n    with no message the velocity will drop back to zero). In AC3.2.1 and earlier the specified\n    velocity persists until it is canceled. The code below should work on either version \n    (sending the message multiple times does not cause problems).\n    \n    See the above link for information on the type_mask (0=enable, 1=ignore). \n    At time of writing, acceleration and yaw bits are ignored.\n    \"\"\"\n    msg = vehicle.message_factory.set_position_target_global_int_encode(\n        0,       # time_boot_ms (not used)\n        0, 0,    # target system, target component\n        mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT_INT, # frame\n        0b0000111111000111, # type_mask (only speeds enabled)\n        0, # lat_int - X Position in WGS84 frame in 1e7 * meters\n        0, # lon_int - Y Position in WGS84 frame in 1e7 * meters\n        0, # alt - Altitude in meters in AMSL altitude(not WGS84 if absolute or relative)\n        # altitude above terrain if GLOBAL_TERRAIN_ALT_INT\n        velocity_x, # X velocity in NED frame in m/s\n        velocity_y, # Y velocity in NED frame in m/s\n        velocity_z, # Z velocity in NED frame in m/s\n        0, 0, 0, # afx, afy, afz acceleration (not supported yet, ignored in GCS_Mavlink)\n        0, 0)    # yaw, yaw_rate (not supported yet, ignored in GCS_Mavlink) \n\n    # send command to vehicle on 1 Hz cycle\n    for x in range(0,duration):\n        vehicle.send_mavlink(msg)\n        time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef readmission(aFileName):\n    print(\"\\nReading mission from file: %s\" % aFileName)\n    cmds = vehicle.commands\n    missionlist=[]\n    with open(aFileName) as f:\n        for i, line in enumerate(f):\n            if i==0:\n                if not line.startswith('QGC WPL 110'):\n                    raise Exception('File is not supported WP version')\n            else:\n                linearray=line.split('\\t')\n                ln_index=int(linearray[0])\n                ln_currentwp=int(linearray[1])\n                ln_frame=int(linearray[2])\n                ln_command=int(linearray[3])\n                ln_param1=float(linearray[4])\n                ln_param2=float(linearray[5])\n                ln_param3=float(linearray[6])\n                ln_param4=float(linearray[7])\n                ln_param5=float(linearray[8])\n                ln_param6=float(linearray[9])\n                ln_param7=float(linearray[10])\n                ln_autocontinue=int(linearray[11].strip())\n                cmd = Command( 0, 0, 0, ln_frame, ln_command, ln_currentwp, ln_autocontinue, ln_param1, ln_param2, ln_param3, ln_param4, ln_param5, ln_param6, ln_param7)\n                missionlist.append(cmd)\n    return missionlist", "response": "Read a mission from a file into a list of commands."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_mission(aFileName):\n    #Read mission from file\n    missionlist = readmission(aFileName)\n    \n    print(\"\\nUpload mission from a file: %s\" % aFileName)\n    #Clear existing mission from vehicle\n    print(' Clear mission')\n    cmds = vehicle.commands\n    cmds.clear()\n    #Add new mission to vehicle\n    for command in missionlist:\n        cmds.add(command)\n    print(' Upload mission')\n    vehicle.commands.upload()", "response": "Upload a mission from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_mission():\n    print(\" Download mission from vehicle\")\n    missionlist=[]\n    cmds = vehicle.commands\n    cmds.download()\n    cmds.wait_ready()\n    for cmd in cmds:\n        missionlist.append(cmd)\n    return missionlist", "response": "Downloads the current mission and returns it in a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave a mission from vehicle to file.", "response": "def save_mission(aFileName):\n    \"\"\"\n    Save a mission in the Waypoint file format \n    (http://qgroundcontrol.org/mavlink/waypoint_protocol#waypoint_file_format).\n    \"\"\"\n    print(\"\\nSave mission from Vehicle to file: %s\" % aFileName)    \n    #Download mission from vehicle\n    missionlist = download_mission()\n    #Add file-format information\n    output='QGC WPL 110\\n'\n    #Add home location as 0th waypoint\n    home = vehicle.home_location\n    output+=\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (0,1,0,16,0,0,0,0,home.lat,home.lon,home.alt,1)\n    #Add commands\n    for cmd in missionlist:\n        commandline=\"%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\t%s\\n\" % (cmd.seq,cmd.current,cmd.frame,cmd.command,cmd.param1,cmd.param2,cmd.param3,cmd.param4,cmd.x,cmd.y,cmd.z,cmd.autocontinue)\n        output+=commandline\n    with open(aFileName, 'w') as file_:\n        print(\" Write mission to file\")\n        file_.write(output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef printfile(aFileName):\n    print(\"\\nMission file: %s\" % aFileName)\n    with open(aFileName) as f:\n        for line in f:\n            print(' %s' % line.strip())", "response": "Print a mission file to demonstrate round trip"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef distance_to_current_waypoint():\n    nextwaypoint = vehicle.commands.next\n    if nextwaypoint==0:\n        return None\n    missionitem=vehicle.commands[nextwaypoint-1] #commands are zero indexed\n    lat = missionitem.x\n    lon = missionitem.y\n    alt = missionitem.z\n    targetWaypointLocation = LocationGlobalRelative(lat,lon,alt)\n    distancetopoint = get_distance_metres(vehicle.location.global_frame, targetWaypointLocation)\n    return distancetopoint", "response": "Gets the distance in metres to the current waypoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef position_messages_from_tlog(filename):\n    # Pull out just the global position msgs\n    messages = []\n    mlog = mavutil.mavlink_connection(filename)\n    while True:\n        try:\n            m = mlog.recv_match(type=['GLOBAL_POSITION_INT'])\n            if m is None:\n                break\n        except Exception:\n            break\n        # ignore we get where there is no fix:\n        if m.lat == 0:\n            continue\n        messages.append(m)\n\n    # Shrink the number of points for readability and to stay within autopilot memory limits. \n    # For coding simplicity we:\n    #   - only keep points that are with 3 metres of the previous kept point.\n    #   - only keep the first 100 points that meet the above criteria.\n    num_points = len(messages)\n    keep_point_distance=3 #metres\n    kept_messages = []\n    kept_messages.append(messages[0]) #Keep the first message\n    pt1num=0\n    pt2num=1\n    while True:\n        #Keep the last point. Only record 99 points.\n        if pt2num==num_points-1 or len(kept_messages)==99:\n            kept_messages.append(messages[pt2num])\n            break\n        pt1 = LocationGlobalRelative(messages[pt1num].lat/1.0e7,messages[pt1num].lon/1.0e7,0)\n        pt2 = LocationGlobalRelative(messages[pt2num].lat/1.0e7,messages[pt2num].lon/1.0e7,0)\n        distance_between_points = get_distance_metres(pt1,pt2)\n        if distance_between_points > keep_point_distance:\n            kept_messages.append(messages[pt2num])\n            pt1num=pt2num\n        pt2num=pt2num+1\n\n    return kept_messages", "response": "Given a telemetry log get a series of wpts approximating the previous flight\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\narms vehicle and takeoff to aTargetAltitude.", "response": "def arm_and_takeoff(aTargetAltitude):\n    \"\"\"\n    Arms vehicle and fly to aTargetAltitude.\n    \"\"\"\n\n    # Don't try to arm until autopilot is ready\n    while not vehicle.is_armable:\n        print(\" Waiting for vehicle to initialise...\")\n        time.sleep(1)\n        \n    # Set mode to GUIDED for arming and takeoff:\n    while (vehicle.mode.name != \"GUIDED\"):\n        vehicle.mode = VehicleMode(\"GUIDED\")\n        time.sleep(0.1)\n\n    # Confirm vehicle armed before attempting to take off\n    while not vehicle.armed:\n        vehicle.armed = True\n        print(\" Waiting for arming...\")\n        time.sleep(1)\n\n    print(\" Taking off!\")\n    vehicle.simple_takeoff(aTargetAltitude) # Take off to target altitude\n\n    # Wait until the vehicle reaches a safe height \n    # before allowing next command to process.\n    while True:\n        requiredAlt = aTargetAltitude*0.95\n        #Break and return from function just below target altitude.        \n        if vehicle.location.global_relative_frame.alt>=requiredAlt: \n            print(\" Reached target altitude of ~%f\" % (aTargetAltitude))\n            break\n        print(\" Altitude: %f < %f\" % (vehicle.location.global_relative_frame.alt,\n                                      requiredAlt))\n        time.sleep(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a LocationGlobal object containing the latitude and longitude dNorth and dEast metres from the specified original_location.", "response": "def get_location_metres(original_location, dNorth, dEast):\n    \"\"\"\n    Returns a LocationGlobal object containing the latitude/longitude `dNorth` and `dEast` metres from the \n    specified `original_location`. The returned Location has the same `alt` value\n    as `original_location`.\n\n    The function is useful when you want to move the vehicle around specifying locations relative to \n    the current vehicle position.\n    The algorithm is relatively accurate over small distances (10m within 1km) except close to the poles.\n    For more information see:\n    http://gis.stackexchange.com/questions/2951/algorithm-for-offsetting-a-latitude-longitude-by-some-amount-of-meters\n    \"\"\"\n    earth_radius=6378137.0 #Radius of \"spherical\" earth\n    #Coordinate offsets in radians\n    dLat = dNorth/earth_radius\n    dLon = dEast/(earth_radius*math.cos(math.pi*original_location.lat/180))\n\n    #New position in decimal degrees\n    newlat = original_location.lat + (dLat * 180/math.pi)\n    newlon = original_location.lon + (dLon * 180/math.pi)\n    return LocationGlobal(newlat, newlon,original_location.alt)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a takeoff command and four waypoint commands to the current mission", "response": "def adds_square_mission(aLocation, aSize):\n    \"\"\"\n    Adds a takeoff command and four waypoint commands to the current mission. \n    The waypoints are positioned to form a square of side length 2*aSize around the specified LocationGlobal (aLocation).\n\n    The function assumes vehicle.commands matches the vehicle mission state \n    (you must have called download at least once in the session and after clearing the mission)\n    \"\"\"\t\n\n    cmds = vehicle.commands\n\n    print(\" Clear any existing commands\")\n    cmds.clear() \n    \n    print(\" Define/add new commands.\")\n    # Add new commands. The meaning/order of the parameters is documented in the Command class. \n     \n    #Add MAV_CMD_NAV_TAKEOFF command. This is ignored if the vehicle is already in the air.\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_TAKEOFF, 0, 0, 0, 0, 0, 0, 0, 0, 10))\n\n    #Define the four MAV_CMD_NAV_WAYPOINT locations and add the commands\n    point1 = get_location_metres(aLocation, aSize, -aSize)\n    point2 = get_location_metres(aLocation, aSize, aSize)\n    point3 = get_location_metres(aLocation, -aSize, aSize)\n    point4 = get_location_metres(aLocation, -aSize, -aSize)\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 0, 0, 0, 0, 0, 0, point1.lat, point1.lon, 11))\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 0, 0, 0, 0, 0, 0, point2.lat, point2.lon, 12))\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 0, 0, 0, 0, 0, 0, point3.lat, point3.lon, 13))\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 0, 0, 0, 0, 0, 0, point4.lat, point4.lon, 14))\n    #add dummy waypoint \"5\" at point 4 (lets us know when have reached destination)\n    cmds.add(Command( 0, 0, 0, mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT, mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 0, 0, 0, 0, 0, 0, point4.lat, point4.lon, 14))    \n\n    print(\" Upload new commands to vehicle\")\n    cmds.upload()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(ip,\n            _initialize=True,\n            wait_ready=None,\n            timeout=30,\n            still_waiting_callback=default_still_waiting_callback,\n            still_waiting_interval=1,\n            status_printer=None,\n            vehicle_class=None,\n            rate=4,\n            baud=115200,\n            heartbeat_timeout=30,\n            source_system=255,\n            source_component=0,\n            use_native=False):\n    \"\"\"\n    Returns a :py:class:`Vehicle` object connected to the address specified by string parameter ``ip``.\n    Connection string parameters (``ip``) for different targets are listed in the :ref:`getting started guide <get_started_connecting>`.\n\n    The method is usually called with ``wait_ready=True`` to ensure that vehicle parameters and (most) attributes are\n    available when ``connect()`` returns.\n\n    .. code:: python\n\n        from dronekit import connect\n\n        # Connect to the Vehicle using \"connection string\" (in this case an address on network)\n        vehicle = connect('127.0.0.1:14550', wait_ready=True)\n\n    :param String ip: :ref:`Connection string <get_started_connecting>` for target address - e.g. 127.0.0.1:14550.\n\n    :param Bool/Array wait_ready: If ``True`` wait until all default attributes have downloaded before\n        the method returns (default is ``None``).\n        The default attributes to wait on are: :py:attr:`parameters`, :py:attr:`gps_0`,\n        :py:attr:`armed`, :py:attr:`mode`, and :py:attr:`attitude`.\n\n        You can also specify a named set of parameters to wait on (e.g. ``wait_ready=['system_status','mode']``).\n\n        For more information see :py:func:`Vehicle.wait_ready <Vehicle.wait_ready>`.\n\n    :param status_printer: (deprecated) method of signature ``def status_printer(txt)`` that prints\n        STATUS_TEXT messages from the Vehicle and other diagnostic information.\n        By default the status information is handled by the ``autopilot`` logger.\n    :param Vehicle vehicle_class: The class that will be instantiated by the ``connect()`` method.\n        This can be any sub-class of ``Vehicle`` (and defaults to ``Vehicle``).\n    :param int rate: Data stream refresh rate. The default is 4Hz (4 updates per second).\n    :param int baud: The baud rate for the connection. The default is 115200.\n    :param int heartbeat_timeout: Connection timeout value in seconds (default is 30s).\n        If a heartbeat is not detected within this time an exception will be raised.\n    :param int source_system: The MAVLink ID of the :py:class:`Vehicle` object returned by this method (by default 255).\n    :param int source_component: The MAVLink Component ID fo the :py:class:`Vehicle` object returned by this method (by default 0).\n    :param bool use_native: Use precompiled MAVLink parser.\n\n        .. note::\n\n            The returned :py:class:`Vehicle` object acts as a ground control station from the\n            perspective of the connected \"real\" vehicle. It will process/receive messages from the real vehicle\n            if they are addressed to this ``source_system`` id. Messages sent to the real vehicle are\n            automatically updated to use the vehicle's ``target_system`` id.\n\n            It is *good practice* to assign a unique id for every system on the MAVLink network.\n            It is possible to configure the autopilot to only respond to guided-mode commands from a specified GCS ID.\n\n            The ``status_printer`` argument is deprecated. To redirect the logging from the library and from the\n            autopilot, configure the ``dronekit`` and ``autopilot`` loggers using the Python ``logging`` module.\n\n\n    :returns: A connected vehicle of the type defined in ``vehicle_class`` (a superclass of :py:class:`Vehicle`).\n    \"\"\"\n\n    from dronekit.mavlink import MAVConnection\n\n    if not vehicle_class:\n        vehicle_class = Vehicle\n\n    handler = MAVConnection(ip, baud=baud, source_system=source_system, source_component=source_component, use_native=use_native)\n    vehicle = vehicle_class(handler)\n\n    if status_printer:\n        vehicle._autopilot_logger.addHandler(ErrprinterHandler(status_printer))\n\n    if _initialize:\n        vehicle.initialize(rate=rate, heartbeat_timeout=heartbeat_timeout)\n\n    if wait_ready:\n        if wait_ready is True:\n            vehicle.wait_ready(still_waiting_interval=still_waiting_interval,\n                               still_waiting_callback=still_waiting_callback,\n                               timeout=timeout)\n        else:\n            vehicle.wait_ready(*wait_ready)\n\n    return vehicle", "response": "Connect to a target address using the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn 3D distance away from home in meters. Returns 2D distance if down is known otherwise 3D distance.", "response": "def distance_home(self):\n        \"\"\"\n        Distance away from home, in meters. Returns 3D distance if `down` is known, otherwise 2D distance.\n        \"\"\"\n\n        if self.north is not None and self.east is not None:\n            if self.down is not None:\n                return math.sqrt(self.north**2 + self.east**2 + self.down**2)\n            else:\n                return math.sqrt(self.north**2 + self.east**2)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding an attribute listener callback to be invoked when a change in the attribute is detected.", "response": "def add_attribute_listener(self, attr_name, observer):\n        \"\"\"\n        Add an attribute listener callback.\n\n        The callback function (``observer``) is invoked differently depending on the *type of attribute*.\n        Attributes that represent sensor values or which are used to monitor connection status are updated\n        whenever a message is received from the vehicle. Attributes which reflect vehicle \"state\" are\n        only updated when their values change (for example :py:attr:`Vehicle.system_status`,\n        :py:attr:`Vehicle.armed`, and :py:attr:`Vehicle.mode`).\n\n        The callback can be removed using :py:func:`remove_attribute_listener`.\n\n        .. note::\n\n            The :py:func:`on_attribute` decorator performs the same operation as this method, but with\n            a more elegant syntax. Use ``add_attribute_listener`` by preference if you will need to remove\n            the observer.\n\n        The argument list for the callback is ``observer(object, attr_name, attribute_value)``:\n\n        * ``self`` - the associated :py:class:`Vehicle`. This may be compared to a global vehicle handle\n          to implement vehicle-specific callback handling (if needed).\n        * ``attr_name`` - the attribute name. This can be used to infer which attribute has triggered\n          if the same callback is used for watching several attributes.\n        * ``value`` - the attribute value (so you don't need to re-query the vehicle object).\n\n        The example below shows how to get callbacks for (global) location changes:\n\n        .. code:: python\n\n            #Callback to print the location in global frame\n            def location_callback(self, attr_name, msg):\n                print \"Location (Global): \", msg\n\n            #Add observer for the vehicle's current location\n            vehicle.add_attribute_listener('global_frame', location_callback)\n\n        See :ref:`vehicle_state_observe_attributes` for more information.\n\n        :param String attr_name: The name of the attribute to watch (or '*' to watch all attributes).\n        :param observer: The callback to invoke when a change in the attribute is detected.\n\n        \"\"\"\n        listeners_for_attr = self._attribute_listeners.get(attr_name)\n        if listeners_for_attr is None:\n            listeners_for_attr = []\n            self._attribute_listeners[attr_name] = listeners_for_attr\n        if observer not in listeners_for_attr:\n            listeners_for_attr.append(observer)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving an attribute listener that was previously added using add_attribute_listener.", "response": "def remove_attribute_listener(self, attr_name, observer):\n        \"\"\"\n        Remove an attribute listener (observer) that was previously added using :py:func:`add_attribute_listener`.\n\n        For example, the following line would remove a previously added vehicle 'global_frame'\n        observer called ``location_callback``:\n\n        .. code:: python\n\n            vehicle.remove_attribute_listener('global_frame', location_callback)\n\n        See :ref:`vehicle_state_observe_attributes` for more information.\n\n        :param String attr_name: The attribute name that is to have an observer removed (or '*' to remove an 'all attribute' observer).\n        :param observer: The callback function to remove.\n\n        \"\"\"\n        listeners_for_attr = self._attribute_listeners.get(attr_name)\n        if listeners_for_attr is not None:\n            listeners_for_attr.remove(observer)\n            if len(listeners_for_attr) == 0:\n                del self._attribute_listeners[attr_name]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef notify_attribute_listeners(self, attr_name, value, cache=False):\n        # Cached values are not re-sent if they are unchanged.\n        if cache:\n            if self._attribute_cache.get(attr_name) == value:\n                return\n            self._attribute_cache[attr_name] = value\n\n        # Notify observers.\n        for fn in self._attribute_listeners.get(attr_name, []):\n            try:\n                fn(self, attr_name, value)\n            except Exception:\n                self._logger.exception('Exception in attribute handler for %s' % attr_name, exc_info=True)\n\n        for fn in self._attribute_listeners.get('*', []):\n            try:\n                fn(self, attr_name, value)\n            except Exception:\n                self._logger.exception('Exception in attribute handler for %s' % attr_name, exc_info=True)", "response": "This method is used to notify the attribute listeners of a named attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_message_listener(self, name, fn):\n        name = str(name)\n        if name not in self._message_listeners:\n            self._message_listeners[name] = []\n        if fn not in self._message_listeners[name]:\n            self._message_listeners[name].append(fn)", "response": "Add a message listener function that will be called every time a message is received."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_message_listener(self, name, fn):\n        name = str(name)\n        if name in self._message_listeners:\n            if fn in self._message_listeners[name]:\n                self._message_listeners[name].remove(fn)\n                if len(self._message_listeners[name]) == 0:\n                    del self._message_listeners[name]", "response": "Removes a message listener."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef battery(self):\n        if self._voltage is None or self._current is None or self._level is None:\n            return None\n        return Battery(self._voltage, self._current, self._level)", "response": "Return a new object containing the current battery information."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new GPSInfo object with only the first GPS position.", "response": "def gps_0(self):\n        \"\"\"\n        GPS position information (:py:class:`GPSInfo`).\n        \"\"\"\n        return GPSInfo(self._eph, self._epv, self._fix_type, self._satellites_visible)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the vehicle is ready to arm false otherwise.", "response": "def is_armable(self):\n        \"\"\"\n        Returns ``True`` if the vehicle is ready to arm, false otherwise (``Boolean``).\n\n        This attribute wraps a number of pre-arm checks, ensuring that the vehicle has booted,\n        has a good GPS fix, and that the EKF pre-arm is complete.\n        \"\"\"\n        # check that mode is not INITIALSING\n        # check that we have a GPS fix\n        # check that EKF pre-arm is complete\n        return self.mode != 'INITIALISING' and (self.gps_0.fix_type is not None and self.gps_0.fix_type > 1) and self._ekf_predposhorizabs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict of the system status.", "response": "def system_status(self):\n        \"\"\"\n        System status (:py:class:`SystemStatus`).\n\n        The status has a ``state`` property with one of the following values:\n\n        * ``UNINIT``: Uninitialized system, state is unknown.\n        * ``BOOT``: System is booting up.\n        * ``CALIBRATING``: System is calibrating and not flight-ready.\n        * ``STANDBY``: System is grounded and on standby. It can be launched any time.\n        * ``ACTIVE``: System is active and might be already airborne. Motors are engaged.\n        * ``CRITICAL``: System is in a non-normal flight mode. It can however still navigate.\n        * ``EMERGENCY``: System is in a non-normal flight mode. It lost control over parts\n          or over the whole airframe. It is in mayday and going down.\n        * ``POWEROFF``: System just initialized its power-down sequence, will shut down now.\n        \"\"\"\n        return {\n            0: SystemStatus('UNINIT'),\n            1: SystemStatus('BOOT'),\n            2: SystemStatus('CALIBRATING'),\n            3: SystemStatus('STANDBY'),\n            4: SystemStatus('ACTIVE'),\n            5: SystemStatus('CRITICAL'),\n            6: SystemStatus('EMERGENCY'),\n            7: SystemStatus('POWEROFF'),\n        }.get(self._system_status, None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if the EKF status is considered acceptable False otherwise.", "response": "def ekf_ok(self):\n        \"\"\"\n        ``True`` if the EKF status is considered acceptable, ``False`` otherwise (``boolean``).\n        \"\"\"\n        # legacy check for dronekit-python for solo\n        # use same check that ArduCopter::system.pde::position_ok() is using\n        if self.armed:\n            return self._ekf_poshorizabs and not self._ekf_constposmode\n        else:\n            return self._ekf_poshorizabs or self._ekf_predposhorizabs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef home_location(self, pos):\n\n        if not isinstance(pos, LocationGlobal):\n            raise ValueError('Expecting home_location to be set to a LocationGlobal.')\n\n        # Set cached home location.\n        self._home_location = copy.copy(pos)\n\n        # Send MAVLink update.\n        self.send_mavlink(self.message_factory.command_long_encode(\n            0, 0,  # target system, target component\n            mavutil.mavlink.MAV_CMD_DO_SET_HOME,  # command\n            0,  # confirmation\n            0,  # param 1: 1 to use current position, 0 to use the entered values.\n            0, 0, 0,  # params 2-4\n            pos.lat, pos.lon, pos.alt))", "response": "Set the home location of the current vehicle."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait for a condition to be True.", "response": "def wait_for(self, condition, timeout=None, interval=0.1, errmsg=None):\n        '''Wait for a condition to be True.\n\n        Wait for condition, a callable, to return True.  If timeout is\n        nonzero, raise a TimeoutError(errmsg) if the condition is not\n        True after timeout seconds.  Check the condition everal\n        interval seconds.\n        '''\n\n        t0 = time.time()\n        while not condition():\n            t1 = time.time()\n            if timeout and (t1 - t0) >= timeout:\n                raise TimeoutError(errmsg)\n\n            time.sleep(interval)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_armable(self, timeout=None):\n        '''Wait for the vehicle to become armable.\n\n        If timeout is nonzero, raise a TimeoutError if the vehicle\n        is not armable after timeout seconds.\n        '''\n\n        def check_armable():\n            return self.is_armable\n\n        self.wait_for(check_armable, timeout=timeout)", "response": "Wait for the vehicle to become armable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arm(self, wait=True, timeout=None):\n        '''Arm the vehicle.\n\n        If wait is True, wait for arm operation to complete before\n        returning.  If timeout is nonzero, raise a TimeouTerror if the\n        vehicle has not armed after timeout seconds.\n        '''\n\n        self.armed = True\n\n        if wait:\n            self.wait_for(lambda: self.armed, timeout=timeout,\n                          errmsg='failed to arm vehicle')", "response": "Arm the vehicle.\n\n        If wait is True, wait for arm operation to complete before\n        returning.  If timeout is nonzero, raise a TimeouTerror if the\n        vehicle has not armed after timeout seconds."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disarm(self, wait=True, timeout=None):\n        '''Disarm the vehicle.\n\n        If wait is True, wait for disarm operation to complete before\n        returning.  If timeout is nonzero, raise a TimeouTerror if the\n        vehicle has not disarmed after timeout seconds.\n        '''\n        self.armed = False\n\n        if wait:\n            self.wait_for(lambda: not self.armed, timeout=timeout,\n                          errmsg='failed to disarm vehicle')", "response": "Disarm the vehicle.\n\n        If wait is True, wait for disarm operation to complete before\n        returning.  If timeout is nonzero, raise a TimeouTerror if the\n        vehicle has not disarmed after timeout seconds."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the flight mode.", "response": "def wait_for_mode(self, mode, timeout=None):\n        '''Set the flight mode.\n\n        If wait is True, wait for the mode to change before returning.\n        If timeout is nonzero, raise a TimeoutError if the flight mode\n        hasn't changed after timeout seconds.\n        '''\n\n        if not isinstance(mode, VehicleMode):\n            mode = VehicleMode(mode)\n\n        self.mode = mode\n\n        self.wait_for(lambda: self.mode.name == mode.name,\n                      timeout=timeout,\n                      errmsg='failed to set flight mode')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef wait_for_alt(self, alt, epsilon=0.1, rel=True, timeout=None):\n        '''Wait for the vehicle to reach the specified altitude.\n\n        Wait for the vehicle to get within epsilon meters of the\n        given altitude.  If rel is True (the default), use the\n        global_relative_frame. If rel is False, use the global_frame.\n        If timeout is nonzero, raise a TimeoutError if the specified\n        altitude has not been reached after timeout seconds.\n        '''\n\n        def get_alt():\n            if rel:\n                alt = self.location.global_relative_frame.alt\n            else:\n                alt = self.location.global_frame.alt\n\n            return alt\n\n        def check_alt():\n            cur = get_alt()\n            delta = abs(alt - cur)\n\n            return (\n                (delta < epsilon) or\n                (cur > alt > start) or\n                (cur < alt < start)\n            )\n\n        start = get_alt()\n\n        self.wait_for(\n            check_alt,\n            timeout=timeout,\n            errmsg='failed to reach specified altitude')", "response": "Wait for the vehicle to reach the specified altitude."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simple_takeoff(self, alt=None):\n        if alt is not None:\n            altitude = float(alt)\n            if math.isnan(altitude) or math.isinf(altitude):\n                raise ValueError(\"Altitude was NaN or Infinity. Please provide a real number\")\n            self._master.mav.command_long_send(0, 0, mavutil.mavlink.MAV_CMD_NAV_TAKEOFF,\n                                               0, 0, 0, 0, 0, 0, 0, altitude)", "response": "Take off and fly the vehicle to the specified altitude."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngoing to a specified global location (:py:class:`LocationGlobal` or :py:class:`LocationGlobalRelative`). There is no mechanism for notification when the target location is reached, and if another command arrives before that point that will be executed immediately. You can optionally set the desired airspeed or groundspeed (this is identical to setting :py:attr:`airspeed` or :py:attr:`groundspeed`). The vehicle will determine what speed to use if the values are not set or if they are both set. The method will change the :py:class:`VehicleMode` to ``GUIDED`` if necessary. .. code:: python # Set mode to guided - this is optional as the simple_goto method will change the mode if needed. vehicle.mode = VehicleMode(\"GUIDED\") # Set the LocationGlobal to head towards a_location = LocationGlobal(-34.364114, 149.166022, 30) vehicle.simple_goto(a_location) :param location: The target location (:py:class:`LocationGlobal` or :py:class:`LocationGlobalRelative`). :param airspeed: Target airspeed in m/s (optional). :param groundspeed: Target groundspeed in m/s (optional).", "response": "def simple_goto(self, location, airspeed=None, groundspeed=None):\n        '''\n        Go to a specified global location (:py:class:`LocationGlobal` or :py:class:`LocationGlobalRelative`).\n\n        There is no mechanism for notification when the target location is reached, and if another command arrives\n        before that point that will be executed immediately.\n\n        You can optionally set the desired airspeed or groundspeed (this is identical to setting\n        :py:attr:`airspeed` or :py:attr:`groundspeed`). The vehicle will determine what speed to\n        use if the values are not set or if they are both set.\n\n        The method will change the :py:class:`VehicleMode` to ``GUIDED`` if necessary.\n\n        .. code:: python\n\n            # Set mode to guided - this is optional as the simple_goto method will change the mode if needed.\n            vehicle.mode = VehicleMode(\"GUIDED\")\n\n            # Set the LocationGlobal to head towards\n            a_location = LocationGlobal(-34.364114, 149.166022, 30)\n            vehicle.simple_goto(a_location)\n\n        :param location: The target location (:py:class:`LocationGlobal` or :py:class:`LocationGlobalRelative`).\n        :param airspeed: Target airspeed in m/s (optional).\n        :param groundspeed: Target groundspeed in m/s (optional).\n        '''\n        if isinstance(location, LocationGlobalRelative):\n            frame = mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT\n            alt = location.alt\n        elif isinstance(location, LocationGlobal):\n            # This should be the proper code:\n            # frame = mavutil.mavlink.MAV_FRAME_GLOBAL\n            # However, APM discards information about the relative frame\n            # and treats any alt value as relative. So we compensate here.\n            frame = mavutil.mavlink.MAV_FRAME_GLOBAL_RELATIVE_ALT\n            if not self.home_location:\n                self.commands.download()\n                self.commands.wait_ready()\n            alt = location.alt - self.home_location.alt\n        else:\n            raise ValueError('Expecting location to be LocationGlobal or LocationGlobalRelative.')\n\n        self._master.mav.mission_item_send(0, 0, 0, frame,\n                                           mavutil.mavlink.MAV_CMD_NAV_WAYPOINT, 2, 0, 0,\n                                           0, 0, 0, location.lat, location.lon,\n                                           alt)\n\n        if airspeed is not None:\n            self.airspeed = airspeed\n        if groundspeed is not None:\n            self.groundspeed = groundspeed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_capabilities_request(self, vehicle, name, m):\n        '''Request an AUTOPILOT_VERSION packet'''\n        capability_msg = vehicle.message_factory.command_long_encode(0, 0, mavutil.mavlink.MAV_CMD_REQUEST_AUTOPILOT_CAPABILITIES, 0, 1, 0, 0, 0, 0, 0, 0)\n        vehicle.send_mavlink(capability_msg)", "response": "Request an AUTOPILOT_VERSION packet"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplay a tune on the vehicle", "response": "def play_tune(self, tune):\n        '''Play a tune on the vehicle'''\n        msg = self.message_factory.play_tune_encode(0, 0, tune)\n        self.send_mavlink(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for the specified attributes to be populated from the vehicle.", "response": "def wait_ready(self, *types, **kwargs):\n        \"\"\"\n        Waits for specified attributes to be populated from the vehicle (values are initially ``None``).\n\n        This is typically called \"behind the scenes\" to ensure that :py:func:`connect` does not return until\n        attributes have populated (via the ``wait_ready`` parameter). You can also use it after connecting to\n        wait on a specific value(s).\n\n        There are two ways to call the method:\n\n        .. code-block:: python\n\n            #Wait on default attributes to populate\n            vehicle.wait_ready(True)\n\n            #Wait on specified attributes (or array of attributes) to populate\n            vehicle.wait_ready('mode','airspeed')\n\n        Using the ``wait_ready(True)`` waits on :py:attr:`parameters`, :py:attr:`gps_0`,\n        :py:attr:`armed`, :py:attr:`mode`, and :py:attr:`attitude`. In practice this usually\n        means that all supported attributes will be populated.\n\n        By default, the method will timeout after 30 seconds and raise an exception if the\n        attributes were not populated.\n\n        :param types: ``True`` to wait on the default set of attributes, or a\n            comma-separated list of the specific attributes to wait on.\n        :param int timeout: Timeout in seconds after which the method will raise an exception\n            (the default) or return ``False``. The default timeout is 30 seconds.\n        :param Boolean raise_exception: If ``True`` the method will raise an exception on timeout,\n            otherwise the method will return ``False``. The default is ``True`` (raise exception).\n        \"\"\"\n        timeout = kwargs.get('timeout', 30)\n        raise_exception = kwargs.get('raise_exception', True)\n\n        # Vehicle defaults for wait_ready(True) or wait_ready()\n        if list(types) == [True] or list(types) == []:\n            types = self._default_ready_attrs\n\n        if not all(isinstance(item, basestring) for item in types):\n            raise ValueError('wait_ready expects one or more string arguments.')\n\n        # Wait for these attributes to have been set.\n        await_attributes = set(types)\n        start = monotonic.monotonic()\n        still_waiting_last_message_sent = start\n        still_waiting_callback = kwargs.get('still_waiting_callback')\n        still_waiting_message_interval = kwargs.get('still_waiting_interval', 1)\n\n        while not await_attributes.issubset(self._ready_attrs):\n            time.sleep(0.1)\n            now = monotonic.monotonic()\n            if now - start > timeout:\n                if raise_exception:\n                    raise TimeoutError('wait_ready experienced a timeout after %s seconds.' %\n                                       timeout)\n                else:\n                    return False\n            if (still_waiting_callback and\n                    now - still_waiting_last_message_sent > still_waiting_message_interval):\n                still_waiting_last_message_sent = now\n                if still_waiting_callback:\n                    still_waiting_callback(await_attributes - self._ready_attrs)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reboot(self):\n\n        reboot_msg = self.message_factory.command_long_encode(\n            0, 0,  # target_system, target_component\n            mavutil.mavlink.MAV_CMD_PREFLIGHT_REBOOT_SHUTDOWN,  # command\n            0,  # confirmation\n            1,  # param 1, autopilot (reboot)\n            0,  # param 2, onboard computer (do nothing)\n            0,  # param 3, camera (do nothing)\n            0,  # param 4, mount (do nothing)\n            0, 0, 0)  # param 5 ~ 7 not used\n\n        self.send_mavlink(reboot_msg)", "response": "Request an autopilot reboot by sending a command long to the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_calibrate_accelerometer(self, simple=False):\n\n        calibration_command = self.message_factory.command_long_encode(\n            self._handler.target_system, 0,  # target_system, target_component\n            mavutil.mavlink.MAV_CMD_PREFLIGHT_CALIBRATION,  # command\n            0,  # confirmation\n            0,  # param 1, 1: gyro calibration, 3: gyro temperature calibration\n            0,  # param 2, 1: magnetometer calibration\n            0,  # param 3, 1: ground pressure calibration\n            0,  # param 4, 1: radio RC calibration, 2: RC trim calibration\n            4 if simple else 1,  # param 5, 1: accelerometer calibration, 2: board level calibration, 3: accelerometer temperature calibration, 4: simple accelerometer calibration\n            0,  # param 6, 2: airspeed calibration\n            0,  # param 7, 1: ESC calibration, 3: barometer temperature calibration\n        )\n        self.send_mavlink(calibration_command)", "response": "Request accelerometer calibration.\n\n        :param simple: if True, perform simple accelerometer calibration"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate(self, pitch, roll, yaw):\n        msg = self._vehicle.message_factory.mount_configure_encode(\n            0, 1,    # target system, target component\n            mavutil.mavlink.MAV_MOUNT_MODE_MAVLINK_TARGETING,  #mount_mode\n            1,  # stabilize roll\n            1,  # stabilize pitch\n            1,  # stabilize yaw\n        )\n        self._vehicle.send_mavlink(msg)\n        msg = self._vehicle.message_factory.mount_control_encode(\n            0, 1,    # target system, target component\n            pitch * 100,  # pitch is in centidegrees\n            roll * 100,  # roll\n            yaw * 100,  # yaw is in centidegrees\n            0  # save position\n        )\n        self._vehicle.send_mavlink(msg)", "response": "Rotate the gimbal to a specific vector."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the target location of the gimbal at a specific region of interest.", "response": "def target_location(self, roi):\n        \"\"\"\n        Point the gimbal at a specific region of interest (ROI).\n\n        .. code-block:: python\n\n            #Set the camera to track the current home location.\n            vehicle.gimbal.target_location(vehicle.home_location)\n\n        The target position must be defined in a :py:class:`LocationGlobalRelative` or :py:class:`LocationGlobal`.\n\n        This function can be called in AUTO or GUIDED mode.\n\n        In order to clear an ROI you can send a location with all zeros (e.g. ``LocationGlobalRelative(0,0,0)``).\n\n        :param roi: Target location in global relative frame.\n        \"\"\"\n        # set gimbal to targeting mode\n        msg = self._vehicle.message_factory.mount_configure_encode(\n            0, 1,    # target system, target component\n            mavutil.mavlink.MAV_MOUNT_MODE_GPS_POINT,  # mount_mode\n            1,  # stabilize roll\n            1,  # stabilize pitch\n            1,  # stabilize yaw\n        )\n        self._vehicle.send_mavlink(msg)\n\n        # Get altitude relative to home irrespective of Location object passed in.\n        if isinstance(roi, LocationGlobalRelative):\n            alt = roi.alt\n        elif isinstance(roi, LocationGlobal):\n            if not self.home_location:\n                self.commands.download()\n                self.commands.wait_ready()\n            alt = roi.alt - self.home_location.alt\n        else:\n            raise ValueError('Expecting location to be LocationGlobal or LocationGlobalRelative.')\n\n        # set the ROI\n        msg = self._vehicle.message_factory.command_long_encode(\n            0, 1,    # target system, target component\n            mavutil.mavlink.MAV_CMD_DO_SET_ROI,  # command\n            0,  # confirmation\n            0, 0, 0, 0,  # params 1-4\n            roi.lat,\n            roi.lon,\n            alt\n        )\n        self._vehicle.send_mavlink(msg)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef release(self):\n        msg = self._vehicle.message_factory.mount_configure_encode(\n            0, 1,    # target system, target component\n            mavutil.mavlink.MAV_MOUNT_MODE_RC_TARGETING,  # mount_mode\n            1,  # stabilize roll\n            1,  # stabilize pitch\n            1,  # stabilize yaw\n        )\n        self._vehicle.send_mavlink(msg)", "response": "Release the current vehicle configuration."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_attribute_listener(self, attr_name, *args, **kwargs):\n        attr_name = attr_name.upper()\n        return super(Parameters, self).add_attribute_listener(attr_name, *args, **kwargs)", "response": "Add a listener function to be invoked when a particular parameter is detected."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_attribute_listener(self, attr_name, *args, **kwargs):\n        attr_name = attr_name.upper()\n        return super(Parameters, self).remove_attribute_listener(attr_name, *args, **kwargs)", "response": "Removes a paremeter listener that was previously added using add_attribute_listener."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(self):\n        '''\n        Download all waypoints from the vehicle.\n        The download is asynchronous. Use :py:func:`wait_ready()` to block your thread until the download is complete.\n        '''\n        self.wait_ready()\n        self._vehicle._ready_attrs.remove('commands')\n        self._vehicle._wp_loaded = False\n        self._vehicle._master.waypoint_request_list_send()", "response": "Download all waypoints from the vehicle."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the command list.", "response": "def clear(self):\n        '''\n        Clear the command list.\n\n        This command will be sent to the vehicle only after you call :py:func:`upload() <Vehicle.commands.upload>`.\n        '''\n\n        # Add home point again.\n        self.wait_ready()\n        home = None\n        try:\n            home = self._vehicle._wploader.wp(0)\n        except:\n            pass\n        self._vehicle._wploader.clear()\n        if home:\n            self._vehicle._wploader.add(home, comment='Added by DroneKit')\n        self._vehicle._wpts_dirty = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add(self, cmd):\n        '''\n        Add a new command (waypoint) at the end of the command list.\n\n        .. note::\n\n            Commands are sent to the vehicle only after you call ::py:func:`upload() <Vehicle.commands.upload>`.\n\n        :param Command cmd: The command to be added.\n        '''\n        self.wait_ready()\n        self._vehicle._handler.fix_targets(cmd)\n        self._vehicle._wploader.add(cmd, comment='Added by DroneKit')\n        self._vehicle._wpts_dirty = True", "response": "Add a new command to the command list."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upload(self, timeout=None):\n        if self._vehicle._wpts_dirty:\n            self._vehicle._master.waypoint_clear_all_send()\n            start_time = time.time()\n            if self._vehicle._wploader.count() > 0:\n                self._vehicle._wp_uploaded = [False] * self._vehicle._wploader.count()\n                self._vehicle._master.waypoint_count_send(self._vehicle._wploader.count())\n                while False in self._vehicle._wp_uploaded:\n                    if timeout and time.time() - start_time > timeout:\n                        raise TimeoutError\n                    time.sleep(0.1)\n                self._vehicle._wp_uploaded = None\n            self._vehicle._wpts_dirty = False", "response": "Uploads a mission to the Waypoint."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\narm vehicle and fly to aTargetAltitude without GPS data.", "response": "def arm_and_takeoff_nogps(aTargetAltitude):\n    \"\"\"\n    Arms vehicle and fly to aTargetAltitude without GPS data.\n    \"\"\"\n\n    ##### CONSTANTS #####\n    DEFAULT_TAKEOFF_THRUST = 0.7\n    SMOOTH_TAKEOFF_THRUST = 0.6\n\n    print(\"Basic pre-arm checks\")\n    # Don't let the user try to arm until autopilot is ready\n    # If you need to disable the arming check,\n    # just comment it with your own responsibility.\n    while not vehicle.is_armable:\n        print(\" Waiting for vehicle to initialise...\")\n        time.sleep(1)\n\n\n    print(\"Arming motors\")\n    # Copter should arm in GUIDED_NOGPS mode\n    vehicle.mode = VehicleMode(\"GUIDED_NOGPS\")\n    vehicle.armed = True\n\n    while not vehicle.armed:\n        print(\" Waiting for arming...\")\n        vehicle.armed = True\n        time.sleep(1)\n\n    print(\"Taking off!\")\n\n    thrust = DEFAULT_TAKEOFF_THRUST\n    while True:\n        current_altitude = vehicle.location.global_relative_frame.alt\n        print(\" Altitude: %f  Desired: %f\" %\n              (current_altitude, aTargetAltitude))\n        if current_altitude >= aTargetAltitude*0.95: # Trigger just below target alt.\n            print(\"Reached target altitude\")\n            break\n        elif current_altitude >= aTargetAltitude*0.6:\n            thrust = SMOOTH_TAKEOFF_THRUST\n        set_attitude(thrust = thrust)\n        time.sleep(0.2)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_attitude_target(roll_angle = 0.0, pitch_angle = 0.0,\n                         yaw_angle = None, yaw_rate = 0.0, use_yaw_rate = False,\n                         thrust = 0.5):\n    \"\"\"\n    use_yaw_rate: the yaw can be controlled using yaw_angle OR yaw_rate.\n                  When one is used, the other is ignored by Ardupilot.\n    thrust: 0 <= thrust <= 1, as a fraction of maximum vertical thrust.\n            Note that as of Copter 3.5, thrust = 0.5 triggers a special case in\n            the code for maintaining current altitude.\n    \"\"\"\n    if yaw_angle is None:\n        # this value may be unused by the vehicle, depending on use_yaw_rate\n        yaw_angle = vehicle.attitude.yaw\n    # Thrust >  0.5: Ascend\n    # Thrust == 0.5: Hold the altitude\n    # Thrust <  0.5: Descend\n    msg = vehicle.message_factory.set_attitude_target_encode(\n        0, # time_boot_ms\n        1, # Target system\n        1, # Target component\n        0b00000000 if use_yaw_rate else 0b00000100,\n        to_quaternion(roll_angle, pitch_angle, yaw_angle), # Quaternion\n        0, # Body roll rate in radian\n        0, # Body pitch rate in radian\n        math.radians(yaw_rate), # Body yaw rate in radian/second\n        thrust  # Thrust\n    )\n    vehicle.send_mavlink(msg)", "response": "Set the current altitude of the vehicle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_attitude(roll_angle = 0.0, pitch_angle = 0.0,\n                 yaw_angle = None, yaw_rate = 0.0, use_yaw_rate = False,\n                 thrust = 0.5, duration = 0):\n    \"\"\"\n    Note that from AC3.3 the message should be re-sent more often than every\n    second, as an ATTITUDE_TARGET order has a timeout of 1s.\n    In AC3.2.1 and earlier the specified attitude persists until it is canceled.\n    The code below should work on either version.\n    Sending the message multiple times is the recommended way.\n    \"\"\"\n    send_attitude_target(roll_angle, pitch_angle,\n                         yaw_angle, yaw_rate, False,\n                         thrust)\n    start = time.time()\n    while time.time() - start < duration:\n        send_attitude_target(roll_angle, pitch_angle,\n                             yaw_angle, yaw_rate, False,\n                             thrust)\n        time.sleep(0.1)\n    # Reset attitude, or it will persist for 1s more due to the timeout\n    send_attitude_target(0, 0,\n                         0, 0, True,\n                         thrust)", "response": "Send an ATTITUDE_TARGET message to the available resource classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_quaternion(roll = 0.0, pitch = 0.0, yaw = 0.0):\n    t0 = math.cos(math.radians(yaw * 0.5))\n    t1 = math.sin(math.radians(yaw * 0.5))\n    t2 = math.cos(math.radians(roll * 0.5))\n    t3 = math.sin(math.radians(roll * 0.5))\n    t4 = math.cos(math.radians(pitch * 0.5))\n    t5 = math.sin(math.radians(pitch * 0.5))\n\n    w = t0 * t2 * t4 + t1 * t3 * t5\n    x = t0 * t3 * t4 - t1 * t2 * t5\n    y = t0 * t2 * t5 + t1 * t3 * t4\n    z = t1 * t2 * t4 - t0 * t3 * t5\n\n    return [w, x, y, z]", "response": "Convert degrees to quaternions"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute the normalized term frequency as explained in http://www. tfidf. com", "response": "def _compute_tf(self, sentences):\n        \"\"\"\n        Computes the normalized term frequency as explained in http://www.tfidf.com/\n        \"\"\"\n        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((k, v / content_words_count) for (k, v) in content_words_freq.items())\n        return content_word_tf"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef f_score(evaluated_sentences, reference_sentences, weight=1.0):\n    p = precision(evaluated_sentences, reference_sentences)\n    r = recall(evaluated_sentences, reference_sentences)\n\n    weight **= 2 # weight = weight^2\n    denominator = weight * p + r\n    if denominator == 0.0:\n        return 0.0\n    else:\n        return ((weight + 1) * p * r) / denominator", "response": "Compute the F - Score of the given set of evaluated sentences and reference sentences."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the length of the Longest Common Subsequence between sequences x and y.", "response": "def _len_lcs(x, y):\n    \"\"\"\n    Returns the length of the Longest Common Subsequence between sequences x\n    and y.\n    Source: http://www.algorithmist.com/index.php/Longest_Common_Subsequence\n\n    :param x: sequence of words\n    :param y: sequence of words\n    :returns integer: Length of LCS between x and y\n    \"\"\"\n    table = _lcs(x, y)\n    n, m = _get_index_of_lcs(x, y)\n    return table[n, m]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _recon_lcs(x, y):\n    table = _lcs(x, y)\n\n    def _recon(i, j):\n        if i == 0 or j == 0:\n            return []\n        elif x[i - 1] == y[j - 1]:\n            return _recon(i - 1, j - 1) + [(x[i - 1], i)]\n        elif table[i - 1, j] > table[i, j - 1]:\n            return _recon(i - 1, j)\n        else:\n            return _recon(i, j - 1)\n\n    i, j = _get_index_of_lcs(x, y)\n    recon_tuple = tuple(map(lambda r: r[0], _recon(i, j)))\n    return recon_tuple", "response": "Returns the Longest Common Subsequence between x and y."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the F - measure of the LCS - based candidate.", "response": "def _f_lcs(llcs, m, n):\n    \"\"\"\n    Computes the LCS-based F-measure score\n    Source: http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n    rouge-working-note-v1.3.1.pdf\n\n    :param llcs: Length of LCS\n    :param m: number of words in reference summary\n    :param n: number of words in candidate summary\n    :returns float: LCS-based F-measure score\n    \"\"\"\n    r_lcs = llcs / m\n    p_lcs = llcs / n\n    beta = p_lcs / r_lcs\n    num = (1 + (beta ** 2)) * r_lcs * p_lcs\n    denom = r_lcs + ((beta ** 2) * p_lcs)\n    return num / denom"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes ROUGE - L sentence level of two text collections of sentences.", "response": "def rouge_l_sentence_level(evaluated_sentences, reference_sentences):\n    \"\"\"\n    Computes ROUGE-L (sentence level) of two text collections of sentences.\n    http://research.microsoft.com/en-us/um/people/cyl/download/papers/\n    rouge-working-note-v1.3.1.pdf\n\n    Calculated according to:\n    R_lcs = LCS(X,Y)/m\n    P_lcs = LCS(X,Y)/n\n    F_lcs = ((1 + beta^2)*R_lcs*P_lcs) / (R_lcs + (beta^2) * P_lcs)\n\n    where:\n    X = reference summary\n    Y = Candidate summary\n    m = length of reference summary\n    n = length of candidate summary\n\n    :param evaluated_sentences:\n        The sentences that have been picked by the summarizer\n    :param reference_sentences:\n        The sentences from the referene set\n    :returns float: F_lcs\n    :raises ValueError: raises exception if a param has len <= 0\n    \"\"\"\n    if len(evaluated_sentences) <= 0 or len(reference_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n    reference_words = _split_into_words(reference_sentences)\n    evaluated_words = _split_into_words(evaluated_sentences)\n    m = len(reference_words)\n    n = len(evaluated_words)\n    lcs = _len_lcs(evaluated_words, reference_words)\n    return _f_lcs(lcs, m, n)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the LCS score of the union of the two given sentences.", "response": "def _union_lcs(evaluated_sentences, reference_sentence):\n    \"\"\"\n    Returns LCS_u(r_i, C) which is the LCS score of the union longest common subsequence\n    between reference sentence ri and candidate summary C. For example, if\n    r_i= w1 w2 w3 w4 w5, and C contains two sentences: c1 = w1 w2 w6 w7 w8 and\n    c2 = w1 w3 w8 w9 w5, then the longest common subsequence of r_i and c1 is\n    \u201cw1 w2\u201d and the longest common subsequence of r_i and c2 is \u201cw1 w3 w5\u201d. The\n    union longest common subsequence of r_i, c1, and c2 is \u201cw1 w2 w3 w5\u201d and\n    LCS_u(r_i, C) = 4/5.\n\n    :param evaluated_sentences:\n        The sentences that have been picked by the summarizer\n    :param reference_sentence:\n        One of the sentences in the reference summaries\n    :returns float: LCS_u(r_i, C)\n    :raises ValueError: raises exception if a param has len <= 0\n    \"\"\"\n    if len(evaluated_sentences) <= 0:\n        raise (ValueError(\"Collections must contain at least 1 sentence.\"))\n\n    lcs_union = set()\n    reference_words = _split_into_words([reference_sentence])\n    combined_lcs_length = 0\n    for eval_s in evaluated_sentences:\n        evaluated_words = _split_into_words([eval_s])\n        lcs = set(_recon_lcs(reference_words, evaluated_words))\n        combined_lcs_length += len(lcs)\n        lcs_union = lcs_union.union(lcs)\n\n    union_lcs_count = len(lcs_union)\n    union_lcs_value = union_lcs_count / combined_lcs_length\n    return union_lcs_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the magnitude of the sequence.", "response": "def magnitude(self):\n        \"\"\"\n        Lenght/norm/magnitude of vector representation of document.\n        This is usually denoted by ||d||.\n        \"\"\"\n        return math.sqrt(sum(t**2 for t in self._terms.values()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the most frequent terms in descending order.", "response": "def most_frequent_terms(self, count=0):\n        \"\"\"\n        Returns ``count`` of terms sorted by their frequency\n        in descending order.\n\n        :parameter int count:\n            Max. number of returned terms. Value 0 means no limit (default).\n        \"\"\"\n        # sort terms by number of occurrences in descending order\n        terms = sorted(self._terms.items(), key=lambda i: -i[1])\n\n        terms = tuple(i[0] for i in terms)\n        if count == 0:\n            return terms\n        elif count > 0:\n            return terms[:count]\n        else:\n            raise ValueError(\n                \"Only non-negative values are allowed for count of terms.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalized_term_frequency(self, term, smooth=0.0):\n        frequency = self.term_frequency(term) / self._max_frequency\n        return smooth + (1.0 - smooth)*frequency", "response": "Returns the normalized frequency of a term in document."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _count_words(self, words):\n        bonus_words_count = 0\n        stigma_words_count = 0\n\n        for word in words:\n            if word in self._bonus_words:\n                bonus_words_count +=1\n            if word in self._stigma_words:\n                stigma_words_count += 1\n\n        return bonus_words_count, stigma_words_count", "response": "Counts the number of bonus and stigma words."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cached_property(getter):\n    @wraps(getter)\n    def decorator(self):\n        key = \"_cached_property_\" + getter.__name__\n\n        if not hasattr(self, key):\n            setattr(self, key, getter(self))\n\n        return getattr(self, key)\n\n    return property(decorator)", "response": "Decorator that converts a method into memoized property."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute cosine similarity of two text documents.", "response": "def cosine_similarity(evaluated_model, reference_model):\n    \"\"\"\n    Computes cosine similarity of two text documents. Each document\n    has to be represented as TF model of non-empty document.\n\n    :returns float:\n        0 <= cos <= 1, where 0 means independence and 1 means\n        exactly the same.\n    \"\"\"\n    if not (isinstance(evaluated_model, TfModel) and isinstance(reference_model, TfModel)):\n        raise ValueError(\n            \"Arguments has to be instances of 'sumy.models.TfDocumentModel'\")\n\n    terms = frozenset(evaluated_model.terms) | frozenset(reference_model.terms)\n\n    numerator = 0.0\n    for term in terms:\n        numerator += evaluated_model.term_frequency(term) * reference_model.term_frequency(term)\n\n    denominator = evaluated_model.magnitude * reference_model.magnitude\n    if denominator == 0.0:\n        raise ValueError(\"Document model can't be empty. Given %r & %r\" % (\n            evaluated_model, reference_model))\n\n    return numerator / denominator"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the unit overlap of two text documents.", "response": "def unit_overlap(evaluated_model, reference_model):\n    \"\"\"\n    Computes unit overlap of two text documents. Documents\n    has to be represented as TF models of non-empty document.\n\n    :returns float:\n        0 <= overlap <= 1, where 0 means no match and 1 means\n        exactly the same.\n    \"\"\"\n    if not (isinstance(evaluated_model, TfModel) and isinstance(reference_model, TfModel)):\n        raise ValueError(\n            \"Arguments has to be instances of 'sumy.models.TfDocumentModel'\")\n\n    terms1 = frozenset(evaluated_model.terms)\n    terms2 = frozenset(reference_model.terms)\n\n    if not terms1 and not terms2:\n        raise ValueError(\n            \"Documents can't be empty. Please pass the valid documents.\")\n\n    common_terms_count = len(terms1 & terms2)\n    return common_terms_count / (len(terms1) + len(terms2) - common_terms_count)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a dictionary of words to row indices.", "response": "def _create_dictionary(self, document):\n        \"\"\"Creates mapping key = word, value = row index\"\"\"\n        words = map(self.normalize_word, document.words)\n        unique_words = frozenset(self.stem_word(w) for w in words if w not in self._stop_words)\n\n        return dict((w, i) for i, w in enumerate(unique_words))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates matrix of shape |unique words|\u00d7|sentences| where cells contains number of occurences of words in senteces.", "response": "def _create_matrix(self, document, dictionary):\n        \"\"\"\n        Creates matrix of shape |unique words|\u00d7|sentences| where cells\n        contains number of occurences of words (rows) in senteces (cols).\n        \"\"\"\n        sentences = document.sentences\n\n        words_count = len(dictionary)\n        sentences_count = len(sentences)\n        if words_count < sentences_count:\n            message = (\n                \"Number of words (%d) is lower than number of sentences (%d). \"\n                \"LSA algorithm may not work properly.\"\n            )\n            warn(message % (words_count, sentences_count))\n\n        # create matrix |unique words|\u00d7|sentences| filled with zeroes\n        matrix = numpy.zeros((words_count, sentences_count))\n        for col, sentence in enumerate(sentences):\n            for word in map(self.stem_word, sentence.words):\n                # only valid words is counted (not stop-words, ...)\n                if word in dictionary:\n                    row = dictionary[word]\n                    matrix[row, col] += 1\n\n        return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_term_frequency(self, matrix, smooth=0.4):\n        assert 0.0 <= smooth < 1.0\n\n        max_word_frequencies = numpy.max(matrix, axis=0)\n        rows, cols = matrix.shape\n        for row in range(rows):\n            for col in range(cols):\n                max_word_frequency = max_word_frequencies[col]\n                if max_word_frequency != 0:\n                    frequency = matrix[row, col]/max_word_frequency\n                    matrix[row, col] = smooth + (1.0 - smooth)*frequency\n\n        return matrix", "response": "Compute the term frequency of each sentence in the given matrix."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_matrix(self, document):\n        sentences_as_words = [self._to_words_set(sent) for sent in document.sentences]\n        sentences_count = len(sentences_as_words)\n        weights = numpy.zeros((sentences_count, sentences_count))\n\n        for i, words_i in enumerate(sentences_as_words):\n            for j, words_j in enumerate(sentences_as_words):\n                weights[i, j] = self._rate_sentences_edge(words_i, words_j)\n        weights /= weights.sum(axis=1)[:, numpy.newaxis]\n\n        # In the original paper, the probability of randomly moving to any of the vertices\n        # is NOT divided by the number of vertices. Here we do divide it so that the power\n        # method works; without this division, the stationary probability blows up. This\n        # should not affect the ranking of the vertices so we can use the resulting stationary\n        # probability as is without any postprocessing.\n        return numpy.full((sentences_count, sentences_count), (1.-self.damping) / sentences_count) \\\n            + self.damping * weights", "response": "Create a stochastic matrix for TextRank."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the normalized term frequency as explained in http://www. tfidf. com /", "response": "def compute_tf(self, sentences):\n        \"\"\"\n        Computes the normalized term frequency as explained in http://www.tfidf.com/\n\n        :type sentences: [sumy.models.dom.Sentence]\n        \"\"\"\n        content_words = self._get_all_content_words_in_doc(sentences)\n        content_words_count = len(content_words)\n        content_words_freq = self._compute_word_freq(content_words)\n        content_word_tf = dict((w, f / content_words_count) for w, f in content_words_freq.items())\n        return content_word_tf"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _kl_divergence(self, summary_freq, doc_freq):\n        sum_val = 0\n        for w in summary_freq:\n            frequency = doc_freq.get(w)\n            if frequency:  # missing or zero = no frequency\n                sum_val += frequency * math.log(frequency / summary_freq[w])\n\n        return sum_val", "response": "Compute KL divergence of the resource frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __remove_trailing_zeros(self, collection):\n        index = len(collection) - 1\n        while index >= 0 and collection[index] == 0:\n            index -= 1\n\n        return collection[:index + 1]", "response": "Removes trailing zeroes from indexable collection of numbers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating matrix of shape |sentences|\u00d7|sentences|.", "response": "def _create_matrix(self, sentences, threshold, tf_metrics, idf_metrics):\n        \"\"\"\n        Creates matrix of shape |sentences|\u00d7|sentences|.\n        \"\"\"\n        # create matrix |sentences|\u00d7|sentences| filled with zeroes\n        sentences_count = len(sentences)\n        matrix = numpy.zeros((sentences_count, sentences_count))\n        degrees = numpy.zeros((sentences_count, ))\n\n        for row, (sentence1, tf1) in enumerate(zip(sentences, tf_metrics)):\n            for col, (sentence2, tf2) in enumerate(zip(sentences, tf_metrics)):\n                matrix[row, col] = self.cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics)\n\n                if matrix[row, col] > threshold:\n                    matrix[row, col] = 1.0\n                    degrees[row] += 1\n                else:\n                    matrix[row, col] = 0\n\n        for row in range(sentences_count):\n            for col in range(sentences_count):\n                if degrees[row] == 0:\n                    degrees[row] = 1\n\n                matrix[row][col] = matrix[row][col] / degrees[row]\n\n        return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cosine_similarity(sentence1, sentence2, tf1, tf2, idf_metrics):\n        unique_words1 = frozenset(sentence1)\n        unique_words2 = frozenset(sentence2)\n        common_words = unique_words1 & unique_words2\n\n        numerator = 0.0\n        for term in common_words:\n            numerator += tf1[term]*tf2[term] * idf_metrics[term]**2\n\n        denominator1 = sum((tf1[t]*idf_metrics[t])**2 for t in unique_words1)\n        denominator2 = sum((tf2[t]*idf_metrics[t])**2 for t in unique_words2)\n\n        if denominator1 > 0 and denominator2 > 0:\n            return numerator / (math.sqrt(denominator1) * math.sqrt(denominator2))\n        else:\n            return 0.0", "response": "This function calculates cosine similarity between two sentences."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts the passed in type to the correct type for the next node.", "response": "def to_weld_type(weld_type, dim):\n    \"\"\"Summary\n\n    Args:\n        weld_type (TYPE): Description\n        dim (TYPE): Description\n\n    Returns:\n        TYPE: Description\n    \"\"\"\n    for i in xrange(dim):\n        weld_type = WeldVec(weld_type)\n    return weld_type"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate(self, verbose=True, decode=True, passes=None, num_threads=1,\n                 apply_experimental_transforms=False):\n        \"\"\"Summary\n\n        Args:\n            verbose (bool, optional): Description\n            decode (bool, optional): Description\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        if isinstance(self.expr, WeldObject):\n            return self.expr.evaluate(\n                to_weld_type(\n                    self.weld_type,\n                    self.dim),\n                verbose,\n                decode,\n                passes=passes,\n                num_threads=num_threads,\n                apply_experimental_transforms=apply_experimental_transforms)\n        return self.expr", "response": "Evaluates the object s expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _gen_weldobj(self, arr):\n        '''\n        Generating a new weldarray from a given arr for self.\n        @arr: weldarray or ndarray.\n            - weldarray: Just update the weldobject with the context from the\n              weldarray.\n            - ndarray: Add the given array to the context of the weldobject.\n        Sets self.name and self.weldobj.\n        '''\n        self.weldobj = WeldObject(NumpyArrayEncoder(), NumpyArrayDecoder())\n        if isinstance(arr, weldarray):\n            self.weldobj.update(arr.weldobj)\n            self.weldobj.weld_code = arr.weldobj.weld_code\n            self.name = arr.name\n        else:\n            # general case for arr being numpy scalar or ndarray\n            # weldobj returns the name bound to the given array. That is also\n            # the array that future ops will act on, so set weld_code to it.\n            self.name = self.weldobj.weld_code = self.weldobj.update(arr,\n                    SUPPORTED_DTYPES[str(arr.dtype)])", "response": "Generates a new weldarray from a given array for self.\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _process_ufunc_inputs(self, input_args, outputs):\n        '''\n        Helper function for __array_ufunc__ that deals with the input/output checks and determines\n        if this should be relegated to numpy's implementation or not.\n        @input_args: args to __array_ufunc__\n        @outputs: specified outputs.\n\n        @ret: Bool, If weld supports the given input/output format or not - if weld doesn't then\n        will just pass it on to numpy.\n        '''\n        if len(input_args) > 2:\n            return False\n\n        arrays = []\n        scalars = []\n        for i in input_args:\n            if isinstance(i, np.ndarray):\n                if not str(i.dtype) in SUPPORTED_DTYPES:\n                    return False\n                if len(i) == 0:\n                    return False\n                arrays.append(i)\n            elif isinstance(i, list):\n                return False\n            else:\n                scalars.append(i)\n\n        if len(arrays) == 2 and arrays[0].dtype != arrays[1].dtype:\n            return False\n\n        # handle all scalar based tests here - later will just assume that scalar type is correct,\n        # and use the suffix based on the weldarray's type.\n        # TODO: add more scalar tests for python int/float: floats -> f32, f64, or ints -> i32, i64 need to\n        # match).\n        elif len(arrays) == 1 and len(scalars) == 1:\n            # need to test for bool before int because it True would be instance of int as well.\n            if isinstance(scalars[0], bool):\n                return False\n            elif isinstance(scalars[0], float):\n                pass\n            elif isinstance(scalars[0], int):\n                pass\n            # assuming its np.float32 etc.\n            elif not str(scalars[0].dtype) in SUPPORTED_DTYPES:\n                return False\n            else:\n                weld_type = SUPPORTED_DTYPES[str(scalars[0].dtype)]\n                if weld_type != arrays[0]._weld_type:\n                    return False\n        # check ouput.\n        if outputs:\n            # if the output is not weldarray, then let np deal with it.\n            if not (len(outputs) == 1 and isinstance(outputs[0], weldarray)):\n                return False\n\n        return True", "response": "This function handles the input and output checks and determines\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrelegating responsibility of executing ufunc to numpy.", "response": "def _handle_numpy(self, ufunc, method, input_args, outputs, kwargs):\n        '''\n        relegate responsibility of executing ufunc to numpy.\n        '''\n        # Relegating the work to numpy. If input arg is weldarray, evaluate it,\n        # and convert to ndarray before passing to super()\n        for i, arg_ in enumerate(input_args):\n            if isinstance(arg_, weldarray):\n                # Evaluate all the lazily stored computations first.\n                input_args[i] = arg_._eval()\n\n        if outputs:\n            out_args = []\n            for j, output in enumerate(outputs):\n                if isinstance(output, weldarray):\n                    out_args.append(output.view(np.ndarray))\n                else:\n                    out_args.append(output)\n            kwargs['out'] = tuple(out_args)\n        else:\n            outputs = (None,) * ufunc.nout\n\n        result = super(weldarray, self).__array_ufunc__(ufunc, method, *input_args, **kwargs)\n\n        # if possible, return weldarray.\n        if str(result.dtype) in SUPPORTED_DTYPES and isinstance(result, np.ndarray):\n            return weldarray(result, verbose=self._verbose)\n        else:\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling the call to get the related object.", "response": "def _handle_call(self, ufunc, input_args, outputs):\n        '''\n        TODO: add description\n        '''\n        # if output is not None, choose the first one - since we don't support any ops with\n        # multiple outputs.\n        if outputs: output = outputs[0]\n        else: output = None\n\n        # check for supported ops.\n        if ufunc.__name__ in UNARY_OPS:\n            assert(len(input_args) == 1)\n            return self._unary_op(UNARY_OPS[ufunc.__name__], result=output)\n\n        if ufunc.__name__ in BINARY_OPS:\n            # weldarray can be first or second arg.\n            if isinstance(input_args[0], weldarray):\n                # first arg is weldarray, must be self\n                assert input_args[0].name == self.name\n                other_arg = input_args[1]\n            else:\n                other_arg = input_args[0]\n                assert input_args[1].name == self.name\n\n            return self._binary_op(other_arg, BINARY_OPS[ufunc.__name__], result=output)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the reduce operation.", "response": "def _handle_reduce(self, ufunc, input_args, outputs):\n        '''\n        TODO: describe.\n        TODO: For multi-dimensional case, might need extra work here if the reduction is being\n        performed only along a certain axis.\n        We force evaluation at the end of the reduction - weird errors if we don't. This seems\n        safer anyway.\n        np supports reduce only for binary ops.\n        '''\n        # input_args[0] must be self so it can be ignored.\n        assert len(input_args) == 1\n        if outputs: output = outputs[0]\n        else: output = None\n        if ufunc.__name__ in BINARY_OPS:\n            return self._reduce_op(BINARY_OPS[ufunc.__name__], result=output)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reduce_op(self, op, result=None):\n        '''\n        TODO: support for multidimensional arrays.\n        '''\n        template = 'result(for({arr},merger[{type}, {op}], |b, i, e| merge(b, e)))'\n        self.weldobj.weld_code = template.format(arr = self.weldobj.weld_code,\n                                                 type = self._weld_type.__str__(),\n                                                 op  = op)\n        return self._eval(restype = self._weld_type)", "response": "Reduce the set of entries in the database by a given operation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new result from self.", "response": "def _get_result(self):\n        '''\n        Creating a new result weldarray from self. If self is view into a weldarray, then evaluate\n        the parent first as self would not be storing the ops that have been registered to it (only\n        base_array would store those).\n        '''\n        if self._weldarray_view:\n            idx = self._weldarray_view.idx\n            result = weldarray(self._weldarray_view.parent._eval()[idx], verbose=self._verbose)\n        else:\n            result = weldarray(self, verbose=self._verbose)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _unary_op(self, unop, result=None):\n        '''\n        @unop: str, weld IR for the given function.\n        @result: output array.\n\n        Create a new array, and updates weldobj.weld_code with the code for\n        unop.\n        '''\n        def _update_array_unary_op(res, unop):\n            '''\n            @res: weldarray to be updated.\n            @unop: str, operator applied to res.\n            '''\n            template = 'map({arr}, |z : {type}| {unop}(z))'\n            code = template.format(arr  = res.weldobj.weld_code,\n                                   type = res._weld_type.__str__(),\n                                   unop = unop)\n            res.weldobj.weld_code = code\n\n        if result is None:\n            result = self._get_result()\n        else:\n            # in place op. If is a view, just update base array and return.\n            if result._weldarray_view:\n                v = result._weldarray_view\n                update_str = '{unop}(e)'.format(unop=unop)\n                v.base_array._update_range(v.start, v.end, update_str)\n                return result\n\n        # back to updating result array\n        _update_array_unary_op(result, unop)\n        return result", "response": "Internal function to apply an atomic operation to the base array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _scalar_binary_op(self, other, binop, result):\n        '''\n        Helper function for _binary_op.\n        @other, scalar values (i32, i64, f32, f64).\n        @result: weldarray to store results in.\n        '''\n        template = 'map({arr}, |z: {type}| z {binop} {other}{suffix})'\n        weld_type = result._weld_type.__str__()\n        result.weldobj.weld_code = template.format(arr = result.weldobj.weld_code,\n                                                  type =  weld_type,\n                                                  binop = binop,\n                                                  other = str(other),\n                                                  suffix = DTYPE_SUFFIXES[weld_type])\n        return result", "response": "Internal helper function for _binary_op."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_views_binary(self, result, other, binop):\n        '''\n        @result: weldarray that is being updated\n        @other: weldarray or scalar. (result binop other)\n        @binop: str, operation to perform.\n\n        FIXME: the common indexing pattern for parent/child in _update_view might be too expensive\n        (uses if statements (unneccessary checks when updating child) and wouldn't be ideal to\n        update a large parent).\n        '''\n        update_str_template = '{e2}{binop}e'\n        v = result._weldarray_view\n        if isinstance(other, weldarray):\n            lookup_ind = 'i-{st}'.format(st=v.start)\n            # update the base array to include the context from other\n            v.base_array.weldobj.update(other.weldobj)\n            e2 = 'lookup({arr2},{i}L)'.format(arr2 = other.weldobj.weld_code, i = lookup_ind)\n        else:\n            # other is just a scalar.\n            e2 = str(other) + DTYPE_SUFFIXES[result._weld_type.__str__()]\n\n        update_str = update_str_template.format(e2 = e2, binop=binop)\n        v.base_array._update_range(v.start, v.end, update_str)", "response": "Update the views of the result array with the binary operation binop."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_column_names(self):\n        column_names = set()\n        for column in self.df:\n            column_names.add(column)\n        for column in self.unmaterialized_cols:\n            column_names.add(column)\n        return list(column_names)", "response": "Returns a list of column names that are used in the summary and description of the log entries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter(self, predicates):\n        tys = []\n        for col_name, raw_column in self.raw_columns.items():\n            dtype = str(raw_column.dtype)\n            if dtype == 'object' or dtype == '|S64':\n                weld_type = WeldVec(WeldChar())\n            else:\n                weld_type = grizzly_impl.numpy_to_weld_type_mapping[dtype]\n            tys.append(weld_type)\n\n        if len(tys) == 1:\n            weld_type = tys[0]\n        else:\n            weld_type = WeldStruct(tys)\n\n        if isinstance(predicates, SeriesWeld):\n            predicates = predicates.expr\n\n        return DataFrameWeldExpr(\n            grizzly_impl.filter(\n                grizzly_impl.zip_columns(\n                    self.raw_columns.values(),\n                ),\n                predicates\n            ),\n            self.raw_columns.keys(),\n            weld_type\n        )", "response": "Summary of the filter function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating the current instance of the object and returns a DataFrameWeld with the result.", "response": "def evaluate(self, verbose=True, passes=None):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        if self.is_pivot:\n            index, pivot, columns = LazyOpResult(\n                self.expr,\n                self.weld_type,\n                0\n            ).evaluate(verbose=verbose, passes=passes)\n            df_dict = {}\n            for i, column_name in enumerate(columns):\n                df_dict[column_name] = pivot[i]\n            return DataFrameWeld(pd.DataFrame(df_dict, index=index))\n        else:\n            df = pd.DataFrame(columns=[])\n            weldvec_type_list = []\n            for type in self.column_types:\n                weldvec_type_list.append(WeldVec(type))\n\n            columns = LazyOpResult(\n                grizzly_impl.unzip_columns(\n                    self.expr,\n                    self.column_types\n                ),\n                WeldStruct(weldvec_type_list),\n                0\n            ).evaluate(verbose=verbose, passes=passes)\n\n            for i, column_name in enumerate(self.column_names):\n                df[column_name] = columns[i]\n\n            return DataFrameWeld(df)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sort_values(self, ascending=False):\n        if self.index_type is not None:\n            index_expr = grizzly_impl.get_field(self.expr, 0)\n            column_expr = grizzly_impl.get_field(self.expr, 1)\n            zip_expr = grizzly_impl.zip_columns([index_expr, column_expr])\n            result_expr = grizzly_impl.sort(zip_expr, 1, self.weld_type, ascending)\n            unzip_expr = grizzly_impl.unzip_columns(\n                result_expr,\n                [self.index_type, self.weld_type]\n            )\n            return SeriesWeld(\n                unzip_expr,\n                self.weld_type,\n                self.df,\n                self.column_name,\n                self.index_type,\n                self.index_name\n            )\n        else:\n            result_expr = grizzly_impl.sort(self.expr)", "response": "Sorts the values of this SeriesWeld object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a copy of the SeriesWeld with the lower cased version of the object.", "response": "def lower(self):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        # TODO : Bug in nested map operating on strings\n        # TODO : Check that self.weld_type is a string type\n        vectype = self.weld_type\n        if isinstance(vectype, WeldVec):\n            elem_type = vectype.elemType\n            if isinstance(elem_type, WeldChar):\n                        return SeriesWeld(\n                            grizzly_impl.to_lower(\n                                self.expr,\n                                elem_type\n                            ),\n                            self.weld_type,\n                            self.df,\n                            self.column_name\n                        )\n        raise Exception(\"Cannot call to_lower on non string type\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prod(self):\n        return LazyOpResult(\n            grizzly_impl.aggr(\n                self.expr,\n                \"*\",\n                1,\n                self.weld_type\n            ),\n            self.weld_type,\n            0\n        )", "response": "Summary\n            descuns a n - ary object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a SeriesWeld object with the new value masked.", "response": "def mask(self, predicates, new_value):\n        \"\"\"Summary\n\n        Args:\n            predicates (TYPE): Description\n            new_value (TYPE): Description\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        if isinstance(predicates, SeriesWeld):\n            predicates = predicates.expr\n        return SeriesWeld(\n            grizzly_impl.mask(\n                self.expr,\n                predicates,\n                new_value,\n                self.weld_type\n            ),\n            self.weld_type,\n            self.df,\n            self.column_name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add(self, other):\n        if isinstance(other, SeriesWeld):\n            other = other.expr\n        return SeriesWeld(\n            grizzly_impl.element_wise_op(\n                self.expr,\n                other,\n                \"+\",\n                self.weld_type\n            ),\n            self.weld_type,\n            self.df,\n            self.column_name\n        )", "response": "Returns a SeriesWeld object with the addition of the other object to self."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slice(self, start, size):\n        return SeriesWeld(\n            grizzly_impl.slice(\n                self.expr,\n                start,\n                size,\n                self.weld_type\n            ),\n            self.weld_type,\n            self.df,\n            self.column_name\n        )", "response": "Returns a copy of the SeriesWeld object with the slice operation applied to the Series object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update(self, value, tys=None, override=True):\n        if isinstance(value, WeldObject):\n            self.context.update(value.context)\n        else:\n            # Ensure that the same inputs always have same names\n            value_str = str(value)\n            if value_str in WeldObject._registry:\n                name = WeldObject._registry[value_str]\n            else:\n                name = WeldObject.generate_input_name(value_str)\n            self.context[name] = value\n            if tys is not None and not override:\n                self.argtypes[name] = tys\n            return name", "response": "Update the internal context with the given value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sum(self):\n        return GroupedDataFrameWeld(\n            grizzly_impl.groupby_sum(\n                self.columns,\n                self.column_types,\n                self.grouping_columns,\n                self.grouping_column_types\n            ),\n            self.grouping_column_names,\n            self.column_names,\n            self.grouping_column_types,\n            self.column_types\n        )", "response": "Summary\n\n        Returns:\n            TYPE: Description"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the sizes of the groups as SeriesWeld", "response": "def size(self):\n        \"\"\"Returns the sizes of the groups as series.\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        if len(self.grouping_column_types) > 1:\n            index_type = WeldStruct([self.grouping_column_types])\n            # Figure out what to use for multi-key index name\n            # index_name = ??\n        else:\n            index_type = self.grouping_column_types[0]\n            index_name = self.grouping_column_names[0]\n        return SeriesWeld(\n            grizzly_impl.groupby_size(\n                self.columns,\n                self.column_types,\n                self.grouping_columns,\n                self.grouping_column_types\n            ),\n            WeldLong(),\n            index_type=index_type,\n            index_name=index_name\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_column(self, column_name, column_type, index, verbose=True):\n        return LazyOpResult(\n            grizzly_impl.get_column(\n                self.expr,\n                self.weld_type,\n                index\n            ),\n            column_type,\n            1\n        )", "response": "Returns the value of a column in the navigated table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nflatten the grouped data structure.", "response": "def reset_index(self, inplace=True, drop=True):\n        \"\"\" Flattens the grouped data structure.\n\n        #TODO: The parameters here are meaningless\n\n        Flattens the grouped data structure.\n        What is returned is a DataFrameWeld object.\n        \"\"\"\n        if len(self.column_types) == 1:\n            vectype = self.column_types[0]\n            if isinstance(vectype, WeldVec):\n                elem_type = vectype.elemType\n                if isinstance(elem_type, WeldStruct):\n                    self.column_types = elem_type.field_types\n                    value_type = WeldStruct(self.column_types)\n                else:\n                    self.column_types = elem_type\n                    value_type = elem_type\n                if len(self.grouping_column_types) == 1:\n                    group_type = self.grouping_column_types[0]\n                else:\n                    group_type = WeldStruct(self.grouping_column_types)\n\n                self.weld_type = WeldStruct([group_type, value_type])\n                self.expr = grizzly_impl.flatten_group(\n                    self.expr,\n                    self.column_types,\n                    self.grouping_column_types\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef evaluate(self, verbose=True, passes=None):\n        i = 0\n        exprs = []\n        for column_name in self.column_names:\n            if len(self.column_names) > 1:\n                index = \"1.$%d\" % i\n            else:\n                index = \"1\"\n            expr = self.get_column(\n                column_name,\n                self.column_types[i],\n                index,\n                verbose=verbose\n            )\n            i += 1\n            exprs.append(expr)\n\n        i = 0\n        for column_name in self.grouping_column_name:\n            if len(self.grouping_column_name) > 1:\n                index = \"0.$%d\" % i\n            else:\n                index = \"0\"\n            expr = self.get_column(\n                column_name,\n                self.grouping_column_types[i],\n                index,\n                verbose=verbose\n            )\n            exprs.append(expr)\n            i += 1\n\n        result = utils.group(exprs).evaluate(verbose=verbose, passes=passes)\n        df = pd.DataFrame(columns=[])\n        all_columns = self.column_names + self.grouping_column_name\n        for i, column_name in enumerate(all_columns):\n            df[column_name] = result[i]\n        return dataframeweld.DataFrameWeld(df)", "response": "Evaluates the table and returns a DataFrame with the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, conf, arg, err):\n        weld_context_new = weld.weld_context_new\n        weld_context_new.argtypes = [c_weld_conf]\n        weld_context_new.restype = c_weld_context\n        ctx = weld_context_new(conf.conf)\n\n        weld_module_run = weld.weld_module_run\n        # module, context, arg, &err\n        weld_module_run.argtypes = [\n            c_weld_module, c_weld_context, c_weld_value, c_weld_err]\n        weld_module_run.restype = c_weld_value\n        ret = weld_module_run(self.module, ctx, arg.val, err.error)\n        return WeldValue(ret, assign=True, _ctx=ctx)", "response": "Run the module with the given conf and arg."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the aggregate of elements in the array.", "response": "def aggr(array, op, initial_value, ty):\n    \"\"\"\n    Computes the aggregate of elements in the array.\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array to aggregate\n        op (str): Op string used to aggregate the array (+ / *)\n        initial_value (int): Initial value for aggregation\n        ty (WeldType): Type of each element in the input array\n\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    weld_template = \"\"\"\n      result(\n        for(\n          %(array)s,\n          merger[%(ty)s,%(op)s],\n          |b, i, e| merge(b, e)\n        )\n      )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\n        \"array\": array_var, \"ty\": ty, \"op\": op}\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncomputes the dot product between two sets of tables.", "response": "def dot(matrix, vector, matrix_ty, vector_ty):\n    \"\"\"\n    Computes the dot product between a matrix and a vector.\n\n    Args:\n        matrix (WeldObject / Numpy.ndarray): 2-d input matrix\n        vector (WeldObject / Numpy.ndarray): 1-d input vector\n        ty (WeldType): Type of each element in the input matrix and vector\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    matrix_var = weld_obj.update(matrix)\n    if isinstance(matrix, WeldObject):\n        matrix_var = matrix.obj_id\n        weld_obj.dependencies[matrix_var] = matrix\n\n    vector_var = weld_obj.update(vector)\n    loopsize_annotation = \"\"\n    if isinstance(vector, WeldObject):\n        vector_var = vector.obj_id\n        weld_obj.dependencies[vector_var] = vector\n    if isinstance(vector, np.ndarray):\n        loopsize_annotation = \"@(loopsize: %dL)\" % len(vector)\n\n    weld_template = \"\"\"\n       map(\n         %(matrix)s,\n         |row: vec[%(matrix_ty)s]|\n           result(\n             %(loopsize_annotation)s\n             for(\n               result(\n                 %(loopsize_annotation)s\n                 for(\n                   zip(row, %(vector)s),\n                   appender,\n                   |b2, i2, e2: {%(matrix_ty)s, %(vector_ty)s}|\n                     merge(b2, f64(e2.$0 * %(matrix_ty)s(e2.$1)))\n                 )\n               ),\n               merger[f64,+],\n               |b, i, e| merge(b, e)\n             )\n           )\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"matrix\": matrix_var,\n                                          \"vector\": vector_var,\n                                          \"matrix_ty\": matrix_ty,\n                                          \"vector_ty\": vector_ty,\n                                          \"loopsize_annotation\": loopsize_annotation}\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exp(array, ty):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    weld_template = \"\"\"\n       map(\n         %(array)s,\n         |ele: %(ty)s| exp(ele)\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"array\": array_var, \"ty\": ty}\n    return weld_obj", "response": "Computes the per - element exponenet of the passed - in array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_view_child(view, par):\n    '''\n    Checks the base address of the given arrays to figure out if child and par\n    have overlapping memory regions.\n    '''\n    if par.base is None:\n        # par is the base array.\n        return view.base is par\n    else:\n        # par is a view of another array as well!\n        # view can only be a child of par if they share base.\n        return view.base is par.base", "response": "Checks if the given array is a child of the given array."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dictionary of the Weld supported binary ops.", "response": "def get_supported_binary_ops():\n    '''\n    Returns a dictionary of the Weld supported binary ops, with values being their Weld symbol.\n    '''\n    binary_ops = {}\n    binary_ops[np.add.__name__] = '+'\n    binary_ops[np.subtract.__name__] = '-'\n    binary_ops[np.multiply.__name__] = '*'\n    binary_ops[np.divide.__name__] = '/'\n    return binary_ops"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_supported_unary_ops():\n    '''\n    Returns a dictionary of the Weld supported unary ops, with values being their Weld symbol.\n    '''\n    unary_ops = {}\n    unary_ops[np.exp.__name__] = 'exp'\n    unary_ops[np.log.__name__] = 'log'\n    unary_ops[np.sqrt.__name__] = 'sqrt'\n    return unary_ops", "response": "Returns a dictionary of the Weld supported unary ops."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the class of the element with the given type.", "response": "def ctype_class(self):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        def vec_factory(elemType):\n            \"\"\"Summary\n\n            Args:\n                elemType (TYPE): Description\n\n            Returns:\n                TYPE: Description\n            \"\"\"\n            class Vec(Structure):\n                \"\"\"Summary\n                \"\"\"\n                _fields_ = [\n                    (\"ptr\", POINTER(elemType.ctype_class)),\n                    (\"size\", c_long),\n                ]\n            return Vec\n\n        if self.elemType not in WeldVec._singletons:\n            WeldVec._singletons[self.elemType] = vec_factory(self.elemType)\n        return WeldVec._singletons[self.elemType]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the class of the object that represents the ctype of the object.", "response": "def ctype_class(self):\n        \"\"\"Summary\n\n        Returns:\n            TYPE: Description\n        \"\"\"\n        def struct_factory(field_types):\n            \"\"\"Summary\n\n            Args:\n                field_types (TYPE): Description\n\n            Returns:\n                TYPE: Description\n            \"\"\"\n            class Struct(Structure):\n                \"\"\"Summary\n                \"\"\"\n                _fields_ = [(str(i), t.ctype_class)\n                            for i, t in enumerate(field_types)]\n            return Struct\n\n        if frozenset(self.field_types) not in WeldVec._singletons:\n            WeldStruct._singletons[\n                frozenset(self.field_types)] = struct_factory(self.field_types)\n        return WeldStruct._singletons[frozenset(self.field_types)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_field(expr, field):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    struct_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        struct_var = expr.obj_id\n        weld_obj.dependencies[struct_var] = expr\n\n    weld_template = \"\"\"\n      %(struct)s.$%(field)s\n    \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"struct\":struct_var,\n                                          \"field\":field}\n    return weld_obj", "response": "Fetch a field from a struct expr."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new array with each element in the original array satisfying the predicate set to new_value.", "response": "def mask(array, predicates, new_value, ty):\n    \"\"\"\n    Returns a new array, with each element in the original array satisfying the\n    passed-in predicate set to `new_value`\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        predicates (WeldObject / Numpy.ndarray<bool>): Predicate set\n        new_value (WeldObject / Numpy.ndarray / str): mask value\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    predicates_var = weld_obj.update(predicates)\n    if isinstance(predicates, WeldObject):\n        predicates_var = predicates.obj_id\n        weld_obj.dependencies[predicates_var] = predicates\n\n    if str(ty).startswith(\"vec\"):\n        new_value_var = weld_obj.update(new_value)\n        if isinstance(new_value, WeldObject):\n            new_value_var = new_value.obj_id\n            weld_obj.dependencies[new_value_var] = new_value\n    else:\n        new_value_var = \"%s(%s)\" % (ty, str(new_value))\n\n    weld_template = \"\"\"\n       map(\n         zip(%(array)s, %(predicates)s),\n         |p: {%(ty)s, bool}| if (p.$1, %(new_value)s, p.$0)\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\n        \"array\": array_var,\n        \"predicates\": predicates_var,\n        \"new_value\": new_value_var,\n        \"ty\": ty}\n\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a new array with each element in the original array satisfying the passed - in predicate set to new_value.", "response": "def filter(array, predicates, ty=None):\n    \"\"\"\n    Returns a new array, with each element in the original array satisfying the\n    passed-in predicate set to `new_value`\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        predicates (WeldObject / Numpy.ndarray<bool>): Predicate set\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    predicates_var = weld_obj.update(predicates)\n    if isinstance(predicates, WeldObject):\n        predicates_var = predicates.obj_id\n        weld_obj.dependencies[predicates_var] = predicates\n\n    weld_template = \"\"\"\n       result(\n         for(\n           zip(%(array)s, %(predicates)s),\n           appender,\n           |b, i, e| if (e.$1, merge(b, e.$0), b)\n         )\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\n        \"array\": array_var,\n        \"predicates\": predicates_var}\n\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pivot_filter(pivot_array, predicates, ty=None):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    pivot_array_var = weld_obj.update(pivot_array)\n    if isinstance(pivot_array, WeldObject):\n        pivot_array_var = pivot_array.obj_id\n        weld_obj.dependencies[pivot_array_var] = pivot_array\n\n    predicates_var = weld_obj.update(predicates)\n    if isinstance(predicates, WeldObject):\n        predicates_var = predicates.obj_id\n        weld_obj.dependencies[predicates_var] = predicates\n\n    weld_template = \"\"\"\n    let index_filtered =\n      result(\n        for(\n          zip(%(array)s.$0, %(predicates)s),\n          appender,\n          |b, i, e| if (e.$1, merge(b, e.$0), b)\n        )\n      );\n    let pivot_filtered =\n      map(\n        %(array)s.$1,\n        |x|\n          result(\n            for(\n              zip(x, %(predicates)s),\n              appender,\n              |b, i, e| if (e.$1, merge(b, e.$0), b)\n            )\n          )\n      );\n    {index_filtered, pivot_filtered, %(array)s.$2}\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\n        \"array\": pivot_array_var,\n        \"predicates\": predicates_var}\n\n    return weld_obj", "response": "Returns a new array with each element in the original array satisfying the predicate set to new_value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef element_wise_op(array, other, op, ty):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    other_var = weld_obj.update(other)\n    if isinstance(other, WeldObject):\n        other_var = other.obj_id\n        weld_obj.dependencies[other_var] = other\n\n    weld_template = \"\"\"\n       map(\n         zip(%(array)s, %(other)s),\n         |a| a.$0 %(op)s a.$1\n       )\n    \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"array\": array_var,\n                                          \"other\": other_var,\n                                          \"ty\": ty, \"op\": op}\n    return weld_obj", "response": "Compute the element - wise operation of series and other."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unzip_columns(expr, column_types):\n    weld_obj = WeldObject(encoder_, decoder_)\n    column_appenders = []\n    struct_fields = []\n    result_fields = []\n    for i, column_type in enumerate(column_types):\n        column_appenders.append(\"appender[%s]\" % column_type)\n        struct_fields.append(\"merge(b.$%s, e.$%s)\" % (i, i))\n        result_fields.append(\"result(unzip_builder.$%s)\" % i)\n    appender_string = \"{%s}\" % \", \".join(column_appenders)\n    struct_string = \"{%s}\" % \", \".join(struct_fields)\n    result_string = \"{%s}\" % \", \".join(result_fields)\n    expr_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        expr_var = expr.obj_id\n        weld_obj.dependencies[expr_var] = expr\n\n    weld_template = \"\"\"\n    let unzip_builder = for(\n      %(expr)s,\n      %(appenders)s,\n      |b,i,e| %(struct_builder)s\n    );\n    %(result)s\n    \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"expr\": expr_var,\n                                          \"appenders\": appender_string,\n                                          \"struct_builder\": struct_string,\n                                          \"result\": result_string}\n    return weld_obj", "response": "Zip together multiple columns."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsorts the vector. If the field parameter is provided then the sort operators on a vector of structs where the sort key is the field of the struct. Args: expr (WeldObject) field (Int)", "response": "def sort(expr, field = None, keytype=None, ascending=True):\n    \"\"\"\n    Sorts the vector.\n    If the field parameter is provided then the sort\n    operators on a vector of structs where the sort key\n    is the field of the struct.\n\n    Args:\n      expr (WeldObject)\n      field (Int)\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    expr_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        expr_var = expr.obj_id\n        weld_obj.dependencies[expr_var] = expr\n\n    if field is not None:\n        key_str = \"x.$%s\" % field\n    else:\n        key_str = \"x\"\n\n    if not ascending:\n        # The type is not necessarily f64.\n        key_str = key_str + \"* %s(-1)\" % keytype\n\n    weld_template = \"\"\"\n    sort(%(expr)s, |x| %(key)s)\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"expr\":expr_var, \"key\":key_str}\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slice_vec(expr, start, stop):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    expr_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        expr_var = expr.obj_id\n        weld_obj.dependencies[expr_var] = expr\n\n    weld_template = \"\"\"\n    slice(%(expr)s, %(start)sL, %(stop)sL)\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"expr\":expr_var,\n                                          \"start\":start,\n                                          \"stop\":stop}\n    return weld_obj", "response": "Returns a new vector where the elements are in the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nzipping together multiple columns.", "response": "def zip_columns(columns):\n    \"\"\"\n    Zip together multiple columns.\n\n    Args:\n        columns (WeldObject / Numpy.ndarray): lust of columns\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n    column_vars = []\n    for column in columns:\n        col_var = weld_obj.update(column)\n        if isinstance(column, WeldObject):\n            col_var = column.obj_id\n            weld_obj.dependencies[col_var] = column\n        column_vars.append(col_var)\n\n    arrays = \", \".join(column_vars)\n\n    weld_template = \"\"\"\n       result(\n         for(\n           zip(%(array)s),\n           appender,\n           |b, i, e| merge(b, e)\n         )\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\n        \"array\": arrays,\n    }\n\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform a comparison between every element in the passed - in array and other and returns an array of booleans.", "response": "def compare(array, other, op, ty_str):\n    \"\"\"\n    Performs passed-in comparison op between every element in the passed-in\n    array and other, and returns an array of booleans.\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        other (WeldObject / Numpy.ndarray): Second input array\n        op (str): Op string used for element-wise comparison (== >= <= !=)\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    # Strings need to be encoded into vec[char] array.\n    # Constants can be added directly to NVL snippet.\n    if isinstance(other, str) or isinstance(other, WeldObject):\n        other_var = weld_obj.update(other)\n        if isinstance(other, WeldObject):\n            other_var = tmp.obj_id\n            weld_obj.dependencies[other_var] = other\n    else:\n        other_var = \"%s(%s)\" % (ty_str, str(other))\n\n    weld_template = \"\"\"\n       map(\n         %(array)s,\n         |a: %(ty)s| a %(op)s %(other)s\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"array\": array_var,\n                                          \"other\": other_var,\n                                          \"op\": op, \"ty\": ty_str}\n\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new array - of - arrays with each array truncated starting at index start for length characters.", "response": "def slice(array, start, size, ty):\n    \"\"\"\n    Returns a new array-of-arrays with each array truncated, starting at\n    index `start` for `length` characters.\n\n    Args:\n        array (WeldObject / Numpy.ndarray): Input array\n        start (int): starting index\n        size (int): length to truncate at\n        ty (WeldType): Type of each element in the input array\n\n    Returns:\n        A WeldObject representing this computation\n    \"\"\"\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    weld_template = \"\"\"\n       map(\n         %(array)s,\n         |array: %(ty)s| slice(array, %(start)dL, %(size)dL)\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"array\": array_var, \"start\": start,\n                                          \"ty\": ty, \"size\": size}\n\n    return weld_obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef contains(array, ty, string):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    string_obj = weld_obj.update(string)\n    if isinstance(string, WeldObject):\n        string_obj =  string.obj_id\n        weld_obj.dependencies[string_obj] = string\n\n    array_var = weld_obj.update(array)\n    if isinstance(array, WeldObject):\n        array_var = array.obj_id\n        weld_obj.dependencies[array_var] = array\n\n    (start, end) = 0, len(string)\n    # Some odd bug where iterating on str and slicing str results\n    # in a segfault\n    weld_template = \"\"\"\n       map(\n         %(array)s,\n         |str: vec[%(ty)s]|\n           let tstr = str;\n           result(\n             for(\n               str,\n               merger[i8,+](i8(0)),\n               |b,i,e|\n                 if(slice(tstr, i, i64(%(end)s)) == %(cmpstr)s,\n                    merge(b, i8(1)),\n                    b\n                 )\n              )\n            ) > i8(0)\n       )\n    \"\"\"\n    weld_obj.weld_code = weld_template % {\"array\": array_var, \"ty\": ty,\n                                          \"start\": start, \"end\": end,\n                                          \"cmpstr\": string_obj}\n\n    return weld_obj", "response": "Checks if given string is contained in each element in the array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef join(expr1, expr2, d1_keys, d2_keys, keys_type, d1_vals, df1_vals_ty, d2_vals, df2_vals_ty):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    df1_var = weld_obj.update(expr1)\n    if isinstance(expr1, WeldObject):\n        df1_var = expr1.obj_id\n        weld_obj.dependencies[df1_var] = expr1\n\n    df2_var = weld_obj.update(expr2)\n    if isinstance(expr2, WeldObject):\n        df2_var = expr2.obj_id\n        weld_obj.dependencies[df2_var] = expr2\n\n    d1_key_fields = \", \".join([\"e.$%s\" % k for k in d1_keys])\n    if len(d1_keys) > 1:\n        d1_key_struct = \"{%s}\" % (d1_key_fields)\n    else:\n        d1_key_struct = d1_key_fields\n\n    d1_val_fields = \", \".join([\"e.$%s\" % k for k in d1_vals])\n\n    d2_key_fields = \", \".join([\"e.$%s\" % k for k in d2_keys])\n    if len(d2_keys) > 1:\n        d2_key_struct = \"{%s}\" % (d2_key_fields)\n    else:\n        d2_key_struct = d2_key_fields\n\n    d2_val_fields = \", \".join([\"e.$%s\" % k for k in d2_vals])\n    d2_val_fields2 = \", \".join([\"e2.$%s\" % i for i, k in enumerate(d2_vals)])\n    d2_val_struct = \"{%s}\" % (d2_val_fields)\n\n    weld_template = \"\"\"\n    let df2_join_table = result(\n      for(\n        %(df2)s,\n        groupmerger[%(kty)s, %(df2ty)s],\n        |b, i, e| merge(b, {%(df2key)s, %(df2vals)s})\n      )\n    );\n\n    result(for(\n      %(df1)s,\n      appender,\n      |b, i, e|\n        for(\n          lookup(df2_join_table, %(df1key)s),\n          b,\n          |b2, i2, e2| merge(b, {%(df1key)s, %(df1vals)s, %(df2vals2)s})\n        )\n    ))\n    \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"df1\":df1_var,\n                                          \"df1key\":d1_key_struct,\n                                          \"df1vals\": d1_val_fields,\n                                          \"df2\":df2_var,\n                                          \"kty\":keys_type,\n                                          \"df2ty\":df2_vals_ty,\n                                          \"df2key\":d2_key_struct,\n                                          \"df2vals\":d2_val_struct,\n                                          \"df2vals2\":d2_val_fields2}\n    \n    return weld_obj", "response": "Computes a join on two tables"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pivot_table(expr, value_index, value_ty, index_index, index_ty, columns_index, columns_ty, aggfunc):\n\n    weld_obj = WeldObject(encoder_, decoder_)\n    zip_var = weld_obj.update(expr)\n    if isinstance(expr, WeldObject):\n        zip_var = expr.obj_id\n        weld_obj.dependencies[zip_var] = expr\n\n    if aggfunc == 'sum':\n        weld_template = \"\"\"\n          let bs = for(\n            %(zip_expr)s,\n            {dictmerger[{%(ity)s,%(cty)s},%(vty)s,+], dictmerger[%(ity)s,i64,+], dictmerger[%(cty)s,i64,+]},\n            |b, i, e| {merge(b.$0, {{e.$%(id)s, e.$%(cd)s}, e.$%(vd)s}),\n                       merge(b.$1, {e.$%(id)s, 1L}),\n                       merge(b.$2, {e.$%(cd)s, 1L})}\n          );\n          let agg_dict = result(bs.$0);\n          let ind_vec = sort(map(tovec(result(bs.$1)), |x| x.$0), |x, y| compare(x,y));\n          let col_vec = map(tovec(result(bs.$2)), |x| x.$0);\n          let pivot = map(\n            col_vec,\n            |x:%(cty)s|\n            map(ind_vec, |y:%(ity)s| f64(lookup(agg_dict, {y, x})))\n        );\n        {ind_vec, pivot, col_vec}\n    \"\"\"\n    elif aggfunc == 'mean':\n        weld_template = \"\"\"\n          let bs = for(\n            %(zip_expr)s,\n            {dictmerger[{%(ity)s,%(cty)s},{%(vty)s, i64},+], dictmerger[%(ity)s,i64,+], dictmerger[%(cty)s,i64,+]},\n            |b, i, e| {merge(b.$0, {{e.$%(id)s, e.$%(cd)s}, {e.$%(vd)s, 1L}}),\n                       merge(b.$1, {e.$%(id)s, 1L}),\n                       merge(b.$2, {e.$%(cd)s, 1L})}\n          );\n          let agg_dict = result(bs.$0);\n          let ind_vec = sort(map(tovec(result(bs.$1)), |x| x.$0), |x, y| compare(x,y));\n          let col_vec = map(tovec(result(bs.$2)), |x| x.$0);\n          let pivot = map(\n            col_vec,\n            |x:%(cty)s|\n            map(ind_vec, |y:%(ity)s| (\n              let sum_len_pair = lookup(agg_dict, {y, x});\n              f64(sum_len_pair.$0) / f64(sum_len_pair.$1))\n            )\n        );\n        {ind_vec, pivot, col_vec}\n    \"\"\"\n\n    else:\n        raise Exception(\"Aggregate operation %s not supported.\" % aggfunc)\n\n    weld_obj.weld_code = weld_template % {\"zip_expr\": zip_var,\n                                          \"ity\": index_ty,\n                                          \"cty\": columns_ty,\n                                          \"vty\": value_ty,\n                                          \"id\": index_index,\n                                          \"cd\": columns_index,\n                                          \"vd\": value_index}\n    return weld_obj", "response": "Construct a pivot table where the index_index and columns_index are used as keys and the value_ty is used as the value which is aggregated."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef groupby_size(columns, column_tys, grouping_columns, grouping_column_tys):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    if len(grouping_columns) == 1 and len(grouping_column_tys) == 1:\n        grouping_column_var = weld_obj.update(grouping_columns[0])\n        if isinstance(grouping_columns[0], WeldObject):\n            grouping_column_var = grouping_columns[0].weld_code\n        grouping_column_ty_str = \"%s\" % grouping_column_tys[0]\n    else:\n        grouping_column_vars = []\n        for column in grouping_columns:\n            column_var = weld_obj.update(column)\n            if isinstance(column_var, WeldObject):\n                column_var = column_var.weld_code\n            grouping_column_vars.append(column_var)\n        grouping_column_var = \", \".join(grouping_column_vars)\n        grouping_column_var = \"zip(%s)\" % grouping_column_var\n        grouping_column_tys = [str(ty) for ty in grouping_column_tys]\n        grouping_column_ty_str = \", \".join(grouping_column_tys)\n        grouping_column_ty_str = \"{%s}\" % grouping_column_ty_str\n\n    weld_template = \"\"\"\n    let group = sort(\n      tovec(\n        result(\n          for(\n            %(grouping_column)s,\n            dictmerger[%(gty)s, i64, +],\n            |b, i, e| merge(b, {e, 1L})\n          )\n        )\n      ),\n      |x:{%(gty)s, i64}, y:{%(gty)s, i64}| compare(x.$0, y.$0)\n    );\n    {\n      map(\n      group,\n        |x:{%(gty)s,i64}| x.$0\n      ),\n      map(\n        group,\n        |x:{%(gty)s,i64}| x.$1\n      )\n    }\n  \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"grouping_column\": grouping_column_var,\n                                          \"gty\": grouping_column_ty_str}\n    return weld_obj", "response": "Groups the given columns by their corresponding grouping column\nAddon value and aggregate by summing values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef groupby_sort(columns, column_tys, grouping_columns, grouping_column_tys, key_index, ascending):\n    weld_obj = WeldObject(encoder_, decoder_)\n    if len(grouping_columns) == 1 and len(grouping_column_tys) == 1:\n        grouping_column_var = weld_obj.update(grouping_columns[0])\n        if isinstance(grouping_columns[0], WeldObject):\n            grouping_column_var = grouping_columns[0].weld_code\n        grouping_column_ty_str = \"%s\" % grouping_column_tys[0]\n    else:\n        grouping_column_vars = []\n        for column in grouping_columns:\n            column_var = weld_obj.update(column)\n            if isinstance(column, WeldObject):\n                column_var = column.obj_id\n                weld_obj.dependencies[column_var] = column\n            grouping_column_vars.append(column_var)\n        grouping_column_var = \", \".join(grouping_column_vars)\n        grouping_column_tys = [str(ty) for ty in grouping_column_tys]\n        grouping_column_ty_str = \", \".join(grouping_column_tys)\n        grouping_column_ty_str = \"{%s}\" % grouping_column_ty_str\n\n    columns_var_list = []\n    for column in columns:\n        column_var = weld_obj.update(column)\n        if isinstance(column, WeldObject):\n            column_var = column.obj_id\n            weld_obj.dependencies[column_var] = column\n        columns_var_list.append(column_var)\n\n    if len(columns_var_list) == 1 and len(grouping_columns) == 1:\n        columns_var = columns_var_list[0]\n        tys_str = column_tys[0]\n        result_str = \"merge(b, e)\"\n    elif len(columns_var_list) == 1 and len(grouping_columns) > 1:\n        columns_var = columns_var_list[0]\n        tys_str = column_tys[0]\n        key_str_list = []\n        for i in xrange(0, len(grouping_columns)):\n            key_str_list.append(\"e.$%d\" % i)\n        key_str = \"{%s}\" % \", \".join(key_str_list)\n        value_str = \"e.$\" + str(len(grouping_columns))\n        result_str_list = [key_str, value_str]\n        result_str = \"merge(b, {%s})\" % \", \".join(result_str_list)\n    elif len(columns_var_list) > 1 and len(grouping_columns) == 1:\n        columns_var = \"%s\" % \", \".join(columns_var_list)\n        column_tys = [str(ty) for ty in column_tys]\n        tys_str = \"{%s}\" % \", \".join(column_tys)\n        key_str = \"e.$0\"\n        value_str_list = []\n        for i in xrange(1, len(columns) + 1):\n            value_str_list.append(\"e.$%d\" % i)\n        value_str = \"{%s}\" % \", \".join(value_str_list)\n        result_str_list = [key_str, value_str]\n        result_str = \"merge(b, {%s})\" % \", \".join(result_str_list)\n    else:\n        columns_var = \"%s\" % \", \".join(columns_var_list)\n        column_tys = [str(ty) for ty in column_tys]\n        tys_str = \"{%s}\" % \", \".join(column_tys)\n        key_str_list = []\n        key_size = len(grouping_columns)\n        value_size = len(columns)\n        for i in xrange(0, key_size):\n            key_str_list.append(\"e.$%d\" % i)\n        key_str = \"{%s}\" % \", \".join(key_str_list)\n        value_str_list = []\n        for i in xrange(key_size, key_size + value_size):\n            value_str_list.append(\"e.$%d\" % i)\n        value_str = \"{%s}\" % \", \".join(value_str_list)\n        result_str_list = [key_str, value_str]\n        result_str = \"merge(b, {%s})\" % \", \".join(result_str_list)\n\n    if key_index == None:\n        key_str_x = \"x\"\n        key_str_y = \"y\"\n    else :\n        key_str_x = \"x.$%d\" % key_index\n        key_str_y = \"y.$%d\" % key_index\n\n    if ascending == False:\n        key_str = key_str + \"* %s(-1)\" % column_tys[key_index]\n\n    weld_template = \"\"\"\n    @(grain_size:1)map(\n     tovec(\n       result(\n         for(\n           zip(%(grouping_column)s, %(columns)s),\n           groupmerger[%(gty)s, %(ty)s],\n           |b, i, e| %(result)s\n         )\n       )\n     ),\n    |a:{%(gty)s, vec[%(ty)s]}| {a.$0, sort(a.$1, |x:%(ty)s, y:%(ty)s| compare(%(key_str_x)s, %(key_str_y)s))}\n    )\n  \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"grouping_column\": grouping_column_var,\n                                          \"columns\": columns_var,\n                                          \"result\": result_str,\n                                          \"ty\": tys_str,\n                                          \"gty\": grouping_column_ty_str,\n                                          \"key_str_x\": key_str_x,\n                                          \"key_str_y\": key_str_y}\n    return weld_obj", "response": "Groups the given columns by the corresponding grouping column\n            value and aggregate by summing values."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef flatten_group(expr, column_tys, grouping_column_tys):\n    weld_obj = WeldObject(encoder_, decoder_)\n\n    group_var = weld_obj.update(expr)\n\n    num_group_cols = len(grouping_column_tys)\n    if num_group_cols == 1:\n        grouping_column_ty_str = \"%s\" % grouping_column_tys[0]\n        grouping_column_key_str = \"a.$0\"\n    else:\n        grouping_column_tys = [str(ty) for ty in grouping_column_tys]\n        grouping_column_ty_str = \", \".join(grouping_column_tys)\n        grouping_column_ty_str = \"{%s}\" % grouping_column_ty_str\n        grouping_column_keys = [\"a.$0.$%s\" % i for i in range(0, num_group_cols)]\n        grouping_column_key_str = \"{%s}\" % \", \".join(grouping_column_keys)\n\n    if len(column_tys) == 1:\n        tys_str = \"%s\" % column_tys[0]\n        column_values_str = \"b.$1\"\n    else:\n        column_tys = [str(ty) for ty in column_tys]\n\n        tys_str = \"{%s}\" % \", \".join(column_tys)\n        column_values = [\"b.$%s\" % i for i in range(0, len(column_tys))]\n        column_values_str = \"{%s}\" % \", \".join(column_values)\n\n    # TODO: The output in pandas keps the keys sorted (even if keys are structs)\n    # We need to allow keyfunctions in sort by clauses to be able to compare structs.\n    # sort(%(group_vec)s, |x:{%(gty)s, vec[%(ty)s]}| x.$0),\n    weld_template = \"\"\"\n    flatten(\n      map(\n        %(group_vec)s,\n        |a:{%(gty)s, vec[%(ty)s]}| map(a.$1, |b:%(ty)s| {%(gkeys)s, %(gvals)s})\n      )\n    )\n  \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"group_vec\": expr.weld_code,\n                                          \"gkeys\": grouping_column_key_str,\n                                          \"gvals\": column_values_str,\n                                          \"ty\": tys_str,\n                                          \"gty\": grouping_column_ty_str}\n    return weld_obj", "response": "Group the given columns by the corresponding grouping column\n    value and aggregate by summing values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef grouped_slice(expr, type, start, size):\n    weld_obj = WeldObject(encoder_, decoder_)\n    weld_obj.update(expr)\n    weld_template = \"\"\"\n       map(\n           %(vec)s,\n           |b: %(type)s| {b.$0, slice(b.$1, i64(%(start)s), i64(%(size)s))}\n         )\n    \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"vec\": expr.weld_code,\n                                          \"type\": type,\n                                          \"start\": str(start),\n                                          \"size\": str(size)}\n    return weld_obj", "response": "Groups the given columns by the corresponding grouping column\n    value and aggregate by summing values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_column(columns, column_tys, index):\n    weld_obj = WeldObject(encoder_, decoder_)\n    columns_var = weld_obj.update(columns, tys=WeldVec(column_tys), override=False)\n    if isinstance(columns, WeldObject):\n        columns_var = columns.obj_id\n        weld_obj.dependencies[columns_var] = columns\n\n    weld_template = \"\"\"\n     map(\n       %(columns)s,\n       |elem: %(ty)s| elem.$%(index)s\n     )\n  \"\"\"\n\n    weld_obj.weld_code = weld_template % {\"columns\": columns_var,\n                                          \"ty\": column_tys,\n                                          \"index\": index}\n    return weld_obj", "response": "Get the column corresponding to passed - in index from ptr returned\n    by groupBySum."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the dot product between a matrix and a vector.", "response": "def dot(matrix, vector):\n    \"\"\"\n    Computes the dot product between a matrix and a vector.\n    TODO: Make this more generic\n\n    Args:\n        matrix (TYPE): Description\n        vector (TYPE): Description\n    \"\"\"\n    matrix_weld_type = None\n    vector_weld_type = None\n\n    if isinstance(matrix, LazyOpResult):\n        matrix_weld_type = matrix.weld_type\n        matrix = matrix.expr\n    elif isinstance(matrix, np.ndarray):\n        matrix_weld_type = numpy_weld_impl.numpy_to_weld_type_mapping[\n            str(matrix.dtype)]\n\n    if isinstance(vector, LazyOpResult):\n        vector_weld_type = vector.weld_type\n        vector = vector.expr\n    elif isinstance(vector, np.ndarray):\n        vector_weld_type = numpy_weld_impl.numpy_to_weld_type_mapping[\n            str(vector.dtype)]\n\n    return NumpyArrayWeld(\n        numpy_weld_impl.dot(\n            matrix,\n            vector,\n            matrix_weld_type,\n            vector_weld_type),\n        WeldDouble())"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes a per - element exponent of the passed - in vector.", "response": "def exp(vector):\n    \"\"\"\n    Computes a per-element exponent of the passed-in vector.\n\n    Args:\n        vector (TYPE): Description\n    \"\"\"\n    weld_type = None\n    if isinstance(vector, LazyOpResult):\n        weld_type = vector.weld_type\n        vector = vector.expr\n    elif isinstance(vector, np.ndarray):\n        weld_type = numpy_weld_impl.numpy_to_weld_type_mapping[\n            str(vector.dtype)]\n    return NumpyArrayWeld(numpy_weld_impl.exp(vector, weld_type), WeldDouble())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sum(self):\n        return NumpyArrayWeld(\n            numpy_weld_impl.aggr(\n                self.expr,\n                \"+\",\n                0,\n                self.weld_type\n            ),\n            self.weld_type,\n            0\n        )", "response": "Summary\n            summation Returns NumpyArrayWeld"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef array(arr, *args, **kwargs):\n    '''\n    Wrapper around weldarray - first create np.array and then convert to\n    weldarray.\n    '''\n    return weldarray(np.array(arr, *args, **kwargs))", "response": "Wrapper around weldarray - first create np. array then convert to\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef py_to_weld_type(self, obj):\n        if isinstance(obj, np.ndarray):\n            dtype = str(obj.dtype)\n            if dtype == 'int16':\n                base = WeldInt16()\n            elif dtype == 'int32':\n                base = WeldInt()\n            elif dtype == 'int64':\n                base = WeldLong()\n            elif dtype == 'float32':\n                base = WeldFloat()\n            elif dtype == 'float64':\n                base = WeldDouble()\n            elif dtype == 'bool':\n                base = WeldBit()\n            else:\n                base = WeldVec(WeldChar())  # TODO: Fix this\n            for i in xrange(obj.ndim):\n                base = WeldVec(base)\n        elif isinstance(obj, str):\n            base = WeldVec(WeldChar())\n        else:\n            raise Exception(\"Invalid object type: unable to infer NVL type\")\n        return base", "response": "Convert a Python object to a Weld type."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a Python object to Weld object.", "response": "def encode(self, obj):\n        \"\"\"Converts Python object to Weld object.\n\n        Args:\n            obj: Python object that needs to be converted to Weld format\n\n        Returns:\n            Weld formatted object\n        \"\"\"\n        if isinstance(obj, np.ndarray):\n            if obj.ndim == 1 and obj.dtype == 'int16':\n                numpy_to_weld = self.utils.numpy_to_weld_int16_arr\n            elif obj.ndim == 1 and obj.dtype == 'int32':\n                numpy_to_weld = self.utils.numpy_to_weld_int_arr\n            elif obj.ndim == 1 and obj.dtype == 'int64':\n                numpy_to_weld = self.utils.numpy_to_weld_long_arr\n            elif obj.ndim == 1 and obj.dtype == 'float32':\n                numpy_to_weld = self.utils.numpy_to_weld_float_arr\n            elif obj.ndim == 1 and obj.dtype == 'float64':\n                numpy_to_weld = self.utils.numpy_to_weld_double_arr\n            elif obj.ndim == 2 and obj.dtype == 'int16':\n                numpy_to_weld = self.utils.numpy_to_weld_int16_arr_arr\n            elif obj.ndim == 2 and obj.dtype == 'int32':\n                numpy_to_weld = self.utils.numpy_to_weld_int_arr_arr\n            elif obj.ndim == 2 and obj.dtype == 'int64':\n                numpy_to_weld = self.utils.numpy_to_weld_long_arr_arr\n            elif obj.ndim == 2 and obj.dtype == 'float32':\n                numpy_to_weld = self.utils.numpy_to_weld_float_arr_arr\n            elif obj.ndim == 2 and obj.dtype == 'float64':\n                numpy_to_weld = self.utils.numpy_to_weld_double_arr_arr\n            elif obj.ndim == 2 and obj.dtype == 'bool':\n                numpy_to_weld = self.utils.numpy_to_weld_bool_arr_arr\n            elif obj.ndim == 1 and obj.dtype == 'bool':\n                numpy_to_weld = self.utils.numpy_to_weld_bool_arr\n            else:\n                numpy_to_weld = self.utils.numpy_to_weld_char_arr_arr\n        elif isinstance(obj, str):\n            numpy_to_weld = self.utils.numpy_to_weld_char_arr\n        else:\n            raise Exception(\"Unable to encode; invalid object type\")\n\n        numpy_to_weld.restype = self.py_to_weld_type(obj).ctype_class\n        numpy_to_weld.argtypes = [py_object]\n        weld_vec = numpy_to_weld(obj)\n        return weld_vec"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decode(self, obj, restype, raw_ptr=False):\n        if raw_ptr:\n            data = obj\n        else:\n            data = cweld.WeldValue(obj).data()\n        result = ctypes.cast(data, ctypes.POINTER(restype.ctype_class)).contents\n\n        if restype == WeldInt16():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_int16)).contents.value\n            return result\n        elif restype == WeldInt():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_int)).contents.value\n            return result\n        elif restype == WeldLong():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_long)).contents.value\n            return result\n        elif restype == WeldFloat():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_float)).contents.value\n            return np.float32(result)\n        elif restype == WeldDouble():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_double)).contents.value\n            return float(result)\n        elif restype == WeldBit():\n            data = cweld.WeldValue(obj).data()\n            result = ctypes.cast(data, ctypes.POINTER(c_bool)).contents.value\n            return bool(result)\n\n        # Obj is a WeldVec(WeldInt()).ctype_class, which is a subclass of\n        # ctypes._structure\n        if restype == WeldVec(WeldBit()):\n            weld_to_numpy = self.utils.weld_to_numpy_bool_arr\n        elif restype == WeldVec(WeldInt16()):\n            weld_to_numpy = self.utils.weld_to_numpy_int16_arr\n        elif restype == WeldVec(WeldInt()):\n            weld_to_numpy = self.utils.weld_to_numpy_int_arr\n        elif restype == WeldVec(WeldLong()):\n            weld_to_numpy = self.utils.weld_to_numpy_long_arr\n        elif restype == WeldVec(WeldFloat()):\n            weld_to_numpy = self.utils.weld_to_numpy_float_arr\n        elif restype == WeldVec(WeldDouble()):\n            weld_to_numpy = self.utils.weld_to_numpy_double_arr\n        elif restype == WeldVec(WeldVec(WeldChar())):\n            weld_to_numpy = self.utils.weld_to_numpy_char_arr_arr\n        elif restype == WeldVec(WeldVec(WeldInt16())):\n            weld_to_numpy = self.utils.weld_to_numpy_int16_arr_arr\n        elif restype == WeldVec(WeldVec(WeldInt())):\n            weld_to_numpy = self.utils.weld_to_numpy_int_arr_arr\n        elif restype == WeldVec(WeldVec(WeldLong())):\n            weld_to_numpy = self.utils.weld_to_numpy_long_arr_arr\n        elif restype == WeldVec(WeldVec(WeldFloat())):\n            weld_to_numpy = self.utils.weld_to_numpy_float_arr_arr\n        elif restype == WeldVec(WeldVec(WeldDouble())):\n            weld_to_numpy = self.utils.weld_to_numpy_double_arr_arr\n        elif restype == WeldVec(WeldVec(WeldBit())):\n            weld_to_numpy = self.utils.weld_to_numpy_bool_arr_arr\n        elif isinstance(restype, WeldStruct):\n            ret_vecs = []\n            # Iterate through all fields in the struct, and recursively call\n            # decode.\n            for field_type in restype.field_types:\n                ret_vec = self.decode(data, field_type, raw_ptr=True)\n                data += sizeof(field_type.ctype_class())\n                ret_vecs.append(ret_vec)\n            return tuple(ret_vecs)\n        else:\n            raise Exception(\"Unable to decode; invalid return type\")\n\n        weld_to_numpy.restype = py_object\n        weld_to_numpy.argtypes = [restype.ctype_class]\n\n        ret_vec = weld_to_numpy(result)\n        return ret_vec", "response": "Converts a Weld object to a Python object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving temp files by filename wildcard path.", "response": "def cleanup(temp_name):\n    ''' Tries to remove temp files by filename wildcard path. '''\n    for filename in iglob(temp_name + '*' if temp_name else temp_name):\n        try:\n            os.remove(filename)\n        except OSError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_tesseract_version():\n    '''\n    Returns LooseVersion object of the Tesseract version\n    '''\n    try:\n        return LooseVersion(\n            subprocess.check_output(\n                [tesseract_cmd, '--version'], stderr=subprocess.STDOUT\n            ).decode('utf-8').split()[1].lstrip(string.printable[10:])\n        )\n    except OSError:\n        raise TesseractNotFoundError()", "response": "Returns LooseVersion object of the Tesseract version"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef image_to_string(image,\n                    lang=None,\n                    config='',\n                    nice=0,\n                    output_type=Output.STRING):\n    '''\n    Returns the result of a Tesseract OCR run on the provided image to string\n    '''\n    args = [image, 'txt', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: {'text': run_and_get_output(*args)},\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()", "response": "Returns the result of a Tesseract OCR run on the provided image to string\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the result of a Tesseract OCR run on the provided image to pdf or hocr.", "response": "def image_to_pdf_or_hocr(image,\n                    lang=None,\n                    config='',\n                    nice=0,\n                    extension='pdf'):\n    '''\n    Returns the result of a Tesseract OCR run on the provided image to pdf/hocr\n    '''\n\n    if extension not in {'pdf', 'hocr'}:\n        raise ValueError('Unsupported extension: {}'.format(extension))\n    args = [image, extension, lang, config, nice, True]\n\n    return run_and_get_output(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef image_to_data(image,\n                  lang=None,\n                  config='',\n                  nice=0,\n                  output_type=Output.STRING):\n    '''\n    Returns string containing box boundaries, confidences,\n    and other information. Requires Tesseract 3.05+\n    '''\n\n    if get_tesseract_version() < '3.05':\n        raise TSVNotSupported()\n\n    config = '{} {}'.format('-c tessedit_create_tsv=1', config.strip()).strip()\n    args = [image, 'tsv', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DATAFRAME: lambda: get_pandas_output(args + [True]),\n        Output.DICT: lambda: file_to_dict(run_and_get_output(*args), '\\t', -1),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()", "response": "Converts an image to a string containing box boundaries confidences and other information. Requires Tesseract 3. 05 +\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an image to OSD.", "response": "def image_to_osd(image,\n                 lang='osd',\n                 config='',\n                 nice=0,\n                 output_type=Output.STRING):\n    '''\n    Returns string containing the orientation and script detection (OSD)\n    '''\n    config = '{}-psm 0 {}'.format(\n        '' if get_tesseract_version() < '3.05' else '-',\n        config.strip()\n    ).strip()\n    args = [image, 'osd', lang, config, nice]\n\n    return {\n        Output.BYTES: lambda: run_and_get_output(*(args + [True])),\n        Output.DICT: lambda: osd_to_dict(run_and_get_output(*args)),\n        Output.STRING: lambda: run_and_get_output(*args),\n    }[output_type]()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_uri(uri: str) -> WebSocketURI:\n    parsed = urllib.parse.urlparse(uri)\n    try:\n        assert parsed.scheme in [\"ws\", \"wss\"]\n        assert parsed.params == \"\"\n        assert parsed.fragment == \"\"\n        assert parsed.hostname is not None\n    except AssertionError as exc:\n        raise InvalidURI(uri) from exc\n\n    secure = parsed.scheme == \"wss\"\n    host = parsed.hostname\n    port = parsed.port or (443 if secure else 80)\n    resource_name = parsed.path or \"/\"\n    if parsed.query:\n        resource_name += \"?\" + parsed.query\n    user_info = None\n    if parsed.username or parsed.password:\n        user_info = (parsed.username, parsed.password)\n    return WebSocketURI(secure, host, port, resource_name, user_info)", "response": "Parses and validates a WebSocket URI."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprepares data for the next frame.", "response": "def prepare_data(data: Data) -> Tuple[int, bytes]:\n    \"\"\"\n    Convert a string or byte-like object to an opcode and a bytes-like object.\n\n    This function is designed for data frames.\n\n    If ``data`` is a :class:`str`, return ``OP_TEXT`` and a :class:`bytes`\n    object encoding ``data`` in UTF-8.\n\n    If ``data`` is a bytes-like object, return ``OP_BINARY`` and a bytes-like\n    object.\n\n    Raise :exc:`TypeError` for other inputs.\n\n    \"\"\"\n    if isinstance(data, str):\n        return OP_TEXT, data.encode(\"utf-8\")\n    elif isinstance(data, (bytes, bytearray)):\n        return OP_BINARY, data\n    elif isinstance(data, memoryview):\n        if data.c_contiguous:\n            return OP_BINARY, data\n        else:\n            return OP_BINARY, data.tobytes()\n    else:\n        raise TypeError(\"data must be bytes-like or str\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nencode a string or byte - like object to bytes.", "response": "def encode_data(data: Data) -> bytes:\n    \"\"\"\n    Convert a string or byte-like object to bytes.\n\n    This function is designed for ping and pon g frames.\n\n    If ``data`` is a :class:`str`, return a :class:`bytes` object encoding\n    ``data`` in UTF-8.\n\n    If ``data`` is a bytes-like object, return a :class:`bytes` object.\n\n    Raise :exc:`TypeError` for other inputs.\n\n    \"\"\"\n    if isinstance(data, str):\n        return data.encode(\"utf-8\")\n    elif isinstance(data, (bytes, bytearray)):\n        return bytes(data)\n    elif isinstance(data, memoryview):\n        return data.tobytes()\n    else:\n        raise TypeError(\"data must be bytes-like or str\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_close(data: bytes) -> Tuple[int, str]:\n    length = len(data)\n    if length >= 2:\n        code, = struct.unpack(\"!H\", data[:2])\n        check_close(code)\n        reason = data[2:].decode(\"utf-8\")\n        return code, reason\n    elif length == 0:\n        return 1005, \"\"\n    else:\n        assert length == 1\n        raise WebSocketProtocolError(\"Close frame too short\")", "response": "Parse the data in a close frame."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize_close(code: int, reason: str) -> bytes:\n    check_close(code)\n    return struct.pack(\"!H\", code) + reason.encode(\"utf-8\")", "response": "Serialize the data for a close frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def read(\n        cls,\n        reader: Callable[[int], Awaitable[bytes]],\n        *,\n        mask: bool,\n        max_size: Optional[int] = None,\n        extensions: Optional[Sequence[\"websockets.extensions.base.Extension\"]] = None,\n    ) -> \"Frame\":\n        \"\"\"\n        Read a WebSocket frame and return a :class:`Frame` object.\n\n        ``reader`` is a coroutine taking an integer argument and reading\n        exactly this number of bytes, unless the end of file is reached.\n\n        ``mask`` is a :class:`bool` telling whether the frame should be masked\n        i.e. whether the read happens on the server side.\n\n        If ``max_size`` is set and the payload exceeds this size in bytes,\n        :exc:`~websockets.exceptions.PayloadTooBig` is raised.\n\n        If ``extensions`` is provided, it's a list of classes with an\n        ``decode()`` method that transform the frame and return a new frame.\n        They are applied in reverse order.\n\n        This function validates the frame before returning it and raises\n        :exc:`~websockets.exceptions.WebSocketProtocolError` if it contains\n        incorrect values.\n\n        \"\"\"\n        # Read the header.\n        data = await reader(2)\n        head1, head2 = struct.unpack(\"!BB\", data)\n\n        # While not Pythonic, this is marginally faster than calling bool().\n        fin = True if head1 & 0b10000000 else False\n        rsv1 = True if head1 & 0b01000000 else False\n        rsv2 = True if head1 & 0b00100000 else False\n        rsv3 = True if head1 & 0b00010000 else False\n        opcode = head1 & 0b00001111\n\n        if (True if head2 & 0b10000000 else False) != mask:\n            raise WebSocketProtocolError(\"Incorrect masking\")\n\n        length = head2 & 0b01111111\n        if length == 126:\n            data = await reader(2)\n            length, = struct.unpack(\"!H\", data)\n        elif length == 127:\n            data = await reader(8)\n            length, = struct.unpack(\"!Q\", data)\n        if max_size is not None and length > max_size:\n            raise PayloadTooBig(\n                f\"Payload length exceeds size limit ({length} > {max_size} bytes)\"\n            )\n        if mask:\n            mask_bits = await reader(4)\n\n        # Read the data.\n        data = await reader(length)\n        if mask:\n            data = apply_mask(data, mask_bits)\n\n        frame = cls(fin, opcode, data, rsv1, rsv2, rsv3)\n\n        if extensions is None:\n            extensions = []\n        for extension in reversed(extensions):\n            frame = extension.decode(frame, max_size=max_size)\n\n        frame.check()\n\n        return frame", "response": "Reads a WebSocket frame and returns a new frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites a single WebSocket frame.", "response": "def write(\n        frame,\n        writer: Callable[[bytes], Any],\n        *,\n        mask: bool,\n        extensions: Optional[Sequence[\"websockets.extensions.base.Extension\"]] = None,\n    ) -> None:\n        \"\"\"\n        Write a WebSocket frame.\n\n        ``frame`` is the :class:`Frame` object to write.\n\n        ``writer`` is a function accepting bytes.\n\n        ``mask`` is a :class:`bool` telling whether the frame should be masked\n        i.e. whether the write happens on the client side.\n\n        If ``extensions`` is provided, it's a list of classes with an\n        ``encode()`` method that transform the frame and return a new frame.\n        They are applied in order.\n\n        This function validates the frame before sending it and raises\n        :exc:`~websockets.exceptions.WebSocketProtocolError` if it contains\n        incorrect values.\n\n        \"\"\"\n        # The first parameter is called `frame` rather than `self`,\n        # but it's the instance of class to which this method is bound.\n\n        frame.check()\n\n        if extensions is None:\n            extensions = []\n        for extension in extensions:\n            frame = extension.encode(frame)\n\n        output = io.BytesIO()\n\n        # Prepare the header.\n        head1 = (\n            (0b10000000 if frame.fin else 0)\n            | (0b01000000 if frame.rsv1 else 0)\n            | (0b00100000 if frame.rsv2 else 0)\n            | (0b00010000 if frame.rsv3 else 0)\n            | frame.opcode\n        )\n\n        head2 = 0b10000000 if mask else 0\n\n        length = len(frame.data)\n        if length < 126:\n            output.write(struct.pack(\"!BB\", head1, head2 | length))\n        elif length < 65536:\n            output.write(struct.pack(\"!BBH\", head1, head2 | 126, length))\n        else:\n            output.write(struct.pack(\"!BBQ\", head1, head2 | 127, length))\n\n        if mask:\n            mask_bits = struct.pack(\"!I\", random.getrandbits(32))\n            output.write(mask_bits)\n\n        # Prepare the data.\n        if mask:\n            data = apply_mask(frame.data, mask_bits)\n        else:\n            data = frame.data\n        output.write(data)\n\n        # Send the frame.\n\n        # The frame is written in a single call to writer in order to prevent\n        # TCP fragmentation. See #68 for details. This also makes it safe to\n        # send frames concurrently from multiple coroutines.\n        writer(output.getvalue())"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck that this frame contains acceptable values. Raise WebSocketProtocolError if not.", "response": "def check(frame) -> None:\n        \"\"\"\n        Check that this frame contains acceptable values.\n\n        Raise :exc:`~websockets.exceptions.WebSocketProtocolError` if this\n        frame contains incorrect values.\n\n        \"\"\"\n        # The first parameter is called `frame` rather than `self`,\n        # but it's the instance of class to which this method is bound.\n\n        if frame.rsv1 or frame.rsv2 or frame.rsv3:\n            raise WebSocketProtocolError(\"Reserved bits must be 0\")\n\n        if frame.opcode in DATA_OPCODES:\n            return\n        elif frame.opcode in CTRL_OPCODES:\n            if len(frame.data) > 125:\n                raise WebSocketProtocolError(\"Control frame too long\")\n            if not frame.fin:\n                raise WebSocketProtocolError(\"Fragmented control frame\")\n        else:\n            raise WebSocketProtocolError(f\"Invalid opcode: {frame.opcode}\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def read_request(stream: asyncio.StreamReader) -> Tuple[str, \"Headers\"]:\n    # https://tools.ietf.org/html/rfc7230#section-3.1.1\n\n    # Parsing is simple because fixed values are expected for method and\n    # version and because path isn't checked. Since WebSocket software tends\n    # to implement HTTP/1.1 strictly, there's little need for lenient parsing.\n\n    request_line = await read_line(stream)\n\n    # This may raise \"ValueError: not enough values to unpack\"\n    method, raw_path, version = request_line.split(b\" \", 2)\n\n    if method != b\"GET\":\n        raise ValueError(\"Unsupported HTTP method: %r\" % method)\n    if version != b\"HTTP/1.1\":\n        raise ValueError(\"Unsupported HTTP version: %r\" % version)\n    path = raw_path.decode(\"ascii\", \"surrogateescape\")\n\n    headers = await read_headers(stream)\n\n    return path, headers", "response": "Read an HTTP GET request from stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread an HTTP response from the stream.", "response": "async def read_response(stream: asyncio.StreamReader) -> Tuple[int, str, \"Headers\"]:\n    \"\"\"\n    Read an HTTP/1.1 response from ``stream``.\n\n    ``stream`` is an :class:`~asyncio.StreamReader`.\n\n    Return ``(status_code, reason, headers)`` where ``status_code`` is an\n    :class:`int`, ``reason`` is a :class:`str`, and ``headers`` is a\n    :class:`Headers` instance.\n\n    Non-ASCII characters are represented with surrogate escapes.\n\n    Raise an exception if the response isn't well formatted.\n\n    Don't attempt to read the response body, because WebSocket handshake\n    responses don't have one. If the response contains a body, it may be\n    read from ``stream`` after this coroutine returns.\n\n    \"\"\"\n    # https://tools.ietf.org/html/rfc7230#section-3.1.2\n\n    # As in read_request, parsing is simple because a fixed value is expected\n    # for version, status_code is a 3-digit number, and reason can be ignored.\n\n    status_line = await read_line(stream)\n\n    # This may raise \"ValueError: not enough values to unpack\"\n    version, raw_status_code, raw_reason = status_line.split(b\" \", 2)\n\n    if version != b\"HTTP/1.1\":\n        raise ValueError(\"Unsupported HTTP version: %r\" % version)\n    # This may raise \"ValueError: invalid literal for int() with base 10\"\n    status_code = int(raw_status_code)\n    if not 100 <= status_code < 1000:\n        raise ValueError(\"Unsupported HTTP status code: %d\" % status_code)\n    if not _value_re.fullmatch(raw_reason):\n        raise ValueError(\"Invalid HTTP reason phrase: %r\" % raw_reason)\n    reason = raw_reason.decode()\n\n    headers = await read_headers(stream)\n\n    return status_code, reason, headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def read_headers(stream: asyncio.StreamReader) -> \"Headers\":\n    # https://tools.ietf.org/html/rfc7230#section-3.2\n\n    # We don't attempt to support obsolete line folding.\n\n    headers = Headers()\n    for _ in range(MAX_HEADERS + 1):\n        line = await read_line(stream)\n        if line == b\"\":\n            break\n\n        # This may raise \"ValueError: not enough values to unpack\"\n        raw_name, raw_value = line.split(b\":\", 1)\n        if not _token_re.fullmatch(raw_name):\n            raise ValueError(\"Invalid HTTP header name: %r\" % raw_name)\n        raw_value = raw_value.strip(b\" \\t\")\n        if not _value_re.fullmatch(raw_value):\n            raise ValueError(\"Invalid HTTP header value: %r\" % raw_value)\n\n        name = raw_name.decode(\"ascii\")  # guaranteed to be ASCII at this point\n        value = raw_value.decode(\"ascii\", \"surrogateescape\")\n        headers[name] = value\n\n    else:\n        raise ValueError(\"Too many HTTP headers\")\n\n    return headers", "response": "Read HTTP headers from stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a single line from stream.", "response": "async def read_line(stream: asyncio.StreamReader) -> bytes:\n    \"\"\"\n    Read a single line from ``stream``.\n\n    ``stream`` is an :class:`~asyncio.StreamReader`.\n\n    Return :class:`bytes` without CRLF.\n\n    \"\"\"\n    # Security: this is bounded by the StreamReader's limit (default = 32\u00a0KiB).\n    line = await stream.readline()\n    # Security: this guarantees header values are small (hard-coded = 4\u00a0KiB)\n    if len(line) > MAX_LINE:\n        raise ValueError(\"Line too long\")\n    # Not mandatory but safe - https://tools.ietf.org/html/rfc7230#section-3.5\n    if not line.endswith(b\"\\r\\n\"):\n        raise ValueError(\"Line without CRLF\")\n    return line[:-2]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_all(self, key: str) -> List[str]:\n        return self._dict.get(key.lower(), [])", "response": "Return the list of all values for a key."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the next character from header at the given position.", "response": "def peek_ahead(header: str, pos: int) -> Optional[str]:\n    \"\"\"\n    Return the next character from ``header`` at the given position.\n\n    Return ``None`` at the end of ``header``.\n\n    We never need to peek more than one character ahead.\n\n    \"\"\"\n    return None if pos == len(header) else header[pos]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_OWS(header: str, pos: int) -> int:\n    # There's always a match, possibly empty, whose content doesn't matter.\n    match = _OWS_re.match(header, pos)\n    assert match is not None\n    return match.end()", "response": "Parse optional whitespace from header at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a token from header at the given position.", "response": "def parse_token(header: str, pos: int, header_name: str) -> Tuple[str, int]:\n    \"\"\"\n    Parse a token from ``header`` at the given position.\n\n    Return the token value and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    match = _token_re.match(header, pos)\n    if match is None:\n        raise InvalidHeaderFormat(header_name, \"expected token\", header, pos)\n    return match.group(), match.end()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a quoted string from header at the given position.", "response": "def parse_quoted_string(header: str, pos: int, header_name: str) -> Tuple[str, int]:\n    \"\"\"\n    Parse a quoted string from ``header`` at the given position.\n\n    Return the unquoted value and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    match = _quoted_string_re.match(header, pos)\n    if match is None:\n        raise InvalidHeaderFormat(header_name, \"expected quoted string\", header, pos)\n    return _unquote_re.sub(r\"\\1\", match.group()[1:-1]), match.end()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a comma - separated list from the given header at the given position.", "response": "def parse_list(\n    parse_item: Callable[[str, int, str], Tuple[T, int]],\n    header: str,\n    pos: int,\n    header_name: str,\n) -> List[T]:\n    \"\"\"\n    Parse a comma-separated list from ``header`` at the given position.\n\n    This is appropriate for parsing values with the following grammar:\n\n        1#item\n\n    ``parse_item`` parses one item.\n\n    ``header`` is assumed not to start or end with whitespace.\n\n    (This function is designed for parsing an entire header value and\n    :func:`~websockets.http.read_headers` strips whitespace from values.)\n\n    Return a list of items.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    # Per https://tools.ietf.org/html/rfc7230#section-7, \"a recipient MUST\n    # parse and ignore a reasonable number of empty list elements\"; hence\n    # while loops that remove extra delimiters.\n\n    # Remove extra delimiters before the first item.\n    while peek_ahead(header, pos) == \",\":\n        pos = parse_OWS(header, pos + 1)\n\n    items = []\n    while True:\n        # Loop invariant: a item starts at pos in header.\n        item, pos = parse_item(header, pos, header_name)\n        items.append(item)\n        pos = parse_OWS(header, pos)\n\n        # We may have reached the end of the header.\n        if pos == len(header):\n            break\n\n        # There must be a delimiter after each element except the last one.\n        if peek_ahead(header, pos) == \",\":\n            pos = parse_OWS(header, pos + 1)\n        else:\n            raise InvalidHeaderFormat(header_name, \"expected comma\", header, pos)\n\n        # Remove extra delimiters before the next item.\n        while peek_ahead(header, pos) == \",\":\n            pos = parse_OWS(header, pos + 1)\n\n        # We may have reached the end of the header.\n        if pos == len(header):\n            break\n\n    # Since we only advance in the header by one character with peek_ahead()\n    # or with the end position of a regex match, we can't overshoot the end.\n    assert pos == len(header)\n\n    return items"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_connection_option(\n    header: str, pos: int, header_name: str\n) -> Tuple[ConnectionOption, int]:\n    \"\"\"\n    Parse a Connection option from ``header`` at the given position.\n\n    Return the protocol value and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    item, pos = parse_token(header, pos, header_name)\n    return cast(ConnectionOption, item), pos", "response": "Parses a Connection option from header at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_upgrade_protocol(\n    header: str, pos: int, header_name: str\n) -> Tuple[UpgradeProtocol, int]:\n    \"\"\"\n    Parse an Upgrade protocol from ``header`` at the given position.\n\n    Return the protocol value and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    match = _protocol_re.match(header, pos)\n    if match is None:\n        raise InvalidHeaderFormat(header_name, \"expected protocol\", header, pos)\n    return cast(UpgradeProtocol, match.group()), match.end()", "response": "Parses an Upgrade protocol from header at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses a single extension parameter from header at the given position.", "response": "def parse_extension_item_param(\n    header: str, pos: int, header_name: str\n) -> Tuple[ExtensionParameter, int]:\n    \"\"\"\n    Parse a single extension parameter from ``header`` at the given position.\n\n    Return a ``(name, value)`` pair and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    # Extract parameter name.\n    name, pos = parse_token(header, pos, header_name)\n    pos = parse_OWS(header, pos)\n    # Extract parameter value, if there is one.\n    value: Optional[str] = None\n    if peek_ahead(header, pos) == \"=\":\n        pos = parse_OWS(header, pos + 1)\n        if peek_ahead(header, pos) == '\"':\n            pos_before = pos  # for proper error reporting below\n            value, pos = parse_quoted_string(header, pos, header_name)\n            # https://tools.ietf.org/html/rfc6455#section-9.1 says: the value\n            # after quoted-string unescaping MUST conform to the 'token' ABNF.\n            if _token_re.fullmatch(value) is None:\n                raise InvalidHeaderFormat(\n                    header_name, \"invalid quoted header content\", header, pos_before\n                )\n        else:\n            value, pos = parse_token(header, pos, header_name)\n        pos = parse_OWS(header, pos)\n\n    return (name, value), pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_extension_item(\n    header: str, pos: int, header_name: str\n) -> Tuple[ExtensionHeader, int]:\n    \"\"\"\n    Parse an extension definition from ``header`` at the given position.\n\n    Return an ``(extension name, parameters)`` pair, where ``parameters`` is a\n    list of ``(name, value)`` pairs, and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    # Extract extension name.\n    name, pos = parse_token(header, pos, header_name)\n    pos = parse_OWS(header, pos)\n    # Extract all parameters.\n    parameters = []\n    while peek_ahead(header, pos) == \";\":\n        pos = parse_OWS(header, pos + 1)\n        parameter, pos = parse_extension_item_param(header, pos, header_name)\n        parameters.append(parameter)\n    return (name, parameters), pos", "response": "Parses an extension definition from header at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_extension_item(name: str, parameters: List[ExtensionParameter]) -> str:\n    return \"; \".join(\n        [name]\n        + [\n            # Quoted strings aren't necessary because values are always tokens.\n            name if value is None else f\"{name}={value}\"\n            for name, value in parameters\n        ]\n    )", "response": "Builds an extension definition."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_extension(extensions: Sequence[ExtensionHeader]) -> str:\n    return \", \".join(\n        build_extension_item(name, parameters) for name, parameters in extensions\n    )", "response": "Build a list of extension items from a list of extension names and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_subprotocol_item(\n    header: str, pos: int, header_name: str\n) -> Tuple[Subprotocol, int]:\n    \"\"\"\n    Parse a subprotocol from ``header`` at the given position.\n\n    Return the subprotocol value and the new position.\n\n    Raise :exc:`~websockets.exceptions.InvalidHeaderFormat` on invalid inputs.\n\n    \"\"\"\n    item, pos = parse_token(header, pos, header_name)\n    return cast(Subprotocol, item), pos", "response": "Parses a subprotocol from header at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild an HTTP Basic Auth header.", "response": "def build_basic_auth(username: str, password: str) -> str:\n    \"\"\"\n    Build an Authorization header for HTTP Basic Auth.\n\n    \"\"\"\n    # https://tools.ietf.org/html/rfc7617#section-2\n    assert \":\" not in username\n    user_pass = f\"{username}:{password}\"\n    basic_credentials = base64.b64encode(user_pass.encode()).decode()\n    return \"Basic \" + basic_credentials"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserving a websocket handler on Unix sockets.", "response": "def unix_serve(\n    ws_handler: Callable[[WebSocketServerProtocol, str], Awaitable[Any]],\n    path: str,\n    **kwargs: Any,\n) -> Serve:\n    \"\"\"\n    Similar to :func:`serve()`, but for listening on Unix sockets.\n\n    This function calls the event loop's\n    :meth:`~asyncio.AbstractEventLoop.create_unix_server` method.\n\n    It is only available on Unix.\n\n    It's useful for deploying a server behind a reverse proxy such as nginx.\n\n    \"\"\"\n    return serve(ws_handler, path=path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when a connection is made.", "response": "def connection_made(self, transport: asyncio.BaseTransport) -> None:\n        \"\"\"\n        Register connection and initialize a task to handle it.\n\n        \"\"\"\n        super().connection_made(transport)\n        # Register the connection with the server before creating the handler\n        # task. Registering at the beginning of the handler coroutine would\n        # create a race condition between the creation of the task, which\n        # schedules its execution, and the moment the handler starts running.\n        self.ws_server.register(self)\n        self.handler_task = self.loop.create_task(self.handler())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling the lifecycle of a WebSocket connection.", "response": "async def handler(self) -> None:\n        \"\"\"\n        Handle the lifecycle of a WebSocket connection.\n\n        Since this method doesn't have a caller able to handle exceptions, it\n        attemps to log relevant ones and guarantees that the TCP connection is\n        closed before exiting.\n\n        \"\"\"\n        try:\n\n            try:\n                path = await self.handshake(\n                    origins=self.origins,\n                    available_extensions=self.available_extensions,\n                    available_subprotocols=self.available_subprotocols,\n                    extra_headers=self.extra_headers,\n                )\n            except ConnectionError:\n                logger.debug(\"Connection error in opening handshake\", exc_info=True)\n                raise\n            except Exception as exc:\n                if isinstance(exc, AbortHandshake):\n                    status, headers, body = exc.status, exc.headers, exc.body\n                elif isinstance(exc, InvalidOrigin):\n                    logger.debug(\"Invalid origin\", exc_info=True)\n                    status, headers, body = (\n                        http.HTTPStatus.FORBIDDEN,\n                        Headers(),\n                        (str(exc) + \"\\n\").encode(),\n                    )\n                elif isinstance(exc, InvalidUpgrade):\n                    logger.debug(\"Invalid upgrade\", exc_info=True)\n                    status, headers, body = (\n                        http.HTTPStatus.UPGRADE_REQUIRED,\n                        Headers([(\"Upgrade\", \"websocket\")]),\n                        (str(exc) + \"\\n\").encode(),\n                    )\n                elif isinstance(exc, InvalidHandshake):\n                    logger.debug(\"Invalid handshake\", exc_info=True)\n                    status, headers, body = (\n                        http.HTTPStatus.BAD_REQUEST,\n                        Headers(),\n                        (str(exc) + \"\\n\").encode(),\n                    )\n                else:\n                    logger.warning(\"Error in opening handshake\", exc_info=True)\n                    status, headers, body = (\n                        http.HTTPStatus.INTERNAL_SERVER_ERROR,\n                        Headers(),\n                        b\"See server log for more information.\\n\",\n                    )\n\n                headers.setdefault(\"Date\", email.utils.formatdate(usegmt=True))\n                headers.setdefault(\"Server\", USER_AGENT)\n                headers.setdefault(\"Content-Length\", str(len(body)))\n                headers.setdefault(\"Content-Type\", \"text/plain\")\n                headers.setdefault(\"Connection\", \"close\")\n\n                self.write_http_response(status, headers, body)\n                self.fail_connection()\n                await self.wait_closed()\n                return\n\n            try:\n                await self.ws_handler(self, path)\n            except Exception:\n                logger.error(\"Error in connection handler\", exc_info=True)\n                if not self.closed:\n                    self.fail_connection(1011)\n                raise\n\n            try:\n                await self.close()\n            except ConnectionError:\n                logger.debug(\"Connection error in closing handshake\", exc_info=True)\n                raise\n            except Exception:\n                logger.warning(\"Error in closing handshake\", exc_info=True)\n                raise\n\n        except Exception:\n            # Last-ditch attempt to avoid leaking connections on errors.\n            try:\n                self.writer.close()\n            except Exception:  # pragma: no cover\n                pass\n\n        finally:\n            # Unregister the connection with the server when the handler task\n            # terminates. Registration is tied to the lifecycle of the handler\n            # task because the server waits for tasks attached to registered\n            # connections before terminating.\n            self.ws_server.unregister(self)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def read_http_request(self) -> Tuple[str, Headers]:\n        try:\n            path, headers = await read_request(self.reader)\n        except ValueError as exc:\n            raise InvalidMessage(\"Malformed HTTP message\") from exc\n\n        logger.debug(\"%s < GET %s HTTP/1.1\", self.side, path)\n        logger.debug(\"%s < %r\", self.side, headers)\n\n        self.path = path\n        self.request_headers = headers\n\n        return path, headers", "response": "Read a request from the HTTP request reader."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_http_response(\n        self, status: http.HTTPStatus, headers: Headers, body: Optional[bytes] = None\n    ) -> None:\n        \"\"\"\n        Write status line and headers to the HTTP response.\n\n        This coroutine is also able to write a response body.\n\n        \"\"\"\n        self.response_headers = headers\n\n        logger.debug(\"%s > HTTP/1.1 %d %s\", self.side, status.value, status.phrase)\n        logger.debug(\"%s > %r\", self.side, headers)\n\n        # Since the status line and headers only contain ASCII characters,\n        # we can keep this simple.\n        response = f\"HTTP/1.1 {status.value} {status.phrase}\\r\\n\"\n        response += str(headers)\n\n        self.writer.write(response.encode())\n\n        if body is not None:\n            logger.debug(\"%s > Body (%d bytes)\", self.side, len(body))\n            self.writer.write(body)", "response": "Write status line and headers to the HTTP response."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_request(\n        self, path: str, request_headers: Headers\n    ) -> Union[Optional[HTTPResponse], Awaitable[Optional[HTTPResponse]]]:\n        \"\"\"\n        Intercept the HTTP request and return an HTTP response if needed.\n\n        ``request_headers`` is a :class:`~websockets.http.Headers` instance.\n\n        If this method returns ``None``, the WebSocket handshake continues.\n        If it returns a status code, headers and a response body, that HTTP\n        response is sent and the connection is closed.\n\n        The HTTP status must be a :class:`~http.HTTPStatus`.\n\n        HTTP headers must be a :class:`~websockets.http.Headers` instance, a\n        :class:`~collections.abc.Mapping`, or an iterable of ``(name, value)``\n        pairs.\n\n        The HTTP response body must be :class:`bytes`. It may be empty.\n\n        This method may be overridden to check the request headers and set a\n        different status, for example to authenticate the request and return\n        ``HTTPStatus.UNAUTHORIZED`` or ``HTTPStatus.FORBIDDEN``.\n\n        It can be declared as a function or as a coroutine because such\n        authentication checks are likely to require network requests.\n\n        It may also be overridden by passing a ``process_request`` argument to\n        the :class:`WebSocketServerProtocol` constructor or the :func:`serve`\n        function.\n\n        \"\"\"\n        if self._process_request is not None:\n            return self._process_request(path, request_headers)\n        return None", "response": "Intercepts the HTTP request and returns an HTTP response if needed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_origin(\n        headers: Headers, origins: Optional[Sequence[Optional[Origin]]] = None\n    ) -> Optional[Origin]:\n        \"\"\"\n        Handle the Origin HTTP request header.\n\n        Raise :exc:`~websockets.exceptions.InvalidOrigin` if the origin isn't\n        acceptable.\n\n        \"\"\"\n        # \"The user agent MUST NOT include more than one Origin header field\"\n        # per https://tools.ietf.org/html/rfc6454#section-7.3.\n        try:\n            origin = cast(Origin, headers.get(\"Origin\"))\n        except MultipleValuesError:\n            raise InvalidHeader(\"Origin\", \"more than one Origin header found\")\n        if origins is not None:\n            if origin not in origins:\n                raise InvalidOrigin(origin)\n        return origin", "response": "Process the Origin HTTP request header."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the Sec - WebSocket - Extensions HTTP request header and the list of extensions.", "response": "def process_extensions(\n        headers: Headers,\n        available_extensions: Optional[Sequence[ServerExtensionFactory]],\n    ) -> Tuple[Optional[str], List[Extension]]:\n        \"\"\"\n        Handle the Sec-WebSocket-Extensions HTTP request header.\n\n        Accept or reject each extension proposed in the client request.\n        Negotiate parameters for accepted extensions.\n\n        Return the Sec-WebSocket-Extensions HTTP response header and the list\n        of accepted extensions.\n\n        Raise :exc:`~websockets.exceptions.InvalidHandshake` to abort the\n        handshake with an HTTP 400 error code. (The default implementation\n        never does this.)\n\n        :rfc:`6455` leaves the rules up to the specification of each\n        :extension.\n\n        To provide this level of flexibility, for each extension proposed by\n        the client, we check for a match with each extension available in the\n        server configuration. If no match is found, the extension is ignored.\n\n        If several variants of the same extension are proposed by the client,\n        it may be accepted severel times, which won't make sense in general.\n        Extensions must implement their own requirements. For this purpose,\n        the list of previously accepted extensions is provided.\n\n        This process doesn't allow the server to reorder extensions. It can\n        only select a subset of the extensions proposed by the client.\n\n        Other requirements, for example related to mandatory extensions or the\n        order of extensions, may be implemented by overriding this method.\n\n        \"\"\"\n        response_header_value: Optional[str] = None\n\n        extension_headers: List[ExtensionHeader] = []\n        accepted_extensions: List[Extension] = []\n\n        header_values = headers.get_all(\"Sec-WebSocket-Extensions\")\n\n        if header_values and available_extensions:\n\n            parsed_header_values: List[ExtensionHeader] = sum(\n                [parse_extension(header_value) for header_value in header_values], []\n            )\n\n            for name, request_params in parsed_header_values:\n\n                for ext_factory in available_extensions:\n\n                    # Skip non-matching extensions based on their name.\n                    if ext_factory.name != name:\n                        continue\n\n                    # Skip non-matching extensions based on their params.\n                    try:\n                        response_params, extension = ext_factory.process_request_params(\n                            request_params, accepted_extensions\n                        )\n                    except NegotiationError:\n                        continue\n\n                    # Add matching extension to the final list.\n                    extension_headers.append((name, response_params))\n                    accepted_extensions.append(extension)\n\n                    # Break out of the loop once we have a match.\n                    break\n\n                # If we didn't break from the loop, no extension in our list\n                # matched what the client sent. The extension is declined.\n\n        # Serialize extension header.\n        if extension_headers:\n            response_header_value = build_extension(extension_headers)\n\n        return response_header_value, accepted_extensions"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_subprotocol(\n        self, headers: Headers, available_subprotocols: Optional[Sequence[Subprotocol]]\n    ) -> Optional[Subprotocol]:\n        \"\"\"\n        Handle the Sec-WebSocket-Protocol HTTP request header.\n\n        Return Sec-WebSocket-Protocol HTTP response header, which is the same\n        as the selected subprotocol.\n\n        \"\"\"\n        subprotocol: Optional[Subprotocol] = None\n\n        header_values = headers.get_all(\"Sec-WebSocket-Protocol\")\n\n        if header_values and available_subprotocols:\n\n            parsed_header_values: List[Subprotocol] = sum(\n                [parse_subprotocol(header_value) for header_value in header_values], []\n            )\n\n            subprotocol = self.select_subprotocol(\n                parsed_header_values, available_subprotocols\n            )\n\n        return subprotocol", "response": "Process the Sec - WebSocket - Protocol HTTP request header."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nselect a subprotocol among the client and server subprotocols.", "response": "def select_subprotocol(\n        self,\n        client_subprotocols: Sequence[Subprotocol],\n        server_subprotocols: Sequence[Subprotocol],\n    ) -> Optional[Subprotocol]:\n        \"\"\"\n        Pick a subprotocol among those offered by the client.\n\n        If several subprotocols are supported by the client and the server,\n        the default implementation selects the preferred subprotocols by\n        giving equal value to the priorities of the client and the server.\n\n        If no subprotocols are supported by the client and the server, it\n        proceeds without a subprotocol.\n\n        This is unlikely to be the most useful implementation in practice, as\n        many servers providing a subprotocol will require that the client uses\n        that subprotocol. Such rules can be implemented in a subclass.\n\n        This method may be overridden by passing a ``select_subprotocol``\n        argument to the :class:`WebSocketServerProtocol` constructor or the\n        :func:`serve` function.\n\n        \"\"\"\n        if self._select_subprotocol is not None:\n            return self._select_subprotocol(client_subprotocols, server_subprotocols)\n\n        subprotocols = set(client_subprotocols) & set(server_subprotocols)\n        if not subprotocols:\n            return None\n        priority = lambda p: (\n            client_subprotocols.index(p) + server_subprotocols.index(p)\n        )\n        return sorted(subprotocols, key=priority)[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms the server side of the handshake.", "response": "async def handshake(\n        self,\n        origins: Optional[Sequence[Optional[Origin]]] = None,\n        available_extensions: Optional[Sequence[ServerExtensionFactory]] = None,\n        available_subprotocols: Optional[Sequence[Subprotocol]] = None,\n        extra_headers: Optional[HeadersLikeOrCallable] = None,\n    ) -> str:\n        \"\"\"\n        Perform the server side of the opening handshake.\n\n        If provided, ``origins`` is a list of acceptable HTTP Origin values.\n        Include ``None`` if the lack of an origin is acceptable.\n\n        If provided, ``available_extensions`` is a list of supported\n        extensions in the order in which they should be used.\n\n        If provided, ``available_subprotocols`` is a list of supported\n        subprotocols in order of decreasing preference.\n\n        If provided, ``extra_headers`` sets additional HTTP response headers.\n        It can be a :class:`~websockets.http.Headers` instance, a\n        :class:`~collections.abc.Mapping`, an iterable of ``(name, value)``\n        pairs, or a callable taking the request path and headers in arguments\n        and returning one of the above.\n\n        Raise :exc:`~websockets.exceptions.InvalidHandshake` if the handshake\n        fails.\n\n        Return the path of the URI of the request.\n\n        \"\"\"\n        path, request_headers = await self.read_http_request()\n\n        # Hook for customizing request handling, for example checking\n        # authentication or treating some paths as plain HTTP endpoints.\n        early_response = self.process_request(path, request_headers)\n        if isinstance(early_response, Awaitable):\n            early_response = await early_response\n\n        # Change the response to a 503 error if the server is shutting down.\n        if not self.ws_server.is_serving():\n            early_response = (\n                http.HTTPStatus.SERVICE_UNAVAILABLE,\n                [],\n                b\"Server is shutting down.\\n\",\n            )\n\n        if early_response is not None:\n            raise AbortHandshake(*early_response)\n\n        key = check_request(request_headers)\n\n        self.origin = self.process_origin(request_headers, origins)\n\n        extensions_header, self.extensions = self.process_extensions(\n            request_headers, available_extensions\n        )\n\n        protocol_header = self.subprotocol = self.process_subprotocol(\n            request_headers, available_subprotocols\n        )\n\n        response_headers = Headers()\n\n        build_response(response_headers, key)\n\n        if extensions_header is not None:\n            response_headers[\"Sec-WebSocket-Extensions\"] = extensions_header\n\n        if protocol_header is not None:\n            response_headers[\"Sec-WebSocket-Protocol\"] = protocol_header\n\n        if extra_headers is not None:\n            if callable(extra_headers):\n                extra_headers = extra_headers(path, self.request_headers)\n            if isinstance(extra_headers, Headers):\n                extra_headers = extra_headers.raw_items()\n            elif isinstance(extra_headers, collections.abc.Mapping):\n                extra_headers = extra_headers.items()\n            for name, value in extra_headers:\n                response_headers[name] = value\n\n        response_headers.setdefault(\"Date\", email.utils.formatdate(usegmt=True))\n        response_headers.setdefault(\"Server\", USER_AGENT)\n\n        self.write_http_response(http.HTTPStatus.SWITCHING_PROTOCOLS, response_headers)\n\n        self.connection_open()\n\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_serving(self) -> bool:\n        try:\n            # Python \u2265 3.7\n            return self.server.is_serving()  # type: ignore\n        except AttributeError:  # pragma: no cover\n            # Python < 3.7\n            return self.server.sockets is not None", "response": "Tells whether the server is serving new connections or shutting down."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self) -> None:\n        if self.close_task is None:\n            self.close_task = self.loop.create_task(self._close())", "response": "Closes the server and terminate connections with close code 1001."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _close(self) -> None:\n        # Stop accepting new connections.\n        self.server.close()\n\n        # Wait until self.server.close() completes.\n        await self.server.wait_closed()\n\n        # Wait until all accepted connections reach connection_made() and call\n        # register(). See https://bugs.python.org/issue34852 for details.\n        await asyncio.sleep(0)\n\n        # Close open connections. fail_connection() will cancel the transfer\n        # data task, which is expected to cause the handler task to terminate.\n        for websocket in self.websockets:\n            if websocket.state is State.OPEN:\n                websocket.fail_connection(1001)\n\n        # asyncio.wait doesn't accept an empty first argument.\n        if self.websockets:\n            # The connection handler can terminate before or after the\n            # connection closes. Wait until both are done to avoid leaking\n            # running tasks.\n            # TODO: it would be nicer to wait only for the connection handler\n            # and let the handler wait for the connection to close.\n            await asyncio.wait(\n                [websocket.handler_task for websocket in self.websockets]\n                + [\n                    websocket.close_connection_task\n                    for websocket in self.websockets\n                    if websocket.state is State.OPEN\n                ],\n                loop=self.loop,\n            )\n\n        # Tell wait_closed() to return.\n        self.closed_waiter.set_result(None)", "response": "Closes the underlying server and all websockets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite the HTTP request line and headers to the HTTP request.", "response": "def write_http_request(self, path: str, headers: Headers) -> None:\n        \"\"\"\n        Write request line and headers to the HTTP request.\n\n        \"\"\"\n        self.path = path\n        self.request_headers = headers\n\n        logger.debug(\"%s > GET %s HTTP/1.1\", self.side, path)\n        logger.debug(\"%s > %r\", self.side, headers)\n\n        # Since the path and headers only contain ASCII characters,\n        # we can keep this simple.\n        request = f\"GET {path} HTTP/1.1\\r\\n\"\n        request += str(headers)\n\n        self.writer.write(request.encode())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the HTTP response from the reader.", "response": "async def read_http_response(self) -> Tuple[int, Headers]:\n        \"\"\"\n        Read status line and headers from the HTTP response.\n\n        Raise :exc:`~websockets.exceptions.InvalidMessage` if the HTTP message\n        is malformed or isn't an HTTP/1.1 GET request.\n\n        Don't attempt to read the response body because WebSocket handshake\n        responses don't have one. If the response contains a body, it may be\n        read from ``self.reader`` after this coroutine returns.\n\n        \"\"\"\n        try:\n            status_code, reason, headers = await read_response(self.reader)\n        except ValueError as exc:\n            raise InvalidMessage(\"Malformed HTTP message\") from exc\n\n        logger.debug(\"%s < HTTP/1.1 %d %s\", self.side, status_code, reason)\n        logger.debug(\"%s < %r\", self.side, headers)\n\n        self.response_headers = headers\n\n        return status_code, self.response_headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_extensions(\n        headers: Headers,\n        available_extensions: Optional[Sequence[ClientExtensionFactory]],\n    ) -> List[Extension]:\n        \"\"\"\n        Handle the Sec-WebSocket-Extensions HTTP response header.\n\n        Check that each extension is supported, as well as its parameters.\n\n        Return the list of accepted extensions.\n\n        Raise :exc:`~websockets.exceptions.InvalidHandshake` to abort the\n        connection.\n\n        :rfc:`6455` leaves the rules up to the specification of each\n        :extension.\n\n        To provide this level of flexibility, for each extension accepted by\n        the server, we check for a match with each extension available in the\n        client configuration. If no match is found, an exception is raised.\n\n        If several variants of the same extension are accepted by the server,\n        it may be configured severel times, which won't make sense in general.\n        Extensions must implement their own requirements. For this purpose,\n        the list of previously accepted extensions is provided.\n\n        Other requirements, for example related to mandatory extensions or the\n        order of extensions, may be implemented by overriding this method.\n\n        \"\"\"\n        accepted_extensions: List[Extension] = []\n\n        header_values = headers.get_all(\"Sec-WebSocket-Extensions\")\n\n        if header_values:\n\n            if available_extensions is None:\n                raise InvalidHandshake(\"No extensions supported\")\n\n            parsed_header_values: List[ExtensionHeader] = sum(\n                [parse_extension(header_value) for header_value in header_values], []\n            )\n\n            for name, response_params in parsed_header_values:\n\n                for extension_factory in available_extensions:\n\n                    # Skip non-matching extensions based on their name.\n                    if extension_factory.name != name:\n                        continue\n\n                    # Skip non-matching extensions based on their params.\n                    try:\n                        extension = extension_factory.process_response_params(\n                            response_params, accepted_extensions\n                        )\n                    except NegotiationError:\n                        continue\n\n                    # Add matching extension to the final list.\n                    accepted_extensions.append(extension)\n\n                    # Break out of the loop once we have a match.\n                    break\n\n                # If we didn't break from the loop, no extension in our list\n                # matched what the server sent. Fail the connection.\n                else:\n                    raise NegotiationError(\n                        f\"Unsupported extension: \"\n                        f\"name = {name}, params = {response_params}\"\n                    )\n\n        return accepted_extensions", "response": "Process the Sec - WebSocket - Extensions HTTP response header and return a list of extensions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprocesses the Sec - WebSocket - Protocol HTTP response header.", "response": "def process_subprotocol(\n        headers: Headers, available_subprotocols: Optional[Sequence[Subprotocol]]\n    ) -> Optional[Subprotocol]:\n        \"\"\"\n        Handle the Sec-WebSocket-Protocol HTTP response header.\n\n        Check that it contains exactly one supported subprotocol.\n\n        Return the selected subprotocol.\n\n        \"\"\"\n        subprotocol: Optional[Subprotocol] = None\n\n        header_values = headers.get_all(\"Sec-WebSocket-Protocol\")\n\n        if header_values:\n\n            if available_subprotocols is None:\n                raise InvalidHandshake(\"No subprotocols supported\")\n\n            parsed_header_values: Sequence[Subprotocol] = sum(\n                [parse_subprotocol(header_value) for header_value in header_values], []\n            )\n\n            if len(parsed_header_values) > 1:\n                subprotocols = \", \".join(parsed_header_values)\n                raise InvalidHandshake(f\"Multiple subprotocols: {subprotocols}\")\n\n            subprotocol = parsed_header_values[0]\n\n            if subprotocol not in available_subprotocols:\n                raise NegotiationError(f\"Unsupported subprotocol: {subprotocol}\")\n\n        return subprotocol"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def handshake(\n        self,\n        wsuri: WebSocketURI,\n        origin: Optional[Origin] = None,\n        available_extensions: Optional[Sequence[ClientExtensionFactory]] = None,\n        available_subprotocols: Optional[Sequence[Subprotocol]] = None,\n        extra_headers: Optional[HeadersLike] = None,\n    ) -> None:\n        \"\"\"\n        Perform the client side of the opening handshake.\n\n        If provided, ``origin`` sets the Origin HTTP header.\n\n        If provided, ``available_extensions`` is a list of supported\n        extensions in the order in which they should be used.\n\n        If provided, ``available_subprotocols`` is a list of supported\n        subprotocols in order of decreasing preference.\n\n        If provided, ``extra_headers`` sets additional HTTP request headers.\n        It must be a :class:`~websockets.http.Headers` instance, a\n        :class:`~collections.abc.Mapping`, or an iterable of ``(name, value)``\n        pairs.\n\n        Raise :exc:`~websockets.exceptions.InvalidHandshake` if the handshake\n        fails.\n\n        \"\"\"\n        request_headers = Headers()\n\n        if wsuri.port == (443 if wsuri.secure else 80):  # pragma: no cover\n            request_headers[\"Host\"] = wsuri.host\n        else:\n            request_headers[\"Host\"] = f\"{wsuri.host}:{wsuri.port}\"\n\n        if wsuri.user_info:\n            request_headers[\"Authorization\"] = build_basic_auth(*wsuri.user_info)\n\n        if origin is not None:\n            request_headers[\"Origin\"] = origin\n\n        key = build_request(request_headers)\n\n        if available_extensions is not None:\n            extensions_header = build_extension(\n                [\n                    (extension_factory.name, extension_factory.get_request_params())\n                    for extension_factory in available_extensions\n                ]\n            )\n            request_headers[\"Sec-WebSocket-Extensions\"] = extensions_header\n\n        if available_subprotocols is not None:\n            protocol_header = build_subprotocol(available_subprotocols)\n            request_headers[\"Sec-WebSocket-Protocol\"] = protocol_header\n\n        if extra_headers is not None:\n            if isinstance(extra_headers, Headers):\n                extra_headers = extra_headers.raw_items()\n            elif isinstance(extra_headers, collections.abc.Mapping):\n                extra_headers = extra_headers.items()\n            for name, value in extra_headers:\n                request_headers[name] = value\n\n        request_headers.setdefault(\"User-Agent\", USER_AGENT)\n\n        self.write_http_request(wsuri.resource_name, request_headers)\n\n        status_code, response_headers = await self.read_http_response()\n        if status_code in (301, 302, 303, 307, 308):\n            if \"Location\" not in response_headers:\n                raise InvalidMessage(\"Redirect response missing Location\")\n            raise RedirectHandshake(response_headers[\"Location\"])\n        elif status_code != 101:\n            raise InvalidStatusCode(status_code)\n\n        check_response(response_headers, key)\n\n        self.extensions = self.process_extensions(\n            response_headers, available_extensions\n        )\n\n        self.subprotocol = self.process_subprotocol(\n            response_headers, available_subprotocols\n        )\n\n        self.connection_open()", "response": "Perform the client side of the handshake."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_connected(\n        self, reader: asyncio.StreamReader, writer: asyncio.StreamWriter\n    ) -> None:\n        \"\"\"\n        Callback when the TCP connection is established.\n\n        Record references to the stream reader and the stream writer to avoid\n        using private attributes ``_stream_reader`` and ``_stream_writer`` of\n        :class:`~asyncio.StreamReaderProtocol`.\n\n        \"\"\"\n        self.reader = reader\n        self.writer = writer", "response": "Callback when the TCP connection is established."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn True if the connection is open False otherwise.", "response": "def open(self) -> bool:\n        \"\"\"\n        This property is ``True`` when the connection is usable.\n\n        It may be used to detect disconnections but this is discouraged per\n        the EAFP_ principle. When ``open`` is ``False``, using the connection\n        raises a :exc:`~websockets.exceptions.ConnectionClosed` exception.\n\n        .. _EAFP: https://docs.python.org/3/glossary.html#term-eafp\n\n        \"\"\"\n        return self.state is State.OPEN and not self.transfer_data_task.done()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def send(\n        self, message: Union[Data, Iterable[Data], AsyncIterable[Data]]\n    ) -> None:\n        \"\"\"\n        This coroutine sends a message.\n\n        It sends a string (:class:`str`) as a text frame and a bytes-like\n        object (:class:`bytes`, :class:`bytearray`, or :class:`memoryview`)\n        as a binary frame.\n\n        It also accepts an iterable or an asynchronous iterator of strings or\n        bytes-like objects. Each item is treated as a message fragment and\n        sent in its own frame. All items must be of the same type, or else\n        :meth:`send` will raise a :exc:`TypeError` and the connection will be\n        closed.\n\n        It raises a :exc:`TypeError` for other inputs.\n\n        \"\"\"\n        await self.ensure_open()\n\n        # Unfragmented message -- this case must be handled first because\n        # strings and bytes-like objects are iterable.\n\n        if isinstance(message, (str, bytes, bytearray, memoryview)):\n            opcode, data = prepare_data(message)\n            await self.write_frame(True, opcode, data)\n\n        # Fragmented message -- regular iterator.\n\n        elif isinstance(message, Iterable):\n\n            # Work around https://github.com/python/mypy/issues/6227\n            message = cast(Iterable[Data], message)\n\n            iter_message = iter(message)\n\n            # First fragment.\n            try:\n                message_chunk = next(iter_message)\n            except StopIteration:\n                return\n            opcode, data = prepare_data(message_chunk)\n            await self.write_frame(False, opcode, data)\n\n            # Other fragments.\n            for message_chunk in iter_message:\n                confirm_opcode, data = prepare_data(message_chunk)\n                if confirm_opcode != opcode:\n                    # We're half-way through a fragmented message and we can't\n                    # complete it. This makes the connection unusable.\n                    self.fail_connection(1011)\n                    raise TypeError(\"data contains inconsistent types\")\n                await self.write_frame(False, OP_CONT, data)\n\n            # Final fragment.\n            await self.write_frame(True, OP_CONT, b\"\")\n\n        # Fragmented message -- asynchronous iterator\n\n        elif isinstance(message, AsyncIterable):\n            # aiter_message = aiter(message) without aiter\n            aiter_message = type(message).__aiter__(message)\n\n            # First fragment.\n            try:\n                # message_chunk = anext(aiter_message) without anext\n                message_chunk = await type(aiter_message).__anext__(aiter_message)\n            except StopAsyncIteration:\n                return\n            opcode, data = prepare_data(message_chunk)\n            await self.write_frame(False, opcode, data)\n\n            # Other fragments.\n            async for message_chunk in aiter_message:\n                confirm_opcode, data = prepare_data(message_chunk)\n                if confirm_opcode != opcode:\n                    # We're half-way through a fragmented message and we can't\n                    # complete it. This makes the connection unusable.\n                    self.fail_connection(1011)\n                    raise TypeError(\"data contains inconsistent types\")\n                await self.write_frame(False, OP_CONT, data)\n\n            # Final fragment.\n            await self.write_frame(True, OP_CONT, b\"\")\n\n        else:\n            raise TypeError(\"data must be bytes, str, or iterable\")", "response": "This coroutine sends a message to the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def close(self, code: int = 1000, reason: str = \"\") -> None:\n        try:\n            await asyncio.wait_for(\n                self.write_close_frame(serialize_close(code, reason)),\n                self.close_timeout,\n                loop=self.loop,\n            )\n        except asyncio.TimeoutError:\n            # If the close frame cannot be sent because the send buffers\n            # are full, the closing handshake won't complete anyway.\n            # Fail the connection to shut down faster.\n            self.fail_connection()\n\n        # If no close frame is received within the timeout, wait_for() cancels\n        # the data transfer task and raises TimeoutError.\n\n        # If close() is called multiple times concurrently and one of these\n        # calls hits the timeout, the data transfer task will be cancelled.\n        # Other calls will receive a CancelledError here.\n\n        try:\n            # If close() is canceled during the wait, self.transfer_data_task\n            # is canceled before the timeout elapses.\n            # This helps closing connections when shutting down a server.\n            await asyncio.wait_for(\n                self.transfer_data_task, self.close_timeout, loop=self.loop\n            )\n        except (asyncio.TimeoutError, asyncio.CancelledError):\n            pass\n\n        # Wait for the close connection task to close the TCP connection.\n        await asyncio.shield(self.close_connection_task)", "response": "This coroutine sends a close frame to the other end and closes the connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_map_file(mapFNH, items, header):\n    if isinstance(header, list):\n        header = \"\\t\".join(header) + \"\\n\"\n\n    with file_handle(mapFNH, \"w\") as mapF:\n        mapF.write(header)\n        for row in items:\n            mapF.write(\"\\t\".join(row)+\"\\n\")", "response": "Writes the given list of items to the given file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns either the full or truncated version of a QIIME - formatted taxonomy string up to the classification givenbyparam level.", "response": "def split_phylogeny(p, level=\"s\"):\n    \"\"\"\n    Return either the full or truncated version of a QIIME-formatted taxonomy string.\n\n    :type p: str\n    :param p: A QIIME-formatted taxonomy string: k__Foo; p__Bar; ...\n\n    :type level: str\n    :param level: The different level of identification are kingdom (k), phylum (p),\n                  class (c),order (o), family (f), genus (g) and species (s). If level is\n                  not provided, the default level of identification is species.\n\n    :rtype: str\n    :return: A QIIME-formatted taxonomy string up to the classification given\n            by param level.\n    \"\"\"\n    level = level+\"__\"\n    result = p.split(level)\n    return result[0]+level+result[1].split(\";\")[0]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ensure_dir(d):\n    if not os.path.exists(d):\n        try:\n            os.makedirs(d)\n        except OSError as oe:\n            # should not happen with os.makedirs\n            # ENOENT: No such file or directory\n            if os.errno == errno.ENOENT:\n                msg = twdd(\"\"\"One or more directories in the path ({}) do not exist. If\n                           you are specifying a new directory for output, please ensure\n                           all other directories in the path currently exist.\"\"\")\n                return msg.format(d)\n            else:\n                msg = twdd(\"\"\"An error occurred trying to create the output directory\n                           ({}) with message: {}\"\"\")\n                return msg.format(d, oe.strerror)", "response": "Ensures that the supplied directory path does not exist. If so create it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file_handle(fnh, mode=\"rU\"):\n    handle = None\n    if isinstance(fnh, file):\n        if fnh.closed:\n            raise ValueError(\"Input file is closed.\")\n        handle = fnh\n    elif isinstance(fnh, str):\n        handle = open(fnh, mode)\n\n    return handle", "response": "Returns a file handle for appropriate usage."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef gather_categories(imap, header, categories=None):\n    # If no categories provided, return all SampleIDs\n    if categories is None:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    cat_ids = [header.index(cat)\n               for cat in categories if cat in header and \"=\" not in cat]\n\n    table = OrderedDict()\n    conditions = defaultdict(set)\n    for i, cat in enumerate(categories):\n        if \"=\" in cat and cat.split(\"=\")[0] in header:\n            cat_name = header[header.index(cat.split(\"=\")[0])]\n            conditions[cat_name].add(cat.split(\"=\")[1])\n\n    # If invalid categories or conditions identified, return all SampleIDs\n    if not cat_ids and not conditions:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n\n    #If only category column given, return column-wise SampleIDs\n    if cat_ids and not conditions:\n        for sid, row in imap.items():\n            cat_name = \"_\".join([row[cid] for cid in cat_ids])\n            if cat_name not in table:\n                table[cat_name] = DataCategory(set(), {})\n            table[cat_name].sids.add(sid)\n        return table\n\n    # Collect all condition names\n    cond_ids = set()\n    for k in conditions:\n        try:\n            cond_ids.add(header.index(k))\n        except ValueError:\n            continue\n    idx_to_test = set(cat_ids).union(cond_ids)\n\n    # If column name and condition given, return overlapping SampleIDs of column and\n    # condition combinations\n    for sid, row in imap.items():\n        if all([row[header.index(c)] in conditions[c] for c in conditions]):\n            key = \"_\".join([row[idx] for idx in idx_to_test])\n            try:\n                assert key in table.keys()\n            except AssertionError:\n                table[key] = DataCategory(set(), {})\n            table[key].sids.add(sid)\n    try:\n        assert len(table) > 0\n    except AssertionError:\n        return {\"default\": DataCategory(set(imap.keys()), {})}\n    else:\n        return table", "response": "Given a mapping file and a list of user - specified categories return a dictionary that contains the relevant data for each type within the user - specified categories."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the unifrac results file into a dictionary containing the keys pcd eigenvals and a values.", "response": "def parse_unifrac(unifracFN):\n    \"\"\"\n    Parses the unifrac results file into a dictionary\n\n    :type unifracFN: str\n    :param unifracFN: The path to the unifrac results file\n\n    :rtype: dict\n    :return: A dictionary with keys: 'pcd' (principle coordinates data) which is a\n             dictionary of the data keyed by sample ID, 'eigvals' (eigenvalues), and\n             'varexp' (variation explained)\n    \"\"\"\n    with open(unifracFN, \"rU\") as uF:\n        first = uF.next().split(\"\\t\")\n        lines = [line.strip() for line in uF]\n\n    unifrac = {\"pcd\": OrderedDict(), \"eigvals\": [], \"varexp\": []}\n    if first[0] == \"pc vector number\":\n        return parse_unifrac_v1_8(unifrac, lines)\n    elif first[0] == \"Eigvals\":\n        return parse_unifrac_v1_9(unifrac, lines)\n    else:\n        raise ValueError(\"File format not supported/recognized. Please check input \"\n                         \"unifrac file.\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_unifrac_v1_8(unifrac, file_data):\n    for line in file_data:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[-2].split(\"\\t\")[1:]]\n    unifrac[\"varexp\"] = [float(entry) for entry in file_data[-1].split(\"\\t\")[1:]]\n    return unifrac", "response": "Function to parse data from older version of unifrac file obtained from Qiime version\n    1. 8 and earlier."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_unifrac_v1_9(unifrac, file_data):\n    unifrac[\"eigvals\"] = [float(entry) for entry in file_data[0].split(\"\\t\")]\n    unifrac[\"varexp\"] = [float(entry)*100 for entry in file_data[3].split(\"\\t\")]\n\n    for line in file_data[8:]:\n        if line == \"\":\n            break\n        line = line.split(\"\\t\")\n        unifrac[\"pcd\"][line[0]] = [float(e) for e in line[1:]]\n    return unifrac", "response": "Function to parse data from newer version of unifrac file obtained from Qiime version\n    1. 9 and later."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines color - category mapping.", "response": "def color_mapping(sample_map, header, group_column, color_column=None):\n    \"\"\"\n    Determine color-category mapping. If color_column was specified, then map the category\n    names to color values. Otherwise, use the palettable colors to automatically generate\n    a set of colors for the group values.\n\n    :type sample_map: dict\n    :param unifracFN: Map associating each line of the mapping file with the appropriate\n                      sample ID (each value of the map also contains the sample ID)\n\n    :type header: tuple\n    :param A tuple of header line for mapping file\n\n    :type group_column: str\n    :param group_column: String denoting the column name for sample groups.\n\n    :type color_column: str\n    :param color_column: String denoting the column name for sample colors.\n\n    :type return: dict\n    :param return: {SampleID: Color}\n    \"\"\"\n    group_colors = OrderedDict()\n    group_gather = gather_categories(sample_map, header, [group_column])\n\n    if color_column is not None:\n        color_gather = gather_categories(sample_map, header, [color_column])\n        # match sample IDs between color_gather and group_gather\n        for group in group_gather:\n            for color in color_gather:\n                # allow incomplete assignment of colors, if group sids overlap at\n                # all with the color sids, consider it a match\n                if group_gather[group].sids.intersection(color_gather[color].sids):\n                    group_colors[group] = color\n    else:\n        bcolors = itertools.cycle(Set3_12.hex_colors)\n        for group in group_gather:\n            group_colors[group] = bcolors.next()\n\n    return group_colors"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _route(self, attr, args, kwargs, **fkwargs):\n\n        key = get_key(args, kwargs)\n\n        found = self._hash.get_node(key)\n\n        if not found and len(self._down_connections) > 0:\n            raise self.HostListExhausted()\n\n        return [i for i, h in self.cluster.hosts.iteritems()\n                if h.identifier == found]", "response": "Returns a list of hosts that have been routed to the given key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting distribution of normal distribution", "response": "def plot_dist_normal(s, mu, sigma):\n    \"\"\"\n    plot distribution\n    \"\"\"\n    import matplotlib.pyplot as plt\n    count, bins, ignored = plt.hist(s, 30, normed=True)\n    plt.plot(bins, 1/(sigma * np.sqrt(2 * np.pi)) \\\n            * np.exp( - (bins - mu)**2 / (2 * sigma**2) ), \\\n            linewidth = 2, color = 'r')\n    plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rev_c(read):\n    rc = []\n    rc_nucs = {'A':'T', 'T':'A', 'G':'C', 'C':'G', 'N':'N'}\n    for base in read:\n        rc.extend(rc_nucs[base.upper()])\n    return rc[::-1]", "response": "Returns the reverse completment of a read"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_analysis_no_cross_validation(self):\n\n        # first model is just the mean\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])] # empty term is the intercept\n        all_model_terms_dict = {x:Term([LookupFactor(x)]) for x in self.list_of_x}\n        # ...then add another term for each candidate\n        #model_terms += [Term([LookupFactor(c)]) for c in candidates]\n        model_desc = ModelDesc(response_term, model_terms)\n        self._list_of_fits.append(fm.ols(model_desc, data=self.df).fit())\n        # try to improve the model until no improvements can be found\n\n        while all_model_terms_dict:\n            # try each x and overwrite the best_fit if we find a better one\n            # the first best_fit is the one from the previous round\n            ref_fit = self._list_of_fits[-1]\n            best_fit = self._list_of_fits[-1]\n            best_bic = best_fit.bic\n            for x, term in all_model_terms_dict.items():\n                # make new_fit, compare with best found so far\n                model_desc = ModelDesc(response_term, ref_fit.model.formula.rhs_termlist + [term])\n                fit = fm.ols(model_desc, data=self.df).fit()\n                if fit.bic < best_bic:\n                    best_bic = fit.bic\n                    best_fit = fit\n                    best_x = x\n            # Sometimes, the obtained fit may be better, but contains unsignificant parameters.\n            # Correct the fit by removing the unsignificant parameters and estimate again\n            best_fit = self._prune(best_fit, p_max=self.p_max)\n\n            # if best_fit does not contain more variables than ref fit, exit\n            if len(best_fit.model.formula.rhs_termlist) == len(ref_fit.model.formula.rhs_termlist):\n                break\n            else:\n                self._list_of_fits.append(best_fit)\n                all_model_terms_dict.pop(best_x)\n        self._fit = self._list_of_fits[-1]", "response": "Find the best fit and create self. list_of_fits and self. fit\n           "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_analysis_cross_validation(self):\n        assert len(self.df) < 15, \"Cross-validation is not implemented if your sample contains more than 15 datapoints\"\n\n        # initialization: first model is the mean, but compute cv correctly.\n        errors = []\n        response_term = [Term([LookupFactor(self.y)])]\n        model_terms = [Term([])]  # empty term is the intercept\n        model_desc = ModelDesc(response_term, model_terms)\n        for i in self.df.index:\n            # make new_fit, compute cross-validation and store error\n            df_ = self.df.drop(i, axis=0)\n            fit = fm.ols(model_desc, data=df_).fit()\n            cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n            errors.append(cross_prediction['predicted'] - cross_prediction[self.y])\n\n        self._list_of_fits = [fm.ols(model_desc, data=self.df).fit()]\n        self.list_of_cverrors = [np.mean(np.abs(np.array(errors)))]\n\n        # try to improve the model until no improvements can be found\n        all_model_terms_dict = {x: Term([LookupFactor(x)]) for x in self.list_of_x}\n        while all_model_terms_dict:\n            # import pdb;pdb.set_trace()\n            # try each x in all_exog and overwrite if we find a better one\n            # at the end of iteration (and not earlier), save the best of the iteration\n            better_model_found = False\n            best = dict(fit=self._list_of_fits[-1], cverror=self.list_of_cverrors[-1])\n            for x, term in all_model_terms_dict.items():\n                model_desc = ModelDesc(response_term, self._list_of_fits[-1].model.formula.rhs_termlist + [term])\n                # cross_validation, currently only implemented for monthly data\n                # compute the mean error for a given formula based on leave-one-out.\n                errors = []\n                for i in self.df.index:\n                    # make new_fit, compute cross-validation and store error\n                    df_ = self.df.drop(i, axis=0)\n                    fit = fm.ols(model_desc, data=df_).fit()\n                    cross_prediction = self._predict(fit=fit, df=self.df.loc[[i], :])\n                    errors.append(cross_prediction['predicted'] - cross_prediction[self.y])\n                cverror = np.mean(np.abs(np.array(errors)))\n                # compare the model with the current fit\n                if cverror < best['cverror']:\n                    # better model, keep it\n                    # first, reidentify using all the datapoints\n                    best['fit'] = fm.ols(model_desc, data=self.df).fit()\n                    best['cverror'] = cverror\n                    better_model_found = True\n                    best_x = x\n\n            if better_model_found:\n                self._list_of_fits.append(best['fit'])\n                self.list_of_cverrors.append(best['cverror'])\n\n            else:\n                # if we did not find a better model, exit\n                break\n\n            # next iteration with the found exog removed\n            all_model_terms_dict.pop(best_x)\n\n        self._fit = self._list_of_fits[-1]", "response": "Perform cross - validation analysis on the sample."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves statistically insignificant parameters from a fit.", "response": "def _prune(self, fit, p_max):\n        \"\"\"\n        If the fit contains statistically insignificant parameters, remove them.\n        Returns a pruned fit where all parameters have p-values of the t-statistic below p_max\n\n        Parameters\n        ----------\n        fit: fm.ols fit object\n            Can contain insignificant parameters\n        p_max : float\n            Maximum allowed probability of the t-statistic\n\n        Returns\n        -------\n        fit: fm.ols fit object\n            Won't contain any insignificant parameters\n\n        \"\"\"\n\n        def remove_from_model_desc(x, model_desc):\n            \"\"\"\n            Return a model_desc without x\n            \"\"\"\n\n            rhs_termlist = []\n            for t in model_desc.rhs_termlist:\n                if not t.factors:\n                    # intercept, add anyway\n                    rhs_termlist.append(t)\n                elif not x == t.factors[0]._varname:\n                    # this is not the term with x\n                    rhs_termlist.append(t)\n\n            md = ModelDesc(model_desc.lhs_termlist, rhs_termlist)\n            return md\n\n        corrected_model_desc = ModelDesc(fit.model.formula.lhs_termlist[:], fit.model.formula.rhs_termlist[:])\n        pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n        try:\n            pars_to_prune.remove('Intercept')\n        except:\n            pass\n        while pars_to_prune:\n            corrected_model_desc = remove_from_model_desc(pars_to_prune[0], corrected_model_desc)\n            fit = fm.ols(corrected_model_desc, data=self.df).fit()\n            pars_to_prune = fit.pvalues.where(fit.pvalues > p_max).dropna().index.tolist()\n            try:\n                pars_to_prune.remove('Intercept')\n            except:\n                pass\n        return fit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_best_rsquared(list_of_fits):\n        res = sorted(list_of_fits, key=lambda x: x.rsquared)\n        return res[-1]", "response": "Return the best fit based on rsquared"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the best fit based on Akaike information criterion", "response": "def find_best_akaike(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.aic)\n        return res[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the best fit based on Akaike information criterion", "response": "def find_best_bic(list_of_fits):\n        \"\"\"Return the best fit, based on Akaike information criterion\"\"\"\n        res = sorted(list_of_fits, key=lambda x: x.bic)\n        return res[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _predict(self, fit, df):\n\n        # Add model results to data as column 'predictions'\n        df_res = df.copy()\n        if 'Intercept' in fit.model.exog_names:\n            df_res['Intercept'] = 1.0\n        df_res['predicted'] = fit.predict(df_res)\n        if not self.allow_negative_predictions:\n            df_res.loc[df_res['predicted'] < 0, 'predicted'] = 0\n\n        prstd, interval_l, interval_u = wls_prediction_std(fit,\n                                                           df_res[fit.model.exog_names],\n                                                           alpha=1 - self.confint)\n        df_res['interval_l'] = interval_l\n        df_res['interval_u'] = interval_u\n\n        if 'Intercept' in df_res:\n            df_res.drop(labels=['Intercept'], axis=1, inplace=True)\n\n        return df_res", "response": "Predicts the model and returns a dataframe with predictions and confidence intervals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd predictions and confidence interval to self. df", "response": "def add_prediction(self):\n        \"\"\"\n        Add predictions and confidence interval to self.df\n        self.df will contain the following columns:\n        - 'predicted': the model output\n        - 'interval_u', 'interval_l': upper and lower confidence bounds.\n\n        Parameters\n        ----------\n        None, but the result depends on the following attributes of self:\n        confint : float (default=0.95)\n            Confidence level for two-sided hypothesis\n        allow_negative_predictions : bool (default=True)\n            If False, correct negative predictions to zero (typically for energy consumption predictions)\n\n        Returns\n        -------\n        Nothing, adds columns to self.df\n        \"\"\"\n        self.df = self._predict(fit=self.fit, df=self.df)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot(self, model=True, bar_chart=True, **kwargs):\n        plot_style()\n        figures = []\n        fit = kwargs.get('fit', self.fit)\n        df = kwargs.get('df', self.df)\n\n        if not 'predicted' in df.columns:\n            df = self._predict(fit=fit, df=df)\n        # split the df in the auto-validation and prognosis part\n        df_auto = df.loc[self.df.index[0]:self.df.index[-1]]\n        if df_auto.empty:\n            df_prog = df\n        else:\n            df_prog = df.loc[df_auto.index[-1]:].iloc[1:]\n\n        if model:\n            # The first variable in the formula is the most significant.  Use it as abcis for the plot\n            try:\n                exog1 = fit.model.exog_names[1]\n            except IndexError:\n                exog1 = self.list_of_x[0]\n\n            # plot model as an adjusted trendline\n            # get sorted model values\n            dfmodel = df[[exog1, 'predicted', 'interval_u', 'interval_l']]\n            dfmodel.index = dfmodel[exog1]\n            dfmodel = dfmodel.sort_index()\n            plt.plot(dfmodel.index, dfmodel['predicted'], '--', color='royalblue')\n            plt.plot(dfmodel.index, dfmodel['interval_l'], ':', color='royalblue')\n            plt.plot(dfmodel.index, dfmodel['interval_u'], ':', color='royalblue')\n            # plot dots for the measurements\n            if len(df_auto) > 0:\n                plt.plot(df_auto[exog1], df_auto[self.y], 'o', mfc='orangered', mec='orangered', ms=8,\n                         label='Data used for model fitting')\n            if len(df_prog) > 0:\n                plt.plot(df_prog[exog1], df_prog[self.y], 'o', mfc='seagreen', mec='seagreen', ms=8,\n                         label='Data not used for model fitting')\n            plt.title('rsquared={:.2f} - BIC={:.1f}'.format(fit.rsquared, fit.bic))\n            plt.xlabel(exog1)\n            figures.append(plt.gcf())\n\n        if bar_chart:\n            ind = np.arange(len(df.index))  # the x locations for the groups\n            width = 0.35  # the width of the bars\n\n            fig, ax = plt.subplots()\n            title = 'Measured'  # will be appended based on the available data\n            if len(df_auto) > 0:\n                model = ax.bar(ind[:len(df_auto)], df_auto['predicted'], width * 2, color='#FDD787', ecolor='#FDD787',\n                               yerr=df_auto['interval_u'] - df_auto['predicted'], label=self.y + ' modelled')\n                title = title + ', modelled'\n            if len(df_prog) > 0:\n                prog = ax.bar(ind[len(df_auto):], df_prog['predicted'], width * 2, color='#6CD5A1', ecolor='#6CD5A1',\n                              yerr=df_prog['interval_u'] - df_prog['predicted'], label=self.y + ' expected')\n                title = title + ' and predicted'\n\n            meas = ax.bar(ind, df[self.y], width, label=self.y + ' measured', color='#D5756C')\n            # add some text for labels, title and axes ticks\n            ax.set_title('{} {}'.format(title, self.y))\n            ax.set_xticks(ind)\n            ax.set_xticklabels([x.strftime('%d-%m-%Y') for x in df.index], rotation='vertical')\n            ax.yaxis.grid(True)\n            ax.xaxis.grid(False)\n\n            plt.legend(ncol=3, loc='upper center')\n            figures.append(plt.gcf())\n\n        plt.show()\n\n        return figures", "response": "Plot the modified energy signature of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _modeldesc_to_dict(self, md):\n        d = {'lhs_termlist': [md.lhs_termlist[0].factors[0].name()]}\n        rhs_termlist = []\n\n        # add other terms, if any\n        for term in md.rhs_termlist[:]:\n            if len(term.factors) == 0:\n                # intercept, represent by empty string\n                rhs_termlist.append('')\n            else:\n                rhs_termlist.append(term.factors[0].name())\n\n        d['rhs_termlist'] = rhs_termlist\n        return d", "response": "Return a string representation of a patsy ModelDesc object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string representation of a patsy ModelDesc object", "response": "def _modeldesc_from_dict(self, d):\n        \"\"\"Return a string representation of a patsy ModelDesc object\"\"\"\n        lhs_termlist = [Term([LookupFactor(d['lhs_termlist'][0])])]\n        rhs_termlist = []\n        for name in d['rhs_termlist']:\n            if name == '':\n                rhs_termlist.append(Term([]))\n            else:\n                rhs_termlist.append(Term([LookupFactor(name)]))\n\n        md = ModelDesc(lhs_termlist, rhs_termlist)\n        return md"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relative_abundance(biomf, sampleIDs=None):\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating relative abundances: The sampleIDs provided do\"\n                \" not match the sampleIDs in biom file. Please double check the sampleIDs\"\n                \" provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n    norm_biomf = biomf.norm(inplace=False)\n\n    return {sample: {otuID: norm_biomf.get_value_by_ids(otuID, sample)\n                     for otuID in otuIDs} for sample in sampleIDs}", "response": "Calculate the relative abundance of each OTUID in a SampleID in a BIOM file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mean_otu_pct_abundance(ra, otuIDs):\n    sids = ra.keys()\n    otumeans = defaultdict(int)\n\n    for oid in otuIDs:\n        otumeans[oid] = sum([ra[sid][oid] for sid in sids\n                             if oid in ra[sid]]) / len(sids) * 100\n    return otumeans", "response": "Calculate the mean OTU abundance percentage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the mean relative abundance percentage for a given set of OTUIDs.", "response": "def MRA(biomf, sampleIDs=None, transform=None):\n    \"\"\"\n    Calculate the mean relative abundance percentage.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :type sampleIDs: list\n    :param sampleIDs: A list of sample id's from BIOM format OTU table.\n\n    :param transform: Mathematical function which is used to transform smax to another\n                      format. By default, the function has been set to None.\n\n    :rtype: dict\n    :return: A dictionary keyed on OTUID's and their mean relative abundance for a given\n             number of sampleIDs.\n    \"\"\"\n    ra = relative_abundance(biomf, sampleIDs)\n    if transform is not None:\n        ra = {sample: {otuID: transform(abd) for otuID, abd in ra[sample].items()}\n              for sample in ra.keys()}\n    otuIDs = biomf.ids(axis=\"observation\")\n    return mean_otu_pct_abundance(ra, otuIDs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef raw_abundance(biomf, sampleIDs=None, sample_abd=True):\n    results = defaultdict(int)\n    if sampleIDs is None:\n        sampleIDs = biomf.ids()\n    else:\n        try:\n            for sid in sampleIDs:\n                assert sid in biomf.ids()\n        except AssertionError:\n            raise ValueError(\n                \"\\nError while calculating raw total abundances: The sampleIDs provided \"\n                \"do not match the sampleIDs in biom file. Please double check the \"\n                \"sampleIDs provided.\\n\")\n    otuIDs = biomf.ids(axis=\"observation\")\n\n    for sampleID in sampleIDs:\n        for otuID in otuIDs:\n            abd = biomf.get_value_by_ids(otuID, sampleID)\n            if sample_abd:\n                results[sampleID] += abd\n            else:\n                results[otuID] += abd\n    return results", "response": "Calculate the total number of sequences in each OTU or SampleID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions to transform the total abundance calculation for each sample ID to another format based on user given transformation function. :type biomf: A BIOM file. :param biomf: OTU table format. :param fn: Mathematical function which is used to transform smax to another format. By default, the function has been given as base 10 logarithm. :rtype: dict :return: Returns a dictionary similar to output of raw_abundance function but with the abundance values modified by the mathematical operation. By default, the operation performed on the abundances is base 10 logarithm.", "response": "def transform_raw_abundance(biomf, fn=math.log10, sampleIDs=None, sample_abd=True):\n    \"\"\"\n    Function to transform the total abundance calculation for each sample ID to another\n    format based on user given transformation function.\n\n    :type biomf: A BIOM file.\n    :param biomf: OTU table format.\n\n    :param fn: Mathematical function which is used to transform smax to another format.\n               By default, the function has been given as base 10 logarithm.\n\n    :rtype: dict\n    :return: Returns a dictionary similar to output of raw_abundance function but with\n             the abundance values modified by the mathematical operation. By default, the\n             operation performed on the abundances is base 10 logarithm.\n    \"\"\"\n    totals = raw_abundance(biomf, sampleIDs, sample_abd)\n    return {sid: fn(abd) for sid, abd in totals.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef arcsine_sqrt_transform(rel_abd):\n    arcsint = lambda p: math.asin(math.sqrt(p))\n    return {col_id: {row_id: arcsint(rel_abd[col_id][row_id])\n                     for row_id in rel_abd[col_id]} for col_id in rel_abd}", "response": "Takes the proportion data from relative_abundance and applies the arcsine square root transformation"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef f2lookup(f, lookup):\n    lookup = {i: r for i, r in [l.strip().split('\\t')[0:2] for l in lookup]}\n    for line in f:\n        line = line.strip().split()\n        for i, w in enumerate(line):\n            if w in lookup:\n                line[i] = lookup[w]\n        yield ' '.join(line)", "response": "yields a list of strings from a file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef print_MannWhitneyU(div_calc):\n    try:\n        x = div_calc.values()[0].values()\n        y = div_calc.values()[1].values()\n    except:\n        return \"Error setting up input arrays for Mann-Whitney U Test. Skipping \"\\\n               \"significance testing.\"\n    T, p = stats.mannwhitneyu(x, y)\n    print \"\\nMann-Whitney U test statistic:\", T\n    print \"Two-tailed p-value: {}\".format(2 * p)", "response": "Print the Mann - Whitney U test."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the Kruskal - Wallis H - test for independent samples.", "response": "def print_KruskalWallisH(div_calc):\n    \"\"\"\n    Compute the Kruskal-Wallis H-test for independent samples. A typical rule is that\n    each group must have at least 5 measurements.\n    \"\"\"\n    calc = defaultdict(list)\n    try:\n        for k1, v1 in div_calc.iteritems():\n            for k2, v2 in v1.iteritems():\n                calc[k1].append(v2)\n    except:\n        return \"Error setting up input arrays for Kruskal-Wallis H-Test. Skipping \"\\\n               \"significance testing.\"\n    h, p = stats.kruskal(*calc.values())\n    print \"\\nKruskal-Wallis H-test statistic for {} groups: {}\".format(str(len(div_calc)), h)\n    print \"p-value: {}\".format(p)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite out the diversity metrics for a single site.", "response": "def write_diversity_metrics(data, sample_ids, fp=None):\n    \"\"\"\n    Given a dictionary of diversity calculations (keyed by method)\n    write out the data to a file.\n    \"\"\"\n    if fp is None:\n        fp = \"./diversity_data.txt\"\n\n    with open(fp, \"w\") as outf:\n        out = csv.writer(outf, delimiter=\"\\t\")\n        out.writerow([\"SampleID\", \"Group\", \"Calculation\"])\n        for group, d in data.iteritems():\n            for sid, value in d.iteritems():\n                out.writerow([sid, group, value])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_program_options():\n    parser = argparse.ArgumentParser(description=\"Calculate the alpha diversity\\\n                                     of a set of samples using one or more \\\n                                     metrics and output a kernal density \\\n                                     estimator-smoothed histogram of the \\\n                                     results.\")\n    parser.add_argument(\"-m\", \"--map_file\",\n                        help=\"QIIME mapping file.\")\n    parser.add_argument(\"-i\", \"--biom_fp\",\n                        help=\"Path to the BIOM table\")\n    parser.add_argument(\"-c\", \"--category\",\n                        help=\"Specific category from the mapping file.\")\n    parser.add_argument(\"-d\", \"--diversity\", default=[\"shannon\"], nargs=\"+\",\n                        help=\"The alpha diversity metric. Default \\\n                             value is 'shannon', which will calculate the Shannon\\\n                             entropy. Multiple metrics can be specified (space separated).\\\n                             The full list of metrics is available at:\\\n                             http://scikit-bio.org/docs/latest/generated/skbio.diversity.alpha.html.\\\n                             Beta diversity metrics will be supported in the future.\")\n    parser.add_argument(\"--x_label\", default=[None], nargs=\"+\",\n                        help=\"The name of the diversity metric to be displayed on the\\\n                        plot as the X-axis label. If multiple metrics are specified,\\\n                        then multiple entries for the X-axis label should be given.\")\n    parser.add_argument(\"--color_by\",\n                        help=\"A column name in the mapping file containing\\\n                              hexadecimal (#FF0000) color values that will\\\n                              be used to color the groups. Each sample ID must\\\n                              have a color entry.\")\n    parser.add_argument(\"--plot_title\", default=\"\",\n                        help=\"A descriptive title that will appear at the top \\\n                        of the output plot. Surround with quotes if there are\\\n                        spaces in the title.\")\n    parser.add_argument(\"-o\", \"--output_dir\", default=\".\",\n                        help=\"The directory plots will be saved to.\")\n    parser.add_argument(\"--image_type\", default=\"png\",\n                        help=\"The type of image to save: png, svg, pdf, eps, etc...\")\n    parser.add_argument(\"--save_calculations\",\n                        help=\"Path and name of text file to store the calculated \"\n                        \"diversity metrics.\")\n    parser.add_argument(\"--suppress_stats\", action=\"store_true\", help=\"Do not display \"\n                        \"significance testing results which are shown by default.\")\n    parser.add_argument(\"--show_available_metrics\", action=\"store_true\",\n                        help=\"Supply this parameter to see which alpha diversity metrics \"\n                             \" are available for usage. No calculations will be performed\"\n                             \" if this parameter is provided.\")\n    return parser.parse_args()", "response": "Parses the command line options passed in at the command line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef blastdb(fasta, maxfile = 10000000):\n    db = fasta.rsplit('.', 1)[0]\n    type = check_type(fasta)\n    if type == 'nucl':\n        type = ['nhr', type]\n    else:\n        type = ['phr', type]\n    if os.path.exists('%s.%s' % (db, type[0])) is False \\\n            and os.path.exists('%s.00.%s' % (db, type[0])) is False:\n        print('# ... making blastdb for: %s' % (fasta), file=sys.stderr)\n        os.system('makeblastdb \\\n                -in %s -out %s -dbtype %s -max_file_sz %s >> log.txt' \\\n                % (fasta, db, type[1], maxfile))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db", "response": "make blast db for a single sequence"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake usearch db for a single fasta file", "response": "def usearchdb(fasta, alignment = 'local', usearch_loc = 'usearch'):\n    \"\"\"\n    make usearch db\n    \"\"\"\n    if '.udb' in fasta:\n        print('# ... database found: %s' % (fasta), file=sys.stderr)\n        return fasta\n    type = check_type(fasta)\n    db = '%s.%s.udb' % (fasta.rsplit('.', 1)[0], type)\n    if os.path.exists(db) is False:\n        print('# ... making usearch db for: %s' % (fasta), file=sys.stderr)\n        if alignment == 'local':\n            os.system('%s -makeudb_ublast %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n        elif alignment == 'global':\n            os.system('%s -makeudb_usearch %s -output %s >> log.txt' % (usearch_loc, fasta, db))\n    else:\n        print('# ... database found for: %s' % (fasta), file=sys.stderr)\n    return db"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef phmmer(query, db, type, out, threads = '4', evalue = '0.01'):\n    if os.path.exists(out) is False:\n        print('# ... running phmmer with %s as query and %s as database' % (query, db))\n        os.system('phmmer -o %s.ph1 --tblout %s.ph2 --acc --noali --notextw -E %s --cpu %s %s %s' % (out, out, evalue, threads, query, db))\n    else:\n        print('# ... phmmer output found for %s as query and %s as database' % (query, db))\n    phmmer2blast('%s.ph2' % out, out)", "response": "run phmmer on the sequence of objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nruns blast on the sequence list", "response": "def blast(query, db, type, out, threads = '4', maxtargets = '100', megablast = False):\n    \"\"\"\n    run blast\n    \"\"\"\n    if os.path.exists(out) is False:\n        db = blastdb(db) # make the database file, if necessary \n        print('# ... running blast with %s as query and %s as database' % (query, db))\n        if type == 'nucl':\n            blast = 'blastn'\n            if megablast == True:\n                blast = 'blastn -task megablast'\n        else:\n            blast = 'blastp'\n        os.system('%s \\\n                -query %s -db %s -out %s -outfmt 6 \\\n                -max_target_seqs %s -num_threads %s >> log.txt' \\\n                % (blast, query, db, out, maxtargets, threads))\n    else:\n        print('# ... blast output found for %s as query and %s as database' % (query, db))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef usearch5(query, db, type, out, threads = '4', evalue = '100', alignment = 'local'):\n    if os.path.exists(out) is False:\n        print('# ... running usearch with %s as query and %s as database' % (query, db))\n        if type[1] == 'nucl':\n            threads = ''\n        else:\n            threads = '-threads %s' % (threads)\n        os.system('usearch \\\n                -query %s -%s %s -blast6out %s \\\n                -evalue %s %s -%s >> log.txt' \\\n                % (query, type[0], db, out, evalue, threads, alignment))\n    else:\n        print('# ... usearch output found for %s as query and %s as database' % (query, db))", "response": "run usearch with query and database and output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun usearch with query db and alignment", "response": "def usearch(query, db, type, out, threads = '6', evalue = '100', alignment = 'local', max_hits = 100, cluster = False):\n    \"\"\"\n    run usearch\n    \"\"\"\n    if 'usearch64' in os.environ:\n        usearch_loc = os.environ['usearch64']\n    else:\n        usearch_loc = 'usearch'\n    if os.path.exists(out) is False:\n        db = usearchdb(db, alignment, usearch_loc) # make the database file, if neceesary\n        print('# ... running usearch with %s as query and %s as database' % (query, db), file=sys.stderr)\n        if type == 'nucl':\n            strand = '-strand both'\n        else:\n            strand = ''\n        if alignment == 'local' and cluster is False:\n            os.system('%s \\\n                    -ublast %s -db %s -blast6out %s \\\n                    -evalue %s -threads %s %s -maxhits %s >> log.txt' \\\n                    % (usearch_loc, query, db, out, evalue, threads, strand, max_hits))\n        elif alignment == 'global' and cluster is False:\n            os.system('%s \\\n                    -usearch_global %s -db %s -blast6out %s \\\n                    -id 0.10 -threads %s %s >> log.txt' \\\n                    % (usearch_loc, query, db, out, threads, strand))\n        elif alignment == 'local' and cluster is True:\n            qsub = 'qsub -V -N usearch'\n            os.system('echo \"%s -ublast `pwd`/%s -db %s -blast6out `pwd`/%s -evalue %s -threads %s %s -maxhits %s >> `pwd`/log.txt\" | %s' \\\n                    % (usearch_loc, query, db, out, evalue, threads, strand, max_hits, qsub))\n        else:\n            print('specify local or global alignment', file=sys.stderr)\n            exit()\n    else:\n        print('# ... usearch output found for %s as query and %s as database' % (query, db), file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        if self.callback:\n            log.info((\"{} - using callback={}\")\n                     .format(self.name,\n                             self.callback))\n            self.callback(name=self.response_name,\n                          task_queue=self.task_queue,\n                          result_queue=self.result_queue,\n                          shutdown_msg=self.shutdown_msg)\n        else:\n\n            log.info(\"did not find a callback method \"\n                     \"using - using default handler\")\n\n            proc_name = self.name\n            while True:\n                next_task = self.task_queue.get()\n                if next_task:\n                    if str(next_task) == self.shutdown_msg:\n                        # Poison pill means shutdown\n                        log.info((\"{}: Exiting msg={}\")\n                                 .format(self.name,\n                                         next_task))\n                        self.task_queue.task_done()\n                        break\n                log.info((\"Consumer: {} {}\")\n                         .format(proc_name, next_task))\n                self.task_queue.task_done()\n\n                if self.need_response:\n                    answer = \"processed: {}\".format(next_task())\n                    self.result_queue.put(answer)\n        # end of if custome callback handler or not\n\n        return", "response": "run - run the consumer"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint licenses. :param argparse.Namespace params: parameter :param bootstrap_py.classifier.Classifiers metadata: package metadata", "response": "def print_licences(params, metadata):\n    \"\"\"Print licenses.\n\n    :param argparse.Namespace params: parameter\n    :param bootstrap_py.classifier.Classifiers metadata: package metadata\n    \"\"\"\n    if hasattr(params, 'licenses'):\n        if params.licenses:\n            _pp(metadata.licenses_desc())\n        sys.exit(0)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_repository_existence(params):\n    repodir = os.path.join(params.outdir, params.name)\n    if os.path.isdir(repodir):\n        raise Conflict(\n            'Package repository \"{0}\" has already exists.'.format(repodir))", "response": "Check repository existence.\n\n    :param argparse.Namespace params: parameters"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating package repository. :param argparse.Namespace params: parameters", "response": "def generate_package(params):\n    \"\"\"Generate package repository.\n\n    :param argparse.Namespace params: parameters\n    \"\"\"\n    pkg_data = package.PackageData(params)\n    pkg_tree = package.PackageTree(pkg_data)\n    pkg_tree.generate()\n    pkg_tree.move()\n    VCS(os.path.join(pkg_tree.outdir, pkg_tree.name), pkg_tree.pkg_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint single reads to stderr", "response": "def print_single(line, rev):\n    \"\"\"\n    print single reads to stderr\n    \"\"\"\n    if rev is True:\n        seq = rc(['', line[9]])[1]\n        qual = line[10][::-1]\n    else:\n        seq = line[9]\n        qual = line[10]\n    fq = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n    print('\\n'.join(fq), file = sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sam2fastq(sam, singles = False, force = False):\n    L, R = None, None\n    for line in sam:\n        if line.startswith('@') is True:\n            continue\n        line = line.strip().split()\n        bit = [True if i == '1' else False \\\n                for i in bin(int(line[1])).split('b')[1][::-1]]\n        while len(bit) < 8:\n            bit.append(False)\n        pair, proper, na, nap, rev, mrev, left, right = bit\n        # make sure read is paired\n        if pair is False:\n            if singles is True:\n                print_single(line, rev)\n            continue\n        # check if sequence is reverse-complemented\n        if rev is True:\n            seq = rc(['', line[9]])[1]\n            qual = line[10][::-1]\n        else:\n            seq = line[9]\n            qual = line[10]\n        # check if read is forward or reverse, return when both have been found\n        if left is True:\n            if L is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if L is not None:\n                L = None\n                continue\n            L = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if R is not None:\n                yield L\n                yield R\n                L, R = None, None\n        if right is True:\n            if R is not None and force is False:\n                print('sam file is not sorted', file = sys.stderr)\n                print('\\te.g.: %s' % (line[0]), file = sys.stderr)\n                exit()\n            if R is not None:\n                R = None\n                continue\n            R = ['@%s' % line[0], seq, '+%s' % line[0], qual]\n            if L is not None:\n                yield L\n                yield R\n                L, R = None, None", "response": "convert sam to fastq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fq2fa(fq):\n    c = cycle([1, 2, 3, 4])\n    for line in fq:\n        n = next(c)\n        if n == 1:\n            seq = ['>%s' % (line.strip().split('@', 1)[1])]\n        if n == 2:\n            seq.append(line.strip())\n            yield seq", "response": "converts a fq to a fa\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields lines from file f with lookup", "response": "def f2lookup(f, lookup):\n\t\"\"\"\n\tfind and replace elements in lookup within file\n\t\"\"\"\n\tlookup = {i: r for i, r in [l.strip().split('\\t')[0:2] for l in lookup]}\n\tfor line in f:\n\t\tline = line.strip()\n\t\tfor find, replace in list(lookup.items()):\n\t\t\tline = line.replace(find, replace)\n\t\tyield line"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_return_type(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        if kwargs.has_key('return_type'):\n            return_type = kwargs['return_type']\n            kwargs.pop('return_type')\n            return return_type(f(*args, **kwargs))\n        elif len(args) > 0:\n            return_type = type(args[0])\n            return return_type(f(*args, **kwargs))\n        else:\n            return f(*args, **kwargs)\n    return wrapper", "response": "A decorator that converts the returned value of wrapped function to the type of the\n virtual item."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting all args to 'set' type via self.setify function.", "response": "def convert_args_to_sets(f):\n    \"\"\"\n    Converts all args to 'set' type via self.setify function.\n    \"\"\"\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        args = (setify(x) for x in args)\n        return f(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef accessible(f):\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        print 'KWARGS= ', kwargs\n        setattr(args[0], kwargs['name'], args[1])\n        print \"property: \", str(getattr(args[0], kwargs['name']))\n        return f(*args, **kwargs)\n    return wrapper", "response": "Decorator to make class properties accessible to self."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenize(args):\n    if args.profile and not Path(args.profile).exists():  # pragma: no cover\n        raise ParserError('--profile must be a path for an existing file')\n    _write(args, Tokenizer(profile=args.profile)(_read(args), column=args.mapping))", "response": "Tokenize a string or read from stdin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_kata_dasar(self, dasar):\n\n        for tiap in dasar:\n            kata = tiap.find('a')\n            dasar_no = kata.find('sup')\n            kata = ambil_teks_dalam_label(kata)\n            self.kata_dasar.append(\n                kata + ' [{}]'.format(dasar_no.text.strip()) if dasar_no else kata\n            )", "response": "Memproses kata dasar yang ada dalam nama entri."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef serialisasi(self):\n\n        return {\n            \"nama\": self.nama,\n            \"nomor\": self.nomor,\n            \"kata_dasar\": self.kata_dasar,\n            \"pelafalan\": self.pelafalan,\n            \"bentuk_tidak_baku\": self.bentuk_tidak_baku,\n            \"varian\": self.varian,\n            \"makna\": [makna.serialisasi() for makna in self.makna]\n        }", "response": "Mengembalikan hasil serialisasi objek Entri ini."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _makna(self):\n\n        if len(self.makna) > 1:\n            return '\\n'.join(\n                str(i) + \". \" + str(makna)\n                for i, makna in enumerate(self.makna, 1)\n            )\n        return str(self.makna[0])", "response": "Mengembalikan representasi untuk semua makna entri ini."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _varian(self, varian):\n\n        if varian == self.bentuk_tidak_baku:\n            nama = \"Bentuk tidak baku\"\n        elif varian == self.varian:\n            nama = \"Varian\"\n        else:\n            return ''\n        return nama + ': ' + ', '.join(varian)", "response": "Mengembalikan representasi string untuk varian entri ini.\n        Dapat digunakan untuk Varian maupun Bentuk tidak baku."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _init_kelas(self, makna_label):\n\n        kelas = makna_label.find(color='red')\n        lain = makna_label.find(color='darkgreen')\n        info = makna_label.find(color='green')\n        if kelas:\n            kelas = kelas.find_all('span')\n        if lain:\n            self.kelas = {lain.text.strip(): lain['title'].strip()}\n            self.submakna = lain.next_sibling.strip()\n            self.submakna += ' ' + makna_label.find(color='grey').text.strip()\n        else:\n            self.kelas = {\n                k.text.strip(): k['title'].strip() for k in kelas\n            } if kelas else {}\n        self.info = info.text.strip() if info else ''", "response": "Memproses kelas kata yang ada dalam makna."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _init_contoh(self, makna_label):\n\n        indeks = makna_label.text.find(': ')\n        if indeks != -1:\n            contoh = makna_label.text[indeks + 2:].strip()\n            self.contoh = contoh.split('; ')\n        else:\n            self.contoh = []", "response": "Initializes the contoh attribute of the object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a TCP message to the node.", "response": "def send_tcp_message():\n    \"\"\"send_tcp_message\n\n    Send a ``TCP`` message to port 80 by default.\n\n    \"\"\"\n    need_response = os.getenv(\"NEED_RESPONSE\", \"0\") == \"1\"\n\n    msg = os.getenv(\n            \"MSG\",\n            \"testing msg time={} - {}\".format(\n                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                uuid.uuid4()))\n    host = \"127.0.0.1\"\n    port = 80\n\n    client = socket.socket()\n    client.connect((host, port))\n\n    print((\"sending: {}\")\n          .format(msg))\n    client.send(msg.encode())\n    if need_response:\n        data = client.recv(1024).decode()\n        print(data)\n    client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding sphinx documentation. :rtype: int :return: subprocess.call return code :param `bootstrap_py.control.PackageData` pkg_data: package meta data :param str projectdir: project root directory", "response": "def build_sphinx(pkg_data, projectdir):\n    \"\"\"Build sphinx documentation.\n\n    :rtype: int\n    :return: subprocess.call return code\n\n    :param `bootstrap_py.control.PackageData` pkg_data: package meta data\n    :param str projectdir: project root directory\n    \"\"\"\n    try:\n        version, _minor_version = pkg_data.version.rsplit('.', 1)\n    except ValueError:\n        version = pkg_data.version\n    args = ' '.join(('sphinx-quickstart',\n                     '--sep',\n                     '-q',\n                     '-p \"{name}\"',\n                     '-a \"{author}\"',\n                     '-v \"{version}\"',\n                     '-r \"{release}\"',\n                     '-l en',\n                     '--suffix=.rst',\n                     '--master=index',\n                     '--ext-autodoc',\n                     '--ext-viewcode',\n                     '--makefile',\n                     '{projectdir}')).format(name=pkg_data.name,\n                                             author=pkg_data.author,\n                                             version=version,\n                                             release=pkg_data.version,\n                                             projectdir=projectdir)\n    if subprocess.call(shlex.split(args)) == 0:\n        _touch_gitkeep(projectdir)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bowtiedb(fa, keepDB):\n    btdir = '%s/bt2' % (os.getcwd())\n    # make directory for\n    if not os.path.exists(btdir):\n        os.mkdir(btdir)\n    btdb = '%s/%s' % (btdir, fa.rsplit('/', 1)[-1])\n    if keepDB is True:\n        if os.path.exists('%s.1.bt2' % (btdb)):\n            return btdb\n    p = subprocess.Popen('bowtie2-build -q %s %s' \\\n        % (fa, btdb), shell = True)\n    p.communicate()\n    return btdb", "response": "make bowtie db\n SARL"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef crossmap(fas, reads, options, no_shrink, keepDB, threads, cluster, nodes):\n    if cluster is True:\n        threads = '48'\n    btc = []\n    for fa in fas:\n        btd = bowtiedb(fa, keepDB)\n        F, R, U = reads\n        if F is not False:\n            if U is False:\n                u = False\n            for i, f in enumerate(F):\n                r = R[i]\n                if U is not False:\n                    u = U[i]\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], f.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n        else:\n            f = False\n            r = False\n            for u in U:\n                sam = '%s/%s-vs-%s' % (os.getcwd(), \\\n                        fa.rsplit('/', 1)[-1], u.rsplit('/', 1)[-1].rsplit('.', 3)[0])\n                btc.append(bowtie(sam, btd, f, r, u, options, no_shrink, threads))\n    if cluster is False:\n        for i in btc:\n            p = subprocess.Popen(i, shell = True)\n            p.communicate()\n    else:\n        ID = ''.join(random.choice([str(i) for i in range(0, 9)]) for _ in range(5))\n        for node, commands in enumerate(chunks(btc, nodes), 1):\n            bs = open('%s/crossmap-qsub.%s.%s.sh' % (os.getcwd(), ID, node), 'w')\n            print('\\n'.join(commands), file=bs)\n            bs.close()\n            p = subprocess.Popen(\\\n                    'qsub -V -N crossmap %s' \\\n                        % (bs.name), \\\n                    shell = True)\n            p.communicate()", "response": "map all read sets against all fasta files"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_conn(self, *args, **kwargs):\n        connections = self.__connections_for('get_conn', args=args, kwargs=kwargs)\n\n        if len(connections) is 1:\n            return connections[0]\n        else:\n            return connections", "response": "Returns a connection object from the router given args."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmasks the sequence by removing gaps between model alignments", "response": "def mask_sequence(seq, gaps):\n    \"\"\"\n    mask (make lower case) regions of sequence found in gaps between model alignments\n    \"\"\"\n    seq = [i.upper() for i in seq]\n    for gap in gaps:\n        for i in range(gap[0] - 1, gap[1]):\n            seq[i] = seq[i].lower()\n    return ''.join(seq)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_23S(fastas, hmms, bit_thresh = float(20), length_thresh = 500, masking = True, buffer = 0):\n    # identify start/stop positions\n    # group2hmm[seq][group] = [model, strand, coordinates, matches, gaps]\n    group2hmm = find_coordinates(hmms, bit_thresh)\n    # get sequences from fasta file\n    for fasta in fastas:\n        for seq in parse_fasta(fasta):\n            id = seq[0].split('>')[1].split()[0]\n            if id not in group2hmm:\n                continue\n            seq[1] = seq[1].upper()\n            count = 0 # how many 23S genes are there on the contig?\n            for group, info in list(group2hmm[id].items()):\n                model, strand, coords, matches, gaps = info\n                # count insertion bases (ib) from gaps\n                ib = sum([i[1] - i[0] + 1 for i in gaps])\n                # calcualte length of non-insertion regions (don't include buffer)\n                tl = coords[1] - coords[0] + 1\n                length = tl - ib\n                if length < length_thresh:\n                    continue \n                # count sequence\n                count += 1\n                # set retrieval coords based on buffer\n                ret_coords = [max([coords[0] - buffer, 1]), \\\n                        min([coords[1] + buffer, len(seq[1])]), coords[2]]\n                buffer_ends = check_buffer(coords, len(seq[1]), buffer)\n                # mask insertion sequences\n                if masking is True:\n                    seq[1] = mask_sequence(seq[1], gaps)\n                S = seq[1][(ret_coords[0] - 1):(ret_coords[1])]\n                inserts = [gap[1] - gap[0] + 1 for gap in gaps]\n                inserts.append('end')\n                model_pos = ';'.join(['%s-%s(%s)' % (match[2], match[3], insert) for match, insert in zip(matches, inserts)])\n                header = '%s 23SfromHMM::model=%s seq=%s pos=%s-%s strand=%s total-len=%s 23S-len=%s model-pos(ins-len)=%s buffer-len=%s/%s ins-bases=%s' % \\\n                        (seq[0], model, count, ret_coords[0], ret_coords[1], strand, tl, length, model_pos, buffer_ends[0], buffer_ends[1], ib)\n                # reverse complement if strand is reverse\n                if strand == '-':\n                    S = rc(['', S])[1]\n                yield [header, S]", "response": "finds 23S sequences from a list of fastas"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run_cmsearch(fastas, threads, cm):\n    out = []\n    for fasta in fastas:\n        cmsearch = '%s.23S.cmsearch' % (fasta.name.rsplit('.', 1)[0])\n        if os.path.exists(cmsearch) is False:\n            p = Popen('\\\n                    cmsearch --cpu %s --hmmonly --acc --noali -T -1 --tblout %s %s %s >> cmsearch.log' \\\n                    % (threads, cmsearch, cm, fasta.name), shell = True)\n            p.communicate()\n        else:\n            print('# cmsearch output found: %s' % (cmsearch), file=sys.stderr)\n        out.append(open(cmsearch))\n    return out", "response": "run cmsearch on the fastas"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the non - direct init if direct algorithm has been selected.", "response": "def __get_nondirect_init(self, init):\n        \"\"\"\n        return the non-direct init if the direct algorithm has been selected.\n        \"\"\"\n        crc = init\n        for i in range(self.Width):\n            bit = crc & 0x01\n            if bit:\n                crc^= self.Poly\n            crc >>= 1\n            if bit:\n                crc |= self.MSB_Mask\n        return crc & self.Mask"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reflect(self, data, width):\n        x = data & 0x01\n        for i in range(width - 1):\n            data >>= 1\n            x = (x << 1) | (data & 0x01)\n        return x", "response": "reflect a data word i. e. reverts the bit order."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bit_by_bit(self, in_data):\n        # If the input data is a string, convert to bytes.\n        if isinstance(in_data, str):\n            in_data = [ord(c) for c in in_data]\n\n        register = self.NonDirectInit\n        for octet in in_data:\n            if self.ReflectIn:\n                octet = self.reflect(octet, 8)\n            for i in range(8):\n                topbit = register & self.MSB_Mask\n                register = ((register << 1) & self.Mask) | ((octet >> (7 - i)) & 0x01)\n                if topbit:\n                    register ^= self.Poly\n\n        for i in range(self.Width):\n            topbit = register & self.MSB_Mask\n            register = ((register << 1) & self.Mask)\n            if topbit:\n                register ^= self.Poly\n\n        if self.ReflectOut:\n            register = self.reflect(register, self.Width)\n        return register ^ self.XorOut", "response": "This function iterates over the augmented input message and returns the calculated CRC."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_publisher():\n    log.info(\"initializing publisher\")\n    pub = None\n    auth_url = \"\"\n    if FORWARD_ENDPOINT_TYPE == \"redis\":\n        auth_url = FORWARD_BROKER_URL\n    else:\n        auth_url = FORWARD_BROKER_URL\n\n    pub = Publisher(name=\"{}_{}\".format(SOURCE, \"-redis\"),\n                    auth_url=auth_url,\n                    ssl_options=FORWARD_SSL_OPTIONS)\n\n    log.info(\"publisher={}\".format(pub))\n    return pub", "response": "get_publisher - returns a publisher object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recv_msg(body,\n             message):\n    \"\"\"recv_msg\n\n    Handler method - fires when a messages is consumed from\n    the ``FORWARD_QUEUE`` queue running in the ``FORWARD_BROKER_URL``\n    broker.\n\n    :param body: message body\n    :param message: message object can ack, requeue or reject\n    \"\"\"\n\n    log.info((\"callback received msg \"))\n\n    agg.handle_msg(\n        body=body,\n        org_message=message)", "response": "This function is called by the broker when a message is received from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef consume_network_packet_messages_from_redis():\n    # end of recv_message\n    # Initialize KombuSubscriber\n    sub = KombuSubscriber(\n        name,\n        FORWARD_BROKER_URL,\n        FORWARD_SSL_OPTIONS)\n\n    # Now consume:\n    seconds_to_consume = 10.0\n    heartbeat = 60\n    serializer = \"application/json\"\n    queue = FORWARD_QUEUE\n\n    sub.consume(\n        callback=recv_msg,\n        queue=queue,\n        exchange=None,\n        routing_key=None,\n        serializer=serializer,\n        heartbeat=heartbeat,\n        time_to_wait=seconds_to_consume)\n\n    log.info(\"end - {}\".format(name))", "response": "This function is used to consume network packet messages from the redis. It will initialize a KombuSubscriber to consume meessages\n    from the FORWARD_BROKER_URL FORWARD_QUEUE and then call the receive method of each message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_masked(seq, min_len):\n    nm, masked = [], [[]]\n    prev = None\n    for base in seq[1]:\n        if base.isupper():\n            nm.append(base)\n            if masked != [[]] and len(masked[-1]) < min_len:\n                nm.extend(masked[-1])\n                del masked[-1]\n            prev = False\n        elif base.islower():\n            if prev is False:\n                masked.append([])\n            masked[-1].append(base)\n            prev = True\n    return nm, masked", "response": "parse masked sequence into non - masked and masked regions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstripping masked regions from a fasta file", "response": "def strip_masked(fasta, min_len, print_masked):\n    \"\"\"\n    remove masked regions from fasta file as long as\n    they are longer than min_len\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        nm, masked = parse_masked(seq, min_len)\n        nm = ['%s removed_masked >=%s' % (seq[0], min_len), ''.join(nm)]\n        yield [0, nm]\n        if print_masked is True:\n            for i, m in enumerate([i for i in masked if i != []], 1):\n                m = ['%s insertion:%s' % (seq[0], i), ''.join(m)]\n                yield [1, m]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning arcsine transformed relative abundance from a BIOM format file.", "response": "def get_relative_abundance(biomfile):\n    \"\"\"\n    Return arcsine transformed relative abundance from a BIOM format file.\n\n    :type biomfile: BIOM format file\n    :param biomfile: BIOM format file used to obtain relative abundances for each OTU in\n                     a SampleID, which are used as node sizes in network plots.\n\n    :type return: Dictionary of dictionaries.\n    :return: Dictionary keyed on SampleID whose value is a dictionarykeyed on OTU Name\n             whose value is the arc sine tranfsormed relative abundance value for that\n             SampleID-OTU Name pair.\n    \"\"\"\n    biomf = biom.load_table(biomfile)\n    norm_biomf = biomf.norm(inplace=False)\n    rel_abd = {}\n    for sid in norm_biomf.ids():\n        rel_abd[sid] = {}\n        for otuid in norm_biomf.ids(\"observation\"):\n            otuname = oc.otu_name(norm_biomf.metadata(otuid, axis=\"observation\")[\"taxonomy\"])\n            otuname = \" \".join(otuname.split(\"_\"))\n            abd = norm_biomf.get_value_by_ids(otuid, sid)\n            rel_abd[sid][otuname] = abd\n    ast_rel_abd = bc.arcsine_sqrt_transform(rel_abd)\n    return ast_rel_abd"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_training_request(\n        csv_file=ev(\n            \"CSV_FILE\",\n            \"/tmp/cleaned_attack_scans.csv\"),\n        meta_file=ev(\n            \"CSV_META_FILE\",\n            \"/tmp/cleaned_metadata.json\"),\n        predict_feature=ev(\n            \"PREDICT_FEATURE\",\n            \"label_value\"),\n        ignore_features=[\n            \"label_name\",\n            \"ip_src\",   # need to make this an int\n            \"ip_dst\",   # need to make this an int\n            \"eth_src\",  # need to make this an int\n            \"eth_dst\"   # need to make this an int\n        ],\n        seed=None,\n        test_size=float(ev(\n            \"TEST_SIZE\",\n            \"0.20\")),\n        preproc_rules=None):\n    \"\"\"build_training_request\n\n    :param csv_file: csv file built with prepare_dataset.py\n    :param meta_file: metadata file built with prepare_dataset.py\n    :param predict_feature: feature (column) to predict\n    :param ignore_features: features to remove from the csv\n                            before the split of test + train\n                            data\n    :param seed: integer to seed\n    :param test_size: percent of records to split into test\n                      vs train\n    :param preproc_rules: future preprocessing rules hooks\n    \"\"\"\n\n    last_step = \"not started\"\n    res = {\n        \"status\": INVALID,\n        \"err\": \"\",\n        \"csv_file\": csv_file,\n        \"meta_file\": meta_file,\n        \"meta_data\": None,\n        \"seed\": None,\n        \"test_size\": test_size,\n        \"predict_feature\": predict_feature,\n        \"features_to_process\": [],\n        \"ignore_features\": ignore_features,\n        \"X_train\": None,\n        \"X_test\": None,\n        \"Y_train\": None,\n        \"Y_test\": None\n    }\n\n    try:\n\n        last_step = (\"building seed={}\").format(\n                        seed)\n\n        log.debug(last_step)\n\n        use_seed = seed\n        if not use_seed:\n            use_seed = 9\n\n        res[\"seed\"] = np.random.seed(use_seed)\n\n        last_step = (\"Loading csv={}\").format(\n                        csv_file)\n\n        log.info(last_step)\n\n        if not os.path.exists(csv_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find csv_file={}\").format(\n                            csv_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid csv file on disk\n\n        if not os.path.exists(meta_file):\n            res[\"status\"] = ERROR\n            res[\"err\"] = (\"Unable to find meta_file={}\").format(\n                            meta_file)\n            log.error(res[\"err\"])\n            return res\n        # end of checking for a valid metadata file on disk\n\n        # load csv file into pandas dataframe\n        df = pd.read_csv(csv_file)\n\n        features_to_process = []\n        meta_data = {}\n\n        try:\n            last_step = (\"opening metadata={}\").format(\n                            meta_file)\n            log.debug(last_step)\n            meta_data = json.loads(\n                open(meta_file, \"r\").read()\n            )\n            res[\"meta_data\"] = meta_data\n            if \"post_proc_rules\" in meta_data:\n                if \"drop_columns\" in meta_data[\"post_proc_rules\"]:\n                    log.debug((\"Found drop_columns={}\")\n                              .format(\n                                meta_data[\"post_proc_rules\"][\"drop_columns\"]\n                              ))\n                    for ign in meta_data[\"post_proc_rules\"][\"drop_columns\"]:\n                        ignore_features.append(ign)\n        except Exception as e:\n            res[\"error\"] = (\"Failed building ignore_features: \"\n                            \"ignore_features={} meta={} meta_data={} \"\n                            \"last_step='{}' ex='{}'\").format(\n                                ignore_features,\n                                meta_file,\n                                meta_data,\n                                last_step,\n                                e)\n            log.error(res[\"error\"])\n            res[\"status\"] = ERROR\n            return res\n        # end of trying to lookup the meta data file\n        # for non-int/float features to ignore\n\n        last_step = (\"metadata={} df has \"\n                     \"columns={} ignore={}\").format(\n                        meta_file,\n                        df.columns.values,\n                        ignore_features)\n\n        log.info(last_step)\n\n        for feature in df.columns.values:\n            keep_it = True\n            for ign in ignore_features:\n                if feature == ign:\n                    keep_it = False\n            if keep_it:\n                if feature != predict_feature:\n                    features_to_process.append(feature)\n        # end of for all features to process\n\n        last_step = (\"Done post-procecessing \"\n                     \"Predicting={} with features={} \"\n                     \"ignore_features={} records={}\").format(\n                        predict_feature,\n                        features_to_process,\n                        ignore_features,\n                        len(df.index))\n\n        log.info(last_step)\n\n        res[\"predict_feature\"] = predict_feature\n\n        res[\"ignore_features\"] = []\n        for k in ignore_features:\n            if k not in res[\"ignore_features\"]:\n                res[\"ignore_features\"].append(k)\n        res[\"features_to_process\"] = []\n        for k in features_to_process:\n            if k not in res[\"features_to_process\"]:\n                if k != predict_feature:\n                    res[\"features_to_process\"].append(k)\n\n        # split the data into training\n        (res[\"X_train\"],\n         res[\"X_test\"],\n         res[\"Y_train\"],\n         res[\"Y_test\"]) = train_test_split(\n                            df[features_to_process],\n                            df[predict_feature],\n                            test_size=test_size,\n                            random_state=res[\"seed\"])\n\n        last_step = (\"Done splitting rows={} into \"\n                     \"X_train={} X_test={} \"\n                     \"Y_train={} Y_test={}\").format(\n                        len(df.index),\n                        len(res[\"X_train\"]),\n                        len(res[\"X_test\"]),\n                        len(res[\"Y_train\"]),\n                        len(res[\"Y_test\"]))\n\n        log.info((\"Success: {}\")\n                 .format(last_step))\n\n        res[\"err\"] = \"\"\n        res[\"status\"] = VALID\n    except Exception as e:\n        res[\"status\"] = ERROR\n        res[\"err\"] = (\"Failed build_training_request \"\n                      \"step='{}' with ex='{}'\").format(\n                        last_step,\n                        e)\n        log.error((\"build_training_request: {}\")\n                  .format(res[\"err\"]))\n    # end of try/ex\n\n    return res", "response": "build training request for a single object"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds an OTU ID in a Newick - format tree.", "response": "def find_otu(otuid, tree):\n    \"\"\"\n    Find an OTU ID in a Newick-format tree.\n    Return the starting position of the ID or None if not found.\n    \"\"\"\n    for m in re.finditer(otuid, tree):\n        before, after = tree[m.start()-1], tree[m.start()+len(otuid)]\n        if before in [\"(\", \",\", \")\"] and after in [\":\", \";\"]:\n            return m.start()\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef newick_replace_otuids(tree, biomf):\n    for val, id_, md in biomf.iter(axis=\"observation\"):\n        otu_loc = find_otu(id_, tree)\n        if otu_loc is not None:\n            tree = tree[:otu_loc] + \\\n                   oc.otu_name(md[\"taxonomy\"]) + \\\n                   tree[otu_loc + len(id_):]\n    return tree", "response": "Replace OTU ids in the Newick phylogenetic tree format with truncated OTU names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart consumers for a queue", "response": "def start_consumers_for_queue(prefix_name=\"worker\",\n                              num_workers=2,\n                              tasks=None,\n                              queue_to_consume=None,\n                              shutdown_msg=\"SHUTDOWN\",\n                              consumer_class=None,\n                              need_response=False,\n                              callback=None):\n    \"\"\"start_consumers_for_queue\n\n    :param prefix_name:\n    :param num_workers:\n    :param tasks:\n    :param queue_to_consume:\n    :param shutdown_msg:\n    :param consumer_class:\n    :param need_response:\n    :param callback:\n    \"\"\"\n\n    consumers = []\n\n    if not consumer_class:\n        log.error(\"Please provide a consumer_class arg\")\n        log.error(\"  like: network_pipeline.packet_consumer.PacketConsumer\")\n        return consumers\n\n    if not tasks:\n        log.error(\"Missing tasks\")\n        return consumers\n\n    if not queue_to_consume:\n        log.error(\"Missing queue\")\n        return consumers\n\n    # Establish communication queues\n    log.info((\"Creating consumers={} for cores={}\")\n             .format(multiprocessing.cpu_count(),\n                     num_workers))\n\n    for i in range(num_workers):\n        consumers.append(consumer_class(\n                            \"{}-{}\".format(prefix_name,\n                                           i + 1),\n                            tasks,\n                            queue_to_consume,\n                            shutdown_msg=shutdown_msg,\n                            need_response=need_response,\n                            callback=callback))\n\n    log.info(\"Starting consumers={}\".format(len(consumers)))\n    for w in consumers:\n        w.start()\n\n    return consumers"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates mash files for multiple fasta files", "response": "def make_mashes(fastas, mash_file, threads, kmer = 21, force = False):\n    \"\"\"\n    Create mash files for multiple fasta files\n    Input:\n        fastas <list[str]>  -- paths to fasta files\n        mash_file <str>     -- path to output mash file\n        threads <int>       -- # threads for parallelization\n        kmer <int>          -- kmer size for mash sketching\n        force <boolean>     -- force overwrite of all mash files\n    \"\"\"\n    mash_processes = set()\n    sketches = [fasta + '.msh' for fasta in fastas]\n    devnull = open(os.devnull, 'w')\n    # Perform the sketching\n    for fasta, sketch in zip(fastas, sketches):\n        if os.path.isfile(sketch):\n            continue\n        mash_cmd = ['/opt/bin/bio/mash', 'sketch', '-o', fasta, '-k', str(kmer), fasta]\n        mash_processes.add(subprocess.Popen(mash_cmd, stderr=devnull))\n        if len(mash_processes) >= threads:\n            os.wait()\n            mash_processes.difference_update([mp for mp in mash_processes if mp.poll() is not None])\n    # Collect stragglers\n    for mp in mash_processes:\n        if mp.poll() is None:\n            mp.wait()\n    # Paste sketches into single mash\n    paste_mashes(sketches, mash_file, force = force)\n    return"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef paste_mashes(sketches, pasted_mash, force = False):\n    if os.path.isfile(pasted_mash):\n        if force:\n            subprocess.Popen(['rm', pasted_mash]).wait()\n        else:\n            return\n    pasted_mash = pasted_mash.rsplit('.msh')[0]\n    mash_cmd = ['/opt/bin/bio/mash', 'paste', pasted_mash]\n    mash_cmd.extend(sketches)\n    process = subprocess.Popen(mash_cmd)\n    process.wait()\n    return", "response": "Paste a list of mash files into a single sketch."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ani(fastas, mash_file, sim_threshold, threads):\n    ANI = Graph()\n    # use Mash to estimate ANI\n    for fasta in fastas:\n        indiv_mash = fasta + '.msh'\n        if os.path.isfile(indiv_mash):\n            cmp_file = indiv_mash\n        else:\n            cmp_file = fasta\n        mash_cmd = ['/opt/bin/bio/mash', 'dist', cmp_file, mash_file]\n        process = subprocess.Popen(mash_cmd, stdout = subprocess.PIPE)\n        for pair in process.communicate()[0].splitlines():\n            a, b, dist, p, shared = pair.decode().strip().split()\n            a = a.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]\n            b = b.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0]\n            p = float(p)\n            similarity = (1 - float(dist)) * 100\n            if similarity >= sim_threshold:\n                ANI.add_edge(a, b, si = similarity, pval = p, sharedK = shared)\n        process.wait()\n    return ANI", "response": "Estimate ANI of genomes from a set of fastas."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns genome info for choosing representative", "response": "def genome_info(genome, info):\n    \"\"\"\n    return genome info for choosing representative\n\n    if ggKbase table provided - choose rep based on SCGs and genome length\n        - priority for most SCGs - extra SCGs, then largest genome\n\n    otherwise, based on largest genome\n    \"\"\"\n    try:\n        scg       = info['#SCGs']\n        dups      = info['#SCG duplicates']\n        length    = info['genome size (bp)']\n        return [scg - dups, length, genome]\n    except:\n        return [False, False, info['genome size (bp)'], genome]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef print_clusters(fastas, info, ANI):\n    header = ['#cluster', 'num. genomes', 'rep.', 'genome', '#SCGs', '#SCG duplicates', \\\n            'genome size (bp)', 'fragments', 'list']\n    yield header\n    in_cluster = []\n    for cluster_num, cluster in enumerate(connected_components(ANI)):\n        cluster = sorted([genome_info(genome, info[genome]) \\\n                            for genome in cluster], \\\n                            key = lambda x: x[0:], reverse = True)\n        rep = cluster[0][-1]\n        cluster = [i[-1] for i in cluster]\n        size = len(cluster)\n        for genome in cluster:\n            in_cluster.append(genome)\n            try:\n                stats = [size, rep, genome, \\\n                            info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            except:\n                stats = [size, rep, genome, \\\n                            'n/a', 'n/a', \\\n                            info[genome]['genome size (bp)'], info[genome]['# contigs'], cluster]\n            if rep == genome:\n                stats = ['*%s' % (cluster_num)] + stats\n            else:\n                stats = [cluster_num] + stats\n            yield stats\n    # print singletons\n    try:\n        start = cluster_num + 1\n    except:\n        start = 0\n    fastas = set([i.rsplit('.', 1)[0].rsplit('/', 1)[-1].rsplit('.contigs')[0] for i in fastas])\n    for cluster_num, genome in \\\n            enumerate(fastas.difference(set(in_cluster)), start):\n        try:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        info[genome]['#SCGs'], info[genome]['#SCG duplicates'], \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        except:\n            stats = ['*%s' % (cluster_num), 1, genome, genome, \\\n                        'n/a', 'n/a', \\\n                        info[genome]['genome size (bp)'], info[genome]['# contigs'], [genome]]\n        yield stats", "response": "prints cluster information for each genome in the list of fastas"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_ggKbase_tables(tables, id_type):\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('name'):\n                header = line\n                header[4] = 'genome size (bp)'\n                header[12] = '#SCGs'\n                header[13] = '#SCG duplicates'\n                continue\n            name, code, info = line[0], line[1], line\n            info = [to_int(i) for i in info]\n            if id_type is False: # try to use name and code ID\n                if 'UNK' in code or 'unknown' in code:\n                    code = name\n                if (name != code) and (name and code in g2info):\n                    print('# duplicate name or code in table(s)', file=sys.stderr)\n                    print('# %s and/or %s' % (name, code), file=sys.stderr)\n                    exit()\n                if name not in g2info:\n                    g2info[name] = {item:stat for item, stat in zip(header, info)}\n                if code not in g2info:\n                    g2info[code] = {item:stat for item, stat in zip(header, info)}\n            else:\n                if id_type == 'name':\n                    ID = name\n                elif id_type == 'code':\n                    ID = code\n                else:\n                    print('# specify name or code column using -id', file=sys.stderr)\n                    exit()\n                ID = ID.replace(' ', '')\n                g2info[ID] = {item:stat for item, stat in zip(header, info)}\n                if g2info[ID]['genome size (bp)'] == '':\n                    g2info[ID]['genome size (bp)'] = 0\n    return g2info", "response": "convert ggKbase genome info tables to dictionary\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert checkM genome info tables to dictionary Invitement", "response": "def parse_checkM_tables(tables):\n    \"\"\"\n    convert checkM genome info tables to dictionary\n    \"\"\"\n    g2info = {}\n    for table in tables:\n        for line in open(table):\n            line = line.strip().split('\\t')\n            if line[0].startswith('Bin Id'):\n                header = line\n                header[8] = 'genome size (bp)'\n                header[5] = '#SCGs'\n                header[6] = '#SCG duplicates'\n                continue\n            ID, info = line[0], line\n            info = [to_int(i) for i in info]\n            ID = ID.replace(' ', '')\n            g2info[ID] = {item:stat for item, stat in zip(header, info)}\n            if g2info[ID]['genome size (bp)'] == '':\n                g2info[ID]['genome size (bp)'] = 0\n    return g2info"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a list of databases to route the given attribute on the connection.", "response": "def get_dbs(self, attr, args, kwargs, **fkwargs):\n        \"\"\"\n        Returns a list of db keys to route the given call to.\n\n        :param attr: Name of attribute being called on the connection.\n        :param args: List of arguments being passed to ``attr``.\n        :param kwargs: Dictionary of keyword arguments being passed to ``attr``.\n\n        >>> redis = Cluster(router=BaseRouter)\n        >>> router = redis.router\n        >>> router.get_dbs('incr', args=('key name', 1))\n        [0,1,2]\n\n        \"\"\"\n        if not self._ready:\n            if not self.setup_router(args=args, kwargs=kwargs, **fkwargs):\n                raise self.UnableToSetupRouter()\n\n        retval = self._pre_routing(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        if retval is not None:\n            args, kwargs = retval\n\n        if not (args or kwargs):\n            return self.cluster.hosts.keys()\n\n        try:\n            db_nums = self._route(attr=attr, args=args, kwargs=kwargs, **fkwargs)\n        except Exception as e:\n            self._handle_exception(e)\n            db_nums = []\n\n        return self._post_routing(attr=attr, db_nums=db_nums, args=args, kwargs=kwargs, **fkwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setup_router(self, args, kwargs, **fkwargs):\n        self._ready = self._setup_router(args=args, kwargs=kwargs, **fkwargs)\n\n        return self._ready", "response": "Setup the router for this instance."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _route(self, attr, args, kwargs, **fkwargs):\n        return self.cluster.hosts.keys()", "response": "This method is used to route the hosts to the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if all connections have expired and marks them as being up.", "response": "def check_down_connections(self):\n        \"\"\"\n        Iterates through all connections which were previously listed as unavailable\n        and marks any that have expired their retry_timeout as being up.\n        \"\"\"\n        now = time.time()\n\n        for db_num, marked_down_at in self._down_connections.items():\n            if marked_down_at + self.retry_timeout <= now:\n                self.mark_connection_up(db_num)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmark all connections which were previously listed as being up.", "response": "def flush_down_connections(self):\n        \"\"\"\n        Marks all connections which were previously listed as unavailable as being up.\n        \"\"\"\n        self._get_db_attempts = 0\n        for db_num in self._down_connections.keys():\n            self.mark_connection_up(db_num)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef standby(df, resolution='24h', time_window=None):\n\n    if df.empty:\n        raise EmptyDataFrame()\n\n    df = pd.DataFrame(df)  # if df was a pd.Series, convert to DataFrame\n    def parse_time(t):\n        if isinstance(t, numbers.Number):\n            return pd.Timestamp.utcfromtimestamp(t).time()\n        else:\n            return pd.Timestamp(t).time()\n\n\n    # first filter based on the time-window\n    if time_window is not None:\n        t_start = parse_time(time_window[0])\n        t_end = parse_time(time_window[1])\n        if t_start > t_end:\n            # start before midnight\n            df = df[(df.index.time >= t_start) | (df.index.time < t_end)]\n        else:\n            df = df[(df.index.time >= t_start) & (df.index.time < t_end)]\n\n    return df.resample(resolution).min()", "response": "Compute the standby power of the specified amount of time for the given resource."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef share_of_standby(df, resolution='24h', time_window=None):\n\n    p_sb = standby(df, resolution, time_window)\n    df = df.resample(resolution).mean()\n    p_tot = df.sum()\n    p_standby = p_sb.sum()\n    share_standby = p_standby / p_tot\n    res = share_standby.iloc[0]\n    return res", "response": "Compute the share of the standby power in the total consumption."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncount the number of times the gas consumption increases with more than 3kW", "response": "def count_peaks(ts):\n    \"\"\"\n    Toggle counter for gas boilers\n\n    Counts the number of times the gas consumption increases with more than 3kW\n\n    Parameters\n    ----------\n    ts: Pandas Series\n        Gas consumption in minute resolution\n\n    Returns\n    -------\n    int\n    \"\"\"\n\n    on_toggles = ts.diff() > 3000\n    shifted = np.logical_not(on_toggles.shift(1))\n    result = on_toggles & shifted\n    count = result.sum()\n    return count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_factor(ts, resolution=None, norm=None):\n    if norm is None:\n        norm = ts.max()\n\n    if resolution is not None:\n        ts = ts.resample(rule=resolution).mean()\n\n    lf = ts / norm\n\n    return lf", "response": "Calculate the ratio of input vs. norm over a given interval."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a new deep neural network from environment variables.", "response": "def build_new_deep_neural_network_from_env_variables():\n    \"\"\"build_new_deep_neural_network_from_env_variables\n\n    Build a new deep neural network from environment variables:\n\n    ``CSV_FILE`` - file to process created during prepare dataset\n    ``CSV_META_FILE`` - metadata header file created during prepare dataset\n    ``PREDICT_FEATURE`` - column to predict\n    ``TEST_SIZE`` - split data into percentage of test to training\n    \"\"\"\n\n    csv_file = ev(\n        \"CSV_FILE\",\n        \"/tmp/cleaned_attack_scans.csv\")\n    meta_file = ev(\n        \"CSV_META_FILE\",\n        \"/tmp/cleaned_metadata.json\")\n    predict_feature = ev(\n        \"PREDICT_FEATURE\",\n        \"label_value\")\n    test_size = float(ev(\n        \"TEST_SIZE\",\n        \"0.20\"))\n\n    if not os.path.exists(csv_file):\n        log.error((\"missing csv_file={}\")\n                  .format(csv_file))\n        sys.exit(1)\n\n    res = build_training_request(\n            csv_file=csv_file,\n            meta_file=meta_file,\n            predict_feature=predict_feature,\n            test_size=test_size)\n\n    if res[\"status\"] != VALID:\n        log.error((\"Stopping for status={} \"\n                   \"errors: {}\")\n                  .format(\n                    res[\"status\"],\n                    res[\"err\"]))\n        sys.exit(1)\n    else:\n        log.info((\"built_training_request={} \"\n                  \"features={} ignore={}\")\n                 .format(\n                    res[\"status\"],\n                    res[\"features_to_process\"],\n                    res[\"ignore_features\"]))\n    # end of validating the training request\n\n    log.info(\"ready for training\")\n\n    log.info(\"creating Keras - sequential model\")\n\n    # create the model\n    model = Sequential()\n    model.add(Dense(8,\n                    input_dim=len(res[\"features_to_process\"]),\n                    kernel_initializer=\"uniform\",\n                    activation=\"relu\"))\n    model.add(Dense(6,\n                    kernel_initializer=\"uniform\",\n                    activation=\"relu\"))\n    model.add(Dense(1,\n                    kernel_initializer=\"uniform\",\n                    activation=\"sigmoid\"))\n\n    log.info(\"compiling model\")\n\n    # compile the model\n    model.compile(\n        loss=\"binary_crossentropy\",\n        optimizer=\"adam\",\n        metrics=[\"accuracy\"])\n\n    log.info(\"fitting model - please wait\")\n\n    # fit the model\n    model.fit(\n        res[\"X_train\"],\n        res[\"Y_train\"],\n        validation_data=(res[\"X_test\"],\n                         res[\"Y_test\"]),\n        epochs=50,\n        batch_size=2,\n        verbose=1)\n\n    # evaluate the model\n    scores = model.evaluate(\n        res[\"X_test\"],\n        res[\"Y_test\"])\n\n    log.info((\"Accuracy: {}\")\n             .format(\n                scores[1] * 100))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_layer_2_socket():\n\n    # create a socket for recording layer 2, 3 and 4 frames\n    s = None\n    try:\n        log.info(\"Creating l234 socket\")\n        s = socket.socket(socket.AF_PACKET,\n                          socket.SOCK_RAW,\n                          socket.ntohs(0x0003))\n    except socket.error as msg:\n        log.error((\"Socket could not be created ex={}\")\n                  .format(msg))\n    return s", "response": "create_layer_2_socket creates a socket for recording layer 2 3 and 4 frames"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget top hits after sorting by column number", "response": "def top_hits(hits, num, column, reverse):\n    \"\"\"\n    get top hits after sorting by column number\n    \"\"\"\n    hits.sort(key = itemgetter(column), reverse = reverse)\n    for hit in hits[0:num]:\n        yield hit"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef numBlast_sort(blast, numHits, evalueT, bitT):\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    hmm = {h:[] for h in header}\n    for line in blast:\n        if line.startswith('#'):\n            continue\n        line = line.strip().split('\\t')\n        # Evalue and Bitscore thresholds\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if evalueT is not False and evalue > evalueT:\n            continue\n        if bitT is not False and bit < bitT:\n            continue\n        for i, h in zip(line, header):\n            hmm[h].append(i)\n    hmm = pd.DataFrame(hmm)\n    for query, df in hmm.groupby(by = ['#query']):\n        df = df.sort_values(by = ['bitscore'], ascending = False)\n        for hit in df[header].values[0:numHits]:\n            yield hit", "response": "parse b6 output with sorting\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef numBlast(blast, numHits, evalueT = False, bitT = False, sort = False):\n    if sort is True:\n        for hit in numBlast_sort(blast, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#query', 'target', 'pident', 'alen', 'mismatch', 'gapopen',\n              'qstart', 'qend', 'tstart', 'tend', 'evalue', 'bitscore']\n    yield header\n    prev, hits = None, []\n    for line in blast:\n        line = line.strip().split('\\t')\n        ID = line[0]\n        line[10], line[11] = float(line[10]), float(line[11])\n        evalue, bit = line[10], line[11]\n        if ID != prev:\n            if len(hits) > 0:\n                # column is 1 + line index\n                for hit in top_hits(hits, numHits, 11, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 11, True):\n        yield hit", "response": "parse b6 output\n    parse b6 output\n    yield all the hits"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a domtblout and return a generator of the top numHits elements", "response": "def numDomtblout(domtblout, numHits, evalueT, bitT, sort):\n    \"\"\"\n    parse hmm domain table output\n    this version is faster but does not work unless the table is sorted\n    \"\"\"\n    if sort is True:\n        for hit in numDomtblout_sort(domtblout, numHits, evalueT, bitT):\n            yield hit\n        return\n    header = ['#target name', 'target accession', 'tlen',\n              'query name', 'query accession', 'qlen',\n              'full E-value', 'full score', 'full bias',\n              'domain #', '# domains',\n              'domain c-Evalue', 'domain i-Evalue', 'domain score', 'domain bias',\n              'hmm from', 'hmm to', 'seq from', 'seq to', 'env from', 'env to',\n              'acc', 'target description']\n    yield header\n    prev, hits = None, []\n    for line in domtblout:\n        if line.startswith('#'):\n            continue\n        # parse line and get description\n        line = line.strip().split()\n        desc = ' '.join(line[18:])\n        line = line[0:18]\n        line.append(desc)\n        # create ID based on query name and domain number\n        ID = line[0] + line[9]\n        # domain c-Evalue and domain score thresholds\n        line[11], line[13] = float(line[11]), float(line[13])\n        evalue, bitscore = line[11], line[13]\n        line[11], line[13] = evalue, bitscore\n        if ID != prev:\n            if len(hits) > 0:\n                for hit in top_hits(hits, numHits, 13, True):\n                    yield hit\n            hits = []\n        if evalueT == False and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bitT == False:\n            hits.append(line)\n        elif evalue <= evalueT and bit >= bitT:\n            hits.append(line)\n        elif evalueT == False and bit >= bitT:\n            hits.append(line)\n        prev = ID\n    for hit in top_hits(hits, numHits, 13, True):\n        yield hit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef stock2fa(stock):\n    seqs = {}\n    for line in stock:\n        if line.startswith('#') is False and line.startswith(' ') is False and len(line) > 3:\n            id, seq = line.strip().split()\n            id = id.rsplit('/', 1)[0]\n            id = re.split('[0-9]\\|', id, 1)[-1]\n            if id not in seqs:\n                seqs[id] = []\n            seqs[id].append(seq)\n        if line.startswith('//'):\n            break\n    return seqs", "response": "convert stockholm to fasta\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concat_align(fastas):\n    # read in sequences\n    fa2len = {}\n    seqs = {}\n    IDs = []\n    for fasta in fastas:\n        seqs[fasta] = {}\n        for seq in parse_fasta(fasta):\n            ID = seq[0].split('>')[1].split()[0]\n            IDs.append(ID)\n            seqs[fasta][ID] = seq[1]\n        fa2len[fasta] = len(seq[1])\n    # concat sequences\n    IDs = set(IDs)\n    concat = {}\n    for fasta in fastas:\n        for ID in IDs:\n            if ID not in concat:\n                concat[ID] = []\n            if ID not in seqs[fasta]:\n                concat[ID].append('-'*fa2len[fasta])\n            else:\n                concat[ID].append(seqs[fasta][ID])\n    return concat", "response": "concatenate alignments with sequences"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns boolean time series following given week schedule.", "response": "def week_schedule(index, on_time=None, off_time=None, off_days=None):\n    \"\"\" Return boolean time series following given week schedule.\n\n    Parameters\n    ----------\n    index : pandas.DatetimeIndex\n        Datetime index\n    on_time : str or datetime.time\n        Daily opening time. Default: '09:00'\n    off_time : str or datetime.time\n        Daily closing time. Default: '17:00'\n    off_days : list of str\n        List of weekdays. Default: ['Sunday', 'Monday']\n\n    Returns\n    -------\n    pandas.Series of bool\n        True when on, False otherwise for given datetime index\n\n    Examples\n    --------\n    >>> import pandas as pd\n    >>> from opengrid.library.utils import week_schedule\n    >>> index = pd.date_range('20170701', '20170710', freq='H')\n    >>> week_schedule(index)\n    \"\"\"\n    if on_time is None:\n        on_time = '9:00'\n    if off_time is None:\n        off_time = '17:00'\n    if off_days is None:\n        off_days = ['Sunday', 'Monday']\n    if not isinstance(on_time, datetime.time):\n        on_time = pd.to_datetime(on_time, format='%H:%M').time()\n    if not isinstance(off_time, datetime.time):\n        off_time = pd.to_datetime(off_time, format='%H:%M').time()\n    times = (index.time >= on_time) & (index.time < off_time) & (~index.weekday_name.isin(off_days))\n    return pd.Series(times, index=index)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef carpet(timeseries, **kwargs):\n\n    # define optional input parameters\n    cmap = kwargs.pop('cmap', cm.coolwarm)\n    norm = kwargs.pop('norm', LogNorm())\n    interpolation = kwargs.pop('interpolation', 'nearest')\n    cblabel = kwargs.pop('zlabel', timeseries.name if timeseries.name else '')\n    title = kwargs.pop('title', 'carpet plot: ' + timeseries.name if timeseries.name else '')\n\n    # data preparation\n    if timeseries.dropna().empty:\n        print('skipped {} - no data'.format(title))\n        return\n    ts = timeseries.resample('15min').interpolate()\n    vmin = max(0.1, kwargs.pop('vmin', ts[ts > 0].min()))\n    vmax = max(vmin, kwargs.pop('vmax', ts.quantile(.999)))\n\n    # convert to dataframe with date as index and time as columns by\n    # first replacing the index by a MultiIndex\n    mpldatetimes = date2num(ts.index.to_pydatetime())\n    ts.index = pd.MultiIndex.from_arrays(\n        [np.floor(mpldatetimes), 2 + mpldatetimes % 1])  # '2 +': matplotlib bug workaround.\n    # and then unstacking the second index level to columns\n    df = ts.unstack()\n\n    # data plotting\n\n    fig, ax = plt.subplots()\n    # define the extent of the axes (remark the +- 0.5  for the y axis in order to obtain aligned date ticks)\n    extent = [df.columns[0], df.columns[-1], df.index[-1] + 0.5, df.index[0] - 0.5]\n    im = plt.imshow(df, vmin=vmin, vmax=vmax, extent=extent, cmap=cmap, aspect='auto', norm=norm,\n                    interpolation=interpolation, **kwargs)\n\n    # figure formatting\n\n    # x axis\n    ax.xaxis_date()\n    ax.xaxis.set_major_locator(HourLocator(interval=2))\n    ax.xaxis.set_major_formatter(DateFormatter('%H:%M'))\n    ax.xaxis.grid(True)\n    plt.xlabel('UTC Time')\n\n    # y axis\n    ax.yaxis_date()\n    dmin, dmax = ax.yaxis.get_data_interval()\n    number_of_days = (num2date(dmax) - num2date(dmin)).days\n    # AutoDateLocator is not suited in case few data is available\n    if abs(number_of_days) <= 35:\n        ax.yaxis.set_major_locator(DayLocator())\n    else:\n        ax.yaxis.set_major_locator(AutoDateLocator())\n    ax.yaxis.set_major_formatter(DateFormatter(\"%a, %d %b %Y\"))\n\n    # plot colorbar\n    cbticks = np.logspace(np.log10(vmin), np.log10(vmax), 11, endpoint=True)\n    cb = plt.colorbar(format='%.0f', ticks=cbticks)\n    cb.set_label(cblabel)\n\n    # plot title\n    plt.title(title)\n\n    return im", "response": "Draw a carpet plot of a pandas timeseries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef boxplot(df, plot_mean=False, plot_ids=None, title=None, xlabel=None, ylabel=None):\n\n    df = df.applymap(float)\n    description = df.apply(pd.DataFrame.describe, axis=1)\n\n    # plot\n    plt = plot_style()\n\n    plt.boxplot(df)\n    #plt.setp(bp['boxes'], color='black')\n    #plt.setp(bp['whiskers'], color='black')\n    if plot_ids is not None:\n        for id in plot_ids:\n            if id in df.columns:\n                plt.scatter(x=range(1, len(df) + 1), y=df[id], label=str(id))\n\n    if plot_mean:\n        plt.scatter(x=range(1, len(df) + 1), y=description['mean'], label=\"Mean\", color='k', s=30, marker='+')\n\n    ax = plt.gca()\n    ax.set_xticklabels(df.index)\n    #plt.xticks(rotation=45)\n\n    plt.legend()\n    if title is not None:\n        plt.title(title)\n    if xlabel is not None:\n        plt.xlabel(xlabel)\n    if ylabel is not None:\n        plt.ylabel(ylabel)\n\n    return plt.gcf()", "response": "Plot the boxplots of a dataframe in time\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate percent identity of a set of pident entries", "response": "def calc_pident_ignore_gaps(a, b):\n    \"\"\"\n    calculate percent identity\n    \"\"\"\n    m = 0 # matches\n    mm = 0 # mismatches\n    for A, B in zip(list(a), list(b)):\n        if A == '-' or A == '.' or B == '-' or B == '.':\n            continue\n        if A == B:\n            m += 1\n        else:\n            mm += 1\n    try:\n        return float(float(m)/float((m + mm))) * 100\n    except:\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_gaps(A, B):\n    a_seq, b_seq = [], []\n    for a, b in zip(list(A), list(B)):\n        if a == '-' or a == '.' or b == '-' or b == '.':\n            continue\n        a_seq.append(a)\n        b_seq.append(b)\n    return ''.join(a_seq), ''.join(b_seq)", "response": "remove gaps from two lists of names"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare_seqs(seqs):\n    A, B, ignore_gaps = seqs\n    a, b = A[1], B[1] # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    if ignore_gaps is True:\n        pident = calc_pident_ignore_gaps(a, b)\n    else:\n        pident = calc_pident(a, b)\n    return A[0], B[0], pident", "response": "compare pairs of sequences\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compare_seqs_leven(seqs):\n    A, B, ignore_gaps = seqs\n    a, b = remove_gaps(A[1], B[1]) # actual sequences\n    if len(a) != len(b):\n        print('# reads are not the same length', file=sys.stderr)\n        exit()\n    pident = lr(a, b) * 100\n    return A[0], B[0], pident", "response": "compare sequences in a sequence"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes pairwise sequence comparisons between aligned sequences", "response": "def pairwise_compare(afa, leven, threads, print_list, ignore_gaps):\n    \"\"\"\n    make pairwise sequence comparisons between aligned sequences\n    \"\"\"\n    # load sequences into dictionary\n    seqs = {seq[0]: seq for seq in nr_fasta([afa], append_index = True)}\n    num_seqs = len(seqs)\n    # define all pairs\n    pairs = ((i[0], i[1], ignore_gaps) for i in itertools.combinations(list(seqs.values()), 2))\n    pool = multithread(threads)\n    # calc percent identity between all pairs - parallelize\n    if leven is True:\n        pident = pool.map(compare_seqs_leven, pairs)\n    else:\n        compare = pool.imap_unordered(compare_seqs, pairs)\n        pident = [i for i in tqdm(compare, total = (num_seqs*num_seqs)/2)]\n    pool.close()\n    pool.terminate()\n    pool.join()\n    return to_dictionary(pident, print_list)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_dictionary(pw, print_list):\n    pairs = {}\n    for p in pw:\n        a, b, pident = p\n        if a not in pairs:\n            pairs[a] = {a: '-'}\n        if b not in pairs:\n            pairs[b] = {b: '-'}\n        pairs[a][b] = pident\n        pairs[b][a] = pident\n        if print_list is True:\n            A, B = a.split('>')[1], b.split('>')[1]\n            print('\\t'.join([str(i) for i in [A, B, pident]]), file=sys.stderr)\n            print('\\t'.join([str(i) for i in [B, A, pident]]), file=sys.stderr)\n    return pairs", "response": "convert list of comparisons to dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint matrix of pidents to stdout", "response": "def print_pairwise(pw, median = False):\n    \"\"\"\n    print matrix of pidents to stdout\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    if len(names) != 0:\n        if '>' in names[0]:\n            yield ['#'] + [i.split('>')[1] for i in names if '>' in i]\n        else:\n            yield ['#'] + names\n        for a in names:\n            if '>' in a:\n                yield [a.split('>')[1]] + [pw[a][b] for b in names]\n            else:\n                out = []\n                for b in names:\n                    if b in pw[a]:\n                        if median is False:\n                            out.append(max(pw[a][b]))\n                        else:\n                            out.append(np.median(pw[a][b]))\n                    else:\n                        out.append('-')\n                yield [a] + out"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints stats for comparisons", "response": "def print_comps(comps):\n    \"\"\"\n    print stats for comparisons\n    \"\"\"\n    if comps == []:\n        print('n/a')\n    else:\n        print('# min: %s, max: %s, mean: %s' % \\\n            (min(comps), max(comps), np.mean(comps)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares the clades in pw and print the minimum and maximum between - clades.", "response": "def compare_clades(pw):\n    \"\"\"\n    print min. pident within each clade and then matrix of between-clade max.\n    \"\"\"\n    names = sorted(set([i for i in pw]))\n    for i in range(0, 4):\n        wi, bt = {}, {}\n        for a in names:\n            for b in pw[a]:\n                if ';' not in a or ';' not in b:\n                    continue\n                pident = pw[a][b]\n                cA, cB = a.split(';')[i], b.split(';')[i]\n                if i == 0 and '_' in cA and '_' in cB:\n                    cA = cA.rsplit('_', 1)[1]\n                    cB = cB.rsplit('_', 1)[1]\n                elif '>' in cA or '>' in cB:\n                    cA = cA.split('>')[1]\n                    cB = cB.split('>')[1]\n                if cA == cB:\n                    if cA not in wi:\n                        wi[cA] = []\n                    wi[cA].append(pident)\n                else:\n                    if cA not in bt:\n                        bt[cA] = {}\n                    if cB not in bt[cA]:\n                        bt[cA][cB] = []\n                    bt[cA][cB].append(pident)\n        print('\\n# min. within')\n        for clade, pidents in list(wi.items()):\n            print('\\t'.join(['wi:%s' % str(i), clade, str(min(pidents))]))\n        # print matrix of maximum between groups\n        comps = []\n        print('\\n# max. between')\n        for comp in print_pairwise(bt):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)\n        # print matrix of median between groups\n        comps = []\n        print('\\n# median between')\n        for comp in print_pairwise(bt, median = True):\n            if comp is not None:\n                print('\\t'.join(['bt:%s' % str(i)] + [str(j) for j in comp]))\n                if comp[0] != '#':\n                    comps.extend([j for j in comp[1:] if j != '-'])\n        print_comps(comps)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert matrix to dictionary of comparisons", "response": "def matrix2dictionary(matrix):\n    \"\"\"\n    convert matrix to dictionary of comparisons\n    \"\"\"\n    pw = {}\n    for line in matrix:\n        line = line.strip().split('\\t')\n        if line[0].startswith('#'):\n            names = line[1:]\n            continue\n        a = line[0]\n        for i, pident in enumerate(line[1:]):\n            b = names[i]\n            if a not in pw:\n                pw[a] = {}\n            if b not in pw:\n                pw[b] = {}\n            if pident != '-':\n                pident = float(pident)\n            pw[a][b] = pident\n            pw[b][a] = pident\n    return pw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset argument parser option.", "response": "def setoption(parser, metadata=None):\n    \"\"\"Set argument parser option.\"\"\"\n    parser.add_argument('-v', action='version',\n                        version=__version__)\n    subparsers = parser.add_subparsers(help='sub commands help')\n    create_cmd = subparsers.add_parser('create')\n    create_cmd.add_argument('name',\n                            help='Specify Python package name.')\n    create_cmd.add_argument('-d', dest='description', action='store',\n                            help='Short description about your package.')\n    create_cmd.add_argument('-a', dest='author', action='store',\n                            required=True,\n                            help='Python package author name.')\n    create_cmd.add_argument('-e', dest='email', action='store',\n                            required=True,\n                            help='Python package author email address.')\n    create_cmd.add_argument('-l', dest='license',\n                            choices=metadata.licenses().keys(),\n                            default='GPLv3+',\n                            help='Specify license. (default: %(default)s)')\n    create_cmd.add_argument('-s', dest='status',\n                            choices=metadata.status().keys(),\n                            default='Alpha',\n                            help=('Specify development status. '\n                                  '(default: %(default)s)'))\n    create_cmd.add_argument('--no-check', action='store_true',\n                            help='No checking package name in PyPI.')\n    create_cmd.add_argument('--with-samples', action='store_true',\n                            help='Generate package with sample code.')\n    group = create_cmd.add_mutually_exclusive_group(required=True)\n    group.add_argument('-U', dest='username', action='store',\n                       help='Specify GitHub username.')\n    group.add_argument('-u', dest='url', action='store', type=valid_url,\n                       help='Python package homepage url.')\n    create_cmd.add_argument('-o', dest='outdir', action='store',\n                            default=os.path.abspath(os.path.curdir),\n                            help='Specify output directory. (default: $PWD)')\n    list_cmd = subparsers.add_parser('list')\n    list_cmd.add_argument('-l', dest='licenses', action='store_true',\n                          help='show license choices.')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef valid_url(url):\n    regex = re.compile(\n        r'^(?:http)s?://'\n        r'(?:(?:[A-Z0-9](?:[A-Z0-9-]{0,61}[A-Z0-9])?\\.)+'\n        r'(?:[A-Z]{2,6}\\.?|[A-Z0-9-]{2,}\\.?))'\n        r'(?:/?|[/?]\\S+)$', re.IGNORECASE)\n    if not regex.match(url):\n        raise argparse.ArgumentTypeError('\"{0}\" is invalid url.'.format(url))\n    return url", "response": "Validate url.\n\n    :rtype: str\n    :return: url\n\n    :param str url: package homepage url."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _set_param(self, name, value):\n        if name == 'status':\n            setattr(self, name, self.metadata.status().get(value))\n        elif name == 'license':\n            setattr(self, name, self.metadata.licenses().get(value))\n        elif name == 'name':\n            setattr(self, name, value)\n            setattr(self, 'module_name', value.replace('-', '_'))\n        else:\n            setattr(self, name, value)", "response": "Set name : value property to Package object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking key and set default vaule when it does not exists.", "response": "def _check_or_set_default_params(self):\n        \"\"\"Check key and set default vaule when it does not exists.\"\"\"\n        if not hasattr(self, 'date'):\n            self._set_param('date', datetime.utcnow().strftime('%Y-%m-%d'))\n        if not hasattr(self, 'version'):\n            self._set_param('version', self.default_version)\n        # pylint: disable=no-member\n        if not hasattr(self, 'description') or self.description is None:\n            getattr(self, '_set_param')('description', self.warning_message)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move(self):\n        if not os.path.isdir(self.outdir):\n            os.makedirs(self.outdir)\n        shutil.move(self.tmpdir, os.path.join(self.outdir, self.name))", "response": "Move directory from working directory to output directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef example_capture():\n\n    dev = ev(\n        \"CAP_DEVICE\",\n        \"lo\")\n\n    \"\"\"\n    Ignore ports for forwarding to consolidators:\n\n    Redis Internal VM: 6379, 16379\n    RabbitMQ Internal VM: 5672, 15672, 25672\n    \"\"\"\n\n    # http://biot.com/capstats/bpf.html\n    custom_filter = (\"(udp and portrange 10000-17001) \"\n                     \"or (tcp and portrange 80) \"\n                     \"or arp \"\n                     \"or icmp\")\n\n    log.info((\"starting device={} filter={}\")\n             .format(\n                dev,\n                custom_filter))\n\n    kamene.sniff(\n        filter=custom_filter,\n        prn=handle_packets)\n\n    log.info(\"done\")", "response": "example_capture\n    Change the network interface by export CAP_DEVICE = eth0"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncaptures UDP packets and call the handle_packets method.", "response": "def capture_udp_packets():\n    \"\"\"capture_udp_packets\n\n    Capture ``UDP`` packets and call the ``handle_packets`` method\n\n    Change the network interface by ``export CAP_DEVICE=eth0``\n\n    \"\"\"\n    dev = ev(\n        \"CAP_DEVICE\",\n        \"lo\")\n\n    \"\"\"\n    Ignore ports for forwarding to consolidators:\n\n    Redis VM: 6379, 16379\n    RabbitMQ VM: 5672, 15672, 25672\n\n    \"\"\"\n\n    # http://biot.com/capstats/bpf.html\n    default_filter = \"udp\"\n    custom_filter = ev(\n        \"NETWORK_FILTER\",\n        default_filter)\n\n    log.info((\"starting device={} filter={}\")\n             .format(\n                dev,\n                custom_filter))\n\n    kamene.sniff(\n        filter=custom_filter,\n        prn=handle_packets)\n\n    log.info(\"done\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the location of the current Steam installation on Windows machines.", "response": "def find_steam_location():\n  \"\"\"\n  Finds the location of the current Steam installation on Windows machines.\n  Returns None for any non-Windows machines, or for Windows machines where\n  Steam is not installed.\n  \"\"\"\n  if registry is None:\n    return None\n\n  key = registry.CreateKey(registry.HKEY_CURRENT_USER,\"Software\\Valve\\Steam\")\n  return registry.QueryValueEx(key,\"SteamPath\")[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a Scapy packet to JSON", "response": "def convert_pkt_to_json(pkg):\n    \"\"\" convert_pkt_to_json\n    Inspired by:\n    https://gist.githubusercontent.com/cr0hn/1b0c2e672cd0721d3a07/raw/9144676ceb12dbd545e6dce366822bbedde8de2c/pkg_to_json.py\n    This function convert a Scapy packet to JSON\n\n    :param pkg: A kamene package\n    :type pkg: objects\n\n    :return: A JSON data\n    :rtype: dict()\n\n    \"\"\"\n    results = defaultdict(dict)\n\n    try:\n        for index in range(0, len(pkg)):\n\n            layer = pkg[index]\n\n            # Get layer name\n            layer_tmp_name = str(layer.__dict__[\"aliastypes\"][0])\n            layer_start_pos = layer_tmp_name.rfind(\".\") + 1\n            layer_name = layer_tmp_name[layer_start_pos:-2].lower()\n\n            # Get the layer info\n            tmp_t = {}\n            for default_x, y in layer.__dict__[\"default_fields\"].items():\n\n                x = \"default_{}\".format(default_x)\n\n                if DEBUG_PACKETS:\n                    log.info(\"default: key={} val={}\".format(x, y))\n\n                try:\n                    tmp_t[\"hex_default_{}\".format(default_x)] = y.hex()\n                except Exception:\n\n                    # http://python3porting.com/differences.html#long\n                    if y and not isinstance(y, (str,\n                                                int,\n                                                int,\n                                                float,\n                                                list,\n                                                dict)):\n                        if x in tmp_t:\n                            tmp_t[x].update(convert_pkt_to_json(y))\n                        else:\n                            tmp_t[x] = y\n                    else:\n                        tmp_t[x] = y\n            # end of fields\n\n            results[layer_name] = tmp_t\n\n            try:\n                tmp_t = {}\n                for fields_x, y in layer.__dict__[\"fields\"].items():\n\n                    if DEBUG_PACKETS:\n                        log.info(\"fields: key={} val={}\".format(x, y))\n\n                    if fields_x == \"qd\":\n                        if y:\n                            tmp_t[\"fields_qd\"] = json.loads(\n                                    convert_pkt_to_json(y))\n                    elif fields_x == \"ar\":\n                        if y:\n                            tmp_t[\"fields_ar\"] = json.loads(\n                                    convert_pkt_to_json(y))\n                    elif fields_x == \"an\":\n                        if y:\n                            tmp_t[\"fields_an\"] = json.loads(\n                                    convert_pkt_to_json(y))\n                    elif fields_x == \"arcount\":\n                        if y:\n                            tmp_t[\"fields_arcount\"] = json.loads(\n                                    convert_pkt_to_json(y))\n                    elif fields_x == \"ns\":\n                        if y:\n                            \"\"\"\n                            'ns': <DNSRR  rrname='ubuntu.com.'\n                            type=SOA rclass=IN ttl=1345\n                            rdata=b'\\x03ns1\\tcanonical\n                            \\xc0\\x19\\nhostmaster\\xc02xHl\\x8e\n                            \\x00\\x00*0\\x00\\x00\\x0e\\x10\\x00\n                            \\t:\\x80\\x00\\x00\\x0e\\x10' |>,\n                            \"\"\"\n                            tmp_t[\"fields_ns\"] = str(y)\n                    elif fields_x == \"proto\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"flags\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"ack\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"id\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"window\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"dataofs\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"frag\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"reserved\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"ttl\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"chksum\":\n                        if y:\n                            tmp_t[x] = y\n                    elif fields_x == \"options\":\n                        if y:\n                            cur_d = {}\n                            try:\n                                test = dict(y)\n                                if \"EOL\" in test:\n                                    cur_d[\"EOL\"] = test[\"EOL\"]\n                                if \"NOP\" in test:\n                                    cur_d[\"NOP\"] = test[\"NOP\"]\n                                if \"MSS\" in test:\n                                    cur_d[\"MSS\"] = test[\"MSS\"]\n                                if \"WScale\" in test:\n                                    cur_d[\"WScale\"] = test[\"WScale\"]\n                                if \"SAckOK\" in test:\n                                    cur_d[\"SAckOK\"] = \\\n                                            test[\"SAckOK\"].decode(\"utf-8\")\n                                if \"SAck\" in test:\n                                    cur_d[\"SAck\"] = test[\"SAck\"]\n                                if \"Timestamp\" in test:\n                                    if test[\"Timestamp\"]:\n                                        cur_d[\"Timestamp\"] = \\\n                                            test[\"Timestamp\"][0]\n                                if \"AltChkSum\" in test:\n                                    cur_d[\"AltChkSum\"] = test[\"AltChkSum\"]\n                                if \"AltChkSumOpt\" in test:\n                                    cur_d[\"AltChkSumOpt\"] = \\\n                                            test[\"AltChkSumOpt\"]\n                                if \"Mood\" in test:\n                                    cur_d[\"Mood\"] = test[\"Mood\"]\n                                if \"Experiment\" in test:\n                                    cur_d[\"Experiment\"] = test[\"Experiment\"]\n                            except Exception as exct:\n                                log.error((\"1 Failed parsing \"\n                                           \"{}={} ex={}\")\n                                          .format(x,\n                                                  y,\n                                                  exct))\n                                cur_d = str(y)\n                            # end of parsing cur_d\n                            tmp_t[\"fields_{}\".format(fields_x)] = cur_d\n                    elif fields_x == \"urgptr\":\n                        if y:\n                            cur_d = {}\n                            try:\n                                for f in y:\n                                    cur_f = \"{}_{}\".format(fields_x,\n                                                           f)\n                                    try:\n                                        cur_d[cur_f] = y.decode(\"utf-8\")\n                                    except Exception:\n                                        cur_d[\"hex_\" + cur_f] = y[f].hex()\n                            except Exception as exct:\n                                log.error((\"2 Failed parsing \"\n                                           \"{}={} ex={}\")\n                                          .format(x,\n                                                  y,\n                                                  exct))\n                                cur_d = y\n                            # end of parsing cur_d\n                            tmp_t[\"fields_{}\".format(fields_x)] = cur_d\n                    else:\n                        x = \"{}\".format(fields_x)\n\n                        try:\n                            hex_key = \"hex_field_{}\".format(fields_x)\n                            if fields_x == \"load\":\n                                try:\n                                    tmp_t[\"load\"] = y.decode(\"utf-8\")\n                                except Exception:\n                                    tmp_t[hex_key] = y.hex()\n                            else:\n                                tmp_t[hex_key] = y.hex()\n                        except Exception:\n                            # http://python3porting.com/differences.html#long\n                            if y and not isinstance(y, (str,\n                                                        int,\n                                                        int,\n                                                        float,\n                                                        list,\n                                                        dict)):\n                                if x in tmp_t:\n                                    tmp_t[x].update(convert_pkt_to_json(y))\n                                else:\n                                    tmp_t[x] = y\n                            else:\n                                tmp_t[x] = y\n                    # end of special handling:\n                    # qd\n                results[layer_name] = tmp_t\n\n            except KeyError:\n                # No custom fields\n                pass\n\n    except Exception:\n        # Package finish -> do nothing\n        pass\n\n    if \"padding\" in results:\n        try:\n            if \"load\" in results[\"padding\"]:\n                results[\"padding\"][\"load\"] = \\\n                    results[\"padding\"][\"load\"].encode(\"utf-8\").hex()\n        except Exception:\n            log.error((\"failed parsing padding={}\")\n                      .format(results[\"padding\"]))\n    # end of fixing padding\n\n    if \"raw\" in results:\n        try:\n            if \"load\" in results[\"raw\"]:\n                results[\"raw\"][\"load\"] = \\\n                    results[\"raw\"][\"load\"].encode(\"utf-8\").hex()\n        except Exception:\n            log.error((\"failed parsing raw={}\")\n                      .format(results[\"raw\"]))\n    # end of fixing raw\n\n    if DEBUG_PACKETS:\n        log.debug(\"\")\n        log.debug(\"pre json serialization:\")\n        log.debug(results)\n        log.debug(\"post json.dumps:\")\n        log.debug(ppj(results))\n        log.debug(\"\")\n    else:\n        log.info(ppj(results))\n\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nplots PCoA principal coordinates scaled by the relative abundances of the relative abundances of the OTU.", "response": "def plot_PCoA(cat_data, otu_name, unifrac, names, colors, xr, yr, outDir,\n              save_as, plot_style):\n    \"\"\"\n    Plot PCoA principal coordinates scaled by the relative abundances of\n    otu_name.\n    \"\"\"\n    fig = plt.figure(figsize=(14, 8))\n    ax = fig.add_subplot(111)\n\n    for i, cat in enumerate(cat_data):\n        plt.scatter(cat_data[cat][\"pc1\"], cat_data[cat][\"pc2\"], cat_data[cat][\"size\"],\n                    color=colors[cat], alpha=0.85, marker=\"o\", edgecolor=\"black\",\n                    label=cat)\n    lgnd = plt.legend(loc=\"best\", scatterpoints=3, fontsize=13)\n    for i in range(len(colors.keys())):\n        lgnd.legendHandles[i]._sizes = [80]  # Change the legend marker size manually\n    plt.title(\" \".join(otu_name.split(\"_\")), style=\"italic\")\n    plt.ylabel(\"PC2 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][1])))\n    plt.xlabel(\"PC1 (Percent Explained Variance {:.3f}%)\".format(float(unifrac[\"varexp\"][0])))\n    plt.xlim(round(xr[0]*1.5, 1), round(xr[1]*1.5, 1))\n    plt.ylim(round(yr[0]*1.5, 1), round(yr[1]*1.5, 1))\n    if plot_style:\n        gu.ggplot2_style(ax)\n        fc = \"0.8\"\n    else:\n        fc = \"none\"\n    fig.savefig(os.path.join(outDir, \"_\".join(otu_name.split())) + \".\" + save_as,\n                facecolor=fc, edgecolor=\"none\", format=save_as,\n                bbox_inches=\"tight\", pad_inches=0.2)\n    plt.close(fig)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_by_category(biom_cols, mapping, category_id):\n    columns = defaultdict(list)\n    for i, col in enumerate(biom_cols):\n        columns[mapping[col['id']][category_id]].append((i, col))\n\n    return columns", "response": "Split the biom table by mapping category value."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef print_line(l):\n    print_lines = ['# STOCKHOLM', '#=GF', '#=GS', ' ']\n    if len(l.split()) == 0:\n        return True\n    for start in print_lines:\n        if l.startswith(start):\n            return True\n    return False", "response": "print line if starts with..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting stockholm to single line format", "response": "def stock2one(stock):\n    \"\"\"\n    convert stockholm to single line format\n    \"\"\"\n    lines = {}\n    for line in stock:\n        line = line.strip()\n        if print_line(line) is True:\n            yield line\n            continue\n        if line.startswith('//'):\n            continue\n        ID, seq = line.rsplit(' ', 1)\n        if ID not in lines:\n            lines[ID] = ''\n        else:\n            # remove preceding white space\n            seq = seq.strip()\n        lines[ID] += seq\n    for ID, line in lines.items():\n        yield '\\t'.join([ID, line])\n    yield '\\n//'"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprinting stats when pings are done and the current state of the application is up.", "response": "def dump_stats(myStats):\n    \"\"\"\n    Show stats when pings are done\n    \"\"\"\n    print(\"\\n----%s PYTHON PING Statistics----\" % (myStats.thisIP))\n\n    if myStats.pktsSent > 0:\n        myStats.fracLoss = (myStats.pktsSent - myStats.pktsRcvd) \\\n                / myStats.pktsSent\n\n    print((\"%d packets transmitted, %d packets received, \"\n           \"%0.1f%% packet loss\") % (\n        myStats.pktsSent,\n        myStats.pktsRcvd,\n        100.0 * myStats.fracLoss\n    ))\n\n    if myStats.pktsRcvd > 0:\n        print(\"round-trip (ms)  min/avg/max = %d/%0.1f/%d\" % (\n            myStats.minTime,\n            myStats.totTime / myStats.pktsRcvd,\n            myStats.maxTime\n        ))\n\n    print(\"\")\n    return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends ping to destIP with the given timeout and display the result.", "response": "def verbose_ping(hostname, timeout=WAIT_TIMEOUT, count=NUM_PACKETS,\n                 packet_size=PACKET_SIZE, path_finder=False):\n    \"\"\"\n    Send >count< ping to >destIP< with the given >timeout< and display\n    the result.\n    \"\"\"\n    signal.signal(signal.SIGINT, signal_handler)   # Handle Ctrl-C\n    if hasattr(signal, \"SIGBREAK\"):\n        # Handle Ctrl-Break e.g. under Windows\n        signal.signal(signal.SIGBREAK, signal_handler)\n\n    myStats = MyStats()  # Reset the stats\n\n    mySeqNumber = 0  # Starting value\n\n    try:\n        destIP = socket.gethostbyname(hostname)\n        print(\"\\nPYTHON PING %s (%s): %d data bytes\" %\n              (hostname, destIP, packet_size))\n    except socket.gaierror as e:\n        print(\"\\nPYTHON PING: Unknown host: %s (%s)\" %\n              (hostname, e.args[1]))\n        print()\n        return\n\n    myStats.thisIP = destIP\n\n    for i in range(count):\n        delay = do_one(myStats,\n                       destIP,\n                       hostname,\n                       timeout,\n                       mySeqNumber,\n                       packet_size)\n\n        if not delay:\n            delay = 0\n\n        mySeqNumber += 1\n\n        # Pause for the remainder of the MAX_SLEEP period (if applicable)\n        if (MAX_SLEEP > delay):\n            time.sleep((MAX_SLEEP - delay) / 1000)\n\n    dump_stats(myStats)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef updatable(self):\n        if self.latest_version > self.current_version:\n            updatable_version = self.latest_version\n        else:\n            updatable_version = False\n        return updatable_version", "response": "bootstrap - py package updatable?."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntraversing the input OTU - sequence file and return a condensed list of unique OTU IDs and sequence IDs that are associated with the non - unique OTU IDs.", "response": "def condense_otus(otuF, nuniqueF):\n    \"\"\"\n    Traverse the input otu-sequence file, collect the non-unique OTU IDs and\n    file the sequences associated with then under the unique OTU ID as defined\n    by the input matrix.\n\n    :@type otuF: file\n    :@param otuF: The output file from QIIME's pick_otus.py\n    :@type nuniqueF: file\n    :@param nuniqueF: The matrix of unique OTU IDs associated to the list of\n                      non-unique OTU IDs they replaced.\n\n    :@rtype: dict\n    :@return: The new condensed table of unique OTU IDs and the sequence IDs\n              associated with them.\n    \"\"\"\n    uniqueOTUs = set()\n    nuOTUs = {}\n\n    # parse non-unique otu matrix\n    for line in nuniqueF:\n        line = line.split()\n        uOTU = line[0]\n        for nuOTU in line[1:]:\n            nuOTUs[nuOTU] = uOTU\n        uniqueOTUs.add(uOTU)\n\n    otuFilter = defaultdict(list)\n    # parse otu sequence file\n    for line in otuF:\n        line = line.split()\n        otuID, seqIDs = line[0], line[1:]\n        if otuID in uniqueOTUs:\n            otuFilter[otuID].extend(seqIDs)\n        elif otuID in nuOTUs:\n            otuFilter[nuOTUs[otuID]].extend(seqIDs)\n\n    return otuFilter"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rna_bases(rna_cov, scaffold, bases, line):\n    start = int(line[3])\n    stop = start + bases - 1\n    if scaffold not in rna_cov:\n        return rna_cov\n    for pos in rna_cov[scaffold][2]:\n        ol = get_overlap([start, stop], pos)\n        rna_cov[scaffold][0] += ol\n    return rna_cov", "response": "count bases of rna"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses ggKbase scaffold - to - bin mapping parse ggKbase scaffold - to - bin mapping - scaffolds - to - bins and bins - to - scaffolds", "response": "def parse_s2bins(s2bins):\n    \"\"\"\n    parse ggKbase scaffold-to-bin mapping\n        - scaffolds-to-bins and bins-to-scaffolds\n    \"\"\"\n    s2b = {}\n    b2s = {}\n    for line in s2bins:\n        line = line.strip().split()\n        s, b = line[0], line[1]\n        if 'UNK' in b:\n            continue\n        if len(line) > 2:\n            g = ' '.join(line[2:])\n        else:\n            g = 'n/a'\n        b = '%s\\t%s' % (b, g)\n        s2b[s] = b \n        if b not in b2s:\n           b2s[b] = []\n        b2s[b].append(s)\n    return s2b, b2s"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_rna(rna, s2bins, min_rna):\n    rna_cov = {}\n    for seq in parse_fasta(rna):\n        # check that length passes threshold\n        length = len(seq[1])\n        if length < min_rna:\n            continue\n        # check if sequence is binnned\n        s = seq[0].split('>')[1].split()[0]\n        if s not in s2bins:\n            continue\n        if s not in rna_cov:\n            rna_cov[s] = [0, 0, []]\n        position = [int(i) for i in seq[0].rsplit('pos=', 1)[1].split()[0].split('-')]\n        rna_cov[s][2].append(position)\n        rna_cov[s][1] += length\n    return rna_cov", "response": "parse rna file and return a dictionary of the n - grams that are binned by the rna"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove any missing RNA from the given bins2s.", "response": "def filter_missing_rna(s2bins, bins2s, rna_cov):\n    \"\"\"\n    remove any bins that don't have 16S\n    \"\"\"\n    for bin, scaffolds in list(bins2s.items()):\n        c = 0\n        for s in scaffolds:\n            if s in rna_cov:\n                c += 1\n        if c == 0:\n            del bins2s[bin]\n    for scaffold, bin in list(s2bins.items()):\n        if bin not in bins2s:\n            del s2bins[scaffold]\n    return s2bins, bins2s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef copies(mapping, s2bins, rna, min_rna = 800, mismatches = 0):\n    cov = {} # cov[scaffold] = [bases, length]\n    s2bins, bins2s = parse_s2bins(s2bins)\n    rna_cov = parse_rna(rna, s2bins, min_rna)\n    s2bins, bins2s = filter_missing_rna(s2bins, bins2s, rna_cov)\n    # count bases mapped to scaffolds and rRNA gene regions\n    for line in mapping:\n        line = line.strip().split()\n        # get scaffold lengths\n        if line[0].startswith('@'):\n            if line[0].startswith('@SQ') is False:\n                continue\n            s = line[1].split(':')[1]\n            l = int(line[2].split(':')[1])\n            # check if scaffold is binned\n            if s not in s2bins:\n                continue\n            if s not in cov:\n                cov[s] = [0, l]\n        # check mismatch threshold\n        mm = count_mismatches(line)\n        if mm is False or mm > mismatches:\n            continue\n        # check that scaffold is in bin\n        s, bases = line[2], len(line[9])\n        if s not in cov:\n            continue\n        cov[s][0] += bases \n        rna_cov = rna_bases(rna_cov, s, bases, line) \n    print('# mismatches threshold: %s' % (mismatches))\n    header = ['#rRNA scaffold', 'rRNA genes >=%sbp on scaffold' % (min_rna), \\\n            'rRNA coverage', \\\n            'bin', 'bin info', 'bin coverage', \\\n            'rRNAs >=%sbp in bin' % (min_rna), \\\n            'rRNA coverage/bin coverage', \\\n            'estimated number of copies']\n    print('\\t'.join(header))\n    for bin, scaffolds in list(bins2s.items()):\n        rna_count = sum([len(rna_cov[s][2]) for s in scaffolds if s in rna_cov])\n        for s in scaffolds:\n            if s not in rna_cov:\n                continue\n            out = []\n            counts = rna_cov[s]\n            bin_cov = calc_bin_cov(bins2s[bin], cov)\n            num_genes = len(counts[2])\n            rna_coverage = float(float(counts[0])/float(counts[1]))\n            if bin_cov == 0:\n                rna_div_bin = 0\n            else:\n                rna_div_bin = float(rna_coverage/bin_cov)\n            est = int(max([rna_count, counts, rna_div_bin]))\n            out = [s, num_genes, rna_coverage, bin, bin_cov, rna_count, rna_div_bin, est]\n            print('\\t'.join([str(i) for i in out]))", "response": "Determine the number of copies in a single rRNA record."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclean the translation formset.", "response": "def clean(self):\n        \"\"\"\n        Make sure there is at least a translation has been filled in. If a\n        default language has been specified, make sure that it exists amongst\n        translations.\n        \"\"\"\n\n        # First make sure the super's clean method is called upon.\n        super(TranslationFormSet, self).clean()\n\n        if settings.HIDE_LANGUAGE:\n            return\n\n        if len(self.forms) > 0:\n            # If a default language has been provided, make sure a translation\n            # is available\n\n            if settings.DEFAULT_LANGUAGE and not any(self.errors):\n                # Don't bother validating the formset unless each form is\n                # valid on its own. Reference:\n                # http://docs.djangoproject.com/en/dev/topics/forms/formsets/#custom-formset-validation\n\n                for form in self.forms:\n                    language_code = form.cleaned_data.get(\n                        'language_code', None\n                    )\n\n                    if language_code == settings.DEFAULT_LANGUAGE:\n\n                        # All is good, don't bother checking any further\n                        return\n\n                raise forms.ValidationError(_(\n                    'No translation provided for default language \\'%s\\'.'\n                ) % settings.DEFAULT_LANGUAGE)\n\n        else:\n            raise forms.ValidationError(\n                _('At least one translation should be provided.')\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_default_language(self):\n\n        assert hasattr(self, 'available_languages'), \\\n            'No available languages have been generated.'\n        assert len(self.available_languages) > 0, \\\n            'No available languages to select from.'\n\n        if (\n            settings.DEFAULT_LANGUAGE and\n            settings.DEFAULT_LANGUAGE in self.available_languages\n        ) or (\n            'language_code' not in self.form.base_fields\n        ):\n            # Default language still available\n\n            self.available_languages.remove(settings.DEFAULT_LANGUAGE)\n            return settings.DEFAULT_LANGUAGE\n\n        else:\n            # Select the first item and return it\n            return self.available_languages.pop(0)", "response": "Returns the default language for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _construct_form(self, i, **kwargs):\n        if not settings.HIDE_LANGUAGE:\n            self._construct_available_languages()\n\n        form = super(TranslationFormSet, self)._construct_form(i, **kwargs)\n\n        if settings.HIDE_LANGUAGE:\n            form.instance.language_code = settings.DEFAULT_LANGUAGE\n        else:\n            language_code = form.instance.language_code\n\n            if language_code:\n                logger.debug(\n                    u'Removing translation choice %s for instance %s'\n                    u' in form %d', language_code, form.instance, i\n                )\n\n                self.available_languages.remove(language_code)\n\n            else:\n                initial_language_code = self._get_default_language()\n\n                logger.debug(\n                    u'Preselecting language code %s for form %d',\n                    initial_language_code, i\n                )\n\n                form.initial['language_code'] = initial_language_code\n\n        return form", "response": "Construct the form overriding the initial value for language_code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_by_sample_pct(otus, nsamples, pct, phyl_level):\n    if phyl_level not in ['k', 'p', 'c', 'o', 'f', 'g', 's']:\n        phyl_level = 's'\n    nsamples = float(nsamples)\n    sample_counts = defaultdict(set)\n    # count the number of sequences per OTU\n    for otuid in otus:\n        phyl = util.split_phylogeny(otus[otuid][0], phyl_level)\n        samples = {seqid.split('_')[0] for seqid in otus[otuid][1]}\n        sample_counts[phyl].update(samples)\n    sample_counts = {phyl: len(sample_counts[phyl])/nsamples\n                     for phyl in sample_counts}\n\n    # separate OTUs\n    above = {}\n    below = {}\n    for otuid in otus:\n        phyl = util.split_phylogeny(otus[otuid][0], phyl_level)\n        if sample_counts[phyl] >= pct:\n            above[otuid] = otus[otuid]\n        else:\n            below[otuid] = [sample_counts[phyl], '',\n                            otus[otuid][0], otus[otuid][1]]\n\n    return above, below", "response": "Filter the OTUs by the percentage of samples."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfiltering the OTUs by sequence percentages.", "response": "def filter_by_sequence_pct(otus, nseqs, pct, phyl_level):\n    \"\"\"\n    Split the list of OTUs (and associated sequence ids) into two lists:\n    those occurring associated with more than some percentage of total sequences\n    and those less than the cutoff.\n\n    :type otus: dict\n    :param otus: {otuid: [taxonomy, [sequence IDs]]}\n    :type nseqs: int\n    :param nseqs: The total number of sequences in the data set\n    :type pct: float\n    :param pct: The cutoff percentage for inclusion in the filtered\n                 set of OTUs\n    :type phyl_level: str\n    :param phyl_level: The phylogenetic level (e.g. family, group, etc...) at\n                       which to combine OTU counts for thresholding. One of\n                       the following: ['k','p','c','o','f','g','s']\n\n    :rtype: tuple\n    :return: Two dicts: the OTU IDs and sequence IDs above and below the\n              percentage threshold.\n    \"\"\"\n    if phyl_level not in ['k', 'p', 'c', 'o', 'f', 'g', 's']:\n        phyl_level = 's'\n    seq_counts = defaultdict(int)\n    nseqs = float(nseqs)\n    # gather counts\n    for oid in otus:\n        phyl = util.split_phylogeny(otus[oid][0], phyl_level)\n        seq_counts[phyl] += len(otus[oid][1])\n    seq_counts = {phyl: seq_counts[phyl]/nseqs for phyl in seq_counts}\n\n    # separate OTUs\n    above = {}\n    below = {}\n    for otuid in otus:\n        phyl = util.split_phylogeny(otus[otuid][0], phyl_level)\n        if seq_counts[phyl] >= pct:\n            above[otuid] = otus[otuid]\n        else:\n            below[otuid] = ['', seq_counts[phyl],\n                            otus[otuid][0], otus[otuid][1]]\n\n    return above, below"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fq_merge(R1, R2):\n    c = itertools.cycle([1, 2, 3, 4])\n    for r1, r2 in zip(R1, R2):\n        n = next(c)\n        if n == 1:\n            pair = [[], []]\n        pair[0].append(r1.strip())\n        pair[1].append(r2.strip())\n        if n == 4:\n            yield pair", "response": "merge two fastq files into one fastq file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_circle(self):\n        total_weight = 0\n        for node in self._nodes:\n            total_weight += self._weights.get(node, 1)\n\n        for node in self._nodes:\n            weight = self._weights.get(node, 1)\n\n            ks = math.floor((40 * len(self._nodes) * weight) / total_weight)\n\n            for i in xrange(0, int(ks)):\n                b_key = self._md5_digest('%s-%s-salt' % (node, i))\n\n                for l in xrange(0, 4):\n                    key = ((b_key[3 + l * 4] << 24)\n                           | (b_key[2 + l * 4] << 16)\n                           | (b_key[1 + l * 4] << 8)\n                           | b_key[l * 4])\n\n                    self._hashring[key] = node\n                    self._sorted_keys.append(key)\n\n        self._sorted_keys.sort()", "response": "Builds the circle for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_node_pos(self, key):\n        if not self._hashring:\n            return None\n\n        key = self._gen_key(key)\n\n        nodes = self._sorted_keys\n        pos = bisect(nodes, key)\n\n        if pos == len(nodes):\n            return 0\n        return pos", "response": "Return the node position for a given key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a key that represents it place on", "response": "def _gen_key(self, key):\n        \"\"\"\n            Return long integer for a given key, that represent it place on\n            the hash ring.\n        \"\"\"\n        b_key = self._md5_digest(key)\n        return self._hashi(b_key, lambda x: x)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_node(self, node):\n        try:\n            self._nodes.remove(node)\n            del self._weights[node]\n        except (KeyError, ValueError):\n            pass\n        self._hashring = dict()\n        self._sorted_keys = []\n\n        self._build_circle()", "response": "Removes node from the circle and rebuilds the circle."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a node to the circle and rebuilds it.", "response": "def add_node(self, node, weight=1):\n        \"\"\"\n            Adds node to circle and rebuild it.\n        \"\"\"\n        self._nodes.add(node)\n        self._weights[node] = weight\n        self._hashring = dict()\n        self._sorted_keys = []\n\n        self._build_circle()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a complex nested json dictionary containing ethernet messages into a flattened dictionary and capture all unique keys for table construction", "response": "def process_ether_frame(self,\n                            id=None,\n                            msg=None):\n        \"\"\"process_ether_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: ether frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"eth_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.eth_keys:\n                self.eth_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"eth_id\"] = id\n        self.all_eth.append(dt)\n\n        log.debug(\"ETHER data updated:\")\n        log.debug(self.eth_keys)\n        log.debug(self.all_eth)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a complex nested json dictionary containing the ip_id and the message to a flattened dictionary and capture all unique keys for table construction", "response": "def process_ip_frame(self,\n                         id=None,\n                         msg=None):\n        \"\"\"process_ip_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: ip frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"ip_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.ip_keys:\n                self.ip_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"ip_id\"] = id\n        self.all_ip.append(dt)\n\n        log.debug(\"IP data updated:\")\n        log.debug(self.ip_keys)\n        log.debug(self.all_ip)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a complex nested json dictionary containing the ipv6 frame into a flattened dictionary and capture all unique keys for table construction", "response": "def process_ipvsix_frame(self,\n                             id=None,\n                             msg=None):\n        \"\"\"process_ipvsix_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: ipv6 frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"ipv6_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.ipvsix_keys:\n                self.ipvsix_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"ipv6_id\"] = id\n        self.all_ipvsix.append(dt)\n\n        log.debug(\"IPV6 data updated:\")\n        log.debug(self.ipvsix_keys)\n        log.debug(self.all_ipvsix)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a complex nested json dictionary containing a tcp frame and a flattened dictionary containing all unique keys for table construction and capture all unique keys for table construction", "response": "def process_tcp_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_tcp_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: tcp frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"tcp_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.tcp_keys:\n                self.tcp_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"tcp_id\"] = id\n        self.all_tcp.append(dt)\n\n        log.debug(\"TCP data updated:\")\n        log.debug(self.tcp_keys)\n        log.debug(self.all_tcp)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef process_udp_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_udp_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: udp frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"udp_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.udp_keys:\n                self.udp_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"udp_id\"] = id\n        self.all_udp.append(dt)\n\n        log.debug(\"UDP data updated:\")\n        log.debug(self.udp_keys)\n        log.debug(self.all_udp)\n        log.debug(\"\")\n\n        return flat_msg", "response": "Convert a complex nested json dictionary containing a dict containing a dict containing a dict containing a dict containing a dict containing a dict containing the udp_id and the udp_msg."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a complex nested json dictionary containing a dns frame and capture all unique keys for table construction", "response": "def process_dns_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_dns_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: dns frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"dns_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.dns_keys:\n                self.dns_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"dns_id\"] = id\n        self.all_dns.append(dt)\n\n        log.debug(\"DNS data updated:\")\n        log.debug(self.dns_keys)\n        log.debug(self.all_dns)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a complex nested json dictionary containing icmp_id and icmp_msg to a flattened dictionary and capture all unique keys for table construction", "response": "def process_icmp_frame(self,\n                           id=None,\n                           msg=None):\n        \"\"\"process_icmp_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: icmp frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"icmp_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.icmp_keys:\n                self.icmp_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"icmp_id\"] = id\n        self.all_icmp.append(dt)\n\n        log.debug(\"ICMP data updated:\")\n        log.debug(self.icmp_keys)\n        log.debug(self.all_icmp)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_arp_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_arp_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: arp frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"arp_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.arp_keys:\n                self.arp_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"arp_id\"] = id\n        self.all_arp.append(dt)\n\n        log.debug(\"ARP data updated:\")\n        log.debug(self.arp_keys)\n        log.debug(self.all_arp)\n        log.debug(\"\")\n\n        return flat_msg", "response": "Convert a complex nested json dictionary containing arp_id and arp_msg to a flattened dictionary and capture all unique keys for table construction\n       "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_raw_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_raw_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: raw frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"raw_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.raw_keys:\n                self.raw_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"raw_id\"] = id\n        self.all_raw.append(dt)\n\n        log.debug(\"RAW data updated:\")\n        log.debug(self.raw_keys)\n        log.debug(self.all_raw)\n        log.debug(\"\")\n\n        return flat_msg", "response": "Convert a complex nested json dictionary to a flattened dictionary and capture all unique keys for table construction\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a complex nested json dictionary containing a pad frame and capture all unique keys for table construction", "response": "def process_pad_frame(self,\n                          id=None,\n                          msg=None):\n        \"\"\"process_pad_frame\n\n        Convert a complex nested json dictionary\n        to a flattened dictionary and capture\n        all unique keys for table construction\n\n        :param id: key for this msg\n        :param msg: pad frame for packet\n        \"\"\"\n\n        # normalize into a dataframe\n        df = json_normalize(msg)\n        # convert to a flattened dictionary\n        dt = json.loads(df.to_json())\n\n        flat_msg = {}\n\n        for k in dt:\n            new_key = \"pad_{}\".format(k)\n            flat_msg[new_key] = dt[k][\"0\"]\n            if new_key not in self.pad_keys:\n                self.pad_keys[new_key] = k\n        # end of capturing all unique keys\n\n        dt[\"pad_id\"] = id\n        self.all_pad.append(dt)\n\n        log.debug(\"PAD data updated:\")\n        log.debug(self.pad_keys)\n        log.debug(self.all_pad)\n        log.debug(\"\")\n\n        return flat_msg"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_flat_msg(self,\n                       id=None,\n                       msg=None):\n        \"\"\"build_flat_msg\n\n        :param id: unique id for this message\n        :param msg: message dictionary to flatten\n        \"\"\"\n\n        flat_msg = {}\n\n        if not id:\n            log.error(\"Please pass in an id\")\n            return None\n        if not msg:\n            log.error(\"Please pass in a msg\")\n            return None\n\n        for k in msg[\"data\"]:\n            if k == \"ether\":\n                flat_msg.update(self.process_ether_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of ether\n            elif k == \"ip\":\n                flat_msg.update(self.process_ip_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of ip\n            elif k == \"ipv6\":\n                flat_msg.update(self.process_ipvsix_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of ipv6\n            elif k == \"tcp\":\n                flat_msg.update(self.process_tcp_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of tcp\n            elif k == \"udp\":\n                flat_msg.update(self.process_udp_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of udp\n            elif k == \"dns\":\n                flat_msg.update(self.process_dns_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of dns\n            elif k == \"icmp\":\n                flat_msg.update(self.process_icmp_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of icmp\n            elif k == \"arp\":\n                flat_msg.update(self.process_arp_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of arp\n            elif k == \"raw\":\n                flat_msg.update(self.process_raw_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of raw\n            elif k == \"padding\":\n                flat_msg.update(self.process_pad_frame(\n                                    id=id,\n                                    msg=msg[\"data\"][k]))\n            # end of pad\n            else:\n                log.error((\"Unsupported frame type={} \"\n                           \"please file an issue to track this \"\n                           \"with data={} msg={}\")\n                          .format(k,\n                                  ppj(msg[\"data\"][k]),\n                                  msg[\"data\"]))\n        # end of processing new message\n\n        return flat_msg", "response": "Builds a flat message from a message dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build_all_keys_dict(self):\n\n        log.info(\"finding keys\")\n        for k in self.eth_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all eths\n        for k in self.ip_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all ips\n        for k in self.ipvsix_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all ipvsixs\n        for k in self.icmp_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all icmps\n        for k in self.arp_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all arps\n        for k in self.tcp_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all tcps\n        for k in self.udp_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all udps\n        for k in self.dns_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all dnss\n        for k in self.raw_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all raws\n        for k in self.pad_keys:\n            ak = \"{}\".format(k)\n            if ak not in self.all_keys:\n                self.all_keys[ak] = k\n        # end of building all pads\n\n        # this will be the columns for the csv\n        for k in self.all_keys:\n            self.all_keys_list.append(k)\n\n        log.debug((\"unique all_keys keys={} values={}\")\n                  .format(len(self.all_keys_list),\n                          self.all_keys))", "response": "build the dict of all keys that are not in self. all_keys"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatten_all(self):\n\n        log.info(\"flattening - START\")\n\n        self.all_rows = []\n\n        for idx, r in enumerate(self.all_flat):\n\n            new_row = {\"idx\": idx}\n\n            for k in self.all_keys_list:\n                if k in r:\n                    new_row[k] = r[k]\n                else:\n                    new_row[k] = None\n            # end of for all keys\n\n            self.all_rows.append(new_row)\n\n        # end of all_keys\n\n        log.info(\"flattening - END\")", "response": "flatten all keys in the database"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the data from the all rows into a dataframe", "response": "def convert_to_df(self):\n        \"\"\"convert_to_df\"\"\"\n\n        log.info((\"converting={}\")\n                 .format(len(self.all_rows)))\n\n        if len(self.all_rows) == 0:\n            return\n\n        self.df = pd.DataFrame(self.all_rows).set_index(\"idx\")\n\n        if len(self.df) != len(self.all_rows):\n            log.error((\"Failed converting={} to rows={}\")\n                      .format(len(self.all_rows),\n                              len(self.df)))\n        else:\n            log.info((\"converted={} into rows={}\")\n                     .format(len(self.all_rows),\n                             len(self.df)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef write_to_file(self,\n                      data_dict,\n                      output_file_path):\n        \"\"\"write_to_file\n\n        :param data_dict:\n        :param output_file_path:\n        \"\"\"\n\n        log.info(\"saving={}\".format(output_file_path))\n        with open(output_file_path, \"w\") as output_file:\n            output_file.write(str(ppj(data_dict)))", "response": "Writes the data dictionary to the file at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_df_as_csv(self):\n\n        if len(self.all_rows) == 0:\n            log.info((\"no df={} to save\")\n                     .format(self.df))\n            return\n        else:\n\n            log.info((\"saving \"\n                      \"packets={} file={} rows={}\")\n                     .format(len(self.recv_msgs),\n                             self.save_to_file,\n                             len(self.df)))\n\n            self.df.to_csv(self.save_to_file,\n                           sep=\",\",\n                           encoding=\"utf-8\",\n                           index=True)\n\n            log.info((\"done saving={}\")\n                     .format(self.save_to_file))", "response": "save_df_as_csv - save the df as a CSV file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_data(self):\n\n        state = \"\"\n        try:\n\n            state = \"create_json_archive\"\n            log.info(\"creating json archive\")\n            self.create_json_archive()\n\n            state = \"building_unique_keys\"\n            log.info(\"processing all unique keys\")\n            self.build_all_keys_dict()\n\n            state = \"flattening\"\n            log.info(\"flattening all data\")\n            self.flatten_all()\n\n            state = \"converting\"\n            log.info(\"converting to df\")\n            self.convert_to_df()\n\n            state = \"saving\"\n            log.info(\"saving to df\")\n            self.save_df_as_csv()\n\n            if ANTINEX_PUBLISH_ENABLED:\n                log.info((\"publishing stream to rest={}\")\n                         .format(\n                            ANTINEX_URL))\n                self.publish_predictions_to_core()\n            # end of if publishing to the core\n\n        except Exception as e:\n            log.error((\"Failed state={} with ex={} to \"\n                       \"save={}\")\n                      .format(state,\n                              e,\n                              self.save_to_file))", "response": "save data to file"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_msg(self,\n                   body,\n                   org_message):\n        \"\"\"handle_msg\n\n        :param body: dictionary contents from the message body\n        :param org_message: message object can ack, requeue or reject\n        \"\"\"\n\n        if os.path.exists(self.stop_for_file):\n            log.info((\"Detected stop_file={} \"\n                      \"shutting down\")\n                     .format(self.stop_for_file))\n\n            # drop the message back in the queue\n            # for next time\n            org_message.requeue()\n\n            sys.exit(1)\n        # end of stop file detection\n\n        try:\n\n            log.debug((\"handle body={}\")\n                      .format(ppj(body)))\n\n            msg = body\n            id = build_packet_key()\n            recv_time = rnow()\n\n            # this could be made into celery tasks...\n\n            flat_msg = self.build_flat_msg(\n                            id=id,\n                            msg=msg)\n\n            if not flat_msg:\n                log.error((\"Failed to build a flat message \"\n                           \"for message={}\")\n                          .format(msg))\n                return\n\n            msg[\"id\"] = id\n            msg[\"received\"] = recv_time\n            if len(flat_msg) > 0:\n                if self.debug:\n                    log.info(ppj(flat_msg))\n                flat_msg[\"id\"] = id\n                flat_msg[\"received\"] = recv_time\n                self.all_flat.append(flat_msg)\n                self.recv_msgs.append(msg)\n            # end of adding all flat messages\n\n            already_saved = False\n            num_recv = len(self.recv_msgs)\n            if (num_recv % self.save_after_num) == 0:\n                already_saved = False\n                self.save_data()\n            # end of saving a snapshot\n\n            if self.stop_after_num:\n                if num_recv >= self.stop_after_num:\n                    if not already_saved:\n                        self.save_data()\n                    # avoid waiting on the save again\n                    log.info(\"archive successful - purging buffer\")\n                    sys.exit(2)\n                # shutdown - good for testing\n            # if now set up for infinite consuming\n\n        except Exception as e:\n            log.error((\"Failed processing msg={} \"\n                       \"ex={}\")\n                      .format(body,\n                              e))\n        # end of processing message\n\n        try:\n            org_message.ack()\n        except Exception as e:\n            log.error((\"Failed ack-ing msg={} \"\n                       \"ex={}\")\n                      .format(body,\n                              e))\n        # end of acknowleding message was processed\n\n        log.info(\"done handle\")", "response": "handle_msg - handles a single message from the queue"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npublishes predictions to core", "response": "def publish_predictions_to_core(self):\n        \"\"\"publish_predictions_to_core\"\"\"\n\n        status = FAILED\n        msg = \"not started\"\n\n        try:\n\n            msg = \"generating request\"\n            log.info(msg)\n\n            # noqa https://stackoverflow.com/questions/29815129/pandas-dataframe-to-list-of-dictionaries\n            publish_req = generate_ai_request(\n                predict_rows=self.df.fillna(\n                    ANTINEX_MISSING_VALUE).to_dict(\"records\"),\n                req_dict=self.request_dict)\n\n            if publish_req[\"status\"] != SUCCESS:\n                log.error((\"failed generate_ai_request with err={}\")\n                          .format(\n                            publish_req[\"error\"]))\n                status = ERROR\n\n            else:\n\n                msg = \"publishing as user={} url={} model={}\".format(\n                    ANTINEX_USER,\n                    ANTINEX_URL,\n                    ANTINEX_USE_MODEL_NAME)\n\n                log.info(msg)\n\n                response = self.client.run_job(\n                    body=publish_req[\"data\"])\n\n                if response[\"status\"] == SUCCESS:\n                    log.info(\"predictions sent\")\n                    status = SUCCESS\n                elif response[\"status\"] == FAILED:\n                    log.error((\"job failed with error='{}' with response={}\")\n                              .format(\n                                response[\"error\"],\n                                response[\"data\"]))\n                    status = ERROR\n                elif response[\"status\"] == ERROR:\n                    log.error((\"job had an error='{}' with response={}\")\n                              .format(\n                                response[\"error\"],\n                                response[\"data\"]))\n                    status = ERROR\n                elif response[\"status\"] == LOGIN_FAILED:\n                    log.error((\"job reported user was not able to log in \"\n                               \"with an error='{}' with response={}\")\n                              .format(\n                                response[\"error\"],\n                                response[\"data\"]))\n                    status = ERROR\n                # logging for good/bad cases during publish\n            # if generated a good request\n        except Exception as e:\n            log.error((\"failed generating request last_step='{}' ex={}\")\n                      .format(\n                        msg,\n                        e))\n        # end of try/ex\n\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the internal data map for the ISO - 19115 standard.", "response": "def _init_data_map(self):\n        \"\"\" OVERRIDDEN: Initialize required FGDC data map with XPATHS and specialized functions \"\"\"\n\n        if self._data_map is not None:\n            return  # Initiation happens once\n\n        # Parse and validate the FGDC metadata root\n\n        if self._xml_tree is None:\n            fgdc_root = FGDC_ROOT\n        else:\n            fgdc_root = get_element_name(self._xml_tree)\n\n        if fgdc_root != FGDC_ROOT:\n            raise InvalidContent('Invalid XML root for ISO-19115 standard: {root}', root=fgdc_root)\n\n        fgdc_data_map = {'_root': FGDC_ROOT}\n        fgdc_data_structures = {}\n\n        # Capture and format other complex XPATHs\n\n        ad_format = _fgdc_tag_formats[ATTRIBUTES]\n        fgdc_data_structures[ATTRIBUTES] = format_xpaths(\n            _fgdc_definitions[ATTRIBUTES],\n            label=ad_format.format(ad_path='attrlabl'),\n            aliases=ad_format.format(ad_path='attalias'),\n            definition=ad_format.format(ad_path='attrdef'),\n            definition_src=ad_format.format(ad_path='attrdefs')\n        )\n\n        bb_format = _fgdc_tag_formats[BOUNDING_BOX]\n        fgdc_data_structures[BOUNDING_BOX] = format_xpaths(\n            _fgdc_definitions[BOUNDING_BOX],\n            east=bb_format.format(bbox_path='eastbc'),\n            south=bb_format.format(bbox_path='southbc'),\n            west=bb_format.format(bbox_path='westbc'),\n            north=bb_format.format(bbox_path='northbc')\n        )\n\n        ct_format = _fgdc_tag_formats[CONTACTS]\n        fgdc_data_structures[CONTACTS] = format_xpaths(\n            _fgdc_definitions[CONTACTS],\n\n            name=ct_format.format(ct_path='cntperp/cntper'),\n            _name=ct_format.format(ct_path='cntorgp/cntper'),  # If not in cntperp\n\n            organization=ct_format.format(ct_path='cntperp/cntorg'),\n            _organization=ct_format.format(ct_path='cntorgp/cntorg'),  # If not in cntperp\n\n            position=ct_format.format(ct_path='cntpos'),\n            email=ct_format.format(ct_path='cntemail')\n        )\n\n        dt_format = _fgdc_tag_formats[DATES]\n        fgdc_data_structures[DATES] = {\n            DATE_TYPE_MULTIPLE: dt_format.format(type_path='mdattim/sngdate/caldate'),\n            DATE_TYPE_RANGE_BEGIN: dt_format.format(type_path='rngdates/begdate'),\n            DATE_TYPE_RANGE_END: dt_format.format(type_path='rngdates/enddate'),\n            DATE_TYPE_SINGLE: dt_format.format(type_path='sngdate/caldate')\n        }\n        fgdc_data_structures[DATES][DATE_TYPE_RANGE] = [\n            fgdc_data_structures[DATES][DATE_TYPE_RANGE_BEGIN],\n            fgdc_data_structures[DATES][DATE_TYPE_RANGE_END]\n        ]\n\n        df_format = _fgdc_tag_formats[DIGITAL_FORMS]\n        fgdc_data_structures[DIGITAL_FORMS] = format_xpaths(\n            _fgdc_definitions[DIGITAL_FORMS],\n            name=df_format.format(df_path='digtinfo/formname'),\n            content=df_format.format(df_path='digtinfo/formcont'),\n            decompression=df_format.format(df_path='digtinfo/filedec'),\n            version=df_format.format(df_path='digtinfo/formvern'),\n            specification=df_format.format(df_path='digtinfo/formspec'),\n            access_desc=df_format.format(df_path='digtopt/onlinopt/oncomp'),\n            access_instrs=df_format.format(df_path='digtopt/onlinopt/accinstr'),\n            network_resource=df_format.format(df_path='digtopt/onlinopt/computer/networka/networkr')\n        )\n\n        lw_format = _fgdc_tag_formats[LARGER_WORKS]\n        fgdc_data_structures[LARGER_WORKS] = format_xpaths(\n            _fgdc_definitions[LARGER_WORKS],\n            title=lw_format.format(lw_path='title'),\n            edition=lw_format.format(lw_path='edition'),\n            origin=lw_format.format(lw_path='origin'),\n            online_linkage=lw_format.format(lw_path='onlink'),\n            other_citation=lw_format.format(lw_path='othercit'),\n            date=lw_format.format(lw_path='pubdate'),\n            place=lw_format.format(lw_path='pubinfo/pubplace'),\n            info=lw_format.format(lw_path='pubinfo/publish')\n        )\n\n        ps_format = _fgdc_tag_formats[PROCESS_STEPS]\n        fgdc_data_structures[PROCESS_STEPS] = format_xpaths(\n            _fgdc_definitions[PROCESS_STEPS],\n            description=ps_format.format(ps_path='procdesc'),\n            date=ps_format.format(ps_path='procdate'),\n            sources=ps_format.format(ps_path='srcused')\n        )\n\n        ri_format = _fgdc_tag_formats[RASTER_INFO]\n        fgdc_data_structures[RASTER_INFO] = format_xpaths(\n            _fgdc_definitions[RASTER_INFO],\n\n            dimensions=ri_format.format(ri_path='rasttype'),\n            row_count=ri_format.format(ri_path='rowcount'),\n            column_count=ri_format.format(ri_path='colcount'),\n            vertical_count=ri_format.format(ri_path='vrtcount'),\n\n            x_resolution=_fgdc_tag_formats['_raster_resolution'] + '/absres',\n            _x_resolution=_fgdc_tag_formats['__raster_resolution'] + '/longres',\n            y_resolution=_fgdc_tag_formats['_raster_resolution'] + '/ordres',\n            _y_resolution=_fgdc_tag_formats['__raster_resolution'] + '/latres',\n        )\n\n        # Assign XPATHS and gis_metadata.utils.ParserProperties to fgdc_data_map\n\n        fgdc_data_formats = dict(_fgdc_tag_formats)\n\n        for prop, xpath in iteritems(fgdc_data_formats):\n            if prop in (ATTRIBUTES, CONTACTS, DIGITAL_FORMS, PROCESS_STEPS):\n                fgdc_data_map[prop] = ParserProperty(self._parse_complex_list, self._update_complex_list)\n\n            elif prop in (BOUNDING_BOX, LARGER_WORKS):\n                fgdc_data_map[prop] = ParserProperty(self._parse_complex, self._update_complex)\n\n            elif prop == DATES:\n                fgdc_data_map[prop] = ParserProperty(self._parse_dates, self._update_dates)\n\n            elif prop == RASTER_INFO:\n                fgdc_data_map[prop] = ParserProperty(self._parse_complex, self._update_raster_info)\n\n            else:\n                fgdc_data_map[prop] = xpath\n\n        self._data_map = fgdc_data_map\n        self._data_structures = fgdc_data_structures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the raster_info and raster_res structures based on the properties given.", "response": "def _update_raster_info(self, **update_props):\n        \"\"\" Ensures complete removal of raster_info given the two roots: <spdoinfo> and <spref> \"\"\"\n\n        xpath_map = self._data_structures[update_props['prop']]\n\n        return [\n            update_complex(xpath_root=self._data_map.get('_raster_info_root'), xpath_map=xpath_map, **update_props),\n            update_complex(xpath_root=self._data_map.get('__raster_res_root'), xpath_map=xpath_map, **update_props)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_custom_image(user_context, app_id):\n  possible_paths = _valid_custom_image_paths(user_context, app_id)\n  return any(map(os.path.exists, possible_paths))", "response": "Returns True if there exists a custom image for app_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the custom image associated with a given app.", "response": "def get_custom_image(user_context, app_id):\n  \"\"\"Returns the custom image associated with a given app. If there are\n  multiple candidate images on disk, one is chosen arbitrarily.\"\"\"\n  possible_paths = _valid_custom_image_paths(user_context, app_id)\n  existing_images = filter(os.path.exists, possible_paths)\n  if len(existing_images) > 0:\n    return existing_images[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_file(cls, fname, form=None):\n        try:\n            tg = TableGroup.from_file(fname)\n            opfname = None\n        except JSONDecodeError:\n            tg = TableGroup.fromvalue(cls.MD)\n            opfname = fname\n        if len(tg.tables) != 1:\n            raise ValueError('profile description must contain exactly one table')\n        metadata = tg.common_props\n        metadata.update(fname=Path(fname), form=form)\n        return cls(\n            *[{k: None if (k != cls.GRAPHEME_COL and v == cls.NULL) else v for k, v in d.items()}\n              for d in tg.tables[0].iterdicts(fname=opfname)],\n            **metadata)", "response": "Read an orthography profile from a metadata file or a default tab - separated profile file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Profile instance from the Unicode graphemes found in text.", "response": "def from_text(cls, text, mapping='mapping'):\n        \"\"\"\n        Create a Profile instance from the Unicode graphemes found in `text`.\n\n        Parameters\n        ----------\n        text\n        mapping\n\n        Returns\n        -------\n        A Profile instance.\n\n        \"\"\"\n        graphemes = Counter(grapheme_pattern.findall(text))\n        specs = [\n            OrderedDict([\n                (cls.GRAPHEME_COL, grapheme),\n                ('frequency', frequency),\n                (mapping, grapheme)])\n            for grapheme, frequency in graphemes.most_common()]\n        return cls(*specs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef split_fasta(f, id2f):\n    opened = {}\n    for seq in parse_fasta(f):\n        id = seq[0].split('>')[1].split()[0]\n        if id not in id2f:\n            continue\n        fasta = id2f[id]\n        if fasta not in opened:\n            opened[fasta] = '%s.fa' % fasta\n        seq[1] += '\\n'\n        with open(opened[fasta], 'a+') as f_out:\n            f_out.write('\\n'.join(seq))", "response": "split fasta file into separate fasta files based on list of scaffolds\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _is_user_directory(self, pathname):\n      fullpath = os.path.join(self.userdata_location(), pathname)\n      # SteamOS puts a directory named 'anonymous' in the userdata directory\n      # by default. Since we assume that pathname is a userID, ignore any name\n      # that can't be converted to a number\n      return os.path.isdir(fullpath) and pathname.isdigit()", "response": "Check whether the given pathname is a user directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an array of user ids for users on the filesystem", "response": "def local_users(self):\n        \"\"\"Returns an array of user ids for users on the filesystem\"\"\"\n        # Any users on the machine will have an entry inside of the userdata\n        # folder. As such, the easiest way to find a list of all users on the\n        # machine is to just list the folders inside userdata\n        userdirs = filter(self._is_user_directory, os.listdir(self.userdata_location()))\n        # Exploits the fact that the directory is named the same as the user id\n        return map(lambda userdir: user.User(self, int(userdir)), userdirs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef calculate_temperature_equivalent(temperatures):\n\n    ret = 0.6*temperatures + 0.3*temperatures.shift(1) + 0.1*temperatures.shift(2)\n    ret.name = 'temp_equivalent'\n    return ret", "response": "Calculates the temperature equivalent from a series of average daily temperatures."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the degree days of a single resource in a series of temperature equivalent values.", "response": "def _calculate_degree_days(temperature_equivalent, base_temperature, cooling=False):\n    \"\"\"\n    Calculates degree days, starting with a series of temperature equivalent values\n\n    Parameters\n    ----------\n    temperature_equivalent : Pandas Series\n    base_temperature : float\n    cooling : bool\n        Set True if you want cooling degree days instead of heating degree days\n\n    Returns\n    -------\n    Pandas Series called HDD_base_temperature for heating degree days or\n    CDD_base_temperature for cooling degree days.\n    \"\"\"\n\n    if cooling:\n        ret = temperature_equivalent - base_temperature\n    else:\n        ret = base_temperature - temperature_equivalent\n\n    # degree days cannot be negative\n    ret[ret < 0] = 0\n\n    prefix = 'CDD' if cooling else 'HDD'\n    ret.name = '{}_{}'.format(prefix, base_temperature)\n\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the degree - days for a given set of base temperatures.", "response": "def compute_degree_days(ts, heating_base_temperatures, cooling_base_temperatures):\n    \"\"\"\n    Compute degree-days for heating and/or cooling\n\n    Parameters\n    ----------\n    ts : pandas.Series\n        Contains ambient (outside) temperature. Series name (ts.name) does not matter.\n    heating_base_temperatures: list\n        For each base temperature the heating degree-days will be computed\n    cooling_base_temperatures: list\n        For each base temperature the cooling degree-days will be computed\n\n    Returns\n    -------\n    df: pandas.DataFrame with DAILY resolution and the following columns:\n        temp_equivalent and columns HDD_baseT and CDD_baseT for each of the given base temperatures.\n    \"\"\"\n\n    # verify the sampling rate: should be at least daily.\n    mean_sampling_rate = (ts.index[-1] - ts.index[0]).total_seconds()/(len(ts)-1)\n    if int(mean_sampling_rate/86400.) > 1:\n        raise UnexpectedSamplingRate(\"The sampling rate should be daily or shorter but found sampling rate: {}s\".format(mean_sampling_rate))\n\n    ts_day = ts.resample(rule='D').mean()\n    df = pd.DataFrame(calculate_temperature_equivalent(ts_day))\n\n    for base in heating_base_temperatures:\n        df = pd.concat([df, _calculate_degree_days(temperature_equivalent=df['temp_equivalent'], base_temperature=base)], axis=1)\n\n    for base in cooling_base_temperatures:\n        df = pd.concat([df, _calculate_degree_days(temperature_equivalent=df['temp_equivalent'], base_temperature=base, cooling=True)],\n                       axis=1)\n\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef calcMD5(path):\n    # check that file exists\n    if os.path.exists(path) is False:\n        yield False\n    else:\n        command = ['md5sum', path]\n        p = Popen(command, stdout = PIPE)\n        for line in p.communicate()[0].splitlines():\n            yield line.decode('ascii').strip().split()[0]\n        p.wait()\n        yield False", "response": "calc MD5 based on path\nAttributeNames"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef md5check(f, ftp, md5, exclude):\n    files = glob(f)\n    # if no md5 file is specified: download files if path does not exist\n    if md5 is False:\n        if len(files) == 0:\n            return False\n        print('## already downloaded:', f)\n        return True\n    # get md5s from server\n    ## path to md5 file on ftp server\n    md5 = '%s/%s' % (ftp.rsplit('/', 1)[0], md5)\n    ## read md5 table from server\n    try:\n        md5 = pd.read_csv(md5, delim_whitespace = True, names = ['ftp md5', 'file'])\n    except:\n        return False\n    ## filter for md5 files that match file type\n    md5 = md5[md5['file'].str.endswith(f.rsplit('*', 1)[-1])]\n    ## remove preceding characters from file paths\n    md5['file'] = [i.replace('./', '') for i in md5['file']]\n    ## exclude md5s for sub directories\n    md5 = md5[~md5['file'].str.contains('/')]\n    ## exclude files\n    md5 = md5[~md5['file'].str.contains(exclude.replace('*', ''))]\n    # get local md5s\n    md5['local md5'] = [[j for j in calcMD5(i)][0] for i in md5['file']]\n    # return false if md5s do not match\n    for i, File in md5.iterrows():\n        if File['ftp md5'] != File['local md5']:\n            try:\n                os.remove(File['file'])\n                return False\n            except:\n                return False\n    print('## already downloaded:', f)\n    return True", "response": "check md5 checksum on server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading files with wget", "response": "def wget(ftp, f = False, exclude = False, name = False, md5 = False, tries = 10):\n    \"\"\"\n    download files with wget\n    \"\"\"\n    # file name\n    if f is False:\n        f = ftp.rsplit('/', 1)[-1]\n    # downloaded file if it does not already exist\n    # check md5s on server (optional)\n    t = 0\n    while md5check(f, ftp, md5, exclude) is not True:\n        t += 1\n        if name is not False:\n            print('# downloading:', name, f)\n        if exclude is False:\n            command = 'wget -q --random-wait %s' % (ftp)\n        else:\n            command = 'wget -q --random-wait -R %s %s' % (exclude, ftp)\n        p = Popen(command, shell = True)\n        p.communicate()\n        if t >= tries:\n            print('not downloaded:', name, f)\n            return [f, False]\n    return [f, True]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking that at least one of the queries is in list", "response": "def check(line, queries):\n    \"\"\"\n    check that at least one of\n    queries is in list, l\n    \"\"\"\n    line = line.strip()\n    spLine = line.replace('.', ' ').split()\n    matches = set(spLine).intersection(queries)\n    if len(matches) > 0:\n        return matches, line.split('\\t')\n    return matches, False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsearch entrez using specified database and accession", "response": "def entrez(db, acc):\n    \"\"\"\n    search entrez using specified database\n    and accession\n    \"\"\"\n    c1 = ['esearch', '-db', db, '-query', acc]\n    c2 = ['efetch', '-db', 'BioSample', '-format', 'docsum']\n    p1 = Popen(c1, stdout = PIPE, stderr = PIPE)\n    p2 = Popen(c2, stdin = p1.stdout, stdout = PIPE, stderr = PIPE)\n    return p2.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef searchAccession(acc):\n    # try genbank file\n    # genome database\n    out, error = entrez('genome', acc)\n    for line in out.splitlines():\n        line = line.decode('ascii').strip()\n        if 'Assembly_Accession' in line or 'BioSample' in line:\n            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]\n            if len(newAcc) > 0:\n                return (True, acc, newAcc)\n    # nucleotide database\n    out, error = entrez('nucleotide', acc)\n    for line in out.splitlines():\n        line = line.decode('ascii').strip()\n        if 'Assembly_Accession' in line or 'BioSample' in line:\n            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]\n            if len(newAcc) > 0:\n                return (True, acc, newAcc)\n    # assembly database\n    out, error = entrez('assembly', acc)\n    for line in out.splitlines():\n        line = line.decode('ascii').strip()\n        if 'Assembly_Accession' in line or 'BioSample' in line:\n            newAcc = line.split('>')[1].split('<')[0].split('.')[0].split(',')[0]\n            if len(newAcc) > 0:\n                return (True, acc, newAcc)\n    for error in error.splitlines():\n        error = error.decode('ascii').strip()\n        if '500 Can' in error:\n            return (False, acc, 'no network')\n    return (False, acc, 'efetch failed')", "response": "Try to use NCBI Entrez to get\n BioSample BioSequence ID\n "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all TPs from NCBI", "response": "def getFTPs(accessions, ftp, search, exclude, convert = False, threads = 1, attempt = 1,\n            max_attempts = 2):\n    \"\"\"\n    download genome info from NCBI\n    \"\"\"\n    info = wget(ftp)[0]\n    allMatches = []\n    for genome in open(info, encoding = 'utf8'):\n        genome = str(genome)\n        matches, genomeInfo = check(genome, accessions)\n        if genomeInfo is not False:\n            f = genomeInfo[0] + search\n            Gftp = genomeInfo[19]\n            Gftp = Gftp + '/' + search\n            allMatches.extend(matches)\n            yield (Gftp, f, exclude, matches)\n    # print accessions that could not be matched\n    # and whether or not they could be converted (optional)\n    newAccs = []\n    missing = accessions.difference(set(allMatches))\n    if convert is True:\n        pool = Pool(threads)\n        pool = pool.imap_unordered(searchAccession, missing)\n        for newAcc in tqdm(pool, total = len(missing)):\n            status, accession, newAcc = newAcc\n            if status is True:\n                newAccs.append(newAcc)\n            print('not found:', accession, '->', newAcc)\n    else:\n        for accession in missing:\n            print('not found:', accession)\n    # re-try after converting accessions (optional)\n    if len(newAccs) > 0 and attempt <= max_attempts:\n        print('convert accession attempt', attempt)\n        attempt += 1\n        for hit in getFTPs(set(newAccs), ftp, search, exclude, convert,\n                threads = 1, attempt = attempt):\n            yield hit"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(args):\n    accessions, infoFTP = set(args['g']), args['i']\n    search, exclude = args['s'], args['e']\n    FTPs = getFTPs(accessions, infoFTP, search, exclude, threads = args['t'],\n            convert = args['convert'])\n    if args['test'] is True:\n        for genome in FTPs:\n            print('found:', ';'.join(genome[-1]), genome[0])\n        return FTPs\n    pool = Pool(args['t'])\n    pool = pool.imap_unordered(wgetGenome, FTPs)\n    files = []\n    for f in tqdm(pool, total = len(accessions)):\n        files.append(f)\n    return files", "response": "download genomes from NCBI\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nplotting transformed LDA data.", "response": "def plot_LDA(X_lda, y_lda, class_colors, exp_var, style, fig_size, label_pad,\n             font_size, sids, dim=2, zangles=None, pt_size=250, out_fp=\"\"):\n    \"\"\"\n    Plot transformed LDA data.\n    \"\"\"\n    cats = class_colors.keys()\n    fig = plt.figure(figsize=fig_size)\n    if dim == 3:\n        try:\n            assert X_lda.shape[1] >= 3\n        except AssertionError:\n            sys.exit(\"\\nLinear Discriminant Analysis requires at least 4 groups of \"\n                     \"samples to create a 3D figure. Please update group information or \"\n                     \"use the default 2D view of the results.\\n\")\n        if sids is not None:\n            print(\"\\nPoint annotations are available only for 2D figures.\\n\")\n        ax = fig.add_subplot(111, projection=\"3d\")\n        ax.view_init(elev=zangles[1], azim=zangles[0])\n        try:\n            ax.set_zlabel(\"LD3 (Percent Explained Variance: {:.3f}%)\".\n                          format(exp_var[2]*100), fontsize=font_size, labelpad=label_pad)\n        except:\n            ax.set_zlabel(\"LD3\", fontsize=font_size, labelpad=label_pad)\n        for i, target_name in zip(range(len(cats)), cats):\n            cat_x = X_lda[:, 0][y_lda == target_name]\n            cat_y = X_lda[:, 1][y_lda == target_name]\n            cat_z = X_lda[:, 2][y_lda == target_name]\n            ax.scatter(xs=cat_x, ys=cat_y, zs=cat_z, label=target_name,\n                       c=class_colors[target_name], alpha=0.85, s=pt_size, edgecolors=\"k\",\n                       zdir=\"z\")\n    else:\n        ax = fig.add_subplot(111)\n        for i, target_name in zip(range(len(cats)), cats):\n            cat_x = X_lda[:, 0][y_lda == target_name]\n            if X_lda.shape[1] == 1:\n                cat_y = np.ones((cat_x.shape[0], 1)) + i\n            else:\n                cat_y = X_lda[:, 1][y_lda == target_name]\n            ax.scatter(x=cat_x, y=cat_y, label=target_name, alpha=0.85, s=pt_size,\n                       color=class_colors[target_name], edgecolors=\"k\")\n        # Annotate data points with sample IDs\n        if sids is not None:\n            for sample, point, group in zip(sids, X_lda, y_lda):\n                try:\n                    assert len(point) >= 2\n                except AssertionError:\n                    point = (point[0], cats.index(group)+1)\n                finally:\n                    ax.annotate(s=sample, xy=point[:2], xytext=(0, -15), ha=\"center\",\n                                va=\"center\", textcoords=\"offset points\")\n    if X_lda.shape[1] == 1:\n        plt.ylim((0.5, 2.5))\n    try:\n        ax.set_xlabel(\"LD1 (Percent Explained Variance: {:.3f}%)\".\n                      format(exp_var[0]*100), fontsize=font_size, labelpad=label_pad)\n    except:\n        ax.set_xlabel(\"LD1\", fontsize=font_size, labelpad=label_pad)\n    try:\n        ax.set_ylabel(\"LD2 (Percent Explained Variance: {:.3f}%)\".\n                      format(exp_var[1]*100), fontsize=font_size, labelpad=label_pad)\n    except:\n        ax.set_ylabel(\"LD2\", fontsize=font_size, labelpad=label_pad)\n\n    leg = plt.legend(loc=\"best\", scatterpoints=3, frameon=True, framealpha=1, fontsize=15)\n    leg.get_frame().set_edgecolor('k')\n    if dim == 2 and style:\n        gu.ggplot2_style(ax)\n        fc = \"0.8\"\n    else:\n        fc = \"none\"\n\n    # save or display result\n    if out_fp:\n        plt.savefig(out_fp, facecolor=fc, edgecolor=\"none\", dpi=300, bbox_inches=\"tight\",\n                    pad_inches=0.1)\n    else:\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun LinearDiscriminantAnalysis on input dataframe and return transformed data scalings and explained variance by discriminants.", "response": "def run_LDA(df):\n    \"\"\"\n    Run LinearDiscriminantAnalysis on input dataframe (df) and return\n    transformed data, scalings and explained variance by discriminants.\n    \"\"\"\n    # Prep variables for sklearn LDA\n    X = df.iloc[:, 1:df.shape[1]].values     # input data matrix\n    y = df[\"Condition\"].values               # data categories list\n\n    # Calculate LDA\n    sklearn_lda = LDA()\n    X_lda_sklearn = sklearn_lda.fit_transform(X, y)\n    try:\n        exp_var = sklearn_lda.explained_variance_ratio_\n    except AttributeError as ae:\n        print(\"\\n{}: explained variance cannot be computed.\\nPlease check this GitHub PR:\"\n              \" https://github.com/scikit-learn/scikit-learn/pull/6027\".format(ae))\n        return X_lda_sklearn, y, \"NA\"\n    return X_lda_sklearn, y, exp_var"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _calc_frames(stats):\n    timings = []\n    callers = []\n    for key, values in iteritems(stats.stats):\n        timings.append(\n            pd.Series(\n                key + values[:-1],\n                index=timing_colnames,\n            )\n        )\n        for caller_key, caller_values in iteritems(values[-1]):\n            callers.append(\n                pd.Series(\n                    key + caller_key + caller_values,\n                    index=caller_columns,\n                )\n            )\n\n    timings_df = pd.DataFrame(timings)\n    callers_df = pd.DataFrame(callers)\n    timings_df['filename:funcname'] = \\\n        (timings_df['filename'] + ':' + timings_df['funcname'])\n    timings_df = timings_df.groupby('filename:funcname').sum()\n    return timings_df, callers_df", "response": "Compute a DataFrame summary of a Stats object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sugartex_preprocess(source: str) -> str:\n    rep = {r'\\\u02ce': '\u02ce', '\u02ce': '$'}\n    return re.sub(r'\\\\\u02ce|\u02ce', lambda m: rep[m.group(0)], source)", "response": "Preprocess text for SugarTeX Pandoc filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreplace all with SugarTeX.", "response": "def sugartex_replace_all(string):\n    \"\"\"\n    Replace all with SugarTeX.\n    Runs ``sugartex_preprocess`` then iterates via regex and\n    replaces each math between '$...$'.\n    \"\"\"\n    string = sugartex_preprocess(string).replace(r'\\$', SESSION_ID)\n    return re.sub(\n        r'(?<=\\$)[^$]*(?=\\$)',\n        lambda m: sugartex.replace(m.group(0)),\n        string\n    ).replace(SESSION_ID, r'\\$')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parallel(processes, threads):\n    pool = multithread(threads)\n    pool.map(run_process, processes)\n    pool.close()\n    pool.join()", "response": "execute jobs in processes using N threads"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine the final log processor that can render the log.", "response": "def define_log_renderer(fmt, fpath, quiet):\n    \"\"\"\n    the final log processor that structlog requires to render.\n    \"\"\"\n    # it must accept a logger, method_name and event_dict (just like processors)\n    # but must return the rendered string, not a dictionary.\n    # TODO tty logic\n\n    if fmt:\n        return structlog.processors.JSONRenderer()\n\n    if fpath is not None:\n        return structlog.processors.JSONRenderer()\n\n    if sys.stderr.isatty() and not quiet:\n        return structlog.dev.ConsoleRenderer()\n\n    return structlog.processors.JSONRenderer()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding unique id type and hostname", "response": "def _structlog_default_keys_processor(logger_class, log_method, event):\n    ''' Add unique id, type and hostname '''\n    global HOSTNAME\n\n    if 'id' not in event:\n        event['id'] = '%s_%s' % (\n            datetime.utcnow().strftime('%Y%m%dT%H%M%S'),\n            uuid.uuid1().hex\n        )\n\n    if 'type' not in event:\n        event['type'] = 'log'\n\n    event['host'] = HOSTNAME\n\n    return event"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef define_log_processors():\n    # these processors should accept logger, method_name and event_dict\n    # and return a new dictionary which will be passed as event_dict to the next one.\n    return [\n        structlog.processors.TimeStamper(fmt=\"iso\"),\n        _structlog_default_keys_processor,\n        structlog.stdlib.PositionalArgumentsFormatter(),\n        structlog.processors.StackInfoRenderer(),\n        structlog.processors.format_exc_info,\n    ]", "response": "define log processors that should be used for structlog rendering"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure a logger when required write to stderr or a file.", "response": "def _configure_logger(fmt, quiet, level, fpath,\n    pre_hooks, post_hooks, metric_grouping_interval):\n    \"\"\"\n    configures a logger when required write to stderr or a file\n    \"\"\"\n\n    # NOTE not thread safe. Multiple BaseScripts cannot be instantiated concurrently.\n    level = getattr(logging, level.upper())\n\n    global _GLOBAL_LOG_CONFIGURED\n    if _GLOBAL_LOG_CONFIGURED:\n        return\n\n    # since the hooks need to run through structlog, need to wrap them like processors\n    def wrap_hook(fn):\n        @wraps(fn)\n        def processor(logger, method_name, event_dict):\n            fn(event_dict)\n            return event_dict\n\n        return processor\n\n    processors = define_log_processors()\n    processors.extend(\n        [ wrap_hook(h) for h in pre_hooks ]\n    )\n    if metric_grouping_interval:\n        processors.append(metrics_grouping_processor)\n\n    log_renderer = define_log_renderer(fmt, fpath, quiet)\n    stderr_required = (not quiet)\n    pretty_to_stderr = (\n        stderr_required\n        and (\n            fmt == \"pretty\"\n            or (fmt is None and sys.stderr.isatty())\n        )\n    )\n\n    should_inject_pretty_renderer = (\n        pretty_to_stderr\n        and not isinstance(log_renderer, structlog.dev.ConsoleRenderer)\n    )\n    if should_inject_pretty_renderer:\n        stderr_required = False\n        processors.append(StderrConsoleRenderer())\n\n    processors.append(log_renderer)\n    processors.extend(\n        [ wrap_hook(h) for h in post_hooks ]\n    )\n\n    streams = []\n    # we need to use a stream if we are writing to both file and stderr, and both are json\n    if stderr_required:\n        streams.append(sys.stderr)\n\n    if fpath is not None:\n        # TODO handle creating a directory for this log file ?\n        # TODO set mode and encoding appropriately\n        streams.append(open(fpath, 'a'))\n\n    assert len(streams) != 0, \"cannot configure logger for 0 streams\"\n\n    stream = streams[0] if len(streams) == 1 else Stream(*streams)\n    atexit.register(stream.close)\n\n    # a global level struct log config unless otherwise specified.\n    structlog.configure(\n        processors=processors,\n        context_class=dict,\n        logger_factory=LevelLoggerFactory(stream, level=level),\n        wrapper_class=BoundLevelLogger,\n        cache_logger_on_first_use=True,\n    )\n\n    # TODO take care of removing other handlers\n    stdlib_root_log = logging.getLogger()\n    stdlib_root_log.addHandler(StdlibStructlogHandler())\n    stdlib_root_log.setLevel(level)\n\n    _GLOBAL_LOG_CONFIGURED = True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding basic information like caller filename etc.", "response": "def _add_base_info(self, event_dict):\n        \"\"\"\n        Instead of using a processor, adding basic information like caller, filename etc\n        here.\n        \"\"\"\n        f = sys._getframe()\n        level_method_frame = f.f_back\n        caller_frame = level_method_frame.f_back\n        return event_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlogs a debug event.", "response": "def debug(self, event=None, *args, **kw):\n        \"\"\"\n        Process event and call :meth:`logging.Logger.debug` with the result.\n        \"\"\"\n        if not self._logger.isEnabledFor(logging.DEBUG):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"debug\"\n        return self._proxy_to_logger('debug', event, *args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs an info event.", "response": "def info(self, event=None, *args, **kw):\n        \"\"\"\n        Process event and call :meth:`logging.Logger.info` with the result.\n        \"\"\"\n        if not self._logger.isEnabledFor(logging.INFO):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"info\"\n        return self._proxy_to_logger('info', event, *args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef warning(self, event=None, *args, **kw):\n        if not self._logger.isEnabledFor(logging.WARNING):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"warning\"\n        return self._proxy_to_logger('warning', event, *args, **kw)", "response": "Log a warning event and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef error(self, event=None, *args, **kw):\n        if not self._logger.isEnabledFor(logging.ERROR):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"error\"\n        return self._proxy_to_logger('error', event, *args, **kw)", "response": "Process event and call logging. Logger. error with the result."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlog a critical event.", "response": "def critical(self, event=None, *args, **kw):\n        \"\"\"\n        Process event and call :meth:`logging.Logger.critical` with the result.\n        \"\"\"\n        if not self._logger.isEnabledFor(logging.CRITICAL):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"critical\"\n        return self._proxy_to_logger('critical', event, *args, **kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exception(self, event=None, *args, **kw):\n        if not self._logger.isEnabledFor(logging.ERROR):\n            return\n\n        kw = self._add_base_info(kw)\n        kw['level'] = \"exception\"\n        kw.setdefault('exc_info', True)\n        return self.error(event, *args, **kw)", "response": "Process event and call logging. Logger. error with the result."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npropagate a method call to the wrapped logger. This is the same as the superclass implementation, except that it also preserves positional arguments in the `event_dict` so that the stdblib's support for format strings can be used.", "response": "def _proxy_to_logger(self, method_name, event, *event_args,\n                         **event_kw):\n        \"\"\"\n        Propagate a method call to the wrapped logger.\n\n        This is the same as the superclass implementation, except that\n        it also preserves positional arguments in the `event_dict` so\n        that the stdblib's support for format strings can be used.\n        \"\"\"\n\n        if isinstance(event, bytes):\n            event = event.decode('utf-8')\n\n        if event_args:\n            event_kw['positional_args'] = event_args\n\n        return super(BoundLevelLogger, self)._proxy_to_logger(method_name,\n                                                         event=event,\n                                                         **event_kw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef translate(rect, x, y, width=1):\n    return ((rect[0][0]+x, rect[0][1]+y), (rect[1][0]+x, rect[1][1]+y),\n            (rect[2][0]+x+width, rect[2][1]+y), (rect[3][0]+x+width, rect[3][1]+y))", "response": "Given four points of a rectangle translate the rectangle to the specified x and y coordinates and change the width."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_core_file(core_fp):\n    core = {}\n    with open(core_fp) as in_f:\n        for line in in_f.read().splitlines():\n            if not line.startswith(\"#\"):\n                otu_id, tax = line.split(\"\\t\")\n                core[otu_id] = oc.otu_name(ast.literal_eval(tax))\n    return core", "response": "Loads the OTU data file and returns a dictionary of genus - species identifier for each data\n             entry."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_overlaps(otus, group_otus, group_colors, \n                  out_fp, fig_size=None, title=\"\",\n                  filter_common=False):\n    \"\"\"\n    Given a list of OTUs and a number of groups containing subsets of\n    the OTU set, plot a presence/absence bar chart showing which species\n    belong to which groups.\n    \n    :type otus: list\n    :param otus: A list of OTU identifiers (names or otherwise) ordered\n                 by greatest presence across the groups, i.e. those that\n                 come first appear in all the groups, next come the OTUs\n                 that appear in n-1 groups, etc...\n    :type group_otus: OrderedDict\n    :param group_otus: A dictionary of OTU identifiers (subset of otus)\n                       keyed on group name (for display purposes) in\n                       display order (bottom to top).\n    :type group_colors: dict\n    :param group_colors: Color assignment for each group.\n    \"\"\"\n    def sort_order_group(sp):\n        \"\"\"\n        Assign a score (for use with sorting) to each OTU based\n        on the number of groups they occur in and the order\n        within those groups (order priority as set by group_otus).\n        \"\"\"\n        count = 0\n        rank = 0\n        in_prev = True\n        max_penalty = len(group_otus)\n\n        for i, grp in enumerate(group_otus):\n            if sp in group_otus[grp]:\n                count += 1\n                if in_prev:\n                    rank += 1\n            else:\n                rank -= max_penalty - i\n                in_prev = False\n        return count, rank\n\n\n    if filter_common:\n        otus = [otu for otu in otus if sort_order_group(otu)[0] < len(group_otus)]\n    otus = sorted(otus, key=sort_order_group, reverse=True)\n\n    #TODO: fill shared_table during the double loop below and add arg to enable output to file\n    shared_table = [merge_dicts({grp: None for grp in group_otus},{\"OTU\": otu_id}) for otu_id in otus]\n    \n    fig, ax = plt.subplots(figsize=fig_size)\n    ax.xaxis.set_major_locator(MaxNLocator(nbins=len(otus), integer=True))\n\n    # rectangle prototype modified for each plot marker\n    base = [(0,0),(0,0.5),(0,0.5),(0,0)]\n    y_step = 1\n    x_step = 2\n\n    bars = []\n    bar_colors = []\n\n    for i, grp in enumerate(group_otus):\n\n        for j, otu in enumerate(otus):\n            if otu in group_otus[grp]:\n                bars.append(translate(base, j*x_step+0.5, i*y_step))\n                bar_colors.append(group_colors[grp])\n\n    black = (0,0,0,1)\n\n    collection = PolyCollection(\n        verts=bars,\n        facecolors = bar_colors,\n        edgecolors = (black,),\n        linewidths = (1,),\n        transOffset = ax.transData,\n        zorder=3\n        )\n\n    ax.add_collection(collection)\n\n    # ax.legend([plt.Rectangle((0, 0), 1, 1, fc=color) for color in group_colors.values()],\n    #               group_colors.keys(), loc=\"best\")\n    \n    # Title\n    axttl = ax.title\n    axttl.set_position([.5, 1.05])\n    ax.set_title(title, {\"fontsize\": fontsize*1.5, \"fontweight\": \"bold\"})\n    \n    plt.xticks(range(1, len(otus)*x_step, x_step), otus, rotation=\"vertical\")\n    plt.yticks([i-0.75 for i in range(1, len(group_otus)*y_step+1, y_step)], \n               group_otus.keys(), rotation=\"horizontal\")\n\n    ax.margins(0.05)\n    ax.yaxis.set_visible(True)\n    ax.set_xlim((0, len(otus)*x_step))\n\n    # save or display result\n    if out_fp:\n        plt.savefig(out_fp, facecolors=\"0.9\", edgecolor=\"none\",\n                    bbox_inches=\"tight\", pad_inches=0.1)\n    else:\n        plt.show()", "response": "Plots a presence bar chart showing which species they belong to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving problem characters from string", "response": "def remove_bad(string):\n    \"\"\"\n    remove problem characters from string\n    \"\"\"\n    remove = [':', ',', '(', ')', ' ', '|', ';', '\\'']\n    for c in remove:\n        string = string.replace(c, '_')\n    return string"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmaking copy of sequences with short identifier", "response": "def get_ids(a):\n    \"\"\"\n    make copy of sequences with short identifier\n    \"\"\"\n    a_id = '%s.id.fa' % (a.rsplit('.', 1)[0])\n    a_id_lookup = '%s.id.lookup' % (a.rsplit('.', 1)[0])\n    if check(a_id) is True:\n        return a_id, a_id_lookup\n    a_id_f = open(a_id, 'w')\n    a_id_lookup_f = open(a_id_lookup, 'w')\n    ids = []\n    for seq in parse_fasta(open(a)):\n        id = id_generator() \n        while id in ids:\n            id = id_generator() \n        ids.append(id)\n        header = seq[0].split('>')[1]\n        name = remove_bad(header)\n        seq[0] = '>%s %s' % (id, header)\n        print('\\n'.join(seq), file=a_id_f)\n        print('%s\\t%s\\t%s' % (id, name, header), file=a_id_lookup_f)\n    return a_id, a_id_lookup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef convert2phylip(convert):\n    out = '%s.phy' % (convert.rsplit('.', 1)[0])\n    if check(out) is False:\n        convert = open(convert, 'rU')\n        out_f = open(out, 'w')\n        alignments = AlignIO.parse(convert, \"fasta\")\n        AlignIO.write(alignments, out, \"phylip\")\n    return out", "response": "convert fasta to phylip"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_raxml(rax_out, boot, a_id_phylip, threads, aligned, model, cluster, node):\n    # set ppn based on threads\n    if threads > 24:\n        ppn = 24\n    else:\n        ppn = threads\n    if 'raxml' in os.environ:\n        raxml = os.environ['raxml']\n        threads = '-T %s' % (threads)\n    else:\n        raxml = 'raxml'\n        threads = ''\n    rax_tree = 'RAxML_bipartitions.%s' % (rax_out)\n    if check(rax_tree) is False:\n        seed = random.randint(123456789, 12345678910000000)\n        print(seed, file=open('seed.txt', 'w'))\n        if check_type(aligned) == 'nucl' and model is False:\n            model = 'GTRCAT'\n        elif model is False:\n            model = 'PROTCATJTT'\n        dir = os.getcwd()\n        command = '%s -f a -m %s -n %s -N %s -s %s -x %s -p %s %s > %s.log 2>>%s.log' % \\\n                    (raxml, model, rax_out, boot, a_id_phylip, seed, seed, threads, rax_out, rax_out)\n        if cluster is False:\n            p = Popen(command, shell = True)\n        else:\n            if node is False:\n               node = '1'\n            qsub = 'qsub -l nodes=%s:ppn=%s -m e -N raxml' % (node, ppn)\n            command = 'cd /tmp; mkdir raxml_%s; cd raxml_%s; cp %s/%s .; %s; mv * %s/; rm -r ../raxml_%s' \\\n                    % (seed, seed, dir, a_id_phylip, command, dir, seed)\n            re_call = 'cd %s; %s --no-fast' % (dir.rsplit('/', 1)[0], ' '.join(sys.argv))\n            p = Popen('echo \"%s;%s\" | %s' % (command, re_call, qsub), shell = True)\n        p.communicate()\n    return rax_tree", "response": "run raxml based on raxml environment"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun IQ - Tree", "response": "def run_iqtree(phy, model, threads, cluster, node):\n    \"\"\"\n    run IQ-Tree\n    \"\"\"\n    # set ppn based on threads\n    if threads > 24:\n        ppn = 24\n    else:\n        ppn = threads\n    tree = '%s.treefile' % (phy)\n    if check(tree) is False:\n        if model is False:\n            model = 'TEST'\n        dir = os.getcwd()\n        command = 'iqtree-omp -s %s -m %s -nt %s -quiet' % \\\n                    (phy, model, threads)\n        if cluster is False:\n            p = Popen(command, shell = True)\n        else:\n            if node is False:\n               node = '1'\n            qsub = 'qsub -l nodes=%s:ppn=%s -m e -N iqtree' % (node, ppn)\n            command = 'cd /tmp; mkdir iqtree; cd iqtree; cp %s/%s .; %s; mv * %s/; rm -r ../iqtree' \\\n                    % (dir, phy, command, dir)\n            re_call = 'cd %s; %s --no-fast --iq' % (dir.rsplit('/', 1)[0], ' '.join(sys.argv))\n            p = Popen('echo \"%s;%s\" | %s' % (command, re_call, qsub), shell = True)\n        p.communicate()\n    return tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fix_tree(tree, a_id_lookup, out):\n    if check(out) is False and check(tree) is True:\n        tree = open(tree).read()\n        for line in open(a_id_lookup):\n            id, name, header = line.strip().split('\\t')\n            tree = tree.replace(id+':', name+':')\n        out_f = open(out, 'w')\n        print(tree.strip(), file=out_f)\n    return out", "response": "fix the names for sequences in the raxml tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns raxml on 'a' (alignment) with 'boot' (bootstraps) and 'threads' (threads) store all files in raxml_a_b 1. give every sequence a short identifier 2. convert fasta to phylip 3. run raxml 4. convert ids in raxml tree to original names", "response": "def rax(a, boot, threads, \\\n            fast = False, run_rax = False, run_iq = False, model = False, cluster = False, node = False):\n    \"\"\"\n    run raxml on 'a' (alignment) with 'boot' (bootstraps) and 'threads' (threads)\n    store all files in raxml_a_b\n    1. give every sequence a short identifier\n    2. convert fasta to phylip\n    3. run raxml\n    4. convert ids in raxml tree to original names\n    \"\"\"\n    a = os.path.abspath(a)\n    a_base = a.rsplit('/', 1)[1]\n    out_dir = '%s/%s_rax_boots_%s' % \\\n                (a.rsplit('/', 1)[0], a_base.rsplit('.', 1)[0], boot)\n    os.system('mkdir -p %s' % (out_dir))\n    os.system('ln -sf %s %s/%s' % (os.path.abspath(a), out_dir, a.rsplit('/', 1)[1]))\n    os.chdir(out_dir)\n    a_id, a_id_lookup = get_ids(a_base)\n    a_id_phylip = convert2phylip(a_id)\n    rax_out = '%s.raxml.txt' % (a_id_phylip)\n    if fast is True:\n        final_fast = '%s.fasttree.tree' % (a_id_lookup.rsplit('.', 2)[0])\n        fast_tree = run_fast(a_id, threads, cluster, node)\n        good_fast = fix_tree(fast_tree, a_id_lookup, final_fast)\n        yield '%s/%s' % (out_dir, final_fast)\n    # run IQ-Tree or RAxML\n    if run_iq is True:\n        final_iq = '%s.iq.tree' % (a_id_lookup.rsplit('.', 2)[0])\n        iq_out = '%s.iq.out' % (a_id_phylip)\n        iq_tree = run_iqtree(a_id_phylip, model, threads, cluster, node)\n        good_tree = fix_tree(iq_tree, a_id_lookup, final_iq)\n        yield '%s/%s' % (out_dir, final_iq)\n    elif run_rax is True:\n        final_rax = '%s.raxml.tree' % (a_id_lookup.rsplit('.', 2)[0])\n        rax_tree = run_raxml(rax_out, boot, a_id_phylip, threads, a_id, model, cluster, node)\n        good_tree = fix_tree(rax_tree, a_id_lookup, final_rax)\n        yield '%s/%s' % (out_dir, final_rax)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an ARP message to the network device.", "response": "def send_arp_msg():\n    \"\"\"send_arp_msg\n\n    Send an ``ARP`` message to the network device (``enp0s3`` by default).\n\n    \"\"\"\n\n    dev = os.getenv(\n            \"ARP_INTERFACE\",\n            \"enp0s3\").strip().lstrip()\n    network_details = netifaces.ifaddresses(\n            dev)\n    dst_ip = os.getenv(\n            \"ARP_DST_IP\",\n            network_details[2][0][\"addr\"])\n    dst_mac = os.getenv(\n            \"ARP_DST_MAC\",\n            network_details[17][0][\"addr\"])\n\n    print((\"Sending ARP to mac={} ip={}\")\n          .format(\n            dst_mac,\n            dst_ip))\n\n    answered, unanswered = kamene.srp(\n                            kamene.Ether(\n                                dst=dst_mac\n                            ) / kamene.ARP(\n                                pdst=dst_ip\n                            ),\n                            timeout=2,\n                            verbose=False)\n\n    if len(answered) > 0:\n        print(answered[0][0].getlayer(\n            kamene.ARP\n        ).pdst + \" is up\")\n    elif len(unanswered) > 0:\n        print(unanswered[0].getlayer(\n            kamene.ARP\n        ).pdst + \" is down\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new Nydus cluster from the given settings.", "response": "def create_cluster(settings):\n    \"\"\"\n    Creates a new Nydus cluster from the given settings.\n\n    :param settings: Dictionary of the cluster settings.\n    :returns: Configured instance of ``nydus.db.base.Cluster``.\n\n    >>> redis = create_cluster({\n    >>>     'backend': 'nydus.db.backends.redis.Redis',\n    >>>     'router': 'nydus.db.routers.redis.PartitionRouter',\n    >>>     'defaults': {\n    >>>         'host': 'localhost',\n    >>>         'port': 6379,\n    >>>     },\n    >>>     'hosts': {\n    >>>         0: {'db': 0},\n    >>>         1: {'db': 1},\n    >>>         2: {'db': 2},\n    >>>     }\n    >>> })\n    \"\"\"\n    # Pull in our client\n    settings = copy.deepcopy(settings)\n    backend = settings.pop('engine', settings.pop('backend', None))\n    if isinstance(backend, basestring):\n        Conn = import_string(backend)\n    elif backend:\n        Conn = backend\n    else:\n        raise KeyError('backend')\n\n    # Pull in our cluster\n    cluster = settings.pop('cluster', None)\n    if not cluster:\n        Cluster = Conn.get_cluster()\n    elif isinstance(cluster, basestring):\n        Cluster = import_string(cluster)\n    else:\n        Cluster = cluster\n\n    # Pull in our router\n    router = settings.pop('router', None)\n    if not router:\n        Router = BaseRouter\n    elif isinstance(router, basestring):\n        Router = import_string(router)\n    else:\n        Router = router\n\n    # Build the connection cluster\n    return Cluster(\n        router=Router,\n        backend=Conn,\n        **settings\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_translation(self, field, code):\n\n        if not code in self._translation_cache:\n            translations = self.translations.select_related()\n\n            logger.debug(\n                u'Matched with field %s for language %s. Attempting lookup.',\n                field, code\n            )\n\n            try:\n                translation_obj = translations.get(language_code=code)\n\n            except ObjectDoesNotExist:\n                translation_obj = None\n\n            self._translation_cache[code] = translation_obj\n\n            logger.debug(u'Translation not found in cache.')\n\n        else:\n            logger.debug(u'Translation found in cache.')\n            # Get the translation from the cache\n            translation_obj = self._translation_cache.get(code)\n\n        # If this is none, it means that a translation does not exist\n        # It is important to cache this one as well\n        if not translation_obj:\n            raise ObjectDoesNotExist\n\n        field_value = getattr(translation_obj, field)\n\n        logger.debug(\n            u'Found translation object %s, returning value %s.',\n            translation_obj, field_value\n        )\n\n        return field_value", "response": "Get the translation of a specific field for a specific language code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwraps to allow for easy unicode representation of an object by the specified property.", "response": "def unicode_wrapper(self, property, default=ugettext('Untitled')):\n        \"\"\"\n        Wrapper to allow for easy unicode representation of an object by\n        the specified property. If this wrapper is not able to find the\n        right translation of the specified property, it will return the\n        default value instead.\n\n        Example::\n            def __unicode__(self):\n                return unicode_wrapper('name', default='Unnamed')\n\n        \"\"\"\n        # TODO: Test coverage!\n        try:\n            value = getattr(self, property)\n        except ValueError:\n            logger.warn(\n                u'ValueError rendering unicode for %s object.',\n                self._meta.object_name\n            )\n\n            value = None\n\n        if not value:\n            value = default\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving insertion columns from aligned fasta file", "response": "def strip_inserts(fasta):\n    \"\"\"\n    remove insertion columns from aligned fasta file\n    \"\"\"\n    for seq in parse_fasta(fasta):\n        seq[1] = ''.join([b for b in seq[1] if b == '-' or b.isupper()])\n        yield seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef characters(self, string, segment_separator=' ', separator=' # ',):\n        return separator.join(segment_separator.join(word) for word in nfd(string).split())", "response": "Returns a space - delimited string of Unicode characters in a string that contains a space - delimited string of code points rendered as glyphs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transform(self, word, column=Profile.GRAPHEME_COL, error=errors.replace):\n        assert self.op, 'method can only be called with orthography profile.'\n\n        if column != Profile.GRAPHEME_COL and column not in self.op.column_labels:\n            raise ValueError(\"Column {0} not found in profile.\".format(column))\n\n        word = self.op.tree.parse(word, error)\n        if column == Profile.GRAPHEME_COL:\n            return word\n        out = []\n        for token in word:\n            try:\n                target = self.op.graphemes[token][column]\n            except KeyError:\n                target = self._errors['replace'](token)\n            if target is not None:\n                if isinstance(target, (tuple, list)):\n                    out.extend(target)\n                else:\n                    out.append(target)\n        return out", "response": "Transform a string s graphemes into the mappings given in a different column."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the orthography rules applied to the input string.", "response": "def rules(self, word):\n        \"\"\"\n        Function to tokenize input string and return output of str with ortho rules\n        applied.\n\n        Parameters\n        ----------\n        word : str\n            The input string to be tokenized.\n\n        Returns\n        -------\n        result : str\n            Result of the orthography rules applied to the input str.\n\n        \"\"\"\n        return self._rules.apply(word) if self._rules else word"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a string that is space-delimited on Unicode grapheme clusters, group Unicode modifier letters with their preceding base characters, deal with tie bars, etc. Parameters ---------- string : str A Unicode string tokenized into grapheme clusters to be tokenized into simple IPA.", "response": "def combine_modifiers(self, graphemes):\n        \"\"\"\n        Given a string that is space-delimited on Unicode grapheme clusters,\n        group Unicode modifier letters with their preceding base characters,\n        deal with tie bars, etc.\n\n        Parameters\n        ----------\n        string : str\n            A Unicode string tokenized into grapheme clusters to be tokenized into simple\n            IPA.\n\n        \"\"\"\n        result = []\n        temp = \"\"\n        count = len(graphemes)\n        for grapheme in reversed(graphemes):\n            count -= 1\n            if len(grapheme) == 1 and unicodedata.category(grapheme) == \"Lm\" \\\n                    and not ord(grapheme) in [712, 716]:\n                temp = grapheme + temp\n                # hack for the cases where a space modifier is the first character in the\n                # string\n                if count == 0:\n                    result[-1] = temp + result[-1]\n                continue  # pragma: no cover\n            # catch and repair stress marks\n            if len(grapheme) == 1 and ord(grapheme) in [712, 716]:\n                result[-1] = grapheme + result[-1]\n                temp = \"\"\n                continue\n\n            # combine contour tone marks (non-accents)\n            if len(grapheme) == 1 and unicodedata.category(grapheme) == \"Sk\":\n                if len(result) == 0:\n                    result.append(grapheme)\n                    temp = \"\"\n                    continue\n                else:\n                    if unicodedata.category(result[-1][0]) == \"Sk\":\n                        result[-1] = grapheme + result[-1]\n                        temp = \"\"\n                        continue\n\n            result.append(grapheme + temp)\n            temp = \"\"\n\n        # last check for tie bars\n        segments = result[::-1]\n        i = 0\n        r = []\n        while i < len(segments):\n            # tie bars\n            if ord(segments[i][-1]) in [865, 860]:\n                r.append(segments[i] + segments[i + 1])\n                i += 2\n            else:\n                r.append(segments[i])\n                i += 1\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing catalytic RNAs to gff format", "response": "def parse_catalytic(insertion, gff):\n    \"\"\"\n    parse catalytic RNAs to gff format\n    \"\"\"\n    offset = insertion['offset']\n    GeneStrand = insertion['strand']\n    if type(insertion['intron']) is not str:\n        return gff\n    for intron in parse_fasta(insertion['intron'].split('|')):\n        ID, annot, strand, pos = intron[0].split('>')[1].split()\n        Start, End = [int(i) for i in pos.split('-')]\n        if strand != GeneStrand:\n            if strand == '+':\n                strand = '-'\n            else:\n                strand = '+'\n            Start, End = End - 2, Start - 2\n        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1\n        gff['#seqname'].append(insertion['ID'])\n        gff['source'].append('Rfam')\n        gff['feature'].append('Catalytic RNA')\n        gff['start'].append(Start)\n        gff['end'].append(End)\n        gff['score'].append('.')\n        gff['strand'].append(strand)\n        gff['frame'].append('.')\n        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))\n    return gff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_orf(insertion, gff):\n    offset = insertion['offset']\n    if type(insertion['orf']) is not str:\n        return gff\n    for orf in parse_fasta(insertion['orf'].split('|')):\n        ID = orf[0].split('>')[1].split()[0]\n        Start, End, strand = [int(i) for i in orf[0].split(' # ')[1:4]]\n        if strand == 1:\n            strand = '+'\n        else:\n            strand = '-'\n        GeneStrand = insertion['strand']\n        if strand != GeneStrand:\n            if strand == '+':\n                strand = '-'\n            else:\n                strand = '+'\n            Start, End = End - 2, Start - 2\n        Start, End = abs(Start + offset) - 1, abs(End + offset) - 1\n        annot = orf[0].split()[1]\n        if annot == 'n/a':\n            annot = 'unknown'\n        gff['#seqname'].append(insertion['ID'])\n        gff['source'].append('Prodigal and Pfam')\n        gff['feature'].append('CDS')\n        gff['start'].append(Start)\n        gff['end'].append(End)\n        gff['score'].append('.')\n        gff['strand'].append(strand)\n        gff['frame'].append('.')\n        gff['attribute'].append('ID=%s; Name=%s' % (ID, annot))\n    return gff", "response": "parse ORF to gff format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses insertion to gff format", "response": "def parse_insertion(insertion, gff):\n    \"\"\"\n    parse insertion to gff format\n    \"\"\"\n    offset = insertion['offset']\n    for ins in parse_fasta(insertion['insertion sequence'].split('|')):\n        strand = insertion['strand']\n        ID = ins[0].split('>')[1].split()[0]\n        Start, End = [int(i) for i in ins[0].split('gene-pos=', 1)[1].split()[0].split('-')]\n        Start, End = abs(Start + offset), abs(End + offset)\n        if strand == '-':\n            Start, End = End, Start\n        gff['#seqname'].append(insertion['ID'])\n        gff['source'].append(insertion['source'])\n        gff['feature'].append('IVS')\n        gff['start'].append(Start)\n        gff['end'].append(End)\n        gff['score'].append('.')\n        gff['strand'].append(strand) # same as rRNA\n        gff['frame'].append('.')\n        gff['attribute'].append('ID=%s' % (ID))\n    return gff"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses rRNA to gff format", "response": "def parse_rRNA(insertion, seq, gff):\n    \"\"\"\n    parse rRNA to gff format\n    \"\"\"\n    offset = insertion['offset']\n    strand = insertion['strand']\n    for rRNA in parse_masked(seq, 0)[0]:\n        rRNA = ''.join(rRNA)\n        Start = seq[1].find(rRNA) + 1\n        End = Start + len(rRNA) - 1\n        if strand == '-':\n            Start, End = End - 2, Start - 2\n        pos = (abs(Start + offset) - 1, abs(End + offset) - 1)\n        Start, End = min(pos), max(pos)\n        source = insertion['source']\n        annot = '%s rRNA' % (source.split('from', 1)[0])\n        gff['#seqname'].append(insertion['ID'])\n        gff['source'].append(source)\n        gff['feature'].append('rRNA')\n        gff['start'].append(Start)\n        gff['end'].append(End)\n        gff['score'].append('.')\n        gff['strand'].append(strand)\n        gff['frame'].append('.')\n        gff['attribute'].append('Name=%s' % (annot))\n    return gff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iTable2GFF(iTable, fa, contig = False):\n    columns = ['#seqname', 'source', 'feature', 'start', 'end', 'score', 'strand', 'frame', 'attribute']\n    gff = {c:[] for c in columns}\n    for insertion in iTable.iterrows():\n        insertion = insertion[1]\n        if insertion['ID'] not in fa:\n            continue\n        # rRNA strand\n        strand = insertion['sequence'].split('strand=', 1)[1].split()[0]\n        # set rRNA positions for reporting features on contig or extracted sequence\n        if contig is True:\n            gene = [int(i) for i in insertion['sequence'].split('pos=', 1)[1].split()[0].split('-')]\n            if strand == '-':\n                offset = -1 * (gene[1])\n            else:\n                offset = gene[0]\n        else:\n            strand = '+'\n            gene = [1, int(insertion['sequence'].split('total-len=', 1)[1].split()[0])]\n            offset = gene[0]\n        insertion['strand'] = strand\n        insertion['offset'] = offset\n        # source for prediction\n        source = insertion['sequence'].split('::model', 1)[0].rsplit(' ', 1)[-1]\n        insertion['source'] = source\n        # rRNA gene\n        geneAnnot = '%s rRNA gene' % (source.split('from', 1)[0])\n        geneNum = insertion['sequence'].split('seq=', 1)[1].split()[0]\n        gff['#seqname'].append(insertion['ID'])\n        gff['source'].append(source)\n        gff['feature'].append('Gene')\n        gff['start'].append(gene[0])\n        gff['end'].append(gene[1])\n        gff['score'].append('.')\n        gff['strand'].append(strand)\n        gff['frame'].append('.')\n        gff['attribute'].append('ID=%s; Name=%s' % (geneNum, geneAnnot))\n        # rRNA\n        gff = parse_rRNA(insertion, fa[insertion['ID']], gff)\n        # insertions\n        gff = parse_insertion(insertion, gff)\n        # orfs\n        gff = parse_orf(insertion, gff)\n        # catalytic RNAs\n        gff = parse_catalytic(insertion, gff)\n    return pd.DataFrame(gff)[columns].drop_duplicates()", "response": "convert iTable to gff file"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting a smoothed kernel density estimate of the current language.", "response": "def plot_kde(data, ax, title=None, color='r', fill_bt=True):\n    \"\"\"\n    Plot a smoothed (by kernel density estimate) histogram.\n    :type data: numpy array\n    :param data: An array containing the data to be plotted\n\n    :type ax: matplotlib.Axes\n    :param ax: The Axes object to draw to\n\n    :type title: str\n    :param title: The plot title\n\n    :type color: str\n    :param color: The color of the histogram line and fill. Note that the fill\n                  will be plotted with an alpha of 0.35.\n\n    :type fill_bt: bool\n    :param fill_bt: Specify whether to fill the area beneath the histogram line\n    \"\"\"\n    if isinstance(data, list):\n        data = np.asarray(data)\n    e = kde.KDEUnivariate(data.astype(np.float))\n    e.fit()\n    ax.plot(e.support, e.density, color=color, alpha=0.9, linewidth=2.25)\n    if fill_bt:\n        ax.fill_between(e.support, e.density, alpha=.35, zorder=1,\n                        antialiased=True, color=color)\n    if title is not None:\n        t = ax.set_title(title)\n        t.set_y(1.05)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstyle the axes to appear like ggplot2", "response": "def ggplot2_style(ax):\n    \"\"\"\n    Styles an axes to appear like ggplot2\n    Must be called after all plot and axis manipulation operations have been\n    carried out (needs to know final tick spacing)\n    \"\"\"\n    #set the style of the major and minor grid lines, filled blocks\n    ax.grid(True, 'major', color='w', linestyle='-', linewidth=1.4)\n    ax.grid(True, 'minor', color='0.92', linestyle='-', linewidth=0.7)\n    ax.patch.set_facecolor('0.85')\n    ax.set_axisbelow(True)\n\n    #set minor tick spacing to 1/2 of the major ticks\n    ax.xaxis.set_minor_locator(MultipleLocator( (plt.xticks()[0][1]-plt.xticks()[0][0]) / 2.0 ))\n    ax.yaxis.set_minor_locator(MultipleLocator( (plt.yticks()[0][1]-plt.yticks()[0][0]) / 2.0 ))\n\n    #remove axis border\n    for child in ax.get_children():\n        if isinstance(child, mpl.spines.Spine):\n            child.set_alpha(0)\n\n    #restyle the tick lines\n    for line in ax.get_xticklines() + ax.get_yticklines():\n        line.set_markersize(5)\n        line.set_color(\"gray\")\n        line.set_markeredgewidth(1.4)\n\n    #remove the minor tick lines\n    for line in ax.xaxis.get_ticklines(minor=True) + ax.yaxis.get_ticklines(minor=True):\n        line.set_markersize(0)\n\n    #only show bottom left ticks, pointing out of axis\n    mpl.rcParams['xtick.direction'] = 'out'\n    mpl.rcParams['ytick.direction'] = 'out'\n    ax.xaxis.set_ticks_position('bottom')\n    ax.yaxis.set_ticks_position('left')\n\n\n    if ax.legend_ <> None:\n        lg = ax.legend_\n        lg.get_frame().set_linewidth(0)\n        lg.get_frame().set_alpha(0.5)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an abundance table group the counts by every taxonomic level.", "response": "def summarize_taxa(biom):\n    \"\"\"\n    Given an abundance table, group the counts by every\n    taxonomic level.\n    \"\"\"\n    tamtcounts = defaultdict(int)\n    tot_seqs = 0.0\n\n    for row, col, amt in biom['data']:\n        tot_seqs += amt\n        rtax = biom['rows'][row]['metadata']['taxonomy']\n        for i, t in enumerate(rtax):\n            t = t.strip()\n            if i == len(rtax)-1 and len(t) > 3 and len(rtax[-1]) > 3:\n                t = 's__'+rtax[i-1].strip().split('_')[-1]+'_'+t.split('_')[-1]\n            tamtcounts[t] += amt\n\n    lvlData = {lvl: levelData(tamtcounts, tot_seqs, lvl) for lvl in ['k', 'p', 'c', 'o', 'f', 'g', 's']}\n\n    return tot_seqs, lvlData"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconnect to a forwarded socket and return a socket object.", "response": "def connect_forwarder(forward_host=None,\n                      forward_port=None,\n                      max_retries=-1,\n                      sleep_interval=1.0):\n    \"\"\"connect_forwarder\n\n    :param forward_host: host for receiving forwarded packets\n    :param forward_port: port for the forwarded packets\n    :param max_retries: retries, -1 = infinite\n    :param sleep_interval: how often to retry in this loop\n    \"\"\"\n\n    forward_skt = None\n    retry_count = 0\n    if max_retries == -1:\n        retry_count = -2\n\n    if forward_host and forward_port:\n        while not forward_skt and \\\n              retry_count < max_retries:\n            try:\n                forward_skt = socket.socket()\n                log.info((\"connecting to forward={}:{}\")\n                         .format(forward_host,\n                                 forward_port))\n                forward_skt.connect((forward_host,\n                                     forward_port))\n                log.debug((\"connected to forward={}:{}\")\n                          .format(forward_host,\n                                  forward_port))\n            except Exception as s:\n                forward_skt = None\n                log.error((\"Failed to connect forward address={}:{} \"\n                           \"with ex={}\")\n                          .format(forward_host,\n                                  forward_port,\n                                  s))\n                if max_retries == -1:\n                    retry_count = -2\n                else:\n                    retry_count += 1\n            # end of try/ex\n            time.sleep(sleep_interval)\n        # end of setting up forward\n    # end forward_host and forward_port\n\n    return forward_skt"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the path to the custom image set for this game or None if no custom image is set.", "response": "def custom_image(self, user):\n        \"\"\"Returns the path to the custom image set for this game, or None if\n        no image is set\"\"\"\n        for ext in self.valid_custom_image_extensions():\n            image_location = self._custom_image_path(user, ext)\n            if os.path.isfile(image_location):\n                return image_location\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_image(self, user, image_path):\n        _, ext = os.path.splitext(image_path)\n        shutil.copy(image_path, self._custom_image_path(user, ext))", "response": "Sets a custom image for the game. image_path should refer to\n        an image file on disk"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a list of mapped reads", "response": "def sam_list(sam):\n\t\"\"\"\n\tget a list of mapped reads\n\t\"\"\"\n\tlist = []\n\tfor file in sam:\n\t\tfor line in file:\n\t\t\tif line.startswith('@') is False:\n\t\t\t\tline = line.strip().split()\n\t\t\t\tid, map = line[0], int(line[1])\n\t\t\t\tif map != 4 and map != 8:\n\t\t\t\t\tlist.append(id)\n\treturn set(list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sam_list_paired(sam):\n\tlist = []\n\tpair = ['1', '2']\n\tprev = ''\n\tfor file in sam:\n\t\tfor line in file:\n\t\t\tif line.startswith('@') is False:\n\t\t\t\tline = line.strip().split()\n\t\t\t\tid, map = line[0], int(line[1])\n\t\t\t\tif map != 4 and map != 8:\n\t\t\t\t\tread = id.rsplit('/')[0]\n\t\t\t\t\tif read == prev:\n\t\t\t\t\t\tlist.append(read)\n\t\t\t\t\tprev = read\n\treturn set(list)", "response": "get a list of mapped reads\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_paired(list):\n\tpairs = {}\n\tfiltered = []\n\tfor id in list:\n\t\tread = id.rsplit('/')[0]\n\t\tif read not in pairs:\n\t\t\tpairs[read] = []\n\t\tpairs[read].append(id)\n\tfor read in pairs:\n\t\tids = pairs[read]\n\t\tif len(ids) == 2:\n\t\t\tfiltered.extend(ids)\n\treturn set(filtered)", "response": "filter_paired - Filter the list of paired reads"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering sequences that are shown to be mapped in the sam file", "response": "def filter(fastq, sam, paired = False):\n\t\"\"\" \n\tfilter sequences that are shown to be mapped in the sam file\n\treads not in sam file are in *.filtered.fastq\n\treads that are in the sam file are in *.matched.fastq\n\t\"\"\"\n\tif paired is False:\n\t\tlist = sam_list(sam)\n\telse:\n\t\tlist = sam_list_paired(sam)\n\tif paired is False:\n\t\tfilter_fastq(fastq, list)\n\telse:\n\t\tfilter_fastq_paired(fastq, list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef handle_processing_packets():\n\n    host = os.getenv(\n        \"LISTEN_ON_HOST\",\n        \"127.0.0.1\").strip().lstrip()\n    port = int(os.getenv(\n        \"LISTEN_ON_PORT\",\n        \"80\").strip().lstrip())\n    backlog = int(os.getenv(\n        \"LISTEN_BACKLOG\",\n        \"5\").strip().lstrip())\n    size = int(os.getenv(\n        \"LISTEN_SIZE\",\n        \"102400\").strip().lstrip())\n    sleep_in_seconds = float(os.getenv(\n        \"LISTEN_SLEEP\",\n        \"0.5\").strip().lstrip())\n    needs_response = bool(os.getenv(\n        \"LISTEN_SEND_RESPONSE\",\n        \"0\").strip().lstrip() == \"1\")\n    shutdown_hook = os.getenv(\n        \"LISTEN_SHUTDOWN_HOOK\",\n        \"/tmp/shutdown-listen-server-{}-{}\".format(\n            host,\n            port)).strip().lstrip()\n    filter_key = os.getenv(\n        \"IGNORE_KEY\",\n        INCLUDED_IGNORE_KEY).strip().lstrip()\n\n    if os.path.exists(shutdown_hook):\n        log.info((\"Please remove the shutdown hook file: \"\n                  \"\\nrm -f {}\")\n                 .format(\n                    shutdown_hook))\n        sys.exit(1)\n\n    default_filter_key = filter_key\n    bytes_for_filter_key = len(default_filter_key)\n    offset_to_filter_key = (-1 * bytes_for_filter_key)\n    offset_to_msg = offset_to_filter_key - 1\n\n    now = datetime.datetime.now().isoformat()\n    log.info((\"{} - Starting Server address={}:{} \"\n              \"backlog={} size={} sleep={} shutdown={} \"\n              \"filter_key={}\")\n             .format(\n                now,\n                host,\n                port,\n                backlog,\n                size,\n                sleep_in_seconds,\n                shutdown_hook,\n                default_filter_key))\n\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    s.bind((host, port))\n    s.listen(backlog)\n    client, address = s.accept()\n\n    midx = 0\n    while 1:\n        data = None\n        address = None\n        ignore_key = None\n\n        try:\n            if not client:\n                client, address = s.accept()\n        except Exception as e:\n            log.error((\"socket accept with ex={}\")\n                      .format(\n                        e))\n        try:\n            if client:\n                data = client.recv(size)\n        except Exception as e:\n            log.error((\"recv - disconnected with ex={}\")\n                      .format(\n                        e))\n        if data:\n            now = datetime.datetime.now().isoformat()\n            packet_to_process = data[0:offset_to_msg]\n            ignore_key = data[offset_to_filter_key:]\n            log.info((\"decoding data={} key={}\")\n                     .format(\n                        packet_to_process,\n                        ignore_key))\n\n            msg = None\n            try:\n                msg = json.loads(\n                            packet_to_process.decode(\"utf-8\"))\n            except Exception as e:\n                msg = None\n                log.error((\"Invalid data={} with ex={}\")\n                          .format(\n                            packet_to_process,\n                            e))\n\n            if msg:\n\n                log.info((\"received msg={} \"\n                          \"data={} replying - ignore='{}'\")\n                         .format(\n                            ppj(msg),\n                            packet_to_process,\n                            ignore_key))\n\n                if msg[\"status\"] == VALID:\n                    if msg[\"data_type\"] == TCP:\n                        log.info(\"TCP\")\n                    elif msg[\"data_type\"] == UDP:\n                        log.info(\"TCP\")\n                    elif msg[\"data_type\"] == ARP:\n                        log.info(\"TCP\")\n                    elif msg[\"data_type\"] == ICMP:\n                        log.info(\"TCP\")\n                    else:\n                        log.error((\"unsuppported type={}\")\n                                  .format(\n                                    msg[\"data_type\"]))\n                    # end of supported eth protocol message types\n                else:\n                    log.error((\"unsuppported msg status={}\")\n                              .format(\n                                msg[\"status\"]))\n                # end if msg was VALID\n            # end of if found msg\n\n            midx += 1\n            if midx > 1000000:\n                midx = 0\n        else:\n            log.debug(\"ignoring invalid data\")\n        # end of if valid msg or not\n\n        if needs_response:\n            client.send(ignore_key)\n        else:\n            log.info(\"no response\")\n            time.sleep(sleep_in_seconds)\n\n        if os.path.exists(shutdown_hook):\n            now = datetime.datetime.now().isoformat()\n            log.info((\"{} detected shutdown \"\n                      \"file={}\")\n                     .format(\n                        now,\n                        shutdown_hook))\n    # end of loop\n\n    log.info(\"shutting down\")\n    client.close()\n\n    log.info(\"done\")", "response": "This function handles the processing of the packet and returns the new object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a UDP message to the specified server.", "response": "def send_udp_message():\n    \"\"\"send_udp_message\n\n    Send a ``UDP`` message to port 80 by default.\n\n    Environment variables:\n\n    ``UDP_SEND_TO_HOST`` - host ip address\n    ``UDP_SEND_TO_PORT`` - send to this UDP port\n\n    \"\"\"\n    host = os.getenv(\n        \"UDP_SEND_TO_HOST\",\n        \"0.0.0.0\").strip().lstrip()\n    port = int(os.getenv(\n        \"UDP_SEND_TO_PORT\",\n        \"17000\").strip().lstrip())\n\n    need_response = os.getenv(\"NEED_RESPONSE\", \"0\") == \"1\"\n\n    msg = os.getenv(\n            \"MSG\",\n            \"testing UDP msg time={} - {}\".format(\n                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                uuid.uuid4()))\n\n    server_address = (host, port)\n\n    client = socket.socket(socket.AF_INET,\n                           socket.SOCK_DGRAM)\n\n    print((\"sending UDP: \"\n           \"address={} msg={}\")\n          .format(server_address,\n                  msg))\n    client.sendto(msg.encode(), server_address)\n    if need_response:\n        data = client.recv(1024).decode()\n        print(data)\n    client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts sam to fastq", "response": "def sam2fastq(line):\n    \"\"\"\n    print fastq from sam\n    \"\"\"\n    fastq = []\n    fastq.append('@%s' % line[0])\n    fastq.append(line[9])\n    fastq.append('+%s' % line[0])\n    fastq.append(line[10])\n    return fastq"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncounts the number of mismatches in a read", "response": "def count_mismatches(read):\n    \"\"\"\n    look for NM:i:<N> flag to determine number of mismatches\n    \"\"\"\n    if read is False:\n        return False\n    mm = [int(i.split(':')[2]) for i in read[11:] if i.startswith('NM:i:')]\n    if len(mm) > 0:\n        return sum(mm)\n    else:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_mismatches(read, pair, mismatches, mm_option, req_map):\n    # if read is not paired, make sure it is mapped and that mm <= thresh\n    if pair is False:\n        mm = count_mismatches(read)\n        if mm is False:\n            return False\n        # if no threshold is supplied, return True\n        if mismatches is False:\n            return True\n        # passes threshold?\n        if mm <= mismatches:\n            return True\n    # paired reads\n    r_mm = count_mismatches(read)\n    p_mm = count_mismatches(pair)\n    # if neither read is mapped, return False\n    if r_mm is False and p_mm is False:\n        return False\n    # if no threshold, return True\n    if mismatches is False:\n        return True\n    # if req_map is True, both reads have to map\n    if req_map is True:\n        if r_mm is False or p_mm is False:\n            return False\n    ## if option is 'one,' only one read has to pass threshold\n    if mm_option == 'one':\n        if (r_mm is not False and r_mm <= mismatches) or (p_mm is not False and p_mm <= mismatches):\n            return True\n    ## if option is 'both,' both reads have to pass threshold\n    if mm_option == 'both':\n        ## if one read in pair does not map to the scaffold,\n        ## make sure the other read passes threshold\n        if r_mm is False:\n            if p_mm <= mismatches:\n                return True\n        elif p_mm is False:\n            if r_mm <= mismatches:\n                return True\n        elif (r_mm is not False and r_mm <= mismatches) and (p_mm is not False and p_mm <= mismatches):\n            return True\n    return False", "response": "check to see if the read maps with < threshold number of mismatches"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines whether or not a read is mapped to a specific region of scaffold", "response": "def check_region(read, pair, region):\n    \"\"\"\n    determine whether or not reads map to specific region of scaffold\n    \"\"\"\n    if region is False:\n        return True\n    for mapping in read, pair:\n        if mapping is False:\n            continue\n        start, length = int(mapping[3]), len(mapping[9])\n        r = [start, start + length - 1]\n        if get_overlap(r, region) > 0:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_reads(sam, \\\n        contigs = False, mismatches = False, mm_option = False, \\\n        sort_sam = True, req_map = False, region = False, sbuffer = False):\n    \"\"\"\n    get mapped reads (and their pairs) from an unsorted sam file\n    \"\"\"\n    tempdir = '%s/' % (os.path.abspath(sam).rsplit('/', 1)[0])\n    if sort_sam is True:\n        mapping = '%s.sorted.sam' % (sam.rsplit('.', 1)[0])\n        if sam != '-':\n            if os.path.exists(mapping) is False:\n                os.system(\"\\\n                    sort -k1 --buffer-size=%sG -T %s -o %s %s\\\n                    \" % (sbuffer, tempdir, mapping, sam))\n        else:\n            mapping = 'stdin-sam.sorted.sam'\n            p = Popen(\"sort -k1 --buffer-size=%sG -T %s -o %s\" \\\n                    % (sbuffer, tempdir, mapping), stdin = sys.stdin, shell = True)\n            p.communicate()\n        mapping = open(mapping)\n    else:\n        if sam == '-':\n            mapping = sys.stdin\n        else:\n            mapping = open(sam)\n    for read in reads_from_mapping(mapping, contigs, mismatches, mm_option, req_map, region):\n        yield read", "response": "get mapped reads from an unsorted sam file"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_steam():\n  # Helper function which checks if the potential userdata directory exists\n  # and returns a new Steam instance with that userdata directory if it does.\n  # If the directory doesnt exist it returns None instead\n  helper = lambda udd: Steam(udd) if os.path.exists(udd) else None\n\n  # For both OS X and Linux, Steam stores it's userdata in a consistent\n  # location.\n  plat = platform.system()\n  if plat == 'Darwin':\n    return helper(paths.default_osx_userdata_path())\n  if plat == 'Linux':\n    return helper(paths.default_linux_userdata_path())\n\n  # Windows is a bit trickier. The userdata directory is stored in the Steam\n  # installation directory, meaning that theoretically it could be anywhere.\n  # Luckily, Valve stores the installation directory in the registry, so its\n  # still possible for us to figure out automatically\n  if plat == 'Windows':\n    possible_dir = winutils.find_userdata_directory()\n    # Unlike the others, `possible_dir` might be None (if something odd\n    # happened with the registry)\n    return helper(possible_dir) if possible_dir is not None else None\n  # This should never be hit. Windows, OS X, and Linux should be the only\n  # supported platforms.\n  # TODO: Add logging here so that the user (developer) knows that something\n  # odd happened.\n  return None", "response": "Returns a Steam object representing the current Steam installation on the users computer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine a simple Genus - species identifier for an OTU.", "response": "def otu_name(tax):\n    \"\"\"\n    Determine a simple Genus-species identifier for an OTU, if possible.\n    If OTU is not identified to the species level, name it as\n    Unclassified (familly/genus/etc...).\n\n    :type tax: list\n    :param tax: QIIME-style taxonomy identifiers, e.g.\n                 [\"k__Bacteria\", u\"p__Firmicutes\", u\"c__Bacilli\", ...\n\n    :rtype: str\n    :return: Returns genus-species identifier based on identified taxonomical\n             level.\n    \"\"\"\n    extract_name = lambda lvl: \"_\".join(lvl.split(\"_\")[2:])\n    spname = \"spp.\"\n    for lvl in tax[::-1]:\n        if len(lvl) <= 3:\n            continue\n        if lvl.startswith(\"s\"):\n            spname = extract_name(lvl)\n        elif lvl.startswith(\"g\"):\n            return \"{}_{}\".format(extract_name(lvl), spname)\n        else:\n            if spname != \"spp.\":\n                return spname\n            else:\n                return \"Unclassified_{}\".format(extract_name(lvl))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the OTU data file and returns a dict of genus - species identifier for each entry in the file.", "response": "def load_core_file(core_fp):\n    \"\"\"\n    For core OTU data file, returns Genus-species identifier for each data\n    entry.\n    :type core_fp: str\n    :param core_fp: A file containing core OTU data.\n    :rtype: str\n    :return: Returns genus-species identifier based on identified taxonomical\n             level.\n    \"\"\"\n    with open(core_fp, \"rU\") as in_f:\n        return {otu_name(ast.literal_eval(line.split(\"\\t\")[1]))\n                for line in in_f.readlines()[1:]}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nassign OTUIDs to each sample.", "response": "def assign_otu_membership(biomfile):\n    \"\"\"\n    Determines the OTUIDs present in each sample.\n    :type biomfile: biom.table.Table\n    :param biomfile: BIOM table object from the biom-format library.\n    :rtype: dict\n    :return: Returns a dictionary keyed on Sample ID with sets containing\n    the IDs of OTUIDs found in each sample.\n    \"\"\"\n    samples = defaultdict(set)\n    _ = biomfile.pa()\n    for sid in biomfile.ids():\n        for otuid in biomfile.ids(\"observation\"):\n            if biomfile.get_value_by_ids(otuid, sid) == 1:\n                samples[sid].add(otuid)\n    return samples"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing from zero to one for table or list of tables", "response": "def zero_to_one(table, option):\n    \"\"\"\n    normalize from zero to one for row or table\n    \"\"\"\n    if option == 'table':\n        m = min(min(table))\n        ma = max(max(table))\n    t = []\n    for row in table:\n        t_row = []\n        if option != 'table':\n            m, ma = min(row), max(row)\n        for i in row:\n            if ma == m:\n                t_row.append(0)\n            else:\n                t_row.append((i - m)/(ma - m))\n        t.append(t_row)\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pertotal(table, option):\n    if option == 'table':\n        total = sum([i for line in table for i in line])\n    t = []\n    for row in table:\n        t_row = []\n        if option != 'table':\n            total = sum(row)\n        for i in row:\n            if total == 0:\n                t_row.append(0)\n            else:\n                t_row.append(i/total*100)\n        t.append(t_row)\n    return t", "response": "calculate percent of total"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstandardizing the given table", "response": "def standardize(table, option):\n    \"\"\"\n    standardize\n    Z = (X - mean) / (standard deviation)\n    \"\"\"\n    if option == 'table':\n        mean = np.mean(table)\n        std = np.std(table)\n    t = []\n    for row in table:\n        t_row = []\n        if option != 'table':\n            mean = np.mean(row)\n            std = np.std(row)\n        for i in row:\n            if std == 0:\n                t_row.append(0)\n            else:\n                t_row.append((i - mean)/std)\n        t.append(t_row)\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nscales table based on the largest sum of the column with the largest sum", "response": "def scale(table):\n    \"\"\"\n    scale table based on the column with the largest sum\n    \"\"\"\n    t = []\n    columns = [[] for i in table[0]]\n    for row in table:\n        for i, v in enumerate(row):\n            columns[i].append(v)\n    sums = [float(sum(i)) for i in columns]\n    scale_to = float(max(sums))\n    scale_factor = [scale_to/i for i in sums if i != 0]\n    for row in table:\n        t.append([a * b for a,b in zip(row, scale_factor)])\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef norm(table):\n    print('# norm dist is broken', file=sys.stderr)\n    exit()\n    from matplotlib.pyplot import hist as hist\n    t = []\n    for i in table:\n        t.append(np.ndarray.tolist(hist(i, bins = len(i), normed = True)[0]))\n    return t", "response": "fit to normal distribution\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlog transform each value in table", "response": "def log_trans(table):\n    \"\"\"\n    log transform each value in table\n    \"\"\"\n    t = []\n    all = [item for sublist in table for item in sublist]\n    if min(all) == 0:\n        scale = min([i for i in all if i != 0]) * 10e-10\n    else:\n        scale = 0\n    for i in table:\n        t.append(np.ndarray.tolist(np.log10([j + scale for j in i])))\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nboxing - cox transform table", "response": "def box_cox(table):\n    \"\"\"\n    box-cox transform table\n    \"\"\"\n    from scipy.stats import boxcox as bc\n    t = []\n    for i in table:\n        if min(i) == 0:\n            scale = min([j for j in i if j != 0]) * 10e-10\n        else:\n            scale = 0\n        t.append(np.ndarray.tolist(bc(np.array([j + scale for j in i]))[0]))\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninverse hyperbolic sine transformation", "response": "def inh(table):\n    \"\"\"\n    inverse hyperbolic sine transformation\n    \"\"\"\n    t = []\n    for i in table:\n        t.append(np.ndarray.tolist(np.arcsinh(i)))\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef diri(table):\n    t = []\n    for i in table:\n        a = [j + 1 for j in i]\n        t.append(np.ndarray.tolist(np.random.mtrand.dirichlet(a)))\n    return t", "response": "from SparCC - randomly draw from the corresponding posterior\n    Dirichlet distribution with a uniform prior"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a large TCP message to the server 80 by default.", "response": "def send_tcp_large_message():\n    \"\"\"send_tcp_large_message\n\n    Send a large ``TCP`` message to port 80 by default.\n\n    \"\"\"\n\n    need_response = os.getenv(\"NEED_RESPONSE\", \"0\") == \"1\"\n\n    msg = os.getenv(\n            \"MSG\",\n            \"testing msg time={} - {}\".format(\n                datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n                uuid.uuid4()))\n    host = \"127.0.0.1\"\n    port = 80\n\n    client = socket.socket()\n    client.connect((host, port))\n\n    msg += \" large_random_data=\"\n    for i in range(0, 100):\n        for j in range(0, 100):\n            msg += str(uuid.uuid4())\n\n    print((\"sending LARGE msg: {}\")\n          .format(len(msg)))\n    client.sendall(msg.encode())\n    if need_response:\n        data = client.recv(1024).decode()\n        print(data)\n    client.close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef open_enc(fn, utf16):\n    if utf16:\n        return codecs.open(fn, encoding='utf-16')  # -be')\n    return open(fn, 'rU')", "response": "Open a file for reading with either the standard open function or the codecs module for reading UTF - 16 encoded files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_barcodes(nIds, codeLen=12):\n    def next_code(b, c, i):\n        return c[:i] + b + (c[i+1:] if i < -1 else '')\n\n    def rand_base():\n        return random.choice(['A', 'T', 'C', 'G'])\n\n    def rand_seq(n):\n        return ''.join([rand_base() for _ in range(n)])\n\n    # homopolymer filter regex: match if 4 identical bases in a row\n    hpf = re.compile('aaaa|cccc|gggg|tttt', re.IGNORECASE)\n\n    while True:\n        codes = [rand_seq(codeLen)]\n        if (hpf.search(codes[0]) is None):\n            break\n    idx = 0\n\n    while len(codes) < nIds:\n        idx -= 1\n        if idx < -codeLen:\n            idx = -1\n            codes.append(rand_seq(codeLen))\n        else:\n            nc = next_code(rand_base(), codes[-1], idx)\n            if hpf.search(nc) is None:\n                codes.append(nc)\n        codes = list(set(codes))\n\n    return codes", "response": "Given a list of sample IDs generate unique n - base barcodes for each."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a mapping from sample IDs to barcodes write out a QIIME - compatible mapping file.", "response": "def write_mapping_file(mapF, sampleIDs, barcodes, treatment=None):\n    \"\"\"\n    Given a mapping from sample IDs to barcodes/primers/other info,\n    write out a QIIME-compatible mapping file.\n\n    File format described at: http://qiime.org/documentation/file_formats.html\n    Note that primer column can be left blank.\n    \"\"\"\n    header = ['#SampleID', 'BarcodeSequence', 'LinkerPrimerSequence',\n              'Description', '\\n']\n    if treatment is not None:\n        header.insert(-2, treatment.name)\n    primer = 'AAACCCGGGTTTAAACTGAC'\n    sampleMap = {}\n\n    with mapF:\n        mapF.write('\\t'.join(header))\n        for sid, bc in zip(sampleIDs, barcodes):\n            sampleMap[sid] = MapRecord(bc, primer, treatment.value, sid)\n            line = [sid, bc, primer, sid]\n            if treatment is not None:\n                line.insert(-1, treatment.value)\n            mapF.write('\\t'.join(line) + '\\n')\n\n    return sampleMap"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a sample ID and a mapping, modify a Sanger FASTA file to include the barcode and 'primer' in the sequence data and change the description line as needed.", "response": "def scrobble_data_dir(dataDir, sampleMap, outF, qualF=None, idopt=None,\n                      utf16=False):\n    \"\"\"\n    Given a sample ID and a mapping, modify a Sanger FASTA file\n    to include the barcode and 'primer' in the sequence data\n    and change the description line as needed.\n    \"\"\"\n    seqcount = 0\n    outfiles = [osp.split(outF.name)[1]]\n    if qualF:\n        outfiles.append(osp.split(qualF.name)[1])\n\n    for item in os.listdir(dataDir):\n        if item in outfiles or not osp.isfile(os.path.join(dataDir, item)):\n            continue\n        # FASTA files\n        if osp.splitext(item)[1] in file_types['fasta']:\n            fh = open_enc(os.path.join(dataDir, item), utf16)\n            records = SeqIO.parse(fh, 'fasta')\n            for record in records:\n                if isinstance(idopt, tuple):\n                    sep, field = idopt\n                    sampleID = record.id.split(sep)[field - 1]\n                else:\n                    sampleID = osp.splitext(item)[0]\n                record.seq = (sampleMap[sampleID].barcode +\n                              sampleMap[sampleID].primer +\n                              record.seq)\n                SeqIO.write(record, outF, 'fasta')\n                seqcount += 1\n            fh.close()\n        # QUAL files\n        elif qualF and osp.splitext(item)[1] in file_types['qual']:\n            fh = open_enc(os.path.join(dataDir, item), utf16)\n            records = SeqIO.parse(fh, 'qual')\n            for record in records:\n                mi = sampleMap[sampleMap.keys()[0]]\n                quals = [40 for _ in range(len(mi.barcode) + len(mi.primer))]\n                record.letter_annotations['phred_quality'][0:0] = quals\n                SeqIO.write(record, qualF, 'qual')\n            fh.close()\n    return seqcount"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_program_options():\n    parser = argparse.ArgumentParser(description=\"Convert Sanger-sequencing \\\n                                     derived data files for use with the \\\n                                     metagenomics analysis program QIIME, by \\\n                                     extracting Sample ID information, adding\\\n                                     barcodes and primers to the sequence \\\n                                     data, and outputting a mapping file and\\\n                                     single FASTA-formatted sequence file \\\n                                     formed by concatenating all input data.\")\n    parser.add_argument('-i', '--input_dir', required=True,\n                        help=\"The directory containing sequence data files. \\\n                              Assumes all data files are placed in this \\\n                              directory. For files organized within folders by\\\n                              sample, use -s in addition.\")\n    parser.add_argument('-m', '--map_file', default='map.txt',\n                        help=\"QIIME-formatted mapping file linking Sample IDs \\\n                              with barcodes and primers.\")\n    parser.add_argument('-o', '--output', default='output.fasta',\n                        metavar='OUTPUT_FILE',\n                        help=\"Single file containing all sequence data found \\\n                              in input_dir, FASTA-formatted with barcode and \\\n                              primer preprended to sequence. If the -q option \\\n                              is passed, any quality data will also be output \\\n                              to a single file of the same name with a .qual \\\n                              extension.\")\n    parser.add_argument('-b', '--barcode_length', type=int, default=12,\n                        help=\"Length of the generated barcode sequences. \\\n                              Default is 12 (QIIME default), minimum is 8.\")\n\n    parser.add_argument('-q', '--qual', action='store_true', default=False,\n                        help=\"Instruct the program to look for quality \\\n                              input files\")\n    parser.add_argument('-u', '--utf16', action='store_true', default=False,\n                        help=\"UTF-16 encoded input files\")\n\n    parser.add_argument('-t', '--treatment',\n                        help=\"Inserts an additional column into the mapping \\\n                              file specifying some treatment or other variable\\\n                              that separates the current set of sequences \\\n                              from any other set of seqeunces. For example:\\\n                              -t DiseaseState=healthy\")\n\n    # data input options\n    sidGroup = parser.add_mutually_exclusive_group(required=True)\n    sidGroup.add_argument('-d', '--identifier_pattern',\n                          action=ValidateIDPattern,\n                          nargs=2, metavar=('SEPARATOR', 'FIELD_NUMBER'),\n                          help=\"Indicates how to extract the Sample ID from \\\n                               the description line. Specify two things: \\\n                               1. Field separator, 2. Field number of Sample \\\n                               ID (1 or greater). If the separator is a space \\\n                               or tab, use \\s or \\\\t respectively. \\\n                               Example: >ka-SampleID-2091, use -i - 2, \\\n                               indicating - is the separator and the Sample ID\\\n                               is field #2.\")\n    sidGroup.add_argument('-f', '--filename_sample_id', action='store_true',\n                          default=False, help='Specify that the program should\\\n                          the name of each fasta file as the Sample ID for use\\\n                          in the mapping file. This is meant to be used when \\\n                          all sequence data for a sample is stored in a single\\\n                          file.')\n\n    return parser.parse_args()", "response": "Handles command - line options for the\n    program."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_packets(pk):\n\n    log.info((\"processing with pub={}\")\n             .format(pub))\n\n    # get the lowest layer\n    eth = pk.getlayer(kamene.Ether)\n\n    should_forward = False\n    send_msg = {\"data\": {},\n                \"created\": rnow(),\n                \"source\": SOURCE}\n\n    if eth:\n        # parse all layer frames under ethernet\n        send_msg[\"data\"] = convert_pkt_to_json(eth)\n        should_forward = True\n    else:\n        log.error((\"unsupported pk={}\")\n                  .format(pk))\n    # end of if supported\n\n    if should_forward:\n\n        log.info(\"forwarding\")\n\n        # Publish the message:\n        msg_sent = pub.publish(body=send_msg,\n                               exchange=FORWARD_EXCHANGE,\n                               routing_key=FORWARD_ROUTING_KEY,\n                               queue=FORWARD_QUEUE,\n                               serializer=\"json\",\n                               retry=True)\n\n        log.info(\"done forwarding={}\".format(msg_sent))", "response": "handle_packets - handle packets from kamene"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_all_headers(\n        pipeline_files=[],\n        label_rules=None):\n    \"\"\"find_all_headers\n\n    :param pipeline_files: files to process\n    :param label_rules: labeling rules\n    \"\"\"\n\n    log.info(\"find_all_headers - START\")\n\n    headers = [\"src_file\"]\n    headers_dict = {\"src_file\": None}\n\n    if label_rules:\n        headers = [\"src_file\", \"label_value\", \"label_name\"]\n        headers_dict = {\"src_file\": None,\n                        \"label_value\": None,\n                        \"label_name\": None}\n\n    for c in pipeline_files:\n        df = pd.read_csv(c)\n        for h in df.columns.values:\n            if h not in headers_dict:\n                headers_dict[h] = \"{}_{}\".format(\n                                    c,\n                                    h)\n                headers.append(h)\n        # end for all headers in the file\n    # end for all files to find common_headers\n\n    log.info((\"headers={}\")\n             .format(len(headers)))\n\n    log.info(\"find_all_headers - END\")\n    return headers, headers_dict", "response": "find all headers in the file that are common to all the pipeline_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef build_csv(\n        pipeline_files=[],\n        fulldata_file=None,\n        clean_file=None,\n        post_proc_rules=None,\n        label_rules=None,\n        metadata_filename=\"metadata.json\"):\n    \"\"\"build_csv\n\n    :param pipeline_files: files to process\n    :param fulldata_file: output all columns to this csv file\n    :param clean_file: output all numeric-ready columns to this csv file\n    :param post_proc_rules: rules after building the DataFrame\n    :param label_rules: labeling rules\n    :param metadata_filename: metadata\n    \"\"\"\n\n    save_node = {\n        \"status\": INVALID,\n        \"pipeline_files\": pipeline_files,\n        \"post_proc_rules\": post_proc_rules,\n        \"label_rules\": label_rules,\n        \"fulldata_file\": fulldata_file,\n        \"fulldata_metadata_file\": None,\n        \"clean_file\": clean_file,\n        \"clean_metadata_file\": None,\n        \"features_to_process\": [],\n        \"feature_to_predict\": None,\n        \"ignore_features\": [],\n        \"df_json\": {}\n    }\n\n    if not fulldata_file:\n        log.error(\"missing fulldata_file - stopping\")\n        save_node[\"status\"] = INVALID\n        return save_node\n    if not clean_file:\n        log.error(\"missing clean_file - stopping\")\n        save_node[\"status\"] = INVALID\n        return save_node\n\n    log.info(\"build_csv - START\")\n\n    common_headers, \\\n        headers_dict = find_all_headers(\n                            pipeline_files=pipeline_files)\n\n    log.info((\"num common_headers={} headers={}\")\n             .format(len(common_headers),\n                     common_headers))\n\n    # since the headers can be different we rebuild a new one:\n\n    hdrs = {}\n    for h in common_headers:\n        hdrs[h] = None\n\n    features_to_process = []\n    feature_to_predict = None\n    ignore_features = []\n\n    set_if_above = None\n    labels = []\n    label_values = []\n    if label_rules:\n        set_if_above = label_rules[\"set_if_above\"]\n        labels = label_rules[\"labels\"]\n        label_values = label_rules[\"label_values\"]\n\n    all_rows = []\n    num_done = 0\n    total_files = len(pipeline_files)\n    for c in pipeline_files:\n        log.info((\"merging={}/{} csv={}\")\n                 .format(num_done,\n                         total_files,\n                         c))\n        cf = pd.read_csv(c)\n        log.info((\" processing rows={}\")\n                 .format(len(cf.index)))\n        for index, row in cf.iterrows():\n            valid_row = True\n            new_row = copy.deepcopy(hdrs)\n            new_row[\"src_file\"] = c\n            for k in hdrs:\n                if k in row:\n                    new_row[k] = row[k]\n            # end of for all headers to copy in\n\n            if label_rules:\n                test_rand = random.randint(0, 100)\n                if test_rand > set_if_above:\n                    new_row[\"label_value\"] = label_values[1]\n                    new_row[\"label_name\"] = labels[1]\n                else:\n                    new_row[\"label_value\"] = label_values[0]\n                    new_row[\"label_name\"] = labels[0]\n            # end of applying label rules\n\n            if valid_row:\n                all_rows.append(new_row)\n        # end of for all rows in this file\n\n        num_done += 1\n    # end of building all files into one list\n\n    log.info((\"fulldata rows={} generating df\")\n             .format(len(all_rows)))\n    df = pd.DataFrame(all_rows)\n    log.info((\"df rows={} headers={}\")\n             .format(len(df.index),\n                     df.columns.values))\n\n    if ev(\"CONVERT_DF\",\n          \"0\") == \"1\":\n        log.info(\"converting df to json\")\n        save_node[\"df_json\"] = df.to_json()\n\n    if clean_file:\n        log.info((\"writing fulldata_file={}\")\n                 .format(fulldata_file))\n        df.to_csv(fulldata_file,\n                  sep=',',\n                  encoding='utf-8',\n                  index=False)\n        log.info((\"done writing fulldata_file={}\")\n                 .format(fulldata_file))\n\n        if post_proc_rules:\n\n            clean_metadata_file = \"\"\n\n            feature_to_predict = \"label_name\"\n            features_to_process = []\n            ignore_features = []\n            if label_rules:\n                ignore_features = [feature_to_predict]\n\n            if \"drop_columns\" in post_proc_rules:\n                for p in post_proc_rules[\"drop_columns\"]:\n                    if p in headers_dict:\n                        ignore_features.append(p)\n                # post proce filter more features out\n                # for non-int/float types\n\n                for d in df.columns.values:\n                    add_this_one = True\n                    for i in ignore_features:\n                        if d == i:\n                            add_this_one = False\n                            break\n                    if add_this_one:\n                        features_to_process.append(d)\n                # for all df columns we're not ignoring...\n                # add them as features to process\n\n                fulldata_metadata_file = \"{}/fulldata_{}\".format(\n                    \"/\".join(fulldata_file.split(\"/\")[:-1]),\n                    metadata_filename)\n                log.info((\"writing fulldata metadata file={}\")\n                         .format(fulldata_metadata_file))\n                header_data = {\"headers\": list(df.columns.values),\n                               \"output_type\": \"fulldata\",\n                               \"pipeline_files\": pipeline_files,\n                               \"post_proc_rules\": post_proc_rules,\n                               \"label_rules\": label_rules,\n                               \"features_to_process\": features_to_process,\n                               \"feature_to_predict\": feature_to_predict,\n                               \"ignore_features\": ignore_features,\n                               \"created\": rnow()}\n\n                with open(fulldata_metadata_file, \"w\") as otfile:\n                    otfile.write(str(ppj(header_data)))\n\n                keep_these = features_to_process\n                keep_these.append(feature_to_predict)\n\n                log.info((\"creating new clean_file={} \"\n                          \"keep_these={} \"\n                          \"predict={}\")\n                         .format(clean_file,\n                                 keep_these,\n                                 feature_to_predict))\n\n                # need to remove all columns that are all nan\n                clean_df = df[keep_these].dropna(\n                                axis=1, how='all').dropna()\n\n                cleaned_features = clean_df.columns.values\n                cleaned_to_process = []\n                cleaned_ignore_features = []\n                for c in cleaned_features:\n                    if c == feature_to_predict:\n                        cleaned_ignore_features.append(c)\n                    else:\n                        keep_it = True\n                        for ign in ignore_features:\n                            if c == ign:\n                                cleaned_ignore_features.append(c)\n                                keep_it = False\n                                break\n                        # end of for all feaures to remove\n                        if keep_it:\n                            cleaned_to_process.append(c)\n                # end of new feature columns\n\n                log.info((\"writing DROPPED clean_file={} \"\n                          \"features_to_process={} \"\n                          \"ignore_features={} \"\n                          \"predict={}\")\n                         .format(clean_file,\n                                 cleaned_to_process,\n                                 cleaned_ignore_features,\n                                 feature_to_predict))\n\n                write_clean_df = clean_df.drop(\n                    columns=cleaned_ignore_features\n                )\n                log.info((\"cleaned_df rows={}\")\n                         .format(len(write_clean_df.index)))\n                write_clean_df.to_csv(\n                         clean_file,\n                         sep=',',\n                         encoding='utf-8',\n                         index=False)\n\n                clean_metadata_file = \"{}/cleaned_{}\".format(\n                    \"/\".join(clean_file.split(\"/\")[:-1]),\n                    metadata_filename)\n                log.info((\"writing clean metadata file={}\")\n                         .format(clean_metadata_file))\n                header_data = {\"headers\": list(write_clean_df.columns.values),\n                               \"output_type\": \"clean\",\n                               \"pipeline_files\": pipeline_files,\n                               \"post_proc_rules\": post_proc_rules,\n                               \"label_rules\": label_rules,\n                               \"features_to_process\": cleaned_to_process,\n                               \"feature_to_predict\": feature_to_predict,\n                               \"ignore_features\": cleaned_ignore_features,\n                               \"created\": rnow()}\n                with open(clean_metadata_file, \"w\") as otfile:\n                    otfile.write(str(ppj(header_data)))\n            else:\n\n                for d in df.columns.values:\n                    add_this_one = True\n                    for i in ignore_features:\n                        if d == i:\n                            add_this_one = False\n                            break\n                    if add_this_one:\n                        features_to_process.append(d)\n                # for all df columns we're not ignoring...\n                # add them as features to process\n\n                fulldata_metadata_file = \"{}/fulldata_{}\".format(\n                    \"/\".join(fulldata_file.split(\"/\")[:-1]),\n                    metadata_filename)\n                log.info((\"writing fulldata metadata file={}\")\n                         .format(fulldata_metadata_file))\n                header_data = {\"headers\": list(df.columns.values),\n                               \"output_type\": \"fulldata\",\n                               \"pipeline_files\": pipeline_files,\n                               \"post_proc_rules\": post_proc_rules,\n                               \"label_rules\": label_rules,\n                               \"features_to_process\": features_to_process,\n                               \"feature_to_predict\": feature_to_predict,\n                               \"ignore_features\": ignore_features,\n                               \"created\": rnow()}\n\n                with open(fulldata_metadata_file, \"w\") as otfile:\n                    otfile.write(str(ppj(header_data)))\n\n                keep_these = features_to_process\n                keep_these.append(feature_to_predict)\n\n                log.info((\"creating new clean_file={} \"\n                          \"keep_these={} \"\n                          \"predict={}\")\n                         .format(clean_file,\n                                 keep_these,\n                                 feature_to_predict))\n\n                # need to remove all columns that are all nan\n                clean_df = df[keep_these].dropna(\n                                axis=1, how='all').dropna()\n\n                cleaned_features = clean_df.columns.values\n                cleaned_to_process = []\n                cleaned_ignore_features = []\n                for c in cleaned_features:\n                    if c == feature_to_predict:\n                        cleaned_ignore_features.append(c)\n                    else:\n                        keep_it = True\n                        for ign in ignore_features:\n                            if c == ign:\n                                cleaned_ignore_features.append(c)\n                                keep_it = False\n                                break\n                        # end of for all feaures to remove\n                        if keep_it:\n                            cleaned_to_process.append(c)\n                # end of new feature columns\n\n                log.info((\"writing DROPPED clean_file={} \"\n                          \"features_to_process={} \"\n                          \"ignore_features={} \"\n                          \"predict={}\")\n                         .format(clean_file,\n                                 cleaned_to_process,\n                                 cleaned_ignore_features,\n                                 feature_to_predict))\n\n                write_clean_df = clean_df.drop(\n                    columns=cleaned_ignore_features\n                )\n                log.info((\"cleaned_df rows={}\")\n                         .format(len(write_clean_df.index)))\n                write_clean_df.to_csv(\n                         clean_file,\n                         sep=',',\n                         encoding='utf-8',\n                         index=False)\n\n                clean_metadata_file = \"{}/cleaned_{}\".format(\n                    \"/\".join(clean_file.split(\"/\")[:-1]),\n                    metadata_filename)\n                log.info((\"writing clean metadata file={}\")\n                         .format(clean_metadata_file))\n                header_data = {\"headers\": list(write_clean_df.columns.values),\n                               \"output_type\": \"clean\",\n                               \"pipeline_files\": pipeline_files,\n                               \"post_proc_rules\": post_proc_rules,\n                               \"label_rules\": label_rules,\n                               \"features_to_process\": cleaned_to_process,\n                               \"feature_to_predict\": feature_to_predict,\n                               \"ignore_features\": cleaned_ignore_features,\n                               \"created\": rnow()}\n                with open(clean_metadata_file, \"w\") as otfile:\n                    otfile.write(str(ppj(header_data)))\n\n            # end of if/else\n\n            save_node[\"clean_file\"] = clean_file\n            save_node[\"clean_metadata_file\"] = clean_metadata_file\n\n            log.info((\"done writing clean_file={}\")\n                     .format(clean_file))\n        # end of post_proc_rules\n\n        save_node[\"fulldata_file\"] = fulldata_file\n        save_node[\"fulldata_metadata_file\"] = fulldata_metadata_file\n\n        save_node[\"status\"] = VALID\n    # end of writing the file\n\n    save_node[\"features_to_process\"] = features_to_process\n    save_node[\"feature_to_predict\"] = feature_to_predict\n    save_node[\"ignore_features\"] = ignore_features\n\n    log.info(\"build_csv - END\")\n\n    return save_node", "response": "build_csv - Builds a node from a list of files"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding all CSVs in a directory and return a list of all the pipeline files", "response": "def find_all_pipeline_csvs(\n        csv_glob_path=\"/opt/antinex/datasets/**/*.csv\"):\n    \"\"\"find_all_pipeline_csvs\n\n    :param csv_glob_path: path to csvs\n    \"\"\"\n\n    log.info(\"finding pipeline csvs in dir={}\".format(csv_glob_path))\n\n    pipeline_files = []\n\n    for csv_file in glob.iglob(csv_glob_path,\n                               recursive=True):\n        log.info((\"adding file={}\")\n                 .format(csv_file))\n        pipeline_files.append(csv_file)\n    # end of for all csvs\n\n    log.info((\"pipeline files={}\")\n             .format(len(pipeline_files)))\n\n    return pipeline_files"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprepare a new dataset for training", "response": "def prepare_new_dataset():\n    \"\"\"prepare_new_dataset\"\"\"\n    clean_dir = ev(\n        \"OUTPUT_DIR\",\n        \"/tmp\")\n    clean_file = ev(\n        \"CLEANED_FILE\",\n        \"{}/cleaned_attack_scans.csv\".format(\n            clean_dir))\n    fulldata_file = ev(\n        \"FULLDATA_FILE\",\n        \"{}/fulldata_attack_scans.csv\".format(\n            clean_dir))\n    dataset_dir = ev(\n        \"DS_DIR\",\n        \"/opt/antinex/datasets\")\n    csv_glob_path = ev(\n        \"DS_GLOB_PATH\",\n        \"{}/*/*.csv\".format(\n            dataset_dir))\n\n    pipeline_files = find_all_pipeline_csvs(\n        csv_glob_path=csv_glob_path)\n\n    post_proc_rules = {\n        \"drop_columns\": [\n            \"src_file\",\n            \"raw_id\",\n            \"raw_load\",\n            \"raw_hex_load\",\n            \"raw_hex_field_load\",\n            \"pad_load\",\n            \"eth_dst\",  # need to make this an int\n            \"eth_src\",  # need to make this an int\n            \"ip_dst\",   # need to make this an int\n            \"ip_src\"    # need to make this an int\n        ],\n        \"predict_feature\": \"label_name\"\n    }\n\n    label_rules = {\n        \"set_if_above\": 85,\n        \"labels\": [\"not_attack\", \"attack\"],\n        \"label_values\": [0, 1]\n    }\n\n    log.info(\"building csv\")\n\n    save_node = build_csv(\n        pipeline_files=pipeline_files,\n        fulldata_file=fulldata_file,\n        clean_file=clean_file,\n        post_proc_rules=post_proc_rules,\n        label_rules=label_rules)\n\n    if save_node[\"status\"] == VALID:\n        log.info(\"Successfully process datasets:\")\n\n        if ev(\"SHOW_SUMMARY\", \"1\") == \"1\":\n            log.info((\"Full csv: {}\")\n                     .format(save_node[\"fulldata_file\"]))\n            log.info((\"Full meta: {}\")\n                     .format(save_node[\"fulldata_metadata_file\"]))\n            log.info((\"Clean csv: {}\")\n                     .format(save_node[\"clean_file\"]))\n            log.info((\"Clean meta: {}\")\n                     .format(save_node[\"clean_metadata_file\"]))\n            log.info(\"------------------------------------------\")\n            log.info((\"Predicting Feature: {}\")\n                     .format(save_node[\"feature_to_predict\"]))\n            log.info((\"Features to Process: {}\")\n                     .format(ppj(save_node[\"features_to_process\"])))\n            log.info((\"Ignored Features: {}\")\n                     .format(ppj(save_node[\"ignore_features\"])))\n            log.info(\"------------------------------------------\")\n        # end of show summary\n\n        log.info(\"\")\n        log.info(\"done saving csv:\")\n        log.info(\"Full: {}\".format(\n            save_node[\"fulldata_file\"]))\n        log.info(\"Cleaned (no-NaNs in columns): {}\".format(\n            save_node[\"clean_file\"]))\n        log.info(\"\")\n    else:\n        log.info(\"Failed to process datasets\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite the given BIOM table to a file.", "response": "def write_biom(biom_tbl, output_fp, fmt=\"hdf5\", gzip=False):\n    \"\"\"\n    Write the BIOM table to a file.\n    :type biom_tbl: biom.table.Table\n    :param biom_tbl: A BIOM table containing the per-sample OTU counts and metadata\n                  to be written out to file.\n    :type output_fp str\n    :param output_fp: Path to the BIOM-format file that will be written.\n    :type fmt: str\n    :param fmt: One of: hdf5, json, tsv. The BIOM version the table will be\n                output (2.x, 1.0, 'classic').\n    \"\"\"\n    opener = open\n    mode = 'w'\n    if gzip and fmt != \"hdf5\":\n        if not output_fp.endswith(\".gz\"):\n            output_fp += \".gz\"\n        opener = gzip_open\n        mode = 'wt'\n\n    # HDF5 BIOM files are gzipped by default\n    if fmt == \"hdf5\":\n        opener = h5py.File\n\n    gen_str = \"PhyloToAST v{} (phylotoast.org)\".format(phylotoast.__version__)\n    biom_tbl.generated_by = gen_str\n\n    with opener(output_fp, mode) as biom_f:\n        if fmt == \"json\":\n            biom_tbl.to_json(biom_tbl.generated_by, direct_io=biom_f)\n        elif fmt == \"tsv\":\n            biom_f.write(biom_tbl.to_tsv())\n        else:\n            biom_tbl.to_hdf5(biom_f, biom_tbl.generated_by)\n\n    return output_fp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\napplying the arcsine square root transform to the given BIOM - format table", "response": "def arcsin_sqrt(biom_tbl):\n    \"\"\"\n    Applies the arcsine square root transform to the\n    given BIOM-format table\n    \"\"\"\n    arcsint = lambda data, id_, md: np.arcsin(np.sqrt(data))\n\n    tbl_relabd = relative_abd(biom_tbl)\n    tbl_asin = tbl_relabd.transform(arcsint, inplace=False)\n\n    return tbl_asin"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse sam file and check mapping quality", "response": "def parse_sam(sam, qual):\n    \"\"\"\n    parse sam file and check mapping quality\n    \"\"\"\n    for line in sam:\n        if line.startswith('@'):\n            continue\n        line = line.strip().split()\n        if int(line[4]) == 0 or int(line[4]) < qual:\n            continue\n        yield line"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves reference sequences to the given genomes and s2b.", "response": "def save_refs(sam, fastas, genomes, s2b):\n    \"\"\"\n    genomes = {} # genomes[genome][contig][sample] = {'bp_stats':[]}\n    \"\"\"\n    if s2b is False:\n        s2b = {}\n    for fasta in fastas:\n        # if no reference sequence supplied, get length from sam file\n        if fasta is False:\n            for line in sam:\n                if line.startswith('@PG'):\n                    break\n                if line.startswith('@SQ') is False:\n                    continue\n                line = line.strip().split()\n                contig, length = line[1].split(':', 1)[1], int(line[2].split(':', 1)[1])\n                if contig not in s2b:\n                    genome = 'n/a'\n                    s2b[contig] = genome\n                else:\n                    genome = s2b[contig]\n                if genome not in genomes:\n                    genomes[genome] = {}\n                if contig not in genomes[genome]:\n                    genomes[genome][contig] = {}\n                bp_stats = []\n                for i in range(0, length):\n                    bp_stats.append({'A':0, 'T':0, 'G':0, 'C':0, 'N':0, \\\n                                        'In':[], 'Del':[], 'ref':'N'})\n                genomes[genome][contig][sam.name] = {'bp_stats':bp_stats}\n                return genomes, s2b\n        # save reference sequences, if available\n        genome = fasta.rsplit('.', 1)[0]\n        if genome not in genomes:\n            genomes[genome] = {}\n        bp_stats = []\n        for seq in parse_fasta(fasta):\n            contig = seq[0].split('>')[1].split()[0]\n            s2b[contig] = genome\n            if contig not in genomes[genome]:\n                genomes[genome][contig] = {}\n            bp_stats = []\n            for base in seq[1]:\n                bp_stats.append({'A':0, 'T':0, 'G':0, 'C':0, 'N':0, \\\n                                    'In':[], 'Del':[], 'ref':base.upper()})\n            genomes[genome][contig][sam.name] = {'bp_stats':bp_stats}\n        return genomes, s2b"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_cigar(cigar):\n    cigar = cigar.replace('M', 'M ').replace('I', 'I ').replace('D', 'D ').split()\n    cigar = [c.replace('M', ' M').replace('I', ' I').replace('D', ' D').split() for c in cigar]\n    return [(int(c[0]), c[1]) for c in cigar]", "response": "parse a cigar string into a list of operations"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_bp_stats(sam, genomes, s2b, qual):\n    for read in parse_sam(sam, qual):\n        ref = read[2] # scaffold that read mapped to\n        if ref not in s2b:\n            continue\n        genome = s2b[ref] # genome that scaffold belongs to\n        refs = genomes[genome][ref][sam.name]['bp_stats']\n        ref_start = int(read[3]) - 1 # position of start of alignment on reference\n        ref_pos = int(read[3]) - 1 # for keeping track of reference region\n        sequence = list(read[9]) # read sequence\n        cigar = parse_cigar(read[5]) # parsed cigar string\n        cigar_start = 0 # for keeping track of start of cigar regions\n        bases = [] # bases to compare with reference\n        for cigar_pos, status in cigar:\n            if status == 'D': # deletion compared to reference\n                refs[ref_pos - 1]['Del'].append(cigar_pos)\n                for b in range(0, cigar_pos):\n                    bases.append(False)\n                ref_pos += cigar_pos\n            else:\n                cigar_stop = cigar_start + cigar_pos\n                if status == 'M': # aligned to reference\n                    for b in sequence[cigar_start:cigar_stop]: # bases for cigar region\n                        bases.append(b)\n                    ref_pos += cigar_pos\n                elif status == 'I': # insertion compared to reference\n                    refs[ref_pos - 1]['In'].append(cigar_pos)\n                else:\n                    print('# unrecognized cigar character: %s' % (status), file=sys.stderr)\n                    exit()\n                cigar_start += cigar_pos\n        # add base to frequency at each position\n        for base, position in zip(bases, list(range(ref_start, ref_start + len(bases)))):\n            if base is False:\n                continue\n            try:\n                refs[position][base.upper()] += 1\n            except IndexError:\n                continue\n    return genomes", "response": "get base pair frequency statistics for each base pair from sam file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_codons(ref, start, end, strand):\n    codon = []\n    c = cycle([1, 2, 3])\n    ref = ref[start - 1:end]\n    if strand == -1:\n        ref = rc_stats(ref)\n    for pos in ref:\n        n = next(c)\n        codon.append(pos)\n        if n == 3:\n            yield codon\n            codon = []", "response": "parse codon nucleotide positions in range start - > end wrt strand\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef calc_coverage(ref, start, end, length, nucs):\n    ref = ref[start - 1:end]\n    bases = 0\n    for pos in ref:\n        for base, count in list(pos.items()):\n            if base in nucs:\n                bases += count\n    return float(bases)/float(length)", "response": "calculate coverage for a sequence of nucleotide"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse gbk file into iterable of contigs locus start and strand", "response": "def parse_gbk(gbks):\n    \"\"\"\n    parse gbk file\n    \"\"\"\n    for gbk in gbks:\n        for record in SeqIO.parse(open(gbk), 'genbank'):\n            for feature in record.features:\n                if feature.type == 'gene':\n                    try:\n                        locus = feature.qualifiers['locus_tag'][0]\n                    except:\n                        continue\n                if feature.type == 'CDS':\n                    try:\n                        locus = feature.qualifiers['locus_tag'][0]\n                    except:\n                        pass\n                    start = int(feature.location.start) + int(feature.qualifiers['codon_start'][0])\n                    end, strand = int(feature.location.end), feature.location.strand\n                    if strand is None:\n                        strand = 1\n                    else:\n                        strand = -1\n                    contig = record.id\n#                    contig = record.id.rsplit('.', 1)[0]\n                    yield contig, [locus, \\\n                            [start, end, strand], \\\n                            feature.qualifiers]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_fasta_annotations(fastas, annot_tables, trans_table):\n    if annot_tables is not False:\n        annots = {}\n        for table in annot_tables:\n            for cds in open(table):\n                ID, start, end, strand = cds.strip().split()\n                annots[ID] = [start, end, int(strand)]\n    for fasta in fastas:\n        for seq in parse_fasta(fasta):\n            if ('# ;gc_cont' not in seq[0] and '# ID=' not in seq[0]) and annot_tables is False:\n                print('# specify fasta from Prodigal or annotations table (-t)', file=sys.stderr)\n                exit()\n            if 'ID=' in seq[0]:\n                ID = seq[0].rsplit('ID=', 1)[1].split(';', 1)[0]\n                contig = seq[0].split()[0].split('>')[1].rsplit('_%s' % (ID), 1)[0]\n            else:\n                contig = seq[0].split()[0].split('>')[1].rsplit('_', 1)[0]\n            locus = seq[0].split()[0].split('>')[1]\n            # annotation info from Prodigal\n            if ('# ;gc_cont' in seq[0] or '# ID=' in seq[0]):\n                info = seq[0].split(' # ')\n                start, end, strand = int(info[1]), int(info[2]), info[3]\n                if strand == '1':\n                    strand = 1\n                else:\n                    strand = -1\n                product = [''.join(info[4].split()[1:])]\n            # annotation info from table\n            else:\n                start, end, strand = annots[locus]\n                product = seq[0].split(' ', 1)[1]\n            info = {'transl_table':[trans_table], \\\n                    'translation':[seq[1]], \\\n                    'product':product}\n            yield contig, [locus, [start, end, strand], info]", "response": "parse gene call information from Prodigal fasta output\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing annotations in either gbk or Prodigal fasta format", "response": "def parse_annotations(annots, fmt, annot_tables, trans_table):\n    \"\"\"\n    parse annotations in either gbk or Prodigal fasta format\n    \"\"\"\n    annotations = {} # annotations[contig] = [features]\n    # gbk format\n    if fmt is False:\n        for contig, feature in parse_gbk(annots):\n            if contig not in annotations:\n                annotations[contig] = []\n            annotations[contig].append(feature)\n    # fasta format\n    else:\n        for contig, feature in parse_fasta_annotations(annots, annot_tables, trans_table):\n            if contig not in annotations:\n                annotations[contig] = []\n            annotations[contig].append(feature)\n    return annotations"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count_mutations(codon, AA, alleles, counts, nucs, trans_table):\n    # find alternative codons based on SNPs\n    obs_codons = [] # codons observed from SNPs\n    for pos, pos_mutations in enumerate(alleles):\n        mutations = [b for b in pos_mutations if b in nucs]\n        for mutation in mutations:\n            alt_codon = codon[:]\n            alt_codon[pos] = mutation\n            obs_codons.append(alt_codon)\n    obs_codons = [i for i in obs_codons if i != codon]\n    obs_AAs = [codon2aa(i, trans_table) for i in obs_codons]\n    obs_AAs = [i for i in obs_AAs if i != AA]\n    num_mutations = len(obs_codons)\n    num_nonSyn    = len(obs_AAs)\n    num_syn       = num_mutations - num_nonSyn\n    counts['obs syn']     += num_syn\n    counts['obs non-syn'] += num_nonSyn\n    # find all possible alternative codons based on single base changes\n    pos_codons = [] # codons inferred from all possible SNPs\n    for pos in range(0, 3):\n        for nuc in nucs:\n            pos_codon = codon[:]\n            pos_codon[pos] = nuc\n            pos_codons.append(pos_codon)\n    pos_codons = [i for i in pos_codons if i != codon]\n    pos_AAs = [codon2aa(i, trans_table) for i in pos_codons]\n    pos_AAs = [i for i in pos_AAs if i != AA]\n    num_mutations = len(pos_codons)\n    num_nonSyn    = len(pos_AAs)\n    num_syn       = num_mutations - num_nonSyn\n    counts['pos syn']     += num_syn\n    counts['pos non-syn'] += num_nonSyn\n    return counts", "response": "Count the number of mutations in a single base codon."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts codon to amino acid", "response": "def codon2aa(codon, trans_table):\n    \"\"\"\n    convert codon to amino acid\n    \"\"\"\n    return Seq(''.join(codon), IUPAC.ambiguous_dna).translate(table = trans_table)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calc_PnPs(counts):\n    if counts['pos non-syn'] == 0 or counts['pos syn'] == 0:\n        return 'n/a'\n    NonSyn = float(counts['obs non-syn'])/float(counts['pos non-syn'])\n    Syn    = float(counts['obs syn'])    /float(counts['pos syn'])\n    if Syn == 0:\n        return 'n/a'\n    return NonSyn/Syn", "response": "Calculates the PnPs of the current language"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sub_rates(stats, annots):\n    nucs = ['A', 'T', 'G', 'C']\n    subs = {} # subs[locus]{pn/ps reference, pn/ps consensus, snp density}\n    for cds in annots:\n        locus, location, info = cds\n        subs[locus] = {}\n        start, end, strand = location\n        trans_table = info['transl_table'][0]\n        wrt_reference = {'obs syn':0, 'pos syn':0, 'obs non-syn':0, 'pos non-syn':0}\n        wrt_consensus = {'obs syn':0, 'pos syn':0, 'obs non-syn':0, 'pos non-syn':0} \n        for codon in parse_codons(stats, start, end, strand):\n            # reference and consensus translations\n            ref_codon = [i['ref'] for i in codon]\n            con_codon = [i['consensus'][0] for i in codon]\n            ref_aa = codon2aa(ref_codon, trans_table)\n            con_aa = codon2aa(con_codon, trans_table)\n            # count types of mutations with respect to reference\n            wrt_reference = \\\n                    count_mutations(ref_codon, ref_aa, codon, wrt_reference, nucs, trans_table) \n            # count types of mutations with respect to consensus \n            wrt_consensus = \\\n                    count_mutations(con_codon, con_aa, codon, wrt_consensus, nucs, trans_table)\n        # calculate pn/ps\n        subs[locus]['ref PnPs']       = calc_PnPs(wrt_reference)\n        subs[locus]['consensus PnPs'] = calc_PnPs(wrt_consensus)\n        # SNP density\n        locus_length = end - start + 1\n        subs[locus]['ref SNP density'] \\\n                = float(wrt_reference['obs syn'] + wrt_reference['obs non-syn']) / locus_length\n        subs[locus]['consensus SNP density'] \\\n                = float(wrt_consensus['obs syn'] + wrt_consensus['obs non-syn']) / locus_length\n        # sequencing coverage\n        subs[locus]['cov'] = calc_coverage(stats, start, end, locus_length, nucs)\n        info['length'] = locus_length\n        info['position'] = ((start, end), strand)\n        # point to cds info\n        subs[locus]['info'] = info\n    return subs", "response": "calculate syn and non - syn sub. rates for each locus and gene"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding consensus base based on nucleotide frequencies", "response": "def find_consensus(bases):\n    \"\"\"\n    find consensus base based on nucleotide\n    frequencies\n    \"\"\"\n    nucs = ['A', 'T', 'G', 'C', 'N']\n    total = sum([bases[nuc] for nuc in nucs if nuc in bases])\n    # save most common base as consensus (random nuc if there is a tie)\n    try:\n        top = max([bases[nuc] for nuc in nucs if nuc in bases])\n    except:\n        bases['consensus'] = ('N', 'n/a')\n        bases['consensus frequency'] = 'n/a'\n        bases['reference frequency'] = 'n/a'\n        return bases\n    top = [(nuc, bases[nuc]) for nuc in bases if bases[nuc] == top]\n    if top[0][1] == 0:\n        bases['consensus'] = ('n/a', 0)\n    else:\n        bases['consensus'] = random.choice(top)\n    if total == 0:\n        c_freq = 'n/a'\n        ref_freq = 'n/a'\n    else:\n        c_freq = float(bases['consensus'][1]) / float(total)\n        if bases['ref'] not in bases:\n            ref_freq = 0\n        else:\n            ref_freq = float(bases[bases['ref']]) / float(total)\n    bases['consensus frequency'] = c_freq\n    bases['reference frequency'] = ref_freq\n    return bases"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the frequency of each genome in the given genome and sample.", "response": "def calc_frequencies(genomes, bp_table, min_cov, min_per):\n    \"\"\"\n    print bp frequencies to table\n    genomes = {} # genomes[genome][contig][sample] = {'bp_stats':{}}\n    \"\"\"\n    nucs = ['A', 'T', 'G', 'C', 'N']\n    if bp_table is not False:\n        bp_table = open(bp_table, 'w')\n        header = ['#genome', 'contig', 'sample', 'position', \\\n                    'reference', 'ref. frequency', \\\n                    'consensus', 'con. frequency', \\\n                    'A', 'T', 'G', 'C', 'N', '# insertions', '# deletions']\n        print('\\t'.join(header), file=bp_table)\n    for genome, contigs in list(genomes.items()):\n        for contig, samples in list(contigs.items()):\n            for sample, stats in list(samples.items()):\n                for pos, ps in enumerate(stats['bp_stats'], 1):\n                    coverage = sum([ps[nuc] for nuc in nucs])\n                    for nuc in nucs:\n                        # make sure support for base passes thresholds\n                        nuc_cov = ps[nuc]\n                        if coverage == 0:\n                            nuc_per = 0\n                        else:\n                            nuc_per = (float(nuc_cov)/coverage)*100\n                        if nuc_cov < min_cov or nuc_per < min_per:\n                            del ps[nuc]\n                    ps = find_consensus(ps)\n                    genomes[genome][contig][sample][pos] = ps\n                    if bp_table is not False:\n                        out = [genome, contig, sample, pos]\n                        for i in ['ref', 'reference frequency', \\\n                                  'consensus', 'consensus frequency', \\\n                                  'A', 'T', 'G', 'C', 'N', \\\n                                  'In', 'Del']:\n                            try:\n                                if i == 'consensus':\n                                    out.append(ps[i][0])\n                                elif i in ['In', 'Del']:\n                                    out.append(len(ps[i]))\n                                else:\n                                    out.append(ps[i])\n                            except:\n                                out.append('n/a')\n                        print('\\t'.join([str(i) for i in out]), file=bp_table)\n    return genomes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting consensensus sequences for each genome and sample", "response": "def print_consensus(genomes):\n    \"\"\"\n    print consensensus sequences for each genome and sample\n    \"\"\"\n    # generate consensus sequences\n    cons = {} # cons[genome][sample][contig] = consensus\n    for genome, contigs in list(genomes.items()):\n        cons[genome] = {}\n        for contig, samples in list(contigs.items()):\n            for sample, stats in list(samples.items()):\n                if sample not in cons[genome]:\n                    cons[genome][sample] = {}\n                seq = cons[genome][sample][contig] = []\n                for pos, ps in enumerate(stats['bp_stats'], 1):\n                    ref, consensus = ps['ref'], ps['consensus'][0]\n                    if consensus == 'n/a':\n                        consensus = ref.lower()\n                    seq.append(consensus)\n    # print consensus sequences\n    for genome, samples in cons.items():\n        for sample, contigs in samples.items():\n            fn = '%s.%s.consensus.fa' % (genome, sample)\n            f = open(fn, 'w')\n            for contig, seq in contigs.items():\n                print('>%s' % (contig), file = f)\n                print(''.join(seq), file = f)\n            f.close()\n    return cons"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_stats(genomes):\n    header = ['#genome', 'contig', 'locus', 'position', 'strand', 'length', \\\n                'sample', 'coverage', \\\n                'ref. Pn/Ps', 'ref SNP density', \\\n                'consensus Pn/Ps', 'consensus SNP density', \\\n                'product']\n    print('\\t'.join(header))\n    for genome, contigs in list(genomes.items()):\n        for contig, samples in list(contigs.items()):\n            for sample, stats in list(samples.items()):\n                for locus, rates in list(stats['sub_rates'].items()):\n                    length = rates['info']['length']\n                    position, strand = rates['info']['position']\n                    position = '%s-%s' % position\n                    out = [genome, contig, locus, position, strand, length, \\\n                            sample, '%.2f' % (rates['cov']), \\\n                            rates['ref PnPs'], rates['ref SNP density'], \\\n                            rates['consensus PnPs'], rates['consensus SNP density'], \\\n                            rates['info']['product'][0]]\n                    print('\\t'.join([str(i) for i in out]))", "response": "Print substitution rate stats to table\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nquantifying genomic variation from mapping", "response": "def variation(fastas, sams, s2b, annots, qual, min_cov, min_per, no_sub, bp_table, print_con):\n    \"\"\"\n    quantify genomic variation from mapping\n    genomes[genome][contig][sample] = \\\n            {'bp_stats':[], 'sub_rates'[locus] = {ref PnPs, consensus PnPs, ...}}\n    \"\"\"\n    genomes = {}\n\n    # save base-pair data structures for each genome-sample pair\n    for sam in sams:\n        genomes, s2b = save_refs(sam, fastas, genomes, s2b)\n\n    # get base-pair frequencies for each genome-sample pair\n    for sam in sams:\n        genomes = get_bp_stats(sam, genomes, s2b, qual)\n\n    # calculate base-pair frequencies\n    genomes = calc_frequencies(genomes, bp_table, min_cov, min_per)\n\n    # print consensus genome\n    if print_con is True:\n        print('# saving consensus sequences', file = sys.stderr)\n        genomes = print_consensus(genomes)\n\n    if no_sub is True:\n        return genomes\n\n    # calculate substitution rates\n    for genome, contigs in list(genomes.items()):\n        for contig, samples in list(contigs.items()):\n            for sample in samples:\n#                print(genomes.keys())\n#                print(genomes[genome].keys())\n#                print(genomes[genome][contig].keys())\n#                print(annots.keys())\n                genomes[genome][contig][sample]['sub_rates'] = \\\n                    sub_rates(genomes[genome][contig][sample]['bp_stats'], annots[contig])\n\n    return genomes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate custom value for scaffold", "response": "def calc_custom(custom, genome, scaffold, sequence, scaffold_coverage, total_bases):\n\t\"\"\"\n\tcustom = {(reads mapped to scaffold)/(total reads for sample)}/(length of scaffold)\n\t\"\"\"\n\tindex = 0\n\tif scaffold in scaffold_coverage: # what if the scaffold does not have bases mapped back to it? (this *should* not happen)\n\t\tif genome not in custom:\n\t\t\tcustom[genome] = [[] for i in scaffold_coverage[scaffold]]\n\t\tfor cov in scaffold_coverage[scaffold]:\n\t\t\tlength = float(len(sequence[1]))\n\t\t\tbases = cov * length\n\t\t\tcustom_value = ((bases) / (total_bases[index])) / length\n\t\t\tcustom[genome][index].append(custom_value)\n\t\t\tindex += 1\n\treturn custom"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates absolute abundance of coverage", "response": "def absolute_abundance(coverage, total_bases):\n\t\"\"\"\n\tabsolute abundance = (number of bases mapped to genome / total number of bases in sample) * 100\n\t\"\"\"\n\tabsolute = {}\n\tfor genome in coverage:\n\t\tabsolute[genome] = []\n\t\tindex = 0\n\t\tfor calc in coverage[genome]:\n\t\t\tbases = calc[0]\n\t\t\ttotal = total_bases[index]\n\t\t\tabsolute[genome].append((bases / total) * float(100))\n\t\t\tindex += 1\n\ttotal_assembled = [0 for i in absolute[genome]]\n\tfor genome in absolute:\n\t\tindex = 0\n\t\tfor cov in absolute[genome]:\n\t\t\ttotal_assembled[index] += cov\n\t\t\tindex += 1\n\tabsolute['Unassembled'] = [(100 - i) for i in total_assembled]\n\treturn absolute"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef relative_abundance(coverage):\n\trelative = {}\n\tsums = []\n\tfor genome in coverage:\n\t\tfor cov in coverage[genome]:\n\t\t\tsums.append(0)\n\t\tbreak\n\tfor genome in coverage:\n\t\tindex = 0\n\t\tfor cov in coverage[genome]:\n\t\t\tsums[index] += cov\n\t\t\tindex += 1\n\tfor genome in coverage:\n\t\tindex = 0\n\t\trelative[genome] = []\n\t\tfor cov in coverage[genome]:\n\t\t\tif sums[index] == 0:\n\t\t\t\trelative[genome].append(0)\n\t\t\telse:\n\t\t\t\trelative[genome].append((cov / sums[index]) * float(100))\n\t\t\tindex += 1\n\treturn relative", "response": "cov = number of bases / length of genome\n\trelative abundance = [(cov) / sum(cov for all genomes)] * 100"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef genome_coverage(genomes, scaffold_coverage, total_bases):\n\tcoverage = {}\n\tcustom = {}\n\tstd = {}\n\tfor genome in genomes:\n\t\tfor sequence in parse_fasta(genome):\n\t\t\tscaffold = sequence[0].split('>')[1].split()[0]\n\t\t\tcoverage, std = sum_coverage(coverage, std, genome, scaffold, sequence, scaffold_coverage)\n\t\t\tcustom = calc_custom(custom, genome, scaffold, sequence, scaffold_coverage, total_bases)\n\tstd = calc_std(std)\n\tcustom_std = calc_std(custom)\n\tcustom_av = {}\n\tfor genome in custom:\n\t\tcustom_av[genome] = []\n\t\tfor sample in custom[genome]:\n\t\t\tcustom_av[genome].append(numpy.mean(sample))\n\tfor genome in coverage:\n\t\tprint('%s\\t%s' % (genome, coverage[genome][0][1]))\n\tif total_bases is True:\n\t\ttotal_bases = calc_total_mapped_bases(coverage)\n\tabsolute = absolute_abundance(coverage, total_bases)\n\tfor genome in coverage:\n\t\tcalculated = []\n\t\tfor calc in coverage[genome]:\n\t\t\tcalculated.append(calc[0] / calc[1])\n\t\tcoverage[genome] = calculated\n\trelative = relative_abundance(coverage)\n\treturn coverage, std, absolute, relative, custom_av, custom_std", "response": "calculate coverage for each genome"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_cov(cov_table, scaffold2genome):\n    size   = {} # size[genome] = genome size\n    mapped = {} # mapped[genome][sample] = mapped bases\n    # parse coverage files\n    for line in open(cov_table):\n        line = line.strip().split('\\t')\n        if line[0].startswith('#'):\n            samples = line[1:]\n            samples = [i.rsplit('/', 1)[-1].split('.', 1)[0] for i in samples]\n            continue\n        scaffold, length = line[0].split(': ')\n        length = float(length)\n        covs  = [float(i) for i in line[1:]]\n        bases = [c * length for c in covs]\n        if scaffold not in scaffold2genome:\n            continue\n        genome = scaffold2genome[scaffold]\n        if genome not in size:\n            size[genome] = 0\n            mapped[genome] = {sample:0 for sample in samples}\n        # keep track of genome size\n        size[genome] += length\n        # keep track of number of mapped bases\n        for sample, count in zip(samples, bases):\n            mapped[genome][sample] += count\n    # calculate coverage from base counts and genome size\n    coverage = {'genome':[], 'genome size (bp)':[], 'sample':[], 'coverage':[]}\n    for genome, length in size.items():\n        for sample in samples:\n            cov = mapped[genome][sample] / length\n            coverage['genome'].append(genome)\n            coverage['genome size (bp)'].append(length)\n            coverage['sample'].append(sample)\n            coverage['coverage'].append(cov)\n    return pd.DataFrame(coverage)", "response": "parse scaffold coverage table into pandas DataFrame"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate genome coverage from scaffold coverage", "response": "def genome_coverage(covs, s2b):\n    \"\"\"\n    calculate genome coverage from scaffold coverage\n    \"\"\"\n    COV = []\n    for cov in covs:\n        COV.append(parse_cov(cov, s2b))\n    return pd.concat(COV)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_s2bs(s2bs):\n    s2b = {}\n    for s in s2bs:\n        for line in open(s):\n            line = line.strip().split('\\t')\n            s, b = line[0], line[1]\n            s2b[s] = b\n    return s2b", "response": "converts a list of s2b files to dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fa2s2b(fastas):\n    s2b = {}\n    for fa in fastas:\n        for seq in parse_fasta(fa):\n            s = seq[0].split('>', 1)[1].split()[0]\n            s2b[s] = fa.rsplit('/', 1)[-1].rsplit('.', 1)[0]\n    return s2b", "response": "convert fastas to s2b dictionary"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eth_addr(f):\n    data = \"%.2x:%.2x:%.2x:%.2x:%.2x:%.2x\" % (f[0],\n                                              f[1],\n                                              f[2],\n                                              f[3],\n                                              f[4],\n                                              f[5])\n    return data", "response": "eth_addr - returns the network address of the last non - empty block"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing network data into a node in the network.", "response": "def parse_network_data(data_packet=None,\n                       include_filter_key=None,\n                       filter_keys=[],\n                       record_tcp=True,\n                       record_udp=True,\n                       record_arp=True,\n                       record_icmp=True):\n    \"\"\"build_node\n\n    :param data_packet: raw recvfrom data\n    :param filter_keys: list of strings to filter\n                        and remove baby-birding\n                        packets to yourself\n    :param record_tcp: want to record TCP frames?\n    :param record_udp: want to record UDP frames?\n    :param record_arp: want to record ARP frames?\n    :param record_icmp: want to record ICMP frames?\n    \"\"\"\n\n    node = {\"id\": build_key(),\n            \"data_type\": UNKNOWN,\n            \"eth_protocol\": None,\n            \"eth_src_mac\": None,\n            \"eth_dst_mac\": None,\n            \"eth_length\": SIZE_ETH_HEADER,\n            \"ip_version_ih1\": None,\n            \"ip_version\": None,\n            \"ip_ih1\": None,\n            \"ip_hdr_len\": None,\n            \"ip_tos\": None,\n            \"ip_tlen\": None,\n            \"ip_id\": None,\n            \"ip_frag_off\": None,\n            \"ip_ttl\": None,\n            \"ip_protocol\": None,\n            \"ip_src_addr\": None,\n            \"ip_dst_addr\": None,\n            \"tcp_src_port\": None,\n            \"tcp_dst_port\": None,\n            \"tcp_sequence\": None,\n            \"tcp_ack\": None,\n            \"tcp_resrve\": None,\n            \"tcp_data_offset\": None,\n            \"tcp_flags\": None,\n            \"tcp_adwind\": None,\n            \"tcp_urg_ptr\": None,\n            \"tcp_ffin\": None,\n            \"tcp_fsyn\": None,\n            \"tcp_frst\": None,\n            \"tcp_fpsh\": None,\n            \"tcp_fack\": None,\n            \"tcp_furg\": None,\n            \"tcp_header_size\": None,\n            \"tcp_data_size\": None,\n            \"tcp_data\": None,\n            \"udp_header_size\": None,\n            \"udp_data_size\": None,\n            \"udp_src_port\": None,\n            \"udp_dst_port\": None,\n            \"udp_data_len\": None,\n            \"udp_csum\": None,\n            \"udp_data\": None,\n            \"icmp_header_size\": None,\n            \"icmp_data\": None,\n            \"icmp_type\": None,\n            \"icmp_code\": None,\n            \"icmp_csum\": None,\n            \"icmp_data_size\": None,\n            \"arp_header_size\": None,\n            \"arp_data\": None,\n            \"arp_hw_type\": None,\n            \"arp_proto_type\": None,\n            \"arp_hw_size\": None,\n            \"arp_proto_size\": None,\n            \"arp_opcode\": None,\n            \"arp_src_mac\": None,\n            \"arp_src_ip\": None,\n            \"arp_dst_mac\": None,\n            \"arp_dst_ip\": None,\n            \"arp_data_size\": None,\n            \"target_data\": None,\n            \"full_offset\": None,\n            \"eth_header_size\": None,\n            \"ip_header_size\": None,\n            \"err\": \"\",\n            \"stream\": None,\n            \"filtered\": None,\n            \"status\": INVALID}\n\n    err = \"no_data\"\n    if not data_packet:\n        node[\"error\"] = err\n        return node\n\n    try:\n\n        err = \"missing_packet\"\n        packet = data_packet[0]\n\n        if len(packet) < 21:\n            node[\"status\"] = INVALID\n            node[\"error\"] = \"invalid packet={}\".format(packet)\n            return node\n\n        err = \"failed_parsing_ethernet\"\n        eth_packet_min = 0\n        eth_packet_max = eth_packet_min + node[\"eth_length\"]\n\n        log.info((\"unpacking ETH[{}:{}]\")\n                 .format(eth_packet_min,\n                         eth_packet_max))\n\n        eth_datagram = packet[eth_packet_min:eth_packet_max]\n        eth_header = unpack(ETH_HEADER_FORMAT, eth_datagram)\n        node[\"eth_protocol\"] = socket.ntohs(eth_header[2])\n        node[\"eth_src_mac\"] = eth_addr(packet[0:6])\n        node[\"eth_dst_mac\"] = eth_addr(packet[6:12])\n        log.debug((\"eth src={} dst={} proto={}\")\n                  .format(node[\"eth_src_mac\"],\n                          node[\"eth_dst_mac\"],\n                          node[\"eth_protocol\"]))\n\n        node[\"eth_header_size\"] = SIZE_ETH_HEADER\n\n        # Is this an IP packet:\n        if node[\"eth_protocol\"] == IP_PROTO_ETH:\n\n            ip_packet_min = SIZE_ETH_HEADER\n            ip_packet_max = SIZE_ETH_HEADER + 20\n\n            log.info((\"unpacking IP[{}:{}]\")\n                     .format(ip_packet_min,\n                             ip_packet_max))\n\n            err = (\"failed_parsing_IP[{}:{}]\").format(\n                    ip_packet_min,\n                    ip_packet_max)\n\n            # take the first 20 characters for the IP header\n            ip_datagram = packet[ip_packet_min:ip_packet_max]\n\n            ip_header = unpack(IP_HEADER_FORMAT, ip_datagram)\n            # https://docs.python.org/2/library/struct.html#format-characters\n\n            node[\"ip_header_size\"] = SIZE_IP_HEADER\n\n            node[\"ip_version_ih1\"] = ip_header[0]\n            node[\"ip_version\"] = node[\"ip_version_ih1\"] >> 4\n            node[\"ip_ih1\"] = node[\"ip_version_ih1\"] & 0xF\n            node[\"ip_hdr_len\"] = node[\"ip_ih1\"] * 4\n            node[\"ip_tos\"] = ip_header[1]\n            node[\"ip_tlen\"] = ip_header[2]\n            node[\"ip_id\"] = ip_header[3]\n            node[\"ip_frag_off\"] = ip_header[4]\n            node[\"ip_ttl\"] = ip_header[5]\n            node[\"ip_protocol\"] = ip_header[6]\n            node[\"ip_src_addr\"] = socket.inet_ntoa(ip_header[8])\n            node[\"ip_dst_addr\"] = socket.inet_ntoa(ip_header[9])\n\n            log.debug(\"-------------------------------------------\")\n            log.debug(\"IP Header - Layer 3\")\n            log.debug(\"\")\n            log.debug(\" - Version: {}\".format(node[\"ip_version\"]))\n            log.debug(\" - HDR Len: {}\".format(node[\"ip_ih1\"]))\n            log.debug(\" - TOS: {}\".format(node[\"ip_tos\"]))\n            log.debug(\" - ID: {}\".format(node[\"ip_id\"]))\n            log.debug(\" - Frag: {}\".format(node[\"ip_frag_off\"]))\n            log.debug(\" - TTL: {}\".format(node[\"ip_ttl\"]))\n            log.debug(\" - Proto: {}\".format(node[\"ip_protocol\"]))\n            log.debug(\" - Src IP: {}\".format(node[\"ip_src_addr\"]))\n            log.debug(\" - Dst IP: {}\".format(node[\"ip_dst_addr\"]))\n            log.debug(\"-------------------------------------------\")\n            log.debug(\"\")\n\n            tcp_data = None\n            udp_data = None\n            arp_data = None\n            icmp_data = None\n            target_data = None\n\n            eh = node[\"eth_header_size\"]\n            ih = node[\"ip_header_size\"]\n\n            log.debug((\"parsing ip_protocol={} data\")\n                      .format(node[\"ip_protocol\"]))\n\n            if node[\"ip_protocol\"] == TCP_PROTO_IP:\n\n                packet_min = node[\"eth_length\"] + node[\"ip_hdr_len\"]\n                packet_max = packet_min + 20\n\n                # unpack the TCP packet\n                log.info((\"unpacking TCP[{}:{}]\")\n                         .format(packet_min,\n                                 packet_max))\n\n                err = (\"failed_parsing_TCP[{}:{}]\").format(\n                        packet_min,\n                        packet_max)\n\n                tcp_datagram = packet[packet_min:packet_max]\n\n                log.debug((\"unpacking TCP Header={}\")\n                          .format(tcp_datagram))\n\n                # unpack the TCP packet\n                tcp_header = unpack(TCP_HEADER_FORMAT, tcp_datagram)\n\n                node[\"tcp_src_port\"] = tcp_header[0]\n                node[\"tcp_dst_port\"] = tcp_header[1]\n                node[\"tcp_sequence\"] = tcp_header[2]\n                node[\"tcp_ack\"] = tcp_header[3]\n                node[\"tcp_resrve\"] = tcp_header[4]\n                node[\"tcp_data_offset\"] = node[\"tcp_resrve\"] >> 4\n\n                node[\"tcp_flags\"] = tcp_header[5]\n                node[\"tcp_adwind\"] = tcp_header[6]\n                node[\"tcp_urg_ptr\"] = tcp_header[7]\n\n                # parse TCP flags\n                flag_data = unshift_flags(node[\"tcp_flags\"])\n                node[\"tcp_ffin\"] = flag_data[0]\n                node[\"tcp_fsyn\"] = flag_data[1]\n                node[\"tcp_frst\"] = flag_data[2]\n                node[\"tcp_fpsh\"] = flag_data[3]\n                node[\"tcp_fack\"] = flag_data[4]\n                node[\"tcp_furg\"] = flag_data[5]\n\n                # process the TCP options if there are\n                # currently just skip it\n                node[\"tcp_header_size\"] = SIZE_TCP_HEADER\n\n                log.debug((\"src={} dst={} seq={} ack={} doff={} flags={} \"\n                           \"f urg={} fin={} syn={} rst={} \"\n                           \"psh={} fack={} urg={}\")\n                          .format(node[\"tcp_src_port\"],\n                                  node[\"tcp_dst_port\"],\n                                  node[\"tcp_sequence\"],\n                                  node[\"tcp_ack\"],\n                                  node[\"tcp_data_offset\"],\n                                  node[\"tcp_flags\"],\n                                  node[\"tcp_urg_ptr\"],\n                                  node[\"tcp_ffin\"],\n                                  node[\"tcp_fsyn\"],\n                                  node[\"tcp_frst\"],\n                                  node[\"tcp_fpsh\"],\n                                  node[\"tcp_fack\"],\n                                  node[\"tcp_furg\"]))\n                # --------------------------------------------------------\n                err = \"failed_tcp_data\"\n\n                node[\"data_type\"] = TCP\n                node[\"tcp_header_size\"] = (\n                        node[\"ip_hdr_len\"] + (node[\"tcp_data_offset\"] * 4))\n                node[\"tcp_data_size\"] = len(packet) - node[\"tcp_header_size\"]\n                th = node[\"tcp_header_size\"]\n                node[\"full_offset\"] = eh + ih + th\n                log.info((\"TCP Data size={} th1={} th2={} \"\n                          \"offset={} value={}\")\n                         .format(node[\"tcp_data_size\"],\n                                 node[\"ip_hdr_len\"],\n                                 node[\"tcp_header_size\"],\n                                 node[\"full_offset\"],\n                                 tcp_data))\n                err = \"failed_tcp_data_offset\"\n                tcp_data = packet[node[\"full_offset\"]:]\n                target_data = tcp_data\n                node[\"error\"] = \"\"\n                node[\"status\"] = VALID\n            elif node[\"ip_protocol\"] == UDP_PROTO_IP:\n\n                packet_min = node[\"eth_length\"] + node[\"ip_hdr_len\"]\n                packet_max = packet_min + 8\n\n                # unpack the UDP packet\n                log.info((\"unpacking UDP[{}:{}]\")\n                         .format(packet_min,\n                                 packet_max))\n\n                err = (\"failed_parsing_UDP[{}:{}]\").format(\n                        packet_min,\n                        packet_max)\n\n                udp_datagram = packet[packet_min:packet_max]\n\n                log.info((\"unpacking UDP Header={}\")\n                         .format(udp_datagram))\n\n                udp_header = unpack(UDP_HEADER_FORMAT, udp_datagram)\n                node[\"udp_header_size\"] = SIZE_UDP_HEADER\n\n                node[\"udp_src_port\"] = udp_header[0]\n                node[\"udp_dst_port\"] = udp_header[1]\n                node[\"udp_data_len\"] = udp_header[2]\n                node[\"udp_csum\"] = udp_header[3]\n\n                node[\"data_type\"] = UDP\n                uh = node[\"udp_header_size\"]\n                node[\"full_offset\"] = eh + ih + uh\n                node[\"udp_data_size\"] = len(packet) - node[\"udp_header_size\"]\n                log.info((\"UDP Data size={} th1={} th2={} \"\n                          \"offset={} value={}\")\n                         .format(node[\"udp_data_size\"],\n                                 node[\"ip_hdr_len\"],\n                                 node[\"udp_header_size\"],\n                                 node[\"full_offset\"],\n                                 udp_data))\n                err = \"failed_udp_data_offset\"\n                udp_data = packet[node[\"full_offset\"]:]\n                target_data = udp_data\n                node[\"error\"] = \"\"\n                node[\"status\"] = VALID\n            elif node[\"ip_protocol\"] == ICMP_PROTO_IP:\n\n                # unpack the ICMP packet\n                packet_min = node[\"eth_length\"] + node[\"ip_hdr_len\"]\n                packet_max = packet_min + 4\n\n                log.info((\"unpacking ICMP[{}:{}]\")\n                         .format(packet_min,\n                                 packet_max))\n\n                err = (\"failed_parsing_ICMP[{}:{}]\").format(\n                        packet_min,\n                        packet_max)\n\n                icmp_datagram = packet[packet_min:packet_max]\n\n                log.info((\"unpacking ICMP Header={}\")\n                         .format(icmp_datagram))\n\n                icmp_header = unpack(ICMP_HEADER_FORMAT, icmp_datagram)\n\n                node[\"icmp_header_size\"] = SIZE_ICMP_HEADER\n\n                node[\"icmp_type\"] = icmp_header[0]\n                node[\"icmp_code\"] = icmp_header[1]\n                node[\"icmp_csum\"] = icmp_header[2]\n\n                node[\"data_type\"] = ICMP\n                ah = node[\"icmp_header_size\"]\n                node[\"full_offset\"] = eh + ih + ah\n                node[\"icmp_data_size\"] = len(packet) - node[\"icmp_header_size\"]\n                log.info((\"ICMP Data size={} th1={} th2={} \"\n                          \"offset={} value={}\")\n                         .format(node[\"icmp_data_size\"],\n                                 node[\"ip_hdr_len\"],\n                                 node[\"icmp_header_size\"],\n                                 node[\"full_offset\"],\n                                 icmp_data))\n                err = \"failed_icmp_data_offset\"\n                icmp_data = packet[node[\"full_offset\"]:]\n                target_data = icmp_data\n                node[\"error\"] = \"\"\n                node[\"status\"] = VALID\n            else:\n                node[\"error\"] = (\"unsupported_ip_protocol={}\").format(\n                                    node[\"ip_protocol\"])\n                node[\"status\"] = IP_UNSUPPORTED\n            # end of parsing supported protocols the final node data\n\n            if node[\"status\"] == VALID:\n\n                log.debug(\"filtering\")\n                # filter out delimiters in the last 64 bytes\n                if filter_keys:\n                    err = \"filtering={}\".format(len(filter_keys))\n                    log.debug(err)\n                    for f in filter_keys:\n                        if target_data:\n                            if str(f) in str(target_data):\n                                log.info((\"FOUND filter={} \"\n                                         \"in data={}\")\n                                         .format(f,\n                                                 target_data))\n                                node[\"error\"] = \"filtered\"\n                                node[\"status\"] = FILTERED\n                                node[\"filtered\"] = f\n                                break\n                    # end of tagging packets to filter out of the\n                    # network-pipe stream\n                # if there are filters\n\n                log.debug((\"was filtered={}\")\n                          .format(node[\"filtered\"]))\n\n                if not node[\"filtered\"]:\n                    err = \"building_stream\"\n                    log.debug((\"building stream target={}\")\n                              .format(target_data))\n                    stream_size = 0\n                    if target_data:\n                        try:\n                            # convert to hex string\n                            err = (\"concerting target_data to \"\n                                   \"hex string\")\n                            node[\"target_data\"] = target_data.hex()\n                        except Exception as e:\n                            log.info((\"failed converting={} to \"\n                                      \"utf-8 ex={}\")\n                                     .format(target_data,\n                                             e))\n                            err = \"str target_data\"\n                            node[\"target_data\"] = target_data\n                        # end of try/ex\n                        stream_size += len(node[\"target_data\"])\n                    # end of target_data\n                    log.debug((\"serializing stream={}\")\n                              .format(node[\"target_data\"]))\n                    node_json = json.dumps(node)\n                    data_stream = str(\"{} {}\").format(node_json,\n                                                      include_filter_key)\n                    log.debug(\"compressing\")\n\n                    if stream_size:\n                        node[\"stream\"] = data_stream\n                # end of building the stream\n\n                log.debug(\"valid\")\n            else:\n                log.error((\"unsupported ip frame ip_protocol={}\")\n                          .format(node[\"ip_protocol\"]))\n            # end of supported IP packet protocol or not\n        elif node[\"eth_protocol\"] == ARP_PROTO_ETH:\n\n            arp_packet_min = SIZE_ETH_HEADER\n            arp_packet_max = SIZE_ETH_HEADER + 28\n\n            log.info((\"unpacking ARP[{}:{}]\")\n                     .format(arp_packet_min,\n                             arp_packet_max))\n\n            err = (\"failed_parsing_ARP[{}:{}]\").format(\n                    arp_packet_min,\n                    arp_packet_max)\n\n            # take the first 28 characters for the ARP header\n            arp_datagram = packet[arp_packet_min:arp_packet_max]\n\n            arp_header = unpack(ARP_HEADER_FORMAT, arp_datagram)\n            # https://docs.python.org/2/library/struct.html#format-characters\n\n            node[\"arp_header_size\"] = SIZE_ARP_HEADER\n            node[\"arp_hw_type\"] = arp_header[0].hex()\n            node[\"arp_proto_type\"] = arp_header[1].hex()\n            node[\"arp_hw_size\"] = arp_header[2].hex()\n            node[\"arp_proto_size\"] = arp_header[3].hex()\n            node[\"arp_opcode\"] = arp_header[4].hex()\n            node[\"arp_src_mac\"] = arp_header[5].hex()\n            node[\"arp_src_ip\"] = socket.inet_ntoa(arp_header[6])\n            node[\"arp_dst_mac\"] = arp_header[7].hex()\n            node[\"arp_dst_ip\"] = socket.inet_ntoa(arp_header[8])\n\n            arp_data = \"\"\n            node[\"arp_data\"] = arp_data\n            node[\"target_data\"] = arp_data\n            node[\"data_type\"] = ARP\n            node[\"status\"] = VALID\n            node[\"arp_data_size\"] = len(packet) - node[\"arp_header_size\"]\n            node_json = json.dumps(node)\n            data_stream = str(\"{} {}\").format(node_json,\n                                              include_filter_key)\n            node[\"stream\"] = data_stream\n        else:\n            node[\"error\"] = (\"unsupported eth_frame protocol={}\").format(\n                                node[\"eth_protocol\"])\n            node[\"status\"] = ETH_UNSUPPORTED\n            log.error(node[\"error\"])\n        # end of supported ETH packet or not\n\n    except Exception as e:\n        node[\"status\"] = ERROR\n        node[\"error\"] = \"err={} failed parsing frame ex={}\".format(err,\n                                                                   e)\n        log.error(node[\"error\"])\n    # end of try/ex\n\n    return node"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef filter_ambiguity(records, percent=0.5):  # , repeats=6)\n    seqs = []\n    # Ns = ''.join(['N' for _ in range(repeats)])\n    count = 0\n    for record in records:\n        if record.seq.count('N')/float(len(record)) < percent:\n#            pos = record.seq.find(Ns)\n#            if pos >= 0:\n#                record.seq = Seq(str(record.seq)[:pos])\n            seqs.append(record)\n        count += 1\n\n    return seqs, count", "response": "Filters out sequences with too much ambiguity as defined by the method\n    parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a line of graphemes into a list of graphemes and incremented character count.", "response": "def _parse(self, root, line, idx):\n        \"\"\"\n        :param root: Tree node.\n        :param line: String to parse.\n        :param idx: Global counter of characters parsed.\n        :return: (list of parsed graphemes, incremented character count)\n        \"\"\"\n        # Base (or degenerate..) case.\n        if len(line) == 0:\n            return [], idx\n\n        parse = []\n        curr = 0\n        node = root\n        cidx = idx\n        while curr < len(line):\n            node = node.children.get(line[curr])\n            curr += 1\n            if not node:\n                break\n            if node.sentinel:\n                subparse, cidx = self._parse(root, line[curr:], idx + curr)\n                # Always keep the latest valid parse, which will be\n                # the longest-matched (greedy match) graphemes.\n                parse = [line[:curr]]\n                parse.extend(subparse)\n        if parse:\n            idx = cidx\n        return parse, idx"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef package_existent(name):\n    try:\n        response = requests.get(PYPI_URL.format(name))\n        if response.ok:\n            msg = ('[error] \"{0}\" is registered already in PyPI.\\n'\n                   '\\tSpecify another package name.').format(name)\n            raise Conflict(msg)\n    except (socket.gaierror,\n            Timeout,\n            ConnectionError,\n            HTTPError) as exc:\n        raise BackendFailure(exc)", "response": "Search package.\n\n    * :class:`bootstrap_py.exceptions.Conflict` exception occurs\n      when user specified name has already existed.\n\n    * :class:`bootstrap_py.exceptions.BackendFailure` exception occurs\n      when PyPI service is down.\n\n    :param str name: package name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite out a relative abundance for each OTU in the BIOM file.", "response": "def write_relative_abundance(rel_abd, biomf, out_fn, sort_by=None):\n    \"\"\"\n    Given a BIOM table, calculate per-sample relative abundance for\n    each OTU and write out to a tab-separated file listing OTUs as\n    rows and Samples as columns.\n    :type biom: biom object\n    :param biom: BIOM-formatted OTU/Sample abundance data\n    :type out_fn: str\n    :param out_fn: The full path to the desired output file.\n    :type sort_by: function\n    :param sort_by: A function acting as a sorting key that will determine\n                     the order in which the Sample IDs appear as columns in\n                     the output file.\n    \"\"\"\n    with open(out_fn, 'w') as out_f:\n        sids = sorted(set(biomf.ids()), key=sort_by)\n        out_f.write('#OTU ID\\t{}\\n'.format('\\t'.join(sids)))\n\n        for otuid in biomf.ids(axis=\"observation\"):\n            otuName = oc.otu_name(biomf.metadata(otuid, \"observation\")\n                                  [\"taxonomy\"])\n            sabd = [str(rel_abd[sid][otuid])\n                    if sid in rel_abd and otuid in rel_abd[sid] else '0'\n                    for sid in sids]\n            out_f.write('{}\\t{}\\n'.format(otuName, '\\t'.join(sabd)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds index to id to make it unique wrt ids", "response": "def append_index_id(id, ids):\n    \"\"\"\n    add index to id to make it unique wrt ids\n    \"\"\"\n    index = 1\n    mod = '%s_%s' % (id, index)\n    while mod in ids:\n        index += 1\n        mod = '%s_%s' % (id, index)\n    ids.append(mod)\n    return mod, ids"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef de_rep(fastas, append_index, return_original = False):\n    ids = []\n    for fasta in fastas:\n        for seq in parse_fasta(fasta):\n            header = seq[0].split('>')[1].split()\n            id = header[0]\n            if id not in ids:\n                ids.append(id)\n                if return_original is True:\n                    yield [header, seq]\n                else:\n                    yield seq\n            elif append_index == True:\n                new, ids = append_index_id(id, ids) \n                if return_original is True:\n                    yield [header, ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]]\n                else:\n                    yield ['>%s %s' % (new, ' '.join(header[1::])), seq[1]]", "response": "de - replicate fastas based on sequence names\nApps"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrequesting data associated with postcode.", "response": "def get(postcode):\n    \"\"\"\n    Request data associated with `postcode`.\n\n    :param postcode: the postcode to search for. The postcode may \n                     contain spaces (they will be removed).\n\n    :returns: a dict of the nearest postcode's data or None if no \n              postcode data is found.\n    \"\"\"\n    postcode = quote(postcode.replace(' ', ''))\n    url = '%s/postcode/%s.json' % (END_POINT, postcode)\n    return _get_json_resp(url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrequest all postcode data within the specified distance miles of the postcode.", "response": "def get_from_postcode(postcode, distance):\n    \"\"\"\n    Request all postcode data within `distance` miles of `postcode`.\n\n    :param postcode: the postcode to search for. The postcode may \n                     contain spaces (they will be removed).\n\n    :param distance: distance in miles to `postcode`.\n\n    :returns: a list of dicts containing postcode data within the \n              specified distance or `None` if `postcode` is not valid.\n    \"\"\"\n    postcode = quote(postcode.replace(' ', ''))\n    return _get_from(distance, 'postcode=%s' % postcode)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_point(self, lat, lng):\n        if abs(lat) > 90 or abs(lng) > 180:\n            msg = \"Illegal lat and/or lng, (%s, %s) provided.\" % (lat, lng)\n            raise IllegalPointException(msg)", "response": "Checks if latitude and longitude are correct."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking for cached responses, before requesting from web-service", "response": "def _lookup(self, skip_cache, fun, *args, **kwargs):\n        \"\"\" \n        Checks for cached responses, before requesting from \n        web-service\n        \"\"\"\n        if args not in self.cache or skip_cache:\n            self.cache[args] = fun(*args, **kwargs)\n        return self.cache[args]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, postcode, skip_cache=False):\n        # remove spaces and change case here due to caching\n        postcode = postcode.lower().replace(' ', '')\n        return self._lookup(skip_cache, get, postcode)", "response": "Calls postcodes. get and by default utilises a local cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_nearest(self, lat, lng, skip_cache=False): \n        lat, lng = float(lat), float(lng)\n        self._check_point(lat, lng)\n        return self._lookup(skip_cache, get_nearest, lat, lng)", "response": "Returns a dict of the nearest postcode s data."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of dicts containing the data within the specified distance from the specified postcode.", "response": "def get_from_postcode(self, postcode, distance, skip_cache=False):\n        \"\"\"\n        Calls `postcodes.get_from_postcode` but checks correctness of \n        `distance`, and by default utilises a local cache.\n\n        :param skip_cache: optional argument specifying whether to skip \n                           the cache and make an explicit request.\n\n        :raises IllegalPointException: if the latitude or longitude \n                                       are out of bounds.\n\n        :returns: a list of dicts containing postcode data within the \n                  specified distance.\n        \"\"\"\n        distance = float(distance)\n        if distance < 0:\n            raise IllegalDistanceException(\"Distance must not be negative\")\n        # remove spaces and change case here due to caching\n        postcode = postcode.lower().replace(' ', '')\n        return self._lookup(skip_cache, get_from_postcode, postcode, \n                            float(distance))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of postcode data within a specified distance from the local cache.", "response": "def get_from_geo(self, lat, lng, distance, skip_cache=False):\n        \"\"\"\n        Calls `postcodes.get_from_geo` but checks the correctness of \n        all arguments, and by default utilises a local cache.\n\n        :param skip_cache: optional argument specifying whether to skip \n                           the cache and make an explicit request.\n\n        :raises IllegalPointException: if the latitude or longitude \n                                       are out of bounds.\n\n        :returns: a list of dicts containing postcode data within the \n                  specified distance.\n        \"\"\"\n        # remove spaces and change case here due to caching\n        lat, lng, distance = float(lat), float(lng), float(distance)\n        if distance < 0:\n            raise IllegalDistanceException(\"Distance must not be negative\")\n        self._check_point(lat, lng)\n        return self._lookup(skip_cache, get_from_geo, lat, lng, distance)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets coordinates of insertions from insertion - masked sequence", "response": "def insertions_from_masked(seq):\n    \"\"\"\n    get coordinates of insertions from insertion-masked sequence\n    \"\"\"\n    insertions = []\n    prev = True\n    for i, base in enumerate(seq):\n        if base.isupper() and prev is True:\n            insertions.append([])\n            prev = False\n        elif base.islower():\n            insertions[-1].append(i)\n            prev = True\n    return [[min(i), max(i)] for i in insertions if i != []]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef analyze_fa(fa):\n    if fa.name == '<stdin>':\n        safe = 'temp.id'\n    else:\n        safe = '%s.id' % (fa.name)\n    safe = open(safe, 'w')\n    sequences = {} # sequences[id] = sequence\n    insertions = {} # insertions[id] = [[start, stop], [start, stop], ...]\n    count = 0\n    id2name = {}\n    names = []\n    for seq in parse_fasta(fa):\n        id = '%010d' % (count,)\n        name = seq[0].split('>', 1)[1]\n        id2name[id] = name\n        id2name[name] = id\n        names.append(name)\n        insertions[id] = insertions_from_masked(seq[1])\n        sequences[id] = seq\n        print('\\n'.join(['>%s' % (id), seq[1].upper()]), file=safe)\n        count += 1\n    safe.close()\n    lookup = open('%s.id.lookup' % (fa.name), 'w')\n    for i in list(id2name.items()):\n        print('\\t'.join(i), file=lookup)\n    lookup.close()\n    return safe.name, sequences, id2name, names, insertions", "response": "analyze a FASTA file and return a tuple of the name sequences id2name insertions and names"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget insertion information from header", "response": "def seq_info(names, id2names, insertions, sequences):\n    \"\"\"\n    get insertion information from header\n    \"\"\"\n    seqs = {} # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]\n    for name in names:\n        id = id2names[name]\n        gene = name.split('fromHMM::', 1)[0].rsplit(' ', 1)[1]\n        model = name.split('fromHMM::', 1)[1].split('=', 1)[1].split()[0]\n        i_gene_pos = insertions[id] # coordinates of each insertion wrt gene\n        i_model_pos = name.split('fromHMM::', 1)[1].split('model-pos(ins-len)=')[1].split()[0].split(';') # model overlap\n        i_info = []\n        for i, ins in enumerate(i_gene_pos):\n            model_pos = i_model_pos[i].split('-')[1].split('(')[0]\n            length = i_model_pos[i].split('(')[1].split(')')[0]\n            iheader = '>%s_%s insertion::seq=%s type=insertion strand=n/a gene-pos=%s-%s model-pos=%s'\\\n                    % (id, (i + 1), (i + 1), ins[0], ins[1], model_pos)\n            iseq = sequences[id][1][ins[0]:(ins[1] + 1)]\n            iseq = [iheader, iseq]\n            info = [ins, model_pos, length, iseq, [], []]\n            i_info.append(info)\n        seqs[id] = [gene, model, i_info]\n    return seqs"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the feature is contained within insertion", "response": "def check_overlap(pos, ins, thresh):\n    \"\"\"\n    make sure thresh % feature is contained within insertion\n    \"\"\"\n    ins_pos = ins[0]\n    ins_len = ins[2]\n    ol = overlap(ins_pos, pos)\n    feat_len = pos[1] - pos[0] + 1\n#    print float(ol) / float(feat_len)\n    if float(ol) / float(feat_len) >= thresh:\n        return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_orfs(fa, seqs):\n    faa = '%s.prodigal.faa' % (fa)\n    fna = '%s.prodigal.fna' % (fa)\n    gbk = '%s.prodigal.gbk' % (fa)\n    if os.path.exists(faa) is False:\n        p = subprocess.Popen('prodigal -q -i %s -a %s -d %s -c -f gbk -m -n -o %s -p meta' \\\n                % (fa, faa, fna, gbk), shell = True)\n        p.communicate()\n    for orf in parse_fasta(faa):\n        if orf[0] == []:\n            continue\n        id = orf[0].split('>')[1].split('_', 1)[0]\n        pos = sorted([int(i) for i in orf[0].split()[2:5] if i != '#'])\n        if id not in seqs:\n            continue\n        for i, ins in enumerate(seqs[id][2]):\n            if check_overlap(pos, ins, 0.90) is True:\n                seqs[id][2][i][4].append(orf)\n    return seqs, faa", "response": "find orfs and see if they overlap with insertions\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding introns in a single Rfam file using cmscan", "response": "def find_introns(fa, seqs, sequences, threads):\n    \"\"\"\n    find introns by searching Rfam intron databse using cmscan\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]\n    \"\"\"\n    db = '%s/rfam/Rfam.cm.1_1.introns' % (os.environ['databases'])\n    out = '%s.rfam-introns.cmscan' % (fa)\n    tblout = '%s.rfam-introns.cmscan-tblout' % (fa)\n    if os.path.exists(tblout) is False:\n        p = subprocess.Popen('cmscan --cpu %s --tblout %s %s %s > %s'\\\n                % (threads, tblout, db, fa, out), shell = True)\n        p.communicate()\n    for line in open(tblout):\n        if line.startswith('#'):\n            continue\n        line = line.strip().split()\n        if line[16] == '?': # does not pass inclusion threshold\n            continue\n        id = line[2]\n        type, start, stop, strand = line[0], int(line[7]), int(line[8]), line[9]\n        if 'intron' not in type.lower():\n            continue\n        pos = sorted([start, stop])\n        if id not in seqs:\n            continue\n        for i, ins in enumerate(seqs[id][2]):\n            if check_overlap(pos, ins, 0.25) is True:\n                seqs[id][2][i][5].append(['>%s_%s %s %s %s-%s' % (id, (i + 1), type, strand, start, stop), sequences[id][1][pos[0]-1:pos[1]]])\n    return seqs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef annotate_orfs(fa, seqs, threads, threshold = 1e-6):\n    db = '%s/pfam/Pfam-A.hmm' % os.environ['databases']\n    base = fa.rsplit('.', 1)[0]\n    out = '%s.pfam.hmmscan' % (base)\n    tblout = '%s.pfam.hmmscan-tblout' % (base)\n    orf2pfam = {} # orf2pfam[scaffold id][orf id] = pfam\n    if os.path.exists(tblout) is False:\n        p = subprocess.Popen('hmmscan --cpu %s --tblout %s -o %s %s %s'\\\n                % (threads, tblout, out, db, fa), shell = True)\n        p.communicate()\n    for line in open(tblout):\n        if line.startswith('#'):\n            continue\n        line = line.strip().split()\n        hit, id, e, inc = line[0], line[2], float(line[4]), int(line[17])\n        contig = id.split('_', 1)[0]\n        if e > threshold:\n            continue\n        if contig not in orf2pfam:\n            orf2pfam[contig] = {}\n        if id not in orf2pfam[contig]:\n            orf2pfam[contig][id] = hit\n    for seq, info in list(seqs.items()):\n        for insertion in info[2]:\n            annotations = {}\n            for orf in insertion[4]:\n                orf  = orf[0].split('>')[1].split()[0]\n                contig = orf.split('_', 1)[0]\n                if contig in orf2pfam and orf in orf2pfam[contig]:\n                    annotations[orf] = orf2pfam[contig][orf]\n                else:\n                    annotations[orf] = 'n/a'\n            insertion.append(annotations)\n    return seqs", "response": "annotate orfs in a set of seqs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nanalyzes the input file and return a list of sequences and id2name pairs for each insertion", "response": "def analyze_insertions(fa, threads = 6):\n    \"\"\"\n    - find ORFs using Prodigal\n    - find introns using cmscan (vs Rfam intron database)\n    - check that ORFs and intron overlap with insertion region\n    - plot insertion length versus model position for each insertion (based on insertion type)\n    \"\"\"\n    safe, sequences, id2name, names, insertions = analyze_fa(fa)\n    seqs = seq_info(names, id2name, insertions, sequences)\n    seqs, orfs = find_orfs(safe, seqs)\n    seqs = find_introns(safe, seqs, sequences, threads)\n    seqs = seqs2bool(seqs)\n    seqs = annotate_orfs(orfs, seqs, threads)\n    return seqs, id2name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting orf and intron information to boolean", "response": "def seqs2bool(seqs):\n    \"\"\"\n    convert orf and intron information to boolean\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns]], ...]]\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns], orfs?, introns?], ...]]\n    \"\"\"\n    for seq in seqs:\n        for i, ins in enumerate(seqs[seq][2]):\n            if len(ins[4]) > 0:\n                ins.append(True)\n            else:\n                ins.append(False)\n            if len(ins[5]) > 0:\n                ins.append(True)\n            else:\n                ins.append(False)\n            seqs[seq][2][i] = ins\n    return seqs"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a table of results from the sequence table", "response": "def print_table(seqs, id2name, name):\n    \"\"\"\n    print table of results\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns], orfs?, introns?], ...]]\n    \"\"\"\n    itable = open('%s.itable' % (name.rsplit('.', 1)[0]), 'w')\n    print('\\t'.join(['#sequence', 'gene', 'model', 'insertion', 'gene position', 'model position', 'length', 'orf?', 'intron?', 'orf?intron?', 'insertion', 'orf', 'intron']), file=itable)\n    for seq, info in list(seqs.items()):\n        gene, model, insertions = info\n        name = id2name[seq]\n        for i, ins in enumerate(insertions, 1):\n            gene_pos, model_pos, length, iseq, \\\n                    orfs, introns, orfs_b, introns_b, orf_annotations = ins\n            # append annotation to orf header\n            for orf in orfs:\n                parts = orf[0].split()\n                annotation = orf_annotations[parts[0].split('>')[1]]\n                orf[0] = '%s %s %s' % (parts[0], annotation, ' '.join(parts[1:]))\n            # get orf position\n            gene_pos = '-'.join([str(j) for j in gene_pos])\n            # check if orf, intron is present\n            if orfs_b is True or introns_b is True:\n                orfs_introns_b = True\n            else:\n                orfs_introns_b = False\n            out = [name, gene, model, i, gene_pos, model_pos, length, orfs_b, introns_b, orfs_introns_b]\n            out.append('|'.join(iseq))\n            out.append('|'.join(['|'.join(orf) for orf in orfs]))\n            out.append('|'.join(['|'.join(intron) for intron in introns]))\n            print('\\t'.join([str(i) for i in out]), file=itable)\n    itable.close()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints a list of sequences", "response": "def print_seqs(seqs, id2name, name):\n    \"\"\"\n    print fasta of introns and ORFs\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, iseq, [orfs], [introns], orfs?, introns?, [orf annotations]], ...]]\n    \"\"\"\n    orfs = open('%s.orfs.faa' % (name.rsplit('.', 1)[0]), 'w')\n    introns = open('%s.introns.fa' % (name.rsplit('.', 1)[0]), 'w')\n    insertions = open('%s.insertions.fa' % (name.rsplit('.', 1)[0]), 'w')\n    for seq in seqs:\n        for i, ins in enumerate(seqs[seq][2], 1):\n            model_pos = ins[1]\n            if ins[6] is True: # orf(s) in ins[4]\n                for orf in ins[4]:\n                    orf_info = orf[0].split('>')[1].split()\n                    id = orf_info[0].split('_', 1)[0]\n                    name = id2name[id]\n                    annotation = orf_info[1]\n                    strand = orf_info[7]\n                    if strand == '1':\n                        strand = '+'\n                    else:\n                        strand = '-'\n                    start, stop = sorted([int(orf_info[3]), int(orf_info[5])])\n                    header = '>%s insertion::seq=%s type=%s strand=%s gene-pos=%s-%s model-pos=%s'\\\n                            % (name, i, annotation, strand, start, stop, model_pos)\n                    print('\\n'.join([header, orf[1]]), file=orfs)\n            if ins[7] is True: # intron(s) in ins[5]\n                for intron in ins[5]:\n                    id, type, strand, pos = intron[0].split('>', 1)[1].split()\n                    name = id2name[id.split('_')[0]]\n                    header = '>%s insertion::seq=%s type=%s strand=%s gene-pos=%s model-pos=%s'\\\n                            % (name, i, type, strand, pos, model_pos)\n                    print('\\n'.join([header, intron[1]]), file=introns)\n            insertion = ins[3]\n            id, info = insertion[0].split('>')[1].split(' ', 1)\n            name = id2name[id.split('_')[0]]\n            header = '>%s %s' % (name, info)\n            print('\\n'.join([header, insertion[1]]), file=insertions)\n    insertions.close()        \n    orfs.close()\n    introns.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef max_insertion(seqs, gene, domain):\n    seqs = [i[2] for i in list(seqs.values()) if i[2] != [] and i[0] == gene and i[1] == domain]\n    lengths = []\n    for seq in seqs:\n        for ins in seq:\n            lengths.append(int(ins[2]))\n    if lengths == []:\n        return 100 \n    return max(lengths)", "response": "Returns the length of the largest insertion in a sequence."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef model_length(gene, domain):\n    if gene == '16S':\n        domain2max = {'E_coli_K12': int(1538), 'bacteria': int(1689), 'archaea': int(1563), 'eukarya': int(2652)}\n        return domain2max[domain]\n    elif gene == '23S':\n        domain2max = {'E_coli_K12': int(2903), 'bacteria': int(3146), 'archaea': int(3774), 'eukarya': int(9079)}\n        return domain2max[domain]\n    else:\n        print(sys.stderr, '# length unknown for gene: %s, domain: %s' % (gene, domain))\n        exit()", "response": "get length of model in the given gene and domain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting insertions wrt model positions", "response": "def plot_insertions(fname, seqs, gene, domain, tax, id2name):\n    \"\"\"\n    plot insertions wrt model positions\n    2 panels:\n        1. insertions with ORF\n            - triangle if has intron\n            - circle if no intron\n        2. insertion does not have ORF\n            - triangle if has intron\n            - circle if no intron\n        *3. conservation of sequence in model\n        *4. e. coli positions\n    # seqs[id] = [gene, model, [[i-gene_pos, i-model_pos, i-length, orf, intron], ...]]\n    \"\"\"\n    # import\n    import matplotlib\n    import matplotlib.pyplot as plt\n    import numpy as np\n    from matplotlib.backends.backend_pdf import PdfPages\n    import matplotlib.cm as cm\n    import matplotlib.colors as col\n    from matplotlib.font_manager import FontProperties\n    plt.rcParams['pdf.fonttype'] = 42 # illustrator pdf\n    # axis\n    max_insert = max_insertion(seqs, gene, domain)\n    height = max_insert + max_insert * 0.10\n    xmax = model_length(gene, domain)\n    f, axarr = plt.subplots(4, sharex = True, sharey = True)\n    plt.axis([0, xmax, 0, height])\n    plt.xticks(np.arange(0, xmax, 100), rotation = 45)\n    plt.yticks(np.arange(0, height, 200))\n    # labels\n    axarr[0].set_title('encodes ORF and intron')\n    axarr[1].set_title('encodes ORF, no intron')\n    axarr[2].set_title('encodes intron, no ORF')\n    axarr[3].set_title('no intron, no ORF')\n    plt.suptitle('%s %s rRNA gene insertions' % (domain, gene))\n    plt.ylabel('insertion length (bp)')\n    plt.xlabel('position on %s %s rRNA gene model' % (domain, gene))\n    # colors\n    color2tax = {}\n    if tax is False:\n        taxa = ['n/a']\n    else:\n        taxa = sorted(set([tax[id2name[i]] for i, j in list(seqs.items()) if j[2] != [] and id2name[i] in tax]))\n        if 'n/a' not in taxa:\n            taxa.append('n/a')\n    colors = cm.spectral(np.linspace(0, 1, len(taxa)))\n    colors = cycle(colors)\n    for t in taxa:\n        color2tax[t] = next(colors)\n    # markers\n    markers = setup_markers(seqs)\n    # plot\n    for name, seq in list(seqs.items()):\n        g, d = seq[0], seq[1]\n        if g != gene or d != domain or seq[2] == []:\n            continue\n        if tax is False or id2name[name] not in tax:\n            t = 'n/a'\n        else:\n            t = tax[id2name[name]]\n        c = color2tax[t] \n        for ins in seq[2]:\n            family = [i for i in list(ins[-1].values()) if i != 'n/a']\n            if len(family) != 1:\n                family = 'n/a'\n            else:\n                family = family[0]\n            x, y = int(ins[1]), int(ins[2])\n            orf, intron = ins[-3], ins[-2]\n            if orf is True: # has orf\n                if intron is True: # has intron\n                    p = 0\n                else:\n                    p = 1\n            else:\n                if intron is True: # intron, no orf\n                    p = 2\n                else:\n                    p = 3\n            marker, size = 'o', 30\n            if orf is True:\n                marker, size = markers[family]\n            axarr[p].scatter(x, y, \\\n                    edgecolors = c, marker = marker, s = size, label = family, \\\n                    facecolors = 'none', clip_on = False)\n    # legend\n    handles, labels = [], []\n    for ax in axarr[0:2]:\n        hs, ls = ax.get_legend_handles_labels()\n        for h, l in zip(hs, ls):\n            if l in labels:\n                continue\n            handles.append(h)\n            labels.append(l)\n    l1 = plt.legend(handles, labels, scatterpoints = 1, \\\n            prop = {'size':10}, loc = 'upper left', bbox_to_anchor = (1, 0.5))\n    names = [t for t in taxa]\n    boxes = [matplotlib.patches.Rectangle((0, 0), 1, 1, fc = color2tax[t]) for t in taxa]\n    plt.legend(boxes, names, scatterpoints = 1, \\\n            prop = {'size':10}, loc = 'lower left', bbox_to_anchor = (1, 0.5))\n    plt.gca().add_artist(l1) # add l1 as a separate legend\n    # save\n#    plt.tight_layout()\n    figure = plt.gcf()\n    figure.set_size_inches(12, 12)\n    pdf = PdfPages('%s.%s-%srRNAgene-insertions.pdf' % (fname.rsplit('.', 1)[0], domain, gene))\n    pdf.savefig()\n    plt.close()\n    pdf.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting insertions for each gene and domain.", "response": "def plot_by_gene_and_domain(name, seqs, tax, id2name):\n    \"\"\"\n    plot insertions for each gene and domain\n    \"\"\"\n    for gene in set([seq[0] for seq in list(seqs.values())]):\n        for domain in set([seq[1] for seq in list(seqs.values())]):\n            plot_insertions(name, seqs, gene, domain, tax, id2name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_bit(A, B, bit, id2desc):\n    Amax, Bmax = id2desc[A][-1], id2desc[B][-1]\n    norm_factor = float(numpy.average([Amax, Bmax]))\n    normalized = bit / norm_factor\n    return normalized", "response": "normalize the bit score for two ORFs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_descriptions(fastas):\n    id2desc = {}\n    for fasta in fastas:\n        for seq in parse_fasta(fasta):\n            header = seq[0].split('>')[1].split(' ')\n            id = header[0]\n            if len(header) > 1:\n                desc = ' '.join(header[1:])\n            else:\n                desc = 'n/a'\n            length = float(len([i for i in seq[1].strip() if i != '*']))\n            id2desc[id] = [fasta, desc, length]\n    return id2desc", "response": "get the description for each ORF \n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_genome_matrix(hits, fastas, id2desc, file_name):\n    out = open(file_name, 'w')\n    fastas = sorted(fastas)\n    print('## percent identity between genomes', file=out)\n    print('# - \\t %s' % ('\\t'.join(fastas)), file=out)\n    for fasta in fastas:\n        line = [fasta]\n        for other in fastas:\n            if other == fasta:\n                average = '-'\n            else:\n                average = numpy.average([hits[fasta][other][i][3] for i in hits[fasta][other]])\n            line.append(str(average))\n        print('\\t'.join(line), file=out)\n    print('', file=out)\n    print('## percent of orfs that are orthologous between genomes', file=out)\n    print('# - \\t %s' % ('\\t'.join(fastas)), file=out)\n    for fasta in fastas:\n        line = [fasta]\n        for other in fastas:\n            if other == fasta:\n                percent = '-'\n            else:\n                orthologs = float(len(hits[fasta][other]))\n                orfs = float(len([i for i in id2desc if id2desc[i][0] == fasta]))\n                percent = float(orthologs / orfs) * 100\n            line.append(str(percent))\n        print('\\t'.join(line), file=out)", "response": "prints the genome matrix for the fastas and id2desc"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef self_compare(fastas, id2desc, algorithm):\n    for fasta in fastas:\n        blast = open(search(fasta, fasta, method = algorithm, alignment = 'local'))\n        for hit in best_blast(blast, 1):\n            id, bit = hit[0].split()[0], float(hit[-1])\n            id2desc[id].append(bit)\n    return id2desc", "response": "compare genome to self to get the best possible bit score for each ORF\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncompares genomes to get rbhs", "response": "def compare_genomes(fastas, id2desc, algorithm):\n    \"\"\"\n    make all pairwise genome comparisons to get rbhs\n    rbh = {genome: {other_genome: {rbh_id: [rbh_match, pident, e, bit, norm_bit]]}}\n    \"\"\"\n    hits = {}\n    for pair in itertools.permutations(fastas, 2):\n        if pair[0] not in hits:\n            hits[pair[0]] = {}\n        hits[pair[0]][pair[1]] = {}\n    for pair in itertools.combinations(fastas, 2):\n        A, B = pair\n        blastF = open(search(A, B, method = algorithm, alignment = 'local'))\n        blastR = open(search(B, A, method = algorithm, alignment = 'local'))\n        for compare in [A, B, blastF], [B, A, blastR]:\n            query, ref, blast = compare\n            for hit in best_blast(blast, 1):\n                a, b, pident, qstart, qend, e, bit = \\\n                        hit[0].split()[0], hit[1].split()[0], float(hit[2]), float(hit[6]), float(hit[7]), float(hit[-2]), float(hit[-1])\n                alignment_length = abs(qstart - qend + 1)\n                norm_bit = normalize_bit(a, b, bit, id2desc)\n                length_fraction = float((alignment_length) / id2desc[a][2])\n                hits[query][ref][a] = ['fbh', a, b, pident, length_fraction, e, bit, norm_bit]\n    return hits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the thresholds of the BLAST between two genome - level BLAST files.", "response": "def calc_thresholds(rbh, file_name, thresholds = [False, False, False, False], stdevs = 2):\n    \"\"\"\n    if thresholds are not specififed, calculate based on the distribution of normalized bit scores\n    \"\"\"\n    calc_threshold = thresholds[-1]\n    norm_threshold = {}\n    for pair in itertools.permutations([i for i in rbh], 2):\n        if pair[0] not in norm_threshold:\n            norm_threshold[pair[0]] = {}\n        norm_threshold[pair[0]][pair[1]] = {}\n    out = open(file_name, 'w')\n    print('#### summary of rbh comparisons\\n', file=out)\n    comparisons = []\n    for genome in rbh:\n        for compare in rbh[genome]:\n            pair = ''.join(sorted([genome, compare]))\n            if pair in comparisons:\n                continue\n            comparisons.append(pair)\n            scores = {'percent identity': [], 'e-value': [], 'bit score': [], 'normalized bit score': [], 'alignment length fraction': []}\n            print('### blast between %s and %s\\n' % (genome, compare), file=out)\n            for id in rbh[genome][compare]:\n                pident, length_fraction, e, bit, norm_bit = rbh[genome][compare][id][3:]\n                scores['percent identity'].append(pident)\n                scores['alignment length fraction'].append(length_fraction)\n                scores['e-value'].append(e)\n                scores['bit score'].append(bit)\n                scores['normalized bit score'].append(norm_bit)\n            if calc_threshold is True:\n                norms = scores['normalized bit score']\n                average = numpy.average(norms) \n                std = numpy.std(norms)\n                normal_thresh = average - (std * stdevs)\n                print('## average normalized bit score: %s' % average, file=out)\n                print('## standard deviation of normalized bit scores: %s' % std, file=out)\n                print('## normalized bit score threshold set to: %s\\n' % (normal_thresh), file=out)\n                norm_threshold[genome][compare], norm_threshold[compare][genome] = normal_thresh, normal_thresh\n            for score in scores:\n                print('## %s' % (score), file=out)\n                if len(scores[score]) > 0:\n                    print('## average: %s' % numpy.average(scores[score]), file=out)\n#                    hist = histogram(scores[score], [])\n#                    for line in hist:\n#                        print >> out, line\n                print('', file=out)\n    out.close()\n    if calc_threshold is True:\n        return thresholds[0:-1] + [norm_threshold]\n    else:\n        return thresholds"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rbh_network(id2desc, rbh, file_name, thresholds = [False, False, False, False]):\n    g = nx.Graph() # network graph for storing rbhs\n    filtered = {}\n    e_thresh, bit_thresh, length_thresh, norm_thresh = thresholds\n    for genome in rbh:\n        filtered[genome] = {}\n        for other in rbh:\n            if other != genome:\n                filtered[genome][other] = {}\n    comparisons = []\n    for genome in rbh:\n        for compare in rbh[genome]:\n            pair = ''.join(sorted([genome, compare]))\n            if pair in comparisons: # make sure you only have to make rbh comparison once\n                continue\n            comparisons.append(pair)\n            for orf in rbh[genome][compare]:\n                scoresA = rbh[genome][compare][orf]\n                match = scoresA[2]\n                if match in rbh[compare][genome]:\n                    scoresB = rbh[compare][genome][match]\n                else:\n                    scoresB = scoresA\n                typeA, AA, BA, pidentA, lengthA, eA, bitA, norm_bitA = scoresA\n                typeB, AB, BB, pidentB, lengthB, eB, bitB, norm_bitB = scoresB\n                if norm_thresh is not False:\n                    if norm_bitA < norm_thresh[genome][compare] \\\n                        or norm_bitB < norm_thresh[compare][genome]:\n                        continue\n                if e_thresh is not False:\n                    if eA > e_thresh \\\n                        or eB > e_thresh:\n                        continue\n                if bit_thresh is not False:\n                    if bitA < bit_thresh \\\n                        or bitB < bit_thresh:\n                        continue\n                if length_thresh is not False:\n                    if lengthA < length_thresh \\\n                        or lengthB < length_thresh:\n                        continue\n                if id2desc[orf][2] > id2desc[match][2]:\n                    scores = scoresA\n                elif id2desc[orf][2] < id2desc[match][2]:\n                    scores = scoresB\n                else:\n                    scores = sorted([scoresA, scoresB], key = itemgetter(-1), reverse = True)[0]\n                type, A, B, pident, length, e, bit, norm_bit = scores\n                g.add_edge(A, B, match_type = type, length_fraction = length, \\\n                        percent_id = pident, e_value = e, bit_score = bit, norm_bit = norm_bit)\n                filtered[genome][compare][orf] = scoresA\n                filtered[compare][genome][match] = scoresB\n    missing = set([i for i in id2desc]).difference(set([i for i in g]))\n    for orf in missing:\n        g.add_edge(orf, orf, percent_id = 0, e_value = 0, bit_score = 0, norm_bit = 0, \\\n                    length_fraction = 0)\n    nx.write_edgelist(g, file_name, delimiter = '\\t', data = ['match_type', 'length_fraction', 'percent_id', 'e_value', 'bit_score', 'norm_bit'])\n    return g, filtered", "response": "make the network based on rbh and file_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef neto(fastas, algorithm = 'usearch', e = 0.01, bit = 40, length = .65, norm_bit = False):\n    thresholds = [e, bit, length, norm_bit]\n    id2desc = get_descriptions(fastas)\n            # get [fasta, description, length] for ORF id\n    id2desc = self_compare(fastas, id2desc, algorithm)\n            # get best possible bit score for each ORF \n            # (comparing with itself) [fasta, description, length, bestbit]\n    hits = compare_genomes(fastas, id2desc, algorithm)\n            # pair wise genome comparisons {genome: {id: [match_type = 'rbh' or 'fbh', scores]}}\n    calc_thresholds(hits, file_name = 'fbh.scores.summary.txt')\n    rbh_network(id2desc, hits, file_name = 'fbh.network.edges.txt')\n    hits, rbh = find_rbh(hits, id2desc)\n            # remove hits that are not reciprocal best blast hits\n    thresholds = calc_thresholds(rbh, 'rbh.scores.summary.txt', thresholds)\n            # print rbh score summary to rbh_score_summary.txt and\n            # calculate normalized bit score cutoff for each pair of\n            # genomes, if desired\n    g = rbh_network(id2desc, rbh, file_name = 'rbh.network.edges.txt')\n    filtered_g, filtered_rbh = rbh_network(id2desc, rbh, 'rbh.filtered.network.edges.txt', thresholds)\n    calc_thresholds(filtered_rbh, file_name = 'rbh.filtered.scores.summary.txt')\n    print_summary(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.nodes.txt')\n    print_network_matrix(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.matrix.txt')\n    print_genome_matrix(filtered_rbh, fastas, id2desc, file_name = 'rbh.filtered.network.genome_matrix.txt')\n    split_g = split_network(filtered_g, id2desc, file_name = 'rbh.filtered.split.network.edges.txt')\n    print_summary(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.nodes.txt')\n    print_network_matrix(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.matrix.txt')\n    return split_g", "response": "This function is used to split a single rbh network into two groups of RBH genomes and a single RBH network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a rbh network for all pair - wise genome comparisons", "response": "def network(thresholds, fastas, algorithm = 'usearch'):\n    \"\"\"\n    make a rbh network for all pair-wise genome comparisons\n    - filter network based on normalized bit score (default, automatic) or specified e-value / bit score\n    - evaluate the scores for each genome pair compared\n    - build second network that is filtered so that clusters have only one ORF per genome\n    - evaluate the scores in this network\n        - consider including fbhs under threshold if they don't violate cluster\n    - compare 'connected-ness' for each genome\n    \"\"\"\n    id2desc = get_descriptions(fastas)\n            # get [fasta, description, length] for ORF id\n    id2desc = self_compare(fastas, id2desc, algorithm)\n            # get best possible bit score for each ORF \n            # (comparing with itself) [fasta, description, length, bestbit]\n    hits = compare_genomes(fastas, id2desc, algorithm)\n            # pair wise genome comparisons {genome: {id: [match_type = 'rbh' or 'fbh', scores]}}\n    calc_thresholds(hits, file_name = 'fbh.scores.summary.txt')\n    rbh_network(id2desc, hits, file_name = 'fbh.network.edges.txt')\n    hits, rbh = find_rbh(hits, id2desc)\n            # remove hits that are not reciprocal best blast hits\n    thresholds = calc_thresholds(rbh, 'rbh.scores.summary.txt', thresholds)\n            # print rbh score summary to rbh_score_summary.txt and\n            # calculate normalized bit score cutoff for each pair of\n            # genomes, if desired\n    g = rbh_network(id2desc, rbh, file_name = 'rbh.network.edges.txt')\n    filtered_g, filtered_rbh = rbh_network(id2desc, rbh, 'rbh.filtered.network.edges.txt', thresholds)\n    calc_thresholds(filtered_rbh, file_name = 'rbh.filtered.scores.summary.txt')\n    print_summary(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.nodes.txt')\n    print_network_matrix(filtered_g, fastas, id2desc, file_name = 'rbh.filtered.network.matrix.txt')\n    print_genome_matrix(filtered_rbh, fastas, id2desc, file_name = 'rbh.filtered.network.genome_matrix.txt')\n    split_g = split_network(filtered_g, id2desc, file_name = 'rbh.filtered.split.network.edges.txt')\n    print_summary(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.nodes.txt')\n    print_network_matrix(split_g, fastas, id2desc, file_name = 'rbh.filtered.split.network.matrix.txt')\n    return split_g"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef user_specific_data_directory(user_context):\n  return  os.path.join(\n    user_context.steam.userdata_directory,\n    user_context.user_id\n  )", "response": "Returns the subdirectory in the userdata which acts as the root of the user - specific filesystem in Steam"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize the ISO - 19115 data map with the XML tree and the XML structure of the ISO - 19115 standard.", "response": "def _init_data_map(self):\n        \"\"\" OVERRIDDEN: Initialize required ISO-19115 data map with XPATHS and specialized functions \"\"\"\n\n        if self._data_map is not None:\n            return  # Initiation happens once\n\n        # Parse and validate the ISO metadata root\n\n        if self._xml_tree is None:\n            iso_root = ISO_ROOTS[0]\n        else:\n            iso_root = get_element_name(self._xml_tree)\n\n        if iso_root not in ISO_ROOTS:\n            raise InvalidContent('Invalid XML root for ISO-19115 standard: {root}', root=iso_root)\n\n        iso_data_map = {'_root': iso_root}\n        iso_data_map.update(_iso_tag_roots)\n        iso_data_map.update(_iso_tag_formats)\n\n        iso_data_structures = {}\n\n        # Capture and format complex XPATHs\n\n        ad_format = iso_data_map[ATTRIBUTES]\n        ft_source = iso_data_map['_attr_src'].replace('/carrierOfCharacteristics/FC_FeatureAttribute', '')\n\n        iso_data_structures[ATTRIBUTES] = format_xpaths(\n            _iso_definitions[ATTRIBUTES],\n\n            label=ad_format.format(ad_path='memberName/LocalName'),\n            aliases=ad_format.format(ad_path='aliases/LocalName'),  # Not in spec\n            definition=ad_format.format(ad_path='definition/CharacterString'),\n\n            # First try to populate attribute definition source from FC_FeatureAttribute\n            definition_src=iso_data_map['_attr_src'] + '/organisationName/CharacterString',\n            _definition_src=iso_data_map['_attr_src'] + '/individualName/CharacterString',\n\n            # Then assume feature type source is the same as attribute: populate from FC_FeatureType\n            __definition_src=ft_source + '/organisationName/CharacterString',\n            ___definition_src=ft_source + '/individualName/CharacterString'\n        )\n\n        bb_format = iso_data_map[BOUNDING_BOX]\n        iso_data_structures[BOUNDING_BOX] = format_xpaths(\n            _iso_definitions[BOUNDING_BOX],\n            east=bb_format.format(bbox_path='eastBoundLongitude/Decimal'),\n            south=bb_format.format(bbox_path='southBoundLatitude/Decimal'),\n            west=bb_format.format(bbox_path='westBoundLongitude/Decimal'),\n            north=bb_format.format(bbox_path='northBoundLatitude/Decimal')\n        )\n\n        ct_format = iso_data_map[CONTACTS]\n        iso_data_structures[CONTACTS] = format_xpaths(\n            _iso_definitions[CONTACTS],\n            name=ct_format.format(ct_path='individualName/CharacterString'),\n            organization=ct_format.format(ct_path='organisationName/CharacterString'),\n            position=ct_format.format(ct_path='positionName/CharacterString'),\n            email=ct_format.format(\n                ct_path='contactInfo/CI_Contact/address/CI_Address/electronicMailAddress/CharacterString'\n            )\n        )\n\n        dt_format = iso_data_map[DATES]\n        iso_data_structures[DATES] = {\n            DATE_TYPE_MULTIPLE: dt_format.format(type_path='TimeInstant/timePosition'),\n            DATE_TYPE_RANGE_BEGIN: dt_format.format(type_path='TimePeriod/begin/TimeInstant/timePosition'),\n            DATE_TYPE_RANGE_END: dt_format.format(type_path='TimePeriod/end/TimeInstant/timePosition'),\n            DATE_TYPE_SINGLE: dt_format.format(type_path='TimeInstant/timePosition')  # Same as multiple\n        }\n        iso_data_structures[DATES][DATE_TYPE_RANGE] = [\n            iso_data_structures[DATES][DATE_TYPE_RANGE_BEGIN],\n            iso_data_structures[DATES][DATE_TYPE_RANGE_END]\n        ]\n\n        df_format = iso_data_map[DIGITAL_FORMS]\n        iso_data_structures[DIGITAL_FORMS] = format_xpaths(\n            _iso_definitions[DIGITAL_FORMS],\n            name=df_format.format(df_path='name/CharacterString'),\n            content='',  # Not supported in ISO-19115 (appending to spec)\n            decompression=df_format.format(df_path='fileDecompressionTechnique/CharacterString'),\n            version=df_format.format(df_path='version/CharacterString'),\n            specification=df_format.format(df_path='specification/CharacterString'),\n            access_desc=iso_data_map['_access_desc'],\n            access_instrs=iso_data_map['_access_instrs'],\n            network_resource=iso_data_map['_network_resource']\n        )\n\n        keywords_structure = {\n            'keyword_root': 'MD_Keywords/keyword',\n            'keyword_type': 'MD_Keywords/type/MD_KeywordTypeCode',\n            'keyword': 'MD_Keywords/keyword/CharacterString'\n        }\n        for keyword_prop in KEYWORD_PROPS:\n            iso_data_structures[keyword_prop] = deepcopy(keywords_structure)\n\n        lw_format = iso_data_map[LARGER_WORKS]\n        iso_data_structures[LARGER_WORKS] = format_xpaths(\n            _iso_definitions[LARGER_WORKS],\n            title=lw_format.format(lw_path='title/CharacterString'),\n            edition=lw_format.format(lw_path='edition/CharacterString'),\n            origin=iso_data_map['_lw_citation'].format(lw_path='individualName/CharacterString'),\n            online_linkage=iso_data_map['_lw_linkage'].format(lw_path='linkage/URL'),\n            other_citation=lw_format.format(lw_path='otherCitationDetails/CharacterString'),\n            date=lw_format.format(lw_path='editionDate/Date'),\n            place=iso_data_map['_lw_contact'].format(lw_path='address/CI_Address/city/CharacterString'),\n            info=iso_data_map['_lw_citation'].format(lw_path='organisationName/CharacterString')\n        )\n\n        ps_format = iso_data_map[PROCESS_STEPS]\n        iso_data_structures[PROCESS_STEPS] = format_xpaths(\n            _iso_definitions[PROCESS_STEPS],\n            description=ps_format.format(ps_path='description/CharacterString'),\n            date=ps_format.format(ps_path='dateTime/DateTime'),\n            sources=ps_format.format(\n                ps_path='source/LI_Source/sourceCitation/CI_Citation/alternateTitle/CharacterString'\n            )\n        )\n\n        ri_format = iso_data_map[RASTER_INFO]\n        iso_data_structures[RASTER_INFO] = format_xpaths(\n            _iso_definitions[RASTER_DIMS],\n            type=ri_format.format(ri_path='dimensionName/MD_DimensionNameTypeCode'),\n            _type=ri_format.format(ri_path='dimensionName/MD_DimensionNameTypeCode/@codeListValue'),\n            size=ri_format.format(ri_path='dimensionSize/Integer'),\n            value=ri_format.format(ri_path='resolution/Measure'),\n            units=ri_format.format(ri_path='resolution/Measure/@uom')\n        )\n\n        # Assign XPATHS and gis_metadata.utils.ParserProperties to data map\n\n        for prop, xpath in iteritems(dict(iso_data_map)):\n            if prop == ATTRIBUTES:\n                iso_data_map[prop] = ParserProperty(self._parse_attribute_details, self._update_attribute_details)\n\n            elif prop in (CONTACTS, PROCESS_STEPS):\n                iso_data_map[prop] = ParserProperty(self._parse_complex_list, self._update_complex_list)\n\n            elif prop in (BOUNDING_BOX, LARGER_WORKS):\n                iso_data_map[prop] = ParserProperty(self._parse_complex, self._update_complex)\n\n            elif prop == DATES:\n                iso_data_map[prop] = ParserProperty(self._parse_dates, self._update_dates)\n\n            elif prop == DIGITAL_FORMS:\n                iso_data_map[prop] = ParserProperty(self._parse_digital_forms, self._update_digital_forms)\n\n            elif prop in KEYWORD_PROPS:\n                iso_data_map[prop] = ParserProperty(self._parse_keywords, self._update_keywords)\n\n            elif prop == RASTER_INFO:\n                iso_data_map[prop] = ParserProperty(self._parse_raster_info, self._update_raster_info)\n\n            else:\n                iso_data_map[prop] = xpath\n\n        self._data_map = iso_data_map\n        self._data_structures = iso_data_structures"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the attribute details file and return the attribute details object", "response": "def _parse_attribute_details(self, prop=ATTRIBUTES):\n        \"\"\" Concatenates a list of Attribute Details data structures parsed from a remote file \"\"\"\n\n        parsed_attributes = self._parse_attribute_details_file(prop)\n        if parsed_attributes is None:\n            # If not in the (official) remote location, try the tree itself\n            parsed_attributes = self._parse_complex_list(prop)\n\n        for attribute in (a for a in parsed_attributes if not a['aliases']):\n            # Aliases are not in ISO standard: default to label\n            attribute['aliases'] = attribute['label']\n\n        return get_default_for_complex(prop, parsed_attributes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the attribute details file and return a list of Attribute Details data structures.", "response": "def _parse_attribute_details_file(self, prop=ATTRIBUTES):\n        \"\"\" Concatenates a list of Attribute Details data structures parsed from a remote file \"\"\"\n\n        # Parse content from remote file URL, which may be stored in one of two places:\n        #    Starting at: contentInfo/MD_FeatureCatalogueDescription/featureCatalogueCitation\n        #    ATTRIBUTE: href\n        #    ELEMENT TEXT: CI_Citation/.../CI_Contact/onlineResource/CI_OnlineResource/linkage\n\n        self._attr_details_file_url = parse_property(\n            self._xml_tree, None, self._data_map, '_attributes_file'\n        )\n        if not self._attr_details_file_url:\n            return None\n\n        try:\n            tree_to_parse = get_remote_element(self._attr_details_file_url)\n        except Exception:\n            self._attr_details_file_url = None\n            return None\n\n        xpath_map = self._data_structures[ATTRIBUTES]\n        xpath_root = self._get_xroot_for(prop)\n\n        return parse_complex_list(tree_to_parse, xpath_root, xpath_map, prop)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the digital forms and transfer options and return a list of dictionaries.", "response": "def _parse_digital_forms(self, prop=DIGITAL_FORMS):\n        \"\"\" Concatenates a list of Digital Form data structures parsed from the metadata \"\"\"\n\n        xpath_map = self._data_structures[prop]\n\n        # Parse base digital form fields: 'name', 'content', 'decompression', 'version', 'specification'\n        xpath_root = self._data_map['_digital_forms_root']\n        digital_forms = parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)\n\n        # Parse digital form transfer option fields: 'access_desc', 'access_instrs', 'network_resource'\n        xpath_root = self._data_map['_transfer_options_root']\n        transfer_opts = parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)\n\n        # Split out digital form content that has been appended to specifications\n\n        content_delim = _DIGITAL_FORMS_CONTENT_DELIM\n\n        for digital_form in digital_forms:\n            specs = reduce_value(digital_form['specification'])\n            specs = specs.splitlines() if isinstance(specs, string_types) else specs\n\n            specifications = wrap_value(s.strip() for s in specs)\n\n            digital_form['content'] = []\n            digital_form['specification'] = []\n            has_content = False\n\n            # For each specification, insert delim before appending content\n            for spec in specifications:\n                has_content = has_content or spec == content_delim\n\n                if not has_content:\n                    digital_form['specification'].append(spec)\n                elif spec != content_delim:\n                    digital_form['content'].append(spec)\n\n            # Reduce spec and content to single string values if possible\n            for form_prop in ('content', 'specification'):\n                digital_form[form_prop] = reduce_value(filter_empty(digital_form[form_prop], u''))\n\n        # Combine digital forms and transfer options into a single complex struct\n\n        df_len = len(digital_forms)\n        to_len = len(transfer_opts)\n        parsed_forms = []\n\n        for idx in xrange(0, max(df_len, to_len)):\n            digital_form = {}.fromkeys(_iso_definitions[prop], u'')\n\n            if idx < df_len:\n                digital_form.update(i for i in digital_forms[idx].items() if i[1])\n            if idx < to_len:\n                digital_form.update(i for i in transfer_opts[idx].items() if i[1])\n\n            if any(digital_form.values()):\n                parsed_forms.append(digital_form)\n\n        return get_default_for_complex(prop, parsed_forms)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_keywords(self, prop):\n\n        keywords = []\n\n        if prop in KEYWORD_PROPS:\n            xpath_root = self._data_map['_keywords_root']\n            xpath_map = self._data_structures[prop]\n\n            xtype = xpath_map['keyword_type']\n            xpath = xpath_map['keyword']\n            ktype = KEYWORD_TYPES[prop]\n\n            for element in get_elements(self._xml_tree, xpath_root):\n                if get_element_text(element, xtype).lower() == ktype.lower():\n                    keywords.extend(get_elements_text(element, xpath))\n\n        return keywords", "response": "Parse type - specific keywords from the metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the raster_info property into a single raster_info complex struct.", "response": "def _parse_raster_info(self, prop=RASTER_INFO):\n        \"\"\" Collapses multiple dimensions into a single raster_info complex struct \"\"\"\n\n        raster_info = {}.fromkeys(_iso_definitions[prop], u'')\n\n        # Ensure conversion of lists to newlines is in place\n        raster_info['dimensions'] = get_default_for_complex_sub(\n            prop=prop,\n            subprop='dimensions',\n            value=parse_property(self._xml_tree, None, self._data_map, '_ri_num_dims'),\n            xpath=self._data_map['_ri_num_dims']\n        )\n\n        xpath_root = self._get_xroot_for(prop)\n        xpath_map = self._data_structures[prop]\n\n        for dimension in parse_complex_list(self._xml_tree, xpath_root, xpath_map, RASTER_DIMS):\n            dimension_type = dimension['type'].lower()\n\n            if dimension_type == 'vertical':\n                raster_info['vertical_count'] = dimension['size']\n\n            elif dimension_type == 'column':\n                raster_info['column_count'] = dimension['size']\n                raster_info['x_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()\n\n            elif dimension_type == 'row':\n                raster_info['row_count'] = dimension['size']\n                raster_info['y_resolution'] = u' '.join(dimension[k] for k in ['value', 'units']).strip()\n\n        return raster_info if any(raster_info[k] for k in raster_info) else {}"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_attribute_details(self, **update_props):\n\n        tree_to_update = update_props['tree_to_update']\n        xpath = self._data_map['_attr_citation']\n\n        # Cannot write to remote file: remove the featureCatalogueCitation element\n\n        self._attr_details_file_url = None\n        remove_element(tree_to_update, xpath, True)\n\n        return self._update_complex_list(**update_props)", "response": "Update the ISO Attribute Details metadata file for the ISO attributes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_digital_forms(self, **update_props):\n\n        digital_forms = wrap_value(update_props['values'])\n\n        # Update all Digital Form properties: distributionFormat*\n\n        xpath_map = self._data_structures[update_props['prop']]\n\n        dist_format_props = ('name', 'decompression', 'version', 'specification')\n        dist_format_xroot = self._data_map['_digital_forms_root']\n        dist_format_xmap = {prop: xpath_map[prop] for prop in dist_format_props}\n        dist_formats = []\n\n        for digital_form in digital_forms:\n            dist_format = {prop: digital_form[prop] for prop in dist_format_props}\n\n            if digital_form.get('content'):\n                dist_spec = wrap_value(digital_form.get('specification'))\n                dist_spec.append(_DIGITAL_FORMS_CONTENT_DELIM)\n                dist_spec.extend(wrap_value(digital_form['content']))\n                dist_format['specification'] = dist_spec\n\n            dist_formats.append(dist_format)\n\n        update_props['values'] = dist_formats\n        dist_formats = update_complex_list(\n            xpath_root=dist_format_xroot, xpath_map=dist_format_xmap, **update_props\n        )\n\n        # Update all Network Resources: transferOptions+\n\n        trans_option_props = ('access_desc', 'access_instrs', 'network_resource')\n        trans_option_xroot = self._data_map['_transfer_options_root']\n        trans_option_xmap = {prop: self._data_map['_' + prop] for prop in trans_option_props}\n\n        trans_options = []\n        for digital_form in digital_forms:\n            trans_options.append({prop: digital_form[prop] for prop in trans_option_props})\n\n        update_props['values'] = trans_options\n        trans_options = update_complex_list(\n            xpath_root=trans_option_xroot, xpath_map=trans_option_xmap, **update_props\n        )\n\n        return {\n            'distribution_formats': dist_formats,\n            'transfer_options': trans_options\n        }", "response": "Update the properties of all Digital Forms metadata structures."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating keywords metadata for ISO type - specific Keywords metadata", "response": "def _update_keywords(self, **update_props):\n        \"\"\" Update operation for ISO type-specific Keywords metadata: Theme or Place \"\"\"\n\n        tree_to_update = update_props['tree_to_update']\n        prop = update_props['prop']\n        values = update_props['values']\n\n        keywords = []\n\n        if prop in KEYWORD_PROPS:\n            xpath_root = self._data_map['_keywords_root']\n            xpath_map = self._data_structures[prop]\n\n            xtype = xpath_map['keyword_type']\n            xroot = xpath_map['keyword_root']\n            xpath = xpath_map['keyword']\n            ktype = KEYWORD_TYPES[prop]\n\n            # Remove descriptiveKeyword nodes according to type\n            for element in get_elements(tree_to_update, xpath_root):\n                if get_element_text(element, xtype).lower() == ktype.lower():\n                    remove_element(tree_to_update, xpath_root)\n\n            element = insert_element(tree_to_update, 0, xpath_root)\n            insert_element(element, 0, xtype, ktype)  # Add the type node\n\n            keywords.extend(update_property(element, xroot, xpath, prop, values))\n\n        return keywords"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the raster_info complex struct with the new values.", "response": "def _update_raster_info(self, **update_props):\n        \"\"\" Derives multiple dimensions from a single raster_info complex struct \"\"\"\n\n        tree_to_update = update_props['tree_to_update']\n        prop = update_props['prop']\n        values = update_props.pop('values')\n\n        # Update number of dimensions at raster_info root (applies to all dimensions below)\n\n        xroot, xpath = None, self._data_map['_ri_num_dims']\n        raster_info = [update_property(tree_to_update, xroot, xpath, prop, values.get('dimensions', u''))]\n\n        # Derive vertical, longitude, and latitude dimensions from raster_info\n\n        xpath_root = self._get_xroot_for(prop)\n        xpath_map = self._data_structures[prop]\n\n        v_dimension = {}\n        if values.get('vertical_count'):\n            v_dimension = v_dimension.fromkeys(xpath_map, u'')\n            v_dimension['type'] = 'vertical'\n            v_dimension['size'] = values.get('vertical_count', u'')\n\n        x_dimension = {}\n        if values.get('column_count') or values.get('x_resolution'):\n            x_dimension = x_dimension.fromkeys(xpath_map, u'')\n            x_dimension['type'] = 'column'\n            x_dimension['size'] = values.get('column_count', u'')\n            x_dimension['value'] = values.get('x_resolution', u'')\n\n        y_dimension = {}\n        if values.get('row_count') or values.get('y_resolution'):\n            y_dimension = y_dimension.fromkeys(xpath_map, u'')\n            y_dimension['type'] = 'row'\n            y_dimension['size'] = values.get('row_count', u'')\n            y_dimension['value'] = values.get('y_resolution', u'')\n\n        # Update derived dimensions as complex list, and append affected elements for return\n\n        update_props['prop'] = RASTER_DIMS\n        update_props['values'] = [v_dimension, x_dimension, y_dimension]\n\n        raster_info += update_complex_list(xpath_root=xpath_root, xpath_map=xpath_map, **update_props)\n\n        return raster_info"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the XML tree with the current XML properties.", "response": "def update(self, use_template=False, **metadata_defaults):\n        \"\"\" OVERRIDDEN: Prevents writing multiple CharacterStrings per XPATH property \"\"\"\n\n        self.validate()\n\n        tree_to_update = self._xml_tree if not use_template else self._get_template(**metadata_defaults)\n        supported_props = self._metadata_props\n\n        # Iterate over keys, and extract non-primitive root for all XPATHs\n        #    xroot = identificationInfo/MD_DataIdentification/abstract/\n        #    xpath = identificationInfo/MD_DataIdentification/abstract/CharacterString\n        #\n        # This prevents multiple primitive tags from being inserted under an element\n\n        for prop, xpath in iteritems(self._data_map):\n            if not prop.startswith('_') or prop.strip('_') in supported_props:\n                # Send only public or alternate properties\n                xroot = self._trim_xpath(xpath, prop)\n                values = getattr(self, prop, u'')\n                update_property(tree_to_update, xroot, xpath, prop, values, supported_props)\n\n        return tree_to_update"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove primitive type tags from an XPATH", "response": "def _trim_xpath(self, xpath, prop):\n        \"\"\" Removes primitive type tags from an XPATH \"\"\"\n\n        xroot = self._get_xroot_for(prop)\n\n        if xroot is None and isinstance(xpath, string_types):\n            xtags = xpath.split(XPATH_DELIM)\n\n            if xtags[-1] in _iso_tag_primitives:\n                xroot = XPATH_DELIM.join(xtags[:-1])\n\n        return xroot"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef shortcut_app_id(shortcut):\n  algorithm = Crc(width = 32, poly = 0x04C11DB7, reflect_in = True, xor_in = 0xffffffff, reflect_out = True, xor_out = 0xffffffff)\n  crc_input = ''.join([shortcut.exe,shortcut.name])\n  high_32 = algorithm.bit_by_bit(crc_input) | 0x80000000\n  full_64 = (high_32 << 32) | 0x02000000\n  return str(full_64)", "response": "Generates the app id for a given shortcut."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexecutes git remote add.", "response": "def _remote_add(self):\n        \"\"\"Execute git remote add.\"\"\"\n        self.repo.create_remote(\n            'origin',\n            'git@github.com:{username}/{repo}.git'.format(\n                username=self.metadata.username,\n                repo=self.metadata.name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstart execution of the script", "response": "def start(self):\n        '''\n        Starts execution of the script\n        '''\n        # invoke the appropriate sub-command as requested from command-line\n        try:\n            self.args.func()\n        except SystemExit as e:\n            if e.code != 0:\n                raise\n        except KeyboardInterrupt:\n            self.log.warning(\"exited via keyboard interrupt\")\n        except:\n            self.log.exception(\"exited start function\")\n            # set exit code so we know it did not end successfully\n            # TODO different exit codes based on signals ?\n        finally:\n            self._flush_metrics_q.put(None, block=True)\n            self._flush_metrics_q.put(None, block=True, timeout=1)\n\n        self.log.debug(\"exited_successfully\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndefining basic command - line arguments required by the script.", "response": "def define_baseargs(self, parser):\n        '''\n        Define basic command-line arguments required by the script.\n        @parser is a parser object created using the `argparse` module.\n        returns: None\n        '''\n        parser.add_argument('--name', default=sys.argv[0],\n            help='Name to identify this instance')\n        parser.add_argument('--log-level', default=None,\n            help='Logging level as picked from the logging module')\n        parser.add_argument('--log-format', default=None,\n            # TODO add more formats\n            choices=(\"json\", \"pretty\",),\n            help=(\"Force the format of the logs. By default, if the \"\n                  \"command is from a terminal, print colorful logs. \"\n                  \"Otherwise print json.\"),\n        )\n        parser.add_argument('--log-file', default=None,\n            help='Writes logs to log file if specified, default: %(default)s',\n        )\n        parser.add_argument('--quiet', default=False, action=\"store_true\",\n            help='if true, does not print logs to stderr, default: %(default)s',\n        )\n        parser.add_argument('--metric-grouping-interval', default=None, type=int,\n            help='To group metrics based on time interval ex:10 i.e;(10 sec)',\n        )\n        parser.add_argument('--debug', default=False, action=\"store_true\",\n            help='To run the code in debug mode',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_xpaths(xpath_map, *args, **kwargs):\n\n    formatted = {}.fromkeys(xpath_map)\n\n    for key, xpath in iteritems(xpath_map):\n        formatted[key] = xpath.format(*args, **kwargs)\n\n    return formatted", "response": "Formats XPATHs in a dictionary of dicts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_xpath_root(xpath):\n\n    if xpath:\n        if xpath.startswith('@'):\n            xpath = ''\n        else:\n            index = xpath.find('/@' if '@' in xpath else '/{')\n            xpath = xpath[:index] if index >= 0 else xpath\n\n    return xpath", "response": "returns the base of an XPATH"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the relative part of an XPATH that extends past the root provided", "response": "def get_xpath_branch(xroot, xpath):\n    \"\"\" :return: the relative part of an XPATH: that which extends past the root provided \"\"\"\n\n    if xroot and xpath and xpath.startswith(xroot):\n        xpath = xpath[len(xroot):]\n        xpath = xpath.lstrip(XPATH_DELIM)\n\n    return xpath"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a tuple with the base of an XPATH followed by any format key or attribute reference", "response": "def get_xpath_tuple(xpath):\n    \"\"\" :return: a tuple with the base of an XPATH followed by any format key or attribute reference \"\"\"\n\n    xroot = get_xpath_root(xpath)\n    xattr = None\n\n    if xroot != xpath:\n        xattr = get_xpath_branch(xroot, xpath).strip('@')\n\n    return (xroot, xattr)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the default value for a complex property.", "response": "def get_default_for(prop, value):\n    \"\"\" Ensures complex property types have the correct default values \"\"\"\n\n    prop = prop.strip('_')     # Handle alternate props (leading underscores)\n    val = reduce_value(value)  # Filtering of value happens here\n\n    if prop in _COMPLEX_LISTS:\n        return wrap_value(val)\n    elif prop in _COMPLEX_STRUCTS:\n        return val or {}\n    else:\n        return u'' if val is None else val"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_property(elem_to_parse, xpath):\n\n    xroot, attr = get_xpath_tuple(xpath)\n\n    if not xroot and not attr:\n        return False\n    elif not attr:\n        return bool(get_elements_text(elem_to_parse, xroot))\n    else:\n        return bool(get_elements_attributes(elem_to_parse, xroot, attr))", "response": "Check if the given xpath is present in the element along with any attribute referenced."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_complex(tree_to_parse, xpath_root, xpath_map, complex_key):\n\n    complex_struct = {}\n\n    for prop in _complex_definitions.get(complex_key, xpath_map):\n        # Normalize complex values: treat values with newlines like values from separate elements\n        parsed = parse_property(tree_to_parse, xpath_root, xpath_map, prop)\n        parsed = reduce_value(flatten_items(v.split(_COMPLEX_DELIM) for v in wrap_value(parsed)))\n\n        complex_struct[prop] = get_default_for_complex_sub(complex_key, prop, parsed, xpath_map[prop])\n\n    return complex_struct if any(complex_struct.values()) else {}", "response": "Parses the XML tree to create a dictionary of data structures that can be used to create a new XML structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate and returns a list of Dictionary data structures parsed from the metadata.", "response": "def parse_complex_list(tree_to_parse, xpath_root, xpath_map, complex_key):\n    \"\"\"\n    Creates and returns a list of Dictionary data structures parsed from the metadata.\n    :param tree_to_parse: the XML tree compatible with element_utils to be parsed\n    :param xpath_root: the XPATH location of each structure inside the parent element\n    :param xpath_map: a dict of XPATHs corresponding to a complex definition\n    :param complex_key: indicates which complex definition describes each structure\n    \"\"\"\n\n    complex_list = []\n\n    for element in get_elements(tree_to_parse, xpath_root):\n        complex_struct = parse_complex(element, xpath_root, xpath_map, complex_key)\n        if complex_struct:\n            complex_list.append(complex_struct)\n\n    return complex_list"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating and returns a Dates Dictionary data structure given the XML tree to parse", "response": "def parse_dates(tree_to_parse, xpath_map):\n    \"\"\"\n    Creates and returns a Dates Dictionary data structure given the parameters provided\n    :param tree_to_parse: the XML tree from which to construct the Dates data structure\n    :param xpath_map: a map containing the following type-specific XPATHs:\n        multiple, range, range_begin, range_end, and single\n    \"\"\"\n\n    # Determine dates to query based on metadata elements\n\n    values = wrap_value(parse_property(tree_to_parse, None, xpath_map, DATE_TYPE_SINGLE))\n    if len(values) == 1:\n        return {DATE_TYPE: DATE_TYPE_SINGLE, DATE_VALUES: values}\n    elif len(values) > 1:\n        return {DATE_TYPE: DATE_TYPE_MULTIPLE, DATE_VALUES: values}\n\n    values = wrap_value(parse_property(tree_to_parse, None, xpath_map, DATE_TYPE_MULTIPLE))\n    if len(values) == 1:\n        return {DATE_TYPE: DATE_TYPE_SINGLE, DATE_VALUES: values}\n    elif len(values) > 1:\n        return {DATE_TYPE: DATE_TYPE_MULTIPLE, DATE_VALUES: values}\n\n    values = flatten_items(\n        d for x in (DATE_TYPE_RANGE_BEGIN, DATE_TYPE_RANGE_END)\n        for d in wrap_value(parse_property(tree_to_parse, None, xpath_map, x))\n    )\n    if len(values) == 1:\n        return {DATE_TYPE: DATE_TYPE_SINGLE, DATE_VALUES: values}\n    elif len(values) == 2:\n        return {DATE_TYPE: DATE_TYPE_RANGE, DATE_VALUES: values}\n    elif len(values) > 2:\n        return {DATE_TYPE: DATE_TYPE_MULTIPLE, DATE_VALUES: values}\n\n    return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef parse_property(tree_to_parse, xpath_root, xpath_map, prop):\n\n    xpath = xpath_map[prop]\n\n    if isinstance(xpath, ParserProperty):\n        if xpath.xpath is None:\n            return xpath.get_prop(prop)\n\n        xpath = xpath.xpath\n\n    if xpath_root:\n        xpath = get_xpath_branch(xpath_root, xpath)\n\n    parsed = None\n\n    if not has_property(tree_to_parse, xpath):\n        # Element has no text: try next alternate location\n\n        alternate = '_' + prop\n        if alternate in xpath_map:\n            return parse_property(tree_to_parse, xpath_root, xpath_map, alternate)\n\n    elif '@' not in xpath:\n        parsed = get_elements_text(tree_to_parse, xpath)\n    else:\n        xroot, xattr = get_xpath_tuple(xpath)\n        parsed = get_elements_attributes(tree_to_parse, xroot, xattr)\n\n    return get_default_for(prop, parsed)", "response": "Parses the property from the XML tree and returns the default value for the property."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the tree with the given property and values.", "response": "def update_property(tree_to_update, xpath_root, xpaths, prop, values, supported=None):\n    \"\"\"\n    Either update the tree the default way, or call the custom updater\n\n    Default Way: Existing values in the tree are overwritten. If xpaths contains a single path,\n    then each value is written to the tree at that path. If xpaths contains a list of xpaths,\n    then the values corresponding to each xpath index are written to their respective locations.\n    In either case, empty values are ignored.\n\n    :param tree_to_update: the XML tree compatible with element_utils to be updated\n    :param xpath_root: the XPATH location shared by all the xpaths passed in\n    :param xpaths: a string or a list of strings representing the XPATH location(s) to which to write values\n    :param prop: the name of the property of the parser containing the value(s) with which to update the tree\n    :param values: a single value, or a list of values to write to the specified XPATHs\n\n    :see: ParserProperty for more on custom updaters\n\n    :return: a list of all elements updated by this operation\n    \"\"\"\n\n    if supported and prop.startswith('_') and prop.strip('_') in supported:\n        values = u''  # Remove alternate elements: write values only to primary location\n    else:\n        values = get_default_for(prop, values)  # Enforce defaults as required per property\n\n    if not xpaths:\n        return []\n    elif not isinstance(xpaths, ParserProperty):\n        return _update_property(tree_to_update, xpath_root, xpaths, values)\n    else:\n        # Call ParserProperty.set_prop without xpath_root (managed internally)\n        return xpaths.set_prop(tree_to_update=tree_to_update, prop=prop, values=values)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_property(tree_to_update, xpath_root, xpaths, values):\n\n    # Inner function to update a specific XPATH with the values provided\n\n    def update_element(elem, idx, root, path, vals):\n        \"\"\" Internal helper function to encapsulate single item update \"\"\"\n\n        has_root = bool(root and len(path) > len(root) and path.startswith(root))\n        path, attr = get_xpath_tuple(path)  # 'path/@attr' to ('path', 'attr')\n\n        if attr:\n            removed = [get_element(elem, path)]\n            remove_element_attributes(removed[0], attr)\n        elif not has_root:\n            removed = wrap_value(remove_element(elem, path))\n        else:\n            path = get_xpath_branch(root, path)\n            removed = [] if idx != 0 else [remove_element(e, path, True) for e in get_elements(elem, root)]\n\n        if not vals:\n            return removed\n\n        items = []\n\n        for i, val in enumerate(wrap_value(vals)):\n            elem_to_update = elem\n\n            if has_root:\n                elem_to_update = insert_element(elem, (i + idx), root)\n\n            val = val.decode('utf-8') if not isinstance(val, string_types) else val\n            if not attr:\n                items.append(insert_element(elem_to_update, i, path, val))\n            elif path:\n                items.append(insert_element(elem_to_update, i, path, **{attr: val}))\n            else:\n                set_element_attributes(elem_to_update, **{attr: val})\n                items.append(elem_to_update)\n\n        return items\n\n    # Code to update each of the XPATHs with each of the values\n\n    xpaths = reduce_value(xpaths)\n    values = filter_empty(values)\n\n    if isinstance(xpaths, string_types):\n        return update_element(tree_to_update, 0, xpath_root, xpaths, values)\n    else:\n        each = []\n\n        for index, xpath in enumerate(xpaths):\n            value = values[index] if values else None\n            each.extend(update_element(tree_to_update, index, xpath_root, xpath, value))\n\n        return each", "response": "Internal helper function to update a single parser property."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the complex element in the XML tree that corresponds to the given property and returns the updated complex element.", "response": "def update_complex(tree_to_update, xpath_root, xpath_map, prop, values):\n    \"\"\"\n    Updates and returns the updated complex Element parsed from tree_to_update.\n    :param tree_to_update: the XML tree compatible with element_utils to be updated\n    :param xpath_root: the XPATH location of the root of the complex Element\n    :param xpath_map: a Dictionary of XPATHs corresponding to the complex structure definition\n    :param prop: the property identifying the complex structure to be serialized\n    :param values: a Dictionary representing the complex structure to be updated\n    \"\"\"\n\n    remove_element(tree_to_update, xpath_root, True)\n\n    values = reduce_value(values, {})\n\n    if not values:\n        # Returns the elements corresponding to property removed from the tree\n        updated = update_property(tree_to_update, xpath_root, xpath_root, prop, values)\n    else:\n        for subprop, value in iteritems(values):\n            xpath = xpath_map[subprop]\n            value = get_default_for_complex_sub(prop, subprop, value, xpath)\n            update_property(tree_to_update, None, xpath, subprop, value)\n        updated = get_element(tree_to_update, xpath_root)\n\n    return updated"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_complex_list(tree_to_update, xpath_root, xpath_map, prop, values):\n\n    complex_list = []\n\n    remove_element(tree_to_update, xpath_root, True)\n\n    if not values:\n        # Returns the elements corresponding to property removed from the tree\n        complex_list.append(update_property(tree_to_update, xpath_root, xpath_root, prop, values))\n    else:\n        for idx, complex_struct in enumerate(wrap_value(values)):\n\n            # Insert a new complex element root for each dict in the list\n            complex_element = insert_element(tree_to_update, idx, xpath_root)\n\n            for subprop, value in iteritems(complex_struct):\n                xpath = get_xpath_branch(xpath_root, xpath_map[subprop])\n                value = get_default_for_complex_sub(prop, subprop, value, xpath)\n                complex_list.append(update_property(complex_element, None, xpath, subprop, value))\n\n    return complex_list", "response": "Updates and returns the list of updated complex elements parsed from tree_to_update."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate any metadata property value.", "response": "def validate_any(prop, value, xpath_map=None):\n    \"\"\" Validates any metadata property, complex or simple (string or array) \"\"\"\n\n    if value is not None:\n        if prop in (ATTRIBUTES, CONTACTS, DIGITAL_FORMS):\n            validate_complex_list(prop, value, xpath_map)\n\n        elif prop in (BOUNDING_BOX, LARGER_WORKS, RASTER_INFO):\n            validate_complex(prop, value, xpath_map)\n\n        elif prop == DATES:\n            validate_dates(prop, value, xpath_map)\n\n        elif prop == PROCESS_STEPS:\n            validate_process_steps(prop, value)\n\n        elif prop not in _supported_props and xpath_map is not None:\n            # Validate custom data structures as complex lists by default\n            validate_complex_list(prop, value, xpath_map)\n\n        else:\n            for val in wrap_value(value, include_empty=True):\n                validate_type(prop, val, (string_types, list))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndefault validation for single complex data structure", "response": "def validate_complex(prop, value, xpath_map=None):\n    \"\"\" Default validation for single complex data structure \"\"\"\n\n    if value is not None:\n        validate_type(prop, value, dict)\n\n        if prop in _complex_definitions:\n            complex_keys = _complex_definitions[prop]\n        else:\n            complex_keys = {} if xpath_map is None else xpath_map\n\n        for complex_prop, complex_val in iteritems(value):\n            complex_key = '.'.join((prop, complex_prop))\n\n            if complex_prop not in complex_keys:\n                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))\n\n            validate_type(complex_key, complex_val, (string_types, list))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefault validation for Attribute Details data structure", "response": "def validate_complex_list(prop, value, xpath_map=None):\n    \"\"\" Default validation for Attribute Details data structure \"\"\"\n\n    if value is not None:\n        validate_type(prop, value, (dict, list))\n\n        if prop in _complex_definitions:\n            complex_keys = _complex_definitions[prop]\n        else:\n            complex_keys = {} if xpath_map is None else xpath_map\n\n        for idx, complex_struct in enumerate(wrap_value(value)):\n            cs_idx = prop + '[' + str(idx) + ']'\n            validate_type(cs_idx, complex_struct, dict)\n\n            for cs_prop, cs_val in iteritems(complex_struct):\n                cs_key = '.'.join((cs_idx, cs_prop))\n\n                if cs_prop not in complex_keys:\n                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))\n\n                if not isinstance(cs_val, list):\n                    validate_type(cs_key, cs_val, (string_types, list))\n                else:\n                    for list_idx, list_val in enumerate(cs_val):\n                        list_prop = cs_key + '[' + str(list_idx) + ']'\n                        validate_type(list_prop, list_val, string_types)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_dates(prop, value, xpath_map=None):\n\n    if value is not None:\n        validate_type(prop, value, dict)\n\n        date_keys = set(value)\n\n        if date_keys:\n            if DATE_TYPE not in date_keys or DATE_VALUES not in date_keys:\n                if prop in _complex_definitions:\n                    complex_keys = _complex_definitions[prop]\n                else:\n                    complex_keys = _complex_definitions[DATES] if xpath_map is None else xpath_map\n\n                _validation_error(prop, None, value, ('keys: {0}'.format(','.join(complex_keys))))\n\n            date_type = value[DATE_TYPE]\n\n            if date_type not in DATE_TYPES:\n                _validation_error('dates.type', None, date_type, DATE_TYPES)\n\n            date_vals = value[DATE_VALUES]\n\n            validate_type('dates.values', date_vals, list)\n\n            dates_len = len(date_vals)\n\n            if date_type == DATE_TYPE_MISSING and dates_len != 0:\n                _validation_error('len(dates.values)', None, dates_len, 0)\n\n            if date_type == DATE_TYPE_SINGLE and dates_len != 1:\n                _validation_error('len(dates.values)', None, dates_len, 1)\n\n            if date_type == DATE_TYPE_RANGE and dates_len != 2:\n                _validation_error('len(dates.values)', None, dates_len, 2)\n\n            if date_type == DATE_TYPE_MULTIPLE and dates_len < 2:\n                _validation_error('len(dates.values)', None, dates_len, 'at least two')\n\n            for idx, date in enumerate(date_vals):\n                date_key = 'dates.value[' + str(idx) + ']'\n                validate_type(date_key, date, string_types)", "response": "Default validation for Date Types data structure"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndefaults validation for Process Steps data structure", "response": "def validate_process_steps(prop, value):\n    \"\"\" Default validation for Process Steps data structure \"\"\"\n\n    if value is not None:\n        validate_type(prop, value, (dict, list))\n\n        procstep_keys = set(_complex_definitions[prop])\n\n        for idx, procstep in enumerate(wrap_value(value)):\n            ps_idx = prop + '[' + str(idx) + ']'\n            validate_type(ps_idx, procstep, dict)\n\n            for ps_prop, ps_val in iteritems(procstep):\n                ps_key = '.'.join((ps_idx, ps_prop))\n\n                if ps_prop not in procstep_keys:\n                    _validation_error(prop, None, value, ('keys: {0}'.format(','.join(procstep_keys))))\n\n                if ps_prop != 'sources':\n                    validate_type(ps_key, ps_val, string_types)\n                else:\n                    validate_type(ps_key, ps_val, (string_types, list))\n\n                    for src_idx, src_val in enumerate(wrap_value(ps_val)):\n                        src_key = ps_key + '[' + str(src_idx) + ']'\n                        validate_type(src_key, src_val, string_types)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that the properties in the key set are valid.", "response": "def validate_properties(props, required):\n    \"\"\"\n    Ensures the key set contains the base supported properties for a Parser\n    :param props: a set of property names to validate against those supported\n    \"\"\"\n\n    props = set(props)\n    required = set(required or _supported_props)\n\n    if len(required.intersection(props)) < len(required):\n        missing = required - props\n        raise ValidationError(\n            'Missing property names: {props}', props=','.join(missing), missing=missing\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndefault validation for all types", "response": "def validate_type(prop, value, expected):\n    \"\"\" Default validation for all types \"\"\"\n\n    # Validate on expected type(s), but ignore None: defaults handled elsewhere\n    if value is not None and not isinstance(value, expected):\n        _validation_error(prop, type(value).__name__, None, expected)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefault validation for updated properties", "response": "def _validation_error(prop, prop_type, prop_value, expected):\n    \"\"\" Default validation for updated properties \"\"\"\n\n    if prop_type is None:\n        attrib = 'value'\n        assigned = prop_value\n    else:\n        attrib = 'type'\n        assigned = prop_type\n\n    raise ValidationError(\n        'Invalid property {attrib} for {prop}:\\n\\t{attrib}: {assigned}\\n\\texpected: {expected}',\n        attrib=attrib, prop=prop, assigned=assigned, expected=expected,\n        invalid={prop: prop_value} if attrib == 'value' else {}\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall the getter with no arguments and returns its value", "response": "def get_prop(self, prop):\n        \"\"\" Calls the getter with no arguments and returns its value \"\"\"\n\n        if self._parser is None:\n            raise ConfigurationError('Cannot call ParserProperty.\"get_prop\" with no parser configured')\n\n        return self._parser(prop) if prop else self._parser()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the property of the tree_to_update with the specified keyword arguments for flexibility.", "response": "def set_prop(self, **setter_args):\n        \"\"\"\n        Calls the setter with the specified keyword arguments for flexibility.\n        :param setter_args: must contain tree_to_update, prop, values\n        :return: None, or the value updated for complex values\n        \"\"\"\n\n        if self.xpath:\n            setter_args['xpaths'] = self.xpath\n\n        return self._updater(**setter_args)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grouped_command(commands):\n    base = commands[0]\n    name = base.get_name()\n    multi_command = EventualCommand('%s_multi' % name)\n    if name in ('get', 'delete'):\n        args = [c.get_args()[0] for c in commands]\n    elif base.get_name() == 'set':\n        args = dict(c.get_args()[0:2] for c in commands)\n    else:\n        raise ValueError('Command not supported: %r' % (base.get_name(),))\n\n    multi_command(args, *grouped_args_for_command(base), **base.get_kwargs())\n\n    return multi_command", "response": "Given a list of commands returns a new command which is a batch ( multi ) command."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a boolean representing whether the given command can be grouped together or not.", "response": "def can_group_commands(command, next_command):\n    \"\"\"\n    Returns a boolean representing whether these commands can be\n    grouped together or not.\n\n    A few things are taken into account for this decision:\n\n    For ``set`` commands:\n\n    - Are all arguments other than the key/value the same?\n\n    For ``delete`` and ``get`` commands:\n\n    - Are all arguments other than the key the same?\n    \"\"\"\n    multi_capable_commands = ('get', 'set', 'delete')\n\n    if next_command is None:\n        return False\n\n    name = command.get_name()\n\n    # TODO: support multi commands\n    if name not in multi_capable_commands:\n        return False\n\n    if name != next_command.get_name():\n        return False\n\n    # if the shared args (key, or key/value) do not match, we cannot group\n    if grouped_args_for_command(command) != grouped_args_for_command(next_command):\n        return False\n\n    # If the keyword arguments do not much (e.g. key_prefix, or timeout on set)\n    # then we cannot group\n    if command.get_kwargs() != next_command.get_kwargs():\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngroup the commands in the list of commands into a list of tuples grouped by the command that was not grouped.", "response": "def regroup_commands(commands):\n    \"\"\"\n    Returns a list of tuples:\n\n        [(command_to_run, [list, of, commands])]\n\n    If the list of commands has a single item, the command was not grouped.\n    \"\"\"\n    grouped = []\n    pending = []\n\n    def group_pending():\n        if not pending:\n            return\n\n        new_command = grouped_command(pending)\n        result = []\n        while pending:\n            result.append(pending.pop(0))\n        grouped.append((new_command, result))\n\n    for command, next_command in peek(commands):\n        # if the previous command was a get, and this is a set we must execute\n        # any pending commands\n        # TODO: unless this command is a get_multi and it matches the same option\n        # signature\n        if can_group_commands(command, next_command):\n            # if previous command does not match this command\n            if pending and not can_group_commands(pending[0], command):\n                group_pending()\n\n            pending.append(command)\n        else:\n            # if pending exists for this command, group it\n            if pending and can_group_commands(pending[0], command):\n                pending.append(command)\n            else:\n                grouped.append((command.clone(), [command]))\n\n            # We couldn't group with previous command, so ensure we bubble up\n            group_pending()\n\n    group_pending()\n\n    return grouped"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of ribosomal proteins and location of curated databases.", "response": "def find_databases(databases):\n    \"\"\"\n    define ribosomal proteins and location of curated databases\n    \"\"\"\n    # 16 ribosomal proteins in their expected order\n    proteins = ['L15', 'L18', 'L6', 'S8', 'L5', 'L24', 'L14',\n            'S17', 'L16', 'S3', 'L22', 'S19', 'L2', 'L4', 'L3', 'S10']\n    # curated databases\n    protein_databases = {\n                'L14': 'rpL14_JGI_MDM.filtered.faa',\n                'L15': 'rpL15_JGI_MDM.filtered.faa',\n                'L16': 'rpL16_JGI_MDM.filtered.faa',\n                'L18': 'rpL18_JGI_MDM.filtered.faa',\n                'L22': 'rpL22_JGI_MDM.filtered.faa',\n                'L24': 'rpL24_JGI_MDM.filtered.faa',\n                'L2': 'rpL2_JGI_MDM.filtered.faa',\n                'L3': 'rpL3_JGI_MDM.filtered.faa',\n                'L4': 'rpL4_JGI_MDM.filtered.faa',\n                'L5': 'rpL5_JGI_MDM.filtered.faa',\n                'L6': 'rpL6_JGI_MDM.filtered.faa',\n                'S10': 'rpS10_JGI_MDM.filtered.faa',\n                'S17': 'rpS17_JGI_MDM.filtered.faa',\n                'S19': 'rpS19_JGI_MDM.filtered.faa',\n                'S3': 'rpS3_JGI_MDM.filtered.faa',\n                'S8': 'rpS8_JGI_MDM.filtered.faa'}\n    protein_databases = {key: '%s/%s' % (databases, database) \\\n            for key, database in list(protein_databases.items())}\n    return proteins, protein_databases"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef scaffold_hits(searches, fasta, max_hits):\n    # initialize\n    ## scaffolds[scaffold] = # ORFs\n    scaffolds = {}\n    for seq in parse_fasta(fasta):\n        scaffold = seq[0].split()[0].split('>', 1)[1].rsplit('_', 1)[0]\n        if scaffold not in scaffolds:\n            scaffolds[scaffold] = 0\n        scaffolds[scaffold] += 1\n    s2rp = {s: {r[0]: []\n        for r in searches}\n        for s in scaffolds}\n    # get hits from blast\n    for search in searches:\n        rp, blast = search\n        hits = [i for i in numblast(open(blast), max_hits, evalue_thresh, bit_thresh)]\n        for hit in hits:\n            s = hit[0].split()[0].rsplit('_', 1)[0]\n            hit[10], hit[11] = float(hit[10]), float(hit[11])\n            s2rp[s][rp].append(hit)\n    return scaffolds, s2rp", "response": "get hits from each search against each RP\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_next(start, stop, i2hits):\n    if start not in i2hits and stop in i2hits:\n        index = stop\n    elif stop not in i2hits and start in i2hits:\n        index = start\n    elif start not in i2hits and stop not in i2hits:\n        index = choice([start, stop])\n        i2hits[index] = [[False]]\n    else:\n        A, B = i2hits[start][0], i2hits[stop][0]\n        if B[10] <= A[10]:\n            index = stop\n        else:\n            index = start\n    if index == start:\n        nstart = start - 1\n        nstop = stop\n    else:\n        nstop = stop + 1\n        nstart = start\n    match = i2hits[index][0]\n    rp = match[-1]\n    return index, nstart, nstop, rp, match", "response": "find next entry in the sequence"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_ribosomal(rps, scaffolds, s2rp, min_hits, max_hits_rp, max_errors):\n    for scaffold, proteins in list(s2rp.items()):\n        # for each scaffold, get best hits for each rp\n        hits = {p: [i for i in sorted(hits, key = itemgetter(10))][0:max_hits_rp]\n            for p, hits in list(proteins.items()) if len(hits) > 0}\n        # skip if fewer than min_hits RPs are identified\n        if len(hits) < min_hits:\n            continue\n        best = sorted([hit[0] + [p]\n            for p, hit in list(hits.items())], key = itemgetter(10))[0]\n        block = find_block(rps, scaffolds[scaffold], hits, best, max_errors)\n        if (len(block) - 1) >= min_hits:\n            yield scaffold, block", "response": "finds ribosomal proteins in a given scaffolds and returns a generator of the best set of hits for each scaffold."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding ribosomal proteins in a scaffold", "response": "def ribosomal(scaffolds, DBdir, min_hits, evalue_thresh, bit_thresh, \\\n                method = 'usearch', threads = 6, \\\n                max_hits = 1, max_hits_rp = 1, max_errors = 35):\n    \"\"\"\n    find ribosomal proteins\n    max_hits = maximum number of blast hits to consider for an orf\n                  if 1, only consider best blast hit for each ORF\n    max_hits_rp = maximum number of hits to consider per ribosomal protein per scaffold\n                     if 1, only consider best RP match to contig\n    max_errors = maximum number of errors when looking for block of proteins (e.g. out of order or gap)\n    \"\"\"\n    # rps = list (in syntenic order) of ribosomal proteins\n    # rp_db = dictionary to find the database files\n    rps, rp_db = find_databases(DBdir)\n    searches = [[rp, search(scaffolds, rp_db[rp], method = method, threads = str(threads), max_hits = 10)]\n            for rp in rp_db]\n    scaffolds, scaffold2rp = scaffold_hits(searches, scaffolds, max_hits)\n    print('# scaffold\\t%s' % ('\\t'.join(rps)))\n    for scaffold, block in \\\n            find_ribosomal(rps, scaffolds, scaffold2rp, min_hits, max_hits_rp, max_errors):\n        id_rps = []\n        for rp in rps:\n            if rp in block:\n                id_rps.append(block[rp][0].split()[0])\n            else:\n                id_rps.append('-')\n        print('%s\\t%s' % (scaffold, '\\t'.join(id_rps)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filter_rep_set(inF, otuSet):\n    seqs = []\n    for record in SeqIO.parse(inF, \"fasta\"):\n        if record.id in otuSet:\n            seqs.append(record)\n    return seqs", "response": "Parse the rep set file and remove all sequences not associated with unique OTUs."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing the data map for ArcGIS metadata.", "response": "def _init_data_map(self):\n        \"\"\" OVERRIDDEN: Initialize required FGDC data map with XPATHS and specialized functions \"\"\"\n\n        if self._data_map is not None:\n            return  # Initiation happens once\n\n        # Parse and validate the ArcGIS metadata root\n\n        if self._xml_tree is None:\n            agis_root = ARCGIS_ROOTS[0]  # Default to uncapitalized\n        else:\n            agis_root = get_element_name(self._xml_tree)\n\n        if agis_root not in ARCGIS_ROOTS:\n            raise InvalidContent('Invalid XML root for ArcGIS metadata: {root}', root=agis_root)\n\n        agis_data_map = {'_root': agis_root}\n        agis_data_map.update(_agis_tag_formats)\n\n        agis_data_structures = {}\n\n        # Capture and format complex XPATHs\n\n        ad_format = agis_data_map[ATTRIBUTES]\n        agis_data_structures[ATTRIBUTES] = format_xpaths(\n            _agis_definitions[ATTRIBUTES],\n            label=ad_format.format(ad_path='attrlabl'),\n            aliases=ad_format.format(ad_path='attalias'),\n            definition=ad_format.format(ad_path='attrdef'),\n            definition_src=ad_format.format(ad_path='attrdefs')\n        )\n\n        bb_format = agis_data_map[BOUNDING_BOX]\n        agis_data_structures[BOUNDING_BOX] = format_xpaths(\n            _agis_definitions[BOUNDING_BOX],\n            east=bb_format.format(bbox_path='eastBL'),\n            south=bb_format.format(bbox_path='southBL'),\n            west=bb_format.format(bbox_path='westBL'),\n            north=bb_format.format(bbox_path='northBL')\n        )\n\n        ct_format = agis_data_map[CONTACTS]\n        agis_data_structures[CONTACTS] = format_xpaths(\n            _agis_definitions[CONTACTS],\n            name=ct_format.format(ct_path='rpIndName'),\n            organization=ct_format.format(ct_path='rpOrgName'),\n            position=ct_format.format(ct_path='rpPosName'),\n            email=ct_format.format(ct_path='rpCntInfo/cntAddress/eMailAdd')\n        )\n\n        dt_format = agis_data_map[DATES]\n        agis_data_structures[DATES] = {\n            DATE_TYPE_MULTIPLE: dt_format.format(type_path='TM_Instant/tmPosition'),\n            '_' + DATE_TYPE_MULTIPLE: dt_format.format(type_path='TM_Instant/tmPosition/@date'),\n            DATE_TYPE_RANGE_BEGIN: dt_format.format(type_path='TM_Period/tmBegin'),\n            '_' + DATE_TYPE_RANGE_BEGIN: dt_format.format(type_path='TM_Period/tmBegin/@date'),\n            DATE_TYPE_RANGE_END: dt_format.format(type_path='TM_Period/tmEnd'),\n            '_' + DATE_TYPE_RANGE_END: dt_format.format(type_path='TM_Period/tmEnd/@date'),\n\n            # Same as multiple dates, but will contain only one\n            DATE_TYPE_SINGLE: dt_format.format(type_path='TM_Instant/tmPosition'),\n            '_' + DATE_TYPE_SINGLE: dt_format.format(type_path='TM_Instant/tmPosition/@date')\n        }\n        agis_data_structures[DATES][DATE_TYPE_RANGE] = [\n            agis_data_structures[DATES][DATE_TYPE_RANGE_BEGIN],\n            agis_data_structures[DATES][DATE_TYPE_RANGE_END]\n        ]\n        agis_data_structures[DATES]['_' + DATE_TYPE_RANGE] = [\n            agis_data_structures[DATES]['_' + DATE_TYPE_RANGE_BEGIN],\n            agis_data_structures[DATES]['_' + DATE_TYPE_RANGE_END]\n        ]\n\n        df_format = agis_data_map[DIGITAL_FORMS]\n        agis_data_structures[DIGITAL_FORMS] = format_xpaths(\n            _agis_definitions[DIGITAL_FORMS],\n            name=df_format.format(df_path='formatName'),\n            content=df_format.format(df_path='formatInfo'),\n            decompression=df_format.format(df_path='fileDecmTech'),\n            version=df_format.format(df_path='formatVer'),\n            specification=df_format.format(df_path='formatSpec'),\n            access_desc=agis_data_map['_access_desc'],\n            access_instrs=agis_data_map['_access_instrs'],\n            network_resource=agis_data_map['_network_resource']\n        )\n\n        lw_format = agis_data_map[LARGER_WORKS]\n        agis_data_structures[LARGER_WORKS] = format_xpaths(\n            _agis_definitions[LARGER_WORKS],\n            title=lw_format.format(lw_path='resTitle'),\n            edition=lw_format.format(lw_path='resEd'),\n            origin=lw_format.format(lw_path='citRespParty/rpIndName'),\n            online_linkage=lw_format.format(lw_path='citRespParty/rpCntInfo/cntOnlineRes/linkage'),\n            other_citation=lw_format.format(lw_path='otherCitDet'),\n            date=lw_format.format(lw_path='date/pubDate'),\n            place=lw_format.format(lw_path='citRespParty/rpCntInfo/cntAddress/city'),\n            info=lw_format.format(lw_path='citRespParty/rpOrgName')\n        )\n\n        ps_format = agis_data_map[PROCESS_STEPS]\n        agis_data_structures[PROCESS_STEPS] = format_xpaths(\n            _agis_definitions[PROCESS_STEPS],\n            description=ps_format.format(ps_path='stepDesc'),\n            date=ps_format.format(ps_path='stepDateTm'),\n            sources=ps_format.format(ps_path='stepSrc/srcDesc')\n        )\n\n        ri_format = agis_data_map[RASTER_INFO]\n        agis_data_structures[RASTER_INFO] = format_xpaths(\n            _agis_definitions[RASTER_DIMS],\n            type=ri_format.format(ri_path='@type'),\n            size=ri_format.format(ri_path='dimSize'),\n            value=ri_format.format(ri_path='dimResol/value'),\n            units=ri_format.format(ri_path='dimResol/value/@uom')\n        )\n\n        # Assign XPATHS and gis_metadata.utils.ParserProperties to data map\n\n        for prop, xpath in iteritems(dict(agis_data_map)):\n            if prop in (ATTRIBUTES, CONTACTS, PROCESS_STEPS):\n                agis_data_map[prop] = ParserProperty(self._parse_complex_list, self._update_complex_list)\n\n            elif prop in (BOUNDING_BOX, LARGER_WORKS):\n                agis_data_map[prop] = ParserProperty(self._parse_complex, self._update_complex)\n\n            elif prop in ('attribute_accuracy', 'dataset_completeness'):\n                agis_data_map[prop] = ParserProperty(self._parse_report_item, self._update_report_item)\n\n            elif prop == DATES:\n                agis_data_map[prop] = ParserProperty(self._parse_dates, self._update_dates)\n\n            elif prop == DIGITAL_FORMS:\n                agis_data_map[prop] = ParserProperty(self._parse_digital_forms, self._update_digital_forms)\n\n            elif prop == RASTER_INFO:\n                agis_data_map[prop] = ParserProperty(self._parse_raster_info, self._update_raster_info)\n\n            else:\n                agis_data_map[prop] = xpath\n\n        self._data_map = agis_data_map\n        self._data_structures = agis_data_structures"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the digital forms and transfer options and return a single complex struct.", "response": "def _parse_digital_forms(self, prop=DIGITAL_FORMS):\n        \"\"\" Concatenates a list of Digital Form data structures parsed from the metadata \"\"\"\n\n        xpath_map = self._data_structures[prop]\n\n        # Parse base digital form fields: 'name', 'content', 'decompression', 'version', 'specification'\n        xpath_root = self._data_map['_digital_forms_root']\n        digital_forms = parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)\n\n        # Parse digital form transfer option fields: 'access_desc', 'access_instrs', 'network_resource'\n        xpath_root = self._data_map['_transfer_options_root']\n        transfer_opts = parse_complex_list(self._xml_tree, xpath_root, xpath_map, prop)\n\n        # Combine digital forms and transfer options into a single complex struct\n\n        df_len = len(digital_forms)\n        to_len = len(transfer_opts)\n        parsed_forms = []\n\n        for idx in xrange(0, max(df_len, to_len)):\n            digital_form = {}.fromkeys(_agis_definitions[prop], u'')\n\n            if idx < df_len:\n                digital_form.update(i for i in digital_forms[idx].items() if i[1])\n            if idx < to_len:\n                digital_form.update(i for i in transfer_opts[idx].items() if i[1])\n\n            if any(digital_form.values()):\n                parsed_forms.append(digital_form)\n\n        return get_default_for_complex(prop, parsed_forms)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _parse_report_item(self, prop):\n\n        item_type = None\n\n        if prop == 'attribute_accuracy':\n            item_type = 'DQQuanAttAcc'\n        elif prop == 'dataset_completeness':\n            item_type = 'DQCompOm'\n\n        xroot = self._get_xroot_for(prop)\n\n        parsed = (element_to_dict(e) for e in get_elements(self._xml_tree, xroot))\n        parsed = flatten_items(e['children'] for e in parsed if e['attributes'].get('type') == item_type)\n\n        return reduce_value([p['text'] for p in parsed if p['name'] == 'measDesc'])", "response": "Parses the report item at the configured path if type attribute matches"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _update_dates(self, **update_props):\n\n        tree_to_update = update_props['tree_to_update']\n        xpath_root = self._data_map['_dates_root']\n\n        if self.dates:\n            date_type = self.dates[DATE_TYPE]\n\n            # First remove all date info from common root\n            remove_element(tree_to_update, xpath_root)\n\n            if date_type == DATE_TYPE_MULTIPLE:\n                xpath_root += '/TempExtent/TM_Instant'\n            elif date_type == DATE_TYPE_RANGE:\n                xpath_root += '/TempExtent/TM_Period'\n\n        return super(ArcGISParser, self)._update_dates(xpath_root, **update_props)", "response": "Update ArcGIS Dates metadata."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the text for each element at the configured path if the attribute matches the value of the attribute.", "response": "def _update_report_item(self, **update_props):\n        \"\"\" Update the text for each element at the configured path if attribute matches \"\"\"\n\n        tree_to_update = update_props['tree_to_update']\n        prop = update_props['prop']\n        values = wrap_value(update_props['values'])\n        xroot = self._get_xroot_for(prop)\n\n        attr_key = 'type'\n        attr_val = u''\n\n        if prop == 'attribute_accuracy':\n            attr_val = 'DQQuanAttAcc'\n        elif prop == 'dataset_completeness':\n            attr_val = 'DQCompOm'\n\n        # Clear (make empty) all elements of the appropriate type\n        for elem in get_elements(tree_to_update, xroot):\n            if get_element_attributes(elem).get(attr_key) == attr_val:\n                clear_element(elem)\n\n        # Remove all empty elements, including those previously cleared\n        remove_empty_element(tree_to_update, xroot)\n\n        # Insert elements with correct attributes for each new value\n\n        attrs = {attr_key: attr_val}\n        updated = []\n\n        for idx, value in enumerate(values):\n            elem = insert_element(tree_to_update, idx, xroot, **attrs)\n            updated.append(insert_element(elem, idx, 'measDesc', value))\n\n        return updated"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting for a response to the VCNL40xx command.", "response": "def _wait_response(self, ready, timeout_sec):\n        \"\"\"Wait for a response to be ready (the provided ready bits are set).\n        If the specified timeout (in seconds) is exceeded and error will be\n        thrown.\n        \"\"\"\n        # Wait for the measurement to be ready (or a timeout elapses).\n        start = time.time()\n        while True:\n            # Check if the timeout has elapsed.\n            if (time.time() - start) >= timeout_sec:\n                raise RuntimeError('Exceeded timeout waiting for VCNL40xx response, check your wiring.')\n            # Check if result is ready and return it.\n            result = self._device.readU8(VCNL40xx_COMMAND)\n            if (result & ready) > 0:\n                return\n            # Otherwise delay for a bit and try reading again.\n            time.sleep(0.001)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_proximity(self, timeout_sec=1):\n        # Ask for a proximity measurement and wait for the response.\n        self._device.write8(VCNL40xx_COMMAND, VCNL40xx_MEASUREPROXIMITY)\n        self._wait_response(VCNL40xx_PROXIMITYREADY, timeout_sec)\n        # Return the proximity response.\n        return self._device.readU16BE(VCNL40xx_PROXIMITYDATA)", "response": "Read the sensor proximity and return it as an unsigned 16 - bit value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_ambient(self, timeout_sec=1):\n        # Ask for an ambient measurement and wait for the response.\n        self._device.write8(VCNL40xx_COMMAND, VCNL40xx_MEASUREAMBIENT)\n        self._wait_response(VCNL40xx_AMBIENTREADY, timeout_sec)\n        # Return the ambient response.\n        return self._device.readU16BE(VCNL40xx_AMBIENTDATA)", "response": "Read the ambient light sensor and return it as an unsigned 16 - bit value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _clear_interrupt(self, intbit):\n        int_status = self._device.readU8(VCNL4010_INTSTAT);\n        int_status &= ~intbit;\n        self._device.write8(VCNL4010_INTSTAT, int_status);", "response": "Clear the specified interrupt bit in the interrupt status register."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the sensor proximity and return it as an unsigned 16 - bit value.", "response": "def read_proximity(self, timeout_sec=1):\n        \"\"\"Read the sensor proximity and return it as an unsigned 16-bit value.\n        The larger the value the closer an object is to the sensor.\n        \"\"\"\n        # Clear any interrupts.\n        self._clear_interrupt(VCNL4010_INT_PROX_READY)\n        # Call base class read_proximity and return result.\n        return super(VCNL4010, self).read_proximity(timeout_sec)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the ambient light sensor and return it as an unsigned 16 - bit value.", "response": "def read_ambient(self, timeout_sec=1):\n        \"\"\"Read the ambient light sensor and return it as an unsigned 16-bit value.\n        \"\"\"\n        # Clear any interrupts.\n        self._clear_interrupt(VCNL4010_INT_ALS_READY)\n        # Call base class read_ambient and return result.\n        return super(VCNL4010, self).read_ambient(timeout_sec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndetects a single core - periphery pair.", "response": "def detect(self, G):\n\t\t\"\"\"Detect a single core-periphery pair.\n\t\n\t\tParameters\n\t\t----------\n\t\tG : NetworkX graph object\n\t\t\n\t\tExamples\n\t\t--------\n\t\t>>> import networkx as nx\n\t\t>>> import cpalgorithm as cpa\n\t\t>>> G = nx.karate_club_graph()  # load the karate club network. \n\t\t>>> rb = cp.Rombach(algorithm='ls') # label switching algorithm\n\t\t>>> rb.detect(G)\n\t\t>>> rb = cp.Rombach(algorithm='sa') # simulated annealing  \n\t\t>>> rb.detect(G)\n\n\t\t\"\"\"\n\t\t\n\t\tQbest = -100\t\n\t\tcbest = 0 \n\t\txbest = 0 \n\t\tqbest = 0 \n\t\tfor i in range(self.num_runs):\n\t\t\tif self.algorithm == 'ls':\n\t\t\t\n\t\t\t\tself._label_switching(G, self.alpha, self.beta)\n\t\n\t\t\telif self.algorithm == 'sa':\n\t\n\t\t\t\tself._simaneal(G, self.alpha, self.beta)\n\t\n\t\t\tif Qbest < self.Q_:\n\t\t\t\tQbest = self.Q_\t\n\t\t\t\tcbest = self.c_\n\t\t\t\txbest = self.x_\n\t\t\t\tqbest = self.qs_\n\t\t\n\t\tself.Q_ = Qbest \t\n\t\tself.c_ = cbest \n\t\tself.x_ = xbest \n\t\tself.qs_ = qbest"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetect a single core - periphery pair using the Borgatti - Everett algorithm.", "response": "def detect(self, G):\n\t\t\"\"\"Detect a single core-periphery pair using the Borgatti-Everett algorithm.\n\t\n\t\tParameters\n\t\t----------\n\t\tG : NetworkX graph object\n\t\t\n\t\tExamples\n\t\t--------\n\t\t>>> import networkx as nx\n\t\t>>> import cpalgorithm as cpa\n\t\t>>> G = nx.karate_club_graph()  # load the karate club network. \n\t\t>>> be = cpa.BE()\n\t\t>>> be.detect(G)\n\n\t\t\"\"\"\n\n\t\tnode_pairs, w, node2id, id2node = self._to_edge_list(G)\n\n\t\tcppairs = _cp.detect_be(edges=node_pairs, ws=w, num_of_runs = self.num_runs)\n\t\t\n\t\tN = len(id2node) \n\t\tself.c_ = dict(zip( [id2node[i] for i in range(N)], cppairs[0].astype(int)))\n\t\tself.x_ = dict(zip( [id2node[i] for i in range(N)], cppairs[1]))\n\t\tself.Q_ = cppairs[2][0]\n\t\tself.qs_ = cppairs[3].tolist()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef self_signed(self, value):\n\n        self._self_signed = bool(value)\n\n        if self._self_signed:\n            self._issuer = None", "response": "A bool - True if the certificate should be self - signed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the serial_number of the a field.", "response": "def serial_number(self, value):\n        \"\"\"\n        An int representable in 160 bits or less - must uniquely identify\n        this certificate when combined with the issuer name.\n        \"\"\"\n\n        if not isinstance(value, int_types):\n            raise TypeError(_pretty_message(\n                '''\n                serial_number must be an integer, not %s\n                ''',\n                _type_name(value)\n            ))\n\n        if value < 0:\n            raise ValueError(_pretty_message(\n                '''\n                serial_number must be a non-negative integer, not %s\n                ''',\n                repr(value)\n            ))\n\n        if len(int_to_bytes(value)) > 20:\n            required_bits = len(int_to_bytes(value)) * 8\n            raise ValueError(_pretty_message(\n                '''\n                serial_number must be an integer that can be represented by a\n                160-bit number, specified requires %s\n                ''',\n                required_bits\n            ))\n\n        self._serial_number = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the issuer field of the object.", "response": "def issuer(self, value):\n        \"\"\"\n        An asn1crypto.x509.Certificate object of the issuer. Used to populate\n        both the issuer field, but also the authority key identifier extension.\n        \"\"\"\n\n        is_oscrypto = isinstance(value, asymmetric.Certificate)\n        if not isinstance(value, x509.Certificate) and not is_oscrypto:\n            raise TypeError(_pretty_message(\n                '''\n                issuer must be an instance of asn1crypto.x509.Certificate or\n                oscrypto.asymmetric.Certificate, not %s\n                ''',\n                _type_name(value)\n            ))\n\n        if is_oscrypto:\n            value = value.asn1\n\n        self._issuer = value.subject\n\n        self._key_identifier = self._subject_public_key.sha1\n        self._authority_key_identifier = x509.AuthorityKeyIdentifier({\n            'key_identifier': value.public_key.sha1\n        })"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the begin date of the certificate.", "response": "def begin_date(self, value):\n        \"\"\"\n        A datetime.datetime object of when the certificate becomes valid.\n        \"\"\"\n\n        if not isinstance(value, datetime):\n            raise TypeError(_pretty_message(\n                '''\n                begin_date must be an instance of datetime.datetime, not %s\n                ''',\n                _type_name(value)\n            ))\n\n        self._begin_date = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the end_date of the certificate.", "response": "def end_date(self, value):\n        \"\"\"\n        A datetime.datetime object of when the certificate is last to be\n        considered valid.\n        \"\"\"\n\n        if not isinstance(value, datetime):\n            raise TypeError(_pretty_message(\n                '''\n                end_date must be an instance of datetime.datetime, not %s\n                ''',\n                _type_name(value)\n            ))\n\n        self._end_date = value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_crl_url(self, distribution_points):\n\n        if distribution_points is None:\n            return None\n\n        for distribution_point in distribution_points:\n            name = distribution_point['distribution_point']\n            if name.name == 'full_name' and name.chosen[0].name == 'uniform_resource_identifier':\n                return name.chosen[0].chosen.native\n\n        return None", "response": "Returns the first URL out of a asn1crypto. x509. CRLDistributionPoints object or None if there is no such object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs an asn1crypto. x509. CRLDistributionPoints object from the given attribute name and value.", "response": "def _make_crl_distribution_points(self, name, value):\n        \"\"\"\n        Constructs an asn1crypto.x509.CRLDistributionPoints object\n\n        :param name:\n            A unicode string of the attribute name to use in exceptions\n\n        :param value:\n            Either a unicode string of a URL, or a 2-element tuple of a\n            unicode string of a URL, plus an asn1crypto.x509.Certificate\n            object that will be signing the CRL (for indirect CRLs).\n\n        :return:\n            None or an asn1crypto.x509.CRLDistributionPoints object\n        \"\"\"\n\n        if value is None:\n            return None\n\n        is_tuple = isinstance(value, tuple)\n        if not is_tuple and not isinstance(value, str_cls):\n            raise TypeError(_pretty_message(\n                '''\n                %s must be a unicode string or tuple of (unicode string,\n                asn1crypto.x509.Certificate), not %s\n                ''',\n                name,\n                _type_name(value)\n            ))\n\n        issuer = None\n        if is_tuple:\n            if len(value) != 2:\n                raise ValueError(_pretty_message(\n                    '''\n                    %s must be a unicode string or 2-element tuple, not a\n                    %s-element tuple\n                    ''',\n                    name,\n                    len(value)\n                ))\n\n            if not isinstance(value[0], str_cls) or not isinstance(value[1], x509.Certificate):\n                raise TypeError(_pretty_message(\n                    '''\n                    %s must be a tuple of (unicode string,\n                    ans1crypto.x509.Certificate), not (%s, %s)\n                    ''',\n                    name,\n                    _type_name(value[0]),\n                    _type_name(value[1])\n                ))\n\n            url = value[0]\n            issuer = value[1].subject\n        else:\n            url = value\n\n        general_names = x509.GeneralNames([\n            x509.GeneralName(\n                name='uniform_resource_identifier',\n                value=url\n            )\n        ])\n        distribution_point_name = x509.DistributionPointName(\n            name='full_name',\n            value=general_names\n        )\n        distribution_point = x509.DistributionPoint({\n            'distribution_point': distribution_point_name\n        })\n        if issuer:\n            distribution_point['crl_issuer'] = x509.GeneralNames([\n                x509.GeneralName(name='directory_name', value=issuer)\n            ])\n\n        return x509.CRLDistributionPoints([distribution_point])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ocsp_url(self):\n\n        if self._authority_information_access is None:\n            return None\n\n        for ad in self._authority_information_access:\n            if ad['access_method'].native == 'ocsp' and ad['access_location'].name == 'uniform_resource_identifier':\n                return ad['access_location'].chosen.native\n\n        return None", "response": "Returns the URL of the OCSP responder for this certificate."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the OCSP no check flag.", "response": "def ocsp_no_check(self, value):\n        \"\"\"\n        A bool - if the certificate should have the OCSP no check extension.\n        Only applicable to certificates created for signing OCSP responses.\n        Such certificates should normally be issued for a very short period of\n        time since they are effectively whitelisted by clients.\n        \"\"\"\n\n        if value is None:\n            self._ocsp_no_check = None\n        else:\n            self._ocsp_no_check = bool(value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self, signing_private_key):\n\n        is_oscrypto = isinstance(signing_private_key, asymmetric.PrivateKey)\n        if not isinstance(signing_private_key, keys.PrivateKeyInfo) and not is_oscrypto:\n            raise TypeError(_pretty_message(\n                '''\n                signing_private_key must be an instance of\n                asn1crypto.keys.PrivateKeyInfo or\n                oscrypto.asymmetric.PrivateKey, not %s\n                ''',\n                _type_name(signing_private_key)\n            ))\n\n        if self._self_signed is not True and self._issuer is None:\n            raise ValueError(_pretty_message(\n                '''\n                Certificate must be self-signed, or an issuer must be specified\n                '''\n            ))\n\n        if self._self_signed:\n            self._issuer = self._subject\n\n        if self._serial_number is None:\n            time_part = int_to_bytes(int(time.time()))\n            random_part = util.rand_bytes(4)\n            self._serial_number = int_from_bytes(time_part + random_part)\n\n        if self._begin_date is None:\n            self._begin_date = datetime.now(timezone.utc)\n\n        if self._end_date is None:\n            self._end_date = self._begin_date + timedelta(365)\n\n        if not self.ca:\n            for ca_only_extension in set(['policy_mappings', 'policy_constraints', 'inhibit_any_policy']):\n                if ca_only_extension in self._other_extensions:\n                    raise ValueError(_pretty_message(\n                        '''\n                        Extension %s is only valid for CA certificates\n                        ''',\n                        ca_only_extension\n                    ))\n\n        signature_algo = signing_private_key.algorithm\n        if signature_algo == 'ec':\n            signature_algo = 'ecdsa'\n\n        signature_algorithm_id = '%s_%s' % (self._hash_algo, signature_algo)\n\n        # RFC 3280 4.1.2.5\n        def _make_validity_time(dt):\n            if dt < datetime(2050, 1, 1, tzinfo=timezone.utc):\n                value = x509.Time(name='utc_time', value=dt)\n            else:\n                value = x509.Time(name='general_time', value=dt)\n\n            return value\n\n        def _make_extension(name, value):\n            return {\n                'extn_id': name,\n                'critical': self._determine_critical(name),\n                'extn_value': value\n            }\n\n        extensions = []\n        for name in sorted(self._special_extensions):\n            value = getattr(self, '_%s' % name)\n            if name == 'ocsp_no_check':\n                value = core.Null() if value else None\n            if value is not None:\n                extensions.append(_make_extension(name, value))\n\n        for name in sorted(self._other_extensions.keys()):\n            extensions.append(_make_extension(name, self._other_extensions[name]))\n\n        tbs_cert = x509.TbsCertificate({\n            'version': 'v3',\n            'serial_number': self._serial_number,\n            'signature': {\n                'algorithm': signature_algorithm_id\n            },\n            'issuer': self._issuer,\n            'validity': {\n                'not_before': _make_validity_time(self._begin_date),\n                'not_after': _make_validity_time(self._end_date),\n            },\n            'subject': self._subject,\n            'subject_public_key_info': self._subject_public_key,\n            'extensions': extensions\n        })\n\n        if signing_private_key.algorithm == 'rsa':\n            sign_func = asymmetric.rsa_pkcs1v15_sign\n        elif signing_private_key.algorithm == 'dsa':\n            sign_func = asymmetric.dsa_sign\n        elif signing_private_key.algorithm == 'ec':\n            sign_func = asymmetric.ecdsa_sign\n\n        if not is_oscrypto:\n            signing_private_key = asymmetric.load_private_key(signing_private_key)\n        signature = sign_func(signing_private_key, tbs_cert.dump(), self._hash_algo)\n\n        return x509.Certificate({\n            'tbs_certificate': tbs_cert,\n            'signature_algorithm': {\n                'algorithm': signature_algorithm_id\n            },\n            'signature_value': signature\n        })", "response": "Validates the certificate information creates the ASN. 1 structure and then signs it."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetects a single core - periphery pair using the Borgatti - Everett algorithm.", "response": "def detect(self, G):\n\t\t\"\"\"Detect a single core-periphery pair using the Borgatti-Everett algorithm.\n\t\n\t\tParameters\n\t\t----------\n\t\tG : NetworkX graph object\n\t\t\n\t\tExamples\n\t\t--------\n\t\t>>> import networkx as nx\n\t\t>>> import cpalgorithm as cpa\n\t\t>>> G = nx.karate_club_graph()  # load the karate club network. \n\t\t>>> spr = cpa.Surprise()\n\t\t>>> spr.detect(G)\n\n\t\t\"\"\"\n\n\t\tnodes = G.nodes()\n\t\tN = len(nodes)\n\t\tCbest =np.zeros(N)\n\t\tqbest = 0 \n\t\tfor it in range(self.num_runs):\t\n\n\t\t\tC, q = self._detect(G)\n\t\t\n\t\t\tif q < qbest:\n\t\t\t\tCbest = C\n\t\t\t\tqbest = q\n\t\t\n\t\t# ------------\t\n\t\t# Post process \n\t\t# ------------\n\t\tself.c_ = dict(zip(nodes, np.zeros(N).astype(int)))\n\t\tself.x_ = dict(zip(nodes, Cbest.astype(int)))\n\t\tself.Q_ = self._score(G, self.c_, self.x_)[0]\n\t\tself.qs_ = [self.Q_]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remodel_run(self, c=None, **global_optargs):\n\n    if not c:\n        with remodel.connection.get_conn() as conn:\n            return run(self, conn, **global_optargs)\n    else:\n        return run(self, c, **global_optargs)", "response": "Run the remodel query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emptylineless(parser, token):\n    nodelist = parser.parse(('endemptylineless',))\n    parser.delete_first_token()\n    return EmptylinelessNode(nodelist)", "response": "Removes empty line.\n\n    Example usage::\n\n        {% emptylineless %}\n            test1\n\n            test2\n\n            test3\n        {% endemptylineless %}\n\n    This example would return this HTML::\n\n            test1\n            test2\n            test3"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetect a single core - periphery pair.", "response": "def detect(self, G):\n\t\t\"\"\"Detect a single core-periphery pair.\n\t\n\t\tParameters\n\t\t----------\n\t\tG : NetworkX graph object\n\t\t\n\t\tExamples\n\t\t--------\n\t\t>>> import networkx as nx\n\t\t>>> import cpalgorithm as cpa\n\t\t>>> G = nx.karate_club_graph()  # load the karate club network. \n\t\t>>> lrc = cp.LowRankCore()\n\t\t>>> lrc.detect(G)\n\n\t\t\"\"\"\n\t\t\n\t\tself.c_, self.x_ = self._low_rank_core(G)\n\t\t\n\t\tself.Q_ = self._score(G, self.c_, self.x_) \n\t\tself.qs_ = self.Q_"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef http_purge_url(url):\n    url = urlparse(url)\n    connection = HTTPConnection(url.hostname, url.port or 80)\n    path = url.path or '/'\n    connection.request('PURGE', '%s?%s' % (path, url.query) if url.query else path, '',\n                       {'Host': '%s:%s' % (url.hostname, url.port) if url.port else url.hostname})\n    response = connection.getresponse()\n    if response.status != 200:\n        logging.error('Purge failed with status: %s' % response.status)\n    return response", "response": "Do an HTTP PURGE of the given asset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetch(self, command):\n        logging.debug('SENT: %s: %s' % (self.host, command))\n        self.write('%s\\n' % command)\n        while 1:\n            buffer = self.read_until('\\n').strip()\n            if len(buffer):\n                break\n        status, length = map(int, buffer.split())\n        content = ''\n        assert status == 200, 'Bad response code: {status} {text} ({command})'.format(status=status, text=self.read_until('\\n').strip(), command=command)\n        while len(content) < length:\n            content += self.read_until('\\n')\n        logging.debug('RECV: %s: %dB %s' % (status,length,content[:30]))\n        self.read_eager()\n        return (status, length), content", "response": "Run a command on the Varnish backend and return the result\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npinging the Varnish cache process keeping the connection alive.", "response": "def ping(self, timestamp=None):\n        \"\"\"\n        ping [timestamp]\n            Ping the Varnish cache process, keeping the connection alive.\n        \"\"\"\n        cmd = 'ping'\n        if timestamp: cmd += ' %s' % timestamp\n        return tuple(map(float, self.fetch(cmd)[1].split()[1:]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef help(self, command=None):\n        cmd = 'help'\n        if command: cmd += ' %s' % command\n        return self.fetch(cmd)[1]", "response": "Display a list of available commands."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef vcl_list(self):\n        vcls = {}\n        for line in self.fetch('vcl.list')[1].splitlines():\n            a = line.split()\n            vcls[a[2]] = tuple(a[:-1])\n        return vcls", "response": "Return a dictionary of available configuration and their respective reference counts."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef param_show(self, param, l=False):\n        cmd = 'param.show '\n        if l: cmd += '-l '\n        return self.fetch(cmd + param)", "response": "Display the value and explanation of a parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning to convert the markdown file to HTML", "response": "def main():\n    \"\"\"function to \"\"\"\n    # parse arg to find file(s)\n    parser = argparse.ArgumentParser()\n    parser.add_argument(\"-f\", \"--file\",\n                        help=\"convert the markdown file to HTML\")\n    parser.add_argument(\"-d\", \"--directory\",\n                        help=\"convert the markdown files in the directory to HTML\")\n    parser.add_argument(\"-o\", \"--output\",\n                        help=\"chose the output filename\")\n    parser.add_argument(\"--no_browser\",  nargs='?', const=True,\n                        help=\"if stated, will prevent browser from opening\")\n    args = parser.parse_args()\n\n    print(args)\n\n    superMarkdown = SuperMarkdown()\n\n    if args.output:  # get the new output url\n        superMarkdown.export_url = args.output\n\n    if args.no_browser:\n        superMarkdown.open_browser = False\n\n    if args.directory:  # get all files from directory\n        superMarkdown.add_toc()\n        files = [file for file in os.listdir(args.directory) if not os.path.isdir(file)]\n        superMarkdown.add_content(*files)\n\n    elif args.file:  # get the file from directory\n        superMarkdown.add_content(args.file)\n\n    else:  # get the default markdown file `ressources/test.markdown`\n        superMarkdown.add_content('SuperMarkdown/ressources/test.md')\n\n    superMarkdown.export()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_content(self, text=None, *markdown_files):\n        for markdown_file in markdown_files:\n            self.markdown_text += self._text_file(markdown_file)\n\n        if text:\n            self.markdown_text += text", "response": "add the content of the files or the text in string in HTML body"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_stylesheets(self, *css_files):\n        for css_file in css_files:\n            self.main_soup.style.append(self._text_file(css_file))", "response": "add stylesheet files in HTML head"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_javascripts(self, *js_files):\n        # create the script tag if don't exists\n        if self.main_soup.script is None:\n            script_tag = self.main_soup.new_tag('script')\n            self.main_soup.body.append(script_tag)\n\n        for js_file in js_files:\n            self.main_soup.script.append(self._text_file(js_file))", "response": "add javascripts files in HTML body"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the object in a file", "response": "def export(self):\n        \"\"\"return the object in a file\"\"\"\n\n        with open(self.export_url, 'w', encoding='utf-8') as file:\n            file.write(self.build())\n            if self.open_browser:\n                webbrowser.open_new_tab(self.export_url)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build(self):\n        markdown_html = markdown.markdown(self.markdown_text, extensions=[\n                TocExtension(), 'fenced_code', 'markdown_checklist.extension',\n                'markdown.extensions.tables'])\n        markdown_soup = BeautifulSoup(markdown_html, 'html.parser')\n\n        # include jquery & mermaid.js only if there are Mermaid graph\n        if markdown_soup.find('code', attrs={'class': 'mermaid'}):\n            self._add_mermaid_js()\n\n        # search in markdown html if there are Dot Graph & replace it with .svg result\n        for dot_tag in markdown_soup.find_all('code', attrs={'class': 'dotgraph'}):\n            grap_svg = self._text_to_graphiz(dot_tag.string)\n            graph_soup = BeautifulSoup(grap_svg, 'html.parser')\n            dot_tag.parent.replaceWith(graph_soup)\n\n        self.main_soup.body.append(markdown_soup)\n        return self.main_soup.prettify()", "response": "convert Markdown text as html. return the html file as string"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _text_file(self, url):\n        try:\n            with open(url, 'r', encoding='utf-8') as file:\n                return file.read()\n        except FileNotFoundError:\n            print('File `{}` not found'.format(url))\n            sys.exit(0)", "response": "return the content of a file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _text_to_graphiz(self, text):\n        dot = Source(text, format='svg')\n        return dot.pipe().decode('utf-8')", "response": "create a graphviz graph from text"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd js libraries and css files of mermaid js_file", "response": "def _add_mermaid_js(self):\n        \"\"\"add js libraries and css files of mermaid js_file\"\"\"\n        self.add_javascripts('{}/js/jquery-1.11.3.min.js'.format(self.resources_path))\n        self.add_javascripts('{}/js/mermaid.min.js'.format(self.resources_path))\n        self.add_stylesheets('{}/css/mermaid.css'.format(self.resources_path))\n        self.main_soup.script.append('mermaid.initialize({startOnLoad:true  });')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef score(*args):\n\t\tself = args[0]\n\n\t\tif len(args) ==1:\n\t\t\treturn self.qs_\n\t\telse:\n\t\t\tG = args[1]\n\t\t\tc = args[2]\n\t\t\tx = args[3]\n\t\t\treturn self._score(G, c, x)", "response": "Get score of core - periphery pairs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _to_edge_list(self, G):\n\t\tnode2id = dict(zip(G.nodes, range(len(G.nodes))))\n\t\tid2node= dict((v,k) for k,v in node2id.items())\n\t\n\t\tnx.relabel_nodes(G, node2id,False)\n\t\tedges = G.edges(data=\"weight\")\t\n\t\n\t\tnode_pairs = np.array([ [edge[0], edge[1]] for edge in edges ]).astype(int)\n\t\tw = np.array([ edge[2] for edge in edges ]).astype(float)\n\t\n\t\tif all(np.isnan(w)):\n\t\t\tnx.set_edge_attributes(G, values =1, name='weight')\n\t\t\tw[:] = 1.0\n\t\t\n\t\tnx.relabel_nodes(G,id2node,False)\n\t\n\t\treturn node_pairs, w, node2id, id2node", "response": "Transform NetworkX object to an edge list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting the identifier out of this construct.", "response": "def getSource(self):\n        \"\"\"Extract the identifier out of this construct: ${mylist}: mylist\n        \"\"\"\n        bracket = self.next()\n        # we should only be here because that was a bracket\n        if not bracket == u'{':\n            raise Exception(u\"parse error getting source\")\n        c = u''\n        identifier = u''\n        while True:\n            c = self.next()\n            if not c:\n                raise Exception(u\"unexpected end of input getting source\")\n            elif c == u'}':\n                break\n            else:\n                identifier += c\n        if not identifier or not isidentifier(identifier):\n            raise StringGenerator.SyntaxError(u\"not a valid identifier: %s\"%identifier)\n        return StringGenerator.Source(identifier)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a character set with individual members or ranges.", "response": "def getCharacterSet(self):\n        '''Get a character set with individual members or ranges.\n\n        Current index is on '[', the start of the character set.\n\n        '''\n        \n        chars = u''\n        c = None\n        cnt = 1\n        start = 0\n\n        while True:\n            escaped_slash = False\n            c = self.next()\n            # print \"pattern   : \", self.pattern\n            # print \"C         : \", c\n            # print \"Slash     : \", c == u'\\\\'\n            # print 'chars     : ', chars\n            # print 'index     : ', self.index\n            # print 'last      : ', self.last()\n            # print 'lookahead : ', self.lookahead()\n            if self.lookahead() == u'-' and not c == u'\\\\':\n                f = c\n                self.next()  # skip hyphen\n                c = self.next()  # get far range\n                if not c or (c in self.meta_chars):\n                    raise StringGenerator.SyntaxError(u\"unexpected end of class range\")\n                chars += self.getCharacterRange(f, c)\n            elif c == u'\\\\':\n                if self.lookahead() in self.meta_chars:\n                    c = self.next()\n                    chars += c\n                    continue\n                elif self.lookahead() in self.string_code:\n                    c = self.next()\n                    chars += self.string_code[c]\n            elif c and c not in self.meta_chars:\n                chars += c\n            if c == u']': \n                if self.lookahead() == u'{':\n                    [start, cnt] = self.getQuantifier()\n                else:\n                    start = -1\n                    cnt = 1\n                break\n            if c and c in self.meta_chars and not self.last() == u\"\\\\\":\n                raise StringGenerator.SyntaxError(u\"Un-escaped character in class definition: %s\" % c)\n            if not c:\n                break\n\n        return StringGenerator.CharacterSet(chars, start, cnt)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a sequence of non - special characters.", "response": "def getLiteral(self):\n        '''Get a sequence of non-special characters.'''\n        # we are on the first non-special character\n        chars = u''\n        c = self.current()\n        while True:\n            if c and c == u\"\\\\\":\n                c = self.next()\n                if c:\n                    chars += c\n                continue\n            elif not c or (c in self.meta_chars):\n                break\n            else:\n                chars += c\n            if self.lookahead() and self.lookahead() in self.meta_chars:\n                break\n            c = self.next()\n        return StringGenerator.Literal(chars)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getSequence(self, level=0):\n        '''Get a sequence of nodes.'''\n\n        seq = []\n        op = ''\n        left_operand = None\n        right_operand = None\n        sequence_closed = False\n        while True:\n            c = self.next()\n            if not c:\n                break\n            if c and c not in self.meta_chars:\n                seq.append(self.getLiteral())\n            elif c and c == u'$' and self.lookahead() == u'{':\n                seq.append(self.getSource())\n            elif c == u'[' and not self.last() == u'\\\\':\n                seq.append(self.getCharacterSet())\n            elif c == u'(' and not self.last() == u'\\\\':\n                seq.append(self.getSequence(level + 1))\n            elif c == u')' and not self.last() == u'\\\\':\n                # end of this sequence\n                if level == 0:\n                    # there should be no parens here\n                    raise StringGenerator.SyntaxError(u\"Extra closing parenthesis\")\n                sequence_closed = True\n                break\n            elif c == u'|' and not self.last() == u'\\\\':\n                op = c\n            elif c == u'&' and not self.last() == u'\\\\':\n                op = c\n            else:\n                if c in self.meta_chars and not self.last() == u\"\\\\\":\n                    raise StringGenerator.SyntaxError(u\"Un-escaped special character: %s\" % c)\n            \n            #print( op,len(seq) )\n            if op and not left_operand:\n                if not seq or len(seq) < 1:\n                    raise StringGenerator.SyntaxError(u\"Operator: %s with no left operand\" % op)\n                left_operand = seq.pop()\n            elif op and len(seq) >= 1 and left_operand:\n                right_operand = seq.pop()\n\n                #print( \"popped: [%s] %s:%s\"%( op, left_operand, right_operand) )\n                if op == u'|':\n                    seq.append(StringGenerator.SequenceOR([left_operand, right_operand]))\n                elif op == u'&':\n                    seq.append(StringGenerator.SequenceAND([left_operand, right_operand]))\n\n                op = u''\n                left_operand = None\n                right_operand = None\n\n        # check for syntax errors\n        if op:\n            raise StringGenerator.SyntaxError(u\"Operator: %s with no right operand\" % op)\n        if level > 0 and not sequence_closed:\n            # it means we are finishing a non-first-level sequence without closing parens\n            raise StringGenerator.SyntaxError(u\"Missing closing parenthesis\")\n\n        return StringGenerator.Sequence(seq)", "response": "Get a sequence of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dump(self, **kwargs):\n        import sys\n        '''Print the parse tree and then call render for an example.'''\n        if not self.seq:\n            self.seq = self.getSequence()\n        print(\"StringGenerator version: %s\" % (__version__))\n        print(\"Python version: %s\" % sys.version)\n        # this doesn't work anymore in p3\n        # print(\"Random method provider class: %s\" % randint.im_class.__name__)\n        self.seq.dump()\n        return self.render(**kwargs)", "response": "Print the parse tree and then call render for an example."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_list(self, cnt, unique=False, progress_callback=None, **kwargs):\n        '''Return a list of generated strings.\n\n        Args:\n            cnt (int): length of list\n            unique (bool): whether to make entries unique\n\n        Returns:\n            list.\n\n        We keep track of total attempts because a template may\n        specify something impossible to attain, like [1-9]{} with cnt==1000\n\n        '''\n        \n        rendered_list = []\n        i = 0\n        total_attempts = 0\n        while True:\n            if i >= cnt:\n                break\n            if total_attempts > cnt * self.unique_attempts_factor:\n                raise StringGenerator.UniquenessError(u\"couldn't satisfy uniqueness\")\n            s = self.render(**kwargs)\n            if unique:\n                if not s in rendered_list:\n                    rendered_list.append(s)\n                    i += 1\n            else:\n                rendered_list.append(s)\n                i += 1\n            total_attempts += 1\n\n            # Optionally trigger the progress indicator to inform others about our progress\n            if progress_callback and callable(progress_callback):\n                progress_callback(i, cnt)\n\n        return rendered_list", "response": "Return a list of generated strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef connect(self):\n        self.conn = boto.connect_s3(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)\n\n        self.bucket = self.conn.get_bucket(self.AWS_STORAGE_BUCKET_NAME)\n\n        self.k = Key(self.bucket)", "response": "Establish the connection to the S3 bucket and set the keys."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef connect_cloudfront(self):\n        \"Connect to Cloud Front. This is done automatically for you when needed.\"\n        self.conn_cloudfront = connect_cloudfront(self.AWS_ACCESS_KEY_ID, self.AWS_SECRET_ACCESS_KEY, debug=self.S3UTILS_DEBUG_LEVEL)", "response": "Connect to Cloud Front. This is done automatically for you when needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mkdir(self, target_folder):\n        self.printv(\"Making directory: %s\" % target_folder)\n        self.k.key = re.sub(r\"^/|/$\", \"\", target_folder) + \"/\"\n        self.k.set_contents_from_string('')\n        self.k.close()", "response": "Create a folder on S3."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rm(self, path):\n\n        list_of_files = list(self.ls(path))\n\n        if list_of_files:\n            if len(list_of_files) == 1:\n                self.bucket.delete_key(list_of_files[0])\n            else:\n                self.bucket.delete_keys(list_of_files)\n            self.printv(\"Deleted: %s\" % list_of_files)\n        else:\n            logger.error(\"There was nothing to remove under %s\", path)", "response": "Delete the path and any files under the path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __put_key(self, local_file, target_file, acl='public-read', del_after_upload=False, overwrite=True, source=\"filename\"):\n        action_word = \"moving\" if del_after_upload else \"copying\"\n\n        try:\n            self.k.key = target_file  # setting the path (key) of file in the container\n\n            if source == \"filename\":\n                # grabs the contents from local_file address. Note that it loads the whole file into memory\n                self.k.set_contents_from_filename(local_file, self.AWS_HEADERS)\n            elif source == \"fileobj\":\n                self.k.set_contents_from_file(local_file, self.AWS_HEADERS)\n            elif source == \"string\":\n                self.k.set_contents_from_string(local_file, self.AWS_HEADERS)\n            else:\n                raise Exception(\"%s is not implemented as a source.\" % source)\n            self.k.set_acl(acl)  # setting the file permissions\n            self.k.close()  # not sure if it is needed. Somewhere I read it is recommended.\n\n            self.printv(\"%s %s to %s\" % (action_word, local_file, target_file))\n            # if it is supposed to delete the local file after uploading\n            if del_after_upload and source == \"filename\":\n                try:\n                    os.remove(local_file)\n                except:\n                    logger.error(\"Unable to delete the file: \", local_file, exc_info=True)\n\n            return True\n\n        except:\n            logger.error(\"Error in writing to %s\", target_file, exc_info=True)\n            return False", "response": "Copy a file to s3."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncopies a file or folder from local to S3.", "response": "def cp(self, local_path, target_path, acl='public-read',\n           del_after_upload=False, overwrite=True, invalidate=False):\n        \"\"\"\n        Copy a file or folder from local to s3.\n\n        Parameters\n        ----------\n\n        local_path : string\n            Path to file or folder. Or if you want to copy only the contents of folder, add /* at the end of folder name\n\n        target_path : string\n            Target path on S3 bucket.\n\n        acl : string, optional\n            File permissions on S3. Default is public-read\n\n            options:\n                - private: Owner gets FULL_CONTROL. No one else has any access rights.\n                - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.\n                - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.\n                - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access\n\n\n        del_after_upload : boolean, optional\n            delete the local file after uploading. This is effectively like moving the file.\n            You can use s3utils.mv instead of s3utils.cp to move files from local to S3.\n            It basically sets this flag to True.\n            default = False\n\n        overwrite : boolean, optional\n            overwrites files on S3 if set to True. Default is True\n\n        invalidate : boolean, optional\n            invalidates the CDN (a.k.a Distribution) cache if the file already exists on S3\n            default = False\n            Note that invalidation might take up to 15 minutes to take place. It is easier and faster to use cache buster\n            to grab lastest version of your file on CDN than invalidation.\n\n        **Returns**\n\n        Nothing on success but it will return what went wrong if something fails.\n\n        Examples\n        --------\n            >>> s3utils.cp(\"path/to/folder\",\"/test/\")\n            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt\n            copying /path/to/myfolder/test.txt to test/myfolder/test.txt\n            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG\n            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff\n\n            >>> # When overwrite is set to False, it returns the file(s) that were already existing on s3 and were not overwritten.\n            >>> s3utils.cp(\"/tmp/test3.txt\", \"test3.txt\", overwrite=False)\n            ERROR:root:test3.txt already exist. Not overwriting.\n            >>> {'existing_files': {'test3.txt'}}\n\n            >>> # To overwrite the files on S3 and invalidate the CDN (cloudfront) cache so the new file goes on CDN:\n            >>> s3utils.cp(\"path/to/folder\",\"/test/\", invalidate=True)\n            copying /path/to/myfolder/test2.txt to test/myfolder/test2.txt\n            copying /path/to/myfolder/test.txt to test/myfolder/test.txt\n            copying /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG\n            copying /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff\n\n            >>> # When file does not exist, it returns a dictionary of what went wrong.\n            >>> s3utils.cp(\"/tmp/does_not_exist\", \"somewhere\")\n            ERROR:root:trying to upload to s3 but file doesn't exist: /tmp/does_not_exist\n            >>> {'file_does_not_exist': '/tmp/does_not_exist'}\n        \"\"\"\n        result = None\n        if overwrite:\n            list_of_files = []\n        else:\n            list_of_files = self.ls(folder=target_path, begin_from_file=\"\", num=-1, get_grants=False, all_grant_data=False)\n\n        # copying the contents of the folder and not folder itself\n        if local_path.endswith(\"/*\"):\n            local_path = local_path[:-2]\n            target_path = re.sub(r\"^/|/$\", \"\", target_path)  # Amazon S3 doesn't let the name to begin with /\n        # copying folder too\n        else:\n            local_base_name = os.path.basename(local_path)\n\n            local_path = re.sub(r\"/$\", \"\", local_path)\n            target_path = re.sub(r\"^/\", \"\", target_path)\n\n            if not target_path.endswith(local_base_name):\n                target_path = os.path.join(target_path, local_base_name)\n\n        if os.path.exists(local_path):\n\n            result = self.__find_files_and_copy(local_path, target_path, acl, del_after_upload, overwrite, invalidate, list_of_files)\n\n        else:\n            result = {'file_does_not_exist': local_path}\n            logger.error(\"trying to upload to s3 but file doesn't exist: %s\" % local_path)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nmoving the file to the S3 and delete the local copy", "response": "def mv(self, local_file, target_file, acl='public-read', overwrite=True, invalidate=False):\n        \"\"\"\n        Similar to Linux mv command.\n\n        Move the file to the S3 and deletes the local copy\n\n        It is basically s3utils.cp that has del_after_upload=True\n\n        Examples\n        --------\n            >>> s3utils.mv(\"path/to/folder\",\"/test/\")\n            moving /path/to/myfolder/test2.txt to test/myfolder/test2.txt\n            moving /path/to/myfolder/test.txt to test/myfolder/test.txt\n            moving /path/to/myfolder/hoho/photo.JPG to test/myfolder/hoho/photo.JPG\n            moving /path/to/myfolder/hoho/haha/ff to test/myfolder/hoho/haha/ff\n\n        **Returns:**\n\n        Nothing on success, otherwise what went wrong.\n\n        Return type:\n        dict\n\n        \"\"\"\n        self.cp(local_file, target_file, acl=acl, del_after_upload=True, overwrite=overwrite, invalidate=invalidate)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndealing with saving cropduster images to S3. Cropduster is a Django library for resizing editorial images. S3utils was originally written to put cropduster images on S3 bucket. Extra Items in your Django Settings ----------------------------------- MEDIA_ROOT : string Django media root. Currently it is ONLY used in cp_cropduster_image method. NOT any other method as this library was originally made to put Django cropduster images on s3 bucket. S3_ROOT_BASE : string S3 media root base. This will be the root folder in S3. Currently it is ONLY used in cp_cropduster_image method. NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.", "response": "def cp_cropduster_image(self, the_image_path, del_after_upload=False, overwrite=False, invalidate=False):\n        \"\"\"\n        Deal with saving cropduster images to S3. Cropduster is a Django library for resizing editorial images.\n        S3utils was originally written to put cropduster images on S3 bucket.\n\n        Extra Items in your Django Settings\n        -----------------------------------\n\n        MEDIA_ROOT : string\n            Django media root.\n            Currently it is ONLY used in cp_cropduster_image method.\n            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.\n\n        S3_ROOT_BASE : string\n            S3 media root base. This will be the root folder in S3.\n            Currently it is ONLY used in cp_cropduster_image method.\n            NOT any other method as this library was originally made to put Django cropduster images on s3 bucket.\n\n\n        \"\"\"\n\n        local_file = os.path.join(settings.MEDIA_ROOT, the_image_path)\n\n        # only try to upload things if the origin cropduster file exists (so it is not already uploaded to the CDN)\n        if os.path.exists(local_file):\n\n            the_image_crops_path = os.path.splitext(the_image_path)[0]\n            the_image_crops_path_full_path = os.path.join(settings.MEDIA_ROOT, the_image_crops_path)\n\n            self.cp(local_path=local_file,\n                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_path),\n                    del_after_upload=del_after_upload,\n                    overwrite=overwrite,\n                    invalidate=invalidate,\n                    )\n\n            self.cp(local_path=the_image_crops_path_full_path + \"/*\",\n                    target_path=os.path.join(settings.S3_ROOT_BASE, the_image_crops_path),\n                    del_after_upload=del_after_upload,\n                    overwrite=overwrite,\n                    invalidate=invalidate,\n                    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the grants for a file", "response": "def __get_grants(self, target_file, all_grant_data):\n        \"\"\"\n        Return grant permission, grant owner, grant owner email and grant id  as a list.\n        It needs you to set k.key to a key on amazon (file path) before running this.\n        note that Amazon returns a list of grants for each file.\n\n        options:\n            - private: Owner gets FULL_CONTROL. No one else has any access rights.\n            - public-read: Owners gets FULL_CONTROL and the anonymous principal is granted READ access.\n            - public-read-write: Owner gets FULL_CONTROL and the anonymous principal is granted READ and WRITE access.\n            - authenticated-read: Owner gets FULL_CONTROL and any principal authenticated as a registered Amazon S3 user is granted READ access\n\n        \"\"\"\n        self.k.key = target_file\n\n        the_grants = self.k.get_acl().acl.grants\n\n        grant_list = []\n\n        for grant in the_grants:\n            if all_grant_data:\n                grant_list.append(\n                    {\"permission\": grant.permission, \"name\": grant.display_name, \"email\": grant.email_address, \"id\": grant.id})\n            else:\n                grant_list.append({\"permission\": grant.permission, \"name\": grant.display_name})\n\n        return grant_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef chmod(self, target_file, acl='public-read'):\n        self.k.key = target_file  # setting the path (key) of file in the container\n        self.k.set_acl(acl)  # setting the file permissions\n        self.k.close()", "response": "Set the permissions for a file on S3 container."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ls(self, folder=\"\", begin_from_file=\"\", num=-1, get_grants=False, all_grant_data=False):\n        # S3 object key can't start with /\n        folder = re.sub(r\"^/\", \"\", folder)\n\n        bucket_files = self.bucket.list(prefix=folder, marker=begin_from_file)\n\n        # in case listing grants\n        if get_grants:\n            list_of_files = OrderedDict()\n            for (i, v) in enumerate(bucket_files):\n                file_info = {v.name: self.__get_grants(v.name, all_grant_data)}\n                list_of_files.update(file_info)\n                if i == num:\n                    break\n\n        else:\n            list_of_files = set([])\n            for (i, v) in enumerate(bucket_files):\n                list_of_files.add(v.name)\n                if i == num:\n                    break\n\n        return list_of_files", "response": "This method returns a list of file names in a folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the list of files and permissions from S3. This is similar to LL (ls -lah) in Linux: List of files with permissions. Parameters ---------- folder : string Path to file on S3 num: integer, optional number of results to return, by default it returns all results. begin_from_file : string, optional which file to start from on S3. This is usedful in case you are iterating over lists of files and you need to page the result by starting listing from a certain file and fetching certain num (number) of files. all_grant_data : Boolean, optional More detailed file permission data will be returned. Examples -------- >>> from s3utils import S3utils >>> s3utils = S3utils( ... AWS_ACCESS_KEY_ID = 'your access key', ... AWS_SECRET_ACCESS_KEY = 'your secret key', ... AWS_STORAGE_BUCKET_NAME = 'your bucket name', ... S3UTILS_DEBUG_LEVEL = 1, #change it to 0 for less verbose ... ) >>> import json >>> # We use json.dumps to print the results more readable: >>> my_folder_stuff = s3utils.ll(\"/test/\") >>> print(json.dumps(my_folder_stuff, indent=2)) { \"test/myfolder/\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" } ], \"test/myfolder/em/\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" } ], \"test/myfolder/hoho/\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" } ], \"test/myfolder/hoho/.DS_Store\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" }, { \"name\": null, \"permission\": \"READ\" } ], \"test/myfolder/hoho/haha/\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" } ], \"test/myfolder/hoho/haha/ff\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" }, { \"name\": null, \"permission\": \"READ\" } ], \"test/myfolder/hoho/photo.JPG\": [ { \"name\": \"owner's name\", \"permission\": \"FULL_CONTROL\" }, { \"name\": null, \"permission\": \"READ\" } ], }", "response": "def ll(self, folder=\"\", begin_from_file=\"\", num=-1, all_grant_data=False):\n        \"\"\"\n        Get the list of files and permissions from S3.\n\n        This is similar to LL (ls -lah) in Linux: List of files with permissions.\n\n        Parameters\n        ----------\n\n        folder : string\n            Path to file on S3\n\n        num: integer, optional\n            number of results to return, by default it returns all results.\n\n        begin_from_file : string, optional\n            which file to start from on S3.\n            This is usedful in case you are iterating over lists of files and you need to page the result by\n            starting listing from a certain file and fetching certain num (number) of files.\n\n        all_grant_data : Boolean, optional\n            More detailed file permission data will be returned.\n\n        Examples\n        --------\n\n            >>> from s3utils import S3utils\n            >>> s3utils = S3utils(\n            ... AWS_ACCESS_KEY_ID = 'your access key',\n            ... AWS_SECRET_ACCESS_KEY = 'your secret key',\n            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',\n            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose\n            ... )\n            >>> import json\n            >>> # We use json.dumps to print the results more readable:\n            >>> my_folder_stuff = s3utils.ll(\"/test/\")\n            >>> print(json.dumps(my_folder_stuff, indent=2))\n            {\n              \"test/myfolder/\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                }\n              ],\n              \"test/myfolder/em/\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                }\n              ],\n              \"test/myfolder/hoho/\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                }\n              ],\n              \"test/myfolder/hoho/.DS_Store\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                },\n                {\n                  \"name\": null,\n                  \"permission\": \"READ\"\n                }\n              ],\n              \"test/myfolder/hoho/haha/\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                }\n              ],\n              \"test/myfolder/hoho/haha/ff\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                },\n                {\n                  \"name\": null,\n                  \"permission\": \"READ\"\n                }\n              ],\n              \"test/myfolder/hoho/photo.JPG\": [\n                {\n                  \"name\": \"owner's name\",\n                  \"permission\": \"FULL_CONTROL\"\n                },\n                {\n                  \"name\": null,\n                  \"permission\": \"READ\"\n                }\n              ],\n            }\n\n        \"\"\"\n        return self.ls(folder=folder, begin_from_file=begin_from_file, num=num, get_grants=True, all_grant_data=all_grant_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninvalidating the CDN (distribution) cache for a certain file of files. This might take up to 15 minutes to be effective. You can check for the invalidation status using check_invalidation_request. Examples -------- >>> from s3utils import S3utils >>> s3utils = S3utils( ... AWS_ACCESS_KEY_ID = 'your access key', ... AWS_SECRET_ACCESS_KEY = 'your secret key', ... AWS_STORAGE_BUCKET_NAME = 'your bucket name', ... S3UTILS_DEBUG_LEVEL = 1, #change it to 0 for less verbose ... ) >>> aa = s3utils.invalidate(\"test/myfolder/hoho/photo.JPG\") >>> print(aa) ('your distro id', u'your request id') >>> invalidation_request_id = aa[1] >>> bb = s3utils.check_invalidation_request(*aa) >>> for inval in bb: ... print('Object: %s, ID: %s, Status: %s' % (inval, inval.id, inval.status))", "response": "def invalidate(self, files_to_be_invalidated):\n        \"\"\"\n        Invalidate the CDN (distribution) cache for a certain file of files. This might take up to 15 minutes to be effective.\n\n        You can check for the invalidation status using check_invalidation_request.\n\n        Examples\n        --------\n\n            >>> from s3utils import S3utils\n            >>> s3utils = S3utils(\n            ... AWS_ACCESS_KEY_ID = 'your access key',\n            ... AWS_SECRET_ACCESS_KEY = 'your secret key',\n            ... AWS_STORAGE_BUCKET_NAME = 'your bucket name',\n            ... S3UTILS_DEBUG_LEVEL = 1,  #change it to 0 for less verbose\n            ... )\n            >>> aa = s3utils.invalidate(\"test/myfolder/hoho/photo.JPG\")\n            >>> print(aa)\n            ('your distro id', u'your request id')\n            >>> invalidation_request_id = aa[1]\n            >>> bb = s3utils.check_invalidation_request(*aa)\n            >>> for inval in bb:\n            ...     print('Object: %s, ID: %s, Status: %s' % (inval, inval.id, inval.status))\n\n\n        \"\"\"\n        if not isinstance(files_to_be_invalidated, Iterable):\n            files_to_be_invalidated = (files_to_be_invalidated,)\n\n        # Your CDN is called distribution on Amazaon. And you can have more than one distro\n        all_distros = self.conn_cloudfront.get_all_distributions()\n\n        for distro in all_distros:\n            invalidation_request = self.conn_cloudfront.create_invalidation_request(distro.id, files_to_be_invalidated)\n\n        return (distro.id, invalidation_request.id)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a SortAnchorNode for the given tag.", "response": "def anchor(parser, token):\n    \"\"\"\n    Parses a tag that's supposed to be in this format: {% anchor field title %}    \n    \"\"\"\n    bits = [b.strip('\"\\'') for b in token.split_contents()]\n    if len(bits) < 2:\n        raise TemplateSyntaxError, \"anchor tag takes at least 1 argument\"\n    try:\n        title = bits[2]\n    except IndexError:\n        title = bits[1].capitalize()\n    return SortAnchorNode(bits[1].strip(), title.strip())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the path from a given url including the querystring.", "response": "def get_path(url):\n    \"\"\"\n    Get the path from a given url, including the querystring.\n\n    Args:\n        url (str)\n    Returns:\n        str\n\n    \"\"\"\n\n    url = urlsplit(url)\n    path = url.path\n    if url.query:\n        path += \"?{}\".format(url.query)\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        # Try to create the directory\n        if not os.path.exists(self.output):\n            try:\n                os.mkdir(self.output)\n            except:\n                print 'failed to create output directory %s' % self.output\n\n        # Be sure it is a directory\n        if not os.path.isdir(self.output):\n            print 'invalid output directory %s' % self.output\n            sys.exit(1)\n\n        # Create the CSV handlers\n        visitors = [\n            _CompaniesCSV(self.output),\n            _ActivitiesCSV(self.output),\n            _ActivitiesSeenCSV(self.output),\n            _QSACSV(self.output),\n        ]\n\n        # Run by each company populating the CSV files\n        for path in glob.glob(os.path.join(self.input, '*.json')):\n            with open(path, 'r') as f:\n                try:\n                    data = json.load(f, encoding='utf-8')\n                except ValueError:\n                    continue\n\n                for visitor in visitors:\n                    visitor.visit(data)", "response": "Reads data from disk and generates CSV files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess a list of simple string field definitions and assign their order based on prefix.", "response": "def process_fields(self, fields):\n\t\t\"\"\"Process a list of simple string field definitions and assign their order based on prefix.\"\"\"\n\t\t\n\t\tresult = []\n\t\tstrip = ''.join(self.PREFIX_MAP)\n\t\t\n\t\tfor field in fields:\n\t\t\tdirection = self.PREFIX_MAP['']\n\t\t\t\n\t\t\tif field[0] in self.PREFIX_MAP:\n\t\t\t\tdirection = self.PREFIX_MAP[field[0]]\n\t\t\t\tfield = field.lstrip(strip)\n\t\t\t\n\t\t\tresult.append((field, direction))\n\t\t\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates this index in the specified collection.", "response": "def create(self, collection, **kw):\n\t\t\"\"\"Create this index in the specified collection; keyword arguments are passed to PyMongo.\n\t\t\n\t\thttp://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.create_index\n\t\t\"\"\"\n\t\t\n\t\toptions = dict(\n\t\t\t\tname = self.__name__,\n\t\t\t\tunique = self.unique,\n\t\t\t\tbackground = self.background,\n\t\t\t\tsparse = self.sparse,\n\t\t\t\texpireAfterSeconds = self.expire,\n\t\t\t\tpartialFilterExpression = self.partial,\n\t\t\t\tbucketSize = self.bucket,\n\t\t\t\tmin = self.min,\n\t\t\t\tmax = self.max,\n\t\t\t)\n\t\toptions.update(kw)\n\t\t\n\t\t# Clear null options.\n\t\tfor key in list(options):\n\t\t\tif options[key] is None:\n\t\t\t\tdel options[key]\n\t\t\n\t\treturn collection.create_index(self.fields, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef search(self, **kwargs):\n\n        point = kwargs.pop('point', False)\n        if point:\n            kwargs['point'] = '%s,%s' % (point[0], point[1])\n\n        bound = kwargs.pop('bound', False)\n        if bound:\n            kwargs['bound[point1]'] = bound[0]\n            kwargs['bound[point2]'] = bound[1]\n\n        filters = kwargs.pop('filters', False)\n        if filters:\n            for k, v in filters.items():\n                kwargs['filters[%s]' % k] = v\n\n        return self._search(**kwargs)", "response": "Firms search\n            http://api. 2gis. ru / doc / firms. ru / search /\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_in_rubric(self, **kwargs):\n\n        point = kwargs.pop('point', False)\n        if point:\n            kwargs['point'] = '%s,%s' % point\n\n        bound = kwargs.pop('bound', False)\n        if bound:\n            kwargs['bound[point1]'] = bound[0]\n            kwargs['bound[point2]'] = bound[1]\n\n        filters = kwargs.pop('filters', False)\n        if filters:\n            for k, v in filters.items():\n                kwargs['filters[%s]' % k] = v\n\n        return self._search_in_rubric(**kwargs)", "response": "Firms search in rubric"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef geo_search(self, **kwargs):\n\n        if 'types' in kwargs:\n            kwargs['types'] = ','.join(kwargs['types'])\n\n        bound = kwargs.pop('bound', False)\n        if bound:\n            kwargs['bound[point1]'] = bound[0]\n            kwargs['bound[point2]'] = bound[1]\n\n        return self._geo_search(**kwargs)", "response": "Geo search\n            http://api. 2gis. ru / doc / geo / search_\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef refresh(self):\n        '''\n        Refresh the list and the screen\n        '''\n        self._screen.force_update()\n        self._screen.refresh()\n        self._update(1)", "response": "Refresh the list and the screen"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts an action in the current virtualenv.", "response": "def start(self, activity, action):\n        '''\n        Mark an action as started\n\n        :param activity: The virtualenv activity name\n        :type  activity: ``str``\n\n        :param action: The virtualenv action\n        :type  action: :class:`tox.session.Action`\n        '''\n        try:\n            self._start_action(activity, action)\n        except ValueError:\n            retox_log.debug(\"Could not find action %s in env %s\" % (activity, self.name))\n        self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping a task in the specified virtualenv.", "response": "def stop(self, activity, action):\n        '''\n        Mark a task as completed\n\n        :param activity: The virtualenv activity name\n        :type  activity: ``str``\n\n        :param action: The virtualenv action\n        :type  action: :class:`tox.session.Action`\n        '''\n        try:\n            self._remove_running_action(activity, action)\n        except ValueError:\n            retox_log.debug(\"Could not find action %s in env %s\" % (activity, self.name))\n        self._mark_action_completed(activity, action)\n        self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef finish(self, status):\n        '''\n        Move laggard tasks over\n\n        :param activity: The virtualenv status\n        :type  activity: ``str``\n        '''\n        retox_log.info(\"Completing %s with status %s\" % (self.name, status))\n        result = Screen.COLOUR_GREEN if not status else Screen.COLOUR_RED\n        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, result)\n        for item in list(self._task_view.options):\n            self._task_view.options.remove(item)\n            self._completed_view.options.append(item)\n        self.refresh()", "response": "Finish the virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the frame between jobs", "response": "def reset(self):\n        '''\n        Reset the frame between jobs\n        '''\n        self.palette['title'] = (Screen.COLOUR_WHITE, Screen.A_BOLD, Screen.COLOUR_BLUE)\n        self._completed_view.options = []\n        self._task_view.options = []\n        self.refresh()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the available kwargs of the called class", "response": "def default_arguments(cls):\n        \"\"\"Returns the available kwargs of the called class\"\"\"\n        func = cls.__init__\n        args = func.__code__.co_varnames\n        defaults = func.__defaults__\n        index = -len(defaults)\n        return {k: v for k, v in zip(args[index:], defaults)}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recreate(cls, *args, **kwargs):\n        cls.check_arguments(kwargs)\n        first_is_callable = True if any(args) and callable(args[0]) else False\n        signature = cls.default_arguments()\n        allowed_arguments = {k: v for k, v in kwargs.items() if k in signature}\n        if (any(allowed_arguments) or any(args)) and not first_is_callable:\n            if any(args) and not first_is_callable:\n                return cls(args[0], **allowed_arguments)\n            elif any(allowed_arguments):\n                return cls(**allowed_arguments)\n\n        return cls.instances[-1] if any(cls.instances) else cls()", "response": "Recreate the class based in your args multiple uses"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nputting warnings of arguments whose can t be handled by the class", "response": "def check_arguments(cls, passed):\n        \"\"\"Put warnings of arguments whose can't be handle by the class\"\"\"\n        defaults = list(cls.default_arguments().keys())\n        template = (\"Pass arg {argument:!r} in {cname:!r}, can be a typo? \"\n                    \"Supported key arguments: {defaults}\")\n        fails = []\n        for arg in passed:\n            if arg not in defaults:\n                warn(template.format(argument=arg,\n                                     cname=cls.__name__,\n                                     defaults=defaults))\n                fails.append(arg)\n\n        return any(fails)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef process(self, data, type, history):\n        if type in history:\n            return\n        if type.enum():\n            return\n        history.append(type)\n        resolved = type.resolve()\n        value = None\n        if type.multi_occurrence():\n            value = []\n        else:\n            if len(resolved) > 0:\n                if resolved.mixed():\n                    value = Factory.property(resolved.name)\n                    md = value.__metadata__\n                    md.sxtype = resolved\n                else:\n                    value = Factory.object(resolved.name)\n                    md = value.__metadata__\n                    md.sxtype = resolved\n                    md.ordering = self.ordering(resolved)\n        setattr(data, type.name, value)\n        if value is not None:\n            data = value\n        if not isinstance(data, list):\n            self.add_attributes(data, resolved)\n            for child, ancestry in resolved.children():\n                if self.skip_child(child, ancestry):\n                    continue\n                self.process(data, child, history[:])", "response": "process the specified type then process its children"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget whether or not to skip the specified child", "response": "def skip_child(self, child, ancestry):\n        \"\"\" get whether or not to skip the specified child \"\"\"\n        if child.any(): return True\n        for x in ancestry:\n            if x.choice():\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsignal endpoint that actually sends knocks whenever an instance is created or saved.", "response": "def notify_items(**kwargs):\n    \"\"\"\n    Signal endpoint that actually sends knocks whenever an instance is created / saved\n    \"\"\"\n    instance = kwargs.get('instance')\n    created = kwargs.get('created', False)\n    if hasattr(instance, 'send_knock') and active_knocks(instance):\n        try:\n            # This is a stupid generic interface for multilanguage models (hvad / parler)\n            if hasattr(instance, 'get_available_languages'):\n                langs = instance.get_available_languages()\n            else:\n                langs = [get_language()]\n            for lang in langs:\n                with override(lang):\n                    instance.send_knock(created)\n            return True\n        except AttributeError:  # pragma: no cover\n            pass\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks whether knocks are enabled for the given object", "response": "def active_knocks(obj):\n    \"\"\"\n    Checks whether knocks are enabled for the model given as argument\n\n    :param obj: model instance\n    :return True if knocks are active\n    \"\"\"\n    if not hasattr(_thread_locals, 'knock_enabled'):\n        return True\n    return _thread_locals.knock_enabled.get(obj.__class__, True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pause_knocks(obj):\n    if not hasattr(_thread_locals, 'knock_enabled'):\n        _thread_locals.knock_enabled = {}\n    obj.__class__._disconnect()\n    _thread_locals.knock_enabled[obj.__class__] = False\n    yield\n    _thread_locals.knock_enabled[obj.__class__] = True\n    obj.__class__._connect()", "response": "Context manager to pause sending knocks for the given model instance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _loopreport(self):\n        '''\n        Loop over the report progress\n        '''\n        while 1:\n            eventlet.sleep(0.2)\n            ac2popenlist = {}\n            for action in self.session._actions:\n                for popen in action._popenlist:\n                    if popen.poll() is None:\n                        lst = ac2popenlist.setdefault(action.activity, [])\n                        lst.append(popen)\n                if not action._popenlist and action in self._actionmayfinish:\n                    super(RetoxReporter, self).logaction_finish(action)\n                    self._actionmayfinish.remove(action)\n\n            self.screen.draw_next_frame(repeat=False)", "response": "Loop over the report progress and display the result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending markdown email to the local cache.", "response": "def send(email, subject=None,\n         from_email=None, to_email=None,\n         cc=None, bcc=None, reply_to=None,\n         smtp=None):\n    \"\"\"Send markdown email\n\n    Args:\n        email (str/obj): A markdown string or EmailContent object \n        subject (str): subject line\n        from_email (str): sender email address\n        to_email (str/list): recipient email addresses\n        cc (str/list): CC email addresses (string or a list)\n        bcc (str/list): BCC email addresses (string or a list)\n        reply_to (str): Reply-to email address\n        smtp (dict): SMTP configuration (dict)\n\n    Schema of smtp dict:\n        host (str): SMTP server host. Default: localhost\n        port (int): SMTP server port. Default: 25\n        tls (bool): Use TLS. Default: False\n        ssl (bool): Use SSL. Default: False\n        user (bool): SMTP login user. Default empty\n        password (bool): SMTP login password. Default empty\n    \"\"\"\n    if is_string(email):\n        email = EmailContent(email)\n\n    from_email = sanitize_email_address(from_email or email.headers.get('from'))\n    to_email = sanitize_email_address(to_email or email.headers.get('to'))\n    cc = sanitize_email_address(cc or email.headers.get('cc'))\n    bcc = sanitize_email_address(bcc or email.headers.get('bcc'))\n    reply_to = sanitize_email_address(reply_to or email.headers.get('reply-to'))\n\n    message_args = {\n        'html': email.html,\n        'text': email.text,\n        'subject': (subject or email.headers.get('subject', '')),\n        'mail_from': from_email,\n        'mail_to': to_email\n    }\n    if cc:\n        message_args['cc'] = cc\n    if bcc:\n        message_args['bcc'] = bcc\n    if reply_to:\n        message_args['headers'] = {'reply-to': reply_to}\n\n    message = emails.Message(**message_args)\n\n    for filename, data in email.inline_images:\n        message.attach(filename=filename, content_disposition='inline', data=data)\n\n    message.send(smtp=smtp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess timezone casting and conversion.", "response": "def _process_tz(self, dt, naive, tz):\n\t\t\"\"\"Process timezone casting and conversion.\"\"\"\n\t\t\n\t\tdef _tz(t):\n\t\t\tif t in (None, 'naive'):\n\t\t\t\treturn t\n\t\t\t\n\t\t\tif t == 'local':\n\t\t\t\tif __debug__ and not localtz:\n\t\t\t\t\traise ValueError(\"Requested conversion to local timezone, but `localtz` not installed.\")\n\t\t\t\t\n\t\t\t\tt = localtz\n\t\t\t\n\t\t\tif not isinstance(t, tzinfo):\n\t\t\t\tif __debug__ and not localtz:\n\t\t\t\t\traise ValueError(\"The `pytz` package must be installed to look up timezone: \" + repr(t))\n\t\t\t\t\n\t\t\t\tt = get_tz(t)\n\t\t\t\n\t\t\tif not hasattr(t, 'normalize') and get_tz:  # Attempt to handle non-pytz tzinfo.\n\t\t\t\tt = get_tz(t.tzname(dt))\n\t\t\t\n\t\t\treturn t\n\t\t\n\t\tnaive = _tz(naive)\n\t\ttz = _tz(tz)\n\t\t\n\t\tif not dt.tzinfo and naive:\n\t\t\tif hasattr(naive, 'localize'):\n\t\t\t\tdt = naive.localize(dt)\n\t\t\telse:\n\t\t\t\tdt = dt.replace(tzinfo=naive)\n\t\t\n\t\tif not tz:\n\t\t\treturn dt\n\t\t\n\t\tif hasattr(tz, 'normalize'):\n\t\t\tdt = tz.normalize(dt.astimezone(tz))\n\t\telif tz == 'naive':\n\t\t\tdt = dt.replace(tzinfo=None)\n\t\telse:\n\t\t\tdt = dt.astimezone(tz)  # Warning: this might not always be entirely correct!\n\t\t\n\t\treturn dt"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntrigger assignment of default values.", "response": "def _prepare_defaults(self):\n\t\t\"\"\"Trigger assignment of default values.\"\"\"\n\t\t\n\t\tfor name, field in self.__fields__.items():\n\t\t\tif field.assign:\n\t\t\t\tgetattr(self, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_mongo(cls, doc):\n\t\t\n\t\tif doc is None:  # To support simplified iterative use, None should return None.\n\t\t\treturn None\n\t\t\n\t\tif isinstance(doc, Document):  # No need to perform processing on existing Document instances.\n\t\t\treturn doc\n\t\t\n\t\tif cls.__type_store__ and cls.__type_store__ in doc:  # Instantiate specific class mentioned in the data.\n\t\t\tcls = load(doc[cls.__type_store__], 'marrow.mongo.document')\n\t\t\n\t\t# Prepare a new instance in such a way that changes to the instance will be reflected in the originating doc.\n\t\tinstance = cls(_prepare_defaults=False)  # Construct an instance, but delay default value processing.\n\t\tinstance.__data__ = doc  # I am Popeye of Borg (pattern); you will be askimilgrated.\n\t\tinstance._prepare_defaults()  # pylint:disable=protected-access -- deferred default value processing.\n\t\t\n\t\treturn instance", "response": "Convert data coming in from the MongoDB wire driver into a Document instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pop(self, name, default=SENTINEL):\n\t\t\n\t\tif default is SENTINEL:\n\t\t\treturn self.__data__.pop(name)\n\t\t\n\t\treturn self.__data__.pop(name, default)", "response": "Retrieve and remove a value from the backing store optionally with a default."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _op(self, operation, other, *allowed):\n\t\t\n\t\tf = self._field\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining,\n\t\t\t\t\t(q._op(operation, other, *allowed) for q in f))  # pylint:disable=protected-access\n\t\t\n\t\t# Optimize this away in production; diagnosic aide.\n\t\tif __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover\n\t\t\traise NotImplementedError(\"{self!r} does not allow {op} comparison.\".format(self=self, op=operation))\n\t\t\n\t\tif other is not None:\n\t\t\tother = f.transformer.foreign(other, (f, self._document))\n\t\t\n\t\treturn Filter({self._name: {operation: other}})", "response": "A basic operation operating on a single value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _iop(self, operation, other, *allowed):\n\t\t\n\t\tf = self._field\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining,\n\t\t\t\t\t(q._iop(operation, other, *allowed) for q in f))  # pylint:disable=protected-access\n\t\t\n\t\t# Optimize this away in production; diagnosic aide.\n\t\tif __debug__ and _complex_safety_check(f, {operation} | set(allowed)):  # pragma: no cover\n\t\t\traise NotImplementedError(\"{self!r} does not allow {op} comparison.\".format(\n\t\t\t\t\tself=self, op=operation))\n\t\t\n\t\tdef _t(o):\n\t\t\tfor value in o:\n\t\t\t\tyield None if value is None else f.transformer.foreign(value, (f, self._document))\n\t\t\n\t\tother = other if len(other) > 1 else other[0]\n\t\tvalues = list(_t(other))\n\t\t\n\t\treturn Filter({self._name: {operation: values}})", "response": "An iterative operation operating on multiple values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef S(self):\n\t\t\n\t\tif self._combining:\n\t\t\traise TypeError(\"Unable to dereference after combining fields.\")\n\t\t\n\t\tinstance = self.__class__(self._document, self._field)\n\t\tinstance._name = self._name + '.' + '$'  # pylint:disable=protected-access\n\t\treturn instance", "response": "Return a new object that is a single match of the array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmatch string values against a regular expression compiled of individual parts.", "response": "def re(self, *parts):\n\t\t\"\"\"Matches string values against a regular expression compiled of individual parts.\n\t\t\n\t\t\tDocument.field.re(r'^', variable_part, r'\\\\.')\n\t\t\n\t\tRegex operator: {$regex: value}\n\t\tDocumentation: https://docs.mongodb.com/manual/reference/operator/query/regex/\n\t\t\"\"\"\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining, (q.re(*parts) for q in self._field))\n\t\t\n\t\treturn Filter({self._name: {'$regex': ''.join(parts)}})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match(self, q):\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining, (qp.match(q) for qp in self._field))\n\t\t\n\t\t# Optimize this away in production; diagnosic aide.\n\t\tif __debug__ and _complex_safety_check(self._field, {'$elemMatch', '#document'}):  # pragma: no cover\n\t\t\traise NotImplementedError(\"{self!r} does not allow $elemMatch comparison.\".format(self=self))\n\t\t\n\t\tif hasattr(q, 'as_query'):\n\t\t\tq = q.as_query\n\t\t\n\t\treturn Filter({self._name: {'$elemMatch': q}})", "response": "Selects documents if element in the array field matches all the specified conditions."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmatches values that are between a minimum and maximum value semi - inclusive.", "response": "def range(self, gte, lt):\n\t\t\"\"\"Matches values that are between a minimum and maximum value, semi-inclusive.\n\t\t\n\t\t\tDocument.field.range(4, 12)\n\t\t\n\t\tThis will find documents with a field whose value is greater than or equal to 4, and less than 12.\n\t\t\n\t\tComparison operator: {$gte: gte, $lt: lt}\n\t\t\"\"\"\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\tprint(\"Combining\", self._combining, self._field)\n\t\t\treturn reduce(self._combining, (q.range(gte, lt) for q in self._field))\n\t\t\n\t\t# Optimize this away in production; diagnosic aide.\n\t\tif __debug__ and _simple_safety_check(self._field, '$eq'):  # pragma: no cover\n\t\t\traise NotImplementedError(\"{self!r} does not allow range comparison.\".format(self=self))\n\t\t\n\t\treturn (self >= gte) & (self < lt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nselects documents if the array field is a specified size.", "response": "def size(self, value):\n\t\t\"\"\"Selects documents if the array field is a specified size.\n\t\t\n\t\t\tDocument.field.size(5)\n\t\t\n\t\tArray operator: {$size: value}\n\t\tDocumentation: https://docs.mongodb.org/manual/reference/operator/query/size/#op._S_size\n\t\t\"\"\"\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining, (q.size(value) for q in self._field))\n\t\t\n\t\t# Optimize this away in production; diagnosic aide.\n\t\tif __debug__ and _complex_safety_check(self._field, {'$size', '#array'}):  # pragma: no cover\n\t\t\traise NotImplementedError(\"{self!r} does not allow $size comparison.\".format(self=self))\n\t\t\n\t\treturn Filter({self._name: {'$size': int(value)}})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef of_type(self, *kinds):\n\t\t\n\t\tif self._combining:  # We are a field-compound query fragment, e.g. (Foo.bar & Foo.baz).\n\t\t\treturn reduce(self._combining, (q.of_type(*kinds) for q in self._field))\n\t\t\n\t\tforeign = set(kinds) if kinds else self._field.__foreign__\n\t\t\n\t\tif not foreign:\n\t\t\treturn Filter()\n\t\t\n\t\tif len(foreign) == 1:  # Simplify if the value is singular.\n\t\t\tforeign, = foreign  # Unpack.\n\t\t\n\t\treturn Filter({self._name: {'$type': foreign}})", "response": "Selects documents if a field is of the correct type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef near(self, center, sphere=False, min=None, max=None):\n\t\t\n\t\tfrom marrow.mongo.geo import Point\n\t\t\n\t\tnear = {'$geometry': Point(*center)}\n\t\t\n\t\tif min:\n\t\t\tnear['$minDistance'] = float(min)\n\t\t\n\t\tif max:\n\t\t\tnear['$maxDistance'] = float(max)\n\t\t\n\t\treturn Filter({self._name: {'$nearSphere' if sphere else '$near': near}})", "response": "Return a new filter that filters results by their distance from the given point optionally with range limits in meters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef within(self, geometry=None, center=None, sphere=None, radius=None, box=None, polygon=None, crs=None):\n\t\t\n\t\tif geometry:\n\t\t\tif crs:\n\t\t\t\tgeometry = dict(geometry)\n\t\t\t\tgeometry['crs'] = {'type': 'name', 'properties': {'name': crs}}\n\t\t\t\n\t\t\tinner = {'$geometry': geometry}\n\t\t\n\t\telif center:\n\t\t\tinner = {'$center': [list(center), radius]}\n\t\t\n\t\telif sphere:\n\t\t\tinner = {'$centerSphere': [list(sphere), radius]}\n\t\t\n\t\telif box:\n\t\t\tinner = {'$box': list(list(i) for i in box)}\n\t\t\n\t\telif polygon:\n\t\t\tinner = {'$polygon': list(list(i) for i in polygon)}\n\t\t\n\t\telse:\n\t\t\traise TypeError(\"Requires at least one argument.\")\n\t\t\n\t\treturn Filter({self._name: {'$geoWithin': inner}})", "response": "Select geometries within a bounding GeoJSON geometry."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nselects geometries that intersect with a GeoJSON geometry.", "response": "def intersects(self, geometry, crs=None):\n\t\t\"\"\"Select geometries that intersect with a GeoJSON geometry.\n\t\t\n\t\tGeospatial operator: {$geoIntersects: {...}}\n\t\tDocumentation: https://docs.mongodb.com/manual/reference/operator/query/geoIntersects/#op._S_geoIntersects\n\t\t\n\t\t\t{\n\t\t\t\t$geoIntersects: { $geometry: <geometry; a GeoJSON object> }\n\t\t\t}\n\t\t\"\"\"\n\t\t\n\t\tif crs:\n\t\t\tgeometry = dict(geometry)\n\t\t\tgeometry['crs'] = {'type': 'name', 'properties': {'name': crs}}\n\t\t\n\t\treturn Filter({self._name: {'$geoIntersects': {'$geometry': geometry}}})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symbol(self, ident, bp=0):\n        '''\n        Gets (and create if not exists) as named symbol.\n\n        Optionally, you can specify a binding power (bp) value, which will be used\n        to control operator presedence; the higher the value, the tighter a token\n        binds to the tokens that follow.\n        '''\n        try:\n            s = self.symbols[ident]\n        except KeyError:\n            class s(SymbolBase):\n                pass\n            s.__name__ = 'symbol-%s' % (ident,)\n            s.ident = ident\n            s.lbp = bp\n            self.symbols[ident] = s\n        else:\n            s.lbp = max(bp, s.lbp)\n        return s", "response": "Gets and creates a new symbol with the given identifier."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef method(self, symbol):\n        '''\n        Symbol decorator.\n        '''\n        assert issubclass(symbol, SymbolBase)\n        def wrapped(fn):\n            setattr(symbol, fn.__name__, fn)\n        return wrapped", "response": "A decorator that adds a method to the specified symbol."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _simpleparsefun(date):\r\n    if hasattr(date, 'year'):\r\n        return date\r\n    try:\r\n        date = datetime.datetime.strptime(date, '%Y-%m-%d')\r\n    except ValueError:\r\n        date = datetime.datetime.strptime(date, '%Y-%m-%d %H:%M:%S')\r\n    return date", "response": "Simple date parsing function"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a given date is a work date ignoring holidays.", "response": "def isworkday(self, date):\r\n        \"\"\"\r\n        Check if a given date is a work date, ignoring holidays.\r\n\r\n        Args:\r\n            date (date, datetime or str): Date to be checked.\r\n\r\n        Returns:\r\n            bool: True if the date is a work date, False otherwise.\r\n        \"\"\"\r\n        date = parsefun(date)\r\n        return self.weekdaymap[date.weekday()].isworkday"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef isholiday(self, date):\r\n        date = parsefun(date)\r\n        if self.holidays:\r\n            # i is the index of first holiday >= date\r\n            i = bisect.bisect_left(self.holidays, date)\r\n            if i == 0 and date < self.holidays[0]:\r\n                warn('Holiday list exhausted at start, ' \\\r\n                     'isholiday(%s) output may be incorrect.' % date)\r\n            elif i == len(self.holidays):\r\n                warn('Holiday list exhausted at end, ' \\\r\n                     'isholiday(%s) output may be incorrect.' % date)\r\n            elif self.holidays[i] == date:\r\n                return True\r\n        return False", "response": "Check if a given date is a holiday."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef adjust(self, date, mode):\r\n        date = parsefun(date)\r\n        if self.isbusday(date):\r\n            return date\r\n\r\n        if mode == FOLLOWING:\r\n            dateadj = self.addbusdays(date, 1)\r\n        elif mode == PREVIOUS:\r\n            dateadj = self.addbusdays(date, -1)\r\n        elif mode == MODIFIEDFOLLOWING:\r\n            dateadj = self.addbusdays(date, 1)\r\n            if dateadj.month != date.month:\r\n                dateadj = self.addbusdays(dateadj, -1)\r\n        else:\r\n            raise ValueError('Invalid mode %s' % mode)\r\n\r\n        return dateadj", "response": "Adjust the date to the closest work date."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the new date with the addition of work days.", "response": "def addworkdays(self, date, offset):\r\n        \"\"\"\r\n        Add work days to a given date, ignoring holidays.\r\n\r\n        Note:\r\n            By definition, a zero offset causes the function to return the\r\n            initial date, even it is not a work date. An offset of 1\r\n            represents the next work date, regardless of date being a work\r\n            date or not.\r\n\r\n        Args:\r\n            date (date, datetime or str): Date to be incremented.\r\n            offset (integer): Number of work days to add. Positive values move\r\n                the date forward and negative values move the date back.\r\n\r\n        Returns:\r\n            datetime: New incremented date.\r\n        \"\"\"\r\n        date = parsefun(date)\r\n        if offset == 0:\r\n            return date\r\n\r\n        if offset > 0:\r\n            direction = 1\r\n            idx_offset = Calendar._idx_offsetnext\r\n            idx_next = Calendar._idx_nextworkday\r\n            idx_offset_other = Calendar._idx_offsetprev\r\n            idx_next_other = Calendar._idx_prevworkday\r\n        else:\r\n            direction = -1\r\n            idx_offset = Calendar._idx_offsetprev\r\n            idx_next = Calendar._idx_prevworkday\r\n            idx_offset_other = Calendar._idx_offsetnext\r\n            idx_next_other = Calendar._idx_nextworkday\r\n\r\n        # adjust date to first work day before/after so counting always\r\n        # starts from a workday\r\n        weekdaymap = self.weekdaymap # speed up\r\n        datewk = date.weekday()\r\n        if not weekdaymap[datewk].isworkday:\r\n            date += datetime.timedelta(days=\\\r\n                                        weekdaymap[datewk][idx_offset_other])\r\n            datewk = weekdaymap[datewk][idx_next_other]\r\n\r\n        nw, nd = divmod(abs(offset), len(self.workdays))\r\n        ndays = nw * 7\r\n        while nd > 0:\r\n            ndays += abs(weekdaymap[datewk][idx_offset])\r\n            datewk = weekdaymap[datewk][idx_next]\r\n            nd -= 1\r\n\r\n        date += datetime.timedelta(days=ndays*direction)\r\n        return date"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef addbusdays(self, date, offset):\r\n        date = parsefun(date)\r\n        if offset == 0:\r\n            return date\r\n\r\n        dateoffset = self.addworkdays(date, offset)\r\n        holidays = self.holidays # speed up\r\n        if not holidays:\r\n            return dateoffset\r\n\r\n        weekdaymap = self.weekdaymap # speed up\r\n        datewk = dateoffset.weekday()\r\n        if offset > 0:\r\n            # i is the index of first holiday > date\r\n            # we don't care if the start date is a holiday\r\n            i = bisect.bisect_right(holidays, date)\r\n            if i == len(holidays):\r\n                warn('Holiday list exhausted at end, ' \\\r\n                     'addbusday(%s,%s) output may be incorrect.' % \\\r\n                     (date, offset))\r\n            else:\r\n                while holidays[i] <= dateoffset:\r\n                    dateoffset += datetime.timedelta(days=\\\r\n                                                weekdaymap[datewk].offsetnext)\r\n                    datewk = weekdaymap[datewk].nextworkday\r\n                    i += 1\r\n                    if i == len(holidays):\r\n                        warn('Holiday list exhausted at end, ' \\\r\n                             'addbusday(%s,%s) output may be incorrect.' % \\\r\n                             (date, offset))\r\n                        break\r\n        else:\r\n            # i is the index of first holiday >= date\r\n            # we don't care if the start date is a holiday\r\n            i = bisect.bisect_left(holidays, date) - 1\r\n            if i == -1:\r\n                warn('Holiday list exhausted at start, ' \\\r\n                     'addbusday(%s,%s) output may be incorrect.' \\\r\n                     % (date, offset))\r\n            else:\r\n                while holidays[i] >= dateoffset:\r\n                    dateoffset += datetime.timedelta(days=\\\r\n                                                weekdaymap[datewk].offsetprev)\r\n                    datewk = weekdaymap[datewk].prevworkday\r\n                    i -= 1\r\n                    if i == -1:\r\n                        warn('Holiday list exhausted at start, ' \\\r\n                             'addbusday(%s,%s) output may be incorrect.' % \\\r\n                             (date, offset))\r\n                        break\r\n\r\n        return dateoffset", "response": "Add business days to a given date taking holidays into consideration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _workdaycount(self, date1, date2):\r\n        assert date2 >= date1\r\n        date1wd = date1.weekday()\r\n        date2wd = date2.weekday()\r\n        if not self.weekdaymap[date2wd].isworkday:\r\n            date2 += datetime.timedelta(days=self.weekdaymap[date2wd].offsetprev)\r\n            date2wd = self.weekdaymap[date2wd].prevworkday\r\n        if date2 <= date1:\r\n            return 0\r\n        \r\n        nw, nd = divmod((date2 - date1).days, 7)\r\n        ndays = nw * len(self.workdays)\r\n        if nd > 0:\r\n            date1wd = date1.weekday()\r\n            date2wd = date2.weekday()\r\n            while date1wd != date2wd:\r\n                ndays += 1\r\n                date1wd = self.weekdaymap[date1wd].nextworkday\r\n        return ndays", "response": "Count the number of work days between two dates ignoring holidays."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the number of work days between two dates ignoring holidays.", "response": "def workdaycount(self, date1, date2):\r\n        \"\"\"\r\n        Count work days between two dates, ignoring holidays.\r\n\r\n        Args:\r\n            date1 (date, datetime or str): Date start of interval.\r\n            date2 (date, datetime or str): Date end of interval.\r\n\r\n        Note:\r\n            The adopted notation is COB to COB, so effectively date1 is not\r\n            included in the calculation result.\r\n\r\n        Example:\r\n            >>> cal = Calendar()\r\n            >>> date1 = datetime.datetime.today()\r\n            >>> date2 = cal.addworkdays(date1, 1)\r\n            >>> cal.workdaycount(date1, date2)\r\n            1\r\n\r\n        Returns:\r\n            int: Number of work days between the two dates. If the dates\r\n                are equal the result is zero. If date1 > date2 the result is\r\n                negative.\r\n        \"\"\"\r\n        date1 = parsefun(date1)\r\n        date2 = parsefun(date2)\r\n        if date1 == date2:\r\n            return 0\r\n        elif date1 > date2:\r\n            date1, date2 = date2, date1\r\n            direction = -1\r\n        else:\r\n            direction = 1\r\n\r\n        ndays = self._workdaycount(date1, date2)\r\n        return ndays * direction"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the number of business days between two dates.", "response": "def busdaycount(self, date1, date2):\r\n        \"\"\"\r\n        Count business days between two dates (private), taking holidays into\r\n        consideration.\r\n\r\n        Args:\r\n            date1 (date, datetime or str): Date start of interval.\r\n            date2 (date, datetime or str): Date end of interval.\r\n\r\n        Note:\r\n            The adopted notation is COB to COB, so effectively date1 is not\r\n            included in the calculation result.\r\n\r\n        Example:\r\n            >>> cal = Calendar()\r\n            >>> date1 = datetime.datetime.today()\r\n            >>> date2 = cal.addbusdays(date1, 1)\r\n            >>> cal.busdaycount(date1, date2)\r\n            1\r\n\r\n        Returns:\r\n            int: Number of business days between the two dates. If the dates\r\n                are equal the result is zero. If date1 > date2 the result is\r\n                negative.\r\n        \"\"\"\r\n        date1 = parsefun(date1)\r\n        date2 = parsefun(date2)\r\n        if date1 == date2:\r\n            return 0\r\n        elif date1 > date2:\r\n            date1, date2 = date2, date1\r\n            direction = -1\r\n        else:\r\n            direction = 1\r\n\r\n        ndays = self._workdaycount(date1, date2)\r\n\r\n        if self.holidays:\r\n            holidays = self.holidays # speed up\r\n            if date1 > holidays[-1]:\r\n                warn('Holiday list exhausted at end, ' \\\r\n                     'busdaycount(%s,%s) output may be incorrect.' % \\\r\n                     (date1, date2))\r\n            elif date2 < holidays[0]:\r\n                warn('Holiday list exhausted at start, ' \\\r\n                     'busdaycount(%s,%s) output may be incorrect.' % \\\r\n                     (date1, date2))\r\n            else:\r\n                if date1 < holidays[0]:\r\n                    warn('Holiday list exhausted at start, ' \\\r\n                         'busdaycount(%s,%s) output may be incorrect.' % \\\r\n                         (date1, date2))\r\n                if date2 > holidays[-1]:\r\n                    warn('Holiday list exhausted at end, ' \\\r\n                         'busdaycount(%s,%s) output may be incorrect.' % \\\r\n                         (date1, date2))\r\n                # i is the index of first holiday > date\r\n                # we don't care if the start date is a holiday\r\n                i = bisect.bisect_right(holidays, date1)\r\n                while holidays[i] <= date2:\r\n                    ndays -= 1\r\n                    i += 1\r\n                    if i == len(holidays):\r\n                        break\r\n\r\n        return ndays * direction"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef caleom(date):\r\n        date = parsefun(date)\r\n        date += datetime.timedelta(days=32-date.day)\r\n        date -= datetime.timedelta(days=date.day)\r\n        return date", "response": "Adjust date to last 32 days regardless of work days."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef range(self, date1, date2):\r\n        date1 = self.adjust(parsefun(date1), FOLLOWING)\r\n        date2 = parsefun(date2)\r\n\r\n        holidays = []\r\n        holidx = 0\r\n        if len(self.holidays):\r\n            index1 = bisect.bisect_left(self.holidays, date1)\r\n            index2 = bisect.bisect_left(self.holidays, date2)\r\n            if index2 > index1:\r\n                holidays = self.holidays[index1:index2]\r\n\r\n        datewk = date1.weekday()\r\n        while date1 < date2:\r\n            if (holidx < len(holidays)) and (holidays[holidx] == date1):\r\n                holidx += 1\r\n            else:\r\n                yield date1\r\n            date1 += datetime.timedelta(days=\\\r\n                                        self.weekdaymap[datewk].offsetnext)\r\n            datewk = self.weekdaymap[datewk].nextworkday", "response": "Returns a generator that yields business days between two dates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _connect(cls):\n        post_save.connect(\n            notify_items, sender=cls,\n            dispatch_uid='knocker_{0}'.format(cls.__name__)\n        )", "response": "Connect signal to current model\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndisconnecting signal from current model", "response": "def _disconnect(cls):\n        \"\"\"\n        Disconnect signal from current model\n        \"\"\"\n        post_save.disconnect(\n            notify_items, sender=cls,\n            dispatch_uid='knocker_{0}'.format(cls.__name__)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_knock(self, created=False):\n        knock = {}\n        if self.should_knock(created):\n            for field, data in self._retrieve_data(None, self._knocker_data):\n                knock[field] = data\n        return knock", "response": "Returns a dictionary with the knock data built from _knocker_data\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend the knock in the associated channels Group", "response": "def send_knock(self, created=False):\n        \"\"\"\n        Send the knock in the associated channels Group\n        \"\"\"\n        knock = self.as_knock(created)\n        if knock:\n            gr = Group('knocker-{0}'.format(knock['language']))\n            gr.send({'text': json.dumps(knock)})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef colorize(printable, color, style='normal', autoreset=True):\n    if not COLORED:  # disable color\n        return printable\n    if color not in COLOR_MAP:\n        raise RuntimeError('invalid color set, no {}'.format(color))\n\n    return '{color}{printable}{reset}'.format(\n        printable=printable,\n        color=COLOR_MAP[color].format(style=STYLE_MAP[style]),\n        reset=COLOR_MAP['reset'] if autoreset else ''\n    )", "response": "Colorize some message with ANSI colors"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning cache Summary: Decorator used to cache the input->output Examples: An fib memoized executes at O(1) time instead O(e^n) Attributes: @param (function): function Returns: wrapped function TODO: Give support to functions with kwargs", "response": "def cache(function):\n    \"\"\"\n    Function: cache\n    Summary: Decorator used to cache the input->output\n    Examples: An fib memoized executes at O(1) time\n              instead O(e^n)\n    Attributes:\n        @param (function): function\n    Returns: wrapped function\n\n    TODO: Give support to functions with kwargs\n    \"\"\"\n\n    memory = {}\n    miss = object()\n\n    @wraps(function)\n    def _wrapper(*args):\n        result = memory.get(args, miss)\n        if result is miss:\n            _wrapper.call += 1\n            result = function(*args)\n            memory[args] = result\n        return result\n    _wrapper.call = 0\n    return _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef color(string, status=True, warning=False, bold=True):\n    attr = []\n    if status:\n        # green\n        attr.append('32')\n    if warning:\n        # red\n        attr.append('31')\n    if bold:\n        attr.append('1')\n    return '\\x1b[%sm%s\\x1b[0m' % (';'.join(attr), string)", "response": "Change text color for the linux terminal defaults to green."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npatches pymongo s Collection object to add a tail method.", "response": "def _patch():\n\t\"\"\"Patch pymongo's Collection object to add a tail method.\n\t\n\tWhile not nessicarily recommended, you can use this to inject `tail` as a method into Collection, making it\n\tgenerally accessible.\n\t\"\"\"\n\t\n\tif not __debug__:  # pragma: no cover\n\t\timport warnings\n\t\twarnings.warn(\"A catgirl has died.\", ImportWarning)\n\t\n\tfrom pymongo.collection import Collection\n\tCollection.tail = tail"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_query(cls, mapping, valid, *args, **kw):\n\t\t\n\t\tcollection = cls.get_collection(kw.pop('source', None))\n\t\tquery = Filter(document=cls, collection=collection)\n\t\toptions = {}\n\t\t\n\t\tif args:\n\t\t\tquery &= reduce(and_, args)\n\t\t\n\t\t# Gather any valid options.\n\t\tfor key in tuple(kw):\n\t\t\tname = mapping.get(key, key)\n\t\t\t\n\t\t\tif name in valid:\n\t\t\t\toptions[name] = kw.pop(key)\n\t\t\n\t\t# Support parametric projection via the use of iterables of strings in the form 'field' or '-field',\n\t\t# with name resolution. See the documentation for P for details.\n\t\tif 'projection' in options and not isinstance(options['projection'], Mapping):\n\t\t\toptions['projection'] = P(cls, *options['projection'])\n\t\t\n\t\t# Support parametric sorting via the use of iterables of strings. See the documentation for S for details.\n\t\tif 'sort' in options:\n\t\t\toptions['sort'] = S(cls, *options['sort'])\n\t\t\n\t\tif kw:  # Remainder are parametric query fragments.\n\t\t\tquery &= F(cls, **kw)\n\t\t\n\t\treturn cls, collection, query, options", "response": "Process arguments to query methods. For internal use only.\n\t\t\n\t\tPositional arguments are treated as query components, combined using boolean AND reduction.\n\t\t\n\t\tKeyword arguments are processed depending on the passed in mapping and set of valid options, with non-\n\t\toption arguments treated as parametric query components, also ANDed with any positionally passed query\n\t\tcomponents.\n\t\t\n\t\tParametric querying with explicit `__eq` against these \"reserved words\" is possible to work around their\n\t\treserved-ness.\n\t\t\n\t\tQuerying options for find and aggregate may differ in use of under_score or camelCase formatting; this\n\t\thelper removes the distinction and allows either."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _prepare_find(cls, *args, **kw):\n\t\t\n\t\tcls, collection, query, options = cls._prepare_query(\n\t\t\t\tcls.FIND_MAPPING,\n\t\t\t\tcls.FIND_OPTIONS,\n\t\t\t\t*args,\n\t\t\t\t**kw\n\t\t\t)\n\t\t\n\t\tif 'await' in options:\n\t\t\traise TypeError(\"Await is hard-deprecated as reserved keyword in Python 3.7, use wait instead.\")\n\t\t\n\t\tif 'cursor_type' in options and {'tail', 'wait'} & set(options):\n\t\t\traise TypeError(\"Can not combine cursor_type and tail/wait arguments.\")\n\t\t\n\t\telif options.pop('tail', False):\n\t\t\toptions['cursor_type'] = CursorType.TAILABLE_AWAIT if options.pop('wait', True) else CursorType.TAILABLE\n\t\t\n\t\telif 'wait' in options:\n\t\t\traise TypeError(\"Wait option only applies to tailing cursors.\")\n\t\t\n\t\tmodifiers = options.get('modifiers', dict())\n\t\t\n\t\tif 'max_time_ms' in options:\n\t\t\tmodifiers['$maxTimeMS'] = options.pop('max_time_ms')\n\t\t\n\t\tif modifiers:\n\t\t\toptions['modifiers'] = modifiers\n\t\t\n\t\treturn cls, collection, query, options", "response": "Execute a find and return the resulting queryset using combined plain and parametric query generation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates and execute an aggregate query pipline using combined plain and parametric query generation.", "response": "def _prepare_aggregate(cls, *args, **kw):\n\t\t\"\"\"Generate and execute an aggregate query pipline using combined plain and parametric query generation.\n\t\t\n\t\tAdditionally, performs argument case normalization, refer to the `_prepare_query` method's docstring.\n\t\t\n\t\tThis provides a find-like interface for generating aggregate pipelines with a few shortcuts that make\n\t\taggregates behave more like \"find, optionally with more steps\". Positional arguments that are not Filter\n\t\tinstances are assumed to be aggregate pipeline stages.\n\t\t\n\t\thttps://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.aggregate\n\t\t\"\"\"\n\t\t\n\t\tstages = []\n\t\tstage_args = []\n\t\tfragments = []\n\t\t\n\t\tfor arg in args:  # Split the positional arguments into filter fragments and projection stages.\n\t\t\t(fragments if isinstance(arg, Filter) else stage_args).append(arg)\n\t\t\n\t\tcls, collection, query, options = cls._prepare_query(\n\t\t\t\tcls.AGGREGATE_MAPPING,\n\t\t\t\tcls.AGGREGATE_OPTIONS,\n\t\t\t\t*fragments,\n\t\t\t\t**kw\n\t\t\t)\n\t\t\n\t\tif query:\n\t\t\tstages.append({'$match': query})\n\t\t\n\t\tstages.extend(stage_args)\n\t\t\n\t\tif 'sort' in options:  # Convert the find-like option to a stage with the correct semantics.\n\t\t\tstages.append({'$sort': odict(options.pop('sort'))})\n\t\t\n\t\tif 'skip' in options:  # Note: Sort + limit memory optimization invalidated when skipping.\n\t\t\tstages.append({'$skip': options.pop('skip')})\n\t\t\n\t\tif 'limit' in options:\n\t\t\tstages.append({'$limit': options.pop('limit')})\n\t\t\n\t\tif 'projection' in options:\n\t\t\tstages.append({'$project': options.pop('projection')})\n\t\t\n\t\treturn cls, collection, stages, options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nquery the collection this class is bound to.", "response": "def find(cls, *args, **kw):\n\t\t\"\"\"Query the collection this class is bound to.\n\t\t\n\t\tAdditional arguments are processed according to `_prepare_find` prior to passing to PyMongo, where positional\n\t\tparameters are interpreted as query fragments, parametric keyword arguments combined, and other keyword\n\t\targuments passed along with minor transformation.\n\t\t\n\t\thttps://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find\n\t\t\"\"\"\n\t\t\n\t\tDoc, collection, query, options = cls._prepare_find(*args, **kw)\n\t\treturn collection.find(query, **options)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a single document from the collection this class is bound to.", "response": "def find_one(cls, *args, **kw):\n\t\t\"\"\"Get a single document from the collection this class is bound to.\n\t\t\n\t\tAdditional arguments are processed according to `_prepare_find` prior to passing to PyMongo, where positional\n\t\tparameters are interpreted as query fragments, parametric keyword arguments combined, and other keyword\n\t\targuments passed along with minor transformation.\n\t\t\n\t\tAutomatically calls `to_mongo` with the retrieved data.\n\t\t\n\t\thttps://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.find_one\n\t\t\"\"\"\n\t\t\n\t\tif len(args) == 1 and not isinstance(args[0], Filter):\n\t\t\targs = (getattr(cls, cls.__pk__) == args[0], )\n\t\t\n\t\tDoc, collection, query, options = cls._prepare_find(*args, **kw)\n\t\tresult = Doc.from_mongo(collection.find_one(query, **options))\n\t\t\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_in_sequence(cls, field, order, *args, **kw):\n\t\t\n\t\tfield = traverse(cls, field)\n\t\torder = list(order)  # We need to coalesce the value to prepare for multiple uses.\n\t\tkw['sort'] = {'__order': 1}\n\t\tkw.setdefault('projection', {'__order': 0})\n\t\t\n\t\tcls, collection, stages, options = cls._prepare_aggregate(\n\t\t\t\tfield.any(order),\n\t\t\t\t{'$addFields': {'__order': {'$indexOfArray': [order, '$' + ~field]}}},\n\t\t\t\t*args,\n\t\t\t\t**kw\n\t\t\t)\n\t\t\n\t\tif __debug__:  # noqa\n\t\t\t# This \"foot shot avoidance\" check requires a server round-trip, potentially, so we only do this in dev.\n\t\t\tif tuple(collection.database.client.server_info()['versionArray'][:2]) < (3, 4):  # pragma: no cover\n\t\t\t\traise RuntimeError(\"Queryable.find_in_sequence only works against MongoDB server versions 3.4 or newer.\")\n\t\t\n\t\treturn collection.aggregate(stages, **options)", "response": "Return a QuerySet iterating the results of a query in a defined order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef reload(self, *fields, **kw):\n\t\t\n\t\tDoc, collection, query, options = self._prepare_find(id=self.id, projection=fields, **kw)\n\t\tresult = collection.find_one(query, **options)\n\t\t\n\t\tif fields:  # Refresh only the requested data.\n\t\t\tfor k in result:  # TODO: Better merge algorithm.\n\t\t\t\tif k == ~Doc.id: continue\n\t\t\t\tself.__data__[k] = result[k]\n\t\telse:\n\t\t\tself.__data__ = result\n\t\t\n\t\treturn self", "response": "Reload the entire document from the database or refresh specific named top - level fields."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(cls):\n        results = {}\n\n        hierarchy = cls.__hierarchy\n        hierarchy.reverse()\n\n        for storeMethod in hierarchy:\n            cls.merger.merge(results, storeMethod.get())\n\n        return results", "response": "Get values gathered from the previously set hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting command line arguments as a source object.", "response": "def argv(cls, name, short_name=None, type=None, help=None):\n        \"\"\" Set command line arguments as a source\n\n        Parses the command line arguments described by the parameters.\n\n        Args:\n            name: the long name of the argument (foo)\n            short_name: the optional short name of the argument (f)\n            type: the optional type of the argument, defaults to bool\n            help: the optional help text for the argument\n        \"\"\"\n        cls.__hierarchy.append(argv.Argv(name, short_name, type, help))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset environment variables as a source.", "response": "def env(cls, separator=None, match=None, whitelist=None, parse_values=None, to_lower=None, convert_underscores=None):\n        \"\"\"Set environment variables as a source.\n\n        By default all environment variables available to the process are used.\n        This can be narrowed by the args.\n\n        Args:\n            separator: Keys are split along this character, the resulting\n                splits are considered nested values.\n            match: Regular expression for key matching. Keys matching the\n                expression are considered whitelisted.\n            whitelist: Only use environment variables that are listed in this\n                list.\n            parse_values: Try to parse all variable for well-known types.\n            to_lower: Convert all variable names to lower case.\n            convert_underscores: Convert all underscores in the name to dashes,\n                this takes place after separation via the separator option.\n        \"\"\"\n        cls.__hierarchy.append(env.Env(separator, match, whitelist, parse_values, to_lower, convert_underscores))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef file(cls, path, encoding=None, parser=None):\n        cls.__hierarchy.append(file.File(path, encoding, parser))", "response": "Set a file as a source."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef P(Document, *fields, **kw):\n\t\n\t__always__ = kw.pop('__always__', set())\n\tprojected = set()\n\tomitted = set()\n\t\n\tfor field in fields:\n\t\tif field[0] in ('-', '!'):\n\t\t\tomitted.add(field[1:])\n\t\telif field[0] == '+':\n\t\t\tprojected.add(field[1:])\n\t\telse:\n\t\t\tprojected.add(field)\n\t\n\tif not projected:  # We only have exclusions from the default projection.\n\t\tnames = set(getattr(Document, '__projection__', Document.__fields__) or Document.__fields__)\n\t\tprojected = {name for name in (names - omitted)}\n\t\n\tprojected |= __always__\n\t\n\tif not projected:\n\t\tprojected = {'_id'}\n\t\n\treturn {unicode(traverse(Document, name, name)): True for name in projected}", "response": "Generate a MongoDB projection dictionary using the Django ORM style."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_login(session, for_what):\n    username, password = request_username_password(for_what)\n    try:\n        session.login(username, password)\n    except ClientInteractionRequest as cir:\n        params = request_interaction(cir)\n        session.continue_login(cir.login_token, **params)", "response": "This method handles the login handshake of a user on the command - line."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_valid(self, context, sid):\n\t\t\n\t\trecord = self._Document.find_one(sid, project=('expires', ))\n\t\t\n\t\tif not record:\n\t\t\treturn\n\t\t\n\t\treturn not record._expired", "response": "Identify if the given session ID is currently valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef invalidate(self, context, sid):\n\t\t\n\t\tresult = self._Document.get_collection().delete_one({'_id': sid})\n\t\t\n\t\treturn result.deleted_count == 1", "response": "Immediately expire a session from the backing store."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef persist(self, context):\n\t\t\n\t\tD = self._Document\n\t\tdocument = context.session[self.name]\n\t\t\n\t\tD.get_collection().replace_one(D.id == document.id, document, True)", "response": "Update or insert the session document into the configured collection"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ws_connect(message):\n    prefix, language = message['path'].strip('/').split('/')\n    gr = Group('knocker-{0}'.format(language))\n    gr.add(message.reply_channel)\n    message.channel_session['knocker'] = language\n    message.reply_channel.send({\"accept\": True})", "response": "This function is called when a websocket connection is established."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ws_disconnect(message):\n    language = message.channel_session['knocker']\n    gr = Group('knocker-{0}'.format(language))\n    gr.discard(message.reply_channel)", "response": "Disconnect from the client"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a shallow copy of this document.", "response": "def copy(self):\n\t\t\"\"\"Return a shallow copy.\"\"\"\n\t\treturn self.__class__(self.operations.copy(), self.collection, self.document)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_foreign(self, obj, name, value):  # pylint:disable=unused-argument\n\t\t\n\t\tif self.cache:\n\t\t\treturn self._populate_cache(value)\n\t\t\n\t\tidentifier = value\n\t\t\n\t\t# First, we handle the typcial Document object case.\n\t\tif isinstance(value, Document):\n\t\t\tidentifier = value.__data__.get('_id', None)\n\t\t\tif identifier is None:\n\t\t\t\traise ValueError(\"Can only store a reference to a saved Document instance with an `_id` stored.\")\n\t\t\n\t\telif isinstance(value, (str, unicode)) and len(value) == 24:\n\t\t\ttry:\n\t\t\t\tidentifier = OID(value)\n\t\t\texcept InvalidId:\n\t\t\t\tpass\n\t\t\n\t\tkind = self._kind(obj.__class__)\n\t\t\n\t\tif self.concrete:\n\t\t\tif isinstance(value, Document) and value.__collection__:\n\t\t\t\treturn DBRef(value.__collection__, identifier)\n\t\t\t\n\t\t\tif getattr(kind, '__collection__', None):\n\t\t\t\treturn DBRef(kind.__collection__, identifier)\n\t\t\t\n\t\t\traise ValueError(\"Could not infer collection name.\")\n\t\t\n\t\treturn identifier", "response": "Transform to a MongoDB - safe value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef space_wave(phase, amplitude=12, frequency=0.1):\n    wave = cycle(horizontal)\n    return ''.join((next(wave) for x in range\n                    (int((amplitude + 1) * abs(sin(frequency * (phase)))))))", "response": "This function is used to generate a wave - like padding - based spacement based on the variable lambda\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a new animation instance.", "response": "def start(self, autopush=True):\n        \"\"\"Start a new animation instance\"\"\"\n        if self.enabled:\n            if autopush:\n                self.push_message(self.message)\n                self.spinner.message = ' - '.join(self.animation.messages)\n            if not self.spinner.running:\n                self.animation.thread = threading.Thread(target=_spinner,\n                                                         args=(self.spinner,))\n                self.spinner.running = True\n                self.animation.thread.start()\n                sys.stdout = stream.Clean(sys.stdout, self.spinner.stream)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef stop(cls):\n        if AnimatedDecorator._enabled:\n            if cls.spinner.running:\n                cls.spinner.running = False\n                cls.animation.thread.join()\n\n            if any(cls.animation.messages):\n                cls.pop_message()\n\n            sys.stdout = sys.__stdout__", "response": "Stop the thread animation gracefully and reset_message"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auto_message(self, args):\n        if any(args) and callable(args[0]) and not self.message:\n            return args[0].__name__\n        elif not self.message:\n            return self.default_message\n        else:\n            return self.message", "response": "Try to guess the message by the args passed in the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef start(self):\n        self.streams.append(sys.stdout)\n        sys.stdout = self.stream", "response": "Activate the TypingStream on stdout"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchanges back the normal stdout after the end", "response": "def stop(cls):\n        \"\"\"Change back the normal stdout after the end\"\"\"\n        if any(cls.streams):\n            sys.stdout = cls.streams.pop(-1)\n        else:\n            sys.stdout = sys.__stdout__"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef acquire(self, timeout=0, force=False):\n\t\t\n\t\tif timeout and not (self.Queue.__collection__ and self.Queue.__capped__):\n\t\t\traise NotImplementedError(name(self.__class__) + \".Queue has not been prepared.\")\n\t\t\n\t\tD = self.__class__\n\t\tcollection = self.get_collection()\n\t\tidentity = self.Lock()\n\t\t\n\t\tif force:\n\t\t\tquery = D.id == self\n\t\t\n\t\telse:\n\t\t\tquery = D.lock == None\n\t\t\tquery |= D.lock.instance == identity.instance\n\t\t\tquery |= D.lock.time < (identity.time - identity.__period__)\n\t\t\tquery &= D.id == self\n\t\t\n\t\tprevious = collection.find_one_and_update(query, {'$set': {~D.lock: identity}}, {~D.lock: True})\n\t\t\n\t\tif previous is None:\n\t\t\tif timeout:\n\t\t\t\ttry:\n\t\t\t\t\tself.wait(timeout)\n\t\t\t\texcept TimeoutError:\n\t\t\t\t\tpass\n\t\t\t\t\n\t\t\t\treturn self.acquire()\n\t\t\t\n\t\t\tlock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)\n\t\t\traise self.Locked(\"Unable to acquire lock.\", lock)\n\t\t\n\t\tif not force and ~D.lock in previous:\n\t\t\tprevious = self.Lock.from_mongo(previous.get(~D.lock))\n\t\t\t\n\t\t\tif previous:\n\t\t\t\tif previous.expires < identity.time:\n\t\t\t\t\tprevious.expired(self)\n\t\t\t\t\n\t\t\t\tif previous.instance != identity.instance:  # Dont re-broadcast acquisition of an already-held lock.\n\t\t\t\t\tidentity.acquired(self, force)\n\t\t\n\t\treturn identity", "response": "Attempt to acquire an exclusive lock on this record."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprolongs the working duration of an already held lock.", "response": "def prolong(self):\n\t\t\"\"\"Prolong the working duration of an already held lock.\n\t\t\n\t\tAttempting to prolong a lock not already owned will result in a Locked exception.\n\t\t\"\"\"\n\t\t\n\t\tD = self.__class__\n\t\tcollection = self.get_collection()\n\t\tidentity = self.Lock()\n\t\t\n\t\tquery = D.id == self\n\t\tquery &= D.lock.instance == identity.instance\n\t\tquery &= D.lock.time >= (identity.time - identity.__period__)\n\t\t\n\t\tprevious = collection.find_one_and_update(query, {'$set': {~D.lock.time: identity.time}}, {~D.lock: True})\n\t\t\n\t\tif previous is None:\n\t\t\tlock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)\n\t\t\t\n\t\t\tif lock and lock.expires <= identity.time:\n\t\t\t\tlock.expired(self)\n\t\t\t\n\t\t\traise self.Locked(\"Unable to prolong lock.\", lock)\n\t\t\n\t\tidentity.prolonged(self)\n\t\t\n\t\treturn identity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef release(self, force=False):\n\t\t\n\t\tD = self.__class__\n\t\tcollection = self.get_collection()\n\t\tidentity = self.Lock()\n\t\t\n\t\tquery = D.id == self\n\t\t\n\t\tif not force:\n\t\t\tquery &= D.lock.instance == identity.instance\n\t\t\n\t\tprevious = collection.find_one_and_update(query, {'$unset': {~D.lock: True}}, {~D.lock: True})\n\t\t\n\t\tif previous is None:\n\t\t\tlock = getattr(self.find_one(self, projection={~D.lock: True}), 'lock', None)\n\t\t\traise self.Locked(\"Unable to release lock.\", lock)\n\t\t\n\t\tlock = self.Lock.from_mongo(previous[~D.lock])\n\t\t\n\t\tif lock and lock.expires <= identity.time:\n\t\t\tlock.expired(self)\n\t\t\n\t\tidentity.released(self, force)", "response": "Release an exclusive lock on this integration task."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef to_native(self, obj, name, value):  # pylint:disable=unused-argument\n\t\t\n\t\tif self.mapping:\n\t\t\tfor original, new in self.mapping.items():\n\t\t\t\tvalue = value.replace(original, new)\n\t\t\n\t\treturn load(value, self.namespace)", "response": "Transform the MongoDB value into a Marrow Mongo value."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform to a MongoDB - safe value.", "response": "def to_foreign(self, obj, name, value):  # pylint:disable=unused-argument\n\t\t\"\"\"Transform to a MongoDB-safe value.\"\"\"\n\t\t\n\t\tnamespace = self.namespace\n\t\t\n\t\ttry:\n\t\t\texplicit = self.explicit\n\t\texcept AttributeError:\n\t\t\texplicit = not namespace\n\t\t\n\t\tif not isinstance(value, (str, unicode)):\n\t\t\tvalue = canon(value)\n\t\t\n\t\tif namespace and ':' in value:  # Try to reduce to a known plugin short name.\n\t\t\tfor point in iter_entry_points(namespace):  # TODO: Isolate.\n\t\t\t\tqualname = point.module_name\n\t\t\t\t\n\t\t\t\tif point.attrs:\n\t\t\t\t\tqualname += ':' + '.'.join(point.attrs)\n\t\t\t\t\n\t\t\t\tif qualname == value:\n\t\t\t\t\tvalue = point.name\n\t\t\t\t\tbreak\n\t\t\n\t\tif ':' in value:\n\t\t\tif not explicit:\n\t\t\t\traise ValueError(\"Explicit object references not allowed.\")\n\t\t\t\n\t\t\treturn value\n\t\t\n\t\tif namespace and value not in (i.name for i in iter_entry_points(namespace)):\n\t\t\traise ValueError('Unknown plugin \"' + value + '\" for namespace \"' + namespace + '\".')\n\t\t\n\t\treturn value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef F(Document, __raw__=None, **filters):\n\t\n\tops = Filter(__raw__)\n\targs = _process_arguments(Document, FILTER_PREFIX_MAP, FILTER_OPERATION_MAP, filters)\n\t\n\tfor prefix, suffix, field, value in args:\n\t\tif suffix:\n\t\t\top = suffix(field, value)\n\t\telse:\n\t\t\top = DEFAULT_FILTER(field, value)\n\t\t\n\t\tif prefix:\n\t\t\top = prefix(op)\n\t\t\n\t\tops &= op\n\t\n\treturn ops", "response": "Generate a MongoDB filter document through parameter interpolation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite to the default stream", "response": "def write(self, message, flush=True):\n        \"\"\"\n        Function: write\n        Summary: write method on the default stream\n        Examples: >>> stream.write('message')\n                  'message'\n        Attributes:\n            @param (message): str-like content to send on stream\n            @param (flush) default=True: flush the stdout after write\n        Returns: None\n        \"\"\"\n        self.stream.write(message)\n        if flush:\n            self.stream.flush()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend something for stdout and erased after delay", "response": "def write(self, message, autoerase=True):\n        \"\"\"Send something for stdout and erased after delay\"\"\"\n        super(Animation, self).write(message)\n        self.last_message = message\n        if autoerase:\n            time.sleep(self.interval)\n            self.erase(message)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef erase(self, message=None):\n        if not message:\n            message = self.last_message\n        # Move cursor to the beginning of line\n        super(Animation, self).write(\"\\033[G\")\n        # Erase in line from cursor\n        super(Animation, self).write(\"\\033[K\")", "response": "Erase something whose you write before message."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting something on the default stream with a prefixed message", "response": "def write(self, message, flush=False):\n        \"\"\"Write something on the default stream with a prefixed message\"\"\"\n        # this need be threadsafe because the concurrent spinning running on\n        # the stderr\n        with self.lock:\n            self.paralell_stream.erase()\n            super(Clean, self).write(message, flush)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbinding a copy of the collection to the class modified per our class settings.", "response": "def bind(cls, target):\n\t\t\"\"\"Bind a copy of the collection to the class, modified per our class' settings.\n\t\t\n\t\tThe given target (and eventual collection returned) must be safe within the context the document sublcass\n\t\tbeing bound is constructed within. E.g. at the module scope this binding must be thread-safe.\n\t\t\"\"\"\n\t\t\n\t\tif cls.__bound__ is not None:\n\t\t\treturn cls\n\t\t\n\t\tcls.__bound__ = cls.get_collection(target)\n\t\t\n\t\treturn cls"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_collection(cls, target=None):\n\t\t\n\t\tif target is None:\n\t\t\tif cls.__bound__ is None:\n\t\t\t\traise TypeError(\"Target required when document class not bound.\")\n\t\t\t\n\t\t\treturn cls.__bound__\n\t\t\n\t\tif isinstance(target, PyMongoCollection):\n\t\t\treturn target.with_options(**cls._collection_configuration())\n\t\t\n\t\telif isinstance(target, Database):\n\t\t\treturn target.get_collection(cls.__collection__, **cls._collection_configuration())\n\t\t\n\t\traise TypeError(\"Can not retrieve collection from: \" + repr(target))", "response": "Retrieve a properly configured collection object as configured by this document class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_collection(cls, target=None, drop=False, indexes=True):\n\t\t\n\t\tif target is None:\n\t\t\tif cls.__bound__ is None:\n\t\t\t\traise TypeError(\"Target required when document class not bound.\")\n\t\t\t\n\t\t\ttarget = cls.__bound__\n\t\t\n\t\tif isinstance(target, PyMongoCollection):\n\t\t\tcollection = target.name\n\t\t\ttarget = target.database\n\t\telif isinstance(target, Database):\n\t\t\tcollection = cls.__collection__\n\t\telse:\n\t\t\traise TypeError(\"Can not retrieve database from: \" + repr(target))\n\t\t\n\t\tif drop:\n\t\t\ttarget.drop_collection(collection)  # TODO: If drop fails, try just emptying?\n\t\t\n\t\tcollection = target.create_collection(collection, **cls._collection_configuration(True))\n\t\t\n\t\tif indexes:\n\t\t\tcls.create_indexes(collection)\n\t\t\n\t\treturn collection", "response": "Ensure the collection identified by this document class exists creating it if not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_indexes(cls, target=None, recreate=False):\n\t\t\n\t\t# TODO: Nested indexes.\n\t\t\n\t\tresults = []\n\t\tcollection = cls.get_collection(target)\n\t\t\n\t\tif recreate:\n\t\t\tcollection.drop_indexes()\n\t\t\n\t\tfor index in cls.__indexes__.values():\n\t\t\tresults.append(index.create(collection))\n\t\t\n\t\treturn results", "response": "Iterate all known indexes and construct them."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_default_projection(cls):\n\t\t\n\t\tprojected = []  # The fields explicitly requested for inclusion.\n\t\tneutral = []  # Fields returning neutral (None) status.\n\t\tomitted = False  # Have any fields been explicitly omitted?\n\t\t\n\t\tfor name, field in cls.__fields__.items():\n\t\t\tif field.project is None:\n\t\t\t\tneutral.append(name)\n\t\t\telif field.project:\n\t\t\t\tprojected.append(name)\n\t\t\telse:\n\t\t\t\tomitted = True\n\t\t\n\t\tif not projected and not omitted:\n\t\t\t# No preferences specified.\n\t\t\treturn None\n\t\t\t\n\t\telif not projected and omitted:\n\t\t\t# No positive inclusions given, but negative ones were.\n\t\t\tprojected = neutral\n\t\t\n\t\treturn {field: True for field in projected}", "response": "Construct the default projection document."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert_one(self, validate=True):\n\t\t\n\t\tkw = {}\n\t\tkw['bypass_document_validation'] = not validate\n\t\t\n\t\tcollection = self.get_collection(kw.pop('source', None))\n\t\treturn collection.insert_one(self, **kw)", "response": "Insert this document into the collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate this document in the database.", "response": "def update_one(self, update=None, validate=True, **kw):\n\t\t\"\"\"Update this document in the database. Local representations will not be affected.\n\t\t\n\t\tA single positional parameter, `update`, may be provided as a mapping. Keyword arguments (other than those\n\t\tidentified in UPDATE_MAPPING) are interpreted as parametric updates, added to any `update` passed in.\n\t\t\n\t\thttps://api.mongodb.com/python/current/api/pymongo/collection.html#pymongo.collection.Collection.update_one\n\t\t\"\"\"\n\t\t\n\t\tD = self.__class__\n\t\tcollection = self.get_collection(kw.pop('source', None))\n\t\t\n\t\tupdate = Update(update or {})\n\t\t\n\t\tif kw:\n\t\t\tupdate &= U(D, **kw)\n\t\t\n\t\tif not update:\n\t\t\traise TypeError(\"Must provide an update operation.\")\n\t\t\n\t\treturn collection.update_one(D.id == self, update, bypass_document_validation=not validate)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_one(self, source=None, **kw):\n\t\t\n\t\tcollection = self.get_collection(source)\n\t\treturn collection.delete_one(self.__class__.id == self, **kw)", "response": "Remove this document from the database passing additional arguments through to PyMongo.\n\t\t\n\thttps://api. mongodb. com / python / current / api. pymongo. collection. Collection. delete_one"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef force_text(s, encoding='utf-8', errors='strict'):\n\n    if not isinstance(s, six.string_types):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            if isinstance(s, Exception):\n                # An Exception subclass containing non-ASCII data that doesn't\n                # know how to print itself properly. We shouldn't raise a\n                # further exception.\n                return ' '.join([force_text(arg, encoding, errors) for arg in s])\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, six.text_type):\n        return s.encode(encoding, errors)\n    elif s and encoding != 'utf-8':\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s", "response": "Returns a bytestring version of s encoded as specified in encoding."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adjust_attribute_sequence(*fields):\n\t\n\tamount = None\n\t\n\tif fields and isinstance(fields[0], int):\n\t\tamount, fields = fields[0], fields[1:]\n\t\n\tdef adjust_inner(cls):\n\t\tfor field in fields:\n\t\t\tif field not in cls.__dict__:\n\t\t\t\t# TODO: Copy the field definition.\n\t\t\t\traise TypeError(\"Can only override sequence on non-inherited attributes.\")\n\t\t\t\n\t\t\t# Adjust the sequence to re-order the field.\n\t\t\tif amount is None:\n\t\t\t\tcls.__dict__[field].__sequence__ = ElementMeta.sequence\n\t\t\telse:\n\t\t\t\tcls.__dict__[field].__sequence__ += amount  # Add the given amount.\n\t\t\n\t\t# Update the attribute collection.\n\t\tcls.__attributes__ = OrderedDict(\n\t\t\t\t\t(k, v) for k, v in \\\n\t\t\t\t\tsorted(cls.__attributes__.items(),\n\t\t\t\t\t\tkey=lambda i: i[1].__sequence__)\n\t\t\t\t)\n\t\t\n\t\treturn cls\n\t\n\treturn adjust_inner", "response": "Adjust the sequence of the attributes of the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrounding a datetime object down to the start of a defined period.", "response": "def datetime_period(base=None, hours=None, minutes=None, seconds=None):\n\t\"\"\"Round a datetime object down to the start of a defined period.\n\t\n\tThe `base` argument may be used to find the period start for an arbitrary datetime, defaults to `utcnow()`.\n\t\"\"\"\n\t\n\tif base is None:\n\t\tbase = utcnow()\n\t\n\tbase -= timedelta(\n\t\t\thours = 0 if hours is None else (base.hour % hours),\n\t\t\tminutes = (base.minute if hours else 0) if minutes is None else (base.minute % minutes),\n\t\t\tseconds = (base.second if minutes or hours else 0) if seconds is None else (base.second % seconds),\n\t\t\tmicroseconds = base.microsecond\n\t\t)\n\t\n\treturn base"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a dictionary of file paths and timestamps.", "response": "def get_hashes(path, exclude=None):\n    '''\n    Get a dictionary of file paths and timestamps.\n\n    Paths matching `exclude` regex will be excluded.\n    '''\n    out = {}\n    for f in Path(path).rglob('*'):\n        if f.is_dir():\n            # We want to watch files, not directories.\n            continue\n        if exclude and re.match(exclude, f.as_posix()):\n            retox_log.debug(\"excluding '{}'\".format(f.as_posix()))\n            continue\n        pytime = f.stat().st_mtime\n        out[f.as_posix()] = pytime\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef request(self, method, params=None, query_continue=None,\n                files=None, auth=None, continuation=False):\n        \"\"\"\n        Sends an HTTP request to the API.\n\n        :Parameters:\n            method : `str`\n                Which HTTP method to use for the request?\n                (Usually \"POST\" or \"GET\")\n            params : `dict`\n                A set of parameters to send with the request.  These parameters\n                will be included in the POST body for post requests or a query\n                string otherwise.\n            query_continue : `dict`\n                A 'continue' field from a past request.  This field represents\n                the point from which a query should be continued.\n            files : `dict`\n                A dictionary of (filename : `str`, data : `bytes`) pairs to\n                send with the request.\n            auth : mixed\n                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.\n            continuation : `bool`\n                If true, a continuation will be attempted and a generator of\n                JSON response documents will be returned.\n\n        :Returns:\n            A response JSON documents (or a generator of documents if\n            `continuation == True`)\n        \"\"\"\n        normal_params = _normalize_params(params, query_continue)\n        if continuation:\n            return self._continuation(method, params=normal_params, auth=auth,\n                                      files=files)\n        else:\n            return self._request(method, params=normal_params, auth=auth,\n                                 files=files)", "response": "Sends an HTTP request to the API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef login(self, username, password, login_token=None):\n        if login_token is None:\n            token_doc = self.post(action='query', meta='tokens', type='login')\n            login_token = token_doc['query']['tokens']['logintoken']\n\n        login_doc = self.post(\n            action=\"clientlogin\", username=username, password=password,\n            logintoken=login_token, loginreturnurl=\"http://example.org/\")\n\n        if login_doc['clientlogin']['status'] == \"UI\":\n            raise ClientInteractionRequest.from_doc(\n                login_token, login_doc['clientlogin'])\n        elif login_doc['clientlogin']['status'] != 'PASS':\n            raise LoginError.from_doc(login_doc['clientlogin'])\n        return login_doc['clientlogin']", "response": "Authenticate with the given credentials."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncontinue a login that requires an additional step. This is common for when login requires a captcha or supplying a two - factor authentication token.", "response": "def continue_login(self, login_token, **params):\n        \"\"\"\n        Continues a login that requires an additional step.  This is common\n        for when login requires completing a captcha or supplying a two-factor\n        authentication token.\n\n        :Parameters:\n            login_token : `str`\n                A login token generated by the MediaWiki API (and used in a\n                previous call to login())\n            params : `mixed`\n                A set of parameters to include with the request.  This depends\n                on what \"requests\" for additional information were made by the\n                MediaWiki API.\n        \"\"\"\n\n        login_params = {\n            'action': \"clientlogin\",\n            'logintoken': login_token,\n            'logincontinue': 1\n        }\n        login_params.update(params)\n        login_doc = self.post(**login_params)\n        if login_doc['clientlogin']['status'] != 'PASS':\n            raise LoginError.from_doc(login_doc['clientlogin'])\n        return login_doc['clientlogin']"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake an API request with the GET method.", "response": "def get(self, query_continue=None, auth=None, continuation=False,\n            **params):\n        \"\"\"Makes an API request with the GET method\n\n        :Parameters:\n            query_continue : `dict`\n                Optionally, the value of a query continuation 'continue' field.\n            auth : mixed\n                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.\n            continuation : `bool`\n                If true, a continuation will be attempted and a generator of\n                JSON response documents will be returned.\n            params :\n                Keyword parameters to be sent in the query string.\n\n        :Returns:\n            A response JSON documents (or a generator of documents if\n            `continuation == True`)\n\n        :Raises:\n            :class:`mwapi.errors.APIError` : if the API responds with an error\n        \"\"\"\n\n        return self.request('GET', params=params, auth=auth,\n                            query_continue=query_continue,\n                            continuation=continuation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes an API request with the POST method.", "response": "def post(self, query_continue=None, upload_file=None, auth=None,\n             continuation=False, **params):\n        \"\"\"Makes an API request with the POST method\n\n        :Parameters:\n            query_continue : `dict`\n                Optionally, the value of a query continuation 'continue' field.\n            upload_file : `bytes`\n                The bytes of a file to upload.\n            auth : mixed\n                Auth tuple or callable to enable Basic/Digest/Custom HTTP Auth.\n            continuation : `bool`\n                If true, a continuation will be attempted and a generator of\n                JSON response documents will be returned.\n            params :\n                Keyword parameters to be sent in the POST message body.\n\n        :Returns:\n            A response JSON documents (or a generator of documents if\n            `continuation == True`)\n\n        :Raises:\n            :class:`mwapi.errors.APIError` : if the API responds with an error\n        \"\"\"\n        if upload_file is not None:\n            files = {'file': upload_file}\n        else:\n            files = None\n\n        return self.request('POST', params=params, auth=auth,\n                            query_continue=query_continue, files=files,\n                            continuation=continuation)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef only_published(cls, at=None):\n\t\t\n\t\tif isinstance(at, timedelta):\n\t\t\tat = utcnow() + at\n\t\telse:\n\t\t\tat = at or utcnow()\n\t\t\n\t\tpub, ret = cls.published, cls.retracted\n\t\t\n\t\tpublication = (-pub) | (pub == None) | (pub <= at)\n\t\tretraction = (-ret) | (ret == None) | (ret > at)\n\t\t\n\t\treturn publication & retraction", "response": "Produce a query fragment suitable for selecting documents published at a specific time."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms the MongoDB value into a Marrow Mongo value.", "response": "def to_native(self, obj, name, value):  # pylint:disable=unused-argument\n\t\t\"\"\"Transform the MongoDB value into a Marrow Mongo value.\"\"\"\n\t\t\n\t\tfrom marrow.mongo import Document\n\t\tfrom marrow.mongo.trait import Derived\n\t\t\n\t\tkind = self._kind(obj.__class__)\n\t\t\n\t\tif isinstance(value, Document):\n\t\t\tif __debug__ and kind and issubclass(kind, Document) and not isinstance(value, kind):\n\t\t\t\traise ValueError(\"Not an instance of \" + kind.__name__ + \" or a sub-class: \" + repr(value))\n\t\t\t\n\t\t\treturn value\n\t\t\n\t\tif isinstance(kind, Field):\n\t\t\treturn kind.transformer.native(value, (kind, obj))\n\t\t\n\t\treturn (kind or Derived).from_mongo(value)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms to a MongoDB - safe value.", "response": "def to_foreign(self, obj, name, value):  # pylint:disable=unused-argument\n\t\t\"\"\"Transform to a MongoDB-safe value.\"\"\"\n\t\t\n\t\tfrom marrow.mongo import Document\n\t\t\n\t\tkind = self._kind(obj if isclass(obj) else obj.__class__)\n\t\t\n\t\tif isinstance(value, Document):\n\t\t\tif __debug__ and kind and issubclass(kind, Document) and not isinstance(value, kind):\n\t\t\t\traise ValueError(\"Not an instance of \" + kind.__name__ + \" or a sub-class: \" + repr(value))\n\t\t\t\n\t\t\treturn value\n\t\t\n\t\tif isinstance(kind, Field):\n\t\t\tkind.validator.validate(value, FieldContext(kind, obj))\n\t\t\treturn kind.transformer.foreign(value, FieldContext(kind, obj))\n\t\t\n\t\tif kind:\n\t\t\tvalue = kind(**value)\n\t\t\n\t\treturn value"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef promote(self, cls, update=False, preserve=True):\n\t\t\n\t\tif not issubclass(cls, self.__class__):\n\t\t\traise TypeError(\"Must promote to a subclass of \" + self.__class__.__name__)\n\t\t\n\t\treturn self._as(cls, update, preserve)", "response": "Transform this record into an instance of a more specialized subclass."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_native(self, obj, name, value):\n\t\t\n\t\tif isinstance(value, self.List):\n\t\t\treturn value\n\t\t\n\t\tresult = self.List(super(Array, self).to_native(obj, name, i) for i in value)\n\t\tobj.__data__[self.__name__] = result\n\t\t\n\t\treturn result", "response": "Transform the MongoDB value into a Marrow Mongo value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_foreign(self, obj, name, value):\n\t\t\n\t\tif isinstance(value, Iterable) and not isinstance(value, Mapping):\n\t\t\treturn self.List(super(Array, self).to_foreign(obj, name, i) for i in value)\n\t\t\n\t\treturn super(Array, self).to_foreign(obj, name, value)", "response": "Transform to a MongoDB - safe value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncut nodes away from menus", "response": "def cut_levels(nodes, start_level):\n    \"\"\"\n    cutting nodes away from menus\n    \"\"\"\n    final = []\n    removed = []\n    for node in nodes:\n        if not hasattr(node, 'level'):\n            # remove and ignore nodes that don't have level information\n            remove(node, removed)\n            continue\n        if node.attr.get('soft_root', False):\n            # remove and ignore nodes that are behind a node marked as 'soft_root'\n            remove(node, removed)\n            continue\n        if node.level == start_level:\n            # turn nodes that are on from_level into root nodes\n            final.append(node)\n            node.parent = None\n            if not node.visible and not node.children:\n                remove(node, removed)\n        elif node.level == start_level + 1:\n            # remove nodes that are deeper than one level\n            node.children = []\n        else:\n            remove(node, removed)\n        if not node.visible:\n            keep_node = False\n            for child in node.children:\n                keep_node = keep_node or child.visible\n            if not keep_node:\n                remove(node, removed)\n    for node in removed:\n        if node in final:\n            final.remove(node)\n    return final"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef U(Document, __raw__=None, **update):\n\t\n\tops = Update(__raw__)\n\targs = _process_arguments(Document, UPDATE_ALIASES, {}, update, UPDATE_PASSTHROUGH)\n\t\n\tfor operation, _, field, value in args:\n\t\tif not operation:\n\t\t\toperation = DEFAULT_UPDATE\n\t\t\n\t\tif isinstance(operation, tuple):\n\t\t\toperation, cast = ('$' + operation[0]), operation[1]\n\t\t\tif cast in UPDATE_MAGIC:\n\t\t\t\tvalue = cast(value, field)\n\t\t\telse:\n\t\t\t\tvalue = cast(value)\n\t\t\t\n\t\t\tif operation in ops and ~field in ops[operation] and isinstance(value, Mapping):\n\t\t\t\tops[operation][~field].update(value)\n\t\t\t\tcontinue\n\t\t\n\t\telse:\n\t\t\toperation = '$' + operation\n\t\t\n\t\tops &= Update({operation: {~field: value}})\n\t\n\treturn ops", "response": "Generates a MongoDB update document through paramater interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef debug(function):\n\n    @wraps(function)\n    def _wrapper(*args, **kwargs):\n        result = function(*args, **kwargs)\n        for key, value in kwargs.items():\n            args += tuple(['{}={!r}'.format(key, value)])\n        if len(args) == 1:\n            args = '({})'.format(args[0])\n        print('@{0}{1} -> {2}'.format(function.__name__, args, result))\n        _wrapper.last_output = [function.__name__, str(args), result]\n        return result\n    _wrapper.last_output = []\n    return _wrapper", "response": "Decorator to debug a function"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions counter Summary: Decorator to count the number of a function is executed each time Examples: You can use that to had a progress of heally heavy computation without progress feedback Attributes: @param (function): function Returns: wrapped function", "response": "def counter(function):\n    \"\"\"\n    Function: counter\n    Summary: Decorator to count the number of a function is executed each time\n    Examples: You can use that to had a progress of heally heavy\n              computation without progress feedback\n    Attributes:\n        @param (function): function\n    Returns: wrapped function\n    \"\"\"\n\n    @wraps(function)\n    def _wrapper(*args, **kwargs):\n        _wrapper.count += 1\n        res = function(*args, **kwargs)\n        funcname = function.__name__\n        count = _wrapper.count\n        print(\"{} has been used: {}x\".format(funcname, count))\n        return res\n    _wrapper.count = 0\n    return _wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef count_time(function):\n    @wraps(function)\n    def _wrapper(*args, **kwargs):\n        before = time()\n        result = function(*args, **kwargs)\n        diff = time() - before\n        funcname = function.__name__\n        print(\"{!r} func leave it {:.2f} ms to finish\".format(funcname, diff))\n        _wrapper.time = diff\n        return result\n\n    _wrapper.time = 0\n    return _wrapper", "response": "Decorator that counts how long a function has finished"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef S(Document, *fields):\n\t\n\tresult = []\n\t\n\tfor field in fields:\n\t\tif isinstance(field, tuple):  # Unpack existing tuple.\n\t\t\tfield, direction = field\n\t\t\tresult.append((field, direction))\n\t\t\tcontinue\n\t\t\n\t\tdirection = ASCENDING\n\t\t\n\t\tif not field.startswith('__'):\n\t\t\tfield = field.replace('__', '.')\n\t\t\n\t\tif field[0] == '-':\n\t\t\tdirection = DESCENDING\n\t\t\n\t\tif field[0] in ('+', '-'):\n\t\t\tfield = field[1:]\n\t\t\n\t\t_field = traverse(Document, field, default=None)\n\t\t\n\t\tresult.append(((~_field) if _field else field, direction))\n\t\n\treturn result", "response": "Generate a MongoDB sort order list using the Django ORM style."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread data from CNPJ list and writes results to output directory.", "response": "def run(self):\n        \"\"\"Reads data from CNPJ list and write results to output directory.\"\"\"\n        self._assure_output_dir(self.output)\n        companies = self.read()\n        print '%s CNPJs found' % len(companies)\n\n        pbar = ProgressBar(\n            widgets=[Counter(), ' ', Percentage(), ' ', Bar(), ' ', Timer()],\n            maxval=len(companies)).start()\n\n        resolved = 0\n        runner = Runner(companies, self.days, self.token)\n\n        try:\n            for data in runner:\n                self.write(data)\n                resolved = resolved + 1\n                pbar.update(resolved)\n        except KeyboardInterrupt:\n            print '\\naborted: waiting current requests to finish.'\n            runner.stop()\n            return\n\n        pbar.finish()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads data from the CSV file.", "response": "def read(self):\n        \"\"\"Reads data from the CSV file.\"\"\"\n        companies = []\n        with open(self.file) as f:\n            reader = unicodecsv.reader(f)\n            for line in reader:\n                if len(line) >= 1:\n                    cnpj = self.format(line[0])\n                    if self.valid(cnpj):\n                        companies.append(cnpj)\n        return companies"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, data):\n        cnpj, data = data\n\n        path = os.path.join(self.output, '%s.json' % cnpj)\n        with open(path, 'w') as f:\n            json.dump(data, f, encoding='utf-8')", "response": "Writes the json data to the output directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef valid(self, cnpj):\n        if len(cnpj) != 14:\n            return False\n\n        tam = 12\n        nums = cnpj[:tam]\n        digs = cnpj[tam:]\n\n        tot = 0\n        pos = tam-7\n        for i in range(tam, 0, -1):\n            tot = tot + int(nums[tam-i])*pos\n            pos = pos - 1\n            if pos < 2:\n                pos = 9\n        res = 0 if tot % 11 < 2 else 11 - (tot % 11)\n        if res != int(digs[0]):\n            return False\n\n        tam = tam + 1\n        nums = cnpj[:tam]\n        tot = 0\n        pos = tam-7\n        for i in range(tam, 0, -1):\n            tot = tot + int(nums[tam-i])*pos\n            pos = pos - 1\n            if pos < 2:\n                pos = 9\n        res = 0 if tot % 11 < 2 else 11 - (tot % 11)\n        if res != int(digs[1]):\n            return False\n\n        return True", "response": "Check if a CNPJ is valid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_taxon_info(taxon, include_anc, output):\n    output.write(u'Taxon info for OTT ID (ot:ottId) = {}\\n'.format(taxon.ott_id))\n    output.write(u'    name (ot:ottTaxonName) = \"{}\"\\n'.format(taxon.name))\n    if taxon.synonyms:\n        output.write(u'    known synonyms: \"{}\"\\n'.format('\", \"'.join(taxon.synonyms)))\n    else:\n        output.write(u'    known synonyms: \\n')\n    output.write(u'    OTT flags for this taxon: {}\\n'.format(taxon.flags))\n    output.write(u'    The taxonomic rank associated with this name is: {}\\n'.format(taxon.rank))\n    output.write(\n        u'    The (unstable) node ID in the current taxomachine instance is: {}\\n'.format(taxon.taxomachine_node_id))\n    if include_anc:\n        if taxon.parent is not None:\n            output.write(u'Taxon {c} is a child of {p}.\\n'.format(c=taxon.ott_id, p=taxon.parent.ott_id))\n            write_taxon_info(taxon.parent, True, output)\n        else:\n            output.write('uTaxon {c} is the root of the taxonomy.'.format(c=taxon.ott_id))", "response": "Writes out information about a taxon object to the output stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_default_config_filename():\n    global _CONFIG_FN\n    if _CONFIG_FN is not None:\n        return _CONFIG_FN\n    with _CONFIG_FN_LOCK:\n        if _CONFIG_FN is not None:\n            return _CONFIG_FN\n        if 'PEYOTL_CONFIG_FILE' in os.environ:\n            cfn = os.path.abspath(os.environ['PEYOTL_CONFIG_FILE'])\n        else:\n            cfn = os.path.expanduser(\"~/.peyotl/config\")\n        if not os.path.isfile(cfn):\n            # noinspection PyProtectedMember\n            if 'PEYOTL_CONFIG_FILE' in os.environ:\n                from peyotl.utility.get_logger import warn_from_util_logger\n                msg = 'Filepath \"{}\" specified via PEYOTL_CONFIG_FILE={} was not found'.format(cfn, os.environ[\n                    'PEYOTL_CONFIG_FILE'])\n                warn_from_util_logger(msg)\n            from pkg_resources import Requirement, resource_filename\n            pr = Requirement.parse('peyotl')\n            cfn = resource_filename(pr, 'peyotl/default.conf')\n        if not os.path.isfile(cfn):\n            raise RuntimeError('The peyotl configuration file cascade failed looking for \"{}\"'.format(cfn))\n        _CONFIG_FN = os.path.abspath(cfn)\n    return _CONFIG_FN", "response": "Returns the default configuration file path."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_raw_default_config_and_read_file_list():\n    global _CONFIG, _READ_DEFAULT_FILES\n    if _CONFIG is not None:\n        return _CONFIG, _READ_DEFAULT_FILES\n    with _CONFIG_LOCK:\n        if _CONFIG is not None:\n            return _CONFIG, _READ_DEFAULT_FILES\n        try:\n            # noinspection PyCompatibility\n            from ConfigParser import SafeConfigParser\n        except ImportError:\n            # noinspection PyCompatibility,PyUnresolvedReferences\n            from configparser import ConfigParser as SafeConfigParser  # pylint: disable=F0401\n        cfg = SafeConfigParser()\n        read_files = cfg.read(get_default_config_filename())\n        _CONFIG, _READ_DEFAULT_FILES = cfg, read_files\n        return _CONFIG, _READ_DEFAULT_FILES", "response": "Returns a ConfigParser object and a list of filenames that were parsed to initialize it"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _raw_config_setting(config_obj, section, param, default=None, config_filename='', warn_on_none_level=logging.WARN):\n    # noinspection PyBroadException\n    try:\n        return config_obj.get(section, param)\n    except:\n        if (default is None) and warn_on_none_level is not None:\n            _warn_missing_setting(section, param, config_filename, warn_on_none_level)\n        return default", "response": "Read a single config setting from config_obj."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a two - level dictionary of d section][option ] from a config parser object.", "response": "def _create_overrides_from_config(config):\n    \"\"\"Creates a two-level dictionary of d[section][option] from a config parser object.\"\"\"\n    d = {}\n    for s in config.sections():\n        d[s] = {}\n        for opt in config.options(s):\n            d[s][opt] = config.get(s, opt)\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_config_object():\n    global _DEFAULT_CONFIG_WRAPPER\n    if _DEFAULT_CONFIG_WRAPPER is not None:\n        return _DEFAULT_CONFIG_WRAPPER\n    with _DEFAULT_CONFIG_WRAPPER_LOCK:\n        if _DEFAULT_CONFIG_WRAPPER is not None:\n            return _DEFAULT_CONFIG_WRAPPER\n        _DEFAULT_CONFIG_WRAPPER = ConfigWrapper()\n        return _DEFAULT_CONFIG_WRAPPER", "response": "Thread - safe accessor for the immutable default ConfigWrapper object"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the first non - None setting from a series where each element in sec_param_list is a section param pair suitable for get_config_setting call.", "response": "def get_from_config_setting_cascade(self, sec_param_list, default=None, warn_on_none_level=logging.WARN):\n        \"\"\"return the first non-None setting from a series where each\n        element in `sec_param_list` is a section, param pair suitable for\n        a get_config_setting call.\n\n        Note that non-None values for overrides for this ConfigWrapper instance will cause\n            this call to only evaluate the first element in the cascade.\n        \"\"\"\n        for section, param in sec_param_list:\n            r = self.get_config_setting(section, param, default=None, warn_on_none_level=None)\n            if r is not None:\n                return r\n        section, param = sec_param_list[-1]\n        if default is None:\n            _warn_missing_setting(section, param, self._config_filename, warn_on_none_level)\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_amendment(obj, retain_deprecated=True, **kwargs):\n    # Gather and report errors in a simple list\n    errors = []\n    n = create_validation_adaptor(obj, errors, **kwargs)\n    return errors, n", "response": "Validates an object that is an amendment object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_anc_lineage_from_id2par(id2par_id, ott_id):\n    curr = ott_id\n    n = id2par_id.get(curr)\n    if n is None:\n        raise KeyError('The OTT ID {} was not found'.format(ott_id))\n    lineage = [curr]\n    while n is not None:\n        lineage.append(n)\n        n = id2par_id.get(n)\n    return lineage", "response": "Returns a list from [ ott_id s par... root ott_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexpects a dict of id2parent ID or a pickled object", "response": "def parse_id2par_dict(id2par=None,\n                      id_list=None,\n                      id2par_stream=None,\n                      id2par_filepath=None,\n                      id_list_stream=None,\n                      id_list_filepath=None,\n                      _class=TreeWithPathsInEdges):\n    \"\"\"Expecting a dict of id2parent ID or a pickled object (passed in as file object `stream` or `filepath`)\n    \"\"\"\n    import pickle\n    if id2par is None:\n        if id2par_stream is None:\n            with open(id2par_filepath, 'rb') as fo:\n                id2par = pickle.load(fo)\n        else:\n            id2par = pickle.load(id2par_stream)\n    if id_list is None:\n        if id_list_stream is None:\n            if id_list_filepath is None:\n                ancs = set(id2par.values())\n                all_keys = set(id2par.keys())\n                id_list = list(all_keys - ancs)\n            else:\n                with open(id_list_filepath, 'rb') as fo:\n                    id_list = pickle.load(fo)\n        else:\n            id_list = pickle.load(id_list_stream)\n\n    _LOG.debug(\"num els {}\".format(len(id2par)))\n    return create_tree_from_id2par(id2par=id2par, id_list=id_list, _class=_class)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef before_after_apply(self, before_fn, after_fn, leaf_fn=None):\n        stack = [self]\n        while stack:\n            node = stack.pop()\n            if node.is_leaf:\n                if leaf_fn:\n                    leaf_fn(node)\n                while node.is_last_child_of_parent:\n                    node = node._parent\n                    if node:\n                        after_fn(node)\n                    else:\n                        break\n            else:\n                before_fn(node)\n                stack.extend([i for i in reversed(node._children)])", "response": "Applies the functions to each node in a subtree using an traversal in which\n        encountered twice : once right before its descendants and once right after its last descendant."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef preorder_iter(self, filter_fn=None):\n        stack = [self]\n        while stack:\n            node = stack.pop()\n            if filter_fn is None or filter_fn(node):\n                yield node\n            stack.extend([i for i in reversed(node._children)])", "response": "From DendroPy\n        Preorder traversal of self and its child_nodes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_bits4subtree_ids(self, relevant_ids):\n        if relevant_ids:\n            checking = True\n        else:\n            checking = False\n            relevant_ids = {}\n            bit = 1\n        self.bits2internal_node = {}\n        for node in self.postorder_node_iter():\n            p = node._parent\n            if p is None:\n                if not node.is_leaf:\n                    self.bits2internal_node[node.bits4subtree_ids] = node\n                continue\n            if not hasattr(p, 'bits4subtree_ids'):\n                p.bits4subtree_ids = 0\n            i = node._id\n            # _LOG.debug('node._id ={}'.format(i))\n            # _LOG.debug('Before par mrca... = {}'.format(p.bits4subtree_ids))\n            if checking:\n                b = relevant_ids.get(i)\n                if b:\n                    if node.is_leaf:\n                        node.bits4subtree_ids = b\n                    else:\n                        node.bits4subtree_ids |= b\n            else:\n                if node.is_leaf:\n                    relevant_ids[i] = bit\n                    node.bits4subtree_ids = bit\n                    bit <<= 1\n            if not node.is_leaf:\n                self.bits2internal_node[node.bits4subtree_ids] = node\n            # _LOG.debug('while add bitrep... self.bits2internal_node = {}'.format(self.bits2internal_node))\n            p.bits4subtree_ids |= node.bits4subtree_ids\n        return relevant_ids", "response": "Adds a long integer bits4subtree_ids to each node in the tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef do_parse(infilename: str, outfilename: str, verbose: bool) -> bool:\n    python = parse(FileStream(infilename, encoding=\"utf-8\"), infilename)\n    if python is not None:\n        with open(outfilename, 'w') as outfile:\n            outfile.write(python)\n        if verbose:\n            print(\"Output written to {}\".format(outfilename))\n        return True\n    return False", "response": "Parse the jsg in infilename and save the results in outfilename\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(input_: Union[str, FileStream], source: str) -> Optional[str]:\n\n    # Step 1: Tokenize the input stream\n    error_listener = ParseErrorListener()\n    if not isinstance(input_, FileStream):\n        input_ = InputStream(input_)\n    lexer = jsgLexer(input_)\n    lexer.addErrorListener(error_listener)\n    tokens = CommonTokenStream(lexer)\n    tokens.fill()\n    if error_listener.n_errors:\n        return None\n\n    # Step 2: Generate the parse tree\n    parser = jsgParser(tokens)\n    parser.addErrorListener(error_listener)\n    parse_tree = parser.doc()\n    if error_listener.n_errors:\n        return None\n\n    # Step 3: Transform the results the results\n    parser = JSGDocParser()\n    parser.visit(parse_tree)\n\n    if parser.undefined_tokens():\n        for tkn in parser.undefined_tokens():\n            print(\"Undefined token: \" + tkn)\n        return None\n\n    return parser.as_python(source)", "response": "Parse the text in infile and save the results in outfile\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a command line parser for the script.", "response": "def genargs() -> ArgumentParser:\n    \"\"\"\n    Create a command line parser\n    :return: parser\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument(\"infile\", help=\"Input JSG specification\")\n    parser.add_argument(\"-o\", \"--outfile\", help=\"Output python file (Default: {infile}.py)\")\n    parser.add_argument(\"-e\", \"--evaluate\", help=\"Evaluate resulting python file as a test\", action=\"store_true\")\n    parser.add_argument(\"-v\", \"--verbose\", help=\"Verbose output\", action=\"store_true\")\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading fname as a module.", "response": "def evaluate(module_name: str, fname: str, verbose: bool):\n    \"\"\"\n    Load fname as a module.  Will raise an exception if there is an error\n    :param module_name: resulting name of module\n    :param fname: name to load \n    \"\"\"\n    if verbose:\n        print(\"Testing {}\".format(fname))\n    spec = importlib.util.spec_from_file_location(module_name, fname)\n    mod = importlib.util.module_from_spec(spec)\n    spec.loader.exec_module(mod)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch(self, request, callback=None, raise_error=True, **kwargs):\n        # accepts request as string then convert it to HTTPRequest\n        if isinstance(request, str):\n            request = HTTPRequest(request, **kwargs)\n\n        try:\n            # The first request calls tornado-client ignoring the\n            # possible exception, in case of 401 response,\n            # renews the access token and replay it\n            response = yield self._authorized_fetch(request,\n                                                    callback,\n                                                    raise_error=False,\n                                                    **kwargs)\n\n            if response.code == BAD_TOKEN:\n                yield self._token_manager.reset_token()\n            elif response.error and raise_error:\n                raise response.error\n            else:\n                raise gen.Return(response)\n\n            # The request with renewed token\n            response = yield self._authorized_fetch(request,\n                                                    callback,\n                                                    raise_error=raise_error,\n                                                    **kwargs)\n            raise gen.Return(response)\n\n        except TokenError as err:\n            yield self._token_manager.reset_token()\n            raise err", "response": "Executes a request by AsyncHTTPClient asynchronously returning an tornado. HTTPResponse."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying for a specific target object in a specific MPC.", "response": "def search_target_obj(target):\n    '''\n    Query for a specific target:\n\n    http://asterank.com/api/skymorph/search?<params>\n    target\tTarget object (lookup in MPC).\n    '''\n    base_url = \"http://asterank.com/api/skymorph/search?\"\n\n    if not isinstance(target, str):\n        raise ValueError(\"The target arg you provided is not the type of str\")\n    else:\n        base_url += \"target=\" + target\n\n    return dispatch_http_get(base_url)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_orbit(**kwargs):\n    '''\n    Query based on orbital elements:\n\n    http://asterank.com/api/skymorph/search_orbit?<params>\n    epoch\tEpoch ([M]JD or ISO)\n    ecc\teccentricity\n    per\tPerihelion distance (AU)\n    per_date\tPerihelion date ([M]JD or ISO)\n    om\tLongitude of ascending node (deg)\n    w\tArgument of perihelion (deg)\n    i\tInclination (deg)\n    H\tAbsolute magnitude\n\n    '''\n\n    base_url = \"http://asterank.com/api/skymorph/search_orbit?\"\n\n    for key in kwargs:\n        base_url += str(key) + \"=\" + kwargs[key] + \"&\"\n\n        # remove the unnecessary & at the end\n    base_url = base_url[:-1]\n\n    return dispatch_http_get(base_url)", "response": "Search for orbital elements in the Asterisk API."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef search_position(**kwargs):\n    '''\n    Query based on position and time (+/- 1 day):\n\n    http://asterank.com/api/skymorph/search_position?<params>\n    ra\tRight ascension (HMS)\n    Dec\tDeclination (DMS)\n    time\tDate and time (UTC)\n    per_date\tPerihelion date ([M]JD or ISO)\n    om\tLongitude of ascending node (deg)\n    w\tArgument of perihelion (deg)\n    i\tInclination (deg)\n    H\tAbsolute magnitude\n\n    '''\n\n    base_url = \"http://asterank.com/api/skymorph/search_position?\"\n\n    for key in kwargs:\n        base_url += str(key) + \"=\" + kwargs[key] + \"&\"\n\n        # remove the unnecessary & at the end\n    base_url = base_url[:-1]\n\n    return dispatch_http_get(base_url)", "response": "This function is used to get the position of a node in the Asterisk database."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntakes a collection object or a filepath to collection object returns a list of dicts that are included trees", "response": "def collection_to_included_trees(collection):\n    \"\"\"Takes a collection object (or a filepath to collection object), returns\n    each element of the `decisions` list that has the decision set to included.\n    \"\"\"\n    if is_str_type(collection):\n        collection = read_as_json(collection)\n    inc = []\n    for d in collection.get('decisions', []):\n        if d['decision'] == 'INCLUDED':\n            inc.append(d)\n    return inc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tree_is_in_collection(collection, study_id=None, tree_id=None):\n    included = collection_to_included_trees(collection)\n    study_id = study_id.strip()\n    tree_id = tree_id.strip()\n    for decision in included:\n        if decision['studyID'] == study_id and decision['treeID'] == tree_id:\n            return True\n    return False", "response": "Takes a collection object or a filepath to collection object returns True if it includes a decision to include the specified tree\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating a configuration dict.", "response": "def validate_config(key: str, config: dict) -> None:\n    \"\"\"\n    Call jsonschema validation to raise JSONValidation on non-compliance or silently pass.\n\n    :param key: validation schema key of interest\n    :param config: configuration dict to validate\n    \"\"\"\n\n    try:\n        jsonschema.validate(config, CONFIG_JSON_SCHEMA[key])\n    except jsonschema.ValidationError as x_validation:\n        raise JSONValidation('JSON validation error on {} configuration: {}'.format(key, x_validation.message))\n    except jsonschema.SchemaError as x_schema:\n        raise JSONValidation('JSON schema error on {} specification: {}'.format(key, x_schema.message))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates an identifier for a callable signal receiver.", "response": "def __make_id(receiver):\n    \"\"\"Generate an identifier for a callable signal receiver.\n\n    This is used when disconnecting receivers, where we need to correctly\n    establish equivalence between the input receiver and the receivers assigned\n    to a signal.\n\n    Args:\n        receiver: A callable object.\n\n    Returns:\n        An identifier for the receiver.\n    \"\"\"\n    if __is_bound_method(receiver):\n        return (id(receiver.__func__), id(receiver.__self__))\n    return id(receiver)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __purge():\n    global __receivers\n    newreceivers = collections.defaultdict(list)\n\n    for signal, receivers in six.iteritems(__receivers):\n        alive = [x for x in receivers if not __is_dead(x)]\n        newreceivers[signal] = alive\n\n    __receivers = newreceivers", "response": "Remove all dead signal receivers from the global receivers collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all signal handlers that are currently alive for the given signal.", "response": "def __live_receivers(signal):\n    \"\"\"Return all signal handlers that are currently still alive for the\n    input `signal`.\n\n    Args:\n        signal: A signal name.\n\n    Returns:\n        A list of callable receivers for the input signal.\n    \"\"\"\n    with __lock:\n        __purge()\n        receivers = [funcref() for funcref in __receivers[signal]]\n\n    return receivers"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef __is_bound_method(method):\n    if not(hasattr(method, \"__func__\") and hasattr(method, \"__self__\")):\n        return False\n\n    # Bound methods have a __self__ attribute pointing to the owner instance\n    return six.get_method_self(method) is not None", "response": "Returns True if the method is a bound method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connect(signal, receiver):\n    __check_receiver(receiver)\n\n    if __is_bound_method(receiver):\n        ref = WeakMethod\n    else:\n        ref = weakref.ref\n\n    with __lock:\n        __purge()\n        __receivers[signal].append(ref(receiver))", "response": "Register receiver method or function as a receiver for the given signal."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisconnect the receiver from the signal identified by signal_id.", "response": "def disconnect(signal, receiver):\n    \"\"\"Disconnect the receiver `func` from the signal, identified by\n    `signal_id`.\n\n    Args:\n        signal: The signal identifier.\n        receiver: The callable receiver to disconnect.\n\n    Returns:\n        True if the receiver was successfully disconnected. False otherwise.\n    \"\"\"\n    inputkey = __make_id(receiver)\n\n    with __lock:\n        __purge()\n        receivers = __receivers.get(signal)\n\n        for idx in six.moves.range(len(receivers)):\n            connected = receivers[idx]()\n\n            if inputkey != __make_id(connected):\n                continue\n\n            del receivers[idx]\n            return True  # receiver successfully disconnected!\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef emit(signal, *args, **kwargs):\n    if signal not in __receivers:\n        return\n\n    receivers = __live_receivers(signal)\n\n    for func in receivers:\n        func(*args, **kwargs)", "response": "Emit a signal by serially calling each registered signal receiver for\n    the signal."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef arrayuniqify(X, retainorder=False):\n    s = X.argsort()\n    X = X[s]\n    D = np.append([True],X[1:] != X[:-1])\n    if retainorder:\n        DD = np.append(D.nonzero()[0],len(X))\n        ind = [min(s[x:DD[i+1]]) for (i,x) in enumerate(DD[:-1])]\n        ind.sort()\n        return ind\n    else:\n        return [D,s]", "response": "This function is a fast uniqify routine for numpy arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef recarrayuniqify(X, retainorder=False):\n    N = X.dtype.names\n    s = X.argsort(order=N)\n    s = s.view(np.ndarray)\n    X = X[s]\n    D = np.append([True],X[1:] != X[:-1])\n    if retainorder:\n        DD = np.append(D.nonzero()[0],len(X))\n        ind = [min(s[x:DD[i+1]]) for (i,x) in enumerate(DD[:-1])]\n        ind.sort()\n        return ind\n    else:\n        return [D,s]", "response": "This function is a fast uniqify routine for numpy record arrays."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef equalspairs(X, Y):\n    T = Y.copy()\n    R = (T[1:] != T[:-1]).nonzero()[0]\n    R = np.append(R,np.array([len(T)-1]))\n    M = R[R.searchsorted(range(len(T)))]\n    D = T.searchsorted(X)\n    T = np.append(T,np.array([0]))\n    M = np.append(M,np.array([0]))\n    A = (T[D] == X) * D\n    B = (T[D] == X) * (M[D] + 1)\n    return [A,B]", "response": "Returns a list of arrays A and B where A and B are arrays of indices in X and Y equal to those in another."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recarrayequalspairs(X,Y,weak=True):\n    if (weak and set(X.dtype.names) != set(Y.dtype.names)) or \\\n       (not weak and X.dtype.names != Y.dtype.names):\n        return [np.zeros((len(X),),int),np.zeros((len(X),),int),None]\n    else:\n        if X.dtype.names != Y.dtype.names:\n            Y = np.rec.fromarrays([Y[a] for a in X.dtype.names], \n                                  names= X.dtype.names)\n        NewX = np.array([str(l) for l in X])\n        NewY = np.array([str(l) for l in Y])\n        s = NewY.argsort()  ; NewY.sort()\n        [A,B] = equalspairs(NewX,NewY)\n        return [A,B,s]", "response": "Returns a list of lists where each element of X is equal to those in another."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a numpy array that contains the elements in X that appear in another numpy array Y.", "response": "def isin(X,Y):\n    \"\"\"\n    Indices of elements in a numpy array that appear in another.\n\n    Fast routine for determining indices of elements in numpy array `X` that \n    appear in numpy array `Y`, returning a boolean array `Z` such that::\n\n            Z[i] = X[i] in Y\n\n    **Parameters**\n\n            **X** :  numpy array\n\n                    Numpy array to comapare to numpy array `Y`.  For each \n                    element of `X`, ask if it is in `Y`.\n\n            **Y** :  numpy array\n\n                    Numpy array to which numpy array `X` is compared.  For each \n                    element of `X`, ask if it is in `Y`.\n\n    **Returns**\n\n            **b** :  numpy array (bool)\n\n                    Boolean numpy array, `len(b) = len(X)`.\n\n    **See Also:**\n\n            :func:`tabular.fast.recarrayisin`, \n            :func:`tabular.fast.arraydifference`\n\n    \"\"\"\n    if len(Y) > 0:\n        T = Y.copy()\n        T.sort()\n        D = T.searchsorted(X)\n        T = np.append(T,np.array([0]))\n        W = (T[D] == X)\n        if isinstance(W,bool):\n            return np.zeros((len(X),),bool)\n        else:\n            return (T[D] == X)\n    else:\n        return np.zeros((len(X),),bool)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a numpy array that contains the indices of elements in numpy record array X that appear in numpy record array Y.", "response": "def recarrayisin(X,Y,weak=True):\n    \"\"\"\n    Indices of elements in a numpy record array (or ndarray with structured \n    dtype) that appear in another.\n\n    Fast routine for determining indices of elements in numpy record array `X` \n    that appear in numpy record array `Y`, returning a boolean array `Z` such \n    that::\n\n            Z[i] = X[i] in Y\n\n    Record array version of func:`tabular.fast.isin`.\n\n    **Parameters**\n\n            **X** :  numpy recarray\n\n                    Numpy recarray to comapare to numpy recarray `Y`.  For each \n                    element of `X`, ask if it is in `Y`.\n\n            **Y** :  numpy recarray\n\n                    Numpy recarray to which numpy recarray `X` is compared.  \n                    For each element of `X`, ask if it is in `Y`.\n\n    **Returns**\n\n            **b** :  numpy array (bool)\n\n                    Boolean numpy array, `len(b) = len(X)`.\n\n    **See Also:**\n\n            :func:`tabular.fast.isin`, :func:`tabular.fast.recarraydifference`\n\n    \"\"\"\n    if (weak and set(X.dtype.names) != set(Y.dtype.names)) or \\\n       (not weak and X.dtype.names != Y.dtype.names):\n        return np.zeros((len(X),),bool)\n    else:\n        if X.dtype.names != Y.dtype.names:\n            Y = np.rec.fromarrays([Y[a] for a in X.dtype.names], \n                                  names=X.dtype.names)\n        NewX = np.array([str(l) for l in X])\n        NewY = np.array([str(l) for l in Y])\n        NewY.sort()\n        return isin(NewX,NewY)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the elements of a numpy array that do not appear in another.", "response": "def arraydifference(X,Y):\n    \"\"\"\n    Elements of a numpy array that do not appear in another.\n\n    Fast routine for determining which elements in numpy array `X`\n    do not appear in numpy array `Y`.\n\n    **Parameters**\n\n            **X** :  numpy array\n\n                    Numpy array to comapare to numpy array `Y`.\n                    Return subset of `X` corresponding to elements not in `Y`.\n\n            **Y** :  numpy array\n\n                    Numpy array to which numpy array `X` is compared.\n                    Return subset of `X` corresponding to elements not in `Y`.\n\n    **Returns**\n\n            **Z** :  numpy array\n\n                    Subset of `X` corresponding to elements not in `Y`.\n\n    **See Also:**\n\n            :func:`tabular.fast.recarraydifference`, :func:`tabular.fast.isin`\n\n    \"\"\"\n    if len(Y) > 0:\n        Z = isin(X,Y)\n        return X[np.invert(Z)]\n    else:\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recarraydifference(X,Y):\n    if len(Y) > 0:\n        Z = recarrayisin(X,Y)\n        return X[np.invert(Z)]\n    else:\n        return X", "response": "Returns the record array that does not appear in another record array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef arraymax(X,Y):\n    Z = np.zeros((len(X),), int)\n    A = X <= Y\n    B = Y < X\n    Z[A] = Y[A]\n    Z[B] = X[B]\n    return Z", "response": "Fast vectorized max function for element - wise comparison of two numpy arrays X and Y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def _seed2did(self) -> str:\n\n        rv = None\n        dids_with_meta = json.loads(await did.list_my_dids_with_meta(self.handle))  # list\n\n        if dids_with_meta:\n            for did_with_meta in dids_with_meta:  # dict\n                if 'metadata' in did_with_meta:\n                    try:\n                        meta = json.loads(did_with_meta['metadata'])\n                        if isinstance(meta, dict) and meta.get('seed', None) == self._seed:\n                            rv = did_with_meta.get('did')\n                    except json.decoder.JSONDecodeError:\n                        continue  # it's not one of ours, carry on\n\n        if not rv:  # seed not in metadata, generate did again on temp wallet\n            temp_wallet = await Wallet(\n                self._seed,\n                '{}.seed2did'.format(self.name),\n                None,\n                {'auto-remove': True}).create()\n\n            rv = temp_wallet.did\n            await temp_wallet.remove()\n\n        return rv", "response": "Derive DID from seed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate wallet as configured and store DID and verkey.", "response": "async def create(self) -> 'Wallet':\n        \"\"\"\n        Create wallet as configured and store DID, or else re-use any existing configuration.\n        Operation sequence create/store-DID/close does not auto-remove the wallet on close,\n        even if so configured.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Wallet.create >>>')\n\n        try:\n            await wallet.create_wallet(\n                config=json.dumps(self.cfg),\n                credentials=json.dumps(self.access_creds))\n            self._created = True\n            LOGGER.info('Created wallet %s', self.name)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletAlreadyExistsError:\n                LOGGER.info('Wallet already exists: %s', self.name)\n            else:\n                LOGGER.debug(\n                    'Wallet.create: <!< indy error code %s on creation of wallet %s',\n                    x_indy.error_code,\n                    self.name)\n                raise\n\n        LOGGER.debug('Attempting to open wallet %s', self.name)\n        self._handle = await wallet.open_wallet(\n            json.dumps(self.cfg),\n            json.dumps(self.access_creds))\n        LOGGER.info('Opened wallet %s on handle %s', self.name, self.handle)\n\n        if self._created:\n            (self._did, self._verkey) = await did.create_and_store_my_did(\n                self.handle,\n                json.dumps({'seed': self._seed}))\n            LOGGER.debug('Wallet %s stored new DID %s, verkey %s from seed', self.name, self.did, self.verkey)\n            await did.set_did_metadata(self.handle, self.did, json.dumps({'seed': self._seed}))\n        else:\n            self._created = True\n            LOGGER.debug('Attempting to derive seed to did for wallet %s', self.name)\n            self._did = await self._seed2did()\n            try:\n                self._verkey = await did.key_for_local_did(self.handle, self.did)\n            except IndyError:\n                LOGGER.debug(\n                    'Wallet.create: <!< no verkey for DID %s on ledger, wallet %s may pertain to another',\n                    self.did,\n                    self.name)\n                raise CorruptWallet(\n                    'No verkey for DID {} on ledger, wallet {} may pertain to another'.format(\n                        self.did,\n                        self.name))\n            LOGGER.info('Wallet %s got verkey %s for existing DID %s', self.name, self.verkey, self.did)\n\n        await wallet.close_wallet(self.handle)\n\n        LOGGER.debug('Wallet.create <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens wallet as configured for later closure via close.", "response": "async def open(self) -> 'Wallet':\n        \"\"\"\n        Explicit entry. Open wallet as configured, for later closure via close().\n        For use when keeping wallet open across multiple calls.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Wallet.open >>>')\n\n        if not self.created:\n            LOGGER.debug('Wallet.open: <!< absent wallet %s', self.name)\n            raise AbsentWallet('Cannot open wallet {}: not created'.format(self.name))\n\n        self._handle = await wallet.open_wallet(\n            json.dumps(self.cfg),\n            json.dumps(self.access_creds))\n        LOGGER.info('Opened wallet %s on handle %s', self.name, self.handle)\n\n        self._did = await self._seed2did()\n        self._verkey = await did.key_for_local_did(self.handle, self.did)\n        LOGGER.info('Wallet %s got verkey %s for existing DID %s', self.name, self.verkey, self.did)\n\n        LOGGER.debug('Wallet.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def remove(self) -> None:\n\n        LOGGER.debug('Wallet.remove >>>')\n\n        try:\n            LOGGER.info('Removing wallet: %s', self.name)\n            await wallet.delete_wallet(json.dumps(self.cfg), json.dumps(self.access_creds))\n        except IndyError as x_indy:\n            LOGGER.info('Abstaining from wallet removal; indy-sdk error code %s', x_indy.error_code)\n\n        LOGGER.debug('Wallet.remove <<<')", "response": "Remove serialized wallet if it exists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple for whether or not pushes succeed and the entire object returned by a call to push_failure on the phylesystem - api.", "response": "def push_failure_state(self):\n        \"\"\"Returns a tuple: the boolean for whether or not pushes succeed, and the\n        entire object returned by a call to push_failure on the phylesystem-api.\n        This should only be called with wrappers around remote services (RuntimeError\n        will be raised if you call this with a local wrapper.\n        \"\"\"\n        if self._src_code == _GET_LOCAL:\n            raise RuntimeError('push_failure_state only pertains to work with remote phyleysystem instances')\n        r = self._remote_push_failure()\n        return r['pushes_succeeding'], r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a specific entry in the database.", "response": "def get(self, study_id, content=None, schema=None, **kwargs):\n        \"\"\"Syntactic sugar around to make it easier to get fine-grained access\n        to the parts of a file without composing a PhyloSchema object.\n        Possible invocations include:\n            w.get('pg_10')\n            w.get('pg_10', 'trees')\n            w.get('pg_10', 'trees', format='nexus')\n            w.get('pg_10', tree_id='tree3')\n        see:\n        \"\"\"\n        if isinstance(study_id, TreeRef):\n            return self.get(study_id=study_id.study_id,\n                            tree_id=study_id.tree_id,\n                            content=content,\n                            schema=schema,\n                            **kwargs)\n        if schema is None:\n            schema = create_content_spec(content=content,\n                                         repo_nexml2json=self.repo_nexml2json,\n                                         **kwargs)\n        r = self.get_study(study_id, schema)\n        if schema.content == 'study' and schema.format_str == 'nexson':\n            return r\n        if isinstance(r, dict) and ('data' in r):\n            return r['data']\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a delimited text file into a numpy record array.", "response": "def loadSV(fname, shape=None, titles=None, aligned=False, byteorder=None,  \n           renamer=None, **kwargs):\n    \"\"\"\n    Load a delimited text file to a numpy record array.\n\n    Basically, this function calls loadSVcols and combines columns returned by \n    that function into a numpy ndarray with stuctured dtype.  Also uses and \n    returns metadata including column names, formats, coloring, &c. if these \n    items are determined during the loading process.\n\n    **Parameters**\n\n        **fname** :  string or file object\n\n            Path (or file object) corresponding to a separated variable\n            (CSV) text file.\n\n         **names** : list of strings\n                \n            Sets the names of the columns of the resulting tabarray.   If \n            not specified, `names` value is determined first by looking for \n            metadata in the header of the file, and if that is not found, \n            are assigned by NumPy's `f0, f1, ... fn` convention.  See \n            **namesinheader** parameter below.\n                \n        **formats** :  string or list of strings\n            \n            Sets the datatypes of the columns.  The value of `formats` can \n            be a list or comma-delimited string of values describing values \n            for each column (e.g. \"str,str,int,float\" or \n            [\"str\", \"str\", \"int\", \"float\"]), a single value to apply to all \n            columns, or anything that can be used in numpy.rec.array \n            constructor.   \n                \n            If the **formats** (or **dtype**) parameter are not  specified, \n            typing is done by inference.  See **typer** parameter below.  \n                                    \n        **dtype** : numpy dtype object\n             \n            Sets the numpy dtype of the resulting tabarray, combining column \n            format and column name information.  If dtype is set, any \n            **names** and **formats** specifications will be overriden.  If \n            the **dtype** (or **formats**) parameter are not  specified, \n            typing is done by inference.  See **typer** parameter below.   \n\n        The **names**, **formats** and **dtype** parameters duplicate \n        parameters of the NumPy record array creation inferface.  Additional \n        paramters of the NumPy inferface that are passed through are \n        **shape**, **titles**, **byteorder** and **aligned** (see NumPy \n        documentation for more information.)\n\n    **kwargs**: keyword argument dictionary of variable length\n\n        Contains various parameters to be passed down to loadSVcols.  These may \n        include  **skiprows**, **comments**, **delimiter**, **lineterminator**, \n        **uselines**, **usecols**, **excludecols**, **metametadata**, \n        **namesinheader**,**headerlines**, **valuefixer**, **linefixer**, \n        **colfixer**, **delimiter_regex**, **inflines**, **typer**, \n        **missingvalues**, **fillingvalues**, **verbosity**, and various CSV \n        module parameters like **escapechar**, **quoting**, **quotechar**, \n        **doublequote**, **skipinitialspace**.              \n\n    **Returns**\n\n        **R** :  numpy record array\n\n            Record array constructed from data in the SV file\n\n        **metadata** :  dictionary\n\n            Metadata read and constructed during process of reading file.\n\n    **See Also:**\n\n            :func:`tabular.io.loadSVcols`, :func:`tabular.io.saveSV`, \n            :func:`tabular.io.DEFAULT_TYPEINFERER`\n\n    \"\"\"    \n    [columns, metadata] = loadSVcols(fname, **kwargs)\n    \n    if 'names' in metadata.keys():\n        names = metadata['names']\n    else:\n        names = None\n \n    if 'formats' in metadata.keys():\n        formats = metadata['formats']\n    else:\n        formats = None\n    \n    if 'dtype' in metadata.keys():\n        dtype = metadata['dtype']\n    else:\n        dtype = None\n \n    if renamer is not None:\n        print 'Trying user-given renamer ...'\n        renamed = renamer(names)\n        if len(renamed) == len(uniqify(renamed)):\n            names = renamed\n            print '''... using renamed names (original names will be in return \n                     metadata)'''\n        else:\n            print '... renamer failed to produce unique names, not using.'\n            \n    if names and len(names) != len(uniqify(names)):\n        print 'Names are not unique, reverting to default naming scheme.'\n        names = None\n\n\n    return [utils.fromarrays(columns, type=np.ndarray, dtype=dtype, \n                             shape=shape, formats=formats, names=names, \n                             titles=titles, aligned=aligned, \n                             byteorder=byteorder), metadata]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef loadSVcols(fname, usecols=None, excludecols=None, valuefixer=None, \n               colfixer=None, missingvalues=None, fillingvalues=None,\n               typeinferer=None, **kwargs):\n    \"\"\"\n    Load a separated value text file to a list of column arrays.\n\n    Basically, this function calls loadSVrecs, and transposes the string-valued \n    row data returned by that function into a Python list of numpy arrays \n    corresponding to columns, each a uniform Python type (int, float, str).  \n    Also uses and returns metadata including column names, formats, coloring, \n    &c. if these items  are determined during the loading process.\n\n    **Parameters**\n\n        **fname** :  string or file object\n\n            Path (or file object) corresponding to a separated variable (CSV) \n            text file.\n                    \n        **usecols** :  sequence of non-negative integers or strings, optional\n\n            Only the columns in *usecols* are loaded and processed.  Columns can \n            be described by number, with 0 being the first column; or if name \n            metadata is present, then by name; or, if color group information is \n            present in the file, then by color group name.  Default is None, \n            e.g. all columns are loaded.\n \n        **excludecols** :  sequence of non-negative integers or strings, optional\n\n            Converse of **usecols**, e.g. all columns EXCEPT those listed \n            will be loaded. \n            \n        **valuefixer**  :  callable, or list or dictionary of callables, optional\n    \n            These callable(s) are applied to every value in each field.  The \n            application is done after line strings are loaded and split into \n            fields, but before any typing or missing-value imputation is done.  \n            The purpose of the **valuefixer** is to prepare column \n            values for typing and imputation.  The valuefixer callable can \n            return a string or a python object.  If `valuefixer` is a single \n            callable, then that same callable is applied to values in all \n            column; if it is a dictionary, then the keys can be either \n            numbers or names and the value for the key will be applied to \n            values in the corresponding column with that name or number; if \n            it is a list, then the list elements must be in 1-to-1 \n            correspondence with the loaded columns, and are applied to each \n            respectively.\n               \n        **colfixer** : callable, or list or dictionary of callables, optional\n\n            Same as **valuefixer**, but instead of being applied to \n            individual values, are applied to whole columns (and must return \n            columns or numpy arrays of identical length).  Like valuefixer, \n            colfixer callable(s) are applied before typing and missing-value \n            imputation.  \n                \n        **missingvalues** : string, callable returning string, or list or dictionary of strings or string-valued callable\n            \n            String value(s) to consider as \"missing data\" and to be replaced \n            before typing is done.   If specified as a callable, the \n            callable will be applied to the column(s) to determine missing \n            value.  If specified as a dictionary, keys are expected to be \n            numbers of names of columns, and values are individual missing \n            values for those columns (like **valuefixer** inferface).                   \n                \n        **fillingvalues** : string, pair of strings, callable returning string, or list or dictionary of strings or string-valued callable\n            \n            Values to be used to replace missing data before typing is done.   \n            If specified as a  single non-callable, non-tuple value, this \n            value is used to replace all missing data.  If specified as a \n            callable, the callable is applied to the column and returns the \n            fill value (e.g. to allow the value to depend on the column \n            type).  If specified as a pair of values, the first value acts \n            as the missing value and the second as the value to replace \n            with.  If a dictionary or list of values, then values are \n            applied to corresponding columns.  \n    \n        NOTE:  all the **missingvalues** and **fillingvalues** \n        functionalities can be replicated (and generalized) using the \n        **valuefixer** or **colfixer** parameters, by specifying function(s) \n        which identify and replace missing values.  While more limited, \n        using **missingvalues** and **fillingvalues**  interface is easier \n        and gives better performance.   \n    \n        **typer** : callable taking python list of strings (or other values) \n        and returning 1-d numpy array; or list dictionary of such callables  \n            \n           Function used to infer type and convert string lists into typed \n           numpy arrays, if no format information has been provided.  When \n           applied at all, this function is applied after string have been \n           loaded and split into fields.  This function is expected to \n           impute missing values as well, and will override any setting of \n           **missingvalues** or **fillingvalues**.  If a callable is passed,  \n           it is used as typer for all columns, while if a dictionary (or \n           list) of callables is passed, they're used on corresponding \n           columns.  If needed (e.g. because formatting information hasn't \n           been supplied) but **typer** isn't specified (at least, for a \n           given column), the constructor defaults to using the \n           `utils.DEFAULT_TYPEINFERER` function.          \n                          \n        **kwargs**: keyword argument dictionary of variable length\n         \n            Contains various parameters to be passed on to loadSVrecs, \n            including **skiprows**, **comments**, **delimiter**, \n            **lineterminator**, **uselines**,  **metametadata**, \n            **namesinheader**,**headerlines**, **linefixer**,  \n            **delimiter_regex**, **inflines**, **verbosity**, and various \n            CSV module parameters like **escapechar**, **quoting**, \n            **quotechar**, **doublequote**, **skipinitialspace**. \n            \n    **Returns**\n\n        **columns** :  list of numpy arrays\n\n            List of arrays corresponding to columns of data.\n\n        **metadata** :  dictionary\n\n            Metadata read and constructed during process of reading file.\n\n    **See Also:**\n\n            :func:`tabular.io.loadSV`, :func:`tabular.io.saveSV`, \n            :func:`tabular.io.DEFAULT_TYPEINFERER`\n\n    \"\"\"\n    [records, metadata] = loadSVrecs(fname, **kwargs)\n     \n    lens = np.array([len(r) for r in records])\n    assert (lens == lens[0]).all(), 'Not all records have same number of fields'\n\n    l0 = lens[0]\n    processmetadata(metadata,items='types,formats', ncols = l0)\n\n    if usecols is not None:\n        getcols = [i if i >= 0 else l0 + i for i in usecols \n                   if isinstance(i, int)]\n        if 'names' in metadata.keys():\n            names = metadata['names']\n            getcols += [names.index(c) for c in usecols if c in names]\n            if 'coloring' in metadata.keys():\n                coloring = metadata['coloring']\n                for c in usecols:\n                    if c in coloring.keys():\n                        getcols += [names.index(n) for n in coloring[c]]\n        getcols = uniqify(getcols)\n    else:\n        if 'names' in metadata.keys():\n            names = metadata['names']\n            getcols = range(len(names))\n        else:\n            getcols = range(l0)\n        if excludecols is not None:\n            dontget = [i if i >= 0 else l0 + i for i in excludecols \n                       if isinstance(i, int)]\n            if 'names' in metadata.keys():\n                dontget += [names.index(c) for c in excludecols if c in names]\n                if 'coloring' in metadata.keys():\n                    coloring = metadata['coloring']\n                    for c in excludecols:\n                        if c in coloring.keys():\n                            dontget += [names.index(n) for n in coloring[c]]\n            getcols = list(set(getcols).difference(dontget))\n    \n    getcols.sort()\n    if max(getcols) > l0:\n        bad = [i for i in getcols if i >= l0]\n        getcols = [i for i in getcols if i < l0]\n        print 'Too many column names. Discarding columns,', bad\n        \n    metadatacolthreshold(metadata,getcols)\n \n    if 'formats' in metadata.keys() or 'types' in metadata.keys():\n        if 'formats' in metadata.keys():\n            formats = metadata['formats']\n        else:\n            formats = metadata['types']\n        formats = dict(zip(getcols,formats))\n    else:\n        formats = dict([(j,None) for j in getcols])\n              \n    if 'names' in metadata.keys():\n        names = metadata['names']\n    else:\n        names = None\n        \n    valfix = utils.processvfd(valuefixer, numbers=getcols, names=names)\n    colfix = utils.processvfd(colfixer, numbers=getcols, names=names)\n    missval = utils.processvfd(missingvalues, numbers=getcols, names=names)\n    fillval = utils.processvfd(fillingvalues, numbers=getcols, names=names)\n    typer = utils.processvfd(typeinferer, numbers=getcols, names=names)\n      \n    return [[preparecol(records, j, formats[j], valfix[j], colfix[j],\n             missval[j], fillval[j], typer[j]) for j in getcols], metadata]", "response": "Loads a separated value text file into a list of column arrays."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loadSVrecs(fname, uselines=None, skiprows=0, linefixer=None, \n               delimiter_regex=None, verbosity=DEFAULT_VERBOSITY, **metadata):\n    \"\"\"\n    Load a separated value text file to a list of lists of strings of records.\n\n    Takes a tabular text file with a specified delimeter and end-of-line \n    character, and return data as a list of lists of strings corresponding to \n    records (rows).  Also uses and returns metadata (including column names, \n    formats, coloring, &c.) if these items are determined during the loading \n    process.   \n\n    **Parameters**\n\n        **fname** :  string or file object\n\n            Path (or file object) corresponding to a separated variable\n            (CSV) text file.\n \n        **delimiter** : single-character string\n        \n            When reading text file, character to use as delimiter to split \n            fields.  If not specified, the delimiter is determined first by \n            looking for special-format metadata specifying the delimiter, and \n            then if no specification is found, attempts are made to infer \n            delimiter from file contents.  (See **inflines** parameter below.)  \n            \n        **delimiter_regex** : regular expression (compiled or in string format)                    \n         \n            Regular expression to use to recognize delimiters, in place of a \n            single character.  (For instance, to have whitespace delimiting, \n            using delimiter_regex = '[\\s*]+')\n                         \n        **lineterminator** : single-character string\n        \n            Line terminator to use when reading in using SVfile.\n            \n        **skipinitialspace** : boolean\n        \n            If true, strips whitespace following the delimiter from field.   \n            \n       The **delimiter**, **linterminator** and **skipinitialspace** \n       parameters are passed on as parameters to the python CSV module, which is \n       used for reading in delimited text files.  Additional parameters from \n       that interface that are replicated in this constructor include \n       **quotechar**, **escapechar**, **quoting**, **doublequote** and \n       **dialect** (see CSV module documentation for more information).\n\n        **skiprows** :  non-negative integer, optional\n\n            When reading from a text file, the first `skiprows` lines are \n            ignored.  Default is 0, e.g no rows are skipped. \n\n        **uselines** : pair of non-negative integer, optional\n        \n            When reading from a text file, range of lines of data to load.  (In \n            contrast to **skiprows**, which specifies file rows to ignore \n            before looking for header information, **uselines** specifies which \n            data (non-header) lines to use, after header has been striped and \n            processed.)  See **headerlines** below.\n            \n        **comments** : single-character string, optional\n            \n            When reading from a text file, character used to distinguish header \n            lines.  If specified, any lines beginning with this character at the \n            top of the file are assumed to contain header information and not \n            row data. \n      \n        **headerlines** : integer, optional\n\n            When reading from a text file, the number of lines at the top of the \n            file (after the first  `skiprows` lines) corresponding to the header \n            of the file, where metadata can be found.  Lines after headerlines \n            are assumed to contain row contents.  If not specified, value is \n            determined first by looking for special metametadata  in first line \n            of file (see Tabular reference documentation for more information \n            about this), and if no such metadata is found, is inferred by \n            looking at file contents.    \n            \n        **namesinheader** : Boolean, optional\n\n            When reading from a text file, if `namesinheader == True`, then \n            assume the column names are in the last header line (unless \n            overridden by existing metadata or metametadata directive).  Default \n            is True.                        \n            \n        **linefixer** : callable, optional\n\n           This callable is applied to every line in the file.  If specified, \n           the called is applied directly to the strings in the file, after \n           they're split in lines but before they're split into fields.  The \n           purpose is to make lines with errors or mistakes amenable to \n           delimiter inference and field-splitting. \n            \n        **inflines** :  integer, optional\n        \n            Number of lines of file to use as sample data when inferring \n            delimiter and header.   \n\n        **metametadata** :  dictionary of integers or pairs of integers\n            \n            Specifies supplementary metametadata information for use \n            with SVfile loading.  See Tabular reference documentation for more \n            information\n            \n    **Returns**\n\n            **records** :  list of lists of strings\n\n                List of lists corresponding to records (rows) of data.\n\n            **metadata** :  dictionary\n\n                Metadata read and constructed during process of reading file.\n\n    **See Also:**\n\n            :func:`tabular.io.loadSV`, :func:`tabular.io.saveSV`, \n            :func:`tabular.io.DEFAULT_TYPEINFERER`\n\n    \"\"\"\n    if delimiter_regex and isinstance(delimiter_regex, types.StringType):\n        import re\n        delimiter_regex = re.compile(delimiter_regex) \n   \n    [metadata, inferedlines, WHOLETHING] = getmetadata(fname, skiprows=skiprows,\n                                                linefixer=linefixer, \n                                                delimiter_regex=delimiter_regex, \n                                                verbosity=verbosity, **metadata)\n\n    if uselines is None:\n        uselines = (0,False)\n    \n    if is_string_like(fname):\n        fh = file(fname, 'rU')\n    elif hasattr(fname, 'readline'):\n        fh = fname\n    else:\n        raise ValueError('fname must be a string or file handle') \n \n    for _ind in range(skiprows+uselines[0] + metadata['headerlines']):\n        fh.readline()\n        \n    if linefixer or delimiter_regex:\n        fh2 = tempfile.TemporaryFile('w+b')\n        F = fh.read().strip('\\n').split('\\n')\n        if linefixer:\n            F = map(linefixer,F)\n        if delimiter_regex:\n            F = map(lambda line: \n                    delimiter_regex.sub(metadata['dialect'].delimiter, line), F)       \n        fh2.write('\\n'.join(F))        \n        fh2.seek(0)\n        fh = fh2        \n\n    reader = csv.reader(fh, dialect=metadata['dialect'])\n\n    if uselines[1]:\n        linelist = []\n        for ln in reader:\n            if reader.line_num <= uselines[1] - uselines[0]:\n                linelist.append(ln)\n            else:\n                break\n    else:\n        linelist = list(reader)\n      \n    fh.close()\n\n    if linelist[-1] == []:\n        linelist.pop(-1)\n\n    return [linelist,metadata]", "response": "Loads a delimited value text file into a list of strings of records."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef saveSV(fname, X, comments=None, metadata=None, printmetadict=None,\n                   dialect=None, delimiter=None, doublequote=True, \n                   lineterminator='\\n', escapechar = None, \n                   quoting=csv.QUOTE_MINIMAL, quotechar='\"', \n                   skipinitialspace=False, stringifier=None,\n                   verbosity=DEFAULT_VERBOSITY):\n    \"\"\"\n    Save a tabarray to a separated-variable (CSV) file.\n\n    **Parameters**\n\n        **fname** :  string\n\n            Path to a separated variable (CSV) text file.\n\n        **X** :  tabarray\n\n            The actual data in a :class:`tabular.tab.tabarray`.\n\n        **comments** :  string, optional\n\n            The character to be used to denote the start of a header (non-data) \n            line, e.g. '#'.  If not specified, it is determined according to the \n            following rule:  '#' if `metadata` argument is set, otherwise ''.\n\n        **delimiter** :  string, optional\n\n            The character to beused to separate values in each line of text, \n            e.g. ','.  If not specified, by default, this is inferred from \n            the file extension: if the file ends in `.csv`, the delimiter is \n            ',', otherwise it is '\\\\t.'\n\n        **linebreak** :  string, optional\n\n            The string separating lines of text.  By default, this is assumed to \n            be '\\\\n', and can also be set to be '\\\\r' or '\\\\r\\\\n'.\n\n        **metadata** :  list of strings or Boolean, optional\n\n            Allowed values are True, False, or any sublists of the list \n            `['names', 'formats', 'types', 'coloring', 'dialect']`.  These \n            keys indicate what special metadata is printed in the header.\n\n            * If a sublist of \n             `['names', 'formats', 'types', 'coloring', 'dialect']`, then the \n             indicated types of metadata are written out.  \n\n            * If `True`, this is the same as \n              `metadata = ['coloring', 'types', 'names','dialect']`, e.g. as \n               many types of metadata as this algorithm currently knows how to \n               write out. \n\n            * If 'False', no metadata is printed at all, e.g. just the data.\n                    \n            * If `metadata` is not specified, the default is `['names']`, that \n              is, just column names are written out.\n                        \n        **printmetadict** :  Boolean, optional\n\n            Whether or not to print a string representation of the \n            `metadatadict` in the first line of the header.\n\n            If `printmetadict` is not specified, then:\n\n            * If `metadata` is specified and is not `False`, then\n              `printmetadata` defaults to `True`.\n\n            * Else if `metadata` is `False`, then `printmetadata` defaults \n              to `False`.\n\n            * Else `metadata` is not specified, and `printmetadata` defaults \n              to `False`.\n\n            See the :func:`tabular.io.loadSV` for more information about \n            `metadatadict`.\n            \n        **stringifier** : callable \n        \n            Callable taking 1-d numpy array and returning Python list of strings \n            of same length, or dictionary or tuple of such callables.  \n                    \n            If specified, the callable will be applied to each column, and the \n            resulting list of strings will be written to the file.  If \n            specified as a list or dictionary of callables, the functions will \n            be applied to correponding columns.  The default used if \n            **stringifier** is not specified, is `tb.utils.DEFAULT_STRINGIFIER`, \n            which merely passes through string-type columns, and converts \n            numerical-type columns directly to corresponding strings with NaNs \n            replaced with blank values.  The main purpose of specifying a \n            non-default value is to encode numerical values in various string \n            encodings that might be used required for other applications like \n            databases.                    \n                               \n            NOTE:  In certain special circumstances (e.g. when the \n            lineterminator or delimiter character appears in a field of the \n            data), the Python CSV writer is used to write out data.  To allow \n            for control of the operation of the writer in these circumstances, \n            the following other parameters replicating the interface of the CSV \n            module are also valid, and values will be passed through:  \n            **doublequote**, **escapechar**, **quoting**, **quotechar**, and \n            **skipinitialspace**.  (See Python CSV module documentation for more \n            information.)     \n\n    **See Also:**\n\n            :func:`tabular.io.loadSV`\n\n    \"\"\"    \n    if metadata is None:\n        metakeys = ['names']\n        if printmetadict is None:\n            printmetadict = False\n            if verbosity > 8:\n                print '''Defaulting to not printing out the metametadata \n                         dictionary line.'''\n        if comments is None:\n            comments = ''\n            if verbosity > 8:\n                print 'Defaulting empty comment string.'\n        if verbosity > 7:\n            print 'Defaulting to writing out names metadata.'\n    elif metadata is True:\n        \n        metakeys = defaultmetadatakeys(X)\n        \n        if printmetadict is None:\n            printmetadict = True\n            if verbosity > 8:\n                print '''Defaulting to printing out the metametadata dictionary \n                         line.'''\n        if comments is None:\n            comments = ''\n            if verbosity > 8:\n                print 'Defaulting empty comment string.'            \n        if verbosity >= 5:\n            print 'Writing out all present metadata keys ... '\n    elif metadata is False:\n        metakeys = []\n        printmetadict = False\n        comments = ''\n        if verbosity >= 5:\n            print 'Writing out no metadata at all.'\n    else:\n        metakeys = metadata\n        if printmetadict is None:\n            if metakeys == []:\n                printmetadict = False\n            else:\n                printmetadict = True\n        if comments is None:\n            comments = ''\n        if verbosity >= 5:\n            print '''Using user-specified metadata keys to contol metadata \n                     writing.'''\n            \n    assert lineterminator in ['\\r','\\n','\\r\\n'], '''lineterminator must be one \n                                              of ''' + repr( ['\\r','\\n','\\r\\n'])\n    dialect = getdialect(fname, dialect, delimiter, lineterminator, doublequote, \n                         escapechar, quoting, quotechar, skipinitialspace)\n    delimiter = dialect.delimiter     \n    \n    if 6 > verbosity > 2:\n        print 'Using delimiter ', repr(delimiter)\n    elif verbosity >= 6:\n        print 'Using dialect with values:', repr(printdialect(dialect))\n            \n    metadata = getstringmetadata(X,metakeys,dialect)\n    \n    metametadata = {}\n    v = 1\n    for k in metakeys:\n        if k in metadata.keys():\n            nl = len(metadata[k].split(lineterminator))\n            metametadata[k] = v if nl == 1 else (v, v + nl)\n            v = v + nl\n\n    F = open(fname,'wb')\n\n    if printmetadict is True:\n        line = \"metametadata=\" + repr(metametadata)\n        F.write(comments + line + lineterminator)\n\n    for k in metakeys:\n        if k in metadata.keys():\n            for line in metadata[k].split(lineterminator):\n                F.write(comments + line + lineterminator)\n        \n    Write(X, F, dialect, stringifier=stringifier)\n    \n    F.close()", "response": "Save a tabarray to a separated - variable file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef inferdialect(fname=None, datalines=None, delimiter_regex=None, \n                 verbosity=DEFAULT_VERBOSITY):\n    \"\"\"\n    Attempts to convert infer dialect from csv file lines. \n    \n    Essentially a small extension of the \"sniff\" function from Python CSV \n    module.   csv.Sniffer().sniff attempts to infer the delimiter from a \n    putative delimited text file by analyzing character frequencies.  This \n    function adds additional analysis in which guesses are checked again the \n    number of entries in each line that would result from splitting relative to \n    that guess. If no plausable guess if found, delimiter is inferred from file \n    name  ('csv' yields ',', everything else yields '\\t'.)\n    \n    **Parameters** \n    \n        **fname** : pathstring\n        \n            Name of file.\n        \n        **datalines** : list of strings\n        \n            List of lines in the data file.\n            \n        **lineterminator** : single-character string\n        \n            Line terminator to join split/join line strings.\n        \n    **Returns**\n    \n        csv.Dialect obejct      \n    \n    \"\"\"\n    if datalines is None:\n        if is_string_like(fname):\n            fh = file(fname, 'rU')\n        elif hasattr(fname, 'readline'):\n            fh = fname\n        else:\n            raise ValueError('fname must be a string or file handle')\n\n        datalines = fh.read().strip().split('\\n')\n        fh.close() \n    \n    if delimiter_regex:\n        matches = []\n        for l in datalines[:10]:\n            matches += delimiter_regex.findall(l)\n        poss = {}\n        for m in matches:\n            for x in set(m):\n                poss[x] = m.count(x) + (poss[x] if x in poss.keys() else 0) \n        MaxVal = max(poss.values())\n        assert MaxVal > 0, 'delimiter_regex found no matches'\n        amax = [x for x in poss.keys() if poss[x] == MaxVal][0]\n        return csv.Sniffer().sniff(amax)\n\n    else:\n        if not is_string_like(fname):\n            fname = None\n    \n        tries = [10, 30, 60, 100, 200, 400, 800]\n    \n        if len(datalines) > 100:\n            starts = [int(len(datalines) / 5) * i for i in range(5)]\n        else:\n            starts = [0, int(len(datalines) / 2)]\n            \n        G = []\n        for s in starts:\n            for t in [tt for (i, tt) in enumerate(tries) \n                      if i == 0 or s + tries[i-1] <= len(datalines)]:\n                try:\n                    g = csv.Sniffer().sniff('\\n'.join(datalines[s:(s+t)]))\n                except:\n                    pass\n                else:\n                    G += [g]\n                    break\n    \n                    \n        delims = [g.delimiter for g in G]   \n        G = [g for (i, g) in enumerate(G) if g.delimiter not in delims[:i]]\n        V = []\n        for g in G:\n            lvec = np.array([len(r) for r in \n                             list(csv.reader(datalines[:1000], dialect=g))])  \n            V += [lvec.var()]\n    \n    \n        if len(G) > 0:\n            V = np.array(V)\n            if V.min() > 0:\n                fnamedelim = inferdelimiterfromname(fname)\n                if fnamedelim not in delims:\n                    fnamevar = np.array([len(r) for r in \n                                         list(csv.reader(datalines[:1000], \n                                                  delimiter=fnamedelim))]).var()\n                    if fnamevar < V.min():\n                        return csv.Sniffer().sniff(fnamedelim)\n            return G[V.argmin()]\n        else:\n            if verbosity > 2:\n                print 'dialect inference failed, infering dialect to be', inferdelimiterfromname(fname) , 'from filename extension.'\n            return csv.Sniffer().sniff(inferdelimiterfromname(fname))", "response": "Infer the dialect from a CSV file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef readstoredmetadata(fname, skiprows=0, linenumber=None, comments='#', \n                       metametadata=None, verbosity=DEFAULT_VERBOSITY):\n    \"\"\"\n    Read metadata from a delimited text file.\n    \n    \"\"\"\n    if is_string_like(fname):\n        fh = file(fname, 'rU')\n    elif hasattr(fname, 'readline'):\n        fh = fname\n    else:\n        raise ValueError('fname must be a string or file handle')\n       \n    if not metametadata:\n        for _ind in range(skiprows):\n            fh.readline()\n\n        phlines = []\n        if linenumber is None:\n            if comments:\n                for line in fh:\n                    if not line.startswith(comments):\n                        if len(phlines) == 0:\n                            phlines = [line]\n                        break\n                    else:\n                        phlines.append(line)\n                if len(phlines) == 1:\n                    if verbosity >= 10:\n                        print '''Looking for metametadata on line 0 \n                                 (no comment lines present).'''\n                else:\n                    if verbosity >= 9:\n                        print '''Searching for metametadata lines up to \n                                 and including line %d where comments \n                                 end.''' % (len(phlines) - 1)\n            else:\n                phlines = [fh.readline()]\n                if verbosity >=9:\n                    print '''No comments found, looking for metametadata on \n                             line 0.'''\n        else:\n            for _ind in range(linenumber):\n                fh.readline()\n            phlines = [fh.readline()]\n                               \n        metametadata = None\n        for (ln, metametaline) in enumerate(phlines):\n            s = re.compile(r'metametadata[\\s]*=[\\s]*{').search(metametaline)\n            if s:\n                l = s.start()\n                if len(uniqify(metametaline[:l])) <= 1:\n                    metametaline = metametaline[l:].rstrip()\n                    try:\n                        X = compiler.parse(metametaline)\n                    except SyntaxError:\n                        pass\n                    else:\n                        if IsMetaMetaDict(X):\n                            exec metametaline\n                            if verbosity > 6:\n                                print 'Found valid metametadata at line', ln, 'in file.  Metametadata is:', metametadata\n                            break\n   \n    if metametadata:\n        \n        metadata = {}\n        metadata['metametadata'] = metametadata\n        Tval = max([v if isinstance(v,int) else max(v) \n                    for v in metametadata.values()])\n        fh = file(fname, 'rU')\n        data = [fh.readline() for _ind in range(Tval + 1 + skiprows)][skiprows:]\n \n        if (max([v if isinstance(v,int) else max(v) \n                for v in  metametadata.values()]) < len(data)):\n            for n in metametadata.keys():\n                [s, e] = [metametadata[n], metametadata[n]+1] \\\n                          if isinstance(metametadata[n],int) \\\n                          else [metametadata[n][0],metametadata[n][1]]\n                metadata[n] = ''.join(data[s:e]).strip('\\n')\n            processmetadata(metadata, comments=comments, verbosity=verbosity)\n        \n            return metadata", "response": "Reads the metadata from a delimited text file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing Metadata from stored (or \"packed\") state to functional state. Metadata can come be read from a file \"packed\" in various ways, e.g. with a string representation of a dialect or coloring dictionary. This function \"unpacks\" the stored metadata into useable Python objects. It consists of a list of quasi-modular parts, one for each type of recognized metadata. **Parameters** **metadata** : dictionary This argument is a dictionary whose keys are strings denoting different kinds of metadata (e.g. \"names\" or \"formats\") and whose values are the metadata of that type. The metadata dictionary is modified IN-PLACE by this function. **items** : string or list of strings, optional The items arguments specifies which metadata keys are to be processed. E.g. of items = 'names,formats', then the \"names\" metadata and \"formats\" metadata will be processed, but no others. Note however, that sometimes, the processing of one type of metadata requires that another be processed first, e.g. \"dialect\" must processed into an actual CSV.dialect object before \"names\" is processed. (The processed of \"names\" metadata involves splitting the names metadata string into a list, using the delimiter. This delimiter is part of the dialect object.) In these cases, if you call processmetadata on one item before its requirements are processed, nothing will happen. **comments** : single-character string, optional The comments character is used to process many pieces of metadata, e.g. it is striped of the left side of names and formats strings before splitting on delimiter. **verbosity** : integer, optional Determines the level of verbosity in the printout of messages during the running of the procedure. **Returns** Nothing.", "response": "def processmetadata(metadata, items=None, comments=None, delimiter_regex=None,\n                    ncols=None, verbosity=DEFAULT_VERBOSITY):\n    \"\"\"\n    Process Metadata from stored (or \"packed\") state to functional state.\n\n    Metadata can come be read from a file \"packed\" in various ways, \n    e.g. with a string representation of a dialect or coloring dictionary.  \n    This function \"unpacks\" the stored metadata into useable Python\n    objects.  It consists of a list of quasi-modular parts, one for each \n    type of recognized metadata.\n\n    **Parameters**\n\n        **metadata** : dictionary\n\n            This argument is a dictionary whose keys are strings denoting\n            different kinds of metadata (e.g. \"names\" or \"formats\") and whose \n            values are the metadata of that type.  The metadata dictionary is \n            modified IN-PLACE by this function.\n\n        **items** : string or list of strings, optional\n\n            The items arguments specifies which metadata keys are to be \n            processed.  E.g. of items = 'names,formats', then the \"names\" \n            metadata and \"formats\" metadata will be processed, but no others.\n            Note however, that sometimes, the processing of one type of metadata \n            requires that another be processed first, e.g. \"dialect\" must \n            processed into an actual CSV.dialect object before \"names\"  is \n            processed.  (The processed of \"names\" metadata involves splitting \n            the names metadata string into a list, using the delimiter.  This \n            delimiter is part of the dialect object.)   In these cases, if you \n            call processmetadata on one item before its requirements are \n            processed, nothing will happen.\n\n        **comments** : single-character string, optional\n\n            The comments character is used to process many pieces of metadata, \n            e.g. it is striped of the left side of names and formats strings\n            before splitting on delimiter.\n\n        **verbosity** : integer, optional\n\n            Determines the level of verbosity in the printout of messages\n            during the running of the procedure. \n\n   **Returns**\n   \n       Nothing.\n\n    \"\"\"\n    items = items.split(',') if isinstance(items,str) else items\n\n    if comments is None:\n        if 'comments' in metadata.keys():\n            comments = metadata['comments']\n        else:\n            comments = '#'\n            if verbosity > 8:\n                print 'processing metadata with comments char = #'\n    else:\n        if (('comments' in metadata.keys()) and \n            (comments != metadata['comments']) and (verbosity > 8)):\n            print 'comments character specified to process metadata (', repr(comments) ,') is different from comments charcater set in metadata dictionary (', repr(metadata['comments']) , ').'\n    \n    if not items:\n        for k in metadata.keys():\n            if is_string_like(metadata[k]):\n                metadata[k] = '\\n'.join([x.lstrip(comments) \n                                         for x in metadata[k].split('\\n') ])\n    \n    if not items or 'dialect' in items:\n        if 'dialect' in metadata.keys():\n            if isinstance(metadata['dialect'],str):\n                D = dialectfromstring(metadata['dialect'].lstrip(comments))\n                if D:\n                    metadata['dialect'] = D\n                    if (verbosity > 8):\n                        print 'processed dialect from string'\n                        \n                else:\n                    if (verbosity > 8):\n                        print '''Dialect failed to be converted properly from \n                                 string representation in metadata.'''\n\n            if 'delimiter' in dir(metadata['dialect']):\n                for a in dir(metadata['dialect']):\n                    if not a.startswith('_') and a in metadata.keys():\n                        setattr(metadata['dialect'],a, metadata[a])\n                        if ((verbosity > 2 and a == 'delimiter') or \n                            (verbosity >= 8)):\n                            print 'Setting dialect attribute', a, 'to equal specified value:', repr(metadata[a])\n                    elif not a.startswith('_') and a not in metadata.keys():\n                        metadata[a] = getattr(metadata['dialect'], a)\n                        if ((verbosity > 2 and a == 'delimiter') or (verbosity >= 8)):\n                            print 'Setting metadata attribute from dialect', a , 'to equal specified value:', repr(metadata[a])\n\n    if (not items or 'names' in items) and ('names' in metadata.keys()): \n        if is_string_like(metadata['names']):\n\n            if delimiter_regex:\n                metadata['names'] = delimiter_regex.split(metadata['names'])\n            elif (('dialect' in metadata.keys()) and \n                  ('delimiter' in dir(metadata['dialect']))):\n                d = metadata['dialect']\n                n = metadata['names']\n                metadata['names'] = list(csv.reader([n.lstrip(comments)],      \n                                                     dialect=d))[0]\n                if (verbosity > 8):\n                    print '... splitting \"names\" metadata from string with delimiter', repr(d.delimiter), '. Resulting names:', metadata['names']\n\n    if (not items or 'formats' in items) and 'formats' in metadata.keys(): \n        if is_string_like(metadata['formats']):   \n            if delimiter_regex:\n                metadata['formats'] = delimiter_regex.split(metadata['formats']) \n            elif (('dialect' in metadata.keys()) and ('delimiter' in dir(metadata['dialect']))):\n                d = metadata['dialect']\n                n = metadata['formats']\n                metadata['formats'] = list(csv.reader([n.lstrip(comments)],  \n                                           dialect=d))[0]\n\n                if (verbosity > 8):\n                    print '... splitting \"formats\" metadata from string with delimiter', repr(d.delimiter), '. Resulting names:', metadata['formats']       \n \n        if ncols:\n            metadata['formats'] = postprocessformats(metadata['formats'],ncols)\n\n\n    if (not items or 'types' in items) and 'types' in metadata.keys(): \n        if is_string_like(metadata['types']):      \n            if delimiter_regex:\n                metadata['types'] = delimiter_regex.split(metadata['types']) \n            elif (('dialect' in metadata.keys()) and \n                  ('delimiter' in dir(metadata['dialect']))):\n                d = metadata['dialect']\n                n = metadata['types']\n                metadata['types'] = list(csv.reader([n.lstrip(comments)],  \n                                         dialect=d))[0]\n                if (verbosity > 8):\n                    print '... splitting \"types\" metadata from string with delimiter', repr(d.delimiter), '. Resulting names:', metadata['types']      \n        \n        if ncols:\n            metadata['types'] = postprocessformats(metadata['types'],ncols)\n\n    if (not items or 'coloring' in items) and ('coloring' in metadata.keys()):\n        if is_string_like(metadata['coloring']):\n            C = coloringfromstring(metadata['coloring'].lstrip(comments))\n            if C:\n                metadata['coloring'] = C\n                if (verbosity > 8):  \n                    print '... processed coloring from string'\n            else:\n                if verbosity > 1:\n                    print 'Coloring failed to be converted properly from string representation in metadata ; removing coloring data from active metadata (putting it in \"loaded_coloring\").'\n                    metadata['loaded_coloring'] = metadata['coloring']\n                    metadata.pop('coloring')\n\n    if (not items or 'headerlines' in items):\n        if 'headerlines' in metadata.keys():\n            if isinstance(metadata['headerlines'], str):\n                try:\n                    h = metadata['headerlines']\n                    metadata['headerlines'] = int(h.lstrip(comments))\n                except (ValueError,TypeError):\n                    if verbosity > 6:\n                        print 'headerlines metadata failed to convert to an integer.'\n                else:\n                    pass\n                        \n\n            if isinstance(metadata['headerlines'], int):\n                if 'metametadata' in metadata.keys():\n                    h= metadata['headerlines']\n                    mmd = metadata['metametadata']\n                    metadata['headerlines'] = max(h, 1 + max([v \n                                               if isinstance(v, int) else max(v) \n                                               for v in mmd.values()]))\n                    if ((metadata['headerlines'] > h) and (verbosity > 8)):\n                        print 'Resetting headerlines from', h, 'to', metadata['headerlines'], 'because of line number indications from metametadata.'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef inferheader(lines, comments=None, metadata=None,\n                verbosity=DEFAULT_VERBOSITY):\n    \"\"\"\n    Infers header from a CSV or other tab-delimited file.\n    \n    This is essentially small extension of the csv.Sniffer.has_header algorithm.\n    provided in the Python csv module.   First, it checks to see whether a \n    metametadata dictionary is present, specifiying the lines numbers of \n    metadata lines in the header, and if so, sets the header lines to include\n    at least those lines.  Then iookms to see if a comments character is \n    present, and if so, includes those lines as well.  If either of the above \n    returns a nono-zero number of headerlines, the function returns that \n    number; otherwise, it uses the csv.Sniffer module, checking each line in \n    succession, and stopping at the first line where the sniffer module finds no \n    evidence of a header, and returning that line numner.\n    \n    **Parameters** \n    \n        **lines** : line of strings \n        \n            The list of lines representing lines in the file\n            \n        **comments** : single-character string, optional\n        \n            Comments character  specification. \n            \n        **metadata** : metadata dictionary, optional\n        \n            Used to determine a comments character and metametadata dicationary, \n            if present.\n\n    **Returns**\n    \n        Integer, representing the number of (inferred) header lines at the top \n        of the file.\n    \n    \"\"\"\n     \n    if ((comments is None) and metadata and ('comments' in metadata.keys())):\n        comments = metadata['comments']\n    if (comments is None):\n        comments = '#'  \n\n    if ('metametadata' in metadata.keys()):\n        mmd = metadata['metametadata']\n        cc = 1 + max([v if isinstance(v, int) else max(v) \n                      for v in mmd.values()])\n    else:\n        cc = 0\n        \n    if (comments != ''):\n        if (cc < len(lines)):\n            for l in xrange(cc,len(lines)):\n                if not lines[l].startswith(comments):\n                    break\n        else:\n            l = cc\n\n    if (l > 0):\n        return l\n    else:\n        for j in xrange(min(1000, len(lines))):\n            hasheader = 'unset'\n            for k in [100, 200, 400, 800, 1600]:\n                F = '\\n'.join(lines[j:(j+k)])\n                try:\n                    hasheader = csv.Sniffer().has_header(F)\n                except:\n                    pass\n                else:\n                    break\n            if not hasheader:\n                return j", "response": "Infer header from a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking whether a given AST represents a metametadata dictionary.", "response": "def IsMetaMetaDict(AST):\n    \"\"\"\n    Checks whether a given AST (abstract syntax tree) object represents a \n    metametadata dictionary.   \n    \n    \"\"\"\n    isintpair = lambda x : (isinstance(x,Tuple) and (len(x.asList()) == 2) and \n                            isctype(x.asList()[0], int) and \n                            isctype(x.asList()[1], int))\n    try:\n        if ((len(AST.getChildren()) > 1) and \n            isinstance(AST.getChildren()[1], Stmt)):\n            if isinstance(AST.getChildren()[1].getChildren()[0], Assign):\n                [s, d] = AST.getChildren()[1].getChildren()[0].asList()\n    except (TypeError, AttributeError):\n        return False\n    else:\n        if (isinstance(s, AssName) and s.name == 'metametadata'):     \n            if isinstance(d, Dict):\n                return all([isctype(k, str) and \n                            (isctype(v, int) or isintpair(v)) \n                            for (k,v) in d.items])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dialectfromstring(s):\n    try:\n        AST = compiler.parse(s)\n    except SyntaxError:\n        return\n    else:\n        try:\n            if (len(AST.getChildren()) > 1):\n                ST = AST.getChildren()[1]\n                if isinstance(ST, Stmt):\n                    if isinstance(ST.getChildren()[0], Discard):\n                        d = ST.getChildren()[0].asList()[0]\n        except (TypeError,AttributeError):\n            pass\n        else:\n            if (isinstance(d,Dict) and (len(d.items) > 0)):\n                if all([isctype(i[0], str) for i in d.items]):\n                    testd = csv.Sniffer().sniff('a,b,c')\n                    if all([n.value in dir(testd) and \n                        isctype(v, type(getattr(testd, n.value))) for (n,v) in \n                                                                      d.items]):\n                        D = eval(s)\n                        for n in D.keys():\n                            setattr(testd, n, D[n])\n                        return testd", "response": "Converts a string representation of a CSVVCTYPE into an actual csv. Dialect object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef coloringfromstring(s):\n    try:\n        AST = compiler.parse(s)\n    except SyntaxError:\n        return\n    else:\n        try:\n            if len(AST.getChildren()) > 1:\n                ST = AST.getChildren()[1]\n                if isinstance(ST, Stmt) and isinstance(ST.getChildren()[0],\n                                                                     Discard):\n                    d = ST.getChildren()[0].asList()[0]\n        except (TypeError, AttributeError):\n            pass\n        else:\n            if isinstance(d,Dict) and len(d.items) > 0:\n                if all([isctype(i[0],str) for i in d.items]):\n                    if all([isinstance(i[1],List) for i in d.items]):\n                        if all([all([isctype(j,str) for j in i[1]]) for i in \n                                                                      d.items]):\n                            return eval(s)", "response": "Converts a string representation of a coloring dictionary into an actual coloring dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef loadbinary(fname):\n\n    X = np.load(fname)\n    if isinstance(X, np.lib.npyio.NpzFile):\n        if 'coloring' in X.files:\n            coloring = X['coloring'].tolist()\n        else:\n            coloring = None\n        if 'data' in X.files:\n            return [X['data'], X['data'].dtype, coloring]\n        else:\n            return [None, None, coloring]\n    else:\n        return [X, X.dtype, None]", "response": "Loads a numpy binary file or archive created by tabular. io. savebinary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef savebinary(fname, X, savecoloring=True):\n    if fname[-4:] == '.npy':\n        np.save(fname, X)\n    else:\n        if savecoloring is True:\n            np.savez(fname, data=X, coloring=X.coloring)\n        else:\n            np.savez(fname, data=X)", "response": "Save a tabarray to a numpy binary file or archive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the types from a structured numpy dtype object.", "response": "def parsetypes(dtype):\n    \"\"\"\n    Parse the types from a structured numpy dtype object.\n\n    Return list of string representations of types from a structured numpy \n    dtype object, e.g. ['int', 'float', 'str'].\n\n    Used by :func:`tabular.io.saveSV` to write out type information in the \n    header.\n\n    **Parameters**\n\n        **dtype** :  numpy dtype object\n\n            Structured numpy dtype object to parse.\n\n    **Returns**\n\n        **out** :  list of strings\n\n            List of strings corresponding to numpy types::\n\n                [dtype[i].name.strip('1234567890').rstrip('ing') \\ \n                 for i in range(len(dtype))]\n\n    \"\"\"\n    return [dtype[i].name.strip('1234567890').rstrip('ing') \n            for i in range(len(dtype))]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nthreshold a coloring dictionary for a given list of column names.", "response": "def thresholdcoloring(coloring, names):\n    \"\"\"\n    Threshold a coloring dictionary for a given list of column names.\n\n    Threshold `coloring` based on `names`, a list of strings in::\n\n        coloring.values()\n\n    **Parameters**\n\n        **coloring** :  dictionary\n\n            Hierarchical structure on the columns given in the header of the \n            file; an attribute of tabarrays.\n\n            See :func:`tabular.tab.tabarray.__new__` for more information about \n            coloring.\n\n        **names** :  list of strings\n\n            List of strings giving column names.\n\n    **Returns**\n\n        **newcoloring** :  dictionary\n\n            The thresholded coloring dictionary.\n\n    \"\"\"\n    for key in coloring.keys():\n        if len([k for k in coloring[key] if k in names]) == 0:\n            coloring.pop(key)\n        else:\n            coloring[key] = utils.uniqify([k for k in coloring[key] if k in \n                                           names])\n    return coloring"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(to_delete):\n    if os.path.isfile(to_delete):\n        os.remove(to_delete)\n    elif os.path.isdir(to_delete):\n        shutil.rmtree(to_delete)", "response": "Delete a file or directory tree."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef makedir(dir_name):\n    if os.path.exists(dir_name):\n        delete(dir_name)\n    os.mkdir(dir_name)", "response": "Create a directory in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apod(date=None, concept_tags=None):\n    '''\n    HTTP REQUEST\n\n    GET https://api.nasa.gov/planetary/apod\n\n    QUERY PARAMETERS\n\n    Parameter\tType\tDefault\tDescription\n    date\tYYYY-MM-DD\ttoday\tThe date of the APOD image to retrieve\n    concept_tags\tbool\tFalse\tReturn an ordered dictionary of concepts from the APOD explanation\n    api_key\tstring\tDEMO_KEY\tapi.nasa.gov key for expanded usage\n    EXAMPLE QUERY\n\n    https://api.nasa.gov/planetary/apod?concept_tags=True&api_key=DEMO_KEY\n    '''\n    base_url = \"https://api.nasa.gov/planetary/apod?\"\n\n    if date:\n        try:\n            vali_date(date)\n            base_url += \"date=\" + date + \"&\"\n        except:\n            raise ValueError(\"Incorrect date format, should be YYYY-MM-DD\")\n    if concept_tags == True:\n        base_url += \"concept_tags=True\" + \"&\"\n\n    req_url = base_url + \"api_key=\" + nasa_api_key()\n\n    return dispatch_http_get(req_url)", "response": "This function returns a dictionary of concepts from the APOD image."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef permission_required(action):\n    def decorator(f):\n        @wraps(f)\n        def inner(community, *args, **kwargs):\n            permission = current_permission_factory(community, action=action)\n            if not permission.can():\n                abort(403)\n            return f(community, *args, **kwargs)\n        return inner\n    return decorator", "response": "Decorator to require permission for a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_item(item, template, name='item'):\n    ctx = {name: item}\n    return render_template_to_string(template, **ctx)", "response": "Render a template to a string with the provided item in context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef index():\n    ctx = mycommunities_ctx()\n\n    p = request.args.get('p', type=str)\n    so = request.args.get('so', type=str)\n    page = request.args.get('page', type=int, default=1)\n\n    so = so or current_app.config.get('COMMUNITIES_DEFAULT_SORTING_OPTION')\n\n    communities = Community.filter_communities(p, so)\n    featured_community = FeaturedCommunity.get_featured_or_none()\n    form = SearchForm(p=p)\n    per_page = 10\n    page = max(page, 1)\n    p = Pagination(page, per_page, communities.count())\n\n    ctx.update({\n        'r_from': max(p.per_page * (p.page - 1), 0),\n        'r_to': min(p.per_page * p.page, p.total_count),\n        'r_total': p.total_count,\n        'pagination': p,\n        'form': form,\n        'title': _('Communities'),\n        'communities': communities.slice(\n            per_page * (page - 1), per_page * page).all(),\n        'featured_community': featured_community,\n    })\n\n    return render_template(\n        current_app.config['COMMUNITIES_INDEX_TEMPLATE'], **ctx)", "response": "Index page with uploader and list of existing depositions."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nindexes page with uploader and list of existing depositions.", "response": "def generic_item(community, template, **extra_ctx):\n    \"\"\"Index page with uploader and list of existing depositions.\"\"\"\n    # Check existence of community\n    ctx = mycommunities_ctx()\n    ctx.update({\n        'is_owner': community.id_user == current_user.get_id(),\n        'community': community,\n        'detail': True,\n    })\n    ctx.update(extra_ctx)\n\n    return render_template(template, **ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new community.", "response": "def new():\n    \"\"\"Create a new community.\"\"\"\n    form = CommunityForm(formdata=request.values)\n\n    ctx = mycommunities_ctx()\n    ctx.update({\n        'form': form,\n        'is_new': True,\n        'community': None,\n    })\n\n    if form.validate_on_submit():\n        data = copy.deepcopy(form.data)\n\n        community_id = data.pop('identifier')\n        del data['logo']\n\n        community = Community.create(\n            community_id, current_user.get_id(), **data)\n\n        file = request.files.get('logo', None)\n        if file:\n            if not community.save_logo(file.stream, file.filename):\n                form.logo.errors.append(_(\n                    'Cannot add this file as a logo. Supported formats: '\n                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))\n                db.session.rollback()\n                community = None\n\n        if community:\n            db.session.commit()\n            flash(\"Community was successfully created.\", category='success')\n            return redirect(url_for('.edit', community_id=community.id))\n\n    return render_template(\n        current_app.config['COMMUNITIES_NEW_TEMPLATE'],\n        community_form=form,\n        **ctx\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edit(community):\n    form = EditCommunityForm(formdata=request.values, obj=community)\n    deleteform = DeleteCommunityForm()\n    ctx = mycommunities_ctx()\n    ctx.update({\n        'form': form,\n        'is_new': False,\n        'community': community,\n        'deleteform': deleteform,\n    })\n\n    if form.validate_on_submit():\n        for field, val in form.data.items():\n            setattr(community, field, val)\n\n        file = request.files.get('logo', None)\n        if file:\n            if not community.save_logo(file.stream, file.filename):\n                form.logo.errors.append(_(\n                    'Cannot add this file as a logo. Supported formats: '\n                    'PNG, JPG and SVG. Max file size: 1.5 MB.'))\n\n        if not form.logo.errors:\n            db.session.commit()\n            flash(\"Community successfully edited.\", category='success')\n            return redirect(url_for('.edit', community_id=community.id))\n\n    return render_template(\n        current_app.config['COMMUNITIES_EDIT_TEMPLATE'],\n        **ctx\n    )", "response": "Create or edit a community."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nindex page with uploader and list of existing depositions.", "response": "def curate(community):\n    \"\"\"Index page with uploader and list of existing depositions.\n\n    :param community_id: ID of the community to curate.\n    \"\"\"\n    if request.method == 'POST':\n        action = request.json.get('action')\n        recid = request.json.get('recid')\n\n        # 'recid' is mandatory\n        if not recid:\n            abort(400)\n        if action not in ['accept', 'reject', 'remove']:\n            abort(400)\n\n        # Resolve recid to a Record\n        resolver = Resolver(\n            pid_type='recid', object_type='rec', getter=Record.get_record)\n        pid, record = resolver.resolve(recid)\n\n        # Perform actions\n        if action == \"accept\":\n            community.accept_record(record)\n        elif action == \"reject\":\n            community.reject_record(record)\n        elif action == \"remove\":\n            community.remove_record(record)\n\n        record.commit()\n        db.session.commit()\n        RecordIndexer().index_by_id(record.id)\n        return jsonify({'status': 'success'})\n\n    ctx = {'community': community}\n    return render_template(\n        current_app.config['COMMUNITIES_CURATE_TEMPLATE'],\n        **ctx\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef evaluate_tree_rooting(nexson, ott, tree_proxy):\n    pruned_phylo, taxo_tree = create_pruned_and_taxonomy_for_tip_ott_ids(tree_proxy, ott)\n    if taxo_tree is None:  # this can happen if no otus are mapped\n        return None\n    has_taxo_groupings = any_early_exit(taxo_tree.root.child_iter(), lambda node: not node.is_leaf)\n    if not has_taxo_groupings:\n        return None\n    has_phylo_groupings = any_early_exit(pruned_phylo.root.child_iter(), lambda node: not node.is_leaf)\n    if not has_phylo_groupings:\n        return None\n    id2bit = pruned_phylo.add_bits4subtree_ids(None)\n    taxo_tree.add_bits4subtree_ids(id2bit)\n    assert taxo_tree.root.bits4subtree_ids == pruned_phylo.root.bits4subtree_ids\n    taxo_nontriv_splits = taxo_tree.bits2internal_node\n    taxon_mask = taxo_tree.root.bits4subtree_ids\n    # _LOG.debug('taxo_nontriv_splits = {}'.format(taxo_nontriv_splits))\n    # might want to copy this dict rather than modify in place..\n    del taxo_nontriv_splits[taxon_mask]  # root bitmask is trivial\n    _LOG.debug('taxon_mask = {} (which is {} bits)'.format(bin(taxon_mask)[2:], len(bin(taxon_mask)) - 2))\n    num_ids = len(id2bit)\n    _LOG.debug('id2bit has length = {}'.format(len(id2bit)))\n    # for checking tips of the phylogeny, it is nice to know which leaf OTUs attach\n    #   at the base of the taxonomy (no other grouping)\n    basal_taxo = set()\n    basal_bits = 0\n    for c in taxo_tree.root.child_iter():\n        if c.is_leaf:\n            basal_taxo.add(c._id)\n            basal_bits |= id2bit[c._id]\n    _LOG.debug('basal_bits = {}'.format(bin(basal_bits)[2:].zfill(num_ids)))\n    _LOG.debug('# nontrivial taxo splits = {}'.format(len(taxo_nontriv_splits)))\n    _EMPTY_SET = frozenset([])\n    non_root_pp_preorder = [nd for nd in pruned_phylo.preorder_node_iter()][1:]\n    curr_root_incompat_set = set()\n    any_root_incompat_set = set()\n    _taxo_node_id_set_cache = {_EMPTY_SET: _EMPTY_SET}\n    for node in non_root_pp_preorder:\n        edge = node.edge\n        if node.is_leaf:\n            edge._displays = None\n            edge._inverted_displays = None\n            b = id2bit[node._id]\n            if node._id in basal_taxo:\n                edge._not_inverted_incompat = _EMPTY_SET\n                edge._inverted_incompat = _EMPTY_SET\n                inv_mask = taxon_mask - b\n                idisp = taxo_nontriv_splits.get(inv_mask)\n                if idisp is not None:\n                    edge._inverted_displays = idisp\n            else:\n                edge._not_inverted_incompat = _EMPTY_SET\n                # TODO would be more efficient to jump to tip and walk back...\n                b = id2bit[node._id]\n                ii = set()\n                for tb, tid in taxo_nontriv_splits.items():\n                    if tb & b:\n                        ii.add(tid)\n                edge._inverted_incompat = _get_cached_set(ii, _taxo_node_id_set_cache)\n                disp = taxo_nontriv_splits.get(b)\n                if disp is not None:\n                    edge._displays = disp\n        else:\n            # TODO this could be more efficient...\n            b = node.bits4subtree_ids\n            nii = set()\n            ii = set()\n            e = set()\n            ie = set()\n            displays = None\n            inv_displays = None\n            # TODO: this loop does not take advantage of the fact that\n            #   taxo_nontriv_splits are splits from a tree (hence compatible with each other)\n            for tb, tid in taxo_nontriv_splits.items():\n                sp_result = compare_bits_as_splits(b, tb, taxon_mask)\n                if sp_result == SplitComparison.UNROOTED_INCOMPATIBLE:\n                    any_root_incompat_set.add(tid)\n                    nii.add(tid)\n                    ii.add(tid)\n                elif sp_result == SplitComparison.UNROOTED_COMPAT:\n                    nii.add(tid)\n                elif sp_result == SplitComparison.ROOTED_COMPAT:\n                    ii.add(tid)\n                elif sp_result == SplitComparison.UNROOTED_EQUIVALENT:\n                    ie.add(tid)\n                    inv_displays = tid\n                elif sp_result == SplitComparison.ROOTED_EQUIVALENT:\n                    e.add(tid)\n                    displays = tid\n            edge._not_inverted_incompat = _get_cached_set(nii, _taxo_node_id_set_cache)\n            edge._inverted_incompat = _get_cached_set(ii, _taxo_node_id_set_cache)\n            edge._equiv = _get_cached_set(e, _taxo_node_id_set_cache)\n            edge._inverted_equiv = _get_cached_set(ie, _taxo_node_id_set_cache)\n            edge._displays = displays\n            edge._inverted_displays = inv_displays\n            curr_root_incompat_set.update(nii)\n            # create a set to be filled in in the loop below (for each internal node)\n            node._inc_contrib_rootward = set()\n            node._displays_contrib_rootward = set()\n    pproot = pruned_phylo.root\n    pproot._incompat_if_rooted_below = set()\n    pproot._inc_contrib_rootward = set()\n    pproot._displays_contrib_rootward = set()\n    for node in reversed(non_root_pp_preorder):\n        edge = node.edge\n        if node.is_leaf:\n            edge._inc_contrib_rootward = _EMPTY_SET\n            node._displays_contrib_rootward = _EMPTY_SET\n        else:\n            par = node.parent\n            iaobc = set(edge._not_inverted_incompat)\n            iaobc.update(node._inc_contrib_rootward)\n            edge._inc_contrib_rootward = _get_cached_set(iaobc, _taxo_node_id_set_cache)\n            par._inc_contrib_rootward.update(edge._inc_contrib_rootward)\n            par._displays_contrib_rootward.update(node._displays_contrib_rootward)\n            if edge._displays is not None:\n                par._displays_contrib_rootward.add(edge._displays)\n\n    _LOG.debug('# root _inc_contrib_rootward = {}'.format(pruned_phylo.root._inc_contrib_rootward))\n    _LOG.debug('# curr_root_incompat_set = {}'.format(curr_root_incompat_set))\n    pproot.rooting_here_incompat = _get_cached_set(pproot._inc_contrib_rootward, _taxo_node_id_set_cache)\n    pproot.rooting_here_incompat_score = len(pproot.rooting_here_incompat)\n    pproot.rooting_here_displays = _get_cached_set(pproot._displays_contrib_rootward, _taxo_node_id_set_cache)\n    pproot.rooting_here_disp_score = len(pproot.rooting_here_displays)\n    pproot.rooting_here_score = (pproot.rooting_here_disp_score, pproot.rooting_here_incompat_score)\n    pproot._inc_contrib_tipward = _EMPTY_SET\n    pproot._disp_contrib_tipward = _EMPTY_SET\n    best_score = pproot.rooting_here_score\n    best_rootings = [pproot]\n    # now sweep up\n    for node in non_root_pp_preorder:\n        edge = node.edge\n        parent = node.parent\n        sib_inc_union = set()\n        sib_disp = set()\n        for sib in node.sib_iter():\n            sib_inc_union.update(sib.edge._inc_contrib_rootward)\n            sib_disp.update(sib._displays_contrib_rootward)\n            if sib.edge._displays is not None:\n                sib_disp.add(sib.edge._displays)\n        # if we are visiting an internal node, we have to figure out the cost of\n        #  rooting at the node too...\n        if not node.is_leaf:\n            icu = set()\n            icu.update(edge._inverted_incompat)\n            icu.update(sib_inc_union)\n            icu.update(parent._inc_contrib_tipward)\n            node._inc_contrib_tipward = _get_cached_set(icu, _taxo_node_id_set_cache)\n            dci = set(sib_disp)\n            if edge._inverted_displays is not None:\n                dci.add(edge._displays)\n            dci.update(parent._disp_contrib_tipward)\n            node._disp_contrib_tipward = _get_cached_set(dci, _taxo_node_id_set_cache)\n\n            rhi = set()\n            rhi.update(icu)\n            rhi.update(node._inc_contrib_rootward)\n            node.rooting_here_incompat = _get_cached_set(rhi, _taxo_node_id_set_cache)\n            rhd = set(node._displays_contrib_rootward)\n            rhd.update(node._disp_contrib_tipward)\n            node.rooting_here_displays = _get_cached_set(rhd, _taxo_node_id_set_cache)\n            best_score, best_rootings = _check_for_opt_score(node, best_score, best_rootings)\n        # figure out the # of conflicts if rooting on this edge...\n        rhi = set()\n        rhi.update(edge._inverted_incompat)\n        rhi.update(sib_inc_union)\n        edge.rooting_here_incompat = _get_cached_set(rhi, _taxo_node_id_set_cache)\n        rhd = set(parent._disp_contrib_tipward)\n        rhd.update(parent.rooting_here_displays)\n        if edge._inverted_displays is not None:\n            rhd.add(edge._inverted_displays)\n        edge.rooting_here_displays = _get_cached_set(rhd, _taxo_node_id_set_cache)\n        best_score, best_rootings = _check_for_opt_score(edge, best_score, best_rootings)\n\n    _LOG.debug('best_score = {}'.format(best_score))\n    _LOG.debug('best_rootings = {}'.format(best_rootings))\n    _LOG.debug('current score = {}'.format(pproot.rooting_here_score))\n    _LOG.debug('any_root_incompat_set (size={}) = {}'.format(len(any_root_incompat_set), any_root_incompat_set))", "response": "Evaluate the rooting decision for a tree."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ot_find_tree(arg_dict, exact=True, verbose=False, oti_wrapper=None):\n    if oti_wrapper is None:\n        from peyotl.sugar import oti\n        oti_wrapper = oti\n    return oti_wrapper.find_trees(arg_dict,\n                                  exact=exact,\n                                  verbose=verbose,\n                                  wrap_response=True)", "response": "Uses a peyotl wrapper around an Open Tree web service to get a list of trees including values value for a given property."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the matching trees in a dict.", "response": "def print_matching_trees(arg_dict, tree_format, exact, verbose):\n    \"\"\"The `TreeRef` instance returned by the oti.find_trees(... wrap_response=True)\n    can be used as an argument to the phylesystem_api.get call.\n    If you pass in a string (instead of a TreeRef), the string will be interpreted as a study ID\n    \"\"\"\n    from peyotl.sugar import phylesystem_api\n    tree_list = ot_find_tree(arg_dict, exact=exact, verbose=verbose)\n    for tree_ref in tree_list:\n        print(tree_ref)\n        print(phylesystem_api.get(tree_ref, format=tree_format))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nresolve etype to an actual type if it is a forward reference", "response": "def proc_forward(etype, namespace: Dict[str, Any]):\n    \"\"\" Resolve etype to an actual type if it is a forward reference \"\"\"\n    return etype._eval_type(namespace, namespace) if type(etype) is _ForwardRef else etype"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine whether etype is a List or other iterable", "response": "def is_iterable(etype) -> bool:\n    \"\"\" Determine whether etype is a List or other iterable \"\"\"\n    return type(etype) is GenericMeta and issubclass(etype.__extra__, Iterable)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef union_conforms(element: Union, etype, namespace: Dict[str, Any], conforms: Callable) -> bool:\n    union_vals = etype.__union_params__ if sys.version_info < (3, 6) else etype.__args__\n    return any(conforms(element, t, namespace) for t in union_vals)", "response": "Tests whether the given element conforms to at least one type in etype with the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend schema to ledger and retrieve it as written to the ledger and return it.", "response": "async def send_schema(self, schema_data_json: str) -> str:\n        \"\"\"\n        Send schema to ledger, then retrieve it as written to the ledger and return it.\n        If schema already exists on ledger, log error and return schema.\n\n        :param schema_data_json: schema data json with name, version, attribute names; e.g.,\n\n        ::\n\n            {\n                'name': 'my-schema',\n                'version': '1.234',\n                'attr_names': ['favourite_drink', 'height', 'last_visit_date']\n            }\n\n        :return: schema json as written to ledger (or existed a priori)\n        \"\"\"\n\n        LOGGER.debug('Origin.send_schema >>> schema_data_json: %s', schema_data_json)\n\n        schema_data = json.loads(schema_data_json)\n        s_key = schema_key(schema_id(self.did, schema_data['name'], schema_data['version']))\n        with SCHEMA_CACHE.lock:\n            try:\n                rv_json = await self.get_schema(s_key)\n                LOGGER.error(\n                    'Schema %s version %s already exists on ledger for origin-did %s: not sending',\n                    schema_data['name'],\n                    schema_data['version'],\n                    self.did)\n            except AbsentSchema:  # OK - about to create and send it\n                (_, schema_json) = await anoncreds.issuer_create_schema(\n                    self.did,\n                    schema_data['name'],\n                    schema_data['version'],\n                    json.dumps(schema_data['attr_names']))\n                req_json = await ledger.build_schema_request(self.did, schema_json)\n                resp_json = await self._sign_submit(req_json)\n                resp = json.loads(resp_json)\n                resp_result_txn = resp['result']['txn']\n                rv_json = await self.get_schema(schema_key(schema_id(\n                    resp_result_txn['metadata']['from'],\n                    resp_result_txn['data']['data']['name'],\n                    resp_result_txn['data']['data']['version'])))  # add to cache en passant\n\n        LOGGER.debug('Origin.send_schema <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _locked_refresh_doc_ids(self):\n        d = {}\n        for s in self._shards:\n            for k in s.doc_index.keys():\n                if k in d:\n                    raise KeyError('doc \"{i}\" found in multiple repos'.format(i=k))\n                d[k] = s\n        self._doc2shard_map = d", "response": "Refreshes the doc_ids for all the shards."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_doc_objs(self, **kwargs):\n        for shard in self._shards:\n            for doc_id, blob in shard.iter_doc_objs(**kwargs):\n                yield doc_id, blob", "response": "Generator that iterates over all detected documents and returns the doc object for each."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter_doc_filepaths(self, **kwargs):\n        for shard in self._shards:\n            for doc_id, blob in shard.iter_doc_filepaths(**kwargs):\n                yield doc_id, blob", "response": "Generator that iterates over all detected documents and returns the filesystem path to each doc."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the identifier of a resource.", "response": "def validate_identifier(self, field):\n        \"\"\"Validate field identifier.\"\"\"\n        if field.data:\n            field.data = field.data.lower()\n            if Community.get(field.data, with_deleted=True):\n                raise validators.ValidationError(\n                    _('The identifier already exists. '\n                      'Please choose a different one.'))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nopen a file for writing to a group.", "response": "def open_for_group_write(fp, mode, encoding='utf-8'):\n    \"\"\"Open with mode=mode and permissions '-rw-rw-r--' group writable is\n    the default on some systems/accounts, but it is important that it be present on our deployment machine\n    \"\"\"\n    d = os.path.split(fp)[0]\n    if not os.path.exists(d):\n        os.makedirs(d)\n    o = codecs.open(fp, mode, encoding=encoding)\n    o.flush()\n    os.chmod(fp, stat.S_IRGRP | stat.S_IROTH | stat.S_IRUSR | stat.S_IWGRP | stat.S_IWUSR)\n    return o"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread the text content of filepath", "response": "def read_filepath(filepath, encoding='utf-8'):\n    \"\"\"Returns the text content of `filepath`\"\"\"\n    with codecs.open(filepath, 'r', encoding=encoding) as fo:\n        return fo.read()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting content to the filepath", "response": "def write_to_filepath(content, filepath, encoding='utf-8', mode='w', group_writeable=False):\n    \"\"\"Writes `content` to the `filepath` Creates parent directory\n    if needed, and uses the specified file `mode` and data `encoding`.\n    If `group_writeable` is True, the output file will have permissions to be\n        writable by the group (on POSIX systems)\n    \"\"\"\n    par_dir = os.path.split(filepath)[0]\n    if not os.path.exists(par_dir):\n        os.makedirs(par_dir)\n    if group_writeable:\n        with open_for_group_write(filepath, mode=mode, encoding=encoding) as fo:\n            fo.write(content)\n    else:\n        with codecs.open(filepath, mode=mode, encoding=encoding) as fo:\n            fo.write(content)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef download(url, encoding='utf-8'):\n    import requests\n    response = requests.get(url)\n    response.encoding = encoding\n    return response.text", "response": "Returns the text fetched via http GET from URL read as encoding"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the given blob as JSON to dest.", "response": "def write_as_json(blob, dest, indent=0, sort_keys=True):\n    \"\"\"Writes `blob` as JSON to the filepath `dest` or the filestream `dest` (if it isn't a string)\n    uses utf-8 encoding if the filepath is given (does not change the encoding if dest is already open).\n    \"\"\"\n    opened_out = False\n    if is_str_type(dest):\n        out = codecs.open(dest, mode='w', encoding='utf-8')\n        opened_out = True\n    else:\n        out = dest\n    try:\n        json.dump(blob, out, indent=indent, sort_keys=sort_keys)\n        out.write('\\n')\n    finally:\n        out.flush()\n        if opened_out:\n            out.close()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nshow JSON indented representation of d", "response": "def pretty_dict_str(d, indent=2):\n    \"\"\"shows JSON indented representation of d\"\"\"\n    b = StringIO()\n    write_pretty_dict_str(b, d, indent=indent)\n    return b.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_pretty_dict_str(out, obj, indent=2):\n    json.dump(obj,\n              out,\n              indent=indent,\n              sort_keys=True,\n              separators=(',', ': '),\n              ensure_ascii=False,\n              encoding=\"utf-8\")", "response": "writes JSON indented representation of obj to out"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing study trees from a file - like object", "response": "def parse_study_tree_list(fp):\n    \"\"\"study trees should be in {'study_id', 'tree_id'} objects, but\n    as legacy support we also need to support files that have the format:\n    pg_315_4246243 # comment\n\n    \"\"\"\n    # noinspection PyBroadException\n    try:\n        sl = read_as_json(fp)\n    except:\n        sl = []\n        with codecs.open(fp, 'rU', encoding='utf-8') as fo:\n            for line in fo:\n                frag = line.split('#')[0].strip()\n                if frag:\n                    sl.append(frag)\n    ret = []\n    for element in sl:\n        if isinstance(element, dict):\n            assert 'study_id' in element\n            assert 'tree_id' in element\n            ret.append(element)\n        else:\n            # noinspection PyUnresolvedReferences,PyUnresolvedReferences\n            assert element.startswith('pg_') or element.startswith('ot_')\n            # noinspection PyUnresolvedReferences\n            s = element.split('_')\n            assert len(s) > 1\n            tree_id = s[-1]\n            study_id = '_'.join(s[:-1])\n            ret.append({'study_id': study_id, 'tree_id': tree_id})\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _format_args():\n    # Ensure we can run outside a application/request context.\n    try:\n        pretty_format = \\\n            current_app.config['JSONIFY_PRETTYPRINT_REGULAR'] and \\\n            not request.is_xhr\n    except RuntimeError:\n        pretty_format = False\n\n    if pretty_format:\n        return dict(\n            indent=2,\n            separators=(', ', ': '),\n        )\n    else:\n        return dict(\n            indent=None,\n            separators=(',', ':'),\n        )", "response": "Get JSON dump indentation and separates."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef community_responsify(schema_class, mimetype):\n    def view(data, code=200, headers=None, links_item_factory=None,\n             page=None, urlkwargs=None, links_pagination_factory=None):\n        \"\"\"Generate the response object.\"\"\"\n        if isinstance(data, Community):\n            last_modified = data.updated\n            response_data = schema_class(\n                context=dict(item_links_factory=links_item_factory)\n            ).dump(data).data\n        else:\n            last_modified = None\n            response_data = schema_class(\n                context=dict(\n                    total=data.query.count(),\n                    item_links_factory=links_item_factory,\n                    page=page,\n                    urlkwargs=urlkwargs,\n                    pagination_links_factory=links_pagination_factory)\n            ).dump(data.items, many=True).data\n\n        response = current_app.response_class(\n            json.dumps(response_data, **_format_args()),\n            mimetype=mimetype)\n        response.status_code = code\n\n        if last_modified:\n            response.last_modified = last_modified\n\n        if headers is not None:\n            response.headers.extend(headers)\n        return response\n    return view", "response": "Create a community response serializer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_unique_filepath(stem):\n    fp = stem\n    if os.path.exists(stem):\n        n = 1\n        fp = stem + str(n)\n        while os.path.exists(fp):\n            n += 1\n            fp = stem + str(n)\n    return fp", "response": "NOT thread - safe!\n    returns stems or stem + 1 where 1 is the smallest\n    positive integer for which the path does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_error(exc_info, json_encoder, debug_url=None):\n        exc = exc_info[1]\n        data = exc.__dict__.copy()\n        for key, value in data.items():\n            try:\n                json_encoder.encode(value)\n            except TypeError:\n                data[key] = repr(value)\n        data[\"traceback\"] = \"\".join(traceback.format_exception(*exc_info))\n        if debug_url is not None:\n            data[\"debug_url\"] = debug_url\n        return InternalError(data)", "response": "Wraps an exception in an InternalError and returns a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the cache contains a schema for the input key sequence number or schema identifier.", "response": "def contains(self, index: Union[SchemaKey, int, str]) -> bool:\n        \"\"\"\n        Return whether the cache contains a schema for the input key, sequence number, or schema identifier.\n\n        :param index: schema key, sequence number, or sequence identifier\n        :return: whether the cache contains a schema for the input index\n        \"\"\"\n\n        LOGGER.debug('SchemaCache.contains >>> index: %s', index)\n\n        rv = None\n        if isinstance(index, SchemaKey):\n            rv = (index in self._schema_key2schema)\n        elif isinstance(index, int) or (isinstance(index, str) and ':2:' not in index):\n            rv = (int(index) in self._seq_no2schema_key)\n        elif isinstance(index, str):\n            rv = (schema_key(index) in self._schema_key2schema)\n        else:\n            rv = False\n\n        LOGGER.debug('SchemaCache.contains <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfeeds schemata into cache.", "response": "def feed(self, schemata: list) -> None:\n        \"\"\"\n        Take schemata from incoming list representation as schemata() returns, unless\n        cache already has schema for an incoming schema sequence number.\n\n        :param schemata: list of schema objects\n        \"\"\"\n\n        LOGGER.debug('SchemaCache.feed >>> schemata: %s', schemata)\n\n        for schema in schemata:\n            seq_no = schema['seqNo']\n            if self.contains(seq_no):\n                LOGGER.warning('Schema cache already has schema at seq no %s: skipping', seq_no)\n            else:\n                self[seq_no] = schema\n                LOGGER.info('Schema cache imported schema on id %s at seq no %s', schema['id'], seq_no)\n\n        LOGGER.debug('SchemaCache.feed <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nculling the cache entry frame list to size favouring most recent query time.", "response": "def cull(self, delta: bool) -> None:\n        \"\"\"\n        Cull cache entry frame list to size, favouring most recent query time.\n\n        :param delta: True to operate on rev reg deltas, False for rev reg states\n        \"\"\"\n\n        LOGGER.debug('RevoCacheEntry.cull >>> delta: %s', delta)\n\n        rr_frames = self.rr_delta_frames if delta else self.rr_state_frames\n        mark = 4096**0.5  # max rev reg size = 4096; heuristic: hover max around sqrt(4096) = 64\n        if len(rr_frames) > int(mark * 1.25):\n            rr_frames.sort(key=lambda x: -x.qtime)  # order by descending query time\n            del rr_frames[int(mark * 0.75):]  # retain most recent, grow again from here\n            LOGGER.info(\n                'Pruned revocation cache entry %s to %s %s frames',\n                self.rev_reg_def['id'],\n                len(rr_frames),\n                'delta' if delta else 'state')\n\n        LOGGER.debug('RevoCacheEntry.cull <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dflt_interval(self, cd_id: str) -> (int, int):\n\n        LOGGER.debug('RevocationCache.dflt_interval >>>')\n\n        fro = None\n        to = None\n\n        for rr_id in self:\n            if cd_id != rev_reg_id2cred_def_id(rr_id):\n                continue\n            entry = self[rr_id]\n            if entry.rr_delta_frames:\n                to = max(entry.rr_delta_frames, key=lambda f: f.to).to\n                fro = min(fro or to, to)\n\n        if not (fro and to):\n            LOGGER.debug(\n                'RevocationCache.dflt_interval <!< No data for default non-revoc interval on cred def id %s',\n                cd_id)\n            raise CacheIndex('No data for default non-revoc interval on cred def id {}'.format(cd_id))\n\n        rv = (fro, to)\n        LOGGER.debug('RevocationCache.dflt_interval <<< %s', rv)\n        return rv", "response": "Return default non - revocation cache interval from latest to times on delta frames of all revocation cache entries on indices stemming from input cred def id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing and update from archived cache files. Only accept new content ; do not overwrite any existing content ; return None if no such archive.", "response": "def parse(base_dir: str, timestamp: int = None) -> int:\n        \"\"\"\n        Parse and update from archived cache files. Only accept new content;\n        do not overwrite any existing cache content.\n\n        :param base_dir: archive base directory\n        :param timestamp: epoch time of cache serving as subdirectory, default most recent\n        :return: epoch time of cache serving as subdirectory, None if there is no such archive.\n        \"\"\"\n\n        LOGGER.debug('parse >>> base_dir: %s, timestamp: %s', base_dir, timestamp)\n\n        if not isdir(base_dir):\n            LOGGER.info('No cache archives available: not feeding cache')\n            LOGGER.debug('parse <<< None')\n            return None\n\n        if not timestamp:\n            timestamps = [int(t) for t in listdir(base_dir) if t.isdigit()]\n            if timestamps:\n                timestamp = max(timestamps)\n            else:\n                LOGGER.info('No cache archives available: not feeding cache')\n                LOGGER.debug('parse <<< None')\n                return None\n\n        timestamp_dir = join(base_dir, str(timestamp))\n        if not isdir(timestamp_dir):\n            LOGGER.error('No such archived cache directory: %s', timestamp_dir)\n            LOGGER.debug('parse <<< None')\n            return None\n\n        with SCHEMA_CACHE.lock:\n            with open(join(timestamp_dir, 'schema'), 'r') as archive:\n                schemata = json.loads(archive.read())\n                SCHEMA_CACHE.feed(schemata)\n\n        with CRED_DEF_CACHE.lock:\n            with open(join(timestamp_dir, 'cred_def'), 'r') as archive:\n                cred_defs = json.loads(archive.read())\n                for cd_id in cred_defs:\n                    if cd_id in CRED_DEF_CACHE:\n                        LOGGER.warning('Cred def cache already has cred def on %s: skipping', cd_id)\n                    else:\n                        CRED_DEF_CACHE[cd_id] = cred_defs[cd_id]\n                        LOGGER.info('Cred def cache imported cred def for cred def id %s', cd_id)\n\n        with REVO_CACHE.lock:\n            with open(join(timestamp_dir, 'revocation'), 'r') as archive:\n                rr_cache_entries = json.loads(archive.read())\n                for (rr_id, entry) in rr_cache_entries.items():\n                    if rr_id in REVO_CACHE:\n                        LOGGER.warning('Revocation cache already has entry on %s: skipping', rr_id)\n                    else:\n                        rr_cache_entry = RevoCacheEntry(entry['rev_reg_def'])\n\n                        rr_cache_entry.rr_delta_frames = [\n                            RevRegUpdateFrame(\n                                f['_to'],\n                                f['_timestamp'],\n                                f['_rr_update']) for f in entry['rr_delta_frames']\n                        ]\n                        rr_cache_entry.cull(True)\n\n                        rr_cache_entry.rr_state_frames = [\n                            RevRegUpdateFrame(\n                                f['_to'],\n                                f['_timestamp'],\n                                f['_rr_update']) for f in entry['rr_state_frames']\n                        ]\n                        rr_cache_entry.cull(False)\n\n                        REVO_CACHE[rr_id] = rr_cache_entry\n                        LOGGER.info('Revocation cache imported entry for rev reg id %s', rr_id)\n\n        LOGGER.debug('parse <<< %s', timestamp)\n        return timestamp"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef associate(base_dir: str, rr_id: str, tails_hash: str) -> None:\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        directory = join(base_dir, cd_id)\n        cwd = getcwd()\n        makedirs(directory, exist_ok=True)\n        chdir(directory)\n        symlink(tails_hash, rr_id)\n        chdir(cwd)", "response": "Associate tails_hash with rr_id with base_dir."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns correct subdirectory of input base dir for artifacts corresponding to input rev reg id.", "response": "def dir(base_dir: str, rr_id: str) -> str:\n        \"\"\"\n        Return correct subdirectory of input base dir for artifacts corresponding to input rev reg id.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param rr_id: rev reg id\n        \"\"\"\n\n        return join(base_dir, rev_reg_id2cred_def_id(rr_id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef linked(base_dir: str, rr_id: str) -> str:\n\n        cd_id = rev_reg_id2cred_def_id(rr_id)\n        link = join(base_dir, cd_id, rr_id)\n        return join(base_dir, cd_id, readlink(link)) if islink(link) else None", "response": "Get the path to the tails file associated with a revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning set of all symbolic links in base_dir associating their respective tails files in specified base tails directory and in specified issuer DID.", "response": "def links(base_dir: str, issuer_did: str = None) -> set:\n        \"\"\"\n        Return set of all paths to symbolic links (rev reg ids) associating\n        their respective tails files, in specified base tails directory, on\n        input issuer DID if specified.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param issuer_did: issuer DID of interest\n        :return: set of paths to symbolic links associating tails files\n        \"\"\"\n\n        return {join(dp, f) for dp, dn, fn in walk(base_dir) for f in fn\n            if islink(join(dp, f)) and (not issuer_did or f.startswith('{}:4:'.format(issuer_did)))}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlinked(base_dir: str) -> set:\n\n        return {join(dp, f) for dp, dn, fn in walk(base_dir)\n            for f in fn if isfile(join(dp, f)) and not islink(join(dp, f))} - {\n                join(dirname(path_link), readlink(path_link)) for path_link in Tails.links(base_dir)}", "response": "Return all paths to tails files in specified base directory that are not links to revocation registry identifiers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef next_tag(base_dir: str, cd_id: str) -> (str, int):\n\n        tag = 1 + max([int(rev_reg_id2tag(basename(f)))\n            for f in Tails.links(base_dir) if cd_id in basename(f)] + [-1])  # -1: next tag is '0' if no tags so far\n        size = min(2**(tag + 8), 4096)\n        return (tag, size)", "response": "Return the next tag name available for a new credential definition identifier on input base directory and credential definition identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the revocation registry identifier for the input credential definition identifier in input directory.", "response": "def current_rev_reg_id(base_dir: str, cd_id: str) -> str:\n        \"\"\"\n        Return the current revocation registry identifier for\n        input credential definition identifier, in input directory.\n\n        Raise AbsentTails if no corresponding tails file, signifying no such revocation registry defined.\n\n        :param base_dir: base directory for tails files, thereafter split by cred def id\n        :param cd_id: credential definition identifier of interest\n        :return: identifier for current revocation registry on input credential definition identifier\n        \"\"\"\n\n        tags = [int(rev_reg_id2tag(basename(f))) for f in Tails.links(base_dir)\n            if cd_id in basename(f)]\n        if not tags:\n            raise AbsentTails('No tails files present for cred def id {}'.format(cd_id))\n\n        return rev_reg_id(cd_id, str(max(tags)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path to the current tails file.", "response": "def path(self) -> str:\n        \"\"\"\n        Accessor for (stringified) path to current tails file.\n\n        :return: (stringified) path to current tails file.\n        \"\"\"\n\n        cfg = json.loads(self._tails_cfg_json)\n        return join(cfg['base_dir'], cfg['file'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfunctions returns indices that contain non - noise genes as an integer array", "response": "def parseNoise(rawArray): \n    '''\n    Function returns indices that contain non-noisy genes as an integer array\n    :param rawArray: numpy ndarray of data set\n    :return nnGenes : numpy ndarray of non-noise gene indices\n    '''\n    nnGenes=[] \n    for i in range(0,(rawArray.shape[1])): #Checks all genes\n        count0=np.asarray(np.where(rawArray[:,i]==0))\n        count1=np.asarray(np.where(rawArray[:,i]==1))\n        if ((count1.shape[1]+count0.shape[1])<rawArray.shape[0]):\n            nnGenes=np.append(nnGenes,i)\n    return nnGenes.astype(int)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning creates an indexed array of gene expression values from ndarray of unparsed data set and ndarray of gene indices of interest", "response": "def mkIndexedArr(unparsedArr,nnGeneIndex):\n    '''\n    Function returns ndarray of gene expression values from ndarray of unmodified data set and ndarray of gene indices\n    :param unparsedArr: numpy ndarray of unmodified data set\n    :param nnGeneIndex: numpy ndarray of indices of genes of interest\n    :return nnGeneExp:  numpy ndarray of data set including all cells and expression data only of genes of interest \n    '''\n    nnGeneExp=np.zeros([unparsedArr.shape[0],nnGeneIndex.shape[0]],dtype=float) #initializes new array for output\n    for i in range(0,nnGeneIndex.shape[0]): #fill in the new array based on indices of interest\n        nnGeneExp[:,i]=unparsedArr[:,nnGeneIndex[i]]\n    return nnGeneExp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfunction returns an ndarray by performing a pointwise inverse hyperbolic sine transformation on the input array", "response": "def pwArcsinh(inputArray,constant): \n    '''\n    Function returns an ndarray by performing a pointwise inverse hyperbolic sine transformation on the input ndarray \n    :param inputArray: ndarray of gene expression data in raw count or RPKM format \n    :param contstant: some constant to normalize for total read count number\n    :return transformed: ndarray of transformed inputArray \n    '''\n    #this function assumes rows are cells and columns are genes in the input array\n    countsum=np.sum(inputArray,axis=1) #calculate the total number of counts per cell\n    holder=np.zeros_like(inputArray,dtype=float)\n    transformed=np.zeros_like(inputArray,dtype=float) #initialize the output array\n    print (\"Completion:\")\n    for i in range(0,inputArray.shape[0]):\n        print (((i/inputArray.shape[0])*100),end='\\r')#progress meter \n        holder[i,:]=(inputArray[i,:]/countsum[i])*constant #divide each genes counts by total number of counts in cell\n        transformed[i,:]=np.arcsinh((holder[i,:]/constant*1000)) #do arcsinh transform for each element of matrix\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfunctioning returns an adjacency matrix based on the euclidean distance of each gene s expression across all cells and the minimum number of connections needed for minimally connected graph", "response": "def adaptive_knn_graph(traj_dist,k): \n    '''\n    Function returns an adjacency matrix based on the euclidean distance of each gene's expression across all cells\n    :param traj_dist: ndarray of euclidean distances\n    :param k: int of the minimum number of connections needed for a minimally connected graph, calculated by min_conn_k()\n    :return adj_mat: ndarray representing the calculated adjacency matrix\n    '''\n    adj_mat = np.zeros_like(traj_dist,dtype=float) #initialize output matrix\n    knn=(np.transpose(np.argsort(traj_dist,0))) #returns the indices that would sort an array, transposed for consistent formatting\n    for i in range(0,(traj_dist.shape[0])): #go through the whole distance matrix and set the corresponding adjacencies\n        adj_mat[i,knn[i,range(1,k)]]=traj_dist[i,knn[i,range(1,k)]]\n    return adj_mat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef min_conn_k(traj_exp):\n    '''\n    Function returns the minimum number of connections, k, that are required to form a fully connected graph based gene expression data\n    :param traj_exp: ndarray representing gene expression\n    :return k: int of the minimum number of connections needed for a minimally connected graph\n    '''\n    traj_dist=sps.distance.squareform(sps.distance.pdist(traj_exp)) #this is the gene expression euclidean distance calculation\n    conn_comp=2 #just a starting value for the number of connected components in the graph\n    k=0 #just a starting value for the number of neighbors required in the graph\n    while (conn_comp>1):\n        k=k+1 #each time this loops, increase the number of neighbors by 1 until we get 1 graph component(indicating a fully connected graph)\n        adj_mat=adaptive_knn_graph(traj_dist,k) #uses adaptive_knn_graph to generate an adjacency matrix\n        traj_graph = nx.Graph(adj_mat) #uses that adjacency matrix to make a graph\n        conn_comp = nx.number_connected_components(traj_graph) #count the number of connected components in that graph\n    return k", "response": "Function returns the minimum number of connections needed for a minimally connected graph based gene expression data\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions returns the sum of squared differences in the expression of a gene per cell", "response": "def dev_ij(i,j,traj_exp,adj_mat): #takes adjacency matrix and raw expression data\n    '''\n    Function returns the sum of squared differences in the expression of a gene, per cell\n    :param i: int representing the index of the cell being processed \n    :param j: int representing the index of the gene being processed\n    :param traj_exp: ndarray representing gene expression\n    :param adj_mat: ndarray representing the calculated adjacency matrix\n    :return: float representing the sum of squared differences\n    '''\n    t=np.asmatrix(traj_exp) \n    wh=np.where(adj_mat[i]>0) #looks for indices where the adjacency is greater than 0\n    return np.sum(np.square(t[i,j]-t[wh,j],dtype=float),dtype=float)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction returns an ndarray of ratios calculated by dividing the summed neighborhood variances by the global variance", "response": "def selection_val(traj_exp,adj_mat):\n    '''\n    Function returns an ndarray of ratios calculated by dividing the summed neighborhood variances by the global variance\n    :param traj_exp: ndarray representing gene expression\n    :param adj_mat: ndarray representing the calculated adjacency matrix\n    :return val: ndarray representing the ratio of summed neighborhood variances by the global variance \n    '''\n    r = traj_exp.shape[0] #keep track of the rows\n    c = traj_exp.shape[1] #keep track of the columns\n    k = np.sum(adj_mat[0]>0,dtype=float) #k calculation based off of adjacency matrix\n    dev=np.zeros_like(traj_exp,dtype=float) #initialize matrix to store dev_ij values\n    val=np.zeros(traj_exp.shape[1],dtype=float) #initialize val matrix to store variance values\n    print (\"Start global variance calculation\")\n    ivar=np.var(traj_exp,axis=0,ddof=1) #calculate variance in traj_exp, this is the global variance matrix\n    print (\"Finished global variance calculation\")\n    print (\"Start neighborhood variance calculation\") \n    print (\"Completion:\")\n    for i in range(0,r): #start of dev_ij calculation loop\n        print (((i/r)*100),end='\\r') #progress meter\n        for j in range(0,c):\n            dev[i,j] = dev_ij(i,j,traj_exp,adj_mat) #this is the part that takes the longest\n    print (\"Finished neighborhood variance calculation\")\n    rowsumdev=np.sum(dev,axis=0) #axis-wise sum of deviations calculated between i and j\n    print (\"Start global to neighborhood variance ratio calculation\")\n    for i in range(0,c): #set values to variance/deviation calculation in loop\n        if rowsumdev[i]!=0: #pretty much just throw out anything that has devsum=0, when considering deviation sums of 0, we could get a divide by zero error\n            val[i] = ((ivar[i])/(rowsumdev[i]/(r*k-1))) #variance calculation done here and written into val matrix\n    print (\"Finished global to neighborhood variance ratio calculation\")\n    return val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrap function to perform neighborhood variance based feature selection", "response": "def select_genes(embedding):\n    '''\n    Wrapper function to perform neighborhood variance based feature selection, will throw error if input is not ndarray\n    :param embedding: ndarray representing gene expression, consists of cells as rows and genes as columns\n    :return genes_mat: ndarray of selected genes\n    '''\n    if not ( isinstance( embedding, np.ndarray)):\n        raise TypeError( 'data variable must be numpy ndarray')\n    print (\"Start min_conn_k\")\n    start=time.time() #just start a timer to see how long this whole thing takes in seconds\n    k = min_conn_k(embedding) #run the min_conn_k to find k\n    print (k, \"connections needed\") #prints the k for reference\n    print (\"Finished min_conn_k \")\n    r = embedding.shape[0]\n    c = embedding.shape[1]\n    print (\"Start traj_dist\")\n    traj_dist = sps.distance.squareform(sps.distance.pdist(embedding)) #distance calculation to use in making the adjacency matrix\n    print (\"Finished traj_dist\")\n    print (\"Start adaptive_knn_graph\")\n    adj_mat = adaptive_knn_graph(traj_dist,k) #use traj_dist to make adjacency matrix\n    print (\"Finished adaptive_knn_graph\")\n    sel_vals = selection_val(embedding,adj_mat) #calculate selection values, or the ratios of global variance to neighborhood variance\n    print (\"Finished selection_val\") \n    genes = np.where(sel_vals > 1) #decision made here, if neighborhood variance is lower than global, select that gene\n    print (\"Finished gene selection in\",time.time()-start, \"seconds\") \n    #genes_mat = np.asmatrix(genes) #output of genes as a matrix\n    print (\"done\")\n    return np.squeeze( genes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions to generate randomly sampled datasets with replacement.", "response": "def subsample(partitions,dataset,seed):\n    '''\n    Function to generate randomly sampled datasets with replacement. This is in the context of cells in the native dataset\n    which are the rows of the matrix\n    :param partitions: int designating the number of evenly spaced sample sizes to randomly select from the native dataset\n    :param dataset: DataFrame of the native dataset compatible with the suffle function\n    :param seed: pseudorandom seed, compatible with the replicate wrapper since it adds the index to the seed\n    :return subOut: dictionary of the randomly sampled datasets, keys are the number of cells\n    '''\n    parts=np.arange(dataset.shape[0]/partitions,dataset.shape[0],dataset.shape[0]/partitions).astype(int)    \n    subOut={}\n    for i in range(parts.shape[0]):\n        subOut[\"{0}cells\".format(parts[i])]=np.asarray(shuffle(dataset,random_state=seed))[0:parts[i],:]\n    return subOut"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subsampleReplicates(repNumber,partitions,dataset,seed):\n    '''\n    Wrapper function that generates replicate datasets using the subsampling function.\n    :param repNumber: int number of replicates to generate based on the parameters given\n    :param partitions: int designating the number of evenly spaced sample sizes to randomly select from the native dataset\n    :param dataset: DataFrame of the native dataset compatible with the suffle function\n    :param seed: pseudorandom seed, compatible with the replicate wrapper since it adds the index to the seed\n    :return repOut: nested dictionary of the randomly sampled datasets, keys are the replicate number\n    '''\n    repOut={}\n    for i in range(repNumber):\n        repOut[\"replicate{0}\".format(i)]=subsample(partitions,dataset,seed+i)\n    return repOut", "response": "Wrapper function that generates a set of randomly sampled replicates from the native dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dictToFile(dictionary,replicateKey,outFileName):\n    '''\n    Function to write dictionary data, from subsampleReplicates, to file an hdf5 file. \n    :param dictionary: nested dictionary returned by subsampleReplicates\n    :param replicateKey: string designating the replicate written to file\n    :param outFileName: string defining the hdf5 filename\n    '''\n    replicateToFile=h5py.File(outFileName,\"w\")\n    for i in range(len(dictionary[replicateKey])):\n        replicateToFile.create_dataset(\"{}\".format(dictionary[replicateKey].keys()[i])\\\n                                    ,data=dictionary[replicateKey].values()[i]\\\n                                    ,compression=\"gzip\")\n    replicateToFile.close()", "response": "Function to write nested dictionary data from subsampleReplicates to hdf5 file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef iter_otus(nexson, nexson_version=None):\n    if nexson_version is None:\n        nexson_version = detect_nexson_version(nexson)\n    if not _is_by_id_hbf(nexson_version):\n        convert_nexson_format(nexson, BY_ID_HONEY_BADGERFISH)  # TODO shouldn't modify...\n    nex = get_nexml_el(nexson)\n    otus_group_by_id = nex['otusById']\n    group_order = nex.get('^ot:otusElementOrder', [])\n    if len(group_order) < len(otus_group_by_id):\n        group_order = list(otus_group_by_id.keys())\n        group_order.sort()\n    for otus_group_id in group_order:\n        otus_group = otus_group_by_id[otus_group_id]\n        otu_by_id = otus_group['otuById']\n        ti_order = list(otu_by_id.keys())\n        for otu_id in ti_order:\n            otu = otu_by_id[otu_id]\n            yield otus_group_id, otu_id, otu", "response": "generator over all OTUs in all OTUs group elements."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef iter_trees(nexson, nexson_version=None):\n    if nexson_version is None:\n        nexson_version = detect_nexson_version(nexson)\n    nex = get_nexml_el(nexson)\n    if _is_by_id_hbf(nexson_version):\n        trees_group_by_id = nex['treesById']\n        group_order = nex.get('^ot:treesElementOrder', [])\n        if len(group_order) < len(trees_group_by_id):\n            group_order = list(trees_group_by_id.keys())\n            group_order.sort()\n        for trees_group_id in group_order:\n            trees_group = trees_group_by_id[trees_group_id]\n            tree_by_id = trees_group['treeById']\n            ti_order = trees_group.get('^ot:treeElementOrder', [])\n            if len(ti_order) < len(tree_by_id):\n                ti_order = list(tree_by_id.keys())\n                ti_order.sort()\n            for tree_id in ti_order:\n                tree = tree_by_id[tree_id]\n                yield trees_group_id, tree_id, tree\n    else:\n        for trees_group in nex.get('trees', []):\n            trees_group_id = trees_group['@id']\n            for tree in trees_group.get('tree', []):\n                tree_id = tree['@id']\n                yield trees_group_id, tree_id, tree", "response": "generator over all trees in all trees elements"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a v1. 2 otuById dict and sets the original label of every OTU in the v1. 2 otuById dict.", "response": "def label_to_original_label_otu_by_id(otu_by_id):\n    \"\"\"Takes a v1.2 otuById dict and, for every otu,\n    checks if ot:originalLabel exists. If it does not,\n    but @label does, then ot:originalLabel is set to\n    @label and @label is deleted.\n    \"\"\"\n    for val in otu_by_id.values():\n        orig = val.get('^ot:originalLabel')\n        if orig is None:\n            label = val.get('@label')\n            if label:\n                del val['@label']\n                val['^ot:originalLabel'] = label"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a nexson object and merges the OTUs and trees into a single tree.", "response": "def merge_otus_and_trees(nexson_blob):\n    \"\"\"Takes a nexson object:\n        1. merges trees elements 2 - # trees into the first trees element.,\n        2. merges otus elements 2 - # otus into the first otus element.\n        3. if there is no ot:originalLabel field for any otu,\n            it sets that field based on @label and deletes @label\n        4. merges an otu elements using the rule:\n              A. treat (ottId, originalLabel) as a key\n              B. If otu objects in subsequent trees match originalLabel and\n                have a matching or absent ot:ottId, then they are merged into\n                the same OTUs (however see C)\n              C. No two leaves of a tree may share an otu (though otu should\n                be shared across different trees). It is important that\n                each leaf node be mapped to a distinct OTU. Otherwise there\n                will be no way of separating them during OTU mapping. we\n                do this indirectly by assuring to no two otu objects in the\n                same otus object get merged with each other (or to a common\n                object)\n\n        5. correct object references to deleted entities.\n\n    This function is used to patch up NexSONs created by multiple imports, hence the\n    substitution of '@label' for 'ot:originalLabel'. Ids are arbitrary for imports from\n    non-nexml tools, so matching is done based on names. This should mimic the behavior\n    of the analysis tools that produced the trees (for most/all such tools unique names\n    constitute unique OTUs).\n    \"\"\"\n    id_to_replace_id = {}\n    orig_version = detect_nexson_version(nexson_blob)\n    convert_nexson_format(nexson_blob, BY_ID_HONEY_BADGERFISH)\n    nexson = get_nexml_el(nexson_blob)\n    otus_group_order = nexson.get('^ot:otusElementOrder', [])\n    # (ott, orig) -> list of otu elements\n    retained_mapped2otu = {}\n    # orig -> list of otu elements\n    retained_orig2otu = {}\n    # For the first (entirely retained) group of otus:\n    #   1. assure that originalLabel is filled in\n    #   2. register the otu in retained_mapped2otu and retained_orig2otu\n    # otu elements that have no label, originalLabel or ottId will not\n    #   be registered, so they'll never be matched.\n    retained_ogi = None\n    if len(otus_group_order) > 0:\n        otus_group_by_id = nexson['otusById']\n        retained_ogi = otus_group_order[0]\n        retained_og = otus_group_by_id[retained_ogi]\n        retained_og_otu = retained_og.setdefault('otuById', {})\n        label_to_original_label_otu_by_id(retained_og_otu)\n        for oid, otu in retained_og_otu.items():\n            ottid = otu.get('^ot:ottId')\n            orig = otu.get('^ot:originalLabel')\n            key = (ottid, orig)\n            if key != (None, None):\n                m = retained_mapped2otu.setdefault(key, [])\n                t = (oid, otu)\n                m.append(t)\n                if orig is not None:\n                    m = retained_orig2otu.setdefault(orig, [])\n                    m.append(t)\n        # For each of the other otus elements, we:\n        #   1. assure that originalLabel is filled in\n        #   2. decide (for each otu) whether it will\n        #       be added to retained_og or merged with\n        #       an otu already in retained_og. In the\n        #       case of the latter, we add to the\n        #       replaced_otu dict (old oid as key, new otu as value)\n        for ogi in otus_group_order[1:]:\n            # _LOG.debug('retained_mapped2otu = {r}'.format(r=retained_mapped2otu))\n            og = otus_group_by_id[ogi]\n            del otus_group_by_id[ogi]\n            otu_by_id = og.get('otuById', {})\n            label_to_original_label_otu_by_id(otu_by_id)\n            used_matches = set()\n            id_to_replace_id[ogi] = retained_ogi\n            for oid, otu in otu_by_id.items():\n                ottid = otu.get('^ot:ottId')\n                orig = otu.get('^ot:originalLabel')\n                key = (ottid, orig)\n                if key == (None, None):\n                    retained_og[oid] = otu\n                else:\n                    match_otu = None\n                    mlist = retained_mapped2otu.get(key)\n                    if mlist is not None:\n                        for m in mlist:\n                            if m[0] not in used_matches:\n                                # _LOG.debug('Matching {k} to {m}'.format(k=repr(key), m=repr(m)))\n                                match_otu = m\n                                break\n                                # else:\n                                #    _LOG.debug('{k} already in {m}'.format(k=repr(m[0]), m=repr(used_matches)))\n                    if match_otu is None:\n                        # _LOG.debug('New el: {k} mlist = {m}'.format(k=repr(key), m=repr(mlist)))\n                        mlist = retained_orig2otu.get(orig, [])\n                        for m in mlist:\n                            if m[0] not in used_matches:\n                                match_otu = m\n                                break\n                    if match_otu is not None:\n                        id_to_replace_id[oid] = match_otu[0]\n                        used_matches.add(match_otu[0])\n                        _merge_otu_do_not_fix_references(otu, match_otu[1])\n                    else:\n                        assert oid not in retained_og_otu\n                        retained_og_otu[oid] = otu\n                        m = retained_mapped2otu.setdefault(key, [])\n                        t = (oid, otu)\n                        m.append(t)\n                        if orig is not None:\n                            m = retained_orig2otu.setdefault(orig, [])\n                            m.append(t)\n        nexson['^ot:otusElementOrder'] = [retained_ogi]\n    # Move all of the tree elements to the first trees group.\n    trees_group_order = nexson.get('^ot:treesElementOrder', [])\n    if len(trees_group_order) > 0:\n        assert retained_ogi is not None  # should not be able to get here with trees, but no OTUs\n        trees_group_by_id = nexson['treesById']\n        retained_tgi = trees_group_order[0]\n        retained_tg = trees_group_by_id[retained_tgi]\n        retained_tg['@otus'] = retained_ogi\n        retained_tg_tree_obj = retained_tg.get('treeById', {})\n        for tgi in trees_group_order[1:]:\n            tg = trees_group_by_id[tgi]\n            del trees_group_by_id[tgi]\n            id_to_replace_id[tgi] = retained_tgi\n            retained_tg['^ot:treeElementOrder'].extend(tg['^ot:treeElementOrder'])\n            for tid, tree_obj in tg.get('treeById', {}).items():\n                retained_tg_tree_obj[tid] = tree_obj\n        for tree_obj in retained_tg_tree_obj.values():\n            for node in tree_obj.get('nodeById', {}).values():\n                o = node.get('@otu')\n                if o is not None:\n                    r = id_to_replace_id.get(o)\n                    if r is not None:\n                        node['@otu'] = r\n        nexson['^ot:treesElementOrder'] = [retained_tgi]\n\n    replace_entity_references_in_meta_and_annotations(nexson, id_to_replace_id)\n    convert_nexson_format(nexson_blob, orig_version)\n    return nexson_blob"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the nexml2json attribute or the default code for badgerfish", "response": "def detect_nexson_version(blob):\n    \"\"\"Returns the nexml2json attribute or the default code for badgerfish\"\"\"\n    n = get_nexml_el(blob)\n    assert isinstance(n, dict)\n    return n.get('@nexml2json', BADGER_FISH_NEXSON_VERSION)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn d [ k ] if the value is not a list", "response": "def _index_list_of_values(d, k):\n    \"\"\"Returns d[k] or [d[k]] if the value is not a list\"\"\"\n    v = d[k]\n    if isinstance(v, list):\n        return v\n    return [v]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_index_list_of_values(d, k, def_value=None):\n    v = d.get(k, def_value)\n    if v is None:\n        return []\n    if isinstance(v, list):\n        return v\n    return [v]", "response": "Like _index_list_of_values but uses get to access the key and returns an empty list if the key is absent."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_value_to_dict_bf(d, k, v):\n    prev = d.get(k)\n    if prev is None:\n        d[k] = v\n    elif isinstance(prev, list):\n        if isinstance(v, list):\n            prev.extend(v)\n        else:\n            prev.append(v)\n    else:\n        if isinstance(v, list):\n            x = [prev]\n            x.extend(v)\n            d[k] = x\n        else:\n            d[k] = [prev, v]", "response": "Adds the k - > v mapping to d."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_uniq_value_to_dict_bf(d, k, v):\n    prev = d.get(k)\n    if prev is None:\n        d[k] = v\n    elif isinstance(prev, list):\n        if not isinstance(v, list):\n            v = [v]\n        for sel in v:\n            found = False\n            for el in prev:\n                if el == sel:\n                    found = True\n                    break\n            if not found:\n                prev.append(sel)\n    else:\n        if isinstance(v, list):\n            prev = [prev]\n            for sel in v:\n                found = False\n                for el in prev:\n                    if el == sel:\n                        found = True\n                        break\n                if not found:\n                    prev.append(sel)\n            if len(prev) > 1:\n                d[k] = prev\n        elif prev != v:\n            d[k] = [prev, v]", "response": "Like _add_value_to_dict_bf but will not add v if another node in under key k has the same value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndebugging helper. Prints out el contents.", "response": "def _debug_dump_dom(el):\n    \"\"\"Debugging helper. Prints out `el` contents.\"\"\"\n    import xml.dom.minidom\n    s = [el.nodeName]\n    att_container = el.attributes\n    for i in range(att_container.length):\n        attr = att_container.item(i)\n        s.append('  @{a}=\"{v}\"'.format(a=attr.name, v=attr.value))\n    for c in el.childNodes:\n        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:\n            s.append('  {a} type=\"TEXT\" data=\"{d}\"'.format(a=c.nodeName, d=c.data))\n        else:\n            s.append('  {a} child'.format(a=c.nodeName))\n    return '\\n'.join(s)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the @about key from the obj dict if that value refers to the dict s '@id'", "response": "def _cull_redundant_about(obj):\n    \"\"\"Removes the @about key from the `obj` dict if that value refers to the\n    dict's '@id'\n    \"\"\"\n    about_val = obj.get('@about')\n    if about_val:\n        id_val = obj.get('@id')\n        if id_val and (('#' + id_val) == about_val):\n            del obj['@about']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _python_instance_to_nexml_meta_datatype(v):\n    if isinstance(v, bool):\n        return 'xsd:boolean'\n    if is_int_type(v):\n        return 'xsd:int'\n    if isinstance(v, float):\n        return 'xsd:float'\n    return 'xsd:string'", "response": "Returns xsd : string or a more specific type for a python instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _convert_hbf_meta_val_for_xml(key, val):\n    if isinstance(val, list):\n        return [_convert_hbf_meta_val_for_xml(key, i) for i in val]\n    is_literal = True\n    content = None\n    if isinstance(val, dict):\n        ret = val\n        if '@href' in val:\n            is_literal = False\n        else:\n            content = val.get('$')\n            if isinstance(content, dict) and _contains_hbf_meta_keys(val):\n                is_literal = False\n    else:\n        ret = {}\n        content = val\n    if is_literal:\n        ret.setdefault('@xsi:type', 'nex:LiteralMeta')\n        ret.setdefault('@property', key)\n        if content is not None:\n            ret.setdefault('@datatype', _python_instance_to_nexml_meta_datatype(content))\n        if ret is not val:\n            ret['$'] = content\n    else:\n        ret.setdefault('@xsi:type', 'nex:ResourceMeta')\n        ret.setdefault('@rel', key)\n    return ret", "response": "Convert to a BadgerFish - style dict for addition to a dict suitable for adding to a XML tree or for v1. 0 to v0. 0 conversion."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value of the first meta element with the property that matches prop_name or None.", "response": "def find_val_for_first_bf_l_meta(d, prop_name):\n    \"\"\"Returns the $ value of the first meta element with\n    the @property that matches @prop_name (or None).\n    \"\"\"\n    m_list = d.get('meta')\n    if not m_list:\n        return None\n    if not isinstance(m_list, list):\n        m_list = [m_list]\n    for m_el in m_list:\n        if m_el.get('@property') == prop_name:\n            return extract_meta(m_el)\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the first meta element with the given property that matches prop_name or None.", "response": "def find_nested_meta_first_bf(d, prop_name):\n    \"\"\"Returns the $ value of the first meta element with\n    the @property that matches @prop_name (or None).\n    \"\"\"\n    m_list = d.get('meta')\n    if not m_list:\n        return None\n    if not isinstance(m_list, list):\n        m_list = [m_list]\n    for m_el in m_list:\n        if m_el.get('@property') == prop_name or m_el.get('@rel') == prop_name:\n            return m_el\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_nested_meta_first(d, prop_name, version):\n    if _is_badgerfish_version(version):\n        return find_nested_meta_first_bf(d, prop_name)\n    p = '^' + prop_name\n    return d.get(p)", "response": "Returns obj. for badgerfish and val for nested literals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat Requests are of the form: http://asterank.com/api/asterank?query={query}&limit={limit} Response data formats are largely derived from NASA/JPL's Small Body Database query browser. Exceptions to this are the delta-v field (dv), the mass field (GM), and the normalized spectral type field (spec). Additional Asterank scores are included: closeness, price ($), and overall score. Sample Request This request returns an asteroid with a roughly circular orbit, low inclination, and semi-major axis less than 1.5 AU: /api/asterank?query={\"e\":{\"$lt\":0.1},\"i\":{\"$lt\":4},\"a\":{\"$lt\":1.5}}&limit=1", "response": "def asterank(query=None, limit=None):\n    '''\n    Format\n    Requests are of the form:\n\n    http://asterank.com/api/asterank?query={query}&limit={limit}\n    Response data formats are largely derived from NASA/JPL's Small Body Database query browser. Exceptions to this are the delta-v field (dv), the mass field (GM), and the normalized spectral type field (spec). Additional Asterank scores are included: closeness, price ($), and overall score.\n\n    Sample Request\n    This request returns an asteroid with a roughly circular orbit, low inclination, and semi-major axis less than 1.5 AU:\n\n    /api/asterank?query={\"e\":{\"$lt\":0.1},\"i\":{\"$lt\":4},\"a\":{\"$lt\":1.5}}&limit=1\n\n    '''\n\n    base_url = \"http://asterank.com/api/asterank?\"\n\n    if query:\n        try:\n            query = json.dumps(query)\n\n            base_url += \"query=\" + query + \"&\"\n        except:\n            raise ValueError(\"query= param is not valid json.\")\n    else:\n        raise ValueError(\n            \"query= param is missing, expecting json data format.\")\n\n    if limit:\n        if not isinstance(limit, int):\n            logger.error(\n                \"The limit arg you provided is not the type of int, ignoring it\")\n        base_url += \"limit=\" + str(limit)\n    else:\n        raise ValueError(\"limit= param is missing, expecting int\")\n\n    return dispatch_http_get(base_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode(raw: Any) -> str:\n\n    if raw is None:\n        return str(I32_BOUND)  # sentinel\n\n    stringified = str(raw)\n    if isinstance(raw, bool):\n        return '{}{}'.format(\n            ENCODE_PREFIX[bool],\n            I32_BOUND + 2 if raw else I32_BOUND + 1)  # decode gotcha: python bool('False') = True; use 2 sentinels\n    if isinstance(raw, int) and -I32_BOUND <= raw < I32_BOUND:\n        return stringified  # it's an i32, leave it (as numeric string)\n\n    hexed = '{}{}'.format(\n        ENCODE_PREFIX.get(type(raw), ENCODE_PREFIX[None]),\n        str(int.from_bytes(hexlify(stringified.encode()), 'big') + I32_BOUND))\n\n    return hexed", "response": "Encode a credential attribute value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode(value: str) -> Union[str, None, bool, int, float]:\n\n    assert value.isdigit() or value[0] == '-' and value[1:].isdigit()\n\n    if -I32_BOUND <= int(value) < I32_BOUND:  # it's an i32: it is its own encoding\n        return int(value)\n    elif int(value) == I32_BOUND:\n        return None\n\n    (prefix, value) = (int(value[0]), int(value[1:]))\n    ival = int(value) - I32_BOUND\n    if ival == 0:\n        return ''  # special case: empty string encodes as 2**31\n    elif ival == 1:\n        return False  # sentinel for bool False\n    elif ival == 2:\n        return True  # sentinel for bool True\n\n    blen = ceil(log(ival, 16)/2)\n    ibytes = unhexlify(ival.to_bytes(blen, 'big'))\n    return DECODE_PREFIX.get(prefix, str)(ibytes.decode())", "response": "Decode encoded credential attribute value."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log(self, txt: str) -> bool:\n        self.nerrors += 1\n        if self._logfile is not None:\n            print(txt, file=self._logfile)\n        return not self.logging", "response": "Log a text to the log file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _strip_nones(d: Dict[str, Any])-> Dict[str, Any]:\n        return OrderedDict({k: None if isinstance(v, JSGNull) else v for k, v in d.items()\n                            if not k.startswith(\"_\") and v is not None and v is not Empty and\n                            (issubclass(type(v), JSGObject) or\n                             (not issubclass(type(v), JSGString) or v.val is not None) and\n                             (not issubclass(type(v), AnyType) or v.val is not Empty))})", "response": "Remove any non - empty values from a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _default(self, obj: object):\n        return None if obj is JSGNull else obj.val if type(obj) is AnyType else \\\n            JSGObject._strip_nones(obj.__dict__) if isinstance(obj, JsonObj) \\\n            else cast(JSGString, obj).val if issubclass(type(obj), JSGString) else str(obj)", "response": "Return a serializable version of obj. Overrides JsonObj. _default method\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_public_url(self, doc_id, branch='master'):\n        name, path_frag = self.get_repo_and_path_fragment(doc_id)\n        return 'https://raw.githubusercontent.com/OpenTreeOfLife/' + name + '/' + branch + '/' + path_frag", "response": "Returns a GitHub URL for the given doc_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_params_match(method, parameters):\n    argspec = inspect.getargspec(method)  # pylint: disable=deprecated-method\n    default_length = len(argspec.defaults) if argspec.defaults is not None else 0\n\n    if isinstance(parameters, list):\n        if len(parameters) > len(argspec.args) and argspec.varargs is None:\n            raise InvalidParamsError(\"Too many parameters\")\n\n        remaining_parameters = len(argspec.args) - len(parameters)\n        if remaining_parameters > default_length:\n            raise InvalidParamsError(\"Not enough parameters\")\n\n    elif isinstance(parameters, dict):\n        missing_parameters = [key for key in argspec.args if key not in parameters]\n        default_parameters = set(argspec.args[len(argspec.args) - default_length:])\n        for key in missing_parameters:\n            if key not in default_parameters:\n                raise InvalidParamsError(\"Parameter {} has not been satisfied\".format(key))\n\n        extra_params = [key for key in parameters if key not in argspec.args]\n        if len(extra_params) > 0 and argspec.keywords is None:\n            raise InvalidParamsError(\"Too many parameters\")", "response": "Validates that the given parameters are exactly the method s declared parameters."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_types(parameters, parameter_types, strict_floats):\n    for name, parameter_type in parameter_types.items():\n        if name not in parameters:\n            raise InvalidParamsError(\"Parameter '{}' is missing.\".format(name))\n        if not _is_instance(parameters[name], parameter_type, strict_floats):\n            raise InvalidParamsError(\"Value '{}' for parameter '{}' is not of expected type {}.\"\n                                     .format(parameters[name], name, parameter_type))", "response": "Checks that the given parameters have the correct types."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_type_declaration(parameter_names, parameter_types):\n    if len(parameter_names) != len(parameter_types):\n        raise Exception(\"Number of method parameters ({}) does not match number of \"\n                        \"declared types ({})\"\n                        .format(len(parameter_names), len(parameter_types)))\n    for parameter_name in parameter_names:\n        if parameter_name not in parameter_types:\n            raise Exception(\"Parameter '{}' does not have a declared type\".format(parameter_name))", "response": "Checks that exactly the given parameter names have declared types."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_return_type(value, expected_type, strict_floats):\n    if expected_type is None:\n        if value is not None:\n            raise InvalidReturnTypeError(\"Returned value is '{}' but None was expected\"\n                                         .format(value))\n    elif not _is_instance(value, expected_type, strict_floats):\n        raise InvalidReturnTypeError(\"Type of return value '{}' does not match expected type {}\"\n                                     .format(value, expected_type))", "response": "Checks that the given return value is of the given type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dictionary of name - > filepath - > repo name - > filepath", "response": "def get_repos(par_list=None, **kwargs):\n    \"\"\"Returns a dictionary of name -> filepath\n    `name` is the repo name based on the dir name (not the get repo). It is not\n        terribly useful, but it is nice to have so that any mirrored repo directory can\n        use the same naming convention.\n    `filepath` will be the full path to the repo directory (it will end in `name`)\n    \"\"\"\n    _repos = {}  # key is repo name, value repo location\n    if par_list is None:\n        par_list = _get_phylesystem_parent(**kwargs)\n    elif not isinstance(par_list, list):\n        par_list = [par_list]\n    for p in par_list:\n        if not os.path.isdir(p):\n            raise ValueError('Phylesystem parent \"{p}\" is not a directory'.format(p=p))\n        for name in os.listdir(p):\n            # TODO: Add an option to filter just phylesystem repos (or any specified type?) here!\n            #  - add optional list arg `allowed_repo_names`?\n            #  - let the FailedShardCreationError work harmlessly?\n            #  - treat this function as truly for phylesystem only?\n            if os.path.isdir(os.path.join(p, name + '/.git')):\n                _repos[name] = os.path.abspath(os.path.join(p, name))\n    if len(_repos) == 0:\n        raise ValueError('No git repos in {parent}'.format(parent=str(par_list)))\n    return _repos"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_id2study_info(path, tag):\n    d = {}\n    for triple in os.walk(path):\n        root, files = triple[0], triple[2]\n        for filename in files:\n            if filename.endswith('.json'):\n                study_id = filename[:-5]\n                d[study_id] = (tag, root, os.path.join(root, filename))\n    return d", "response": "Searchers for files in this repo and returns\n    a map of study id == > study filepath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new region for the Phylesystem cache.", "response": "def _make_phylesystem_cache_region(**kwargs):\n    \"\"\"Only intended to be called by the Phylesystem singleton.\n    \"\"\"\n    global _CACHE_REGION_CONFIGURED, _REGION\n    if _CACHE_REGION_CONFIGURED:\n        return _REGION\n    _CACHE_REGION_CONFIGURED = True\n    try:\n        # noinspection PyPackageRequirements\n        from dogpile.cache import make_region\n    except:\n        _LOG.debug('dogpile.cache not available')\n        return\n    region = None\n    trial_key = 'test_key'\n    trial_val = {'test_val': [4, 3]}\n    trying_redis = True\n    if trying_redis:\n        try:\n            a = {\n                'host': 'localhost',\n                'port': 6379,\n                'db': 0,  # default is 0\n                'redis_expiration_time': 60 * 60 * 24 * 2,  # 2 days\n                'distributed_lock': False  # True if multiple processes will use redis\n            }\n            region = make_region().configure('dogpile.cache.redis', arguments=a)\n            _LOG.debug('cache region set up with cache.redis.')\n            _LOG.debug('testing redis caching...')\n            region.set(trial_key, trial_val)\n            assert trial_val == region.get(trial_key)\n            _LOG.debug('redis caching works')\n            region.delete(trial_key)\n            _REGION = region\n            return region\n        except:\n            _LOG.debug('redis cache set up failed.')\n            region = None\n    trying_file_dbm = False\n    if trying_file_dbm:\n        _LOG.debug('Going to try dogpile.cache.dbm ...')\n        first_par = _get_phylesystem_parent(**kwargs)[0]\n        cache_db_dir = os.path.split(first_par)[0]\n        cache_db = os.path.join(cache_db_dir, 'phylesystem-cachefile.dbm')\n        _LOG.debug('dogpile.cache region using \"{}\"'.format(cache_db))\n        try:\n            a = {'filename': cache_db}\n            region = make_region().configure('dogpile.cache.dbm',\n                                             expiration_time=36000,\n                                             arguments=a)\n            _LOG.debug('cache region set up with cache.dbm.')\n            _LOG.debug('testing anydbm caching...')\n            region.set(trial_key, trial_val)\n            assert trial_val == region.get(trial_key)\n            _LOG.debug('anydbm caching works')\n            region.delete(trial_key)\n            _REGION = region\n            return region\n        except:\n            _LOG.debug('anydbm cache set up failed')\n            _LOG.debug('exception in the configuration of the cache.')\n    _LOG.debug('Phylesystem will not use caching')\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main(argv):\n    import argparse\n    description = 'Uses Open Tree of Life web services to find information for each OTT ID.'\n    parser = argparse.ArgumentParser(prog='ot-taxon-info', description=description)\n    parser.add_argument('ids', nargs='+', type=int, help='OTT IDs')\n    args = parser.parse_args(argv)\n    id_list = args.ids\n    for ott_id in id_list:\n        fetch_and_write_taxon_subtree(ott_id, sys.stdout)", "response": "This function is the main entry point for the ott - taxon - info command."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_nexson(obj, warning_codes_to_skip=None, retain_deprecated=True, **kwargs):\n    if warning_codes_to_skip:\n        v = FilteringLogger(codes_to_skip=list(warning_codes_to_skip), store_messages=True)\n    else:\n        v = ValidationLogger(store_messages=True)\n    v.retain_deprecated = retain_deprecated\n    n = create_validation_adaptor(obj, v, **kwargs)\n    return v, n", "response": "Validates a NexSON object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a nexson dict and return a tuple of annotation dict v_log and adaptor", "response": "def ot_validate(nexson, **kwargs):\n    \"\"\"Returns three objects:\n        an annotation dict (NexSON formmatted),\n        the validation_log object created when NexSON validation was performed, and\n        the object of class NexSON which was created from nexson. This object may\n            alias parts of the nexson dict that is passed in as an argument.\n\n    Currently the only kwargs used is 'max_num_trees_per_study'\n    \"\"\"\n    # stub function for hooking into NexSON validation\n    codes_to_skip = [NexsonWarningCodes.UNVALIDATED_ANNOTATION]  # pylint: disable=E1101\n    v_log, adaptor = validate_nexson(nexson, codes_to_skip, **kwargs)\n    annotation = v_log.prepare_annotation(author_name='api.opentreeoflife.org/validate',\n                                          description='Open Tree NexSON validation')\n    return annotation, v_log, adaptor"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget SHA1 of the HEAD file.", "response": "def get_HEAD_SHA1(git_dir):\n    \"\"\"Not locked!\n    \"\"\"\n    head_file = os.path.join(git_dir, 'HEAD')\n    with open(head_file, 'r') as hf:\n        head_contents = hf.read().strip()\n    assert head_contents.startswith('ref: ')\n    ref_filename = head_contents[5:]  # strip off \"ref: \"\n    real_ref = os.path.join(git_dir, ref_filename)\n    with open(real_ref, 'r') as rf:\n        return rf.read().strip()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef path_for_doc(self, doc_id):\n        full_path = self.path_for_doc_fn(self.repo, doc_id)\n        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc_fn: {}'.format(self.path_for_doc_fn))\n        # _LOG.debug('>>>>>>>>>> GitActionBase.path_for_doc returning: [{}]'.format(full_path))\n        return full_path", "response": "Returns doc_dir and doc_filepath for doc_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the current branch name", "response": "def current_branch(self):\n        \"\"\"Return the current branch name\"\"\"\n        branch_name = git(self.gitdir, self.gitwd, \"symbolic-ref\", \"HEAD\")\n        return branch_name.replace('refs/heads/', '').strip()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef branch_exists(self, branch):\n        try:\n            git(self.gitdir, self.gitwd, \"rev-parse\", branch)\n        except sh.ErrorReturnCode:\n            return False\n        return True", "response": "Returns true if a branch exists False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fetch(self, remote='origin'):\n        git(self.gitdir, \"fetch\", remote, _env=self.env())", "response": "fetch from a remote"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_version_history_for_file(self, filepath):\n        # define the desired fields for logout output, matching the order in these lists!\n        GIT_COMMIT_FIELDS = ['id',\n                             'author_name',\n                             'author_email',\n                             'date',\n                             'date_ISO_8601',\n                             'relative_date',\n                             'message_subject',\n                             'message_body']\n        GIT_LOG_FORMAT = ['%H', '%an', '%ae', '%aD', '%ai', '%ar', '%s', '%b']\n        # make the final format string, using standard ASCII field/record delimiters\n        GIT_LOG_FORMAT = '%x1f'.join(GIT_LOG_FORMAT) + '%x1e'\n        try:\n            log = git(self.gitdir,\n                      self.gitwd,\n                      '--no-pager',\n                      'log',\n                      '--format=%s' % GIT_LOG_FORMAT,\n                      '--follow',  # Track file's history when moved/renamed...\n                      '--find-renames=100%',  # ... but only if the contents are identical!\n                      '--',\n                      filepath)\n            # _LOG.debug('log said \"{}\"'.format(log))\n            log = log.strip('\\n\\x1e').split(\"\\x1e\")\n            log = [row.strip().split(\"\\x1f\") for row in log]\n            log = [dict(zip(GIT_COMMIT_FIELDS, row)) for row in log]\n        except:\n            _LOG.exception('git log failed')\n            raise\n        return log", "response": "Return a dict representation of this file s commit history."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_and_commit(self, doc_filepath, author, commit_msg):\n        try:\n            git(self.gitdir, self.gitwd, \"add\", doc_filepath)\n            git(self.gitdir, self.gitwd, \"commit\", author=author, message=commit_msg)\n        except Exception as e:\n            # We can ignore this if no changes are new,\n            # otherwise raise a 400\n            if \"nothing to commit\" in e.message:  # @EJM is this dangerous?\n                _LOG.debug('\"nothing to commit\" found in error response')\n            else:\n                _LOG.exception('\"git commit\" failed')\n                self.reset_hard()\n                raise", "response": "Low level function used internally when you have an absolute filepath to add and commit"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef merge(self, branch, destination=\"master\"):\n        current_branch = self.current_branch()\n        if current_branch != destination:\n            _LOG.debug('checking out ' + destination)\n            git(self.gitdir, self.gitwd, \"checkout\", destination)\n        try:\n            git(self.gitdir, self.gitwd, \"merge\", branch)\n        except sh.ErrorReturnCode:\n            _LOG.exception('merge failed')\n            # attempt to reset things so other operations can continue\n            git(self.gitdir, self.gitwd, \"merge\", \"--abort\")\n            # raise an MergeException so that caller will know that the merge failed\n            raise MergeException()\n\n        new_sha = git(self.gitdir, self.gitwd, \"rev-parse\", \"HEAD\")\n        return new_sha.strip()", "response": "Merge the given branch to the given destination branch."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef return_document(self, doc_id, branch='master', commit_sha=None, return_WIP_map=False):\n        # _LOG.debug('return_document({s}, {b}, {c}...)'.format(s=doc_id, b=branch, c=commit_sha))\n        if commit_sha is None:\n            self.checkout(branch)\n            head_sha = get_HEAD_SHA1(self.git_dir)\n        else:\n            self.checkout(commit_sha)\n            head_sha = commit_sha\n        doc_filepath = self.path_for_doc(doc_id)\n        try:\n            with codecs.open(doc_filepath, mode='r', encoding='utf-8') as f:\n                content = f.read()\n        except:\n            content = None\n        if return_WIP_map:\n            d = self.find_WIP_branches(doc_id)\n            return content, head_sha, d\n        return content, head_sha", "response": "Return the contents of the given doc_id branch and commit_sha."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the set of documents that have changed on the master since the given commit sha.", "response": "def _get_changed_docs(self,\n                          ancestral_commit_sha,\n                          doc_id_from_repo_path,\n                          doc_ids_to_check=None):\n        \"\"\"Returns the set of documents that have changed on the master since\n        commit `ancestral_commit_sha` or `False` (on an error)\n\n        'doc_id_from_repo_path' is a required function\n\n        if `doc_ids_to_check` is passed in, it should be an iterable list of\n            IDs. Only IDs in this list will be returned.\n        \"\"\"\n        try:\n            x = git(self.gitdir,\n                    self.gitwd,\n                    \"diff-tree\",\n                    \"--name-only\",\n                    \"-r\",\n                    ancestral_commit_sha,\n                    \"master\")\n        except:\n            _LOG.exception('diff-tree failed')\n            return False\n        touched = set()\n        for f in x.split('\\n'):\n            found_id = doc_id_from_repo_path(f)\n            if found_id:\n                touched.add(found_id)\n\n        if doc_ids_to_check:\n            tc = set(doc_ids_to_check)\n            return tc.intersection(touched)\n        return touched"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving a document on the given branch and attribute the commit to author.", "response": "def _remove_document(self, gh_user, doc_id, parent_sha, author, commit_msg=None):\n        \"\"\"Remove a document\n        Remove a document on the given branch and attribute the commit to author.\n        Returns the SHA of the commit on branch.\n        \"\"\"\n        # _LOG.debug(\"@@@@@@@@ GitActionBase._remove_document, doc_id={}\".format(doc_id))\n        doc_filepath = self.path_for_doc(doc_id)\n        # _LOG.debug(\"@@@@@@@@ GitActionBase._remove_document, doc_filepath={}\".format(doc_filepath))\n\n        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)\n        prev_file_sha = None\n        if commit_msg is None:\n            msg = \"Delete document '%s' via OpenTree API\" % doc_id\n        else:\n            msg = commit_msg\n        if os.path.exists(doc_filepath):\n            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)\n            if self.doc_type == 'nexson':\n                # delete the parent directory entirely\n                doc_dir = os.path.split(doc_filepath)[0]\n                # _LOG.debug(\"@@@@@@@@ GitActionBase._remove_document, doc_dir={}\".format(doc_dir))\n                git(self.gitdir, self.gitwd, \"rm\", \"-rf\", doc_dir)\n            elif self.doc_type in ('collection', 'favorites', 'amendment'):\n                # delete just the target file\n                git(self.gitdir, self.gitwd, \"rm\", doc_filepath)\n            else:\n                raise NotImplementedError(\"No deletion rules for doc_type '{}'\".format(self.doc_type))\n            git(self.gitdir,\n                self.gitwd,\n                \"commit\",\n                author=author,\n                message=msg)\n        new_sha = git(self.gitdir, self.gitwd, \"rev-parse\", \"HEAD\").strip()\n        return {'commit_sha': new_sha,\n                'branch': branch,\n                'prev_file_sha': prev_file_sha,\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a document to the local - dep database.", "response": "def write_document(self, gh_user, doc_id, file_content, branch, author, commit_msg=None):\n        \"\"\"Given a document id, temporary filename of content, branch and auth_info\n\n        Deprecated but needed until we merge api local-dep to master...\n\n        \"\"\"\n        parent_sha = None\n        fc = tempfile.NamedTemporaryFile()\n        # N.B. we currently assume file_content is text/JSON, or should be serialized from a dict\n        if is_str_type(file_content):\n            fc.write(file_content)\n        else:\n            write_as_json(file_content, fc)\n        fc.flush()\n        try:\n            doc_filepath = self.path_for_doc(doc_id)\n            doc_dir = os.path.split(doc_filepath)[0]\n            if parent_sha is None:\n                self.checkout_master()\n                parent_sha = self.get_master_sha()\n            branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha, force_branch_name=True)\n            # create a document directory if this is a new doc EJM- what if it isn't?\n            if not os.path.isdir(doc_dir):\n                os.makedirs(doc_dir)\n            shutil.copy(fc.name, doc_filepath)\n            git(self.gitdir, self.gitwd, \"add\", doc_filepath)\n            if commit_msg is None:\n                commit_msg = \"Update document '%s' via OpenTree API\" % doc_id\n            try:\n                git(self.gitdir,\n                    self.gitwd,\n                    \"commit\",\n                    author=author,\n                    message=commit_msg)\n            except Exception as e:\n                # We can ignore this if no changes are new,\n                # otherwise raise a 400\n                if \"nothing to commit\" in e.message:  # @EJM is this dangerous?\n                    pass\n                else:\n                    _LOG.exception('\"git commit\" failed')\n                    self.reset_hard()\n                    raise\n            new_sha = git(self.gitdir, self.gitwd, \"rev-parse\", \"HEAD\")\n        except Exception as e:\n            _LOG.exception('write_document exception')\n            raise GitWorkflowError(\"Could not write to document #%s ! Details: \\n%s\" % (doc_id, e.message))\n        finally:\n            fc.close()\n        return new_sha"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrite a document from a temporary file.", "response": "def write_doc_from_tmpfile(self,\n                               doc_id,\n                               tmpfi,\n                               parent_sha,\n                               auth_info,\n                               commit_msg='',\n                               doctype_display_name=\"document\"):\n        \"\"\"Given a doc_id, temporary filename of content, branch and auth_info\n        \"\"\"\n        gh_user, author = get_user_author(auth_info)\n        doc_filepath = self.path_for_doc(doc_id)\n        doc_dir = os.path.split(doc_filepath)[0]\n        if parent_sha is None:\n            self.checkout_master()\n            parent_sha = self.get_master_sha()\n        branch = self.create_or_checkout_branch(gh_user, doc_id, parent_sha)\n\n        # build complete (probably type-specific) commit message\n        default_commit_msg = \"Update %s '%s' via OpenTree API\" % (doctype_display_name, doc_id)\n        if commit_msg:\n            commit_msg = \"%s\\n\\n(%s)\" % (commit_msg, default_commit_msg)\n        else:\n            commit_msg = default_commit_msg\n\n        # create a doc directory if this is a new document  EJM- what if it isn't?\n        if not os.path.isdir(doc_dir):\n            os.makedirs(doc_dir)\n\n        if os.path.exists(doc_filepath):\n            prev_file_sha = self.get_blob_sha_for_file(doc_filepath)\n        else:\n            prev_file_sha = None\n        shutil.copy(tmpfi.name, doc_filepath)\n        self._add_and_commit(doc_filepath, author, commit_msg)\n        new_sha = git(self.gitdir, self.gitwd, \"rev-parse\", \"HEAD\")\n        _LOG.debug('Committed document \"{i}\" to branch \"{b}\" commit SHA: \"{s}\"'.format(i=doc_id,\n                                                                                       b=branch,\n                                                                                       s=new_sha.strip()))\n        return {'commit_sha': new_sha.strip(),\n                'branch': branch,\n                'prev_file_sha': prev_file_sha,\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(src, dest, **kwargs):\n        if src == dest:\n            return None\n        trivial_order = [(i, i) for i in range(min(len(src), len(dest)))]\n        optimal_order = trivial_order\n        src_ind = 0\n        dest_ind = 0\n        add_offset = 0\n        num_deletions = 0\n        diffs = ListDiff()\n        for p in optimal_order:\n            ns, nd = p\n            while src_ind < ns:\n                diffs.add_deletion(src_ind, src[src_ind])\n                src_ind += 1\n                num_deletions += 1\n            while dest_ind < nd:\n                diffs.add_insertion(src_ind - num_deletions, add_offset, dest[dest_ind])\n                dest_ind += 1\n                add_offset += 1\n            sv, dv = src[ns], dest[nd]\n            if sv != dv:\n                rec_call = None\n                if isinstance(sv, dict) and isinstance(dv, dict):\n                    rec_call = DictDiff.create(sv, dv, **kwargs)\n                elif isinstance(sv, list) and isinstance(dv, list):\n                    rec_call = ListDiff.create(sv, dv, **kwargs)\n                elif kwargs.get('wrap_dict_in_list', False):\n                    if isinstance(sv, dict) and isinstance(dv, list):\n                        rec_call = ListDiff.create([sv], dv, **kwargs)\n                    elif isinstance(dv, dict) or isinstance(sv, list):\n                        rec_call = ListDiff.create(sv, [dv], **kwargs)\n                if rec_call is not None:\n                    diffs.add_modificaton(src_ind, rec_call)\n                else:\n                    diffs.add_modificaton(src_ind, (sv, dv))\n            src_ind += 1\n            dest_ind += 1\n        while src_ind < len(src):\n            diffs.add_deletion(src_ind, src[src_ind])\n            src_ind += 1\n        while dest_ind < len(dest):\n            diffs.add_insertion(src_ind, add_offset, dest[dest_ind])\n            dest_ind += 1\n            add_offset += 1\n        diffs.finish()\n        return diffs", "response": "Inefficient comparison of src and dest dicts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nperforming taxonomic name resolution.", "response": "def match_names(self, *valist, **kwargs):\n        \"\"\"performs taxonomic name resolution.\n        See https://github.com/OpenTreeOfLife/opentree/wiki/Open-Tree-of-Life-APIs#match_names\n        with the exception that \"ids\" in the API call is referred has the name \"id_list\" in this function.\n        The most commonly used kwargs are:\n            - context_name=<name> (see contexts and infer_context methods)\n            - do_approximate_matching=False (to speed up the search)\n            - include_dubious=True see https://github.com/OpenTreeOfLife/reference-taxonomy/wiki/taxon-flags\n            - include_deprecated=True to see deprecated taxa (see previous link to documentation about flags)\n            - wrap_response=True to return a TNRSRespose object (rather than the \"raw\" response of the web-services).\n        \"\"\"\n        if len(valist) == 1:\n            if not is_str_type(valist[0]):\n                return self.taxomachine.TNRS(*valist, **kwargs)\n        return self.taxomachine.TNRS(*valist, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_amendment(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):\n        if fourth_arg is None:\n            amendment_id, branch_name, author = first_arg, sec_arg, third_arg\n            gh_user = branch_name.split('_amendment_')[0]\n            parent_sha = self.get_master_sha()\n        else:\n            gh_user, amendment_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg\n        if commit_msg is None:\n            commit_msg = \"Delete Amendment '%s' via OpenTree API\" % amendment_id\n        return self._remove_document(gh_user, amendment_id, parent_sha, author, commit_msg)", "response": "Remove an amendment from the given branch and optionally an author."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_amendment(self, amendment_id, file_content, branch, author):\n        gh_user = branch.split('_amendment_')[0]\n        msg = \"Update Amendment '%s' via OpenTree API\" % amendment_id\n        return self.write_document(gh_user,\n                                   amendment_id,\n                                   file_content,\n                                   branch, author,\n                                   commit_msg=msg)", "response": "Write an amendment to the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_amendment_from_tmpfile(self, amendment_id, tmpfi, parent_sha, auth_info, commit_msg=''):\n        return self.write_doc_from_tmpfile(amendment_id,\n                                           tmpfi,\n                                           parent_sha,\n                                           auth_info,\n                                           commit_msg,\n                                           doctype_display_name=\"amendment\")", "response": "Given an amendment_id temporary filename of content branch and auth_info write the amendment document from the given tmpfi."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef iter_doc_filepaths(self, **kwargs):  # pylint: disable=W0613\n        with self._index_lock:\n            for doc_id, info in self._doc_index.items():\n                if not self._is_alias(doc_id):\n                    yield doc_id, info[-1]", "response": "Iterate over the document file paths in this repository."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iter_doc_objs(self, **kwargs):\n        _LOG = get_logger('TypeAwareGitShard')\n        try:\n            for doc_id, fp in self.iter_doc_filepaths(**kwargs):\n                if not self._is_alias(doc_id):\n                    # TODO:hook for type-specific parser?\n                    with codecs.open(fp, 'r', 'utf-8') as fo:\n                        try:\n                            nex_obj = anyjson.loads(fo.read())\n                            yield (doc_id, nex_obj)\n                        except Exception:\n                            pass\n        except Exception as x:\n            f = 'iter_doc_filepaths FAILED with this error:\\n{}'\n            f = f.format(str(x))\n            _LOG.warn(f)", "response": "Iterate over the file paths of all the documents in this repository."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_configuration_dict(self, secret_attrs=False):\n        rd = {'name': self.name,\n              'path': self.path,\n              'git_dir': self.git_dir,\n              'assumed_doc_version': self.assumed_doc_version,\n              'doc_dir': self.doc_dir,\n              'git_ssh': self.git_ssh, }\n        if secret_attrs:\n            rd['pkey'] = self.pkey\n        with self._index_lock:\n            si = self._doc_index\n        r = _invert_dict_list_val(si)\n        key_list = list(r.keys())\n        rd['number of documents'] = len(key_list)\n        key_list.sort()\n        m = []\n        for k in key_list:\n            v = r[k]\n            fp = k[2]\n            assert fp.startswith(self.doc_dir)\n            rp = fp[len(self.doc_dir) + 1:]\n            m.append({'keys': v, 'relpath': rp})\n        rd['documents'] = m\n        return rd", "response": "Return a dictionary of the configuration of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading the master branch resource from file", "response": "def _read_master_branch_resource(self, fn, is_json=False):\n        \"\"\"This will force the current branch to master! \"\"\"\n        with self._master_branch_repo_lock:\n            ga = self._create_git_action_for_global_resource()\n            with ga.lock():\n                ga.checkout_master()\n                if os.path.exists(fn):\n                    if is_json:\n                        return read_as_json(fn)\n                    with codecs.open(fn, 'rU', encoding='utf-8') as f:\n                        ret = f.read()\n                    return ret\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_master_branch_resource(self, content, fn, commit_msg, is_json=False):\n        # TODO: we might want this to push, but currently it is only called in contexts in which\n        # we are about to push anyway (document creation)\n        with self._master_branch_repo_lock:\n            ga = self._create_git_action_for_global_resource()\n            with ga.lock():\n                ga.checkout_master()\n                if is_json:\n                    write_as_json(content, fn)\n                else:\n                    write_to_filepath(content, fn)\n                ga._add_and_commit(fn, self._infrastructure_commit_author, commit_msg)", "response": "Write the content to the master branch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create(cls, community, record, user=None, expires_at=None,\n               notify=True):\n        \"\"\"Create a record inclusion request to a community.\n\n        :param community: Community object.\n        :param record: Record API object.\n        :param expires_at: Time after which the request expires and shouldn't\n            be resolved anymore.\n        \"\"\"\n        if expires_at and expires_at < datetime.utcnow():\n            raise InclusionRequestExpiryTimeError(\n                community=community, record=record)\n\n        if community.has_record(record):\n            raise InclusionRequestObsoleteError(\n                community=community, record=record)\n\n        try:\n            # Create inclusion request\n            with db.session.begin_nested():\n                obj = cls(\n                    id_community=community.id,\n                    id_record=record.id,\n                    user=user,\n                    expires_at=expires_at\n                )\n                db.session.add(obj)\n        except (IntegrityError, FlushError):\n            raise InclusionRequestExistsError(\n                community=community, record=record)\n\n        # Send signal\n        inclusion_request_created.send(\n            current_app._get_current_object(),\n            request=obj,\n            notify=notify\n        )\n\n        return obj", "response": "Create a record inclusion request to a community."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget an inclusion request.", "response": "def get(cls, community_id, record_uuid):\n        \"\"\"Get an inclusion request.\"\"\"\n        return cls.query.filter_by(\n            id_record=record_uuid, id_community=community_id\n        ).one_or_none()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves a logo to a file.", "response": "def save_logo(self, stream, filename):\n        \"\"\"Get a community.\"\"\"\n        logo_ext = save_and_validate_logo(stream, filename, self.id)\n        if logo_ext:\n            self.logo_ext = logo_ext\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_by_user(cls, user_id, with_deleted=False):\n        query = cls.query.filter_by(\n            id_user=user_id\n        )\n        if not with_deleted:\n            query = query.filter(cls.deleted_at.is_(None))\n\n        return query.order_by(db.asc(Community.title))", "response": "Get a community by user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef filter_communities(cls, p, so, with_deleted=False):\n        query = cls.query if with_deleted else \\\n            cls.query.filter(cls.deleted_at.is_(None))\n\n        if p:\n            p = p.replace(' ', '%')\n            query = query.filter(db.or_(\n                cls.id.ilike('%' + p + '%'),\n                cls.title.ilike('%' + p + '%'),\n                cls.description.ilike('%' + p + '%'),\n            ))\n\n        if so in current_app.config['COMMUNITIES_SORTING_OPTIONS']:\n            order = so == 'title' and db.asc or db.desc\n            query = query.order_by(order(getattr(cls, so)))\n        else:\n            query = query.order_by(db.desc(cls.ranking))\n        return query", "response": "Search for communities which match search criteria."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_record(self, record):\n        key = current_app.config['COMMUNITIES_RECORD_KEY']\n        record.setdefault(key, [])\n\n        if self.has_record(record):\n            current_app.logger.warning(\n                'Community addition: record {uuid} is already in community '\n                '\"{comm}\"'.format(uuid=record.id, comm=self.id))\n        else:\n            record[key].append(self.id)\n            record[key] = sorted(record[key])\n        if current_app.config['COMMUNITIES_OAI_ENABLED']:\n            if not self.oaiset.has_record(record):\n                self.oaiset.add_record(record)", "response": "Add a record to the community."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving an already accepted record from the community.", "response": "def remove_record(self, record):\n        \"\"\"Remove an already accepted record from the community.\n\n        :param record: Record object.\n        :type record: `invenio_records.api.Record`\n        \"\"\"\n        if not self.has_record(record):\n            current_app.logger.warning(\n                'Community removal: record {uuid} was not in community '\n                '\"{comm}\"'.format(uuid=record.id, comm=self.id))\n        else:\n            key = current_app.config['COMMUNITIES_RECORD_KEY']\n            record[key] = [c for c in record[key] if c != self.id]\n\n        if current_app.config['COMMUNITIES_OAI_ENABLED']:\n            if self.oaiset.has_record(record):\n                self.oaiset.remove_record(record)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\naccepts a record for inclusion in the community.", "response": "def accept_record(self, record):\n        \"\"\"Accept a record for inclusion in the community.\n\n        :param record: Record object.\n        \"\"\"\n        with db.session.begin_nested():\n            req = InclusionRequest.get(self.id, record.id)\n            if req is None:\n                raise InclusionRequestMissingError(community=self,\n                                                   record=record)\n            req.delete()\n            self.add_record(record)\n            self.last_record_accepted = datetime.utcnow()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreject a record for inclusion in the community.", "response": "def reject_record(self, record):\n        \"\"\"Reject a record for inclusion in the community.\n\n        :param record: Record object.\n        \"\"\"\n        with db.session.begin_nested():\n            req = InclusionRequest.get(self.id, record.id)\n            if req is None:\n                raise InclusionRequestMissingError(community=self,\n                                                   record=record)\n            req.delete()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmarks the community for deletion.", "response": "def delete(self):\n        \"\"\"Mark the community for deletion.\n\n        :param delete_time: DateTime after which to delete the community.\n        :type delete_time: datetime.datetime\n        :raises: CommunitiesError\n        \"\"\"\n        if self.deleted_at is not None:\n            raise CommunitiesError(community=self)\n        else:\n            self.deleted_at = datetime.utcnow()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting URL to community logo.", "response": "def logo_url(self):\n        \"\"\"Get URL to collection logo.\n\n        :returns: Path to community logo.\n        :rtype: str\n        \"\"\"\n        if self.logo_ext:\n            return '/api/files/{bucket}/{key}'.format(\n                bucket=current_app.config['COMMUNITIES_BUCKET_UUID'],\n                key='{0}/logo.{1}'.format(self.id, self.logo_ext),\n            )\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef oaiset(self):\n        if current_app.config['COMMUNITIES_OAI_ENABLED']:\n            from invenio_oaiserver.models import OAISet\n            return OAISet.query.filter_by(spec=self.oaiset_spec).one()\n        else:\n            return None", "response": "Returns the corresponding OAISet object for given community."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef oaiset_url(self):\n        return url_for(\n            'invenio_oaiserver.response',\n            verb='ListRecords',\n            metadataPrefix='oai_dc', set=self.oaiset_spec, _external=True)", "response": "Return the OAISet URL for given community."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the version of the community.", "response": "def version_id(self):\n        \"\"\"Return the version of the community.\n\n        :returns: hash which encodes the community id and its las update.\n        :rtype: str\n        \"\"\"\n        return hashlib.sha1('{0}__{1}'.format(\n            self.id, self.updated).encode('utf-8')).hexdigest()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_featured_or_none(cls, start_date=None):\n        start_date = start_date or datetime.utcnow()\n\n        comm = cls.query.filter(\n            FeaturedCommunity.start_date <= start_date\n        ).order_by(\n            cls.start_date.desc()\n        ).first()\n        return comm if comm is None else comm.community", "response": "Get the latest featured community or None if it s not there."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getConnectorVersion(self):\n\t\tresult = asyncResult()\n\t\tdata = self._getURL(\"/\",versioned=False)\n\t\tresult.fill(data)\n\t\tif data.status_code == 200:\n\t\t\tresult.error = False\n\t\telse:\n\t\t\tresult.error = response_codes(\"get_mdc_version\",data.status_code)\n\t\tresult.is_done = True\n\t\treturn result", "response": "GET the current Connector version.\n\n\t\t:returns:  asyncResult object, populates error and result fields\n\t\t:rtype: asyncResult"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getEndpoints(self,typeOfEndpoint=\"\"):\n\t\tq = {}\n\t\tresult = asyncResult()\n\t\tif typeOfEndpoint:\n\t\t\tq['type'] = typeOfEndpoint\n\t\t\tresult.extra['type'] = typeOfEndpoint\n\t\tdata = self._getURL(\"/endpoints\", query = q)\n\t\tresult.fill(data)\n\t\tif data.status_code == 200:\n\t\t\tresult.error = False\n\t\telse:\n\t\t\tresult.error = response_codes(\"get_endpoints\",data.status_code)\n\t\tresult.is_done = True\n\t\treturn result", "response": "Get all endpoints on the domain."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getResources(self,ep,noResp=False,cacheOnly=False):\n\t\t# load query params if set to other than defaults\n\t\tq = {}\n\t\tresult = asyncResult()\n\t\tresult.endpoint = ep\n\t\tif noResp or cacheOnly:\n\t\t\tq['noResp'] = 'true' if noResp == True else 'false'\n\t\t\tq['cacheOnly'] = 'true' if cacheOnly == True else 'false'\n\t\t# make query\n\t\tself.log.debug(\"ep = %s, query=%s\",ep,q)\n\t\tdata = self._getURL(\"/endpoints/\"+ep, query=q)\n\t\tresult.fill(data)\n\t\t# check sucess of call\n\t\tif data.status_code == 200: # sucess\n\t\t\tresult.error = False\n\t\t\tself.log.debug(\"getResources sucess, status_code = `%s`, content = `%s`\", str(data.status_code),data.content)\n\t\telse: # fail\n\t\t\tresult.error = response_codes(\"get_resources\",data.status_code)\n\t\t\tself.log.debug(\"getResources failed with error code `%s`\" %str(data.status_code))\n\t\tresult.is_done = True\n\t\treturn result", "response": "Get list of resources on an endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getResourceValue(self,ep,res,cbfn=\"\",noResp=False,cacheOnly=False):\n\t\tq = {}\n\t\tresult = asyncResult(callback=cbfn) #set callback fn for use in async handler\n\t\tresult.endpoint = ep\n\t\tresult.resource = res\n\t\tif noResp or cacheOnly:\n\t\t\tq['noResp'] = 'true' if noResp == True else 'false'\n\t\t\tq['cacheOnly'] = 'true' if cacheOnly == True else 'false'\n\t\t# make query\n\t\tdata = self._getURL(\"/endpoints/\"+ep+res, query=q)\n\t\tresult.fill(data)\n\t\tif data.status_code == 200: # immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\t\tif cbfn:\n\t\t\t\tcbfn(result)\n\t\t\treturn result\n\t\telif data.status_code == 202:\n\t\t\tself.database['async-responses'][json.loads(data.content)[\"async-response-id\"]]= result\n\t\telse: # fail\n\t\t\tresult.error = response_codes(\"resource\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result", "response": "Get the value of a specific resource on a specific endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef putResourceValue(self,ep,res,data,cbfn=\"\"):\n\t\tresult = asyncResult(callback=cbfn)\n\t\tresult.endpoint = ep\n\t\tresult.resource = res\n\t\tdata = self._putURL(\"/endpoints/\"+ep+res,payload=data)\n\t\tif data.status_code == 200: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\telif data.status_code == 202:\n\t\t\tself.database['async-responses'][json.loads(data.content)[\"async-response-id\"]]= result\n\t\telse:\n\t\t\tresult.error = response_codes(\"resource\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result", "response": "This method sends a value to a resource on an endpoint"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deleteEndpoint(self,ep,cbfn=\"\"):\n\t\t'''\n\t\tSend DELETE message to an endpoint.\n\t\t\n\t\t:param str ep: name of endpoint\n\t\t:param fnptr cbfn: Optional - callback funtion to call when operation is completed\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult(callback=cbfn)\n\t\tresult.endpoint = ep\n\t\tdata = self._deleteURL(\"/endpoints/\"+ep)\n\t\tif data.status_code == 200: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\telif data.status_code == 202:\n\t\t\tself.database['async-responses'][json.loads(data.content)[\"async-response-id\"]]= result\n\t\telse:\n\t\t\tresult.error = response_codes(\"resource\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result", "response": "Send DELETE message to an endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete all subscriptions on specified endpoint", "response": "def deleteEndpointSubscriptions(self,ep):\n\t\t'''\n\t\tDelete all subscriptions on specified endpoint ``ep``\n\t\t\n\t\t:param str ep: name of endpoint\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error`` \n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tresult.endpoint = ep\n\t\tdata = self._deleteURL(\"/subscriptions/\"+ep)\n\t\tif data.status_code == 204: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\telse:\n\t\t\tresult.error = response_codes(\"delete_endpoint_subscription\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete all subscriptions on the domain (all endpoints, all resources) :return: successful ``.status_code`` / ``.is_done``. Check the ``.error`` :rtype: asyncResult", "response": "def deleteAllSubscriptions(self):\n\t\t'''\n\t\tDelete all subscriptions on the domain (all endpoints, all resources)\n\t\t\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tdata = self._deleteURL(\"/subscriptions/\")\n\t\tif data.status_code == 204: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\telse:\n\t\t\tresult.error = response_codes(\"unsubscribe\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets all subscriptions on a given endpoint", "response": "def getEndpointSubscriptions(self,ep):\n\t\t'''\n\t\tGet list of all subscriptions on a given endpoint ``ep``\n\t\t\n\t\t:param str ep: name of endpoint\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tresult.endpoint = ep\n\t\tdata = self._getURL(\"/subscriptions/\"+ep)\n\t\tif data.status_code == 200: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\t\tresult.result = data.content\n\t\telse:\n\t\t\tresult.error = response_codes(\"unsubscribe\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getResourceSubscription(self,ep,res):\n\t\t'''\n\t\tGet list of all subscriptions for a resource ``res`` on an endpoint ``ep``\n\t\t\n\t\t:param str ep: name of endpoint\n\t\t:param str res: name of resource\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tresult.endpoint = ep\n\t\tresult.resource = res\n\t\tdata = self._getURL(\"/subscriptions/\"+ep+res)\n\t\tif data.status_code == 200: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\t\tresult.result = data.content\n\t\telse:\n\t\t\tresult.error = response_codes(\"unsubscribe\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result", "response": "Get all subscriptions for a resource on an endpoint"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset pre-subscription rules for all endpoints / resources on the domain. This can be useful for all current and future endpoints/resources. :param json JSONdata: data to use as pre-subscription data. Wildcards are permitted :return: successful ``.status_code`` / ``.is_done``. Check the ``.error`` :rtype: asyncResult", "response": "def putPreSubscription(self,JSONdata):\n\t\t'''\n\t\tSet pre-subscription rules for all endpoints / resources on the domain.\n\t\tThis can be useful for all current and future endpoints/resources.\n\t\t\n\t\t:param json JSONdata: data to use as pre-subscription data. Wildcards are permitted\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tif isinstance(JSONdata,str) and self._isJSON(JSONdata):\n\t\t\tself.log.warn(\"pre-subscription data was a string, converting to a list : %s\",JSONdata)\n\t\t\tJSONdata = json.loads(JSONdata) # convert json string to list\n\t\tif not (isinstance(JSONdata,list) and self._isJSON(JSONdata)):\n\t\t\tself.log.error(\"pre-subscription data is not valid. Please make sure it is a valid JSON list\")\n\t\tresult = asyncResult()\n\t\tdata = self._putURL(\"/subscriptions\",JSONdata, versioned=False)\n\t\tif data.status_code == 204: # immediate success with no response\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\t\tresult.result = []\n\t\telse:\n\t\t\tresult.error = response_codes(\"presubscription\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getPreSubscription(self):\n\t\t'''\n\t\tGet the current pre-subscription data from connector\n\t\t\n\t\t:return: JSON that represents the pre-subscription data in the ``.result`` field\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tdata = self._getURL(\"/subscriptions\")\n\t\tif data.status_code == 200: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.is_done = True\n\t\t\tresult.result = data.json()\n\t\telse:\n\t\t\tresult.error = response_codes(\"presubscription\",data.status_code)\n\t\t\tresult.is_done = True\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\treturn result", "response": "Get the current pre - subscription data from connector\n\t"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the callback URL. To be used in place of LongPolling when deploying a webapp. **note**: make sure you set up a callback URL in your web app :param str url: complete url, including port, where the callback url is located :param str headers: Optional - Headers to have Connector send back with all calls :return: successful ``.status_code`` / ``.is_done``. Check the ``.error`` :rtype: asyncResult", "response": "def putCallback(self,url,headers=\"\"):\n\t\t'''\n\t\tSet the callback URL. To be used in place of LongPolling when deploying a webapp.\n\t\t\n\t\t**note**: make sure you set up a callback URL in your web app\n\t\t\n\t\t:param str url: complete url, including port, where the callback url is located\n\t\t:param str headers: Optional - Headers to have Connector send back with all calls\n\t\t:return: successful ``.status_code`` / ``.is_done``. Check the ``.error``\n\t\t:rtype: asyncResult\n\t\t'''\n\t\tresult = asyncResult()\n\t\tpayloadToSend = {\"url\":url}\n\t\tif headers:\n\t\t\tpayload['headers':headers]\n\t\tdata = self._putURL(url=\"/notification/callback\",payload=payloadToSend, versioned=False)\n\t\tif data.status_code == 204: #immediate success\n\t\t\tresult.error = False\n\t\t\tresult.result = data.content\n\t\telse:\n\t\t\tresult.error = response_codes(\"put_callback_url\",data.status_code)\n\t\tresult.raw_data = data.content\n\t\tresult.status_code = data.status_code\n\t\tresult.is_done = True\n\t\treturn result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef setHandler(self,handler,cbfn):\n\t\t'''\n\t\tRegister a handler for a particular notification type.\n\t\tThese are the types of notifications that are acceptable. \n\t\t\n\t\t| 'async-responses'\n\t\t| 'registrations-expired'\n\t\t| 'de-registrations'\n\t\t| 'reg-updates'\n\t\t| 'registrations'\n\t\t| 'notifications'\n\n\t\t:param str handler: name of the notification type\n\t\t:param fnptr cbfn: function to pass the notification channel messages to.\n\t\t:return: Nothing.\n\t\t'''\n\t\tif handler == \"async-responses\":\n\t\t\tself.async_responses_callback = cbfn\n\t\telif handler == \"registrations-expired\":\n\t\t\tself.registrations_expired_callback = cbfn\n\t\telif handler == \"de-registrations\":\n\t\t\tself.de_registrations_callback = cbfn\n\t\telif handler == \"reg-updates\":\n\t\t\tself.reg_updates_callback = cbfn\n\t\telif handler == \"registrations\":\n\t\t\tself.registrations_callback = cbfn\n\t\telif handler == \"notifications\":\n\t\t\tself.notifications_callback = cbfn\n\t\telse:\n\t\t\tself.log.warn(\"'%s' is not a legitimate notification channel option. Please check your spelling.\",handler)", "response": "set handler for notifications"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef startLongPolling(self, noWait=False):\n\t\t'''\n\t\tStart LongPolling Connector for notifications.\n\t\t\n\t\t:param bool noWait: Optional - use the cached values in connector, do not wait for the device to respond\n\t\t:return: Thread of constantly running LongPoll. To be used to kill the thred if necessary.\n\t\t:rtype: pythonThread\n\t\t'''\n\t\t# check Asynch ID's against insternal database of ID's\n\t\t# Call return function with the value given, maybe decode from base64?\n\t\twait = ''\n\t\tif(noWait == True):\n\t\t\twait = \"?noWait=true\"\n\t\t# check that there isn't another thread already running, only one longPolling instance per is acceptable\n\t\tif(self.longPollThread.isAlive()):\n\t\t\tself.log.warn(\"LongPolling is already active.\")\n\t\telse:\n\t\t\t# start infinite longpolling thread\n\t\t\tself._stopLongPolling.clear()\n\t\t\tself.longPollThread.start()\n\t\t\tself.log.info(\"Spun off LongPolling thread\")\n\t\treturn self.longPollThread", "response": "Start a long polling thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stopLongPolling(self):\n\t\t'''\n\t\tStop LongPolling thread\n\t\t\n\t\t:return: none\n\t\t'''\n\t\tif(self.longPollThread.isAlive()):\n\t\t\tself._stopLongPolling.set()\n\t\t\tself.log.debug(\"set stop longpolling flag\")\n\t\telse:\n\t\t\tself.log.warn(\"LongPolling thread already stopped\")\n\t\treturn", "response": "Stop LongPolling thread\n\t\t\n\t\t:return: none"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfunctioning to handle notification data as part of Callback URL handler. :param str data: data posted to Callback URL by connector. :return: nothing", "response": "def handler(self,data):\n\t\t'''\n\t\tFunction to handle notification data as part of Callback URL handler.\n\t\t\n\t\t:param str data: data posted to Callback URL by connector. \n\t\t:return: nothing\n\t\t'''\n\t\tif isinstance(data,r.models.Response):\n\t\t\tself.log.debug(\"data is request object =  %s\", str(data.content))\n\t\t\tdata = data.content\n\t\telif isinstance(data,str):\n\t\t\tself.log.info(\"data is json string with len %d\",len(data))\n\t\t\tif len(data) == 0:\n\t\t\t\tself.log.warn(\"Handler received data of 0 length, exiting handler.\")\n\t\t\t\treturn\n\t\telse:\n\t\t\tself.log.error(\"Input is not valid request object or json string : %s\" %str(data))\n\t\t\treturn False\n\t\ttry:\n\t\t\tdata = json.loads(data)\n\t\t\tif 'async-responses' in data.keys():\n\t\t\t\tself.async_responses_callback(data)\n\t\t\tif 'notifications' in data.keys():\n\t\t\t\tself.notifications_callback(data)\n\t\t\tif 'registrations' in data.keys():\n\t\t\t\tself.registrations_callback(data)\n\t\t\tif 'reg-updates' in data.keys():\n\t\t\t\tself.reg_updates_callback(data)\n\t\t\tif 'de-registrations' in data.keys():\n\t\t\t\tself.de_registrations_callback(data)\n\t\t\tif 'registrations-expired' in data.keys():\n\t\t\t\tself.registrations_expired_callback(data)\n\t\texcept:\n\t\t\tself.log.error(\"handle router had an issue and threw an exception\")\n\t\t\tex_type, ex, tb = sys.exc_info()\n\t\t\ttraceback.print_tb(tb)\n\t\t\tself.log.error(sys.exc_info())\n\t\t\tdel tb"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nturning debugging on / off", "response": "def debug(self,onOff,level='DEBUG'):\n\t\t'''\n\t\tEnable / Disable debugging \n\t\t\n\t\t:param bool onOff: turn debugging on / off\n\t\t:return: none\n\t\t'''\n\t\tif onOff:\n\t\t\tif level == 'DEBUG':\n\t\t\t\tself.log.setLevel(logging.DEBUG)\n\t\t\t\tself._ch.setLevel(logging.DEBUG)\n\t\t\t\tself.log.debug(\"Debugging level DEBUG enabled\")\n\t\t\telif level == \"INFO\":\n\t\t\t\tself.log.setLevel(logging.INFO)\n\t\t\t\tself._ch.setLevel(logging.INFO)\n\t\t\t\tself.log.info(\"Debugging level INFO enabled\")\n\t\t\telif level == \"WARN\":\n\t\t\t\tself.log.setLevel(logging.WARN)\n\t\t\t\tself._ch.setLevel(logging.WARN)\n\t\t\t\tself.log.warn(\"Debugging level WARN enabled\")\n\t\t\telif level == \"ERROR\":\n\t\t\t\tself.log.setLevel(logging.ERROR)\n\t\t\t\tself._ch.setLevel(logging.ERROR)\n\t\t\t\tself.log.error(\"Debugging level ERROR enabled\")\n\t\telse:\n\t\t\tself.log.setLevel(logging.ERROR)\n\t\t\tself._ch.setLevel(logging.ERROR)\n\t\t\tself.log.error(\"Unrecognized debug level `%s`, set to default level `ERROR` instead\",level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the appropriate python typing to subject.", "response": "def python_cardinality(self, subject: str, all_are_optional: bool = False) -> str:\n        \"\"\"Add the appropriate python typing to subject (e.g. Optional, List, ...)\n\n        :param subject: Subject to be decorated\n        :param all_are_optional: Force everything to be optional\n        :return: Typed subject\n        \"\"\"\n        if self.multiple_elements:\n            rval = f\"typing.List[{subject}]\"\n        elif self.one_optional_element:\n            rval = subject if subject.startswith(\"typing.Optional[\") else f\"typing.Optional[{subject}]\"\n        elif self.max == 0:\n            rval = \"type(None)\"\n        else:\n            rval = subject\n        if all_are_optional and not self.one_optional_element:\n            rval = f\"typing.Optional[{rval}]\"\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef signature_cardinality(self, subject: str, all_are_optional: bool = False) -> str:\n        if self.multiple_elements:\n            rval = f\"jsg.ArrayFactory('{{name}}', _CONTEXT, {subject}, {self.min}, {self.max})\"\n        elif self.one_optional_element:\n            rval = subject if subject.startswith(\"typing.Optional[\") else f\"typing.Optional[{subject}]\"\n        elif self.max == 0:\n            rval = \"type(None)\"\n        else:\n            rval = subject\n        if all_are_optional and not self.one_optional_element:\n            rval = f\"typing.Optional[{rval}]\"\n        return rval", "response": "Add the appropriate python typing to subject"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvisits EBNFSuffixContext : return an instance of the class attribute", "response": "def visitEbnfSuffix(self, ctx: jsgParser.EbnfSuffixContext):\n        \"\"\" ebnfSuffix: QMARK | STAR | PLUS | OBRACE INT (COMMA (INT|STAR)?)? CBRACE \"\"\"\n        self._ebnftext = ctx.getText()\n        if ctx.INT():\n            self.min = int(ctx.INT(0).getText())\n            if ctx.COMMA():\n                if len(ctx.INT()) > 1:\n                    self.max = int(ctx.INT(1).getText())\n                else:\n                    self.max = None\n            else:\n                self.max = self.min\n        elif ctx.QMARK():\n            self.min = 0\n            self.max = 1\n        elif ctx.STAR():\n            self.min = 0\n            self.max = None\n        elif ctx.PLUS():\n            self.min = 1\n            self.max = None\n        else:\n            raise NotImplementedError(\"Unknown ebnf construct: {}\".format(self._ebnftext))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_python(self, infile, include_original_shex: bool=False):\n        self._context.resolve_circular_references()            # add forwards for any circular entries\n        body = ''\n        for k in self._context.ordered_elements():\n            v = self._context.grammarelts[k]\n            if isinstance(v, (JSGLexerRuleBlock, JSGObjectExpr)):\n                body += v.as_python(k)\n                if isinstance(v, JSGObjectExpr) and not self._context.has_typeid:\n                    self._context.directives.append(f'_CONTEXT.TYPE_EXCEPTIONS.append(\"{k}\")')\n            elif isinstance(v, JSGForwardRef):\n                pass\n            elif isinstance(v, (JSGValueType, JSGArrayExpr)):\n                body += f\"\\n\\n\\n{k} = {v.signature_type()}\"\n            else:\n                raise NotImplementedError(\"Unknown grammar elt for {}\".format(k))\n            self._context.forward_refs.pop(k, None)\n\n        body = '\\n' + '\\n'.join(self._context.directives) + body\n        return _jsg_python_template.format(infile=infile,\n                                           original_shex='# ' + self.text if include_original_shex else \"\",\n                                           version=__version__,\n                                           gendate=datetime.datetime.now().strftime(\"%Y-%m-%d %H:%M\"),\n                                           body=body)", "response": "Return the python representation of the document."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visitTypeDirective(self, ctx: jsgParser.TypeDirectiveContext):\n        self._context.directives.append('_CONTEXT.TYPE = \"{}\"'.format(as_token(ctx.name())))\n        self._context.has_typeid = True\n        self.visitChildren(ctx)", "response": "visitTypeDirective - Adds a type directive to the context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvisiting Type Exceptions ctx", "response": "def visitTypeExceptions(self, ctx: jsgParser.TypeExceptionsContext):\n        \"\"\" typeExceptions: DASH idref+ \"\"\"\n        for tkn in as_tokens(ctx.idref()):\n            self._context.directives.append('_CONTEXT.TYPE_EXCEPTIONS.append(\"{}\")'.format(tkn))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef visitObjectDef(self, ctx: jsgParser.ObjectDefContext):\n        name = as_token(ctx)\n        self._context.grammarelts[name] = JSGObjectExpr(self._context, ctx.objectExpr(), name)", "response": "Object Definition is a simple object expression."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\narrays definition is a JSGArrayExpr", "response": "def visitArrayDef(self, ctx: jsgParser.ArrayDefContext):\n        \"\"\" arrayDef : ID arrayExpr \"\"\"\n        self._context.grammarelts[as_token(ctx)] = JSGArrayExpr(self._context, ctx.arrayExpr())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef visitObjectMacro(self, ctx: jsgParser.ObjectExprContext):\n        name = as_token(ctx)\n        self._context.grammarelts[name] = JSGObjectExpr(self._context, ctx.membersDef(), name)", "response": "Object macro is a member of the object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef visitLexerRuleSpec(self, ctx: jsgParser.LexerRuleSpecContext):\n        self._context.grammarelts[as_token(ctx)] = JSGLexerRuleBlock(self._context, ctx.lexerRuleBlock())", "response": "visit the lexerRuleSpec as a JSGLexerRuleBlock"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for JSON files in this repo and returns a map of amendment id == > filepath", "response": "def create_id2amendment_info(path, tag):\n    \"\"\"Searches for JSON files in this repo and returns\n    a map of amendment id ==> (`tag`, dir, amendment filepath)\n    where `tag` is typically the shard name\n    \"\"\"\n    d = {}\n    for triple in os.walk(path):\n        root, files = triple[0], triple[2]\n        for filename in files:\n            if filename.endswith('.json'):\n                # trim its file extension \n                amendment_id = n = filename[:-5]\n                d[amendment_id] = (tag, root, os.path.join(root, filename))\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides the base class method and renames some properties", "response": "def get_configuration_dict(self, secret_attrs=False):\n        \"\"\"Overrides superclass method and renames some properties\"\"\"\n        cd = super(TaxonomicAmendmentsShard, self).get_configuration_dict(secret_attrs=secret_attrs)\n        # \"rename\" some keys in the dict provided\n        cd['number of amendments'] = cd.pop('number of documents')\n        cd['amendments'] = cd.pop('documents')\n        # add keys particular to this shard subclass\n        if self._next_ott_id is not None:\n            cd['_next_ott_id'] = self._next_ott_id,\n        return cd"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _determine_next_ott_id(self):\n        if self._doc_counter_lock is None:\n            self._doc_counter_lock = Lock()\n        with self._doc_counter_lock:\n            _LOG.debug('Reading \"{}\"'.format(self._id_minting_file))\n            noi_contents = self._read_master_branch_resource(self._id_minting_file, is_json=True)\n            if noi_contents:\n                self._next_ott_id = noi_contents['next_ott_id']\n            else:\n                raise RuntimeError('Stored ottid minting file not found (or invalid)!')", "response": "Read an initial value from our stored counter file and set self. _next_ott_id to None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmint the next set of ottid s in the master branch file.", "response": "def _mint_new_ott_ids(self, how_many=1):\n        \"\"\" ASSUMES the caller holds the _doc_counter_lock !\n        Checks the current int value of the next ottid, reserves a block of\n        {how_many} ids, advances the counter to the next available value,\n        stores the counter in a file in case the server is restarted.\n        Checks out master branch as a side effect.\"\"\"\n        first_minted_id = self._next_ott_id\n        self._next_ott_id = first_minted_id + how_many\n        content = u'{\"next_ott_id\": %d}\\n' % self._next_ott_id\n        # The content is JSON, but we hand-rolled the string above\n        #       so that we can use it as a commit_msg\n        self._write_master_branch_resource(content,\n                                           self._id_minting_file,\n                                           commit_msg=content,\n                                           is_json=False)\n        last_minted_id = self._next_ott_id - 1\n        return first_minted_id, last_minted_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a git action for a new amendment.", "response": "def create_git_action_for_new_amendment(self, new_amendment_id=None):\n        \"\"\"Checks out master branch as a side effect\"\"\"\n        ga = self.create_git_action()\n        assert new_amendment_id is not None\n        # id should have been sorted out by the caller\n        self.register_doc_id(ga, new_amendment_id)\n        return ga, new_amendment_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef __getDummyDateList():\n\n    D = []\n    for y in xrange(2001, 2010):\n        for d in xrange(1, 365, 1):\n            D.append('A%04d%03d' % (y, d))\n\n    return D", "response": "Generate a dummy date list for testing without \n\thitting the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the webserver formatted dates to an integer format by stripping the leading char and casting", "response": "def mkIntDate(s):\n    \"\"\"\n\tConvert the webserver formatted dates\n\tto an integer format by stripping the\n\tleading char and casting\n\t\"\"\"\n    n = s.__len__()\n    d = int(s[-(n - 1):n])\n\n    return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef modisClient(client=None,\n                product=None,\n                band=None,\n                lat=None,\n                lon=None,\n                startDate=None,\n                endDate=None,\n                chunkSize=8,\n                kmAboveBelow=0,\n                kmLeftRight=0):\n    \"\"\"\n\tmodisClient: function for building a modisData object\n\t\"\"\"\n\n    m = modisData()\n\n    m.kmABoveBelow = kmAboveBelow\n    m.kmLeftRight = kmLeftRight\n\n    if client == None:\n        client = setClient()\n\n    m.server = client.wsdl.url\n\n    if product == None:\n        prodList = client.service.getproducts()\n        return prodList\n\n    m.product = product\n\n    if band == None:\n        bandList = client.service.getbands(product)\n        return bandList\n\n    m.band = band\n\n    if lat == None or lon == None:\n        latLonErr()\n\n    m.latitude = lat\n    m.longitude = lon\n\n    # get the date list regardless so we can\n    # process it into appropriately sized chunks\n\n    dateList = client.service.getdates(lat, lon, product)\n\n    if startDate == None or endDate == None:\n        return dateList\n\n    #count up the total number of dates\n    i = -1\n    nDates = 0\n    while i < dateList.__len__() - 1:\n        i = i + 1\n\n        thisDate = mkIntDate(dateList[i])\n\n        if thisDate < startDate:\n            continue\n        if thisDate > endDate:\n            break\n\n        nDates = nDates + 1\n\n        m.dateInt.append(thisDate)\n        m.dateStr.append(dateList[i])\n\n    n = 0\n    i = -1\n    while i < dateList.__len__() - 1:\n        i = i + 1\n\n        thisDate = mkIntDate(dateList[i])\n\n        if thisDate < startDate:\n            continue\n        if thisDate > endDate:\n            break\n\n        requestStart = dateList[i]\n\n        j = min(chunkSize, dateList.__len__() - i)\n\n        while mkIntDate(dateList[i + j - 1]) > endDate:\n            j = j - 1\n\n        requestEnd = dateList[i + j - 1]\n        i = i + j - 1\n\n        data = client.service.getsubset(lat, lon, product, band, requestStart,\n                                        requestEnd, kmAboveBelow, kmLeftRight)\n\n        # now fill up the data structure with the returned data...\n\n        if n == 0:\n\n            m.nrows = data.nrows\n            m.ncols = data.ncols\n            m.cellsize = data.cellsize\n            m.scale = data.scale\n            m.units = data.units\n            m.yllcorner = data.yllcorner\n            m.xllcorner = data.xllcorner\n\n            m.data = np.zeros((nDates, m.nrows * m.ncols))\n\n        for j in xrange(data.subset.__len__()):\n            kn = 0\n\n            for k in data.subset[j].split(\",\")[5:]:\n\n                try:\n                    m.data[n * chunkSize + j, kn] = int(k)\n                except ValueError:\n                    serverDataErr()\n\n                kn = kn + 1\n\n        n = n + 1\n\n    return (m)", "response": "function for building a modisData object from a modisClient object"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending input agent s cryptonym to the distributed ledger.", "response": "async def send_nym(self, did: str, verkey: str, alias: str = None, role: str = None) -> None:\n        \"\"\"\n        Send input agent's cryptonym (including DID, verification key, plus optional alias and role)\n        to the distributed ledger.\n\n        Raise BadLedgerTxn on failure.\n\n        :param did: agent DID to send to ledger\n        :param verkey: agent verification key\n        :param alias: optional alias\n        :param role: agent role on the ledger; specify one of 'TRUSTEE', 'STEWARD', 'TRUST_ANCHOR',\n            or else '' to reset role\n        \"\"\"\n\n        LOGGER.debug(\n            'AgentRegistrar.send_nym >>> did: %s, verkey: %s, alias: %s, role: %s', did, verkey, alias, role or '')\n\n        req_json = await ledger.build_nym_request(self.did, did, verkey, alias, role or '')\n        await self._sign_submit(req_json)\n\n        LOGGER.debug('AgentRegistrar.send_nym <<<')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an ID. Note that if `prefix` is not provided, it will be `guid`, even if the `method` is `METHOD_INT`.", "response": "def create_id(self, prefix=\"guid\"):\n        \"\"\"Create an ID.\n\n        Note that if `prefix` is not provided, it will be `guid`, even if the\n        `method` is `METHOD_INT`.\n        \"\"\"\n        if self.method == IDGenerator.METHOD_UUID:\n            id_ = str(uuid.uuid4())\n        elif self.method == IDGenerator.METHOD_INT:\n            id_ = self.next_int\n            self.next_int += 1\n        else:\n            raise InvalidMethodError(self.method)\n\n        return \"%s:%s-%s\" % (self.namespace.prefix, prefix, id_)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef imagery(lon=None, lat=None, dim=None, date=None, cloud_score=None):\n    '''\n    # ----------QUERY PARAMETERS----------\n\n    # Parameter\tType\tDefault\tDescription\n    # lat\tfloat\tn/a\tLatitude\n    # lon\tfloat\tn/a\tLongitude\n    # dim\tfloat\t0.025\twidth and height of image in degrees\n    # date\tYYYY-MM-DD  today\tdate of image ----if not supplied, then the most recent image (i.e., closest to today) is returned\n    #cloud_score\tbool\tFalse\tcalculate the percentage of the image covered by clouds\n    #api_key\tstring\tvDEMO_KEY\tapi.nasa.gov key for expanded usage\n\n    # ---------EXAMPLE QUERY--------\n\n    # https://api.nasa.gov/planetary/earth/imagery?lon=100.75&lat=1.5&date=2014-02-01&cloud_score=True&api_key=DEMO_KEY\n\n    '''\n\n    base_url = \"https://api.nasa.gov/planetary/earth/imagery?\"\n\n    if not lon or not lat:\n        raise ValueError(\n            \"imagery endpoint expects lat and lon, type has to be float. Call the method with keyword args. Ex : lon=100.75, lat=1.5\")\n    else:\n        try:\n            validate_float(lon, lat)\n            # Floats are entered/displayed as decimal numbers, but your computer \n            # (in fact, your standard C library) stores them as binary. \n            # You get some side effects from this transition:\n            # >>> print len(repr(0.1))\n            # 19\n            # >>> print repr(0.1)\n            # 0.10000000000000001\n            # Thus using decimal to str transition is more reliant\n            lon = decimal.Decimal(lon)\n            lat = decimal.Decimal(lat)\n            base_url += \"lon=\" + str(lon) + \"&\" + \"lat=\" + str(lat) + \"&\"\n        except:\n            raise ValueError(\n                \"imagery endpoint expects lat and lon, type has to be float. Call the method with keyword args. Ex : lon=100.75, lat=1.5\")\n\n    if dim:\n        try:\n            validate_float(dim)\n            dim = decimal.Decimal(dim)\n            base_url += \"dim=\" + str(dim) + \"&\"\n        except:\n            raise ValueError(\"imagery endpoint expects dim to be a float\")\n\n    if date:\n        try:\n            vali_date(date)\n            base_url += \"date=\" + date + \"&\"\n        except:\n            raise ValueError(\"Incorrect date format, should be YYYY-MM-DD\")\n\n    if cloud_score == True:\n        base_url += \"cloud_score=True\" + \"&\"\n\n    req_url = base_url + \"api_key=\" + nasa_api_key()\n\n    return dispatch_http_get(req_url)", "response": "This method returns the url of the image in the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\naggregates a dataset X on columns for a set of specified values for a particular set of factors.", "response": "def aggregate(X, On=None, AggFuncDict=None, AggFunc=None,\n              AggList=None, returnsort=False, KeepOthers=True,\n              keyfuncdict=None):\n    \"\"\"\n    Aggregate a ndarray with structured dtype (or recarray) on columns for \n    given functions.\n\n    Aggregate a numpy recarray (or tabular tabarray) on a set of specified \n    factors, using specified aggregation functions.\n\n    Intuitively, this function will aggregate the dataset `X` on a set of \n    columns, whose names are listed in `On`, so that the resulting aggregate \n    data set has one record for each unique tuples of values in those columns.\n\n    The more factors listed in `On` argument, the \"finer\" is the aggregation, \n    the fewer factors, the \"coarser\" the aggregation.  For example, if::\n\n            On = 'A'\n\n    the resulting tabarray will have one record for each unique value of a in \n    X['A'], while if On = ['A', 'B'] then the resulting tabarray will have \n    one record for each unique (a, b) pair in X[['A', 'B']]\n\n    The `AggFunc` argument is a function that specifies how to aggregate the \n    factors _not_ listed in `On`, e.g. the so-called `Off` columns.  For \n    example.  For instance, if On = ['A', 'B'] and `C` is a third column, then ::\n\n            AggFunc = numpy.mean\n\n    will result in a tabarray containing a `C` column whose values are the \n    average of the values from the original `C` columns corresponding to each \n    unique (a, b) pair. \n    \n    If you want to specify a different aggreagtion method for each `Off` column,\n    use `AggFuncDict` instead of AggFunc.  `AggFuncDict` is a dictionary of\n    functions whose keys are column names.  AggFuncDict[C] will be applied to \n    the C column, AggFuncDict[D] to the D column, etc.   AggFunc and AggFuncDict\n    can be used simultaneously, with the elements of AggFuncDict overriding \n    AggFunc for the specified columns. \n    \n    Using either AggFunc or AggFuncDict, the resulting tabarray has the same \n    columns as the original tabarray.  Sometimes you want to specify the ability\n    to create new aggregate columns not corresponding to one specific column in the \n    original tabarray, and taking data from several.  To achieve this, use the\n    AggList argument.   AggList is a list of three-element lists of the form:\n         (name, func, col_names)\n    where `name` specifies the resulting column name in the aggregated tabarray,\n    `func` specifies the aggregation function, and `col_names` specifies the \n    list of columns names from the original tabarray that will be needed to \n    compute the aggregate values.  (That is, for each unique tuple `t` in the `On` \n    columns, the subarray of X[col_names] for which X[On] == t is passed to \n    `func`.)\n        \n    If an `Off` column is _not_ provided as a key in `AggFuncDict`, a default \n    aggregator function will be used:  the sum function for numerical columns, \n    concatenation for string columns.\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.aggregate`.\n\n    **Parameters**\n\n            **X** :  numpy ndarray with structured dtype or recarray\n\n                    The data set to aggregate.\n\n            **On** :  string or list of  strings, optional\n\n                    List of column names in `X`.\n\n            **AggFuncDict** :  dictionary, optional\n\n                    Dictionary where\n\n                    *   keys are some (all) column names of `X` that are NOT\n                        in `On`\n\n                    *   values are functions that can be applied to lists or\n                        numpy arrays.\n\n                    This specifies how to aggregate the factors _not_ listed in\n                    `On`, e.g. the so-called `Off` columns.\n\n            **AggFunc** :  function, optional\n\n                    Function that can be applied to lists or numpy arrays,\n                    specifying how to aggregate factors not listed in either\n                    `On` or the keys of `AggFuncDict`, e.g. a \"default\"\n                    aggregation function for the `Off` columns not explicitly\n                    listed in `AggFuncDict`.\n                    \n            **AggList** :  list, optional\n\n                    List of tuples \n\n            **returnsort** :        Boolean, optional\n\n                    If `returnsort == True`, then return a list of indices\n                    describing how `X` was sorted as a result of aggregation.\n                    Default value is `False`.\n\n    **Returns**\n\n            **agg** :  numpy ndarray with structured dtype\n\n                    Aggregated data set.\n\n            **index_array** :  numpy ndarray (int, 1D)\n\n                    Returned only if `returnsort == True`.  List of indices\n                    describing how `X` was sorted as a result of aggregation.\n\n    **See also:**\n\n            :func:`tabular.spreadsheet.aggregate_in`\n\n\n    \"\"\"\n    \n    names = X.dtype.names\n    if len(X) == 0:\n        if returnsort:\n            return [X,None]\n        else:\n            return X\n\n    if On == None:\n        On = []\n    elif isinstance(On,str):\n        On = On.split(',')\n\n    assert all([o in names for o in On]), \\\n           (\"Axes \" + str([o for o in On if o not in names]) + \n            \" can't be  found.\")\n   \n    if AggList is None:\n        AggList = []\n    \n    if AggFuncDict:\n        AggList += AggFuncDict.items()\n        \n    for (i,x) in enumerate(AggList):\n        if utils.is_string_like(x):\n            AggList[i] = (x,)\n        elif isinstance(x,tuple):\n            assert 1 <= len(x) <= 3\n            assert isinstance(x[0],str)\n            if len(x) == 2 and isinstance(x[1],tuple):\n                assert len(x[1]) == 2\n                AggList[i] = (x[0],) + x[1]\n        else:\n            raise ValueError, 'bork'\n\n    Names = [x[0] for x in AggList]\n    assert Names == utils.uniqify(Names)\n        \n    if KeepOthers:\n        AggList = [(x,) for x in X.dtype.names if x not in Names + On] + AggList\n   \n    DefaultChoices = {'string':[], 'sum':[], 'first':[]}\n\n    for (i,v) in enumerate(AggList):\n        if len(v) == 1:\n            assert v[0] in X.dtype.names\n            if AggFunc:\n                AggList[i] = v + (AggFunc,v[0])\n            else:\n                AggList[i] = v + (DefaultChooser(X,v[0], DefaultChoices),v[0])\n        elif len(v) == 2:\n            if isftype(v[1]):\n                assert v[0] in X.dtype.names\n                AggList[i] = v + (v[0],)\n            elif utils.is_string_like(v[1]):\n                if AggFunc:\n                    _a = v[1] in X.dtype.names\n                    _b = isinstance(v[1],list) and set(v[1]) <= set(X.dtype.names)\n                    assert _a or _b\n                    AggList[i] = (v[0], AggFunc, v[1])\n                else:\n                    assert v[1] in X.dtype.names\n                    AggList[i] = (v[0],\n                                  DefaultChooser(X,v[1],\n                                  DefaultChoices),\n                                  v[1])\n            else:\n                raise ValueError,'No specific of name for column.'\n        elif len(v) == 3:\n            if utils.is_string_like(v[2]):\n                assert isftype(v[1]) and v[2] in X.dtype.names\n            else:\n                assert isftype(v[1]) and \\\n                (isinstance(v[2],list) and \\\n                set(v[2]) <= set(X.dtype.names))   \n\n    if len(DefaultChoices['sum']) > 0:\n        print('No aggregation function provided for', DefaultChoices['sum'], \n              'so assuming \"sum\" by default.')\n    if len(DefaultChoices['string']) > 0:\n        print('No aggregation function provided for', DefaultChoices['string'], \n              'so assuming string concatenation by default.')\n    if len(DefaultChoices['first']) > 0:\n        print('No aggregation function provided for', DefaultChoices['first'], \n              'and neither summing nor concatenation works, so choosing ' \n              'first value by default.')\n\n    return strictaggregate(X, On, AggList, returnsort, keyfuncdict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef aggregate_in(Data, On=None, AggFuncDict=None, AggFunc=None, AggList=None,\n                  interspersed=True):\n    \"\"\"\n    Aggregate a ndarray with structured dtype or recarray\n    and include original data in the result.\n\n    Take aggregate of data set on specified columns, then add the resulting \n    rows back into data set to make a composite object containing both original \n    non-aggregate data rows as well as the aggregate rows.\n\n    First read comments for :func:`tabular.spreadsheet.aggregate`.\n\n    This function returns a numpy ndarray, with the number of rows equaling::\n\n            len(Data) + len(A)\n\n    where `A` is the the result of::\n\n            Data.aggregate(On,AggFuncDict)\n\n    `A` represents the aggregate rows; the other rows were the original data \n    rows.\n\n    This function supports _multiple_ aggregation, meaning that one can first \n    aggregate on one set of factors, then repeat aggregation on the result for \n    another set of factors, without the results of the first aggregation \n    interfering the second.  To achieve this, the method adds two new columns:\n\n    *   a column called \"__aggregates__\" specifying on which factors the rows \n        that are aggregate rows were aggregated.  Rows added by aggregating on \n        factor `A` (a column in the original data set) will have `A` in the \n        \"__aggregates__\" column.  When multiple factors `A1`, `A2` , ... are \n        aggregated on, the notation is a comma-separated list:  `A1,A2,...`.  \n        This way, when you call `aggregate_in` again, the function only \n        aggregates on the columns that have the empty char '' in their \n        \"__aggregates__\" column.\n\n    *   a column called '__color__', specifying Gray-Scale colors for \n        aggregated rows that will be used by the Data Environment system \n        browser for colorizing the  data.   When there are multiple levels of \n        aggregation, the coarser aggregate groups (e.g. on fewer factors) get \n        darker gray color then those on finer aggregate groups (e.g. more \n        factors).\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.aggregate_in`.\n\n    **Parameters**\n\n            **Data** :  numpy ndarray with structured dtype or recarray\n\n                    The data set to aggregate in.\n\n            **On** :  list of  strings, optional\n\n                    List of column names in `X`.\n\n            **AggFuncDict** :  dictionary, optional\n\n                    Dictionary where\n\n                    *   keys are some (all) column names of `X` that are NOT in \n                        `On`\n\n                    *   values are functions that can be applied to lists or\n                        numpy arrays.\n\n                    This specifies how to aggregate the factors _not_ listed in\n                    `On`, e.g. the so-called `Off` columns.\n\n            **AggFunc** :  function, optional\n\n                    Function that can be applied to lists or numpy arrays,\n                    specifying how to aggregate factors not listed in either \n                    `On` or the keys of `AggFuncDict`, e.g. a \"default\"\n                    aggregation function for the `Off` columns not explicitly\n                    listed in `AggFuncDict`.\n\n            **interspersed** :  boolean, optional\n\n                    *   If `True`, aggregate rows are interleaved with the data \n                        of which they are aggregates.\n\n                    *   If `False`, all aggregate rows placed at the end of the \n                        array.\n\n    **Returns**\n\n            **agg** :  numpy ndarray with structured dtype\n\n                    Composite aggregated data set plus original data set.\n\n    **See also:**\n\n            :func:`tabular.spreadsheet.aggregate`\n\n    \"\"\"\n\n    # See if there's an '__aggregates__ column'.  \n    # If so, strip off all those that are nontrivial.\n\n    Data = deletecols(Data,'__color__')\n    if '__aggregates__' in Data.dtype.names:\n        X = Data[Data['__aggregates__'] == ''][:]\n        OldAggregates = Data[Data['__aggregates__'] != ''][:]\n        AggVars = utils.uniqify(utils.listunion([x.split(',') for x in \n                                OldAggregates['__aggregates__']]))\n    else:\n        X = Data\n        OldAggregates = Data[0:0]\n        AggVars = []\n\n    if On == None:\n        On = []\n\n    NewAggregates = aggregate(X, On, AggFuncDict=AggFuncDict, \n                AggFunc=AggFunc, AggList=AggList, KeepOthers=True)\n    on = ','.join(On)\n    NewAggregates = addcols(NewAggregates,   \n                            utils.fromarrays([[on]*len(NewAggregates)], \n                            type=np.ndarray, names=['__aggregates__']))\n    AggVars = utils.uniqify(AggVars + On)\n    Aggregates = rowstack([OldAggregates,NewAggregates],mode='nulls')\n\n    ANLen = np.array([len(x.split(',')) for x in Aggregates['__aggregates__']])\n    U = np.array(utils.uniqify(ANLen)); U.sort()\n    [A,B] = fast.equalspairs(ANLen,U)\n    Grays = np.array(grayspec(len(U)))\n    AggColor = utils.fromarrays([Grays[A]], type=np.ndarray, \n           names = ['__color__'])\n\n    Aggregates = addcols(Aggregates,AggColor)\n\n    if not interspersed or len(AggVars) == 0:\n        return rowstack([X,Aggregates],mode='nulls')\n    else:\n        s = ANLen.argsort()\n        Aggregates = Aggregates[s[range(len(Aggregates) - 1, -1, -1)]]\n        X.sort(order = AggVars)\n        Diffs = np.append(np.append([0], 1 + (X[AggVars][1:] != \n                                    X[AggVars][:-1]).nonzero()[0]), [len(X)])\n        DiffAtts = ([[t for t in AggVars if X[t][Diffs[i]] != X[t][Diffs[i+1]]] \n                      for i in range(len(Diffs) - 2)] \n                     if len(Diffs) > 2 else []) + [AggVars]\n\n        HH = {}\n        for l in utils.uniqify(Aggregates['__aggregates__']):\n            Avars = l.split(',')\n            HH[l] = fast.recarrayequalspairs(X[Avars][Diffs[:-1]], \n                                             Aggregates[Avars])\n\n        Order = []\n        for i in range(len(Diffs)-1):\n            Order.extend(range(Diffs[i], Diffs[i+1]))\n\n            Get = []\n            for l in HH.keys():\n                Get += [len(X) + j for j in \n                        HH[l][2][range(HH[l][0][i], HH[l][1][i])] if \n                        len(set(DiffAtts[i]).intersection(\n                        Aggregates['__aggregates__'][j].split(','))) > 0 and \n                        set(Aggregates['__aggregates__'][j].split(',')) == \n                        set(l.split(','))]\n\n            Order.extend(Get)\n\n        return rowstack([X, Aggregates], mode='nulls')[Order]", "response": "Aggregate a list of data sets into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning list of gray - scale colors in HSV space.", "response": "def grayspec(k):\n    \"\"\"\n    List of gray-scale colors in HSV space as web hex triplets.\n\n    For integer argument k, returns list of `k` gray-scale colors, increasingly \n    light, linearly in the HSV color space, as web hex triplets.\n\n    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.\n\n    **Parameters**\n\n            **k** :  positive integer\n\n                    Number of gray-scale colors to return.\n\n    **Returns**\n\n            **glist** :  list of strings\n\n                    List of `k` gray-scale colors.\n\n    \"\"\"\n    ll = .5\n    ul = .8\n    delta = (ul - ll) / k\n    return [GrayScale(t) for t in np.arange(ll, ul, delta)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pivot(X, a, b, Keep=None, NullVals=None, order = None, prefix='_'):\n    '''\n    Implements pivoting on numpy ndarrays (with structured dtype) or recarrays.\n\n    See http://en.wikipedia.org/wiki/Pivot_table for information about pivot \n    tables.\n\n    Returns `X` pivoted on (a,b) with `a` as the row axis and `b` values as the \n    column axis.\n\n    So-called \"nontrivial columns relative to `b`\" in `X` are added as \n    color-grouped sets of columns, and \"trivial columns relative to `b`\" are \n    also retained as cross-grouped sets of columns if they are listed in `Keep` \n    argument.\n\n    Note that a column `c` in `X` is \"trivial relative to `b`\" if for all rows \n    i, X[c][i] can be determined from X[b][i], e.g the elements in X[c] are in \n    many-to-any correspondence with the values in X[b].\n\n    The function will raise an exception if the list of pairs of value in \n    X[[a,b]] is not the product of the individual columns values, e.g.::\n\n            X[[a,b]] == set(X[a]) x set(X[b])\n\n    in some ordering.\n\n    Implemented by the tabarray method :func:`tabular.tab.tabarray.pivot`\n\n    **Parameters**\n\n            **X** :  numpy ndarray with structured dtype or recarray\n\n                    The  data set to pivot.\n\n            **a** : string\n\n                    Column name in `X`.\n\n            **b** : string\n\n                    Another column name in `X`.\n\n            **Keep** :  list of strings, optional\n\n                    List of other columns names in `X`.\n\n            **NullVals** :  optional\n\n                    Dictionary mapping column names in `X` other than `a` or \n                    `b` to appropriate null values for their types.\n\n                    If `None`, then the null values defined by the `nullvalue`\n                    function are used, see\n                    :func:`tabular.spreadsheet.nullvalue`.\n\n            **prefix** :  string, optional\n\n                    Prefix to add to `coloring` keys corresponding to \n                    cross-grouped \"trivial columns relative to `b`\".  Default \n                    value is an underscore, '_'.\n\n    **Returns**\n\n            **ptable** :  numpy ndarray with structured dtype\n\n                    The resulting pivot table.\n\n            **coloring** :  dictionary\n\n                    Dictionary whose keys are strings and corresponding values \n                    are lists of column names (e.g. strings).\n\n                    There are two groups of keys:\n\n                    *   So-called \"nontrivial columns relative to `b`\" in `X`.  \n                        These correspond to columns in::\n\n                                    set(`X.dtype.names`) - set([a, b])\n\n                    *   Cross-grouped \"trivial columns relative to `b`\".  The \n                        `prefix` is used to distinguish these.\n\n                    The `coloring` parameter is used by the the tabarray pivot \n                    method, :func:`tabular.tab.tabarray.pivot`.\n\n                    See :func:`tabular.tab.tabarray.__new__` for more\n                    information about coloring.\n\n    '''\n\n    othernames = [o for o in X.dtype.names if o not in [a,b]]\n\n    for c in [a,b]:\n        assert c in X.dtype.names, 'Column ' + c + ' not found.'\n\n    [D,s] = fast.recarrayuniqify(X[[a,b]])\n    unique_ab = X[[a,b]][s[D.nonzero()[0]]]\n    assert len(X) == len(unique_ab) , \\\n           ('Pairs of values in columns', a, 'and', b, 'must be unique.')\n\n    [D,s] = fast.arrayuniqify(X[a])\n    unique_a = X[a][s[D.nonzero()[0]]]\n    [D,s] = fast.arrayuniqify(X[b])\n    unique_b = X[b][s[D.nonzero()[0]]]\n\n    Da = len(unique_a)\n    Db = len(unique_b)\n\n    if len(X) != Da * Db:\n        if list(X.dtype.names).index(a) < list(X.dtype.names).index(b):\n            n1 = a ; f1 = unique_a; n2 = b ; f2 = unique_b\n        else:\n            n1 = b ; f1 = unique_b; n2 = a ; f2 = unique_a\n            \n        dtype = np.dtype([(n1,f1.dtype.descr[0][1]),(n2,f2.dtype.descr[0][1])])\n        allvalues = utils.fromarrays([np.repeat(f1,\n              len(f2)),\n              np.tile(f2,len(f1))],\n              np.ndarray,\n              dtype=dtype)\n        \n        \n        missingvalues = allvalues[np.invert(fast.recarrayisin(allvalues,\n                                      X[[a,b]]))]\n\n        if NullVals == None:\n           NullVals = {}\n        \n        if not isinstance(NullVals,dict):\n            if hasattr(NullVals,'__call__'):\n                NullVals = dict([(o,NullVals(o)) for o in othernames])\n            else:\n                NullVals = dict([(o,NullVals) for o in othernames])\n\n        nullvals = utils.fromrecords([[NullVals[o] if o in NullVals.keys() \n                            else utils.DEFAULT_NULLVALUE(X[o][0]) for o in \n                            othernames]], type=np.ndarray, names=othernames)\n        \n        nullarray = nullvals.repeat(len(missingvalues))\n        Y = colstack([missingvalues, nullarray])\n        Y = Y.astype(np.dtype([(o,\n                  X.dtype[o].descr[0][1]) for o in Y.dtype.names]))\n        X = rowstack([X, Y])\n\n    X.sort(order = [a,b])\n\n    Bvals = X[b][:Db]\n    bnames = [str(bv).replace(' ','') for bv in Bvals]\n\n    assert (len(set(othernames).intersection(bnames)) == 0 and \n            a not in bnames), ('Processed values of column', b, \n                               'musn\\'t intersect with other column names.')\n\n    acol = X[a][::Db]\n\n    Cols = [acol]\n    names = [a]\n    Trivials = []\n    NonTrivials = []\n    for c in othernames:\n        Z = X[c].reshape((Da,Db))\n\n        if all([len(set(Z[:,i])) == 1 for i in range(Z.shape[1])]):\n            Trivials.append(c)\n        else:\n            NonTrivials.append(c)\n        Cols += [Z[:,i] for i in range(Z.shape[1])]\n        names += [bn + '_' + c for bn in bnames]\n    \n    if order is not None:\n        ordering = [names.index(ord) for ord in order]\n        Cols = [Cols[i] for i in ordering]\n        names = [names[i] for i in ordering]\n    dtype = np.dtype([(n,c.dtype.descr[0][1]) for (n,c) in zip(names,Cols)])\n    D = utils.fromarrays(Cols,type=np.ndarray,dtype=dtype)\n\n    coloring = {}\n    if Keep != None:\n        Trivials = set(Trivials).intersection(Keep)\n        for c in Trivials:\n            X.sort(order=[c])\n            cvals = np.array(utils.uniqify(X[c]))\n            [AA,BB] = fast.equalspairs(cvals,X[c])\n\n            for (i,cc) in enumerate(cvals):\n                blist = [str(bv).replace(' ', '') for bv in Bvals if bv in \n                         X[b][AA[i]:BB[i]]]\n                coloring[str(cc)] = [a] + [bn + '_' + d for bn in blist for d \n                                           in NonTrivials]\n                for d in NonTrivials:\n                    coloring[str(cc) + '_' + d] = [a] + blist\n\n    for c in NonTrivials:\n        coloring[c] = [a] + [bn + '_' + c for bn in bnames]\n    for bn in bnames:\n        coloring[prefix + bn] = [a] + [bn + '_' + c for c in NonTrivials]\n\n    return [D, coloring]", "response": "Pivot a set of values from a column to b."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd one or more records to the end of a numpy array.", "response": "def addrecords(X, new):\n    \"\"\"\n    Append one or more records to the end of a numpy recarray or ndarray .\n\n    Can take a single record, void or tuple, or a list of records, voids or \n    tuples.\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.addrecords`.\n\n    **Parameters**\n\n            **X** :  numpy ndarray with structured dtype or recarray\n\n                    The array to add records to.\n\n            **new** :  record, void or tuple, or list of them\n\n                    Record(s) to add to `X`.\n\n    **Returns**\n\n            **out** :  numpy ndarray with structured dtype\n\n                    New numpy array made up of `X` plus the new records.\n\n    **See also:**  :func:`tabular.spreadsheet.rowstack`\n\n    \"\"\"\n    if isinstance(new, np.record) or isinstance(new, np.void) or \\\n                                                        isinstance(new, tuple):\n        new = [new]\n    return np.append(X, utils.fromrecords(new, type=np.ndarray,\n                                              dtype=X.dtype), axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd one or more columns to a numpy array.", "response": "def addcols(X, cols, names=None):\n    \"\"\"\n    Add one or more columns to a numpy ndarray.\n\n    Technical dependency of :func:`tabular.spreadsheet.aggregate_in`.\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.addcols`.\n\n    **Parameters**\n\n            **X** :  numpy ndarray with structured dtype or recarray\n\n                    The recarray to add columns to.\n\n            **cols** :  numpy ndarray, or list of arrays of columns\n            \n                    Column(s) to add.\n\n            **names**:  list of strings, optional\n\n                    Names of the new columns. Only applicable when `cols` is a \n                    list of arrays.\n\n    **Returns**\n\n            **out** :  numpy ndarray with structured dtype\n\n                    New numpy array made up of `X` plus the new columns.\n\n    **See also:**  :func:`tabular.spreadsheet.colstack`\n\n    \"\"\"\n\n    if isinstance(names,str):\n        names = [n.strip() for n in names.split(',')]\n\n    if isinstance(cols, list):\n        if any([isinstance(x,np.ndarray) or isinstance(x,list) or \\\n                                     isinstance(x,tuple) for x in cols]):\n            assert all([len(x) == len(X) for x in cols]), \\\n                   'Trying to add columns of wrong length.'\n            assert names != None and len(cols) == len(names), \\\n                   'Number of columns to add must equal number of new names.'\n            cols = utils.fromarrays(cols,type=np.ndarray,names = names)\n        else:\n            assert len(cols) == len(X), 'Trying to add column of wrong length.'\n            cols = utils.fromarrays([cols], type=np.ndarray,names=names)\n    else:\n        assert isinstance(cols, np.ndarray)\n        if cols.dtype.names == None:\n            cols = utils.fromarrays([cols],type=np.ndarray, names=names)\n\n    Replacements = [a for a in cols.dtype.names if a in X.dtype.names]\n    if len(Replacements) > 0:\n        print('Replacing columns', \n              [a for a in cols.dtype.names if a in X.dtype.names])\n\n    return utils.fromarrays(\n      [X[a] if a not in cols.dtype.names else cols[a] for a in X.dtype.names] + \n      [cols[a] for a in cols.dtype.names if a not in X.dtype.names], \n      type=np.ndarray,\n      names=list(X.dtype.names) + [a for a in cols.dtype.names \n                                   if a not in X.dtype.names])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting columns from a numpy recarray or recarray with structured dtype cols.", "response": "def deletecols(X, cols):\n    \"\"\"\n    Delete columns from a numpy ndarry or recarray.\n\n    Can take a string giving a column name or comma-separated list of column \n    names, or a list of string column names.\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.deletecols`.\n\n    **Parameters**\n\n            **X** :  numpy recarray or ndarray with structured dtype\n\n                    The numpy array from which to delete columns.\n\n            **cols** :  string or list of strings\n\n                    Name or list of names of columns in `X`.  This can be\n                    a string giving a column name or comma-separated list of \n                    column names, or a list of string column names.\n\n    **Returns**\n\n            **out** :  numpy ndarray with structured dtype\n\n                    New numpy ndarray with structured dtype\n                    given by `X`, excluding the columns named in `cols`.\n\n    \"\"\"\n    if isinstance(cols, str):\n        cols = cols.split(',')\n    retain = [n for n in X.dtype.names if n not in cols]\n    if len(retain) > 0:\n        return X[retain]\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef renamecol(X, old, new):\n    NewNames = tuple([n if n != old else new for n in X.dtype.names])\n    X.dtype.names = NewNames", "response": "This function renames a column of a numpy ndarray with structured dtype in - place."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreplacing values of old with new in - place.", "response": "def replace(X, old, new, strict=True, cols=None, rows=None):\n    \"\"\"\n    Replace value `old` with `new` everywhere it appears in-place.\n\n    Implemented by the tabarray method \n    :func:`tabular.tab.tabarray.replace`.\n\n    **Parameters**\n\n            **X** :  numpy ndarray with structured dtype\n\n                    Numpy array for which in-place replacement of `old` with \n                    `new` is to be done.\n\n            **old** : string\n\n            **new** : string\n\n            **strict** :  boolean, optional\n\n            *   If `strict` = `True`, replace only exact occurences of `old`.\n\n            *   If `strict` = `False`, assume `old` and `new` are strings and   \n                replace all occurences of substrings (e.g. like \n                :func:`str.replace`)\n\n            **cols** :  list of strings, optional\n\n                    Names of columns to make replacements in; if `None`, make \n                    replacements everywhere.\n\n            **rows** : list of booleans or integers, optional\n\n                    Rows to make replacements in; if `None`, make replacements \n                    everywhere.\n\n    Note:  This function does in-place replacements.  Thus there are issues \n    handling data types here when replacement dtype is larger than original \n    dtype.  This can be resolved later by making a new array when necessary ...\n\n    \"\"\"\n\n    if cols == None:\n        cols = X.dtype.names\n    elif isinstance(cols, str):\n        cols = cols.split(',')\n\n    if rows == None:\n        rows = np.ones((len(X),), bool)\n\n    if strict:\n        new = np.array(new)\n        for a in cols:\n            if X.dtype[a] < new.dtype:\n                print('WARNING: dtype of column', a, \n                      'is inferior to dtype of ', new, \n                      'which may cause problems.')\n            try:\n                X[a][(X[a] == old)[rows]] = new\n            except:\n                print('Replacement not made on column', a, '.')\n    else:\n        for a in cols:\n            QuickRep = True\n            try:\n                colstr = ''.join(X[a][rows])\n            except TypeError:\n                print('Not replacing in column', a, 'due to type mismatch.')\n            else:\n                avoid = [ord(o) for o in utils.uniqify(old + new + colstr)]\n                ok = set(range(256)).difference(avoid)\n                if len(ok) > 0:\n                    sep = chr(list(ok)[0])\n                else:\n                    ok = set(range(65536)).difference(avoid)\n                    if len(ok) > 0:\n                        sep = unichr(list(ok)[0])\n                    else:\n                        print('All unicode characters represented in column', \n                              a, ', can\\t replace quickly.')\n                        QuickRep = False\n\n                if QuickRep:\n                    newrows = np.array(sep.join(X[a][rows])\n                                       .replace(old, new).split(sep))\n                else:\n                    newrows = np.array([aa.replace(old,new) for aa in \n                                        X[a][rows]])\n                X[a][rows] = np.cast[X.dtype[a]](newrows)\n\n                if newrows.dtype > X.dtype[a]:\n                    print('WARNING: dtype of column', a, 'is inferior to the ' \n                          'dtype of its replacement which may cause problems '\n                          '(ends of strings might get chopped off).')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef colstack(seq, mode='abort',returnnaming=False):\n    assert mode in ['first','drop','abort','rename'], \\\n       'mode argument must take on value \"first\",\"drop\", \"rename\", or \"abort\".'\n\n    AllNames = utils.uniqify(utils.listunion(\n                                           [list(l.dtype.names) for l in seq]))\n    NameList = [(x, [i for i in range(len(seq)) if x in seq[i].dtype.names]) \n                     for x in AllNames]\n    Commons = [x[0] for x in NameList if len(x[1]) > 1]\n\n    if len(Commons) > 0 or mode == 'first':\n        if mode == 'abort':\n            raise ValueError('There are common column names with differing ' +              \n                             'values in the columns')\n        elif mode == 'drop':\n            Names = [(L[0], x,x) for (x, L) in NameList if x not in Commons]\n        elif mode == 'rename':\n            NameDict = dict(NameList)\n            Names = utils.listunion([[(i,n,n) if len(NameDict[n]) == 1 else \\\n               (i,n,n + '_' + str(i)) for n in s.dtype.names] \\\n                                   for (i,s) in enumerate(seq)])                           \n    else:\n        Names = [(L[0], x,x) for (x, L) in NameList]\n    \n    if returnnaming:\n        return  utils.fromarrays([seq[i][x] for (i, x,y) in Names], \n                 type= np.ndarray,names=zip(*Names)[2]),Names\n    else:\n        return utils.fromarrays([seq[i][x] for (i, x,y) in Names], \n                 type= np.ndarray,names=zip(*Names)[2])", "response": "Returns a numpy array with the columns in the given sequence."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncombine two or more numpy ndarray with structured dtype on common key column(s). Merge a list (or dictionary) of numpy ndarray with structured dtype, given by `L`, on key columns listed in `keycols`. This function is actually a wrapper for :func:`tabular.spreadsheet.strictjoin`. The ``strictjoin`` function has a few restrictions, and this ``join`` function will try to ensure that they are satisfied: * each element of `keycol` must be a valid column name in `X` and each array in `L`, and all of the same data-type. * for each column `col` in `keycols`, and each array `A` in `L`, the values in `A[col]` must be unique, -- and same for `X[col]`. (Actually this uniqueness doesn't have to hold for the first tabarray in L, that is, L[0], but must for all the subsequent ones.) * the *non*-key-column column names in each of the arrays must be disjoint from each other -- or disjoint after a renaming (see below). An error will be thrown if these conditions are not met. If you don't provide a value of `keycols`, the algorithm will attempt to infer which columns should be used by trying to find the largest set of common column names that contain unique values in each array and have the same data type. An error will be thrown if no such inference can be made. *Renaming of overlapping columns* If the non-keycol column names of the arrays overlap, ``join`` will by default attempt to rename the columns by using a simple convention: * If `L` is a list, it will append the number in the list to the key associated with the array. * If `L` is a dictionary, the algorithm will append the string representation of the key associated with an array to the overlapping columns from that array. You can override the default renaming scheme using the `renamer` parameter. *Nullvalues for keycolumn differences* If there are regions of the keycolumns that are not overlapping between merged arrays, `join` will fill in the relevant entries with null values chosen by default: * '0' for integer columns * '0.0' for float columns * the empty character ('') for string columns. **Parameters** **L** : list or dictionary Numpy recarrays to merge. If `L` is a dictionary, the keys name each numpy recarray, and the corresponding values are the actual numpy recarrays. **keycols** : list of strings List of the names of the key columns along which to do the merging. **nullvals** : function, optional A function that returns a null value for a numpy format descriptor string, e.g. ``'<i4'`` or ``'|S5'``. See the default function for further documentation: :func:`tabular.spreadsheet.DEFAULT_NULLVALUEFORMAT` **renamer** : function, optional A function for renaming overlapping non-key column names among the numpy recarrays to merge. See the default function for further documentation: :func:`tabular.spreadsheet.DEFAULT_RENAMER` **returnrenaming** : Boolean, optional Whether to return the result of the `renamer` function. See the default function for further documentation: :func:`tabular.spreadsheet.DEFAULT_RENAMER` **Names**: list of strings: If `L` is a list, than names for elements of `L` can be specified with `Names` (without losing the ordering as you would if you did it with a dictionary). `len(L)` must equal `len(Names)` **Returns** **result** : numpy ndarray with structured dtype Result of the join, e.g. the result of merging the input numpy arrays defined in `L` on the key columns listed in `keycols`. **renaming** : dictionary of dictionaries, optional The result returned by the `renamer` function. Returned only if `returnrenaming == True`. See the default function for further documentation: :func:`tabular.spreadsheet.DEFAULT_RENAMER` **See Also:** :func:`tabular.spreadsheet.strictjoin`", "response": "def join(L, keycols=None, nullvals=None, renamer=None, \n         returnrenaming=False, Names=None):\n    \"\"\"\n    Combine two or more numpy ndarray with structured dtype on common key \n    column(s).\n\n    Merge a list (or dictionary) of numpy ndarray with structured dtype, given\n    by `L`, on key columns listed in `keycols`.\n\n    This function is actually a wrapper for \n    :func:`tabular.spreadsheet.strictjoin`.\n\n    The ``strictjoin`` function has a few restrictions, and this ``join``\n    function will try to ensure that they are satisfied:\n\n    *   each element of `keycol` must be a valid column name in `X`\n        and each array in `L`, and all of the same data-type.\n\n    *   for each column `col`  in `keycols`, and each array `A` in `L`, the \n        values in `A[col]` must be unique, -- and same for `X[col]`.  \n        (Actually this uniqueness doesn't have to hold for the first tabarray\n        in L, that is, L[0], but must for all the subsequent ones.)\n\n    *   the *non*-key-column column names in each of the arrays must be \n        disjoint from each other -- or disjoint after a renaming (see below).\n\n    An error will be thrown if these conditions are not met.\n\n    If you don't provide a value of `keycols`, the algorithm will attempt to \n    infer which columns should be used by trying to find the largest set of \n    common column names that contain unique values in each array and have the \n    same data type.  An error will be thrown if no such inference can be made.\n\n    *Renaming of overlapping columns*\n\n            If the non-keycol column names of the arrays overlap, ``join`` will \n            by default attempt to rename the columns by using a simple \n            convention:\n\n            *   If `L` is a list, it will append the number in the list to the \n                key associated with the array.\n\n            *   If `L` is a dictionary, the algorithm will append the string \n                representation of the key associated with an array to the \n                overlapping columns from that array.\n\n            You can override the default renaming scheme using the `renamer` \n            parameter.\n\n    *Nullvalues for keycolumn differences*\n\n            If there are regions of the keycolumns that are not overlapping \n            between merged arrays, `join` will fill in the relevant entries \n            with null values chosen by default:\n\n            *   '0' for integer columns\n\n            *   '0.0' for float columns\n\n            *   the empty character ('') for string columns.\n\n    **Parameters**\n\n            **L** :  list or dictionary\n\n                    Numpy recarrays to merge.  If `L` is a dictionary, the keys\n                    name each numpy recarray, and the corresponding values are \n                    the actual numpy recarrays.\n\n            **keycols** :  list of strings\n\n                    List of the names of the key columns along which to do the \n                    merging.\n\n            **nullvals** :  function, optional\n\n                    A function that returns a null value for a numpy format\n                    descriptor string, e.g. ``'<i4'`` or ``'|S5'``.\n\n                    See the default function for further documentation:\n\n                            :func:`tabular.spreadsheet.DEFAULT_NULLVALUEFORMAT`\n\n            **renamer** :  function, optional\n\n                    A function for renaming overlapping non-key column names \n                    among the numpy recarrays to merge.\n\n                    See the default function for further documentation:\n\n                            :func:`tabular.spreadsheet.DEFAULT_RENAMER`\n\n            **returnrenaming** :  Boolean, optional\n\n                    Whether to return the result of the `renamer` function.\n\n                    See the default function for further documentation:\n\n                            :func:`tabular.spreadsheet.DEFAULT_RENAMER`\n\n            **Names**: list of strings:\n\n                    If `L` is a list, than names for elements of `L` can be \n                    specified with `Names` (without losing the ordering as you \n                    would if you did it with a dictionary).  \n                    \n                    `len(L)` must equal `len(Names)`\n\n    **Returns**\n\n            **result** :  numpy ndarray with structured dtype\n\n                    Result of the join, e.g. the result of merging the input\n                    numpy arrays defined in `L` on the key columns listed in \n                    `keycols`.\n\n            **renaming** :  dictionary of dictionaries, optional\n\n                    The result returned by the `renamer` function. Returned \n                    only if `returnrenaming == True`.\n\n                    See the default function for further documentation:\n\n                            :func:`tabular.spreadsheet.DEFAULT_RENAMER`\n\n    **See Also:**\n\n            :func:`tabular.spreadsheet.strictjoin`\n\n    \"\"\"\n\n    if isinstance(L, dict):\n        Names = L.keys()\n        LL = L.values()\n    else:\n        if Names == None:\n            Names = range(len(L))\n        else:\n            assert len(Names) == len(L)\n        LL = L\n\n    if not keycols:\n        keycols = utils.listintersection([a.dtype.names for a in LL])\n        if len(keycols) == 0:\n            raise ValueError('No common column names found.')\n\n        keycols = [l for l in keycols if all([a.dtype[l] == LL[0].dtype[l] \n                   for a in LL])]\n        if len(keycols) == 0:\n            raise ValueError('No suitable common keycolumns, '\n                             'with identical datatypes found.')\n\n        keycols = [l for l in keycols if all([isunique(a[keycols]) \n                   for a in LL])]\n        if len(keycols) == 0:\n            raise ValueError('No suitable common keycolumns, '\n                             'with unique value sets in all arrays to be '\n                             'merged, were found.')\n        else:\n            print('Inferring keycols to be:', keycols)\n\n    elif isinstance(keycols,str):\n        keycols = [l.strip() for l in keycols.split(',')]\n\n    commons = set(Commons([l.dtype.names for l in LL])).difference(keycols)\n    renaming = {}\n    if len(commons) > 0:\n        print 'common attributes, forcing a renaming ...'\n        if renamer == None:\n            print('Using default renamer ...')\n            renamer = DEFAULT_RENAMER\n        renaming = renamer(L, Names=Names)\n        if not RenamingIsInCorrectFormat(renaming, L, Names=Names):\n            print('Renaming from specified renamer is not in correct format,'\n                  'using default renamer instead ...')\n            renaming = DEFAULT_RENAMER(L, Names = Names)\n        NewNames = [[l if l not in renaming[k].keys() else renaming[k][l] \n                     for l in ll.dtype.names] for (k, ll) in zip(Names, LL)]\n        if set(Commons(NewNames)).difference(keycols):\n            raise ValueError('Renaming convention failed to produce '\n                             'separated names.')\n\n    Result = strictjoin(L, keycols, nullvals, renaming, Names=Names)\n\n    if returnrenaming:\n        return [Result, renaming]\n    else:\n        if renaming:\n            print('There was a nontrivial renaming, to get it set '      \n                  '\"returnrenaming = True\" in keyword to join function.')\n        return Result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncombines two or more numpy ndarray with structured dtypes on common key column(s). Merge a list (or dictionary) of numpy arrays, given by `L`, on key columns listed in `keycols`. The ``strictjoin`` assumes the following restrictions: * each element of `keycol` must be a valid column name in `X` and each array in `L`, and all of the same data-type. * for each column `col` in `keycols`, and each array `A` in `L`, the values in `A[col]` must be unique, e.g. no repeats of values -- and same for `X[col]`. (Actually, the uniqueness criterion need not hold to the first tabarray in L, but first for all the subsequent ones.) * the *non*-key-column column names in each of the arrays must be disjoint from each other -- or disjoint after a renaming (see below). An error will be thrown if these conditions are not met. For a wrapper that attempts to meet these restrictions, see :func:`tabular.spreadsheet.join`. If you don't provide a value of `keycols`, the algorithm will attempt to infer which columns should be used by trying to find the largest set of common column names that contain unique values in each array and have the same data type. An error will be thrown if no such inference can be made. *Renaming of overlapping columns* If the non-keycol column names of the arrays overlap, ``join`` will by default attempt to rename the columns by using a simple convention: * If `L` is a list, it will append the number in the list to the key associated with the array. * If `L` is a dictionary, the algorithm will append the string representation of the key associated with an array to the overlapping columns from that array. You can override the default renaming scheme using the `renamer` parameter. *Nullvalues for keycolumn differences* If there are regions of the keycolumns that are not overlapping between merged arrays, `join` will fill in the relevant entries with null values chosen by default: * '0' for integer columns * '0.0' for float columns * the empty character ('') for string columns. **Parameters** **L** : list or dictionary Numpy recarrays to merge. If `L` is a dictionary, the keys name each numpy recarray, and the corresponding values are the actual numpy recarrays. **keycols** : list of strings List of the names of the key columns along which to do the merging. **nullvals** : function, optional A function that returns a null value for a numpy format descriptor string, e.g. ``'<i4'`` or ``'|S5'``. See the default function for further documentation: :func:`tabular.spreadsheet.DEFAULT_NULLVALUEFORMAT` **renaming** : dictionary of dictionaries, optional Dictionary mapping each input numpy recarray to a dictionary mapping each original column name to its new name following the convention above. For example, the result returned by: :func:`tabular.spreadsheet.DEFAULT_RENAMER` **Returns** **result** : numpy ndarray with structured dtype Result of the join, e.g. the result of merging the input numpy arrays defined in `L` on the key columns listed in `keycols`. **See Also:** :func:`tabular.spreadsheet.join`", "response": "def strictjoin(L, keycols, nullvals=None, renaming=None, Names=None):\n    \"\"\"\n    Combine two or more numpy ndarray with structured dtypes on common key\n    column(s).\n\n    Merge a list (or dictionary) of numpy arrays, given by `L`, on key \n    columns listed in `keycols`.\n\n    The ``strictjoin`` assumes the following restrictions:\n\n    *   each element of `keycol` must be a valid column name in `X` and each \n        array in `L`, and all of the same data-type.\n\n    *   for each column `col`  in `keycols`, and each array `A` in `L`, the \n        values in `A[col]` must be unique, e.g. no repeats of values -- and \n        same for `X[col]`.  (Actually, the uniqueness criterion need not hold\n        to the first tabarray in L, but first for all the subsequent ones.)\n\n    *   the *non*-key-column column names in each of the arrays must be \n        disjoint from each other -- or disjoint after a renaming (see below).\n\n    An error will be thrown if these conditions are not met.\n\n    For a wrapper that attempts to meet these restrictions, see \n    :func:`tabular.spreadsheet.join`.\n\n    If you don't provide a value of `keycols`, the algorithm will attempt to \n    infer which columns should be used by trying to find the largest set of \n    common column names that contain unique values in each array and have the \n    same data type.  An error will be thrown if no such inference can be made.\n\n    *Renaming of overlapping columns*\n\n            If the non-keycol column names of the arrays overlap, ``join`` will \n            by default attempt to rename the columns by using a simple \n            convention:\n\n            *   If `L` is a list, it will append the number in the list to the \n                key associated with the array.\n\n            *   If `L` is a dictionary, the algorithm will append the string \n                representation of the key associated with an array to the \n                overlapping columns from that array.\n\n            You can override the default renaming scheme using the `renamer` \n            parameter.\n\n    *Nullvalues for keycolumn differences*\n\n            If there are regions of the keycolumns that are not overlapping \n            between merged arrays, `join` will fill in the relevant entries \n            with null values chosen by default:\n\n            *   '0' for integer columns\n\n            *   '0.0' for float columns\n\n            *   the empty character ('') for string columns.\n\n    **Parameters**\n\n            **L** :  list or dictionary\n\n                    Numpy recarrays to merge.  If `L` is a dictionary, the keys\n                    name each numpy recarray, and the corresponding values are \n                    the actual numpy recarrays.\n\n            **keycols** :  list of strings\n\n                    List of the names of the key columns along which to do the \n                    merging.\n\n            **nullvals** :  function, optional\n\n                    A function that returns a null value for a numpy format\n                    descriptor string, e.g. ``'<i4'`` or ``'|S5'``.\n\n                    See the default function for further documentation:\n\n                            :func:`tabular.spreadsheet.DEFAULT_NULLVALUEFORMAT`\n\n            **renaming** :  dictionary of dictionaries, optional\n\n                    Dictionary mapping each input numpy recarray to a \n                    dictionary mapping each original column name to its new \n                    name following the convention above.\n\n                    For example, the result returned by:\n\n                            :func:`tabular.spreadsheet.DEFAULT_RENAMER`\n\n    **Returns**\n\n            **result** :  numpy ndarray with structured dtype\n\n                    Result of the join, e.g. the result of merging the input\n                    numpy arrays defined in `L` on the key columns listed in \n                    `keycols`.\n\n    **See Also:**\n\n            :func:`tabular.spreadsheet.join`\n    \"\"\"\n\n    if isinstance(L,dict):\n        Names = L.keys()\n        LL = L.values()\n    else:\n        if Names == None:\n            Names = range(len(L))\n        else:\n            assert len(Names) == len(L)\n        LL = L\n\n    if isinstance(keycols,str):\n        keycols = [l.strip() for l in keycols.split(',')]\n\n    assert all([set(keycols) <= set(l.dtype.names) for l in LL]), \\\n           ('keycols,', str(keycols), \n            ', must be valid column names in all arrays being merged.')\n    assert all([isunique(l[keycols]) for l in LL[1:]]), \\\n           ('values in keycol columns,', str(keycols), \n            ', must be unique in all arrays being merged.')\n\n    if renaming == None:\n        renaming = {}\n    assert RenamingIsInCorrectFormat(renaming, L, Names=Names), \\\n           'renaming is not in proper format ... '\n\n    L = dict([(k,ll.copy()) for (k,ll) in zip(Names,LL)])\n    LL = L.values()\n\n    for i in Names:\n        l = L[i]\n        l.dtype = np.dtype(l.dtype.descr)\n        if i in renaming.keys():\n            for k in renaming[i].keys():\n                if k not in keycols:\n                    renamecol(L[i], k, renaming[i][k])\n        l.sort(order = keycols)\n\n\n    commons = set(Commons([l.dtype.names for l in LL])).difference(keycols)\n    assert len(commons) == 0, ('The following (non-keycol) column names ' \n                    'appear in more than on array being merged:', str(commons))\n\n    Result = colstack([(L[Names[0]][keycols])[0:0]] + \n                      [deletecols(L[k][0:0], keycols) \\\n                      for k in Names if deletecols(L[k][0:0], keycols) != None])\n\n    PL = powerlist(Names)\n    ToGet = utils.listunion([[p for p in PL if len(p) == k] \n                             for k in range(1, len(Names))]) + [PL[-1]]\n\n    for I in ToGet[::-1]:\n        Ref = L[I[0]][keycols]\n\n        for j in I[1:]:\n            if len(Ref) > 0:\n                Ref = Ref[fast.recarrayisin(Ref, L[j][keycols], weak=True)]\n            else:\n                break\n\n        if len(Ref) > 0:\n            D = [fast.recarrayisin(L[j][keycols], Ref, weak=True) for j in I]\n            \n            Ref0 = L[I[0]][keycols][D[0]]\n            Reps0 = np.append(np.append([-1],\n                (Ref0[1:] != Ref0[:-1]).nonzero()[0]),[len(Ref0)-1])\n            Reps0 = Reps0[1:] - Reps0[:-1]\n                        \n            NewRows = colstack([Ref0] + \n                  [deletecols(L[j][D[i]], keycols).repeat(Reps0) if i > 0 else \n                   deletecols(L[j][D[i]], keycols) for (i, j) in enumerate(I)  \n                   if deletecols(L[j][D[i]], keycols) != None])\n            for (i,j) in enumerate(I):\n                L[j] = L[j][np.invert(D[i])]\n            Result = rowstack([Result, NewRows], mode='nulls', \n                              nullvals=nullvals)\n\n    return Result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getjp2header(Id):\n    '''\n    GET /api/v1/getJP2Header/\n\n\n    Get the XML header embedded in a JPEG2000 image. Includes the FITS header as well as a section of Helioviewer-specific metadata.\n\n    Request Parameters:\n\n    Parameter\tRequired\tType\tExample\tDescription\n    id\tRequired\tnumber\t7654321\tUnique JP2 image identifier.\n    callback\tOptional\tstring\t\tWrap the response object in a function call of your choosing.\n\n    Example (A):\n\n    string (XML)\n\n    Example Request:\n\n    http://helioviewer.org/api/v1/getJP2Header/?id=7654321\n    '''\n    base_url = 'http://helioviewer.org/api/v1/getJP2Header/?'\n\n    if not isinstance(Id, int):\n        raise ValueError(\"The Id argument should be an int, ignoring it\")\n    else:\n        base_url += \"id=\" + str(Id)\n\n    return dispatch_http_get(base_url)", "response": "This function returns the XML header embedded in a JPEG2000 image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a json objecthook based on the given module and the given key - value pairs", "response": "def loads_loader(load_module: types.ModuleType, pairs: Dict[str, str]) -> Optional[JSGValidateable]:\n    \"\"\"json loader objecthook\n\n    :param load_module: Module that contains the various types\n    :param pairs: key/value tuples (In our case, they are str/str)\n    :return:\n    \"\"\"\n    cntxt = load_module._CONTEXT\n\n    # If the type element is a member of the JSON, load it\n    possible_type = pairs[cntxt.TYPE] if cntxt.TYPE in pairs else None\n    target_class = getattr(load_module, possible_type, None) if isinstance(possible_type, str) else None\n    if target_class:\n        return target_class(**pairs)\n\n    # See whether there are any exception types that are valid for the incoming data\n    for type_exception in cntxt.TYPE_EXCEPTIONS:\n        if not hasattr(load_module, type_exception):\n            raise ValueError(UNKNOWN_TYPE_EXCEPTION.format(type_exception))\n        target_class = getattr(load_module, type_exception)\n        target_strict = target_class._strict\n        target_class._strict = False\n        try:\n            rval = target_class(**pairs)\n        finally:\n            target_class._strict = target_strict\n        if is_valid(rval):\n            return rval\n\n    # If there is not a type variable and nothing fits, just load up the first (and perhaps only) exception\n    # It will later fail any is_valid tests\n    if not cntxt.TYPE and cntxt.TYPE_EXCEPTIONS:\n        return getattr(load_module, cntxt.TYPE_EXCEPTIONS[0])(**pairs)\n\n    if cntxt.TYPE in pairs:\n        raise ValueError(f'Unknown reference type: \"{cntxt.TYPE}\": \"{pairs[cntxt.TYPE]}\"')\n    else:\n        raise ValueError(f'Missing \"{cntxt.TYPE}\" element')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loads(s: str, load_module: types.ModuleType, **kwargs):\n    return json.loads(s, object_hook=lambda pairs: loads_loader(load_module, pairs), **kwargs)", "response": "Convert a JSON string into a JSGObject"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load(fp: Union[TextIO, str], load_module: types.ModuleType, **kwargs):\n    if isinstance(fp, str):\n        with open(fp) as f:\n            return loads(f.read(), load_module, **kwargs)\n    else:\n        return loads(fp.read(), load_module, **kwargs)", "response": "Convert a file - like object containing stringified JSON into a JSGObject"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isinstance_(x, A_tuple):\n    if is_union(A_tuple):\n        return any(isinstance_(x, t) for t in A_tuple.__args__)\n    elif getattr(A_tuple, '__origin__', None) is not None:\n        return isinstance(x, A_tuple.__origin__)\n    else:\n        return isinstance(x, A_tuple)", "response": "native isinstance_ with the test for typing. Union overridden"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid(obj: JSGValidateable, log: Optional[Union[TextIO, Logger]] = None) -> bool:\n    return obj._is_valid(log)", "response": "Determines whether the object is valid for the current node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a set of pickle files for the current OTT.", "response": "def _create_pickle_files(self, out_dir=None):  # pylint: disable=R0914,R0915\n        \"\"\"\n       preorder2tuple maps a preorder number to a node definition.\n       ottID2preorder maps every OTT ID in taxonomy.tsv to a preorder #\n        'ottID2preorder'\n        'ottID2preorder'\n        'preorder2ottID'\n        'ottID2uniq'\n        'uniq2ottID'\n        'name2ottID'\n        'ottID2names'\n        'ottID2ranks'\n        'ottID2parentOttId'\n        'preorder2tuple'\n        \"\"\"\n        if out_dir is None:\n            out_dir = self.ott_dir\n        taxonomy_file = self.taxonomy_filepath\n        if not os.path.isfile(taxonomy_file):\n            raise ValueError('Expecting to find \"{}\" based on ott_dir of \"{}\"'.format(taxonomy_file, self.ott_dir))\n        num_lines = 0\n        _LOG.debug('Reading \"{f}\"...'.format(f=taxonomy_file))\n        id2par = {}  # UID to parent UID\n        id2name = {}  # UID to 'name' field\n        id2rank = {}  # UID to 'rank' field\n        id2uniq = {}  # UID to 'uniqname' field\n        uniq2id = {}  # uniqname to UID\n        id2flag = {}  # UID to a key in flag_set_id2flag_set\n        id2source = {}  # UID to {rank: ... silva: ..., ncbi: ... gbif:..., irmng : , f}\n        # where the value for f is\n        flag_set_id2flag_set = {}\n        flag_set2flag_set_id = {}\n        sources = set()\n        flag_set = set()\n        f_set_id = 0\n        source = {}\n        root_ott_id = None\n        with codecs.open(taxonomy_file, 'r', encoding='utf-8') as tax_fo:\n            it = iter(tax_fo)\n            first_line = next(it)\n            assert first_line == 'uid\\t|\\tparent_uid\\t|\\tname\\t|\\trank\\t|\\tsourceinfo\\t|\\tuniqname\\t|\\tflags\\t|\\t\\n'\n            # now parse the rest of the taxonomy file\n            for rown in it:\n                ls = rown.split('\\t|\\t')\n                uid, par, name = ls[:3]\n                rank,sourceinfo, uniqname, flags = ls[3:7]\n                skip = False\n                for p in self.skip_prefixes:\n                    if uniqname.startswith(p):\n                        skip = True\n                        break\n                if skip:\n                    continue\n                uid = int(uid)\n                if par == '':\n                    # parse the root node (name = life; no parent)\n                    par = NONE_PAR\n                    root_ott_id = uid\n                    assert name == 'life'\n                    self._root_name = name\n                else:\n                    # this is not the root node\n                    par = int(par)\n                    if par not in id2par:\n                        raise ValueError('parent {} not found in OTT parsing'.format(par))\n                assert ls[7] == '\\n'\n                assert uid not in id2par\n                id2par[uid] = par\n                id2name[uid] = name\n                if rank:\n                    id2rank[uid] = rank\n                if uniqname:\n                    id2uniq[uid] = uniqname\n                    if uniqname in uniq2id:\n                        _LOG.error('uniqname \"{u}\" used for OTT ID \"{f:d}\" and \"{n:d}\"'.format(\n                            u=uniqname,\n                            f=uniq2id[uniqname],\n                            n=uid))\n                    uniq2id[uniqname] = uid\n                if sourceinfo:\n                    s_list = sourceinfo.split(',')\n                    for x in s_list:\n                        src, sid = x.split(':')\n                        try:\n                            sid = int(sid)\n                        except:\n                            pass\n                        sources.add(src)\n                        source[src] = sid\n                if flags:\n                    f_list = flags.split(',')\n                    if len(f_list) > 1:\n                        f_list.sort()\n                    f_set = frozenset(f_list)\n                    for x in f_list:\n                        flag_set.add(x)\n                    fsi = flag_set2flag_set_id.get(f_set)\n                    if fsi is None:\n                        fsi = f_set_id\n                        f_set_id += 1\n                        flag_set_id2flag_set[fsi] = f_set\n                        flag_set2flag_set_id[f_set] = fsi\n                    id2flag[uid] = fsi\n                if source:\n                    id2source[uid] = source\n                    source = {}\n\n                num_lines += 1\n                if num_lines % 100000 == 0:\n                    _LOG.debug('read {n:d} lines...'.format(n=num_lines))\n        _LOG.debug('read taxonomy file. total of {n:d} lines.'.format(n=num_lines))\n        _write_pickle(out_dir, 'ottID2parentOttId', id2par)\n        synonyms_file = self.synonyms_filepath\n        _LOG.debug('Reading \"{f}\"...'.format(f=synonyms_file))\n        if not os.path.isfile(synonyms_file):\n            raise ValueError('Expecting to find \"{}\" based on ott_dir of \"{}\"'.format(synonyms_file, self.ott_dir))\n        num_lines = 0\n        with codecs.open(synonyms_file, 'r', encoding='utf-8') as syn_fo:\n            it = iter(syn_fo)\n            first_line = next(it)\n            # modified to allow for final 'source column'\n            assert first_line.startswith('name\\t|\\tuid\\t|\\ttype\\t|\\tuniqname')\n            for rown in it:\n                ls = rown.split('\\t|\\t')\n                name, ott_id = ls[0], ls[1]\n                ott_id = int(ott_id)\n                if ott_id in id2name:\n                    n = id2name[ott_id]\n                    if isinstance(n, list):\n                        n.append(name)\n                    else:\n                        id2name[ott_id] = [n, name]\n                else:\n                    _f = u'synonym \"{n}\" maps to an ott_id ({u}) that was not in the taxonomy!'\n                    _m = _f.format(n=name, u=ott_id)\n                    _LOG.debug(_m)\n                num_lines += 1\n                if num_lines % 100000 == 0:\n                    _LOG.debug('read {n:d} lines...'.format(n=num_lines))\n        _LOG.debug('read synonyms file. total of {n:d} lines.'.format(n=num_lines))\n        _LOG.debug('normalizing id2name dict. {s:d} entries'.format(s=len(id2name)))\n        _swap = {}\n        for k, v in id2name.items():\n            if isinstance(v, list):\n                v = tuple(v)\n            _swap[k] = v\n        id2name = _swap\n        _LOG.debug('inverting id2name dict. {s:d} entries'.format(s=len(id2name)))\n        name2id = {}\n        for ott_id, v in id2name.items():\n            if not isinstance(v, tuple):\n                v = [v]\n            for el in v:\n                prev = name2id.get(el)\n                if prev is None:\n                    name2id[el] = ott_id\n                elif isinstance(prev, list):\n                    prev.append(ott_id)\n                else:\n                    name2id[el] = [prev, ott_id]\n        _LOG.debug('normalizing name2id dict. {s:d} entries'.format(s=len(name2id)))\n        _swap = {}\n        for k, v in name2id.items():\n            if isinstance(v, list):\n                v = tuple(v)\n            _swap[k] = v\n        name2id = _swap\n        homonym2id = {}\n        nonhomonym2id = {}\n        for name, ott_ids in name2id.items():\n            if isinstance(ott_ids, tuple) and len(ott_ids) > 1:\n                homonym2id[name] = ott_ids\n            else:\n                nonhomonym2id[name] = ott_ids\n        _LOG.debug('Making heavy tree')\n        tt = make_tree_from_taxonomy(id2par)\n        _LOG.debug('preorder numbering nodes')\n        root = tt[root_ott_id]\n        root.number_tree(0)\n        _LOG.debug('creating ott_id <--> preorder maps')\n        ott_id2preorder = {}\n        preorder2ott_id = {}\n        for ott_id, node in tt.items():\n            ott_id2preorder[ott_id] = node.preorder_number\n            preorder2ott_id[node.preorder_number] = ott_id\n        ott_id2preorder['root_ott_id'] = root_ott_id\n        ott_id2preorder['root'] = root.preorder_number\n        preorder2ott_id['root'] = root_ott_id\n        preorder2ott_id['root_preorder'] = root.preorder_number\n        self._root_ott_id = root_ott_id\n        self._write_root_properties(out_dir, self._root_name, self._root_ott_id)\n        _write_pickle(out_dir, 'ottID2preorder', ott_id2preorder)\n        _write_pickle(out_dir, 'preorder2ottID', preorder2ott_id)\n        _write_pickle(out_dir, 'ottID2uniq', id2uniq)\n        _write_pickle(out_dir, 'uniq2ottID', uniq2id)\n        _write_pickle(out_dir, 'name2ottID', name2id)\n        _write_pickle(out_dir, 'homonym2ottID', homonym2id)\n        _write_pickle(out_dir, 'nonhomonym2ottID', nonhomonym2id)\n        _write_pickle(out_dir, 'ottID2names', id2name)\n        _write_pickle(out_dir, 'ottID2sources', id2source)\n        _write_pickle(out_dir, 'ottID2ranks', id2rank)\n        _write_pickle(out_dir, 'ottID2flags', id2flag)\n        _write_pickle(out_dir, 'flagSetID2FlagSet', flag_set_id2flag_set)\n        _write_pickle(out_dir, 'taxonomicSources', sources)\n        forward_table = self._parse_forwarding_files()\n        _write_pickle(out_dir, 'forwardingTable', forward_table)\n        _LOG.debug('creating tree representation with preorder # to tuples')\n        preorder2tuples = {}\n        root.par = _TransitionalNode()  # fake parent of root\n        root.par.preorder_number = None\n        root.fill_preorder2tuples(None, preorder2tuples)\n        preorder2tuples['root'] = root.preorder_number\n        _write_pickle(out_dir, 'preorder2tuple', preorder2tuples)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of recognized ott_ids and their corresponding Entry - IDs.", "response": "def map_ott_ids(self, ott_id_list, to_prune_fsi_set, root_ott_id):\n        \"\"\"returns:\n          - a list of recognized ott_ids.\n          - a list of unrecognized ott_ids\n          - a list of ott_ids that forward to unrecognized ott_ids\n          - a list of ott_ids that do not appear in the tree because they are flagged to be pruned.\n          - a list of ott_ids that do not appear in the tree because they are above the root of the relevant subtree.\n          - a dict mapping input Id to forwarded Id\n        The relative order will be the input order, but the unrecognized elements will\n            be deleted.\n        \"\"\"\n        mapped, unrecog, forward2unrecog, pruned, above_root, old2new = [], [], [], [], [], {}\n        known_unpruned, known_pruned = set(), set()\n        known_above_root, known_below_root = set(), set()\n        oi2poi = self.ott_id2par_ott_id\n        ft = self.forward_table\n        for old_id in ott_id_list:\n            if old_id in oi2poi:\n                if self.check_if_above_root(old_id, known_above_root, known_below_root, root_ott_id):\n                    above_root.append(old_id)\n                elif (to_prune_fsi_set is not None) and \\\n                        self.check_if_in_pruned_subtree(old_id, known_unpruned, known_pruned, to_prune_fsi_set):\n                    pruned.append(old_id)\n                else:\n                    mapped.append(old_id)\n            else:\n                new_id = ft.get(old_id)\n                if new_id is None:\n                    unrecog.append(old_id)\n                else:\n                    if new_id in oi2poi:\n                        if (to_prune_fsi_set is not None) and \\\n                                self.check_if_in_pruned_subtree(new_id, known_unpruned, known_pruned, to_prune_fsi_set):\n                            pruned.append(old_id)  # could be in a forward2pruned\n                        else:\n                            old2new[old_id] = new_id\n                            mapped.append(new_id)\n                    else:\n                        forward2unrecog.append(old_id)\n        return mapped, unrecog, forward2unrecog, pruned, above_root, old2new"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a set of argument tuples set their value in a dictionary if not blank", "response": "def arg_tup_to_dict(argument_tuples):\n    \"\"\"Given a set of argument tuples, set their value in a data dictionary if not blank\"\"\"\n    data = dict()\n    for arg_name, arg_val in argument_tuples:\n        if arg_val is not None:\n            if arg_val is True:\n                arg_val = 'true'\n            elif arg_val is False:\n                arg_val = 'false'\n            data[arg_name] = arg_val\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nraises appropriate exceptions if necessary.", "response": "def handle_error(response):\n    \"\"\"Raise appropriate exceptions if necessary.\"\"\"\n    status_code = response.status_code\n\n    if status_code not in A_OK_HTTP_CODES:\n        error_explanation = A_ERROR_HTTP_CODES.get(status_code)\n        raise_error = \"{}: {}\".format(status_code, error_explanation)\n        raise Exception(raise_error)\n    else:\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd or replaces an annotation for an object.", "response": "def add_or_replace_annotation(self,  # pylint: disable=R0201\n                                  obj,\n                                  annotation,\n                                  agent,\n                                  add_agent_only=False):\n        \"\"\"Takes an `annotation` dictionary which is\n        expected to have a string as the value of annotation['author']['name']\n        This function will remove all annotations from obj that:\n            1. have the same author/name, and\n            2. have no messages that are flagged as messages to be preserved (values for 'preserve'\n                that evaluate to true)\n        \"\"\"\n        nex = get_nexml_el(obj)\n        nvers = detect_nexson_version(obj)\n        _LOG.debug('detected version as ' + nvers)\n        agents_obj = find_val_literal_meta_first(nex, 'ot:agents', nvers)\n        if not agents_obj:\n            agents_obj = add_literal_meta(nex, 'ot:agents', {'agent': []}, nvers)\n        agents_list = agents_obj.setdefault('agent', [])\n        found_agent = False\n        aid = agent['@id']\n        for a in agents_list:\n            if a.get('@id') == aid:\n                found_agent = True\n                break\n        if not found_agent:\n            agents_list.append(agent)\n        if add_agent_only:\n            delete_same_agent_annotation(obj, annotation)\n        else:\n            replace_same_agent_annotation(obj, annotation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_list_key(self, obj, key, vc, obj_nex_id=None):\n        k = obj.get(key)\n        if k is None:\n            return None\n        if isinstance(k, dict):\n            k = [k]\n        if not isinstance(k, list):\n            self._error_event(vc.curr_element_type,\n                              obj=obj,\n                              err_type=gen_MissingExpectedListWarning,\n                              anc=vc.anc_list,\n                              obj_nex_id=obj_nex_id,\n                              key_list=[key, ])\n            return None\n        return k", "response": "Returns a list key if the key is not a dict or None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_obj_by_schema(self, obj, obj_nex_id, vc):\n        return self._validate_id_obj_list_by_schema([(obj_nex_id, obj)], vc, group_by_warning=False)", "response": "Validate obj by schema."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd several attributes to the given container.", "response": "def add_schema_attributes(container, nexson_version):\n    \"\"\"Adds several attributes to `container`:\n    _using_hbf_meta  - boolean. True for HoneyBadgerFish v1-style meta elements ('^prop': value\n                rather than 'meta': {'$':value})\n    and the following _SchemaFragment instances:\n    _NexmlEl_Schema\n    \"\"\"\n    if _is_by_id_hbf(nexson_version):\n        _add_by_id_nexson_schema_attributes(container)\n    elif _is_badgerfish_version(nexson_version):\n        _add_badgerfish_nexson_schema_attributes(container)\n    elif _is_direct_hbf(nexson_version):\n        _add_direct_nexson_schema_attributes(container)\n    else:\n        raise NotImplementedError('unrecognized nexson variant {}'.format(nexson_version))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening the current object in the context manager.", "response": "async def open(self) -> '_BaseAgent':\n        \"\"\"\n        Context manager entry; open wallet.\n        For use when keeping agent open across multiple calls.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('_BaseAgent.open >>>')\n\n        # Do not open pool independently: let relying party decide when to go on-line and off-line\n        await self.wallet.open()\n\n        LOGGER.debug('_BaseAgent.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget json cryptonym for input ( agent ) DID.", "response": "async def get_nym(self, did: str) -> str:\n        \"\"\"\n        Get json cryptonym (including current verification key) for input (agent) DID from ledger.\n\n        Raise BadLedgerTxn on failure.\n\n        :param did: DID of cryptonym to fetch\n        :return: cryptonym json\n        \"\"\"\n\n        LOGGER.debug('_BaseAgent.get_nym >>> did: %s', did)\n\n        rv = json.dumps({})\n        get_nym_req = await ledger.build_get_nym_request(self.did, did)\n        resp_json = await self._submit(get_nym_req)\n\n        data_json = (json.loads(resp_json))['result']['data']  # it's double-encoded on the ledger\n        if data_json:\n            rv = data_json\n\n        LOGGER.debug('_BaseAgent.get_nym <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def _sign_submit(self, req_json: str) -> str:\n\n        LOGGER.debug('_BaseAgent._sign_submit >>> json: %s', req_json)\n\n        if not self.pool.handle:\n            LOGGER.debug('_BaseAgent._submit <!< closed pool %s', self.pool.name)\n            raise ClosedPool('Cannot submit request to closed pool {}'.format(self.pool.name))\n\n        try:\n            rv_json = await ledger.sign_and_submit_request(self.pool.handle, self.wallet.handle, self.did, req_json)\n            await asyncio.sleep(0)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletIncompatiblePoolError:\n                LOGGER.debug(\n                    '_BaseAgent._sign_submit: <!< Corrupt wallet %s is not compatible with pool %s',\n                    self.wallet.name,\n                    self.pool.name)\n                raise CorruptWallet(\n                    'Corrupt wallet {} is not compatible with pool {}'.format(self.wallet.name, self.pool.name))\n            else:\n                LOGGER.debug(\n                    '_BaseAgent._sign_submit: <!<  cannot sign/submit request for ledger: indy error code %s',\n                    self.wallet.name)\n                raise BadLedgerTxn('Cannot sign/submit request for ledger: indy error code {}'.format(\n                    x_indy.error_code))\n\n        resp = json.loads(rv_json)\n        if ('op' in resp) and (resp['op'] in ('REQNACK', 'REJECT')):\n            LOGGER.debug('_BaseAgent._sign_submit: ledger rejected request: %s', resp['reason'])\n            raise BadLedgerTxn('Ledger rejected transaction request: {}'.format(resp['reason']))\n\n        if 'reason' in resp and 'result' in resp and resp['result'].get('seqNo', None) is None:\n            LOGGER.debug('_BaseAgent._sign_submit: <!< response indicates no transaction: %s', resp['reason'])\n            raise BadLedgerTxn('Response indicates no transaction: {}'.format(resp['reason']))\n\n        LOGGER.debug('_BaseAgent._sign_submit <<< %s', rv_json)\n        return rv_json", "response": "Sign and submit a request to the ledger."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def get_cred_def(self, cd_id: str) -> str:\n\n        LOGGER.debug('_BaseAgent.get_cred_def >>> cd_id: %s', cd_id)\n\n        rv_json = json.dumps({})\n\n        with CRED_DEF_CACHE.lock:\n            if cd_id in CRED_DEF_CACHE:\n                LOGGER.info('_BaseAgent.get_cred_def: got cred def for %s from cache', cd_id)\n                rv_json = json.dumps(CRED_DEF_CACHE[cd_id])\n                LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)\n                return rv_json\n\n            req_json = await ledger.build_get_cred_def_request(self.did, cd_id)\n            resp_json = await self._submit(req_json)\n            resp = json.loads(resp_json)\n            if not ('result' in resp and resp['result'].get('data', None)):\n                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)\n                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))\n            try:\n                (_, rv_json) = await ledger.parse_get_cred_def_response(resp_json)\n            except IndyError:  # ledger replied, but there is no such cred def\n                LOGGER.debug('_BaseAgent.get_cred_def: <!< no cred def exists on %s', cd_id)\n                raise AbsentCredDef('No cred def exists on {}'.format(cd_id))\n            CRED_DEF_CACHE[cd_id] = json.loads(rv_json)\n            LOGGER.info('_BaseAgent.get_cred_def: got cred def %s from ledger', cd_id)\n\n        LOGGER.debug('_BaseAgent.get_cred_def <<< %s', rv_json)\n        return rv_json", "response": "Get credential definition by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting schema from ledger by schema key or schema identifier.", "response": "async def get_schema(self, index: Union[SchemaKey, int, str]) -> str:\n        \"\"\"\n        Get schema from ledger by SchemaKey namedtuple (origin DID, name, version),\n        sequence number, or schema identifier.\n\n        Raise AbsentSchema for no such schema, logging any error condition and raising\n        BadLedgerTxn on bad request.\n\n        Retrieve the schema from the agent's schema cache if it has it; cache it\n        en passant if it does not (and there is a corresponding schema on the ledger).\n\n        :param index: schema key (origin DID, name, version), sequence number, or schema identifier\n        :return: schema json, parsed from ledger\n        \"\"\"\n\n        LOGGER.debug('_BaseAgent.get_schema >>> index: %s', index)\n\n        rv_json = json.dumps({})\n        with SCHEMA_CACHE.lock:\n            if SCHEMA_CACHE.contains(index):\n                LOGGER.info('_BaseAgent.get_schema: got schema %s from schema cache', index)\n                rv_json = SCHEMA_CACHE[index]\n                LOGGER.debug('_BaseAgent.get_schema <<< %s', rv_json)\n                return json.dumps(rv_json)\n\n            if isinstance(index, SchemaKey) or (isinstance(index, str) and ':2:' in index):\n                s_id = schema_id(*index) if isinstance(index, SchemaKey) else index\n                s_key = schema_key(s_id)\n                req_json = await ledger.build_get_schema_request(self.did, s_id)\n                resp_json = await self._submit(req_json)\n                resp = json.loads(resp_json)\n\n                if not ('result' in resp and resp['result'].get('data', {}).get('attr_names', None)):\n                    LOGGER.debug('_BaseAgent.get_schema: <!< no schema exists on %s', index)\n                    raise AbsentSchema('No schema exists on {}'.format(index))\n                try:\n                    (_, rv_json) = await ledger.parse_get_schema_response(resp_json)\n                except IndyError:  # ledger replied, but there is no such schema\n                    LOGGER.debug('_BaseAgent.get_schema: <!< no schema exists on %s', index)\n                    raise AbsentSchema('No schema exists on {}'.format(index))\n                SCHEMA_CACHE[s_key] = json.loads(rv_json)  # cache indexes by both txn# and schema key en passant\n                LOGGER.info('_BaseAgent.get_schema: got schema %s from ledger', index)\n\n            elif isinstance(index, (int, str)):  # ':2:' not in index - it's a stringified int txn num if it's a str\n                txn_json = await self.get_txn(int(index))\n                txn = json.loads(txn_json)\n                if txn.get('type', None) == '101':  # {} for no such txn; 101 marks indy-sdk schema txn type\n                    rv_json = await self.get_schema(SchemaKey(\n                        txn['metadata']['from'],\n                        txn['data']['data']['name'],\n                        txn['data']['data']['version']))\n                else:\n                    LOGGER.info('_BaseAgent.get_schema: no schema at seq #%s on ledger', index)\n\n            else:\n                LOGGER.debug('_BaseAgent.get_schema: <!< bad schema index type')\n                raise AbsentSchema('Attempt to get schema on ({}) {} , must use schema key or an int'.format(\n                    type(index),\n                    index))\n\n        LOGGER.debug('_BaseAgent.get_schema <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a transaction by its sequence number.", "response": "async def get_txn(self, txn: int) -> str:\n        \"\"\"\n        Find a transaction on the distributed ledger by its sequence number.\n\n        :param txn: transaction number\n        :return: json sequence number of transaction, null for no match\n        \"\"\"\n\n        LOGGER.debug('_BaseAgent.get_txn >>> txn: %s', txn)\n\n        rv_json = json.dumps({})\n        req_json = await ledger.build_get_txn_request(self.did, None, txn)\n        resp = json.loads(await self._submit(req_json))\n\n        rv_json = json.dumps((resp['result'].get('data', {}) or {}).get('txn', {}))  # \"data\": null for no such txn\n        LOGGER.debug('_BaseAgent.get_txn <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef proc_forward(etype, namespace: Dict[str, Any]):\n    return etype._evaluate(namespace, namespace) if type(etype) is ForwardRef else etype", "response": "Resolve etype to an actual type if it is a forward reference"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_union(etype) -> bool:\n    return getattr(etype, '__origin__', None) is not None and \\\n           getattr(etype.__origin__, '_name', None) and\\\n           etype.__origin__._name == 'Union'", "response": "Determines whether etype is a Union"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntest if the given element conforms to at least one of the types in etype with the given namespace.", "response": "def union_conforms(element: Union, etype, namespace: Dict[str, Any], conforms: Callable) -> bool:\n    \"\"\" Determine whether element conforms to at least one of the types in etype\n\n    :param element: element to test\n    :param etype: type to test against\n    :param namespace: Namespace to use for resolving forward references\n    :param conforms: conformance test function\n    :return: True if element conforms to at least one type in etype\n    \"\"\"\n    return any(conforms(element, t, namespace) for t in etype.__args__)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the input TypedField contains instance attributes that match the input parameters.", "response": "def _matches(field, params):\n    \"\"\"Return True if the input TypedField `field` contains instance attributes\n    that match the input parameters.\n\n    Args:\n        field: A TypedField instance.\n        params: A dictionary of TypedField instance attribute-to-value mappings.\n\n    Returns:\n        True if the input TypedField matches the input parameters.\n    \"\"\"\n    fieldattrs = six.iteritems(params)\n    return all(getattr(field, attr) == val for attr, val in fieldattrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iterfields(klass):\n    is_field = lambda x: isinstance(x, TypedField)\n\n    for name, field in inspect.getmembers(klass, predicate=is_field):\n        yield name, field", "response": "Iterate over the input class members and yield its TypedFields."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns all TypedFields found on the input Entity that were initialized with the input kwargs.", "response": "def find(entity, **kwargs):\n    \"\"\"Return all TypedFields found on the input `Entity` that were initialized\n    with the input **kwargs.\n\n    Example:\n        >>> find(myentity, multiple=True, type_=Foo)\n\n    Note:\n        TypedFields.__init__() can accept a string or a class as a type_\n        argument, but this method expects a class.\n\n    Args:\n        **kwargs: TypedField __init__ **kwargs to search on.\n\n    Returns:\n        A list of TypedFields with matching **kwarg values.\n    \"\"\"\n    try:\n        typedfields = entity.typed_fields()\n    except AttributeError:\n        typedfields = iterfields(entity.__class__)\n\n    matching = [x for x in typedfields if _matches(x, kwargs)]\n    return matching"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _clean(self, value):\n        if value is None:\n            return None\n        elif self.type_ is None:\n            return value\n        elif self.check_type(value):\n            return value\n        elif self.is_type_castable:  # noqa\n            return self.type_(value)\n\n        error_fmt = \"%s must be a %s, not a %s\"\n        error = error_fmt % (self.name, self.type_, type(value))\n        raise TypeError(error)", "response": "Validate and clean a candidate value for this field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transformer(self):\n        if self.factory:\n            return self.factory\n        elif self.type_:\n            return self.type_\n        else:\n            return None", "response": "Returns the class for this field that transforms non - Entity objects into Entity instances."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bowshock_logger():\n    '''creates a logger obj'''\n\n    FORMAT = '%(asctime)-15s %(message)s'\n    logging.basicConfig(format=FORMAT, level=logging.INFO)\n    logger = logging.getLogger('bowshock_logger')\n\n    return logger", "response": "creates a logger obj"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_collection(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):\n        if fourth_arg is None:\n            collection_id, branch_name, author = first_arg, sec_arg, third_arg\n            gh_user = branch_name.split('_collection_')[0]\n            parent_sha = self.get_master_sha()\n        else:\n            gh_user, collection_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg\n        if commit_msg is None:\n            commit_msg = \"Delete Collection '%s' via OpenTree API\" % collection_id\n        return self._remove_document(gh_user, collection_id, parent_sha, author, commit_msg)", "response": "Given a collection_id branch and optionally an anatomical author remove a collection on the given branch and optionally anatomical author. Returns the SHA of the commit on branch."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_collection(self, collection_id, file_content, branch, author):\n        gh_user = branch.split('_collection_')[0]\n        msg = \"Update Collection '%s' via OpenTree API\" % collection_id\n        return self.write_document(gh_user,\n                                   collection_id,\n                                   file_content,\n                                   branch, author,\n                                   commit_msg=msg)", "response": "Given a collection_id temporary filename of content branch and auth_info\n        Returns a file_content"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_collection_from_tmpfile(self, collection_id, tmpfi, parent_sha, auth_info, commit_msg=''):\n        return self.write_doc_from_tmpfile(collection_id,\n                                           tmpfi,\n                                           parent_sha,\n                                           auth_info,\n                                           commit_msg,\n                                           doctype_display_name=\"collection\")", "response": "Given a collection_id temporary filename of content branch and auth_info write a document from a temporary file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def load_cache(self, archive: bool = False) -> int:\n\n        LOGGER.debug('Verifier.load_cache >>> archive: %s', archive)\n\n        rv = int(time())\n        for s_id in self.cfg.get('archive-on-close', {}).get('schema_id', {}):\n            with SCHEMA_CACHE.lock:\n                await self.get_schema(s_id)\n        for cd_id in self.cfg.get('archive-on-close', {}).get('cred_def_id', {}):\n            with CRED_DEF_CACHE.lock:\n                await self.get_cred_def(cd_id)\n        for rr_id in self.cfg.get('archive-on-close', {}).get('rev_reg_id', {}):\n            await self._get_rev_reg_def(rr_id)\n            with REVO_CACHE.lock:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                if revo_cache_entry:\n                    try:\n                        await revo_cache_entry.get_state_json(self._build_rr_state_json, rv, rv)\n                    except ClosedPool:\n                        LOGGER.warning(\n                            'Verifier %s is offline from pool %s, cannot update revo cache reg state for %s to %s',\n                            self.wallet.name,\n                            self.pool.name,\n                            rr_id,\n                            rv)\n\n        if archive:\n            Caches.archive(self.dir_cache)\n        LOGGER.debug('Verifier.load_cache <<< %s', rv)\n        return rv", "response": "Load caches and archive enough to go offline and be able to verify proof\n        on content marked of interest in configuration."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nopen current object and parse cache from archive if parse - cache - on - open is set to True.", "response": "async def open(self) -> 'HolderProver':\n        \"\"\"\n        Explicit entry. Perform ancestor opening operations,\n        then parse cache from archive if so configured, and\n        synchronize revocation registry to tails tree content.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Verifier.open >>>')\n\n        await super().open()\n        if self.cfg.get('parse-cache-on-open', False):\n            Caches.parse(self.dir_cache)\n\n        LOGGER.debug('Verifier.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def close(self) -> None:\n\n        LOGGER.debug('Verifier.close >>>')\n\n        if self.cfg.get('archive-on-close', {}):\n            await self.load_cache(True)\n            Caches.purge_archives(self.dir_cache, True)\n\n        await super().close()\n\n        LOGGER.debug('Verifier.close <<<')", "response": "Explicit exit. If archive - on - close is set to True load cache and purge prior cache archives."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nverify proof as Verifier creates and returns the proof as json encoded True if proof is valid False if proof is not valid", "response": "async def verify_proof(self, proof_req: dict, proof: dict) -> str:\n        \"\"\"\n        Verify proof as Verifier. Raise AbsentRevReg if a proof cites a revocation registry\n        that does not exist on the distributed ledger.\n\n        :param proof_req: proof request as Verifier creates, as per proof_req_json above\n        :param proof: proof as HolderProver creates\n        :return: json encoded True if proof is valid; False if not\n        \"\"\"\n\n        LOGGER.debug('Verifier.verify_proof >>> proof_req: %s, proof: %s', proof_req, proof)\n\n        s_id2schema = {}\n        cd_id2cred_def = {}\n        rr_id2rr_def = {}\n        rr_id2rr = {}\n        proof_ids = proof['identifiers']\n\n        for proof_id in proof_ids:\n            # schema\n            s_id = proof_id['schema_id']\n            if s_id not in s_id2schema:\n                schema = json.loads(await self.get_schema(s_id))  # add to cache en passant\n                if not schema:\n                    LOGGER.debug(\n                        'Verifier.verify_proof: <!< absent schema %s, proof req may be for another ledger',\n                        s_id)\n                    raise AbsentSchema(\n                        'Absent schema {}, proof req may be for another ledger'.format(s_id))\n                s_id2schema[s_id] = schema\n\n            # cred def\n            cd_id = proof_id['cred_def_id']\n            if cd_id not in cd_id2cred_def:\n                cred_def = json.loads(await self.get_cred_def(cd_id))  # add to cache en passant\n                cd_id2cred_def[cd_id] = cred_def\n\n            # rev reg def\n            rr_id = proof_id['rev_reg_id']\n            if not rr_id:\n                continue\n\n            rr_def_json = await self._get_rev_reg_def(rr_id)\n            rr_id2rr_def[rr_id] = json.loads(rr_def_json)\n\n            # timestamp\n            timestamp = proof_id['timestamp']\n            with REVO_CACHE.lock:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                (rr_json, _) = await revo_cache_entry.get_state_json(self._build_rr_state_json, timestamp, timestamp)\n                if rr_id not in rr_id2rr:\n                    rr_id2rr[rr_id] = {}\n                rr_id2rr[rr_id][timestamp] = json.loads(rr_json)\n\n        rv = json.dumps(await anoncreds.verifier_verify_proof(\n            json.dumps(proof_req),\n            json.dumps(proof),\n            json.dumps(s_id2schema),\n            json.dumps(cd_id2cred_def),\n            json.dumps(rr_id2rr_def),\n            json.dumps(rr_id2rr)))\n\n        LOGGER.debug('Verifier.verify_proof <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngrants permission if owner or admin.", "response": "def can(self):\n        \"\"\"Grant permission if owner or admin.\"\"\"\n        return str(current_user.get_id()) == str(self.community.id_user) or \\\n            DynamicPermission(ActionNeed('admin-access')).can()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tabular2html(fname=None, X=None, fin=None, title=None, printheader=False, \n                 split=True, usecss=None, writecss=None, SERVERNAME=None, \n                 SERVER_FROM_CURDIR='../', ROWS_PER_PAGE=1000, \n                 returnstring = False, **kwargs):\n    \"\"\"\n    Creates an html representation of tabular data, either from a tabarray or \n    an externa file (`including ``.hsv``, ``.csv``, ``.tsv``).  If no data is \n    directly provided by passing a tabarray to `X`, then a tabarray is \n    constructed using :func:`tabular.tabarray.tabarray.__new__`.\n\n    **Parameters**\n\n            **fname** :  string\n\n                    Path to the \"main\" HTML file to be created.  This file path\n                    must end in ``.html``.\n\n                    Note that this function will create additional files (e.g. \n                    a ``.css`` file and multiple linked ``.html`` files for \n                    large datasets) linked to this HTML file, inside of the \n                    directory containing `fname`.\n\n            **X** :  tabarray\n\n                    If `X` is `None`, then one of `fin`, `array`, `records`,\n                    `columns`, `SV`, `HSV`, or `HSVlist` must not be `None`.\n\n            **fin** :  string\n\n                    File path to to a source of tabular dat, which will be\n                    loaded using the tabarray constructor.  The load method\n                    will be inferred from the file extension and whether or not\n                    there is a headerkey in the first line of the file.\n\n            **title** :  string\n\n                    Description to be used in the <title> tag of the created\n                    html file.\n\n            **printheader** : boolean\n\n                    If `printheader = True`, will print out a \"header\" and \n                    \"footer\" (both in the body of the HTML documents) on every \n                    page.  The header contains information about the name of \n                    the input file and the number of rows, as well what the \n                    current page number is (if split between multiple \n                    documents) and links to any other pages.  The footer \n                    contains the same page number and links.\n\n            **split** : boolean\n\n                    If `split = False`, will not split to multiple HTML pages, \n                    regardless of value of `ROWS_PER_PAGE`.\n\n            **usecss** : False or None or string\n\n                    If usecss is False, no link to a cssfile is included in the \n                    page header, and no cssfile is written out.  If is a \n                    string, that string is assumed to be a path and is linked \n                    to as the CSS file.  If it is None, then consideration of \n                    the `writecss` variable is made.\n\n            **writecss** : boolean\n\n                    If `usecss` is not `None`, then if `writecss` is not \n                    `False`:  the default css sheet is generated and written to \n                    a file whose name is either generated by default (if \n                    writecss is None) else given by writecss itself, and linked \n                    to in the file header\n\n            **SERVERNAME** :  string\n\n                    Server name.  For example, this could be the ServerName\n                    of a VirtualHost on your local machine, assuming that\n                    `fname` describes a path on the server.\n\n            **SERVER_FROM_CURDIR** :  string\n\n                    Root path of server relative to the current directory.\n                    Assumed to be '../'.\n\n            **ROWS_PER_PAGE** :  positive integer or 'all'\n\n                    This sets the number of records displayed per .html page\n                    (if the tabular file has more than ROWS_PER_PAGE rows,\n                    it will be split into multiple sections on several .html\n                    pages (default = 1000).\n\n                    If the value is 'all' then the page is not split (e.g. it \n                    is as if split = False)\n\n            **See also:**  the kwargs arguments must be valid keyword arguments \n            for :func:`tabular.tabarray.tabarray.__new__`, the tabarray \n            constructor, see documentation for descriptions.\n\n    \"\"\"\n\n    # Must write to an HTML file.\n\n    assert returnstring or fname.endswith( '.html' ), 'fname must end in \".html\".'\n      \n    if X is None:\n        if fin is not None:\n            if fin.lstrip('/').endswith('.hsv'):\n                kwargs['HSVfile'] = fin\n            elif fin.endswith('.tsv') or fin.endswith('.csv'):\n                kwargs['SVfile'] = fin\n            elif fin.endswith(('.npy','.npz')):\n                kwargs['binary'] = fin\n            else:\n                assert False, ('This algorithm is being forced to determine '\n                              'the proper file type for web representation '\n                              'from file\\'s path (e.g. by looking at '\n                              'extension) since the type is not given ' \n                              'explicitly by use of a keyword argument, but '\n                              'is having problems deducing the intended file ' \n                              'type from the path (e.g., because the '\n                              'extension is not one of those this algorithm ' \n                              'recognizes).')\n        else:\n            assert any([l in kwargs.keys() and kwargs[l] != None \n                        for l in ['SVfile','binary','HSVfile']]), \\\n                   ('Either a tabarray is given, or file path \"fin\" is '\n                    'given, or one of \"HSV\", \"binary\", or \"SV\" keyword '\n                    'arguments are given.')\n\n        X = tb.tabarray(**kwargs)\n\n    names = X.dtype.names\n    try:\n        RowColors = X['__color__']\n    except:\n        if '__color__' in names:\n            cspot = names.index('__color__')\n            RowColors = [r[cspot] for r in X]\n        else:\n            RowColors = [''] * len(X)\n    try:\n        coloring = X.coloring\n    except:\n        coloring = {}\n\n    Num_Records = len(X)\n    Num_Cols = len(names)\n    ColorStyles = CSSColoring(names, coloring)\n    HdrNts = HeaderNotations(names, coloring)\n\n    # If I specify usecss and it is not false, it must be a string and I want \n    # to put that file name in the link and not write anything out.\n    # If I specify writecss I want it to write out file to that name and use it \n    # in the link.\n    # If usecss = false, writecss is false and nothing is put in the link.\n    # If usecss is not specified, then ...\n\n    if usecss != None:\n        if isinstance(usecss, str):\n            cssfile = usecss\n            CSSLINK = ('<link rel=\"stylesheet\" type=\"text/css\" href=\"' + '/' + \n                       cssfile[len(SERVER_FROM_CURDIR):] + '\"</link>')\n        else:\n            assert usecss == False\n            CSSLINK = ''\n    else:\n        if writecss == False or returnstring:\n            CSSLINK = ''\n        else:\n            if not isinstance(writecss,str):\n                cssfile = fname[:-5] + '.css'\n            else:\n                cssfile = writecss\n            WriteOutCSS(ColorStyles[1],cssfile)\n            CSSLINK = ('<link rel=\"stylesheet\" type=\"text/css\" href=\"' + '/'  + \n                       cssfile[len(SERVER_FROM_CURDIR):] + '\"</link>')\n\n    if returnstring:\n        split = False\n    \n    if not split or ROWS_PER_PAGE == 'all':\n        ROWS_PER_PAGE = Num_Records + 1\n\n    numSections = int(Num_Records / ROWS_PER_PAGE) + 1   \n    \n    # section2file(i) returns the name of the .html file corresponding to \n    # section number i.\n    section2file = (lambda sectionNum: fname if sectionNum == 0 \n                else splitext(fname)[0] + str(sectionNum) + splitext(fname)[1])\n    \n    if title is None:\n        if not fin is None:\n            title = fin\n        else:\n            title = 'Title Not Given'\n    for section in range(numSections):\n        sectionfname = section2file(section)\n        fromRow = section * ROWS_PER_PAGE  # Start record # for this section.\n        toRow = min( fromRow + ROWS_PER_PAGE, Num_Records)  # End record #.\n\n        if printheader and not returnstring:\n            prefix = '/' + DirName(fname[len(SERVER_FROM_CURDIR):]) + '/'\n        else:\n            prefix = ''\n\n        # Open the output file for the section to fileobject 'f'.\n        if not returnstring:\n            f = open(sectionfname,'w')\n        else:\n            f = tempfile.TemporaryFile('w+b')\n                     \n        # Write out file header.\n        if not returnstring:\n            f.write('<html><META HTTP-EQUIV=\"Content-Type\" '\n                'CONTENT=\"text/html; charset=utf-8\" /><head><title>' + \n                title + '</title>' + CSSLINK + '</head><body>\\n' )\n        if printheader:\n            f.write('<p>Tabular File (page ' + str(section + 1) + ' of ' + \n                    str(numSections) + ', rows ' + str(fromRow + 1) + \n                    ' - ' + str(toRow) + '): ' + title + '</p>\\n')\n            f.write('<p>page ')\n            if section > 0:\n                f.write(' <a href=\"' + prefix + \n                       basename(section2file(section - 1)) + '\">prev</a> ')\n            if section < numSections - 1:\n                f.write(' <a href=\"' + prefix + \n                       basename(section2file(section + 1)) + '\">next</a> ')\n            for page in range(numSections):\n                f.write((' <a href=\"' + prefix + \n                      basename(section2file(page)) + '\">' + str(page + 1) + \n                      '</a>') if page != section else ' ' + str(page + 1))\n            f.write( '</p>' )\n\n        # Write out table with number of cols.\n        f.write('<table border=\"1\" cellspacing=\"0\" cellpadding=\"4\">\\n')\n        f.write('<col span=\"' + str(Num_Cols) + '\" align=\"center\">\\n')\n\n        # Write out table header line.\n        f.write('<thead>')\n        if len(HdrNts) > 0:\n            for h in HdrNts:\n                f.write(h + '\\n')\n        f.write('<tr align=\"center\">')\n        for name in names:\n            f.write('<th class=\"' + ColorStyles[0][name] + '\">' + \n                    cgi.escape(name) + '</th>')\n        f.write('</tr>')\n        f.write('</thead>\\n')\n\n        # Write out each record in the section.\n        f.write( '<tbody>\\n' )\n        if (len(names) > 1) or (fin != None and fin.endswith('.csv')):\n            for row in range( fromRow, toRow ):\n                colorst = (' style=\"background-color:' + RowColors[row] + \n                           '\" ' if  RowColors[row] != '' else '')\n                f.write('<tr align=\"center\">')\n                for (i, val) in enumerate(X[ row ]):\n                    #f.write('<td>' + cgi.escape(str(val)) + '</td>')\n                    f.write('<td ' + colorst + ' class=\"' + \n                            ColorStyles[0][names[i]] + '\">' + str(val).replace('\\n','<br/>') + \n                            '</td>')\n                f.write('</tr>\\n')\n        else:\n            for row in range(fromRow, toRow):\n                f.write('<tr align=\"center\">')\n                #f.write('<td>' + cgi.escape(str(X[row])) + '</td>')\n                f.write('<td>' + str(X[row]).replace('\\n','<br/>') + '</td>')\n                f.write('</tr>\\n')\n        f.write('</tbody>\\n')\n\n        f.write( '</table>' )\n\n        # Write out hyperlinks to other sections.\n        if printheader:\n            f.write('<p>page ')\n            if section > 0:\n                f.write(' <a href=\"' + prefix + \n                       basename(section2file(section - 1)) + '\">prev</a> ')\n            if section < numSections - 1:\n                f.write(' <a href=\"' + prefix + \n                       basename(section2file(section + 1)) + '\">next</a> ')\n            for page in range(numSections):\n                f.write((' <a href=\"' + prefix + \n                         basename(section2file(page)) + '\">' + \n                         str(page + 1) + '</a>') if page != section \n                                                 else ' ' + str(page + 1))\n\n            f.write('</p>')\n\n        # End file.\n        if not returnstring:\n            f.write('</body></html>\\n')\n    \n        if returnstring:\n            f.seek(0)\n            s = f.read()\n            f.close()\n            return s\n        else:\n            f.close()", "response": "Returns an html representation of a tabular data file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef space_events(lon=None, lat=None, limit=None, date=None):\n    '''\n    \n    lat & lon expect decimal latitude and longitude values. (Required)\n    elevation assumes meters. (Optional)\n    limit assumes an integer. Default is 5. (Optional)\n    date expects an ISO 8601 formatted date. (Optional)\n    '''\n\n    base_url = 'http://api.predictthesky.org/?'\n\n    if not lon or not lat:\n        raise ValueError(\n            \"space_events endpoint expects lat and lon, type has to be float. Call the method with keyword args. Ex : lon=100.75, lat=1.5\")\n    else:\n        try:\n            validate_float(lon, lat)\n            # Floats are entered/displayed as decimal numbers, but your computer \n            # (in fact, your standard C library) stores them as binary. \n            # You get some side effects from this transition:\n            # >>> print len(repr(0.1))\n            # 19\n            # >>> print repr(0.1)\n            # 0.10000000000000001\n            # Thus using decimal to str transition is more reliant\n            lon = decimal.Decimal(lon)\n            lat = decimal.Decimal(lat)\n            base_url += \"lon=\" + str(lon) + \"&\" + \"lat=\" + str(lat)\n        except:\n            raise ValueError(\n                \"space_events endpoint expects lat and lon, type has to be float. Call the method with keyword args. Ex : lon=100.75, lat=1.5\")\n\n    if date:\n        try:\n            validate_iso8601(date)\n            base_url += \"&\" + 'date=' + date\n        except:\n            raise ValueError(\n                \"Your date input is not in iso8601 format. ex: 2014-01-01T23:59:59\")\n\n    if limit:\n        if not isinstance(limit, int):\n            logger.error(\n                \"The limit arg you provided is not the type of int, ignoring it\")\n        base_url += \"&\" + \"limit=\" + str(limit)\n\n    return dispatch_http_get(base_url)", "response": "This method returns the space events of the specified object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntake the union of a list of lists and returns the aggregated list of the union of the lists.", "response": "def listunion(ListOfLists):\n    \"\"\"\n    Take the union of a list of lists.\n\n    Take a Python list of Python lists::\n\n            [[l11,l12, ...], [l21,l22, ...], ... , [ln1, ln2, ...]]\n\n    and return the aggregated list::\n\n            [l11,l12, ..., l21, l22 , ...]\n\n    For a list of two lists, e.g. `[a, b]`, this is like::\n\n            a.extend(b)\n\n    **Parameters**\n\n            **ListOfLists** :  Python list\n\n                    Python list of Python lists.\n\n    **Returns**\n\n            **u** :  Python list\n\n                    Python list created by taking the union of the\n                    lists in `ListOfLists`.\n\n    \"\"\"\n    u = []\n    for s in ListOfLists:\n        if s != None:\n            u.extend(s)\n    return u"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a numpy array of indices giving the inverse of a permutation.", "response": "def perminverse(s):\n    '''\n    Fast inverse of a (numpy) permutation.\n\n    **Paramters**\n\n            **s** :  sequence\n\n                    Sequence of indices giving a permutation.\n\n    **Returns**\n\n            **inv** :  numpy array\n\n                    Sequence of indices giving the inverse of permutation `s`.\n\n    '''\n    X = np.array(range(len(s)))\n    X[s] = range(len(s))\n    return X"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DEFAULT_TYPEINFERER(column):\n    try:\n        return np.array([int(x) for x in column], 'int')\n    except:\n        try:\n            return np.array([float(x) if x != '' else np.nan for x in column],\n                            'float')\n        except:\n            return np.array(column, 'str')", "response": "This function is used to determine the data type of a column in a random order."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef DEFAULT_NULLVALUE(test):\n    return False if isinstance(test,bool) \\\n           else 0 if isinstance(test,int) \\\n           else 0.0 if isinstance(test,float) \\\n           else ''", "response": "Returns a default value for each of the types of test values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef as_python(self, name: str) -> str:\n        if self._map_valuetype:\n            return self.map_as_python(name)\n        else:\n            return self.obj_as_python(name)", "response": "Return the python representation of the class represented by this object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef members_entries(self, all_are_optional: bool=False) -> List[Tuple[str, str]]:\n        rval = []\n        if self._members:\n            for member in self._members:\n                rval += member.members_entries(all_are_optional)\n        elif self._choices:\n            for choice in self._choices:\n                rval += self._context.reference(choice).members_entries(True)\n        else:\n            return []\n        return rval", "response": "Return an ordered list of elements for the _members section"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visitObjectExpr(self, ctx: jsgParser.ObjectExprContext):\n        if not self._name:\n            self._name = self._context.anon_id()\n        if ctx.membersDef():\n            self.visitChildren(ctx)\n        elif ctx.MAPSTO():\n            if ctx.LEXER_ID_REF():\n                self._map_name_type = as_token(ctx)\n            # Any and absent mean the same thing\n            self._map_valuetype = JSGValueType(self._context, ctx.valueType())\n            if ctx.ebnfSuffix():\n                self._map_ebnf = JSGEbnf(self._context, ctx.ebnfSuffix())", "response": "visitObjectExpr - Gets the name type and value type of the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visitMember(self, ctx: jsgParser.MemberContext):\n        self._strict = ctx.COMMA() is None\n        self.visitChildren(ctx)", "response": "Member is a pairDef COMMA?"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_filtered_study_ids(shard, include_aliases=False):\n    from peyotl.phylesystem.helper import DIGIT_PATTERN\n    k = shard.get_doc_ids()\n    if shard.has_aliases and (not include_aliases):\n        x = []\n        for i in k:\n            if DIGIT_PATTERN.match(i) or ((len(i) > 1) and (i[-2] == '_')):\n                pass\n            else:\n                x.append(i)\n        return x", "response": "Optionally filters out aliases from standard doc - id list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef diagnose_repo_nexml2json(shard):\n    with shard._index_lock:\n        fp = next(iter(shard.study_index.values()))[2]\n    with codecs.open(fp, mode='r', encoding='utf-8') as fo:\n        fj = json.load(fo)\n        from peyotl.nexson_syntax import detect_nexson_version\n        return detect_nexson_version(fj)", "response": "Optimistic test for Nexson version in a shard"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the numeric part of the newest study_id", "response": "def _determine_next_study_id(self):\n        \"\"\"Return the numeric part of the newest study_id\n\n        Checks out master branch as a side effect!\n        \"\"\"\n        if self._doc_counter_lock is None:\n            self._doc_counter_lock = Lock()\n        prefix = self._new_study_prefix\n        lp = len(prefix)\n        n = 0\n        # this function holds the lock for quite awhile,\n        #   but it only called on the first instance of\n        #   of creating a new study\n        with self._doc_counter_lock:\n            with self._index_lock:\n                for k in self.study_index.keys():\n                    if k.startswith(prefix):\n                        try:\n                            pn = int(k[lp:])\n                            if pn > n:\n                                n = pn\n                        except:\n                            pass\n            nsi_contents = self._read_master_branch_resource(self._id_minting_file, is_json=True)\n            if nsi_contents:\n                self._next_study_id = nsi_contents['next_study_id']\n                if self._next_study_id <= n:\n                    m = 'next_study_id in {} is set lower than the ID of an existing study!'\n                    m = m.format(self._id_minting_file)\n                    raise RuntimeError(m)\n            else:\n                # legacy support for repo with no next_study_id.json file\n                self._next_study_id = n\n                self._advance_new_study_id()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _advance_new_study_id(self):\n        c = self._next_study_id\n        self._next_study_id = 1 + c\n        content = u'{\"next_study_id\": %d}\\n' % self._next_study_id\n        # The content is JSON, but we hand-rolled the string above\n        #       so that we can use it as a commit_msg\n        self._write_master_branch_resource(content,\n                                           self._id_minting_file,\n                                           commit_msg=content,\n                                           is_json=False)\n        return c", "response": "Advance the counter to the next value and store it in the master branch resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _diagnose_prefixes(self):\n        from peyotl.phylesystem import STUDY_ID_PATTERN\n        p = set()\n        for name in os.listdir(self.doc_dir):\n            if STUDY_ID_PATTERN.match(name):\n                p.add(name[:3])\n        return p", "response": "Returns a set of all of the prefixes seen in the main document dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _mint_new_study_id(self):\n        # studies created by the OpenTree API start with ot_,\n        # so they don't conflict with new study id's from other sources\n        with self._doc_counter_lock:\n            c = self._advance_new_study_id()\n        # @TODO. This form of incrementing assumes that\n        #   this codebase is the only service minting\n        #   new study IDs!\n        return \"{p}{c:d}\".format(p=self._new_study_prefix, c=c)", "response": "Checks out master branch as a side effect"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_git_action_for_new_study(self, new_study_id=None):\n        ga = self.create_git_action()\n        if new_study_id is None:\n            new_study_id = self._mint_new_study_id()\n        self.register_doc_id(ga, new_study_id)\n        return ga, new_study_id", "response": "Creates a git action for a new study."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of all non - list items in l", "response": "def flatten(l: Iterable) -> List:\n    \"\"\"Return a list of all non-list items in l\n\n    :param l: list to be flattened\n    :return:\n    \"\"\"\n    rval = []\n    for e in l:\n        if not isinstance(e, str) and isinstance(e, Iterable):\n            if len(list(e)):\n                rval += flatten(e)\n        else:\n            rval.append(e)\n    return rval"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef flatten_unique(l: Iterable) -> List:\n    rval = OrderedDict()\n    for e in l:\n        if not isinstance(e, str) and isinstance(e, Iterable):\n            for ev in flatten_unique(e):\n                rval[ev] = None\n        else:\n            rval[e] = None\n    return list(rval.keys())", "response": "Return a list of UNIQUE non - list items in l"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_terminal(ctx: ParserRuleContext) -> str:\n    tkn = None\n    for ele_name in identifier_types:\n        ele = getattr(ctx, ele_name, None)\n        if ele and ele():\n            tkn = ele().getText()[1:-1] if ele_name == 'STRING' else ele().getText()\n    return str(tkn)", "response": "Extract the terminal token from the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef as_tokens(ctx: List[ParserRuleContext]) -> List[str]:\n    return [as_token(e) for e in ctx]", "response": "Return a list of tokens in a list of identifiers in ctx"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_valid_python(tkn: str) -> bool:\n    try:\n        root = ast.parse(tkn)\n    except SyntaxError:\n        return False\n    return len(root.body) == 1 and isinstance(root.body[0], ast.Expr) and isinstance(root.body[0].value, ast.Name)", "response": "Determine whether tkn is a valid python identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_study(self, first_arg, sec_arg, third_arg, fourth_arg=None, commit_msg=None):\n        if fourth_arg is None:\n            study_id, branch_name, author = first_arg, sec_arg, third_arg\n            gh_user = branch_name.split('_study_')[0]\n            parent_sha = self.get_master_sha()\n        else:\n            gh_user, study_id, parent_sha, author = first_arg, sec_arg, third_arg, fourth_arg\n        if commit_msg is None:\n            commit_msg = \"Delete Study #%s via OpenTree API\" % study_id\n        return self._remove_document(gh_user, study_id, parent_sha, author, commit_msg)", "response": "Given a study_id branch and optionally an anonymization author remove a study on the given branch and optionally anonymization author. Returns the SHA of the commit on branch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_study(self, study_id, file_content, branch, author):\n        gh_user = branch.split('_study_')[0]\n        msg = \"Update Study #%s via OpenTree API\" % study_id\n        return self.write_document(gh_user,\n                                   study_id,\n                                   file_content,\n                                   branch,\n                                   author,\n                                   commit_msg=msg)", "response": "Given a study_id temporary filename of content branch and auth_info\n        Returns a file_content"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a study_id temporary filename of content branch and auth_info write a study document from a temporary file.", "response": "def write_study_from_tmpfile(self, study_id, tmpfi, parent_sha, auth_info, commit_msg=''):\n        \"\"\"Given a study_id, temporary filename of content, branch and auth_info\n        \"\"\"\n        return self.write_doc_from_tmpfile(study_id,\n                                           tmpfi,\n                                           parent_sha,\n                                           auth_info,\n                                           commit_msg,\n                                           doctype_display_name=\"study\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the communities file storage.", "response": "def init():\n    \"\"\"Initialize the communities file storage.\"\"\"\n    try:\n        initialize_communities_bucket()\n        click.secho('Community init successful.', fg='green')\n    except FilesException as e:\n        click.secho(e.message, fg='red')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addlogo(community_id, logo):\n    # Create the bucket\n    c = Community.get(community_id)\n    if not c:\n        click.secho('Community {0} does not exist.'.format(community_id),\n                    fg='red')\n        return\n    ext = save_and_validate_logo(logo, logo.name, c.id)\n    c.logo_ext = ext\n    db.session.commit()", "response": "Add logo to the community."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef request(community_id, record_id, accept):\n    c = Community.get(community_id)\n    assert c is not None\n    record = Record.get_record(record_id)\n    if accept:\n        c.add_record(record)\n        record.commit()\n    else:\n        InclusionRequest.create(community=c, record=record,\n                                notify=False)\n    db.session.commit()\n    RecordIndexer().index_by_id(record.id)", "response": "Request a record acceptance to a community."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove(community_id, record_id):\n    c = Community.get(community_id)\n    assert c is not None\n    c.remove_record(record_id)\n    db.session.commit()\n    RecordIndexer().index_by_id(record_id)", "response": "Remove a record from community."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sets_are_rooted_compat(one_set, other):\n    if one_set.issubset(other) or other.issubset(one_set):\n        return True\n    return not intersection_not_empty(one_set, other)", "response": "returns True if one_set and other are rooted compatible and False otherwise"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntakes a NexSON object and returns a dict of articles otu_id -> oxid", "response": "def gen_otu_dict(nex_obj, nexson_version=None):\n    \"\"\"Takes a NexSON object and returns a dict of\n    otu_id -> otu_obj\n    \"\"\"\n    if nexson_version is None:\n        nexson_version = detect_nexson_version(nex_obj)\n    if _is_by_id_hbf(nexson_version):\n        otus = nex_obj['nexml']['otusById']\n        if len(otus) > 1:\n            d = {}\n            for v in otus.values():\n                d.update(v['otuById'])\n            return d\n        else:\n            return otus.values()[0]['otuById']\n    o_dict = {}\n    for ob in nex_obj.get('otus', []):\n        for o in ob.get('otu', []):\n            oid = o['@id']\n            o_dict[oid] = o\n    return o_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nopening pool with given configuration.", "response": "async def open(self) -> 'NodePool':\n        \"\"\"\n        Explicit entry. Opens pool as configured, for later closure via close().\n        For use when keeping pool open across multiple calls.\n\n        Raise any IndyError causing failure to create ledger configuration.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('NodePool.open >>>')\n\n        try:\n            await pool.set_protocol_version(2)  # 1 for indy-node 1.3, 2 for indy-node 1.4\n            await pool.create_pool_ledger_config(self.name, json.dumps({'genesis_txn': str(self.genesis_txn_path)}))\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.PoolLedgerConfigAlreadyExistsError:\n                LOGGER.info('Pool ledger config for %s already exists', self.name)\n            else:\n                LOGGER.debug('NodePool.open: <!< indy error code %s', x_indy.error_code)\n                raise x_indy\n\n        self._handle = await pool.open_pool_ledger(self.name, json.dumps(self.cfg))\n\n        LOGGER.debug('NodePool.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef acquire_lock_raise(git_action, fail_msg=''):\n    try:\n        git_action.acquire_lock()\n    except LockError as e:\n        msg = '{o} Details: {d}'.format(o=fail_msg, d=e.message)\n        _LOG.debug(msg)\n        raise GitWorkflowError(msg)", "response": "Adapts LockError to HTTP."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmerge from master into WIP", "response": "def merge_from_master(git_action, doc_id, auth_info, parent_sha, doctype_display_name=\"document\"):\n    \"\"\"merge from master into the WIP for this document/author\n    this is needed to allow a worker's future saves to\n    be merged seamlessly into master\n    \"\"\"\n    gh_user = get_user_author(auth_info)[0]\n    acquire_lock_raise(git_action,\n                       fail_msg=\"Could not acquire lock to merge %s #%s\" % (doctype_display_name, doc_id))\n    try:\n        git_action.checkout_master()\n        written_fp = git_action.path_for_doc(doc_id)\n        if os.path.exists(written_fp):\n            master_file_blob_sha = git_action.get_blob_sha_for_file(written_fp)\n        else:\n            raise GitWorkflowError('{t} \"{i}\" does not exist on master'.format(t=doctype_display_name, i=doc_id))\n        branch = git_action.create_or_checkout_branch(gh_user, doc_id, parent_sha)\n        new_sha = git_action.merge('master', branch)\n    finally:\n        git_action.release_lock()\n    # What other useful information should be returned on a successful write?\n    return {\n        \"error\": 0,\n        \"resource_id\": doc_id,\n        \"branch_name\": branch,\n        \"description\": \"Updated %s #%s\" % (doctype_display_name, doc_id),\n        \"sha\": new_sha,\n        \"merged_sha\": master_file_blob_sha,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generic_commit_and_try_merge2master_wf(git_action,\n                                           file_content,\n                                           doc_id,\n                                           auth_info,\n                                           parent_sha,\n                                           commit_msg='',\n                                           merged_sha=None,\n                                           doctype_display_name=\"document\"):\n    \"\"\"Actually make a local Git commit and push it to our remote\n    \"\"\"\n    # _LOG.debug('generic_commit_and_try_merge2master_wf: doc_id=\"{s}\" \\\n    #            parent_sha=\"{p}\" merged_sha=\"{m}\"'.format(\n    #            s=doc_id, p=parent_sha, m=merged_sha))\n    merge_needed = False\n    fc = tempfile.NamedTemporaryFile()\n    # N.B. we currently assume file_content is text/JSON, or should be serialized from a dict\n    try:\n        if is_str_type(file_content):\n            fc.write(file_content)\n        else:\n            write_as_json(file_content, fc)\n        fc.flush()\n        try:\n            max_file_size = git_action.max_file_size\n        except:\n            max_file_size = None\n        if max_file_size is not None:\n            file_size = os.stat(fc.name).st_size\n            if file_size > max_file_size:\n                m = 'Commit of {t} \"{i}\" had a file size ({a} bytes) which ' \\\n                    'exceeds the maximum size allowed ({b} bytes).'\n                m = m.format(t=doctype_display_name, i=doc_id, a=file_size, b=max_file_size)\n                raise GitWorkflowError(m)\n        f = \"Could not acquire lock to write to %s #%s\" % (doctype_display_name, doc_id)\n        acquire_lock_raise(git_action, fail_msg=f)\n        try:\n            try:\n                commit_resp = git_action.write_doc_from_tmpfile(doc_id,\n                                                                fc,\n                                                                parent_sha,\n                                                                auth_info,\n                                                                commit_msg,\n                                                                doctype_display_name)\n            except Exception as e:\n                _LOG.exception('write_doc_from_tmpfile exception')\n                raise GitWorkflowError(\"Could not write to %s #%s ! Details: \\n%s\" %\n                                       (doctype_display_name, doc_id, e.message))\n            written_fp = git_action.path_for_doc(doc_id)\n            branch_name = commit_resp['branch']\n            new_sha = commit_resp['commit_sha']\n            _LOG.debug('write of {t} {i} on parent {p} returned = {c}'.format(t=doctype_display_name,\n                                                                              i=doc_id,\n                                                                              p=parent_sha,\n                                                                              c=str(commit_resp)))\n            m_resp = _do_merge2master_commit(git_action,\n                                             new_sha,\n                                             branch_name,\n                                             written_fp,\n                                             merged_sha=merged_sha,\n                                             prev_file_sha=commit_resp.get('prev_file_sha'))\n            new_sha, branch_name, merge_needed = m_resp\n        finally:\n            git_action.release_lock()\n    finally:\n        fc.close()\n    # What other useful information should be returned on a successful write?\n    r = {\n        \"error\": 0,\n        \"resource_id\": doc_id,\n        \"branch_name\": branch_name,\n        \"description\": \"Updated %s #%s\" % (doctype_display_name, doc_id),\n        \"sha\": new_sha,\n        \"merge_needed\": merge_needed,\n    }\n    _LOG.debug('returning {r}'.format(r=str(r)))\n    return r", "response": "Generic commit and try to merge the document to the master."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the chosen country in the session or cookie.", "response": "def set_country(request):\n    \"\"\"\n    Sets the chosen country in the session or cookie.\n\n    If `next' query param is present, it redirects to a given url.\n    \"\"\"\n    if request.method == 'POST':\n        next = request.POST.get('next', request.GET.get('next'))\n        if is_safe_url(url=next, host=request.get_host()):\n            response = http.HttpResponseRedirect(next)\n        else:\n            response = http.HttpResponse()\n\n        country_code = request.POST.get('country', '').upper()\n        if country_code != geo.get_supported_country(country_code):\n            return http.HttpResponseBadRequest()\n\n        if hasattr(request, 'session'):\n            request.session[geo.COUNTRY_SESSION_KEY] = country_code\n        else:\n            response.set_cookie(geo.COUNTRY_COOKIE_NAME,\n                               country_code,\n                               max_age=geo.COUNTRY_COOKIE_AGE,\n                               path=geo.COUNTRY_COOKIE_PATH)\n        return response\n    else:\n        return http.HttpResponseNotAllowed(['POST'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reference(self, tkn: str):\n        return self.grammarelts[tkn] if tkn in self.grammarelts else UndefinedElement(tkn)", "response": "Return the element that tkn represents"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dependency_list(self, tkn: str) -> List[str]:\n        if tkn not in self.dependency_map:\n            self.dependency_map[tkn] = [tkn]        # Force a circular reference\n            self.dependency_map[tkn] = self.reference(tkn).dependency_list()\n        return self.dependency_map[tkn]", "response": "Return a list all of the grammarelts that depend on tkn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dependencies(self, tkn: str) -> Set[str]:\n        return set(self.dependency_list(tkn))", "response": "Return all the items that tkn depends on as a set\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef undefined_entries(self) -> Set[str]:\n        return as_set([[d for d in self.dependencies(k) if d not in self.grammarelts]\n                       for k in self.grammarelts.keys()])", "response": "Return the set of tokens that are referenced but not defined."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dependency_closure(self, tkn: str, seen: Optional[Set[str]]=None) -> Set[str]:\n        if seen is None:\n            seen = set()\n        for k in self.dependencies(tkn):\n            if k not in seen:\n                seen.add(k)\n                self.dependency_closure(k, seen)\n        return seen", "response": "Determine the transitive closure of tkn s dependents and dependents of tkn."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the set of circular references", "response": "def circular_references(self) -> Set[str]:\n        \"\"\"\n        Return the set of recursive (circular) references\n        :return:\n        \"\"\"\n        rval = set()\n        for k in self.grammarelts.keys():\n            if k in self.dependency_closure(k):\n                rval.add(k)\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate forward references for all circular references", "response": "def resolve_circular_references(self) -> None:\n        \"\"\"\n        Create forward references for all circular references\n        :return:\n        \"\"\"\n        circulars = self.circular_references()\n        for c in circulars:\n            fwdref = JSGForwardRef(c)\n            self.grammarelts[fwdref.label] = fwdref\n            self.forward_refs[c] = fwdref.label"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ordered_elements(self) -> str:\n        from pyjsg.parser_impl.jsg_lexerruleblock_parser import JSGLexerRuleBlock\n        from pyjsg.parser_impl.jsg_arrayexpr_parser import JSGArrayExpr\n        from pyjsg.parser_impl.jsg_objectexpr_parser import JSGObjectExpr\n        from pyjsg.parser_impl.jsg_builtinvaluetype_parser import JSGBuiltinValueType\n        from pyjsg.parser_impl.jsg_valuetype_parser import JSGValueType\n\n        state = 0\n        self.depths = {}\n        for k in self.dependency_map.keys():\n            self.calc_depths(k)\n        # NOTE that depth is not in the closure -- if you create an iterator and then bump depth\n        #      the iterator will work against the bumped depth\n        depth = -1\n        max_depth = max(self.depths.values()) if self.depths else 0\n        while state >= 0:\n            iter_ = iter([])\n            if state == 0:\n                depth += 1\n                if depth <= max_depth:\n                    iter_ = (k for k, v in self.grammarelts.items()\n                             if isinstance(v, (JSGLexerRuleBlock, JSGBuiltinValueType)) and self.depths[k] == depth)\n                else:\n                    depth = -1\n                    state += 1\n            elif state == 1:\n                depth += 1\n                if depth <= max_depth:\n                    iter_ = (k for k, v in self.grammarelts.items()\n                             if isinstance(v, (JSGObjectExpr, JSGArrayExpr, JSGValueType)) and\n                             self.depths[k] == depth and k not in self.forward_refs)\n                else:\n                    depth = -1\n                    state += 1\n            elif state == 2:          # Forward references\n                depth += 1\n                if depth <= max_depth:\n                    iter_ = (k for k, v in self.grammarelts.items()\n                             if isinstance(v, (JSGObjectExpr, JSGArrayExpr, JSGValueType)) and\n                             self.depths[k] == depth and k in self.forward_refs)\n                else:\n                    state = -1\n            while state >= 0:\n                rval = next(iter_, None)\n                if rval is None:\n                    break\n                yield rval", "response": "Generator that returns items in the order of increasing dependency depth"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_request(sender, request=None, notify=True, **kwargs):\n    if current_app.config['COMMUNITIES_MAIL_ENABLED'] and notify:\n        send_community_request_email(request)", "response": "Send a new request email to the community."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninject provisional_communities key to ES index.", "response": "def inject_provisional_community(sender, json=None, record=None, index=None,\n                                 **kwargs):\n    \"\"\"Inject 'provisional_communities' key to ES index.\"\"\"\n    if index and not index.startswith(\n            current_app.config['COMMUNITIES_INDEX_PREFIX']):\n        return\n\n    json['provisional_communities'] = list(sorted([\n        r.id_community for r in InclusionRequest.get_by_record(record.id)\n    ]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_oaipmh_set(mapper, connection, community):\n    from invenio_oaiserver.models import OAISet\n    with db.session.begin_nested():\n        obj = OAISet(spec=community.oaiset_spec,\n                     name=community.title,\n                     description=community.description)\n        db.session.add(obj)", "response": "Signal for creating OAI - PMH sets during community creation."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsignals for creating OAI - PMH sets during community creation.", "response": "def destroy_oaipmh_set(mapper, connection, community):\n    \"\"\"Signal for creating OAI-PMH sets during community creation.\"\"\"\n    from invenio_oaiserver.models import OAISet\n    with db.session.begin_nested():\n        oaiset = OAISet.query.filter_by(\n            spec=community.oaiset_spec).one_or_none()\n        if oaiset is None:\n            raise Exception(\n                \"OAISet for community {0} is missing\".format(community.id))\n        db.session.delete(oaiset)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_nodes(self, query_dict=None, exact=False, verbose=False, **kwargs):\n        assert self.use_v1\n        return self._do_query('{p}/singlePropertySearchForTreeNodes'.format(p=self.query_prefix),\n                              query_dict=query_dict,\n                              exact=exact,\n                              verbose=verbose,\n                              valid_keys=self.node_search_term_set,\n                              kwargs=kwargs)", "response": "Query on node properties. See documentation for _OTIWrapper class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nqueries on tree properties. See documentation for _OTIWrapper class.", "response": "def find_trees(self, query_dict=None, exact=False, verbose=False, wrap_response=False, **kwargs):\n        \"\"\"Query on tree properties. See documentation for _OTIWrapper class.\"\"\"\n        if self.use_v1:\n            uri = '{p}/singlePropertySearchForTrees'.format(p=self.query_prefix)\n        else:\n            uri = '{p}/find_trees'.format(p=self.query_prefix)\n        resp = self._do_query(uri,\n                              query_dict=query_dict,\n                              exact=exact,\n                              verbose=verbose,\n                              valid_keys=self.tree_search_term_set,\n                              kwargs=kwargs)\n        if wrap_response:\n            return TreeRefList(resp)\n        return resp"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_studies(self, query_dict=None, exact=False, verbose=False, **kwargs):\n        if self.use_v1:\n            uri = '{p}/singlePropertySearchForStudies'.format(p=self.query_prefix)\n        else:\n            uri = '{p}/find_studies'.format(p=self.query_prefix)\n        return self._do_query(uri,\n                              query_dict=query_dict,\n                              exact=exact,\n                              verbose=verbose,\n                              valid_keys=self.study_search_term_set,\n                              kwargs=kwargs)", "response": "Query on study properties. See documentation for _OTIWrapper class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_all_studies(self, include_trees=False, verbose=False):\n        if not self.use_v1:\n            return self.find_studies(verbose=verbose)\n        url = '{p}/findAllStudies'.format(p=self.query_prefix)\n        data = {'includeTreeMetadata': include_trees,\n                'verbose': verbose, }\n        response = self.json_http_post(url, data=anyjson.dumps(data))\n        return response", "response": "Returns a list of dicts for the entire set of studies indexed by oti."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_requirements():\n    '''returns requirements array for package'''\n    packages = []\n    with open(\"requirements.txt\", \"r\") as req_doc:\n        for package in req_doc:\n            packages.append(package.replace(\"\\n\", \"\"))\n    return packages", "response": "returns requirements array for package"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a _TaxonomicAmendmentStore object.", "response": "def TaxonomicAmendmentStore(repos_dict=None,\n                            repos_par=None,\n                            with_caching=True,\n                            assumed_doc_version=None,\n                            git_ssh=None,\n                            pkey=None,\n                            git_action_class=TaxonomicAmendmentsGitAction,\n                            mirror_info=None,\n                            infrastructure_commit_author='OpenTree API <api@opentreeoflife.org>'):\n    \"\"\"Factory function for a _TaxonomicAmendmentStore object.\n\n    A wrapper around the _TaxonomicAmendmentStore class instantiation for\n    the most common use case: a singleton _TaxonomicAmendmentStore.\n    If you need distinct _TaxonomicAmendmentStore objects, you'll need to\n    call that class directly.\n    \"\"\"\n    global _THE_TAXONOMIC_AMENDMENT_STORE\n    if _THE_TAXONOMIC_AMENDMENT_STORE is None:\n        _THE_TAXONOMIC_AMENDMENT_STORE = _TaxonomicAmendmentStore(repos_dict=repos_dict,\n                                                                  repos_par=repos_par,\n                                                                  with_caching=with_caching,\n                                                                  assumed_doc_version=assumed_doc_version,\n                                                                  git_ssh=git_ssh,\n                                                                  pkey=pkey,\n                                                                  git_action_class=git_action_class,\n                                                                  mirror_info=mirror_info,\n                                                                  infrastructure_commit_author=infrastructure_commit_author)\n    return _THE_TAXONOMIC_AMENDMENT_STORE"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate and save this JSON. Ensure a unique amendment id.", "response": "def add_new_amendment(self,\n                          json_repr,\n                          auth_info,\n                          commit_msg=''):\n        \"\"\"Validate and save this JSON. Ensure (and return) a unique amendment id\"\"\"\n        amendment = self._coerce_json_to_amendment(json_repr)\n        if amendment is None:\n            msg = \"File failed to parse as JSON:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not self._is_valid_amendment_json(amendment):\n            msg = \"JSON is not a valid amendment:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n\n        # Mint any needed ottids, update the document accordingly, and\n        # prepare a response with\n        #  - per-taxon mapping of tag to ottid\n        #  - resulting id (or URL) to the stored amendment\n        # To ensure synchronization of ottids and amendments, this should be an\n        # atomic operation!\n\n        # check for tags and confirm count of new ottids required (if provided)\n        num_taxa_eligible_for_ids = 0\n        for taxon in amendment.get(\"taxa\"):\n            # N.B. We don't require 'tag' in amendment validation; check for it now!\n            if \"tag\" not in taxon:\n                raise KeyError('Requested Taxon is missing \"tag\" property!')\n            # allow for taxa that have already been assigned (use cases?)\n            if \"ott_id\" not in taxon:\n                num_taxa_eligible_for_ids += 1\n        if 'new_ottids_required' in amendment:\n            requested_ids = amendment['new_ottids_required']\n            try:\n                assert (requested_ids == num_taxa_eligible_for_ids)\n            except:\n                m = 'Number of OTT ids requested ({r}) does not match eligible taxa ({t})'\n                m = m.format(r=requested_ids, t=num_taxa_eligible_for_ids)\n                raise ValueError(m)\n\n        # mint new ids and assign each to an eligible taxon\n        with self._growing_shard._doc_counter_lock:\n            # build a map of tags to ottids, to return to the caller\n            tag_to_id = {}\n            first_new_id = self._growing_shard.next_ott_id\n            last_new_id = first_new_id + num_taxa_eligible_for_ids - 1\n            if last_new_id < first_new_id:\n                # This can happen if ther are no eligible taxa! In this case,\n                # repeat and \"burn\" the next ottid (ie, it will be used to\n                # identify this amendment, but it won't be assigned)\n                last_new_id = first_new_id\n            new_id = first_new_id\n            for taxon in amendment.get(\"taxa\"):\n                if \"ott_id\" not in taxon:\n                    taxon[\"ott_id\"] = new_id\n                    ttag = taxon[\"tag\"]\n                    tag_to_id[ttag] = new_id\n                    new_id += 1\n                    ptag = taxon.get(\"parent_tag\")\n                    if ptag is not None:\n                        taxon[\"parent\"] = tag_to_id[ptag]\n            if num_taxa_eligible_for_ids > 0:\n                try:\n                    assert (new_id == (last_new_id + 1))\n                except:\n                    applied = last_new_id - first_new_id + 1\n                    raise ValueError(\n                        'Number of OTT ids requested ({r}) does not match ids actually applied ({a})'.format(\n                            r=requested_ids, a=applied))\n\n            # Build a proper amendment id, in the format '{subtype}-{first ottid}-{last-ottid}'\n            amendment_subtype = 'additions'\n            # TODO: Handle other subtypes (beyond additions) by examining JSON?\n            amendment_id = \"{s}-{f}-{l}\".format(s=amendment_subtype, f=first_new_id, l=last_new_id)\n\n            # Check the proposed id for uniqueness (just to be safe), then\n            # \"reserve\" it using a placeholder value.\n            with self._index_lock:\n                if amendment_id in self._doc2shard_map:\n                    # this should never happen!\n                    raise KeyError('Amendment \"{i}\" already exists!'.format(i=amendment_id))\n                self._doc2shard_map[amendment_id] = None\n\n            # Set the amendment's top-level \"id\" property to match\n            amendment[\"id\"] = amendment_id\n\n            # pass the id and amendment JSON to a proper git action\n            new_amendment_id = None\n            r = None\n            try:\n                # assign the new id to a shard (important prep for commit_and_try_merge2master)\n                gd_id_pair = self.create_git_action_for_new_amendment(new_amendment_id=amendment_id)\n                new_amendment_id = gd_id_pair[1]\n                # For amendments, the id should not have changed!\n                try:\n                    assert new_amendment_id == amendment_id\n                except:\n                    raise KeyError('Amendment id unexpectedly changed from \"{o}\" to \"{n}\"!'.format(\n                        o=amendment_id, n=new_amendment_id))\n                try:\n                    # it's already been validated, so keep it simple\n                    r = self.commit_and_try_merge2master(file_content=amendment,\n                                                         doc_id=new_amendment_id,\n                                                         auth_info=auth_info,\n                                                         parent_sha=None,\n                                                         commit_msg=commit_msg,\n                                                         merged_sha=None)\n                except:\n                    self._growing_shard.delete_doc_from_index(new_amendment_id)\n                    raise\n\n                # amendment is now in the repo, so we can safely reserve the ottids\n                first_minted_id, last_minted_id = self._growing_shard._mint_new_ott_ids(\n                    how_many=max(num_taxa_eligible_for_ids, 1))\n                # do a final check for errors!\n                try:\n                    assert first_minted_id == first_new_id\n                except:\n                    raise ValueError('First minted ottid is \"{m}\", expected \"{e}\"!'.format(\n                        m=first_minted_id, e=first_new_id))\n                try:\n                    assert last_minted_id == last_new_id\n                except:\n                    raise ValueError('Last minted ottid is \"{m}\", expected \"{e}\"!'.format(\n                        m=last_minted_id, e=last_new_id))\n                # Add the tag-to-ottid mapping to the response, so a caller\n                # (e.g. the curation webapp) can provisionally assign them\n                r['tag_to_ottid'] = tag_to_id\n            except:\n                with self._index_lock:\n                    if new_amendment_id in self._doc2shard_map:\n                        del self._doc2shard_map[new_amendment_id]\n                raise\n\n        with self._index_lock:\n            self._doc2shard_map[new_amendment_id] = self._growing_shard\n        return new_amendment_id, r"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates and save this JSON and return a new amendment id.", "response": "def update_existing_amendment(self,\n                                  amendment_id=None,\n                                  json_repr=None,\n                                  auth_info=None,\n                                  parent_sha=None,\n                                  merged_sha=None,\n                                  commit_msg=''):\n        \"\"\"Validate and save this JSON. Ensure (and return) a unique amendment id\"\"\"\n        amendment = self._coerce_json_to_amendment(json_repr)\n        if amendment is None:\n            msg = \"File failed to parse as JSON:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not self._is_valid_amendment_json(amendment):\n            msg = \"JSON is not a valid amendment:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not amendment_id:\n            raise ValueError(\"Amendment id not provided (or invalid)\")\n        if not self.has_doc(amendment_id):\n            msg = \"Unexpected amendment id '{}' (expected an existing id!)\".format(amendment_id)\n            raise ValueError(msg)\n        # pass the id and amendment JSON to a proper git action\n        r = None\n        try:\n            # it's already been validated, so keep it simple\n            r = self.commit_and_try_merge2master(file_content=amendment,\n                                                 doc_id=amendment_id,\n                                                 auth_info=auth_info,\n                                                 parent_sha=parent_sha,\n                                                 commit_msg=commit_msg,\n                                                 merged_sha=merged_sha)\n            # identify shard for this id!?\n        except:\n            raise\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _build_amendment_id(self, json_repr):\n        amendment = self._coerce_json_to_amendment(json_repr)\n        if amendment is None:\n            return None\n        amendment_subtype = 'additions'\n        # TODO: Look more deeply once we have other subtypes!\n        first_ottid = amendment['TODO']\n        last_ottid = amendment['TODO']\n        return slugify('{s}-{f}-{l}'.format(s=amendment_subtype, f=first_ottid, l=last_ottid))", "response": "Parse the JSON and return a slug in the form subtype - first ottid - last - otid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _is_valid_amendment_json(self, json_repr):\n        amendment = self._coerce_json_to_amendment(json_repr)\n        if amendment is None:\n            # invalid JSON, definitely broken\n            return False\n        aa = validate_amendment(amendment)\n        errors = aa[0]\n        for e in errors:\n            _LOG.debug('> invalid JSON: {m}'.format(m=e.encode('utf-8')))\n        if len(errors) > 0:\n            return False\n        return True", "response": "Check if the given JSON representation is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _coerce_json_to_amendment(self, json_repr):\n        if isinstance(json_repr, dict):\n            amendment = json_repr\n        else:\n            try:\n                amendment = anyjson.loads(json_repr)\n            except:\n                _LOG.warn('> invalid JSON (failed anyjson parsing)')\n                return None\n        return amendment", "response": "Use to ensure that a JSON string is parsed to the equivalent dict in python."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_marked_communities():\n    # TODO: Delete the community ID from all records metadata first\n    raise NotImplementedError()\n    Community.query.filter_by(\n        Community.delete_time > datetime.utcnow()).delete()\n    db.session.commit()", "response": "Delete communities after holdout time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_expired_requests():\n    InclusionRequest.query.filter_by(\n        InclusionRequest.expiry_date > datetime.utcnow()).delete()\n    db.session.commit()", "response": "Delete expired inclusion requests."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a PhyloSchema object for a content_spec.", "response": "def create_content_spec(**kwargs):\n    \"\"\"Sugar. factory for a PhyloSchema object.\n\n    Repackages the kwargs to kwargs for PhyloSchema so that our\n    PhyloSchema.__init__ does not have to be soo rich\n    \"\"\"\n    format_str = kwargs.get('format', 'nexson')\n    nexson_version = kwargs.get('nexson_version', 'native')\n    otu_label = kwargs.get('otu_label')\n    if otu_label is None:\n        otu_label = kwargs.get('tip_label')\n    content = kwargs.get('content')\n    if content is not None:\n        content_id = kwargs.get('content_id')\n        if content_id is None:\n            content_id = _get_content_id_from(**kwargs)\n    else:\n        content, content_id = _sniff_content_from_kwargs(**kwargs)\n    if content is None:\n        content = 'study'\n    return PhyloSchema(content=content,\n                       content_id=content_id,\n                       format_str=format_str,\n                       version=nexson_version,\n                       otu_label=otu_label,\n                       repo_nexml2json=kwargs.get('repo_nexml2json'),\n                       bracket_ingroup=bool(kwargs.get('bracket_ingroup', False)),\n                       cull_nonmatching=kwargs.get('cull_nonmatching'))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts an XML document to JSON using the honeybadgerfish convention.", "response": "def get_ot_study_info_from_nexml(src=None,\n                                 nexml_content=None,\n                                 encoding=u'utf8',\n                                 nexson_syntax_version=DEFAULT_NEXSON_VERSION):\n    \"\"\"Converts an XML doc to JSON using the honeybadgerfish convention (see to_honeybadgerfish_dict)\n    and then prunes elements not used by open tree of life study curartion.\n\n    If nexml_content is provided, it is interpreted as the contents\n    of an NeXML file in utf-8 encoding.\n\n    If nexml_content is None, then the src arg will be used src can be either:\n        * a file_object, or\n        * a string\n    If `src` is a string then it will be treated as a filepath unless it\n        begins with http:// or https:// (in which case it will be downloaded\n        using peyotl.utility.download)\n    Returns a dictionary with the keys/values encoded according to the honeybadgerfish convention\n    See https://github.com/OpenTreeOfLife/api.opentreeoflife.org/wiki/HoneyBadgerFish\n\n    Currently:\n        removes nexml/characters @TODO: should replace it with a URI for\n            where the removed character data can be found.\n    \"\"\"\n    if _is_by_id_hbf(nexson_syntax_version):\n        nsv = DIRECT_HONEY_BADGERFISH\n    else:\n        nsv = nexson_syntax_version\n    if nexml_content is None:\n        if is_str_type(src):\n            if src.startswith('http://') or src.startswith('https://'):\n                from peyotl.utility import download\n                nexml_content = download(url=src, encoding=encoding)\n                nexml_content = nexml_content.encode('utf-8')\n            else:\n                with codecs.open(src, 'r', encoding=encoding) as src:\n                    nexml_content = src.read().encode('utf-8')\n        else:\n            nexml_content = src.read().encode('utf-8')\n    doc = xml.dom.minidom.parseString(nexml_content)\n    doc_root = doc.documentElement\n\n    ccfg = ConversionConfig(output_format=nsv, input_format=NEXML_NEXSON_VERSION)\n    converter = Nexml2Nexson(ccfg)\n    o = converter.convert(doc_root)\n    if _is_by_id_hbf(nexson_syntax_version):\n        o = convert_nexson_format(o, BY_ID_HONEY_BADGERFISH, current_format=nsv)\n    if 'nex:nexml' in o:\n        n = o['nex:nexml']\n        del o['nex:nexml']\n        o['nexml'] = n\n    return o"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_nexson_format(blob,\n                          out_nexson_format,\n                          current_format=None,\n                          remove_old_structs=True,\n                          pristine_if_invalid=False,\n                          sort_arbitrary=False):\n    \"\"\"Take a dict form of NexSON and converts its datastructures to\n    those needed to serialize as out_nexson_format.\n    If current_format is not specified, it will be inferred.\n    If `remove_old_structs` is False and different honeybadgerfish varieties\n        are selected, the `blob` will be 'fat\" containing both types\n        of lookup structures.\n    If pristine_if_invalid is False, then the object may be corrupted if it\n        is an invalid nexson struct. Setting this to False can result in\n        faster translation, but if an exception is raised the object may\n        be polluted with partially constructed fields for the out_nexson_format.\n    \"\"\"\n    if not current_format:\n        current_format = detect_nexson_version(blob)\n    out_nexson_format = resolve_nexson_format(out_nexson_format)\n    if current_format == out_nexson_format:\n        if sort_arbitrary:\n            sort_arbitrarily_ordered_nexson(blob)\n        return blob\n    two2zero = _is_by_id_hbf(out_nexson_format) and _is_badgerfish_version(current_format)\n    zero2two = _is_by_id_hbf(current_format) and _is_badgerfish_version(out_nexson_format)\n    if two2zero or zero2two:\n        # go from 0.0 -> 1.0 then the 1.0->1.2 should succeed without nexml...\n        blob = convert_nexson_format(blob,\n                                     DIRECT_HONEY_BADGERFISH,\n                                     current_format=current_format,\n                                     remove_old_structs=remove_old_structs,\n                                     pristine_if_invalid=pristine_if_invalid)\n        current_format = DIRECT_HONEY_BADGERFISH\n    ccdict = {'output_format': out_nexson_format,\n              'input_format': current_format,\n              'remove_old_structs': remove_old_structs,\n              'pristine_if_invalid': pristine_if_invalid}\n    ccfg = ConversionConfig(ccdict)\n    if _is_badgerfish_version(current_format):\n        converter = Badgerfish2DirectNexson(ccfg)\n    elif _is_badgerfish_version(out_nexson_format):\n        assert _is_direct_hbf(current_format)\n        converter = Direct2BadgerfishNexson(ccfg)\n    elif _is_direct_hbf(current_format) and (out_nexson_format == BY_ID_HONEY_BADGERFISH):\n        converter = Direct2OptimalNexson(ccfg)\n    elif _is_direct_hbf(out_nexson_format) and (current_format == BY_ID_HONEY_BADGERFISH):\n        converter = Optimal2DirectNexson(ccfg)\n    else:\n        raise NotImplementedError('Conversion from {i} to {o}'.format(i=current_format, o=out_nexson_format))\n    blob = converter.convert(blob)\n    if sort_arbitrary:\n        sort_arbitrarily_ordered_nexson(blob)\n    return blob", "response": "Take a dict form of NexSON and converts it to a new nexson structure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sort_meta_elements(blob):\n    v = detect_nexson_version(blob)\n    if _is_badgerfish_version(v):\n        _recursive_sort_meta(blob, '')\n    return blob", "response": "This function sorts the meta elements in a list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntaking a list of dicts each of which has an '@id' key, sorts the elements in the list by the value of the @id key. Assumes that @id is unique or the dicts have a meaningul < operator", "response": "def _inplace_sort_by_id(unsorted_list):\n    \"\"\"Takes a list of dicts each of which has an '@id' key,\n    sorts the elements in the list by the value of the @id key.\n    Assumes that @id is unique or the dicts have a meaningul < operator\n    \"\"\"\n    if not isinstance(unsorted_list, list):\n        return\n    sorted_list = [(i.get('@id'), i) for i in unsorted_list]\n    sorted_list.sort()\n    del unsorted_list[:]\n    unsorted_list.extend([i[1] for i in sorted_list])"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the given blob by the ID.", "response": "def sort_arbitrarily_ordered_nexson(blob):\n    \"\"\"Primarily used for testing (getting nice diffs). Calls\n    sort_meta_elements and then sorts otu, node and edge list by id\n    \"\"\"\n    # otu, node and edge elements have no necessary orger in v0.0 or v1.0\n    v = detect_nexson_version(blob)\n    nex = get_nexml_el(blob)\n    if _is_by_id_hbf(v):\n        return blob\n    sort_meta_elements(blob)\n    for ob in _get_index_list_of_values(nex, 'otus'):\n        _inplace_sort_by_id(ob.get('otu', []))\n    for tb in _get_index_list_of_values(nex, 'trees'):\n        for tree in _get_index_list_of_values(tb, 'tree'):\n            _inplace_sort_by_id(tree.get('node', []))\n            _inplace_sort_by_id(tree.get('edge', []))\n    return blob"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_newick_leaf_label(out, node_id, node, otu_group, label_key, leaf_labels, unlabeled_counter, needs_quotes_pattern):\n    otu_id = node['@otu']\n    otu = otu_group[otu_id]\n    if is_str_type(label_key):\n        label = otu.get(label_key)\n        if label is None:\n            unlabeled_counter += 1\n            o = otu.get('^ot:originalLabel', '<unknown>')\n            label = \"'*tip #{n:d} not mapped to OTT. Original label - {o}'\"\n            label = label.format(n=unlabeled_counter, o=o)\n        else:\n            label = quote_newick_name(label, needs_quotes_pattern)\n    else:\n        label = quote_newick_name(label_key(node_id, node, otu), needs_quotes_pattern)\n    if leaf_labels is not None:\n        if label not in leaf_labels[1]:\n            leaf_labels[1][label] = len(leaf_labels[0])\n            leaf_labels[0].append(label)\n    out.write(label)\n    return unlabeled_counter", "response": "Write a leaf label for a node."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the internal label for a node.", "response": "def _write_newick_internal_label(out, node_id, node, otu_group, label_key, needs_quotes_pattern):\n    \"\"\"`label_key` is a string (a key in the otu object) or a callable that takes two arguments:\n        the node, and the otu (which may be None for an internal node)\n    If `leaf_labels` is not None, it shoulr be a (list, dict) pair which will be filled. The list will\n        hold the order encountered,\n        and the dict will map name to index in the list\n    \"\"\"\n    otu_id = node.get('@otu')\n    if is_str_type(label_key):\n        if otu_id is None:\n            return\n        otu = otu_group[otu_id]\n        label = otu.get(label_key)\n    else:\n        label = label_key(node_id, node, None)\n    if label is not None:\n        label = quote_newick_name(label, needs_quotes_pattern)\n        out.write(label)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_tree_to_newick(tree,\n                           otu_group,\n                           label_key,\n                           leaf_labels,\n                           needs_quotes_pattern=NEWICK_NEEDING_QUOTING,\n                           subtree_id=None,\n                           bracket_ingroup=False):\n    \"\"\"`label_key` is a string (a key in the otu object) or a callable that takes two arguments:\n        the node, and the otu (which may be None for an internal node)\n    If `leaf_labels` is not None, it shoulr be a (list, dict) pair which will be filled. The list will\n        hold the order encountered,\n        and the dict will map name to index in the list\n    \"\"\"\n    assert (not is_str_type(label_key)) or (label_key in PhyloSchema._NEWICK_PROP_VALS)  # pylint: disable=W0212\n    ingroup_node_id = tree.get('^ot:inGroupClade')\n    if subtree_id:\n        if subtree_id == 'ingroup':\n            root_id = ingroup_node_id\n            ingroup_node_id = None  # turns of the comment pre-ingroup-marker\n        else:\n            root_id = subtree_id\n    else:\n        root_id = tree['^ot:rootNodeId']\n    edges = tree['edgeBySourceId']\n    if root_id not in edges:\n        return None\n    nodes = tree['nodeById']\n    sio, out = get_utf_8_string_io_writer()\n    nexson_frag_write_newick(out,\n                             edges,\n                             nodes,\n                             otu_group,\n                             label_key,\n                             leaf_labels,\n                             root_id,\n                             needs_quotes_pattern=needs_quotes_pattern,\n                             ingroup_id=ingroup_node_id,\n                             bracket_ingroup=bracket_ingroup)\n    flush_utf_8_writer(out)\n    return sio.getvalue()", "response": "Convert a tree to a newick tree."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef nexson_frag_write_newick(out,\n                             edges,\n                             nodes,\n                             otu_group,\n                             label_key,\n                             leaf_labels,\n                             root_id,\n                             needs_quotes_pattern=NEWICK_NEEDING_QUOTING,\n                             ingroup_id=None,\n                             bracket_ingroup=False,\n                             with_edge_lengths=True):\n    \"\"\"`label_key` is a string (a key in the otu object) or a callable that takes two arguments:\n        the node, and the otu (which may be None for an internal node)\n    If `leaf_labels` is not None, it shoulr be a (list, dict) pair which will be filled. The list will\n        hold the order encountered,\n        and the dict will map name to index in the list\n    \"\"\"\n    unlabeled_counter = 0\n    curr_node_id = root_id\n    assert curr_node_id\n    curr_edge = None\n    curr_sib_list = []\n    curr_stack = []\n    going_tipward = True\n    while True:\n        if going_tipward:\n            outgoing_edges = edges.get(curr_node_id)\n            if outgoing_edges is None:\n                curr_node = nodes[curr_node_id]\n                assert curr_node_id is not None\n                assert curr_node_id is not None\n                unlabeled_counter = _write_newick_leaf_label(out,\n                                                             curr_node_id,\n                                                             curr_node,\n                                                             otu_group,\n                                                             label_key,\n                                                             leaf_labels,\n                                                             unlabeled_counter,\n                                                             needs_quotes_pattern)\n                if with_edge_lengths:\n                    _write_newick_edge_len(out, curr_edge)\n                going_tipward = False\n            else:\n                te = [(i, e) for i, e in outgoing_edges.items()]\n                te.sort()  # produce a consistent rotation... Necessary?\n                if bracket_ingroup and (ingroup_id == curr_node_id):\n                    out.write('[pre-ingroup-marker]')\n                out.write('(')\n                next_p = te.pop(0)\n                curr_stack.append((curr_edge, curr_node_id, curr_sib_list))\n                curr_edge, curr_sib_list = next_p[1], te\n                curr_node_id = curr_edge['@target']\n        if not going_tipward:\n            next_up_edge_id = None\n            while True:\n                if curr_sib_list:\n                    out.write(',')\n                    next_up_edge_id, next_up_edge = curr_sib_list.pop(0)\n                    break\n                if curr_stack:\n                    curr_edge, curr_node_id, curr_sib_list = curr_stack.pop(-1)\n                    curr_node = nodes[curr_node_id]\n                    out.write(')')\n                    _write_newick_internal_label(out,\n                                                 curr_node_id,\n                                                 curr_node,\n                                                 otu_group,\n                                                 label_key,\n                                                 needs_quotes_pattern)\n                    if with_edge_lengths:\n                        _write_newick_edge_len(out, curr_edge)\n                    if bracket_ingroup and (ingroup_id == curr_node_id):\n                        out.write('[post-ingroup-marker]')\n                else:\n                    break\n            if next_up_edge_id is None:\n                break\n            curr_edge = next_up_edge\n            curr_node_id = curr_edge['@target']\n            going_tipward = True\n    out.write(';')", "response": "Write a newick file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmodifies nexson and returns it in version 1. 2. 1 with any tree that does not match the ID removed.", "response": "def cull_nonmatching_trees(nexson, tree_id, curr_version=None):\n    \"\"\"Modifies `nexson` and returns it in version 1.2.1\n    with any tree that does not match the ID removed.\n\n    Note that this does not search through the NexSON for\n    every node, edge, tree that was deleted. So the resulting\n    NexSON may have broken references !\n    \"\"\"\n    if curr_version is None:\n        curr_version = detect_nexson_version(nexson)\n    if not _is_by_id_hbf(curr_version):\n        nexson = convert_nexson_format(nexson, BY_ID_HONEY_BADGERFISH)\n\n    nexml_el = get_nexml_el(nexson)\n    tree_groups = nexml_el['treesById']\n    tree_groups_to_del = []\n    for tgi, tree_group in tree_groups.items():\n        tbi = tree_group['treeById']\n        if tree_id in tbi:\n            trees_to_del = [i for i in tbi.keys() if i != tree_id]\n            for tid in trees_to_del:\n                tree_group['^ot:treeElementOrder'].remove(tid)\n                del tbi[tid]\n        else:\n            tree_groups_to_del.append(tgi)\n    for tgid in tree_groups_to_del:\n        nexml_el['^ot:treesElementOrder'].remove(tgid)\n        del tree_groups[tgid]\n    return nexson"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef extract_tree_nexson(nexson, tree_id, curr_version=None):\n    if curr_version is None:\n        curr_version = detect_nexson_version(nexson)\n    if not _is_by_id_hbf(curr_version):\n        nexson = convert_nexson_format(nexson, BY_ID_HONEY_BADGERFISH)\n\n    nexml_el = get_nexml_el(nexson)\n    tree_groups = nexml_el['treesById']\n    tree_obj_otus_group_list = []\n    for tree_group in tree_groups.values():\n        if tree_id:\n            tree_list = [(tree_id, tree_group['treeById'].get(tree_id))]\n        else:\n            tree_list = tree_group['treeById'].items()\n        for tid, tree in tree_list:\n            if tree is not None:\n                otu_groups = nexml_el['otusById']\n                ogi = tree_group['@otus']\n                otu_group = otu_groups[ogi]['otuById']\n                tree_obj_otus_group_list.append((tid, tree, otu_group))\n                if tree_id is not None:\n                    return tree_obj_otus_group_list\n    return tree_obj_otus_group_list", "response": "Extracts the list of tuples for the tree_id specified tree_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef phylesystem_api_url(self, base_url, study_id):\n        p = self._phylesystem_api_params()\n        e = self._phylesystem_api_ext()\n        if self.content == 'study':\n            return '{d}/study/{i}{e}'.format(d=base_url, i=study_id, e=e), p\n        elif self.content == 'tree':\n            if self.content_id is None:\n                return '{d}/study/{i}/tree{e}'.format(d=base_url, i=study_id, e=e), p\n            return '{d}/study/{i}/tree/{t}{e}'.format(d=base_url, i=study_id, t=self.content_id, e=e), p\n        elif self.content == 'subtree':\n            assert self.content_id is not None\n            t, n = self.content_id\n            p['subtree_id'] = n\n            return '{d}/study/{i}/subtree/{t}{e}'.format(d=base_url, i=study_id, t=t, e=e), p\n        elif self.content == 'meta':\n            return '{d}/study/{i}/meta{e}'.format(d=base_url, i=study_id, e=e), p\n        elif self.content == 'otus':\n            if self.content_id is None:\n                return '{d}/study/{i}/otus{e}'.format(d=base_url, i=study_id, e=e), p\n            return '{d}/study/{i}/otus/{t}{e}'.format(d=base_url, i=study_id, t=self.content_id, e=e), p\n        elif self.content == 'otu':\n            if self.content_id is None:\n                return '{d}/study/{i}/otu{e}'.format(d=base_url, i=study_id, e=e), p\n            return '{d}/study/{i}/otu/{t}{e}'.format(d=base_url, i=study_id, t=self.content_id, e=e), p\n        elif self.content == 'otumap':\n            return '{d}/otumap/{i}{e}'.format(d=base_url, i=study_id, e=e), p\n        else:\n            assert False", "response": "Returns URL and param dict for a GET call to phylesystem_api\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine whether the current contents are valid.", "response": "def _is_valid(self, log: Optional[Logger] = None) -> bool:\n        \"\"\" Determine whether the current contents are valid \"\"\"\n        return self._validate(self, log)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate(self, val: list, log: Optional[Logger] = None) -> Tuple[bool, List[str]]:\n        errors = []\n        if not isinstance(val, list):\n            errors.append(f\"{self._variable_name}: {repr(val)} is not an array\")\n        else:\n            for i in range(0, len(val)):\n                v = val[i]\n                if not conforms(v, self._type, self._context.NAMESPACE):\n                    errors.append(f\"{self._variable_name} element {i}: {v} is not a {self._type.__name__}\")\n\n            if len(val) < self._min:\n                errors.append(\n                    f\"{self._variable_name}: at least {self._min} value{'s' if self._min > 1 else ''} required - \"\n                    f\"element has {len(val) if len(val) else 'none'}\")\n            if self._max is not None and len(val) > self._max:\n                errors.append(\n                    f\"{self._variable_name}: no more than {self._max} values permitted - element has {len(val)}\")\n\n        if log:\n            for error in errors:\n                log.log(error)\n        return not bool(errors), errors", "response": "Determine whether the value is a valid instance of this array."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef convert(self, obj):\n        if self.pristine_if_invalid:\n            raise NotImplementedError('pristine_if_invalid option is not supported yet')\n\n        nex = get_nexml_el(obj)\n        assert nex\n        self._recursive_convert_dict(nex)\n        nex['@nexml2json'] = str(BADGER_FISH_NEXSON_VERSION)\n        self._single_el_list_to_dicts(nex, 'otus')\n        self._single_el_list_to_dicts(nex, 'trees')\n        #\n        # otu and tree are always arrays in phylografter\n        emulate_phylografter_pluralization = True\n        if not emulate_phylografter_pluralization:\n            self._single_el_list_to_dicts(nex, 'otus', 'otu')\n            self._single_el_list_to_dicts(nex, 'trees', 'tree')\n            self._single_el_list_to_dicts(nex, 'trees', 'tree', 'node')\n            self._single_el_list_to_dicts(nex, 'trees', 'tree', 'edge')\n        return obj", "response": "Takes a dict corresponding to the honeybadgerfish JSON blob of the 1. 0. type and\n        converts it to BY_ID_HONEY_BADGERFISH version. The object is returned in place\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a dict corresponding to the honeybadgerfish JSON blob of the 1. 2. type and converts it to DIRECT_HONEY_BADGERFISH version. The object is returned.", "response": "def convert(self, obj):\n        \"\"\"Takes a dict corresponding to the honeybadgerfish JSON blob of the 1.2.* type and\n        converts it to DIRECT_HONEY_BADGERFISH version. The object is modified in place\n        and returned.\n        \"\"\"\n        if self.pristine_if_invalid:\n            raise NotImplementedError('pristine_if_invalid option is not supported yet')\n\n        nex = get_nexml_el(obj)\n        assert nex\n        # Create the new objects as locals. This section should not\n        #   mutate obj, so that if there is an exception the object\n        #   is unchanged on the error exit\n        otusById = nex['otusById']\n        otusElementOrder = nex['^ot:otusElementOrder']\n        otus = self.convert_otus(otusById, otusElementOrder)\n        nex['otus'] = otus\n        treesById = nex['treesById']\n        treesElementOrder = nex['^ot:treesElementOrder']\n        trees = self.convert_trees(treesById, treesElementOrder)\n        # add the locals to the object\n        nex['trees'] = trees\n        nex['@nexml2json'] = str(DIRECT_HONEY_BADGERFISH)\n        # Make the struct leaner\n        if self.remove_old_structs:\n            del nex['otusById']\n            del nex['^ot:otusElementOrder']\n            del nex['treesById']\n            del nex['^ot:treesElementOrder']\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsplits a table into two lists of integers.", "response": "def splitbins(t, trace=0):\n    \"\"\"t, trace=0 -> (t1, t2, shift).  Split a table to save space.\n\n    t is a sequence of ints.  This function can be useful to save space if\n    many of the ints are the same.  t1 and t2 are lists of ints, and shift\n    is an int, chosen to minimize the combined size of t1 and t2 (in C\n    code), and where for each i in range(len(t)),\n        t[i] == t2[(t1[i >> shift] << shift) + (i & mask)]\n    where mask is a bitmask isolating the last \"shift\" bits.\n\n    If optional arg trace is non-zero (default zero), progress info\n    is printed to sys.stderr.  The higher the value, the more info\n    you'll get.\n    \"\"\"\n\n    if trace:\n        def dump(t1, t2, shift, bytes):\n            print(\"%d+%d bins at shift %d; %d bytes\" % (\n                len(t1), len(t2), shift, bytes), file=sys.stderr)\n        print(\"Size of original table:\", len(t)*getsize(t), \\\n                            \"bytes\", file=sys.stderr)\n    n = len(t)-1    # last valid index\n    maxshift = 0    # the most we can shift n and still have something left\n    if n > 0:\n        while n >> 1:\n            n >>= 1\n            maxshift += 1\n    del n\n    bytes = sys.maxsize  # smallest total size so far\n    t = tuple(t)    # so slices can be dict keys\n    for shift in range(maxshift + 1):\n        t1 = []\n        t2 = []\n        size = 2**shift\n        bincache = {}\n        for i in range(0, len(t), size):\n            bin = t[i:i+size]\n            index = bincache.get(bin)\n            if index is None:\n                index = len(t2)\n                bincache[bin] = index\n                t2.extend(bin)\n            t1.append(index >> shift)\n        # determine memory size\n        b = len(t1)*getsize(t1) + len(t2)*getsize(t2)\n        if trace > 1:\n            dump(t1, t2, shift, b)\n        if b < bytes:\n            best = t1, t2, shift\n            bytes = b\n    t1, t2, shift = best\n    if trace:\n        print(\"Best:\", end=' ', file=sys.stderr)\n        dump(t1, t2, shift, bytes)\n    if __debug__:\n        # exhaustively verify that the decomposition is correct\n        mask = ~((~0) << shift) # i.e., low-bit mask of shift bits\n        for i in range(len(t)):\n            assert t[i] == t2[(t1[i >> shift] << shift) + (i & mask)]\n    return best"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over OTUs in a Nexson proxy.", "response": "def otu_iter_nexson_proxy(nexson_proxy, otu_sort=None):\n    \"\"\"otu_sort can be None (not sorted or stable), True (sorted by ID lexigraphically)\n    or a key function for a sort function on list of otuIDs\n\n    Note that if there are multiple OTU groups, the NexSON specifies the order of sorting\n        of the groups (so the sort argument here only refers to the sorting of OTUs within\n        a group)\n    \"\"\"\n    nexml_el = nexson_proxy._nexml_el\n    og_order = nexml_el['^ot:otusElementOrder']\n    ogd = nexml_el['otusById']\n    for og_id in og_order:\n        og = ogd[og_id]\n        if otu_sort is None:\n            for k, v in og:\n                yield nexson_proxy._create_otu_proxy(k, v)\n        else:\n            key_list = list(og.keys())\n            if otu_sort is True:\n                key_list.sort()\n            else:\n                key_list.sort(key=otu_sort)\n            for k in key_list:\n                v = og[k]\n                yield nexson_proxy._create_otu_proxy(k, v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\niterates over the NexsonTreeProxy objects in order determined by the nexson blob", "response": "def tree_iter_nexson_proxy(nexson_proxy):\n    \"\"\"Iterates over NexsonTreeProxy objects in order determined by the nexson blob\"\"\"\n    nexml_el = nexson_proxy._nexml_el\n    tg_order = nexml_el['^ot:treesElementOrder']\n    tgd = nexml_el['treesById']\n    for tg_id in tg_order:\n        tg = tgd[tg_id]\n        tree_order = tg['^ot:treeElementOrder']\n        tbid = tg['treeById']\n        otus = tg['@otus']\n        for k in tree_order:\n            v = tbid[k]\n            yield nexson_proxy._create_tree_proxy(tree_id=k, tree=v, otus=otus)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nexson_tree_preorder_iter(tree_proxy, node_id=None, node=None, edge_id=None, edge=None):\n    tree = tree_proxy._nexson_tree\n    ebsid = tree['edgeBySourceId']\n    nbid = tree['nodeById']\n    if edge_id is not None:\n        assert edge is not None\n        if node_id is None:\n            node_id = edge['@target']\n        else:\n            assert node_id == edge['@target']\n        if node is None:\n            node = nbid[node_id]\n        else:\n            assert node == nbid[node_id]\n        yield tree_proxy._create_node_proxy_from_edge(edge_id, edge, node_id=node_id, node=node)\n        root_id = node_id\n    elif node_id is not None:\n        if node is None:\n            node = nbid[node_id]\n        else:\n            assert node == nbid[node_id]\n        yield tree_proxy._create_node_proxy_from_edge(None, None, node_id=node_id, node=node)\n        root_id = node_id\n    else:\n        root_id = tree['^ot:rootNodeId']\n        root = nbid[root_id]\n        yield tree_proxy._create_node_proxy_from_edge(None, None, node_id=root_id, node=root)\n    stack = []\n    new_stack = [(i['@target'], edge_id, i) for edge_id, i in ebsid[root_id].items()]\n    stack.extend(new_stack)\n    while stack:\n        target_node_id, edge_id, edge = stack.pop()\n        node = nbid[target_node_id]\n        yield tree_proxy._create_node_proxy_from_edge(edge_id=edge_id, edge=edge, node_id=target_node_id)\n        daughter_edges = ebsid.get(target_node_id)\n        if daughter_edges is not None:\n            new_stack = [(i['@target'], edge_id, i) for edge_id, i in daughter_edges.items()]\n            stack.extend(new_stack)", "response": "Takes a Nexson tree in By ID NexSON ( v1. 2 ) yields a NexsonNodeProxy object where the edge is connected to the node and the edge is connected to the parent."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a reference to the dict of target node id to edge", "response": "def edge_by_target(self):\n        \"\"\"Returns a reference to the dict of target node id to (edge_id, edge)\"\"\"\n        if self._edge_by_target is None:\n            self._edge_by_target = reverse_edge_by_source_dict(self._edge_by_source_id,\n                                                               self._nexson_tree['^ot:rootNodeId'])\n        return self._edge_by_target"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets status from APC NIS and print output on stdout.", "response": "def main():\n    \"\"\"Get status from APC NIS and print output on stdout.\"\"\"\n    # No need to use \"proper\" names on such simple code.\n    # pylint: disable=invalid-name\n    p = argparse.ArgumentParser()\n    p.add_argument(\"--host\", default=\"localhost\")\n    p.add_argument(\"--port\", type=int, default=3551)\n    p.add_argument(\"--strip-units\", action=\"store_true\", default=False)\n    args = p.parse_args()\n    status.print_status(\n        status.get(args.host, args.port),\n        strip_units=args.strip_units\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the server with the given host and port.", "response": "def run(self, host, port, **options):\n        \"\"\"For debugging purposes, you can run this as a standalone server.\n\n        .. WARNING:: **Security vulnerability**\n\n            This uses :class:`DebuggedJsonRpcApplication` to assist debugging. If you want to use\n            this in production, you should run :class:`Server` as a standard WSGI app with\n            `uWSGI <https://uwsgi-docs.readthedocs.org/en/latest/>`_ or another similar WSGI server.\n\n        .. versionadded:: 0.1.0\n        \"\"\"\n        self.registry.debug = True\n        debugged = DebuggedJsonRpcApplication(self, evalex=True)\n        run_simple(host, port, debugged, use_reloader=True, **options)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _try_trigger_before_first_request_funcs(self):  # pylint: disable=C0103\n        if self._after_first_request_handled:\n            return\n        else:\n            with self._before_first_request_lock:\n                if self._after_first_request_handled:\n                    return\n                for func in self._before_first_request_funcs:\n                    func()\n                self._after_first_request_handled = True", "response": "Runs each function from self. before_first_request_funcs once and only once."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns the application and preserve the traceback frames.", "response": "def debug_application(self, environ, start_response):\n        \"\"\"Run the application and preserve the traceback frames.\n\n        :param environ: The environment which is passed into the wsgi application\n        :type environ: dict[str, object]\n        :param start_response: The start_response function of the wsgi application\n        :type start_response: (str, list[(str, str)]) -> None\n        :rtype: generator[str]\n\n        .. versionadded:: 0.1.0\n        \"\"\"\n        adapter = self._debug_map.bind_to_environ(environ)\n        if adapter.test():\n            _, args = adapter.match()\n            return self.handle_debug(environ, start_response, args[\"traceback_id\"])\n        else:\n            return super(DebuggedJsonRpcApplication, self).debug_application(environ,\n                                                                             start_response)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle the debug endpoint for inspecting previous errors.", "response": "def handle_debug(self, environ, start_response, traceback_id):\n        \"\"\"Handles the debug endpoint for inspecting previous errors.\n\n        :param environ: The environment which is passed into the wsgi application\n        :type environ: dict[str, object]\n        :param start_response: The start_response function of the wsgi application\n        :type start_response: (str, list[(str, str)]) -> NoneType\n        :param traceback_id: The id of the traceback to inspect\n        :type traceback_id: int\n\n        .. versionadded:: 0.1.0\n        \"\"\"\n        if traceback_id not in self.app.registry.tracebacks:\n            abort(404)\n        self._copy_over_traceback(traceback_id)\n        traceback = self.tracebacks[traceback_id]\n        rendered = traceback.render_full(evalex=self.evalex, secret=self.secret)\n        response = Response(rendered.encode('utf-8', 'replace'),\n                            headers=[('Content-Type', 'text/html; charset=utf-8'),\n                                     ('X-XSS-Protection', '0')])\n        return response(environ, start_response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ot_study_info_from_treebase_nexml(src=None,\n                                          nexml_content=None,\n                                          encoding=u'utf8',\n                                          nexson_syntax_version=DEFAULT_NEXSON_VERSION,\n                                          merge_blocks=True,\n                                          sort_arbitrary=False):\n    \"\"\"Normalize treebase-specific metadata into the locations where\n    open tree of life software that expects it.\n\n    See get_ot_study_info_from_nexml for the explanation of the src,\n    nexml_content, encoding, and nexson_syntax_version arguments\n    If merge_blocks is True then peyotl.manip.merge_otus_and_trees\n\n    Actions to \"normalize\" TreeBase objects to ot Nexson\n        1. the meta id for any meta item that has only a value and an id\n        2. throw away rdfs:isDefinedBy\n        3. otu @label -> otu ^ot:originalLabel\n        4. ^tb:indentifier.taxon, ^tb:indentifier.taxonVariant and some skos:closeMatch\n            fields to ^ot:taxonLink\n        5. remove \"@xml:base\"\n        6. coerce edge lengths to native types\n    \"\"\"\n    # pylint: disable=R0915\n    raw = get_ot_study_info_from_nexml(src=src,\n                                       nexml_content=nexml_content,\n                                       encoding=encoding,\n                                       nexson_syntax_version=BY_ID_HONEY_BADGERFISH)\n    nexml = raw['nexml']\n    SKOS_ALT_LABEL = '^skos:altLabel'\n    SKOS_CLOSE_MATCH = '^skos:closeMatch'\n    strippable_pre = {\n        'http://www.ubio.org/authority/metadata.php?lsid=urn:lsid:ubio.org:namebank:': '@ubio',\n        'http://purl.uniprot.org/taxonomy/': '@uniprot',\n    }\n    moveable2taxon_link = {\"^tb:identifier.taxon\": '@tb:identifier.taxon',\n                           \"^tb:identifier.taxonVariant\": '@tb:identifier.taxonVariant', }\n    to_del = ['^rdfs:isDefinedBy', '@xml:base']\n    for tag in to_del:\n        if tag in nexml:\n            del nexml[tag]\n    _simplify_all_meta_by_id_del(nexml)\n    _otu2label = {}\n    prefix_map = {}\n    # compose dataDeposit\n    nexid = nexml['@id']\n    tb_url = 'http://purl.org/phylo/treebase/phylows/study/TB2:' + nexid\n    nexml['^ot:dataDeposit'] = {'@href': tb_url}\n    # compose dataDeposit\n    bd = nexml.get(\"^dcterms:bibliographicCitation\")\n    if bd:\n        nexml['^ot:studyPublicationReference'] = bd\n    doi = nexml.get('^prism:doi')\n    if doi:\n        doi = doi2url(doi)\n        nexml['^ot:studyPublication'] = {'@href': doi}\n    year = nexml.get('^prism:publicationDate')\n    if year:\n        try:\n            nexml['^ot:studyYear'] = int(year)\n        except:\n            pass\n    #\n    for otus in nexml['otusById'].values():\n        for tag in to_del:\n            if tag in otus:\n                del otus[tag]\n        _simplify_all_meta_by_id_del(otus)\n        for oid, otu in otus['otuById'].items():\n            for tag in to_del:\n                if tag in otu:\n                    del otu[tag]\n            _simplify_all_meta_by_id_del(otu)\n            label = otu['@label']\n            _otu2label[oid] = label\n            otu['^ot:originalLabel'] = label\n            del otu['@label']\n            al = otu.get(SKOS_ALT_LABEL)\n            if al is not None:\n                if otu.get('^ot:altLabel') is None:\n                    otu['^ot:altLabel'] = al\n                del otu[SKOS_ALT_LABEL]\n            tl = {}\n            scm = otu.get(SKOS_CLOSE_MATCH)\n            # _LOG.debug('scm = ' + str(scm))\n            if scm:\n                if isinstance(scm, dict):\n                    h = scm.get('@href')\n                    if h:\n                        try:\n                            for p, t in strippable_pre.items():\n                                if h.startswith(p):\n                                    ident = h[len(p):]\n                                    tl[t] = ident\n                                    del otu[SKOS_CLOSE_MATCH]\n                                    prefix_map[t] = p\n                        except:\n                            pass\n                else:\n                    nm = []\n                    try:\n                        for el in scm:\n                            h = el.get('@href')\n                            if h:\n                                found = False\n                                for p, t in strippable_pre.items():\n                                    if h.startswith(p):\n                                        ident = h[len(p):]\n                                        tl[t] = ident\n                                        found = True\n                                        prefix_map[t] = p\n                                        break\n                                if not found:\n                                    nm.append(el)\n                    except:\n                        pass\n                    if len(nm) < len(scm):\n                        if len(nm) > 1:\n                            otu[SKOS_CLOSE_MATCH] = nm\n                        elif len(nm) == 1:\n                            otu[SKOS_CLOSE_MATCH] = nm[0]\n                        else:\n                            del otu[SKOS_CLOSE_MATCH]\n            # _LOG.debug('tl =' + str(tl))\n            for k, t in moveable2taxon_link.items():\n                al = otu.get(k)\n                if al:\n                    tl[t] = al\n                    del otu[k]\n            if tl:\n                otu['^ot:taxonLink'] = tl\n    for trees in nexml['treesById'].values():\n        for tag in to_del:\n            if tag in trees:\n                del trees[tag]\n        _simplify_all_meta_by_id_del(trees)\n        for tree in trees['treeById'].values():\n            for tag in to_del:\n                if tag in tree:\n                    del tree[tag]\n            _simplify_all_meta_by_id_del(tree)\n            tt = tree.get('@xsi:type', 'nex:FloatTree')\n            if tt.lower() == 'nex:inttree':\n                e_len_coerce = int\n            else:\n                e_len_coerce = float\n            for edge_d in tree['edgeBySourceId'].values():\n                for edge in edge_d.values():\n                    try:\n                        x = e_len_coerce(edge['@length'])\n                        edge['@length'] = x\n                    except:\n                        pass\n            for node in tree['nodeById'].values():\n                nl = node.get('@label')\n                if nl:\n                    no = node.get('@otu')\n                    if no and _otu2label[no] == nl:\n                        del node['@label']\n\n    if prefix_map:\n        nexml['^ot:taxonLinkPrefixes'] = prefix_map\n    if merge_blocks:\n        from peyotl.manip import merge_otus_and_trees\n        merge_otus_and_trees(raw)\n    if nexson_syntax_version != BY_ID_HONEY_BADGERFISH:\n        convert_nexson_format(raw,\n                              nexson_syntax_version,\n                              current_format=BY_ID_HONEY_BADGERFISH,\n                              sort_arbitrary=sort_arbitrary)\n    elif sort_arbitrary:\n        sort_arbitrarily_ordered_nexson(raw)\n    return raw", "response": "This function takes a source file and returns a treebase - specific metadata as a dictionary of RDFs and Nexsons."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef init_app(self, app):\n        self.init_config(app)\n        app.cli.add_command(cmd)\n        app.extensions['invenio-communities'] = self\n        # Register the jinja do extension\n        app.jinja_env.add_extension('jinja2.ext.do')\n        self.register_signals(app)", "response": "Initialize the Flask application."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a command line parser for JSG.", "response": "def genargs() -> ArgumentParser:\n    \"\"\"\n    Create a command line parser\n\n    :return: parser\n    \"\"\"\n    parser = ArgumentParser()\n    parser.add_argument(\"spec\", help=\"JSG specification - can be file name, URI or string\")\n    parser.add_argument(\"-o\", \"--outfile\", help=\"Output python file - if omitted, python is not saved\")\n    parser.add_argument(\"-p\", \"--print\", help=\"Print python file to stdout\")\n    parser.add_argument(\"-id\", \"--inputdir\", help=\"Input directory with JSON files\")\n    parser.add_argument(\"-i\", \"--json\", help=\"URL, file name or json text\", nargs='*')\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a URL or file name to a string", "response": "def _to_string(inp: str) -> str:\n        \"\"\" Convert a URL or file name to a string \"\"\"\n        if '://' in inp:\n            req = requests.get(inp)\n            if not req.ok:\n                raise ValueError(f\"Unable to read {inp}\")\n            return req.text\n        else:\n            with open(inp) as infile:\n                return infile.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conforms(self, json: str, name: str = \"\", verbose: bool=False) -> ValidationResult:\n        json = self._to_string(json) if not self.is_json(json) else json\n        try:\n            self.json_obj = loads(json, self.module)\n        except ValueError as v:\n            return ValidationResult(False, str(v), name, None)\n        logfile = StringIO()\n        logger = Logger(cast(TextIO, logfile))      # cast because of bug in ide\n        if not is_valid(self.json_obj, logger):\n            return ValidationResult(False, logfile.getvalue().strip('\\n'), name, None)\n        return ValidationResult(True, \"\", name, type(self.json_obj).__name__)", "response": "Determine whether the given JSON string conforms with the JSG specification."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites the given blob to the next free entry in the pool.", "response": "def _write_to_next_free(tag, blob):\n    \"\"\"#WARNING not thread safe just a easy of debugging routine!\"\"\"\n    ind = 0\n    pref = '/tmp/peyotl-' + tag + str(ind)\n    while os.path.exists(pref):\n        ind += 1\n        pref = '/tmp/peyotl-' + tag + str(ind)\n    write_as_json(blob, pref)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun the nexson validator and returns a converted 4 object.", "response": "def validate_and_convert_nexson(nexson, output_version, allow_invalid, **kwargs):\n    \"\"\"Runs the nexson validator and returns a converted 4 object:\n        nexson, annotation, validation_log, nexson_adaptor\n\n    `nexson` is the nexson dict.\n    `output_version` is the version of nexson syntax to be used after validation.\n    if `allow_invalid` is False, and the nexson validation has errors, then\n        a GitWorkflowError will be generated before conversion.\n    \"\"\"\n    try:\n        if TRACE_FILES:\n            _write_to_next_free('input', nexson)\n        annotation, validation_log, nexson_adaptor = ot_validate(nexson, **kwargs)\n        if TRACE_FILES:\n            _write_to_next_free('annotation', annotation)\n    except:\n        msg = 'exception in ot_validate: ' + traceback.format_exc()\n        raise GitWorkflowError(msg)\n    if (not allow_invalid) and validation_log.has_error():\n        raise GitWorkflowError('ot_validation failed: ' + json.dumps(annotation))\n    nexson = convert_nexson_format(nexson, output_version)\n    if TRACE_FILES:\n        _write_to_next_free('converted', nexson)\n    return nexson, annotation, validation_log, nexson_adaptor"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_from_master(git_action, study_id, auth_info, parent_sha):\n    return _merge_from_master(git_action,\n                              doc_id=study_id,\n                              auth_info=auth_info,\n                              parent_sha=parent_sha,\n                              doctype_display_name=\"study\")", "response": "merge from master into WIP"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming ancestor opening operations synchronize revocation registry to tails tree content.", "response": "async def open(self) -> 'Issuer':\n        \"\"\"\n        Explicit entry. Perform ancestor opening operations,\n        then synchronize revocation registry to tails tree content.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('Issuer.open >>>')\n\n        await super().open()\n        for path_rr_id in Tails.links(self._dir_tails, self.did):\n            await self._sync_revoc(basename(path_rr_id))\n\n        LOGGER.debug('Issuer.open <<<')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _create_rev_reg(self, rr_id: str, rr_size: int = None) -> None:\n\n        LOGGER.debug('Issuer._create_rev_reg >>> rr_id: %s, rr_size: %s', rr_id, rr_size)\n\n        rr_size = rr_size or 256\n        (cd_id, tag) = rev_reg_id2cred_def_id__tag(rr_id)\n\n        LOGGER.info('Creating revocation registry (capacity %s) for rev reg id %s', rr_size, rr_id)\n        tails_writer_handle = await blob_storage.open_writer(\n            'default',\n            json.dumps({\n                'base_dir': Tails.dir(self._dir_tails, rr_id),\n                'uri_pattern': ''\n            }))\n        apriori = Tails.unlinked(self._dir_tails)\n        (rr_id, rrd_json, rre_json) = await anoncreds.issuer_create_and_store_revoc_reg(\n            self.wallet.handle,\n            self.did,\n            'CL_ACCUM',\n            tag,\n            cd_id,\n            json.dumps({\n                'max_cred_num': rr_size,\n                'issuance_type': 'ISSUANCE_ON_DEMAND'\n            }),\n            tails_writer_handle)\n        delta = Tails.unlinked(self._dir_tails) - apriori\n        if len(delta) != 1:\n            LOGGER.debug(\n                'Issuer._create_rev_reg: <!< Could not create tails file for rev reg id: %s', rr_id)\n            raise CorruptTails('Could not create tails file for rev reg id {}'.format(rr_id))\n        tails_hash = basename(delta.pop())\n        Tails.associate(self._dir_tails, rr_id, tails_hash)\n\n        with REVO_CACHE.lock:\n            rrd_req_json = await ledger.build_revoc_reg_def_request(self.did, rrd_json)\n            await self._sign_submit(rrd_req_json)\n            await self._get_rev_reg_def(rr_id)  # add to cache en passant\n\n        rre_req_json = await ledger.build_revoc_reg_entry_request(self.did, rr_id, 'CL_ACCUM', rre_json)\n        await self._sign_submit(rre_req_json)\n\n        LOGGER.debug('Issuer._create_rev_reg <<<')", "response": "Create revocation registry and tails file for input revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _sync_revoc(self, rr_id: str, rr_size: int = None) -> None:\n\n        LOGGER.debug('Issuer._sync_revoc >>> rr_id: %s, rr_size: %s', rr_id, rr_size)\n\n        (cd_id, tag) = rev_reg_id2cred_def_id__tag(rr_id)\n\n        try:\n            await self.get_cred_def(cd_id)\n        except AbsentCredDef:\n            LOGGER.debug(\n                'Issuer._sync_revoc: <!< tails tree %s may be for another ledger; no cred def found on %s',\n                self._dir_tails,\n                cd_id)\n            raise AbsentCredDef('Tails tree {} may be for another ledger; no cred def found on {}'.format(\n                self._dir_tails,\n                cd_id))\n\n        with REVO_CACHE.lock:\n            revo_cache_entry = REVO_CACHE.get(rr_id, None)\n            tails = None if revo_cache_entry is None else revo_cache_entry.tails\n            if tails is None:  #  it's a new revocation registry, or not yet set in cache\n                try:\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()\n                except AbsentTails:\n                    await self._create_rev_reg(rr_id, rr_size)   # it's a new revocation registry\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()  # symlink should exist now\n\n                if revo_cache_entry is None:\n                    REVO_CACHE[rr_id] = RevoCacheEntry(None, tails)\n                else:\n                    REVO_CACHE[rr_id].tails = tails\n\n        LOGGER.debug('Issuer._sync_revoc <<<')", "response": "Synchronize revocation registry with tails file reader."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef path_tails(self, rr_id: str) -> str:\n\n        return Tails.linked(self._dir_tails, rr_id)", "response": "Return path to tails file for input revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend credential definition to ledger.", "response": "async def send_cred_def(self, s_id: str, revocation: bool = True, rr_size: int = None) -> str:\n        \"\"\"\n        Create a credential definition as Issuer, store it in its wallet, and send it to the ledger.\n\n        Raise CorruptWallet for wallet not pertaining to current ledger, BadLedgerTxn on failure\n        to send credential definition to ledger if need be, or IndyError for any other failure\n        to create and store credential definition in wallet.\n\n        :param s_id: schema identifier\n        :param revocation: whether to support revocation for cred def\n        :param rr_size: size of initial revocation registry (default as per _create_rev_reg()), if revocation supported\n        :return: json credential definition as it appears on ledger\n        \"\"\"\n\n        LOGGER.debug('Issuer.send_cred_def >>> s_id: %s, revocation: %s, rr_size: %s', s_id, revocation, rr_size)\n\n        rv_json = json.dumps({})\n        schema_json = await self.get_schema(schema_key(s_id))\n        schema = json.loads(schema_json)\n\n        cd_id = cred_def_id(self.did, schema['seqNo'])\n        private_key_ok = True\n        with CRED_DEF_CACHE.lock:\n            try:\n                rv_json = await self.get_cred_def(cd_id)\n                LOGGER.info(\n                    'Cred def on schema %s version %s already exists on ledger; Issuer %s not sending another',\n                    schema['name'],\n                    schema['version'],\n                    self.wallet.name)\n            except AbsentCredDef:\n                pass  # OK - about to create, store, and send it\n\n            try:\n                (_, cred_def_json) = await anoncreds.issuer_create_and_store_credential_def(\n                    self.wallet.handle,\n                    self.did,  # issuer DID\n                    schema_json,\n                    CD_ID_TAG,  # expect only one cred def per schema and issuer\n                    'CL',\n                    json.dumps({'support_revocation': revocation}))\n                if json.loads(rv_json):\n                    private_key_ok = False\n                    LOGGER.warning(\n                        'New cred def on %s in wallet shadows existing one on ledger: private key not usable', cd_id)\n                        # carry on though, this agent may have other roles so public key may be good enough\n            except IndyError as x_indy:\n                if x_indy.error_code == ErrorCode.AnoncredsCredDefAlreadyExistsError:\n                    if json.loads(rv_json):\n                        LOGGER.info(\n                            'Issuer wallet %s reusing existing cred def on schema %s version %s',\n                            self.wallet.name,\n                            schema['name'],\n                            schema['version'])\n                    else:\n                        LOGGER.debug('Issuer.send_cred_def: <!< corrupt wallet %s', self.wallet.name)\n                        raise CorruptWallet(\n                            'Corrupt Issuer wallet {} has cred def on schema {} version {} not on ledger'.format(\n                                self.wallet.name,\n                                schema['name'],\n                                schema['version']))\n                else:\n                    LOGGER.debug(\n                        'Issuer.send_cred_def: <!< cannot store cred def in wallet %s: indy error code %s',\n                        self.wallet.name,\n                        x_indy.error_code)\n                    raise\n\n            if not json.loads(rv_json):  # checking the ledger returned no cred def: send it\n                req_json = await ledger.build_cred_def_request(self.did, cred_def_json)\n                await self._sign_submit(req_json)\n                rv_json = await self.get_cred_def(cd_id)  # pick up from ledger and parse; add to cache\n\n                if revocation:\n                    await self._sync_revoc(rev_reg_id(cd_id, 0), rr_size)  # create new rev reg, tails file for tag 0\n\n        if revocation and private_key_ok:\n            for tag in [str(t) for t in range(int(Tails.next_tag(self._dir_tails, cd_id)[0]))]:  # '0' to str(next-1)\n                await self._sync_revoc(rev_reg_id(cd_id, tag), rr_size if tag == 0 else None)\n\n        dir_cred_def = join(self._dir_tails, cd_id)\n        if not isdir(dir_cred_def):  # make sure a directory exists for box id collection when required, revo or not\n            makedirs(dir_cred_def, exist_ok=True)\n\n        LOGGER.debug('Issuer.send_cred_def <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create_cred_offer(self, schema_seq_no: int) -> str:\n\n        LOGGER.debug('Issuer.create_cred_offer >>> schema_seq_no: %s', schema_seq_no)\n\n        rv = None\n        cd_id = cred_def_id(self.did, schema_seq_no)\n        try:\n            rv = await anoncreds.issuer_create_credential_offer(self.wallet.handle, cd_id)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.WalletNotFoundError:\n                LOGGER.debug(\n                    'Issuer.create_cred_offer: <!< did not issue cred definition from wallet %s',\n                    self.wallet.name)\n                raise CorruptWallet(\n                    'Cannot create cred offer: did not issue cred definition from wallet {}'.format(self.wallet.name))\n            else:\n                LOGGER.debug(\n                    'Issuer.create_cred_offer: <!<  cannot create cred offer, indy error code %s',\n                    x_indy.error_code)\n                raise\n\n        LOGGER.debug('Issuer.create_cred_offer <<< %s', rv)\n        return rv", "response": "Create credential offer for given schema sequence number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate credential json and return it.", "response": "async def create_cred(\n            self,\n            cred_offer_json,\n            cred_req_json: str,\n            cred_attrs: dict,\n            rr_size: int = None) -> (str, str, int):\n        \"\"\"\n        Create credential as Issuer out of credential request and dict of key:value (raw, unencoded)\n        entries for attributes.\n\n        Return credential json, and if cred def supports revocation, credential revocation identifier\n        and revocation registry delta ledger timestamp (epoch seconds).\n\n        If the credential definition supports revocation, and the current revocation registry is full,\n        the processing creates a new revocation registry en passant. Depending on the revocation\n        registry size (by default starting at 256 and doubling iteratively through 4096), this\n        operation may delay credential creation by several seconds.\n\n        :param cred_offer_json: credential offer json as created by Issuer\n        :param cred_req_json: credential request json as created by HolderProver\n        :param cred_attrs: dict mapping each attribute to its raw value (the operation encodes it); e.g.,\n\n        ::\n\n            {\n                'favourite_drink': 'martini',\n                'height': 180,\n                'last_visit_date': '2017-12-31',\n                'weaknesses': None\n            }\n\n        :param rr_size: size of new revocation registry (default as per _create_rev_reg()) if necessary\n        :return: newly issued credential json; credential revocation identifier (if cred def supports\n            revocation, None otherwise), and ledger timestamp (if cred def supports revocation, None otherwise)\n        \"\"\"\n\n        LOGGER.debug(\n            'Issuer.create_cred >>> cred_offer_json: %s, cred_req_json: %s, cred_attrs: %s, rr_size: %s',\n            cred_offer_json,\n            cred_req_json,\n            cred_attrs,\n            rr_size)\n\n        cd_id = json.loads(cred_offer_json)['cred_def_id']\n        cred_def = json.loads(await self.get_cred_def(cd_id))  # ensure cred def is in cache\n\n        if 'revocation' in cred_def['value']:\n            with REVO_CACHE.lock:\n                rr_id = Tails.current_rev_reg_id(self._dir_tails, cd_id)\n                tails = REVO_CACHE[rr_id].tails\n                assert tails  # at (re)start, at cred def, Issuer sync_revoc() sets this index in revocation cache\n\n                try:\n                    (cred_json, cred_revoc_id, rr_delta_json) = await anoncreds.issuer_create_credential(\n                        self.wallet.handle,\n                        cred_offer_json,\n                        cred_req_json,\n                        json.dumps({k: cred_attr_value(cred_attrs[k]) for k in cred_attrs}),\n                        tails.rr_id,\n                        tails.reader_handle)\n                    # do not create rr delta frame and append to cached delta frames list: timestamp could lag or skew\n                    rre_req_json = await ledger.build_revoc_reg_entry_request(\n                        self.did,\n                        tails.rr_id,\n                        'CL_ACCUM',\n                        rr_delta_json)\n                    await self._sign_submit(rre_req_json)\n                    resp_json = await self._sign_submit(rre_req_json)\n                    resp = json.loads(resp_json)\n                    rv = (cred_json, cred_revoc_id, resp['result']['txnMetadata']['txnTime'])\n\n                except IndyError as x_indy:\n                    if x_indy.error_code == ErrorCode.AnoncredsRevocationRegistryFullError:\n                        (tag, rr_size_suggested) = Tails.next_tag(self._dir_tails, cd_id)\n                        rr_id = rev_reg_id(cd_id, tag)\n                        await self._create_rev_reg(rr_id, rr_size or rr_size_suggested)\n                        REVO_CACHE[rr_id].tails = await Tails(self._dir_tails, cd_id).open()\n                        return await self.create_cred(cred_offer_json, cred_req_json, cred_attrs)  # should be ok now\n                    else:\n                        LOGGER.debug(\n                            'Issuer.create_cred: <!<  cannot create cred, indy error code %s',\n                            x_indy.error_code)\n                        raise\n        else:\n            try:\n                (cred_json, _, _) = await anoncreds.issuer_create_credential(\n                    self.wallet.handle,\n                    cred_offer_json,\n                    cred_req_json,\n                    json.dumps({k: cred_attr_value(cred_attrs[k]) for k in cred_attrs}),\n                    None,\n                    None)\n                rv = (cred_json, _, _)\n            except IndyError as x_indy:\n                LOGGER.debug('Issuer.create_cred: <!<  cannot create cred, indy error code %s', x_indy.error_code)\n                raise\n\n        LOGGER.debug('Issuer.create_cred <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def revoke_cred(self, rr_id: str, cr_id) -> int:\n\n        LOGGER.debug('Issuer.revoke_cred >>> rr_id: %s, cr_id: %s', rr_id, cr_id)\n\n        tails_reader_handle = (await Tails(\n            self._dir_tails,\n            *rev_reg_id2cred_def_id__tag(rr_id)).open()).reader_handle\n        try:\n            rrd_json = await anoncreds.issuer_revoke_credential(\n                self.wallet.handle,\n                tails_reader_handle,\n                rr_id,\n                cr_id)\n        except IndyError as x_indy:\n            LOGGER.debug(\n                'Issuer.revoke_cred: <!< Could not revoke revoc reg id %s, cred rev id %s: indy error code %s',\n                rr_id,\n                cr_id,\n                x_indy.error_code)\n            raise BadRevocation(\n                'Could not revoke revoc reg id {}, cred rev id {}: indy error code {}'.format(\n                    rr_id,\n                    cr_id,\n                    x_indy.error_code))\n\n        rre_req_json = await ledger.build_revoc_reg_entry_request(self.did, rr_id, 'CL_ACCUM', rrd_json)\n        resp_json = await self._sign_submit(rre_req_json)\n        resp = json.loads(resp_json)\n\n        rv = resp['result']['txnMetadata']['txnTime']\n        LOGGER.debug('Issuer.revoke_cred <<< %s', rv)\n        return rv", "response": "Revoke a revocation credential for the given revocation registry identifier and credential revocation identifier."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the json object of all unique box identifiers for all credential definitions and credentials issued.", "response": "async def get_box_ids_json(self) -> str:\n        \"\"\"\n        Return json object on lists of all unique box identifiers (schema identifiers,\n        credential definition identifiers, and revocation registry identifiers) for\n        all credential definitions and credentials issued; e.g.,\n\n        ::\n\n        {\n            \"schema_id\": [\n                \"R17v42T4pk...:2:tombstone:1.2\",\n                ...\n            ],\n            \"cred_def_id\": [\n                \"R17v42T4pk...:3:CL:19:0\",\n                ...\n            ]\n            \"rev_reg_id\": [\n                \"R17v42T4pk...:4:R17v42T4pk...:3:CL:19:0:CL_ACCUM:0\",\n                \"R17v42T4pk...:4:R17v42T4pk...:3:CL:19:0:CL_ACCUM:1\",\n                ...\n            ]\n        }\n\n        An issuer must issue a credential definition to include its schema identifier\n        in the returned values; the schema identifier in isolation belongs properly\n        to an Origin, not necessarily to an Issuer.\n\n        The operation may be useful for a Verifier agent going off-line to seed its\n        cache before doing so.\n\n        :return: tuple of sets for schema ids, cred def ids, rev reg ids\n        \"\"\"\n\n        LOGGER.debug('Issuer.get_box_ids_json >>>')\n\n        cd_ids = [d for d in listdir(self._dir_tails)\n            if isdir(join(self._dir_tails, d)) and d.startswith('{}:3:'.format(self.did))]\n        s_ids = []\n        for cd_id in cd_ids:\n            try:\n                s_ids.append(json.loads(await self.get_schema(cred_def_id2seq_no(cd_id)))['id'])\n            except AbsentSchema:\n                LOGGER.error(\n                    'Issuer %s has issued cred def %s but no corresponding schema on ledger',\n                    self.wallet.name,\n                    cd_id)\n        rr_ids = [basename(link) for link in Tails.links(self._dir_tails, self.did)]\n\n        rv = json.dumps({\n            'schema_id': s_ids,\n            'cred_def_id': cd_ids,\n            'rev_reg_id': rr_ids\n        })\n        LOGGER.debug('Issuer.get_box_ids_json <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a value for display as an XML text node.", "response": "def quote_xml(text):\n    \"\"\"Format a value for display as an XML text node.\n\n    Returns:\n        Unicode string (str on Python 3, unicode on Python 2)\n    \"\"\"\n    text = _coerce_unicode(text)\n\n    # If it's a CDATA block, return the text as is.\n    if text.startswith(CDATA_START):\n        return text\n\n    # If it's not a CDATA block, escape the XML and return the character\n    # encoded string.\n    return saxutils.escape(text)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize this instance from a namespace URI and optional prefix and schema location URI.", "response": "def __construct_from_components(self, ns_uri, prefix=None, schema_location=None):\n        \"\"\"Initialize this instance from a namespace URI, and optional\n        prefix and schema location URI.\"\"\"\n\n        assert ns_uri  # other fields are optional\n\n        self.uri = ns_uri\n        self.schema_location = schema_location or None\n        self.prefixes = OrderedSet()\n\n        if prefix:\n            self.prefixes.add(prefix)\n        self.preferred_prefix = prefix or None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that the incoming_prefix is not already mapped to the existing_ni_or_ns_uri. If it is a _NamespaceInfo object and that _NamespaceInfo is not already assigned to the incoming_prefix raises DuplicatePrefixError.", "response": "def __check_prefix_conflict(self, existing_ni_or_ns_uri, incoming_prefix):\n        \"\"\"If existing_ni_or_ns_uri is a _NamespaceInfo object (which must\n        be in this set), then caller wants to map incoming_prefix to that\n        namespace.  This function verifies that the prefix isn't already mapped\n        to a different namespace URI.  If it is, an exception is raised.\n\n        Otherwise, existing_ni_or_ns_uri is treated as a string namespace URI\n        which must not already exist in this set.  Caller wants to map\n        incoming_prefix to that URI.  If incoming_prefix maps to anything\n        already, that represents a prefix conflict and an exception is raised.\n        \"\"\"\n        if incoming_prefix not in self.__prefix_map:\n            return\n\n        # Prefix found in the prefix map. Check that there are no conflicts\n        prefix_check_ni = self.__prefix_map[incoming_prefix]\n\n        if isinstance(existing_ni_or_ns_uri, _NamespaceInfo):\n            existing_ni = existing_ni_or_ns_uri  # makes following code clearer?\n\n            if prefix_check_ni is not existing_ni:\n                # A different obj implies a different namespace URI is\n                # already assigned to the prefix.\n                raise DuplicatePrefixError(incoming_prefix, prefix_check_ni.uri, existing_ni.uri)\n        else:\n            ns_uri = existing_ni_or_ns_uri  # makes following code clearer?\n            assert not self.contains_namespace(ns_uri)  # TODO (bworrell): Should this be a raise?\n            raise DuplicatePrefixError(incoming_prefix, prefix_check_ni.uri, ns_uri)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the namespace the given prefix maps to.", "response": "def namespace_for_prefix(self, prefix):\n        \"\"\"Get the namespace the given prefix maps to.\n\n        Args:\n            prefix (str): The prefix\n\n        Returns:\n            str: The namespace, or None if the prefix isn't mapped to\n                anything in this set.\n        \"\"\"\n        try:\n            ni = self.__lookup_prefix(prefix)\n        except PrefixNotFoundError:\n            return None\n        else:\n            return ni.uri"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the preferred prefix for ns_uri.", "response": "def set_preferred_prefix_for_namespace(self, ns_uri, prefix, add_if_not_exist=False):\n        \"\"\"Sets the preferred prefix for ns_uri.  If add_if_not_exist is True,\n        the prefix is added if it's not already registered.  Otherwise,\n        setting an unknown prefix as preferred is an error.  The default\n        is False.  Setting to None always works, and indicates a preference\n        to use the namespace as a default.  The given namespace must already\n        be in this set.\n\n        Args:\n            ns_uri (str): the namespace URI whose prefix is to be set\n            prefix (str): the preferred prefix to set\n            add_if_not_exist (bool): Whether to add the prefix if it is not\n                already set as a prefix of ``ns_uri``.\n\n        Raises:\n            NamespaceNotFoundError: If namespace ``ns_uri`` isn't in this set.\n            DuplicatePrefixError: If ``prefix`` already maps to a different\n                namespace.\n        \"\"\"\n        ni = self.__lookup_uri(ns_uri)\n\n        if not prefix:\n            ni.preferred_prefix = None\n        elif prefix in ni.prefixes:\n            ni.preferred_prefix = prefix\n        elif add_if_not_exist:\n            self.add_prefix(ns_uri, prefix, set_as_preferred=True)\n        else:\n            raise PrefixNotFoundError(prefix)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef __merge_schema_locations(self, ni, incoming_schemaloc):\n        if ni.schema_location == incoming_schemaloc:  # TODO (bworrell): empty strings?\n            return\n        elif not ni.schema_location:\n            ni.schema_location = incoming_schemaloc or None\n        elif not incoming_schemaloc:\n            return\n        else:\n            raise ConflictingSchemaLocationError(ni.uri, ni.schema_location, incoming_schemaloc)", "response": "Merge incoming_schemaloc into ours\n       ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_namespace_uri(self, ns_uri, prefix=None, schema_location=None):\n        assert ns_uri\n\n        if ns_uri in self.__ns_uri_map:\n            # We have a _NamespaceInfo object for this URI already.  So this\n            # is a merge operation.\n            #\n            # We modify a copy of the real _NamespaceInfo so that we are\n            # exception-safe: if something goes wrong, we don't end up with a\n            # half-changed NamespaceSet.\n            ni = self.__lookup_uri(ns_uri)\n            new_ni = copy.deepcopy(ni)\n\n            # Reconcile prefixes\n            if prefix:\n                self.__check_prefix_conflict(ni, prefix)\n                new_ni.prefixes.add(prefix)\n\n            self.__merge_schema_locations(new_ni, schema_location)\n\n            # At this point, we have a legit new_ni object.  Now we update\n            # the set, ensuring our invariants.  This should replace\n            # all instances of the old ni in this set.\n            for p in new_ni.prefixes:\n                self.__prefix_map[p] = new_ni\n            self.__ns_uri_map[new_ni.uri] = new_ni\n\n        else:\n            # A brand new namespace.  The incoming prefix should not exist at\n            # all in the prefix map.\n            if prefix:\n                self.__check_prefix_conflict(ns_uri, prefix)\n\n            ni = _NamespaceInfo(ns_uri, prefix, schema_location)\n            self.__add_namespaceinfo(ni)", "response": "Adds a new namespace to the set optionally with a prefix and schema location URI."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove the indicated namespace from this set.", "response": "def remove_namespace(self, ns_uri):\n        \"\"\"Removes the indicated namespace from this set.\"\"\"\n        if not self.contains_namespace(ns_uri):\n            return\n\n        ni = self.__ns_uri_map.pop(ns_uri)\n        for prefix in ni.prefixes:\n            del self.__prefix_map[prefix]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_prefix(self, ns_uri, prefix, set_as_preferred=False):\n        assert prefix\n\n        ni = self.__lookup_uri(ns_uri)\n\n        self.__check_prefix_conflict(ni, prefix)\n        ni.prefixes.add(prefix)\n        self.__prefix_map[prefix] = ni\n\n        if set_as_preferred:\n            ni.preferred_prefix = prefix", "response": "Adds a prefix to the set of prefix for the given namespace URI."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_prefixes(self, ns_uri):\n        ni = self.__lookup_uri(ns_uri)\n        return ni.prefixes.copy()", "response": "Gets a copy of the prefix set for the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prefix_iter(self, ns_uri):\n        ni = self.__lookup_uri(ns_uri)\n        return iter(ni.prefixes)", "response": "Gets an iterator over the prefixes for the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving a prefix from this set.", "response": "def remove_prefix(self, prefix):\n        \"\"\"Removes prefix from this set.  This is a no-op if the prefix\n        doesn't exist in it.\n        \"\"\"\n        if prefix not in self.__prefix_map:\n            return\n\n        ni = self.__lookup_prefix(prefix)\n        ni.prefixes.discard(prefix)\n        del self.__prefix_map[prefix]\n\n        # If we removed the preferred prefix, find a new one.\n        if ni.preferred_prefix == prefix:\n            ni.preferred_prefix = next(iter(ni.prefixes), None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the schema location of the given namespace.", "response": "def set_schema_location(self, ns_uri, schema_location, replace=False):\n        \"\"\"Sets the schema location of the given namespace.\n\n        If ``replace`` is ``True``, then any existing schema location is\n        replaced.  Otherwise, if the schema location is already set to a\n        different value, an exception is raised.  If the schema location is set\n        to None, it is effectively erased from this set (this is not considered\n        \"replacement\".)\n\n        Args:\n            ns_uri (str): The namespace whose schema location is to be set\n            schema_location (str): The schema location URI to set, or None\n            replace (bool): Whether to replace any existing schema location\n\n        Raises:\n            NamespaceNotFoundError: If the given namespace isn't in this set.\n            ConflictingSchemaLocationError: If replace is False,\n                schema_location is not None, and the namespace already has a\n                different schema location in this set.\n        \"\"\"\n        ni = self.__lookup_uri(ns_uri)\n\n        if ni.schema_location == schema_location:\n            return\n        elif replace or ni.schema_location is None:\n            ni.schema_location = schema_location\n        elif schema_location is None:\n            ni.schema_location = None  # Not considered \"replacement\".\n        else:\n            raise ConflictingSchemaLocationError(ns_uri, ni.schema_location, schema_location)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xmlns_string(self, ns_uris=None, sort=False,\n                         preferred_prefixes_only=True, delim=\"\\n\"):\n        \"\"\"Generates XML namespace declarations for namespaces in this\n        set.  It must be suitable for use in an actual XML document,\n        so an exception is raised if this can't be done, e.g. if it would\n        have more than one default namespace declaration.\n\n        If ``preferred_prefixes_only`` is ``True`` and a namespace's prefix\n        preference is to be a default namespace, a default declaration will\n        be used if possible.  If that's not possible, a prefix will be\n        chosen (is this a good idea?).  If a default declaration can't be used\n        and no other prefixes are defined, an exception is raised.\n\n        Args:\n            ns_uris (iterable): If non-None, it should be an iterable over\n                namespace URIs.  Only the given namespaces will occur in the\n                returned string.  If None, all namespace are included.\n            sort (bool): If True, the string is constructed from URIs in sorted\n                order.\n            preferred_prefixes_only (bool): Whether to include only the\n                preferred prefix or all of them, for each namespace.\n            delim (str): The delimiter to use between namespace declarations.\n                Should be some kind of whitespace.\n\n        Returns:\n            str: A string in the following format:\n                ``xmlns:foo=\"bar\"<delim>xmlns:foo2=\"bar2\"<delim>...``\n\n        Raises:\n            NamespaceNotFoundError: If ``ns_uris`` is given and contains any\n                URIs not in this set.\n            TooManyDefaultNamespacesError: If too many namespaces didn't have\n                a prefix.  The algorithm is very simple for deciding whose\n                default preference is honored: the first default preference\n                encountered gets to be default.  Any subsequent namespaces\n                without any prefixes will cause this error.\n        \"\"\"\n        if ns_uris is None:\n            ns_uris = self.namespace_uris\n\n        if sort:\n            ns_uris = sorted(ns_uris)\n\n        have_default  = False  # Flag for default xmlns entry.\n        xmlns_entries = []     # Stores all the xmlns:prefix=uri entries.\n\n        for ns_uri in ns_uris:\n            ni = self.__lookup_uri(ns_uri)\n\n            if preferred_prefixes_only:\n                if ni.preferred_prefix is not None:\n                    xmlns = 'xmlns:{0.preferred_prefix}=\"{0.uri}\"'.format(ni)\n                    xmlns_entries.append(xmlns)\n            else:\n                xmlns = 'xmlns:{0}=\"{1.uri}\"'\n                xmlns_entries.extend(xmlns.format(prefix, ni) for prefix in ni.prefixes)\n\n            if ni.preferred_prefix is None:\n                if have_default:\n                    # Already have a default namespace; try to choose a prefix\n                    # for this one from the set of registered prefixes.\n                    if len(ni.prefixes) == 0:\n                        raise TooManyDefaultNamespacesError(ni.uri)\n                    elif preferred_prefixes_only:\n                        prefix = next(iter(ni.prefixes))\n                        xmlns  = 'xmlns:{0}=\"{1.uri}\"'.format(prefix, ni)\n                        xmlns_entries.append(xmlns)\n\n                    # else, we already declared some prefixes for this\n                    # namespace, so don't worry about our inability to use this\n                    # as a default namespace.\n                else:\n                    xmlns = 'xmlns=\"{0.uri}\"'.format(ni)\n                    xmlns_entries.append(xmlns)\n                    have_default = True\n\n        xmlns_str = delim.join(xmlns_entries) + delim\n        return xmlns_str", "response": "Generates a string that can be used to generate XML namespace declarations for the given set of namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing and returns a schemalocation attribute value.", "response": "def get_schemaloc_string(self, ns_uris=None, sort=False, delim=\"\\n\"):\n        \"\"\"Constructs and returns a schemalocation attribute.  If no\n        namespaces in this set have any schema locations defined, returns\n        an empty string.\n\n        Args:\n            ns_uris (iterable): The namespaces to include in the constructed\n                attribute value.  If None, all are included.\n            sort (bool): Whether the sort the namespace URIs.\n            delim (str): The delimiter to use between namespace/schemaloc\n                *pairs*.\n\n        Returns:\n            str: A schemalocation attribute in the format:\n                ``xsi:schemaLocation=\"nsuri schemaloc<delim>nsuri2 schemaloc2<delim>...\"``\n\n        \"\"\"\n        if not ns_uris:\n            ns_uris = six.iterkeys(self.__ns_uri_map)\n\n        if sort:\n            ns_uris = sorted(ns_uris)\n\n        schemalocs = []\n\n        for ns_uri in ns_uris:\n            ni = self.__lookup_uri(ns_uri)\n\n            if ni.schema_location:\n                schemalocs.append(\"{0.uri} {0.schema_location}\".format(ni))\n\n        if not schemalocs:\n            return \"\"\n\n        return 'xsi:schemaLocation=\"{0}\"'.format(delim.join(schemalocs))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_uri_prefix_map(self):\n        mapping = {}\n        \n        for ni in six.itervalues(self.__ns_uri_map):\n            if ni.preferred_prefix:\n                mapping[ni.uri] = ni.preferred_prefix\n            elif len(ni.prefixes) > 0:\n                mapping[ni.uri] = next(iter(ni.prefixes))\n            else:\n                # The reason I don't let any namespace map to None here is that\n                # I don't think generateDS supports it.  It requires prefixes\n                # for all namespaces.\n                raise NoPrefixesError(ni.uri)\n\n        return mapping", "response": "Constructs and returns a map from namespace URI to prefix and the preferred_prefix of each namespace in this set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_uri_schemaloc_map(self):\n        mapping = {}\n\n        for ni in six.itervalues(self.__ns_uri_map):\n            if ni.schema_location:\n                mapping[ni.uri] = ni.schema_location\n\n        return mapping", "response": "Constructs and returns a map from namespace URI to schema location\n        URI. Namespaces without schema locations are excluded."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subset(self, ns_uris):\n        sub_ns = NamespaceSet()\n\n        for ns_uri in ns_uris:\n            ni = self.__lookup_uri(ns_uri)\n            new_ni = copy.deepcopy(ni)\n\n            # We should be able to reach into details of our own\n            # implementation on another obj, right??  This makes the subset\n            # operation faster.  We can set up the innards directly from a\n            # cloned _NamespaceInfo.\n            sub_ns._NamespaceSet__add_namespaceinfo(new_ni)\n\n        return sub_ns", "response": "Return a subset of this set containing only data for the given namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef import_from(self, other_ns, replace=False):\n        for other_ns_uri in other_ns.namespace_uris:\n            ni = self.__ns_uri_map.get(other_ns_uri)\n\n            if ni is None:\n                other_ni = other_ns._NamespaceSet__ns_uri_map[other_ns_uri]\n\n                # Gotta make sure that the other set isn't mapping its prefixes\n                # incompatibly with respect to this set.\n                for other_prefix in other_ni.prefixes:\n                    self.__check_prefix_conflict(other_ns_uri, other_prefix)\n\n                cloned_ni = copy.deepcopy(other_ni)\n                self.__add_namespaceinfo(cloned_ni)\n            elif replace:\n                other_ni = other_ns._NamespaceSet__ns_uri_map[other_ns_uri]\n                for other_prefix in other_ni.prefixes:\n                    self.__check_prefix_conflict(ni, other_prefix)\n\n                cloned_ni = copy.deepcopy(other_ni)\n                self.remove_namespace(other_ns_uri)\n                self.__add_namespaceinfo(cloned_ni)\n            else:\n                continue", "response": "Imports namespaces into this set from other_ns."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assert_valid(self):\n\n        for ns_uri, ni in six.iteritems(self.__ns_uri_map):\n            if not ni.uri:\n                raise InvalidNamespaceSetError(\n                    \"URI not set in _NamespaceInfo (id={0}):\\n{1}\".format(\n                        id(ni), ni\n                    )\n                )\n\n            if ns_uri != ni.uri:\n                raise InvalidNamespaceSetError(\n                    \"URI mismatch in dict ({0}) and _NamespaceInfo ({1})\".format(\n                        ns_uri, ni.uri\n                    )\n                )\n\n            if (ni.preferred_prefix is not None and\n               ni.preferred_prefix not in ni.prefixes):\n                raise InvalidNamespaceSetError(\n                    \"Namespace {0.uri}: preferred prefix \" \\\n                    '\"{0.preferred_prefix}\" not in prefixes ' \\\n                    \"{0.prefixes}\".format(ni)\n                )\n\n            for prefix in ni.prefixes:\n                if not prefix:\n                    raise InvalidNamespaceSetError(\n                        \"Namespace {0.uri}: empty value in prefix \" \\\n                        \"set: {0.prefixes}\".format(ni)\n                    )\n                other_ni = self.__prefix_map.get(prefix)\n                if other_ni is None:\n                    raise InvalidNamespaceSetError(\n                        'Namespace {0.uri}: prefix \"{1}\" not in ' \\\n                        'prefix map'.format(ni, prefix)\n                    )\n                if other_ni is not ni:\n                    raise InvalidNamespaceSetError(\n                        'Namespace {0.uri}: prefix \"{1}\" maps to ' \\\n                        'wrong _NamespaceInfo (id={2}, uri={3.uri})'.format(\n                            ni, prefix, id(other_ni), other_ni\n                        )\n                    )\n\n        if None in self.__prefix_map:\n            # None can be a preferred prefix, but should not be in the\n            # prefix map.\n            raise InvalidNamespaceSetError(\"None is in prefix map!\")", "response": "For debugging raises InvalidNamespaceSetError if this namespace set is invalid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the version of the root element passed in.", "response": "def _get_version(self, root):\n        \"\"\"Return the version of the root element passed in.\n\n        Args:\n            root (etree.Element)\n\n        Returns:\n            distutils.StrictVersion\n\n        Raises:\n            UnknownVersionError\n        \"\"\"\n        # Note: STIX and MAEC use a \"version\" attribute. To support CybOX, a\n        # subclass will need to combine \"cybox_major_version\",\n        # \"cybox_minor_version\", and \"cybox_update_version\".\n        version = self.get_version(root)\n        if version:\n            return StrictVersion(version)\n\n        raise UnknownVersionError(\n            \"Unable to determine the version of the input document. No \"\n            \"version information found on the root element.\"\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking that the root element is a supported version.", "response": "def _check_version(self, root):\n        \"\"\"Ensure the root element is a supported version.\n\n        Args:\n            root (etree.Element)\n\n        Raises:\n            UnsupportedVersionError\n        \"\"\"\n        version = self._get_version(root)\n        supported = [StrictVersion(x) for x in\n                     self.supported_versions(root.tag)]\n\n        if version in supported:\n            return\n\n        error = \"Document version ({0}) not in supported versions ({1})\"\n        raise UnsupportedVersionError(\n            message=error.format(version, supported),\n            expected=supported,\n            found=version\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_root_tag(self, root):\n        supported = self.supported_tags()\n        if root.tag in supported:\n            return\n\n        error = \"Document root element ({0}) not one of ({1})\"\n        raise UnsupportedRootElementError(\n            message=error.format(root.tag, supported),\n            expected=supported,\n            found=root.tag,\n        )", "response": "Check that the XML element tree has a supported root element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_xml_to_obj(self, xml_file, check_version=True, check_root=True,\n                         encoding=None):\n        \"\"\"Creates a STIX binding object from the supplied xml file.\n\n        Args:\n            xml_file: A filename/path or a file-like object representing a STIX\n                instance document\n            check_version: Inspect the version before parsing.\n            check_root: Inspect the root element before parsing.\n            encoding: The character encoding of the input `xml_file`.\n\n        Raises:\n            .UnknownVersionError: If `check_version` is ``True`` and `xml_file`\n                does not contain STIX version information.\n            .UnsupportedVersionError: If `check_version` is ``False`` and\n                `xml_file` contains an unsupported STIX version.\n            .UnsupportedRootElement: If `check_root` is ``True`` and `xml_file`\n                contains an invalid root element.\n\n        \"\"\"\n        root = get_etree_root(xml_file, encoding=encoding)\n\n        if check_root:\n            self._check_root_tag(root)\n\n        if check_version:\n            self._check_version(root)\n\n        entity_class = self.get_entity_class(root.tag)\n        entity_obj = entity_class._binding_class.factory()\n        entity_obj.build(root)\n\n        return entity_obj", "response": "Parses an XML file into a STIX binding object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing an XML file into a Python - stix STIX object.", "response": "def parse_xml(self, xml_file, check_version=True, check_root=True,\n                  encoding=None):\n        \"\"\"Creates a python-stix STIXPackage object from the supplied xml_file.\n\n        Args:\n            xml_file: A filename/path or a file-like object representing a STIX\n                instance document\n            check_version: Inspect the version before parsing.\n            check_root: Inspect the root element before parsing.\n            encoding: The character encoding of the input `xml_file`. If\n                ``None``, an attempt will be made to determine the input\n                character encoding.\n\n        Raises:\n            .UnknownVersionError: If `check_version` is ``True`` and `xml_file`\n                does not contain STIX version information.\n            .UnsupportedVersionError: If `check_version` is ``False`` and\n                `xml_file` contains an unsupported STIX version.\n            .UnsupportedRootElement: If `check_root` is ``True`` and `xml_file`\n                contains an invalid root element.\n\n        \"\"\"\n\n        xml_etree = get_etree(xml_file, encoding=encoding)\n        entity_obj = self.parse_xml_to_obj(\n            xml_file=xml_etree,\n            check_version=check_version,\n            check_root=check_root\n        )\n\n        xml_root_node = xml_etree.getroot()\n        entity = self.get_entity_class(xml_root_node.tag).from_obj(entity_obj)\n\n        # Save the parsed nsmap and schemalocations onto the parsed Entity\n        entity.__input_namespaces__ = dict(iteritems(xml_root_node.nsmap))\n        with ignored(KeyError):\n            pairs = get_schemaloc_pairs(xml_root_node)\n            entity.__input_schemalocations__ = dict(pairs)\n\n        return entity"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, collection_id, content=None, **kwargs):\n        assert COLLECTION_ID_PATTERN.match(collection_id)\n        r = self.get_collection(collection_id)\n        if isinstance(r, dict) and ('data' in r):\n            return r['data']\n        return r", "response": "Get the data for a specific collection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef default_links_pagination_factory(page, urlkwargs):\n    endpoint = '.communities_list'\n\n    links = {\n        'self': url_for(endpoint, page=page.page, _external=True, **urlkwargs),\n    }\n\n    if page.has_prev:\n        links['prev'] = url_for(endpoint, page=page.prev_num, _external=True,\n                                **urlkwargs)\n    if page.has_next:\n        links['next'] = url_for(endpoint, page=page.next_num, _external=True,\n                                **urlkwargs)\n\n    return links", "response": "Factory for record links generation."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes a dict corresponding to the honeybadgerfish JSON blob of the 1. 0. type and converts it to BY_ID_HONEY_BADGERFISH version. The object is returned in place .", "response": "def convert(self, obj):\n        \"\"\"Takes a dict corresponding to the honeybadgerfish JSON blob of the 1.0.* type and\n        converts it to BY_ID_HONEY_BADGERFISH version. The object is modified in place\n        and returned.\n        \"\"\"\n        if self.pristine_if_invalid:\n            raise NotImplementedError('pristine_if_invalid option is not supported yet')\n\n        nex = get_nexml_el(obj)\n        assert nex\n        self._recursive_convert_dict(nex)\n        # pluralization simplifications in hbf:\n        # convert dicts to lists for the primary datastructures...\n        self._dict_to_list_of_dicts(nex, 'otus')\n        self._dict_to_list_of_dicts(nex, 'otus', 'otu')\n        self._dict_to_list_of_dicts(nex, 'trees')\n        self._dict_to_list_of_dicts(nex, 'trees', 'tree')\n        self._dict_to_list_of_dicts(nex, 'trees', 'tree', 'node')\n        self._dict_to_list_of_dicts(nex, 'trees', 'tree', 'edge')\n        if self._add_tree_xsi_type:\n            for tb in nex.get('trees', []):\n                for t in tb.get('tree', []):\n                    t.setdefault('@xsi:type', 'nex:FloatTree')\n        nex['@nexml2json'] = str(DIRECT_HONEY_BADGERFISH)\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_logo_url(self, obj):\n        if current_app and obj.logo_url:\n            return u'{site_url}{path}'.format(\n                site_url=current_app.config.get('THEME_SITEURL'),\n                path=obj.logo_url,\n            )", "response": "Get the community logo URL."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef item_links_addition(self, data):\n        links_item_factory = self.context.get('links_item_factory',\n                                              default_links_item_factory)\n        data['links'] = links_item_factory(data)\n        return data", "response": "Add the links for each community."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps result in envelope.", "response": "def envelope(self, data, many):\n        \"\"\"Wrap result in envelope.\"\"\"\n        if not many:\n            return data\n\n        result = dict(\n            hits=dict(\n                hits=data,\n                total=self.context.get('total', len(data))\n            )\n        )\n\n        page = self.context.get('page')\n        if page:\n            links_pagination_factory = self.context.get(\n                'links_pagination_factory',\n                default_links_pagination_factory\n            )\n\n            urlkwargs = self.context.get('urlkwargs', {})\n\n            result['links'] = links_pagination_factory(page, urlkwargs)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_datetime(value):\n    if not value:\n        return None\n    elif isinstance(value, datetime.datetime):\n        return value\n    return dateutil.parser.parse(value)", "response": "Attempts to parse value into an instance of datetime. If value is None this function will return None. If value is a string this function will return value. If value is a datetime. datetime this function will return value. If value is None this function will return None."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nattempts to parse value into an instance of datetime. date. If value is None this function will return None. If value is a string datetime. date this function will return value. If value is a datetime. date this function will return value. If value is None this function will return None.", "response": "def parse_date(value):\n    \"\"\"Attempts to parse `value` into an instance of ``datetime.date``. If\n    `value` is ``None``, this function will return ``None``.\n\n    Args:\n        value: A timestamp. This can be a string, datetime.date, or\n            datetime.datetime value.\n\n    \"\"\"\n    if not value:\n        return None\n\n    if isinstance(value, datetime.date):\n        return value\n\n    return parse_datetime(value).date()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize_date(value):\n    if not value:\n        return None\n    elif isinstance(value, datetime.datetime):\n        return value.date().isoformat()\n    elif isinstance(value, datetime.date):\n        return value.isoformat()\n    else:\n        return parse_date(value).isoformat()", "response": "Attempts to convert value into an xs : date string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the word_string with the highest relative probability of the word_string.", "response": "def correct_word(word_string):\n    '''\n    Finds all valid one and two letter corrections for word_string, returning the word\n    with the highest relative probability as type str.\n    '''\n    if word_string is None:\n        return \"\"\n    elif isinstance(word_string, str):\n        return max(find_candidates(word_string), key=find_word_prob)\n    else:\n        raise InputError(\"string or none type variable not passed as argument to correct_word\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds all potential words word_string could have intended to mean.", "response": "def find_candidates(word_string):\n    '''\n    Finds all potential words word_string could have intended to mean. If a word is not incorrectly\n    spelled, it will return this word first, else if will look for one letter edits that are correct.\n    If there are no valid one letter edits, it will perform a two letter edit search.\n\n    If valid corrections are found, all are returned as a set instance. Should a valid word not be\n    found, the original word is returned as a set instance.\n    '''\n    if word_string is None:\n        return {}\n    elif isinstance(word_string, str):\n        return (validate_words([word_string]) or validate_words(list(find_one_letter_edits(word_string)))\n                or validate_words(list(find_two_letter_edits(word_string))) or set([word_string]))\n    else:\n        raise InputError(\"string or none type variable not passed as argument to find_candidates\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding all possible one letter edits of a word_string.", "response": "def find_one_letter_edits(word_string):\n    '''\n    Finds all possible one letter edits of word_string:\n    - Splitting word_string into two words at all character locations\n    - Deleting one letter at all character locations\n    - Switching neighbouring characters\n    - Replacing a character with every alphabetical letter\n    - Inserting all possible alphabetical characters between each character location including boundaries\n\n    Returns all one letter edits as a set instance.\n    '''\n    if word_string is None:\n        return {}\n    elif isinstance(word_string, str):\n        splits = [(word_string[:i], word_string[i:]) for i in range(len(word_string) + 1)]\n        deletes = [L + R[1:] for L, R in splits if R]\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R) > 1]\n        replaces = [L + c + R[1:] for L, R in splits if R for c in EN_ALPHABET]\n        inserts = [L + c + R for L, R in splits for c in EN_ALPHABET]\n        return set(deletes + transposes + replaces + inserts)\n    else:\n        raise InputError(\"string or none type variable not passed as argument to find_one_letter_edits\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding all possible two letter edits of a word_string.", "response": "def find_two_letter_edits(word_string):\n    '''\n    Finds all possible two letter edits of word_string:\n    - Splitting word_string into two words at all character locations\n    - Deleting one letter at all character locations\n    - Switching neighbouring characters\n    - Replacing a character with every alphabetical letter\n    - Inserting all possible alphabetical characters between each character location including boundaries\n\n    This can be seen as a reapplication of find_one_letter_edits to all words found via a first\n    instantiation of find_one_letter_edits on word_string.\n\n    Returns all two letter edits as a set instance.\n    '''\n    if word_string is None:\n        return {}\n    elif isinstance(word_string, str):\n        return (e2 for e1 in find_one_letter_edits(word_string) for e2 in find_one_letter_edits(e1))\n    else:\n        raise InputError(\"string or none type variable not passed as argument to find_two_letter_edits\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_word_prob(word_string, word_total=sum(WORD_DISTRIBUTION.values())):\n    '''\n    Finds the relative probability of the word appearing given context of a base corpus.\n    Returns this probability value as a float instance.\n    '''\n    if word_string is None:\n        return 0\n    elif isinstance(word_string, str):\n        return WORD_DISTRIBUTION[word_string] / word_total\n    else:\n        raise InputError(\"string or none type variable not passed as argument to find_word_prob\")", "response": "Find the relative probability of the word appearing given context of a base corpus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_words(word_list):\n    '''\n    Checks for each edited word in word_list if that word is a valid english word.abs\n    Returns all validated words as a set instance.\n    '''\n    if word_list is None:\n        return {}\n    elif isinstance(word_list, list):\n        if not word_list:\n            return {}\n        else:\n            return set(word for word in word_list if word in WORD_DISTRIBUTION)\n    else:\n        raise InputError(\"list variable not passed as argument to validate_words\")", "response": "Checks for each edited word in word_list is a valid english word. abs\n    Returns all validated words as a set instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef search_exoplanet(exoplanet):\n    '''\n    It is also possible to query the exoplanets by label, here is an example of querying for the exoplanet labeled as 11 Com\n\n    http://star-api.herokuapp.com/api/v1/exo_planets/11 Com\n    '''\n\n    base_url = \"http://star-api.herokuapp.com/api/v1/exo_planets/\"\n\n    if not isinstance(exoplanet, str):\n        raise ValueError(\n            \"The exoplanet arg you provided is not the type of str\")\n    else:\n        base_url += exoplanet\n\n    return dispatch_http_get(base_url)", "response": "This function searches the Heroku App for a specific exoplanet."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_local_galaxies(galaxy):\n    '''\n    It is also possible to query the local galaxies by label, here is an example of querying for the local galaxy labeled  IC 10\n\n    http://star-api.herokuapp.com/api/v1/local_groups/IC 10\n    '''\n\n    base_url = \"http://star-api.herokuapp.com/api/v1/local_groups/\"\n\n    if not isinstance(galaxy, str):\n        raise ValueError(\"The galaxy arg you provided is not the type of str\")\n    else:\n        base_url += galaxy\n\n    return dispatch_http_get(base_url)", "response": "Search for local galaxies by label"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, amendment_id, content=None, **kwargs):\n        assert AMENDMENT_ID_PATTERN.match(amendment_id)\n        r = self.get_amendment(amendment_id)\n        if isinstance(r, dict) and ('data' in r):\n            return r['data']\n        return r", "response": "Get the data for a specific amendment ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the python representation of the class.", "response": "def as_python(self, name: str) -> str:\n        \"\"\" Return the python representation \"\"\"\n        if self._ruleTokens:\n            pattern = \"jsg.JSGPattern(r'{}'.format({}))\".\\\n                format(self._rulePattern, ', '.join(['{v}={v}.pattern'.format(v=v) for v in sorted(self._ruleTokens)]))\n        else:\n            pattern = \"jsg.JSGPattern(r'{}')\".format(self._rulePattern)\n        base_type = self._jsontype.signature_type() if self._jsontype else \"jsg.JSGString\"\n        return python_template.format(name=name, base_type=base_type, pattern=pattern)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvisit the lexerAltList ctx and add the rule pattern to the rule list", "response": "def visitLexerAltList(self, ctx: jsgParser.LexerAltListContext):\n        \"\"\" lexerAltList: lexerAlt (LBAR lexerAlt)* \"\"\"\n        altlist = ctx.lexerAlt()\n        self.visit(altlist[0])\n        for alt in altlist[1:]:\n            self._rulePattern += '|'\n            self.visit(alt)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding rule pattern to _rulePattern", "response": "def visitLexerElement(self, ctx: jsgParser.LexerElementContext):\n        \"\"\" lexerElement: lexerAtom ebnfSuffix? | lexerBlock ebnfSuffix? \"\"\"\n        self.visitChildren(ctx)\n        if ctx.ebnfSuffix():\n            self._rulePattern += ctx.ebnfSuffix().getText()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef visitLexerBlock(self, ctx: jsgParser.LexerBlockContext):\n        self._rulePattern += '('\n        self.visitChildren(ctx)\n        self._rulePattern += ')'", "response": "visitLexerBlock - Gets the lexeraltList CPREN and adds it to the rule pattern"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visitLexerAtom(self, ctx: jsgParser.LexerAtomContext):\n        if ctx.LEXER_CHAR_SET() or ctx.ANY():\n            self._rulePattern += str(ctx.getText())\n        else:\n            self.visitChildren(ctx)", "response": "lexerAtom : lexerTerminal | LEXER_CHAR_SET | ANY"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvisit the LEXER_ID | STRING.", "response": "def visitLexerTerminal(self, ctx: jsgParser.LexerTerminalContext):\n        \"\"\" terminal: LEXER_ID | STRING  \"\"\"\n        if ctx.LEXER_ID():\n            # Substitute LEXER_ID with its string equivalent - \"{LEXER_ID}\".format(LEXER_ID=LEXER_ID.pattern)\n            idtoken = as_token(ctx)\n            self._rulePattern += '({' + idtoken + '})'\n            self._ruleTokens.add(idtoken)\n        else:\n            self.add_string(ctx.getText()[1:-1], False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert any string to a slug suitable for filename and URL part.", "response": "def slugify(s):\n    \"\"\"Convert any string to a \"slug\", a simplified form suitable for filename and URL part.\n     EXAMPLE: \"Trees about bees\" => 'trees-about-bees'\n     EXAMPLE: \"My favorites!\" => 'my-favorites'\n    N.B. that its behavior should match this client-side slugify function, so\n    we can accurately \"preview\" slugs in the browser:\n     https://github.com/OpenTreeOfLife/opentree/blob/553546942388d78545cc8dcc4f84db78a2dd79ac/curator/static/js/curation-helpers.js#L391-L397\n    TODO: Should we also trim leading and trailing spaces (or dashes in the final slug)?\n    \"\"\"\n    slug = s.lower()  # force to lower case\n    slug = re.sub('[^a-z0-9 -]', '', slug)  # remove invalid chars\n    slug = re.sub(r'\\s+', '-', slug)  # collapse whitespace and replace by -\n    slug = re.sub('-+', '-', slug)  # collapse dashes\n    if not slug:\n        slug = 'untitled'\n    return slug"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating next slug for a series.", "response": "def increment_slug(s):\n    \"\"\"Generate next slug for a series.\n\n       Some docstore types will use slugs (see above) as document ids. To\n       support unique ids, we'll serialize them as follows:\n         TestUserA/my-test\n         TestUserA/my-test-2\n         TestUserA/my-test-3\n         ...\n    \"\"\"\n    slug_parts = s.split('-')\n    # advance (or add) the serial counter on the end of this slug\n    # noinspection PyBroadException\n    try:\n        # if it's an integer, increment it\n        slug_parts[-1] = str(1 + int(slug_parts[-1]))\n    except:\n        # there's no counter! add one now\n        slug_parts.append('2')\n    return '-'.join(slug_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting ott_id to ottId.", "response": "def underscored2camel_case(v):\n    \"\"\"converts ott_id to ottId.\"\"\"\n    vlist = v.split('_')\n    c = []\n    for n, el in enumerate(vlist):\n        if el:\n            if n == 0:\n                c.append(el)\n            else:\n                c.extend([el[0].upper(), el[1:]])\n    return ''.join(c)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unvalidated_parm(self, parm: str) -> bool:\n        return parm.startswith(\"_\") or parm == self.TYPE or parm in self.IGNORE or \\\n            (self.JSON_LD and parm.startswith('@'))", "response": "Return true if the pair name should be ignored\n           "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dispatch(self, request):\n        def _wrapped():\n            messages = self._get_request_messages(request)\n            results = [self._dispatch_and_handle_errors(message) for message in messages]\n            non_notification_results = [x for x in results if x is not None]\n            if len(non_notification_results) == 0:\n                return None\n            elif len(messages) == 1:\n                return non_notification_results[0]\n            else:\n                return non_notification_results\n\n        result, _ = self._handle_exceptions(_wrapped)\n        if result is not None:\n            return self._encode_complete_result(result)", "response": "Takes a request and dispatches its data to a jsonrpc method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register(self, name, method, method_signature=None):\n        if inspect.ismethod(method):\n            raise Exception(\"typedjsonrpc does not support making class methods into endpoints\")\n        self._name_to_method_info[name] = MethodInfo(name, method, method_signature)", "response": "Registers a method with a given name and signature."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef method(self, returns, **parameter_types):\n        @wrapt.decorator\n        def type_check_wrapper(method, instance, args, kwargs):\n            \"\"\"Wraps a method so that it is type-checked.\n\n            :param method: The method to wrap\n            :type method: (T) -> U\n            :return: The result of calling the method with the given parameters\n            :rtype: U\n            \"\"\"\n            if instance is not None:\n                raise Exception(\"Instance shouldn't be set.\")\n\n            parameter_names = inspect.getargspec(method).args  # pylint: disable=deprecated-method\n            defaults = inspect.getargspec(method).defaults  # pylint: disable=deprecated-method\n            parameters = self._collect_parameters(parameter_names, args, kwargs, defaults)\n\n            parameter_checker.check_types(parameters, parameter_types, self._strict_floats)\n\n            result = method(*args, **kwargs)\n            parameter_checker.check_return_type(result, returns, self._strict_floats)\n\n            return result\n\n        def register_method(method):\n            \"\"\"Registers a method with its fully qualified name.\n\n            :param method: The method to register\n            :type method: function\n            :return: The original method wrapped into a type-checker\n            :rtype: function\n            \"\"\"\n            parameter_names = inspect.getargspec(method).args  # pylint: disable=deprecated-method\n            parameter_checker.check_type_declaration(parameter_names, parameter_types)\n\n            wrapped_method = type_check_wrapper(method, None, None, None)\n            fully_qualified_name = \"{}.{}\".format(method.__module__, method.__name__)\n            self.register(fully_qualified_name, wrapped_method,\n                          MethodSignature.create(parameter_names, parameter_types, returns))\n            return wrapped_method\n\n        return register_method", "response": "Decorator for registering a method in a registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _collect_parameters(parameter_names, args, kwargs, defaults):\n        parameters = {}\n        if defaults is not None:\n            zipped_defaults = zip(reversed(parameter_names), reversed(defaults))\n            for name, default in zipped_defaults:\n                parameters[name] = default\n        for name, value in zip(parameter_names, args):\n            parameters[name] = value\n        for name, value in kwargs.items():\n            parameters[name] = value\n        return parameters", "response": "Creates a dictionary mapping parameter names to their values in the method call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the request as a json message.", "response": "def _get_request_messages(self, request):\n        \"\"\"Parses the request as a json message.\n\n        :param request: a werkzeug request with json data\n        :type request: werkzeug.wrappers.Request\n        :return: The parsed json object\n        :rtype: dict[str, object]\n        \"\"\"\n        data = request.get_data(as_text=True)\n        try:\n            msg = self.json_decoder.decode(data)\n        except Exception:\n            raise ParseError(\"Could not parse request data '{}'\".format(data))\n        if isinstance(msg, list):\n            return msg\n        else:\n            return [msg]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_request(self, msg):\n        if \"jsonrpc\" not in msg:\n            raise InvalidRequestError(\"'\\\"jsonrpc\\\": \\\"2.0\\\"' must be included.\")\n        if msg[\"jsonrpc\"] != \"2.0\":\n            raise InvalidRequestError(\"'jsonrpc' must be exactly the string '2.0', but it was '{}'.\"\n                                      .format(msg[\"jsonrpc\"]))\n        if \"method\" not in msg:\n            raise InvalidRequestError(\"No method specified.\")\n        if \"id\" in msg:\n            if msg[\"id\"] is None:\n                raise InvalidRequestError(\"typedjsonrpc does not allow id to be None.\")\n            if isinstance(msg[\"id\"], float):\n                raise InvalidRequestError(\"typedjsonrpc does not support float ids.\")\n            if not isinstance(msg[\"id\"], (six.string_types, six.integer_types)):\n                raise InvalidRequestError(\"id must be a string or integer; '{}' is of type {}.\"\n                                          .format(msg[\"id\"], type(msg[\"id\"])))\n        if msg[\"method\"] not in self._name_to_method_info:\n            raise MethodNotFoundError(\"Could not find method '{}'.\".format(msg[\"method\"]))", "response": "Checks that the request json is well - formed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_template_to_string(input, _from_string=False, **context):\n    if _from_string:\n        template = current_app.jinja_env.from_string(input)\n    else:\n        template = current_app.jinja_env.get_or_select_template(input)\n    return template.render(context)", "response": "Render a template from the template folder with the given context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_and_validate_logo(logo_stream, logo_filename, community_id):\n    cfg = current_app.config\n\n    logos_bucket_id = cfg['COMMUNITIES_BUCKET_UUID']\n    logo_max_size = cfg['COMMUNITIES_LOGO_MAX_SIZE']\n    logos_bucket = Bucket.query.get(logos_bucket_id)\n    ext = os.path.splitext(logo_filename)[1]\n    ext = ext[1:] if ext.startswith('.') else ext\n\n    logo_stream.seek(SEEK_SET, SEEK_END)  # Seek from beginning to end\n    logo_size = logo_stream.tell()\n    if logo_size > logo_max_size:\n        return None\n\n    if ext in cfg['COMMUNITIES_LOGO_EXTENSIONS']:\n        key = \"{0}/logo.{1}\".format(community_id, ext)\n        logo_stream.seek(0)  # Rewind the stream to the beginning\n        ObjectVersion.create(logos_bucket, key, stream=logo_stream,\n                             size=logo_size)\n        return ext\n    else:\n        return None", "response": "Validate if communities logo is in limit size and save it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the communities file bucket.", "response": "def initialize_communities_bucket():\n    \"\"\"Initialize the communities file bucket.\n\n    :raises: `invenio_files_rest.errors.FilesException`\n    \"\"\"\n    bucket_id = UUID(current_app.config['COMMUNITIES_BUCKET_UUID'])\n\n    if Bucket.query.get(bucket_id):\n        raise FilesException(\"Bucket with UUID {} already exists.\".format(\n            bucket_id))\n    else:\n        storage_class = current_app.config['FILES_REST_DEFAULT_STORAGE_CLASS']\n        location = Location.get_default()\n        bucket = Bucket(id=bucket_id,\n                        location=location,\n                        default_storage_class=storage_class)\n        db.session.add(bucket)\n        db.session.commit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_request_email_templ(increq, template, **ctx):\n    # Add minimal information to the contex (without overwriting).\n    curate_link = '{site_url}/communities/{id}/curate/'.format(\n        site_url=current_app.config['THEME_SITEURL'],\n        id=increq.community.id)\n\n    min_ctx = dict(\n        record=Record.get_record(increq.record.id),\n        requester=increq.user,\n        community=increq.community,\n        curate_link=curate_link,\n    )\n    for k, v in min_ctx.items():\n        if k not in ctx:\n            ctx[k] = v\n\n    msg_element = render_template_to_string(template, **ctx)\n    return msg_element", "response": "Formats the message element for inclusion request notification."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting the email message title for inclusion request notification.", "response": "def format_request_email_title(increq, **ctx):\n    \"\"\"Format the email message title for inclusion request notification.\n\n    :param increq: Inclusion request object for which the request is made.\n    :type increq: `invenio_communities.models.InclusionRequest`\n    :param ctx: Optional extra context parameters passed to formatter.\n    :type ctx: dict.\n    :returns: Email message title.\n    :rtype: str\n    \"\"\"\n    template = current_app.config[\"COMMUNITIES_REQUEST_EMAIL_TITLE_TEMPLATE\"],\n    return format_request_email_templ(increq, template, **ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nformat the email message body for inclusion request notification.", "response": "def format_request_email_body(increq, **ctx):\n    \"\"\"Format the email message body for inclusion request notification.\n\n    :param increq: Inclusion request object for which the request is made.\n    :type increq: `invenio_communities.models.InclusionRequest`\n    :param ctx: Optional extra context parameters passed to formatter.\n    :type ctx: dict.\n    :returns: Email message body.\n    :rtype: str\n    \"\"\"\n    template = current_app.config[\"COMMUNITIES_REQUEST_EMAIL_BODY_TEMPLATE\"],\n    return format_request_email_templ(increq, template, **ctx)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsignals for sending emails after community inclusion request.", "response": "def send_community_request_email(increq):\n    \"\"\"Signal for sending emails after community inclusion request.\"\"\"\n    from flask_mail import Message\n    from invenio_mail.tasks import send_email\n\n    msg_body = format_request_email_body(increq)\n    msg_title = format_request_email_title(increq)\n\n    sender = current_app.config['COMMUNITIES_REQUEST_EMAIL_SENDER']\n\n    msg = Message(\n        msg_title,\n        sender=sender,\n        recipients=[increq.community.owner.email, ],\n        body=msg_body\n    )\n\n    send_email.delay(msg.__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef modifydocs(a, b, desc=''):\n    newdoc = a.func_doc.replace('\\t\\t', '\\t')\n    newdoc += \"Documentation from \" + desc + \":\\n\" + b.func_doc\n    return newdoc", "response": "This function is used to modify the documentation of a class method that is essentially a wrapper for an outside \n function b rope in the docstring of a and append to that of b."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tab_colstack(ListOfTabArrays, mode='abort'):\n    (data, naming) = spreadsheet.colstack(ListOfTabArrays, mode=mode, \n                                          returnnaming=True)\n        \n    coloring = {}\n    for (i, a) in enumerate(ListOfTabArrays):\n        namedict = dict([(x,y) for (j,x,y) in naming if i == j])\n        for k in a.coloring:\n            s = [namedict[kk] for kk in a.coloring[k]]\n            if k in coloring.keys():\n                coloring[k] = utils.uniqify(coloring[k] + s)\n            else:\n                coloring[k] = s\n\n    for k in coloring.keys():\n        s = [x for x in coloring[k] if x in data.dtype.names]\n        if len(s) > 0:\n            coloring[k] = s\n        else:\n            coloring.pop(k)\n\n    data = data.view(tabarray)\n    data.coloring = coloring\n    return data", "response": "Horizontal stacking of tabarrays e. g. adding columns."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping for tab. join that deals with the coloring and returns the result as a tabarray.", "response": "def tab_join(ToMerge, keycols=None, nullvals=None, renamer=None, \n             returnrenaming=False, Names=None):\n    '''\n    Database-join for tabular arrays.\n\n    Wrapper for :func:`tabular.spreadsheet.join` that deals with the coloring \n    and returns the result as a tabarray.\n\n    Method calls::\n\n            data = tabular.spreadsheet.join\n\n    '''\n\n    [Result,Renaming] = spreadsheet.join(ToMerge, keycols=keycols, \n          nullvals=nullvals, renamer=renamer, returnrenaming=True, Names=Names)\n\n    if isinstance(ToMerge,dict):\n        Names = ToMerge.keys()\n    else:\n        Names = range(len(ToMerge))\n\n    Colorings = dict([(k,ToMerge[k].coloring) if 'coloring' in dir(ToMerge[k])  \n                                              else {} for k in Names])\n    for k in Names:\n        if k in Renaming.keys():\n            l = ToMerge[k]\n            Colorings[k] = \\\n                dict([(g, [n if not n in Renaming[k].keys() else Renaming[k][n] \n                       for n in l.coloring[g]]) for g in Colorings[k].keys()])\n    Coloring = {}\n    for k in Colorings.keys():\n        for j in Colorings[k].keys():\n            if j in Coloring.keys():\n                Coloring[j] = utils.uniqify(Coloring[j] + Colorings[k][j])\n            else:\n                Coloring[j] = utils.uniqify(Colorings[k][j])\n\n    Result = Result.view(tabarray)\n    Result.coloring = Coloring\n\n    if returnrenaming:\n        return [Result,Renaming]\n    else:\n        return Result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract(self):\n        return np.vstack([self[r] for r in self.dtype.names]).T.squeeze()", "response": "Returns a copy of this tabarray in the form of a numpy ndarray."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addrecords(self, new):\n        data = spreadsheet.addrecords(self,new)\n        data = data.view(tabarray)\n        data.coloring = self.coloring\n        return data", "response": "Adds one or more records to the end of the array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd one or more new columns. Method wraps:: tabular.spreadsheet.addcols(self, cols, names)", "response": "def addcols(self, cols, names=None):\n        \"\"\"\n        Add one or more new columns.\n\n        Method wraps::\n\n                tabular.spreadsheet.addcols(self, cols, names)\n\n        \"\"\"\n        data = spreadsheet.addcols(self, cols, names)\n        data = data.view(tabarray)\n        data.coloring = self.coloring\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndelete columns and/or colors. Method wraps:: tabular.spreadsheet.deletecols(self, cols)", "response": "def deletecols(self, cols):\n        \"\"\"\n        Delete columns and/or colors.\n\n        Method wraps::\n\n                tabular.spreadsheet.deletecols(self, cols)\n\n        \"\"\"\n        if isinstance(cols, str):\n        \tcols = cols.split(',')\n        deletenames = utils.uniqify(utils.listunion([[c] if c in \n        self.dtype.names else self.coloring[c] for c in cols]))\n        return spreadsheet.deletecols(self,deletenames)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef renamecol(self, old, new):\n        spreadsheet.renamecol(self,old,new)\n        for x in self.coloring.keys():\n            if old in self.coloring[x]:\n                ind = self.coloring[x].index(old)\n                self.coloring[x][ind] = new", "response": "Method wraps the renamecol method to rename column or color in - place."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef saveSV(self, fname, comments=None, metadata=None, printmetadict=None,\n                       dialect = None, delimiter=None, doublequote=True, \n                       lineterminator='\\n', escapechar = None, quoting=csv.QUOTE_MINIMAL, \n                       quotechar='\"', skipinitialspace=False, \n                       stringifier=None, verbosity=DEFAULT_VERBOSITY):\n        \"\"\"\n        Save the tabarray to a single flat separated variable (CSV) text file.   \n        \n        Method wraps::\n\n                tabular.io.saveSV.      \n                \n        See docstring of tabular.io.saveSV, or Tabular reference documentation,  for more information.        \n\n        \"\"\"\n        io.saveSV(fname,self, comments, metadata, printmetadict, \n                        dialect, delimiter, doublequote, lineterminator, escapechar, quoting, quotechar,skipinitialspace,stringifier=stringifier,verbosity=verbosity)", "response": "Save the object to a single flat separated variable file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef savebinary(self, fname, savecoloring=True):\n        io.savebinary(fname=fname, X=self, savecoloring=savecoloring)", "response": "Save the tabarray to a numpy binary archive."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef colstack(self, new, mode='abort'):\n        if isinstance(new,list):\n            return tab_colstack([self] + new,mode)\n        else:\n            return tab_colstack([self, new], mode)", "response": "Horizontal stacking for tabarrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef aggregate(self, On=None, AggFuncDict=None, AggFunc=None, AggList =\n                  None, returnsort=False,KeepOthers=True, keyfuncdict=None):\n        \"\"\"\n        Aggregate a tabarray on columns for given functions.\n\n        Method wraps::\n\n                tabular.spreadsheet.aggregate(self, On, AggFuncDict, AggFunc, returnsort)\n\n        \"\"\"\n        if returnsort:\n            [data, s] = spreadsheet.aggregate(X=self, \n                     On=On, \n                     AggFuncDict=AggFuncDict, \n                     AggFunc=AggFunc, \n                     AggList=AggList, \n                     returnsort=returnsort, \n                     keyfuncdict=keyfuncdict)\n        else:\n            data = spreadsheet.aggregate(X=self, On=On, AggFuncDict=AggFuncDict, \n                     AggFunc=AggFunc, AggList = AggList, returnsort=returnsort, \n                     KeepOthers=KeepOthers,\n                     keyfuncdict=keyfuncdict)\n        data = data.view(tabarray)\n        data.coloring = self.coloring\n        if returnsort:\n            return [data, s]\n        else:\n            return data", "response": "A method that aggregates a tabarray on columns for given functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\naggregate a tabarray and include original data in the result.", "response": "def aggregate_in(self, On=None, AggFuncDict=None, AggFunc=None,\n                 AggList=None, interspersed=True):\n        \"\"\"\n        Aggregate a tabarray and include original data in the result.\n\n        See the :func:`aggregate` method.\n\n        Method wraps::\n\n                tabular.summarize.aggregate_in(self, On, AggFuncDict, AggFunc, interspersed)\n\n        \"\"\"\n        data = spreadsheet.aggregate_in(Data=self, On=On, \n               AggFuncDict=AggFuncDict, AggFunc=AggFunc, \n               AggList = AggList, interspersed=interspersed)\n        data = data.view(tabarray)\n        data.view = self.coloring\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npivots with a column axis and b as the row axis and a values as the column axis.", "response": "def pivot(self, a, b, Keep=None, NullVals=None, order = None, prefix='_'):\n        \"\"\"\n        Pivot with `a` as the row axis and `b` values as the column axis.\n\n        Method wraps::\n\n                tabular.spreadsheet.pivot(X, a, b, Keep)\n\n        \"\"\"\n        [data,coloring] = spreadsheet.pivot(X=self, a=a, b=b, Keep=Keep, \n                          NullVals=NullVals, order=order, prefix=prefix)\n        data = data.view(tabarray)\n        data.coloring = coloring\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrap for spreadsheet. join that handles coloring attributes.", "response": "def join(self, ToMerge, keycols=None, nullvals=None, \n             renamer=None, returnrenaming=False, selfname=None, Names=None):\n        \"\"\"\n        Wrapper for spreadsheet.join, but handles coloring attributes.\n\n        The `selfname` argument allows naming of `self` to be used if `ToMerge` \n        is a dictionary.\n\n        **See also:** :func:`tabular.spreadsheet.join`, :func:`tab_join`\n        \"\"\"\n\n        if isinstance(ToMerge,np.ndarray):\n            ToMerge = [ToMerge]\n\n        if isinstance(ToMerge,dict):\n            assert selfname not in ToMerge.keys(), \\\n             ('Can\\'t use \"', selfname + '\" for name of one of the things to '  \n              'merge, since it is the same name as the self object.')\n            if selfname == None:\n                try:\n                    selfname = self.name\n                except AttributeError:\n                    selfname = 'self'\n            ToMerge.update({selfname:self})\n        else:\n            ToMerge = [self] + ToMerge\n\n        return tab_join(ToMerge, keycols=keycols, nullvals=nullvals, \n                   renamer=renamer, returnrenaming=returnrenaming, Names=Names)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef argsort(self, axis=-1, kind='quicksort', order=None):\n        index_array = np.core.fromnumeric._wrapit(self, 'argsort', axis, \n                                                     kind, order)\n        index_array = index_array.view(np.ndarray)\n        return index_array", "response": "A method that sorts the array along the given axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate_collection(obj, retain_deprecated=True, **kwargs):\n    # Gather and report errors in a simple list\n    errors = []\n    n = create_validation_adaptor(obj, errors, **kwargs)\n    return errors, n", "response": "Takes an object that is a collection object. Returns a list of errors and a simple list of messages that are not valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef matches(self, txt: str) -> bool:\n        # rval = ref.getText()[1:-1].encode('utf-8').decode('unicode-escape')\n        if r'\\\\u' in self.pattern_re.pattern:\n            txt = txt.encode('utf-8').decode('unicode-escape')\n        match = self.pattern_re.match(txt)\n        return match is not None and match.end() == len(txt)", "response": "Determine whether txt matches the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef Point2HexColor(a, lfrac, tfrac):\n\n    [H,S,V] = [math.floor(360 * a), lfrac, tfrac]\n\n    RGB = hsvToRGB(H, S, V)\n\n    H = [hex(int(math.floor(255 * x))) for x in RGB]\n\n    HEX = [a[a.find('x') + 1:] for a in H]\n    HEX = ['0' + h if len(h) == 1 else h for h in HEX]\n\n    return '#' + ''.join(HEX)", "response": "Convert a point to a hex color."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts HSV to RGB color space.", "response": "def hsvToRGB(h, s, v):\n    \"\"\"\n    Convert HSV (hue, saturation, value) color space to RGB (red, green blue) \n    color space.\n\n    **Parameters**\n\n            **h** :  float\n\n                    Hue, a number in [0, 360].\n\n            **s** :  float\n\n                    Saturation, a number in [0, 1].\n\n            **v** :  float\n\n                    Value, a number in [0, 1].\n\n    **Returns**\n\n            **r** :  float\n\n                    Red, a number in [0, 1].\n\n            **g** :  float\n\n                    Green, a number in [0, 1].\n\n            **b** :  float\n\n                    Blue, a number in [0, 1].\n\n    \"\"\"\n    hi = math.floor(h / 60.0) % 6\n    f =  (h / 60.0) - math.floor(h / 60.0)\n    p = v * (1.0 - s)\n    q = v * (1.0 - (f * s))\n    t = v * (1.0 - ((1.0 - f) * s))\n\n    D = {0: (v, t, p), 1: (q, v, p), 2: (p, v, t), 3: (p, q, v), 4: (t, p, v), \n         5: (v, p, q)}\n\n    return D[hi]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a logger with the given name.", "response": "def get_logger(name=\"peyotl\"):\n    \"\"\"Returns a logger with name set as given. See _read_logging_config for a description of the env var/config\n    file cascade that controls configuration of the logger.\n    \"\"\"\n    logger = logging.getLogger(name)\n    if len(logger.handlers) == 0:\n        log_init_warnings = []\n        lc = _read_logging_config(log_init_warnings)\n        logger.setLevel(lc['level'])\n        if lc['filepath'] is not None:\n            log_dir = lc['log_dir']\n            if log_dir and not os.path.exists(log_dir):\n                os.makedirs(log_dir)\n            ch = logging.FileHandler(lc['filepath'])\n        else:\n            ch = logging.StreamHandler()\n        ch.setLevel(lc['level'])\n        ch.setFormatter(lc['formatter'])\n        logger.addHandler(ch)\n        if log_init_warnings:\n            for w in log_init_warnings:\n                logger.warn(w)\n    return logger"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _logging_env_conf_overrides(log_init_warnings=None):\n    # This is called from a locked section of _read_logging_config, so don't call that function or you'll get deadlock\n    global _LOGGING_ENV_CONF_OVERRIDES\n    if _LOGGING_ENV_CONF_OVERRIDES is not None:\n        return _LOGGING_ENV_CONF_OVERRIDES\n    with _LOGGING_ENV_CONF_OVERRIDES_LOCK:\n        if _LOGGING_ENV_CONF_OVERRIDES is not None:\n            return _LOGGING_ENV_CONF_OVERRIDES\n        level_from_env = os.environ.get(\"PEYOTL_LOGGING_LEVEL\")\n        format_from_env = os.environ.get(\"PEYOTL_LOGGING_FORMAT\")\n        log_file_path_from_env = os.environ.get(\"PEYOTL_LOG_FILE_PATH\")\n        _LOGGING_ENV_CONF_OVERRIDES = {}\n        if level_from_env:\n            env_w_list = []\n            _get_logging_level(level_from_env, env_w_list)\n            if len(env_w_list) > 0:\n                if log_init_warnings is not None:\n                    log_init_warnings.extend(env_w_list)\n                    log_init_warnings.append('PEYOTL_LOGGING_LEVEL is invalid. Relying on setting from conf file.')\n            else:\n                _LOGGING_ENV_CONF_OVERRIDES.setdefault(\"logging\", {})['level'] = level_from_env\n        if format_from_env:\n            env_w_list = []\n            _get_logging_formatter(format_from_env, env_w_list)\n            if len(env_w_list) > 0:\n                if log_init_warnings is not None:\n                    log_init_warnings.extend(env_w_list)\n                    log_init_warnings.append('PEYOTL_LOGGING_FORMAT was invalid. Relying on setting from conf file.')\n            else:\n                _LOGGING_ENV_CONF_OVERRIDES.setdefault(\"logging\", {})['formatter'] = format_from_env\n        if log_file_path_from_env is not None:\n            _LOGGING_ENV_CONF_OVERRIDES.setdefault(\"logging\", {})['filepath'] = log_file_path_from_env\n        return _LOGGING_ENV_CONF_OVERRIDES", "response": "Returns a dictionary that contains the overrides of logging and logging settings that are set in the environment variables."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the logging configuration from the environment variables PEYOTL_LOGGING_LEVEL PEYOTL_LOGGING_FORMAT and PEYOTL_LOG_FILE_PATH.", "response": "def _read_logging_config(log_init_warnings=None):\n    \"\"\"Returns a dictionary (should be treated as immutable) of settings needed to configure a logger.\n    If PEYOTL_LOGGING_LEVEL, PEYOTL_LOGGING_FORMAT, and PEYOTL_LOG_FILE_PATH are all in the env, then\n        no config file will be read.\n    If PEYOTL_LOG_FILE_PATH is set to an empty string, then stderr will be used.\n    Otherwise the config will be read, and any of those env vars that are present will then override\n        the settings from the config file.\n    Crucial keys-value pairs are:\n    'level' -> logging.level as returned by _get_logging_level from the string obtained from PEYOTL_LOGGING_LEVEL\n        or config.logging.level\n    'formatter' -> None or a logging.Formatter as returned by _get_logging_format from the string obtained from\n        PEYOTL_LOGGING_FORMAT or config.logging.formatter\n    'filepath' -> None (for StreamHandler) or a filepath\n    'log_dir' -> None or the parent of the 'filepath' key\n\n    If log_init_warnings is a list, warnings pertaining reading the logging configuration will be appended to\n    the list.\n    This call is cached via a private global, so log_init_warnings is only used on the first call to the function.\n    \"\"\"\n    global _LOGGING_CONF\n    from peyotl.utility.get_config import get_config_object\n    if _LOGGING_CONF is not None:\n        return _LOGGING_CONF\n    try:\n        with _LOGGING_CONF_LOCK:\n            if _LOGGING_CONF is not None:\n                return _LOGGING_CONF\n            leco = _logging_env_conf_overrides(log_init_warnings).get('logging', {})\n            lc = {}\n            level_from_env = leco.get(\"level\")\n            format_from_env = leco.get(\"format\")\n            log_file_path_from_env = leco.get(\"filepath\")\n            level_enum = None\n            formatter = None\n            if not (level_from_env and format_from_env and log_file_path_from_env):\n                # If any aspect is missing from the env, then we need to check the config file\n                cfg = get_config_object()\n                level = cfg.get_config_setting('logging', 'level', 'WARNING', warn_on_none_level=None)\n                logging_format_name = cfg.get_config_setting('logging', 'formatter', 'NONE', warn_on_none_level=None)\n                logging_filepath = cfg.get_config_setting('logging', 'filepath', '', warn_on_none_level=None)\n                if logging_filepath == '':\n                    logging_filepath = None\n                lc['level_name'] = level\n                level_enum = _get_logging_level(level, log_init_warnings)\n                lc['formatter_name'] = logging_format_name\n                formatter = _get_logging_formatter(logging_format_name, log_init_warnings)\n                lc['filepath'] = logging_filepath\n            # Override\n            if level_from_env:\n                lc['level_name'] = level_from_env\n                level_enum = _get_logging_level(level_from_env)\n            if format_from_env:\n                lc['formatter_name'] = format_from_env\n                formatter = _get_logging_formatter(format_from_env)\n            if log_file_path_from_env is not None:\n                lc['filepath'] = log_file_path_from_env\n            fp = lc['filepath']\n            if not fp:\n                lc['filepath'] = None\n            lc['log_dir'] = os.path.split(fp)[0] if fp else None\n            lc['level'] = level_enum\n            lc['formatter'] = formatter\n            _LOGGING_CONF = lc\n            return _LOGGING_CONF\n    except Exception as x:\n        sys.stderr.write('Exception in peyotl.utility.get_logger._read_logging_config: {}'.format(str(x)))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list_lights(self, selector='all'):\n\n        return self.client.perform_request(\n            method='get', endpoint='lights/{}',\n            endpoint_args=[selector], parse_data=False)", "response": "Returns a list of all lights in the current user s account."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_state(self, selector='all',\n        power=None, color=None, brightness=None, duration=None):\n        \"\"\"Given a selector (defaults to all), set the state of a light.\n        Selector can be based on id, scene_id, group_id, label, etc.\n        Returns list of lightbulb statuses if successful.\n        See http://api.developer.lifx.com/v1/docs/selectors\n\n        selector: required String\n            The selector to limit which lights will run the effect.\n\n        power: String\n            e.g \"on\" or \"off\"\n\n        color: String\n            e.g #ff0000 or \"red\"\n            Color to set selected bulbs.\n            Hex color code, color name, saturation percentage, hue, RGB, etc.\n            See http://api.developer.lifx.com/v1/docs/colors\n\n        brightness: Double\n            e.g 0.5\n            Set brightness level from 0 to 1\n\n        duration: Double\n            e.g 10\n            Setting transition time, in seconds, from 0.0 to\n            3155760000.0 (100 years).\n        \"\"\"\n\n        argument_tuples = [\n            ('power', power),\n            ('color', color),\n            ('brightness', brightness),\n            ('duration', duration)\n        ]\n\n        return self.client.perform_request(\n            method='put', endpoint='lights/{}/state',\n            endpoint_args=[selector], argument_tuples=argument_tuples)", "response": "Set the state of a specific light."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\napply the modifications to lights state delta over a given period of time.", "response": "def state_delta(self, selector='all',\n        power=None, duration=1.0, infrared=None, hue=None,\n        saturation=None, brightness=None, kelvin=None):\n        \"\"\"Given a state delta, apply the modifications to lights' state\n        over a given period of time.\n\n        selector: required String\n            The selector to limit which lights are controlled.\n\n        power: String\n            The power state you want to set on the selector. on or off\n\n        duration: Double\n            How long in seconds you want the power action to take.\n            Range: 0.0 \u2013 3155760000.0 (100 years)\n\n        infrared: Double\n            The maximum brightness of the infrared channel.\n\n        hue: Double\n            Rotate the hue by this angle in degrees.\n\n        saturation: Double\n            Change the saturation by this additive amount; the resulting\n            saturation is clipped to [0, 1].\n\n        brightness: Double\n            Change the brightness by this additive amount; the resulting\n            brightness is clipped to [0, 1].\n\n        kelvin: Double\n            Change the kelvin by this additive amount; the resulting kelvin is\n            clipped to [2500, 9000].\n        \"\"\"\n\n        argument_tuples = [\n            (\"power\", power),\n            (\"duration\", duration),\n            (\"infrared\", infrared),\n            (\"hue\", hue),\n            (\"saturation\", saturation),\n            (\"brightness\", brightness),\n            (\"kelvin\", kelvin)\n        ]\n\n        return self.client.perform_request(\n            method='post', endpoint='lights/{}/state/delta',\n            endpoint_args=[selector], argument_tuples=argument_tuples)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntoggle power of a user s lights.", "response": "def toggle_power(self, selector='all', duration=1.0):\n        \"\"\"Given a selector and transition duration, toggle lights (on/off)\"\"\"\n\n        argument_tuples = [\n            (\"duration\", duration)\n        ]\n\n        return self.client.perform_request(\n            method='post', endpoint='lights/{}/toggle',\n            endpoint_args=[selector], argument_tuples=argument_tuples)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef breathe_lights(self, color, selector='all',\n        from_color=None, period=1.0, cycles=1.0,\n        persist=False, power_on=True, peak=0.5):\n        \"\"\"Perform breathe effect on lights.\n\n        selector: String\n            The selector to limit which lights will run the effect.\n            default: all\n\n        color: required String\n            Color attributes to use during effect. See set_state for more.\n\n        from_color:\tString\n            The color to start the effect from. See set_state for more.\n            default: current bulb color\n\n        period:\tDouble\n            The time in seconds for one cyles of the effect.\n            default: 1.0\n\n        cycles:\tDouble\n            The number of times to repeat the effect.\n            default: 1.0\n\n        persist: Boolean\n            If false set the light back to its previous\n            value when effect ends, if true leave the last effect color.\n            default: false\n\n        power_on: Boolean\n            If true, turn the bulb on if it is not already on.\n            default: true\n\n        peak: String\n            Defines where in a period the target color is at its maximum.\n            Minimum 0.0, maximum 1.0.\n            default: 0.5\n        \"\"\"\n\n        argument_tuples = [\n            (\"color\", color),\n            (\"from_color\", from_color),\n            (\"period\", period),\n            (\"cycles\", cycles),\n            (\"persist\", persist),\n            (\"power_on\", power_on),\n            (\"peak\", peak),\n        ]\n\n        return self.client.perform_request(\n            method='post', endpoint='lights/{}/effects/breathe',\n            endpoint_args=[selector], argument_tuples=argument_tuples)", "response": "Perform breathe effect on lights."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cycle_lights(self, states,\n        defaults, direction='forward', selector='all'):\n        \"\"\"Cycle through list of effects.\n\n        Provide array states as a list of dictionaries with set_state arguments.\n        See http://api.developer.lifx.com/docs/cycle\n\n        selector: String\n            The selector to limit which lights will run the effect.\n            default: all\n\n        states: required List of Dicts\n            List of arguments, named as per set_state. Must have 2 to 5 entries.\n\n        defaults: Object\n            Default values to use when not specified in each states[] object.\n            Argument names as per set_state.\n\n        direction: String\n            Direction in which to cycle through the list. Can be forward or backward\n            default: forward\n        \"\"\"\n\n        argument_tuples = [\n            (\"states\", states),\n            (\"defaults\", defaults),\n            (\"direction\", direction)\n        ]\n\n        return self.client.perform_request(\n            method='post', endpoint='lights/{}/cycle', endpoint_args=[selector],\n            argument_tuples=argument_tuples, json_body=True)", "response": "Cycle through list of effects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef activate_scene(self, scene_uuid, duration=1.0):\n\n        argument_tuples = [\n            (\"duration\", duration),\n        ]\n\n        return self.client.perform_request(\n            method='put', endpoint='scenes/scene_id:{}/activate',\n            endpoint_args=[scene_uuid], argument_tuples=argument_tuples)", "response": "Activate a scene.\n\n        See http://api.developer.lifx.com/docs/activate-scene\n\n        scene_uuid: required String\n            The UUID for the scene you wish to activate\n\n        duration: Double\n            The time in seconds to spend performing the scene transition.\n            default: 1.0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef visitBuiltinValueType(self, ctx: jsgParser.BuiltinValueTypeContext):\n        self._value_type_text = ctx.getText()\n        self._typeinfo = self.parserTypeToImplClass[self._value_type_text]", "response": "visitBuiltinValueType - Gets the typeinfo from the parser typeToImplClass mapping."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef count_num_trees(nexson, nexson_version=None):\n    if nexson_version is None:\n        nexson_version = detect_nexson_version(nexson)\n    nex = get_nexml_el(nexson)\n    num_trees_by_group = []\n    if _is_by_id_hbf(nexson_version):\n        for tree_group in nex.get('treesById', {}).values():\n            nt = len(tree_group.get('treeById', {}))\n            num_trees_by_group.append(nt)\n    else:\n        trees_group = nex.get('trees', [])\n        if isinstance(trees_group, dict):\n            trees_group = [trees_group]\n        for tree_group in trees_group:\n            t = tree_group.get('tree')\n            if isinstance(t, list):\n                nt = len(t)\n            else:\n                nt = 1\n            num_trees_by_group.append(nt)\n    return sum(num_trees_by_group)", "response": "Returns the number of trees summed across all tree\n    groups."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a _TreeCollectionStore object.", "response": "def TreeCollectionStore(repos_dict=None,\n                        repos_par=None,\n                        with_caching=True,\n                        assumed_doc_version=None,\n                        git_ssh=None,\n                        pkey=None,\n                        git_action_class=TreeCollectionsGitAction,\n                        mirror_info=None,\n                        infrastructure_commit_author='OpenTree API <api@opentreeoflife.org>'):\n    \"\"\"Factory function for a _TreeCollectionStore object.\n\n    A wrapper around the _TreeCollectionStore class instantiation for\n    the most common use case: a singleton _TreeCollectionStore.\n    If you need distinct _TreeCollectionStore objects, you'll need to\n    call that class directly.\n    \"\"\"\n    global _THE_TREE_COLLECTION_STORE\n    if _THE_TREE_COLLECTION_STORE is None:\n        _THE_TREE_COLLECTION_STORE = _TreeCollectionStore(repos_dict=repos_dict,\n                                                          repos_par=repos_par,\n                                                          with_caching=with_caching,\n                                                          assumed_doc_version=assumed_doc_version,\n                                                          git_ssh=git_ssh,\n                                                          pkey=pkey,\n                                                          git_action_class=git_action_class,\n                                                          mirror_info=mirror_info,\n                                                          infrastructure_commit_author=infrastructure_commit_author)\n    return _THE_TREE_COLLECTION_STORE"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_new_collection(self,\n                           owner_id,\n                           json_repr,\n                           auth_info,\n                           collection_id=None,\n                           commit_msg=''):\n        \"\"\"Validate and save this JSON. Ensure (and return) a unique collection id\"\"\"\n        collection = self._coerce_json_to_collection(json_repr)\n        if collection is None:\n            msg = \"File failed to parse as JSON:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not self._is_valid_collection_json(collection):\n            msg = \"JSON is not a valid collection:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if collection_id:\n            # try to use this id\n            found_owner_id, slug = collection_id.split('/')\n            assert found_owner_id == owner_id\n        else:\n            # extract a working title and \"slugify\" it\n            slug = self._slugify_internal_collection_name(json_repr)\n            collection_id = '{i}/{s}'.format(i=owner_id, s=slug)\n        # Check the proposed id for uniqueness in any case. Increment until\n        # we have a new id, then \"reserve\" it using a placeholder value.\n        with self._index_lock:\n            while collection_id in self._doc2shard_map:\n                collection_id = increment_slug(collection_id)\n            self._doc2shard_map[collection_id] = None\n        # pass the id and collection JSON to a proper git action\n        new_collection_id = None\n        r = None\n        try:\n            # assign the new id to a shard (important prep for commit_and_try_merge2master)\n            gd_id_pair = self.create_git_action_for_new_collection(new_collection_id=collection_id)\n            new_collection_id = gd_id_pair[1]\n            try:\n                # let's remove the 'url' field; it will be restored when the doc is fetched (via API)\n                del collection['url']\n                # keep it simple (collection is already validated! no annotations needed!)\n                r = self.commit_and_try_merge2master(file_content=collection,\n                                                     doc_id=new_collection_id,\n                                                     auth_info=auth_info,\n                                                     parent_sha=None,\n                                                     commit_msg=commit_msg,\n                                                     merged_sha=None)\n            except:\n                self._growing_shard.delete_doc_from_index(new_collection_id)\n                raise\n        except:\n            with self._index_lock:\n                if new_collection_id in self._doc2shard_map:\n                    del self._doc2shard_map[new_collection_id]\n            raise\n        with self._index_lock:\n            self._doc2shard_map[new_collection_id] = self._growing_shard\n        return new_collection_id, r", "response": "Validate and save this JSON. Ensure that a unique collection id is provided."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nvalidate and save this JSON and return a new version of the existing collection.", "response": "def update_existing_collection(self,\n                                   owner_id,\n                                   collection_id=None,\n                                   json_repr=None,\n                                   auth_info=None,\n                                   parent_sha=None,\n                                   merged_sha=None,\n                                   commit_msg=''):\n        \"\"\"Validate and save this JSON. Ensure (and return) a unique collection id\"\"\"\n        collection = self._coerce_json_to_collection(json_repr)\n        if collection is None:\n            msg = \"File failed to parse as JSON:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not self._is_valid_collection_json(collection):\n            msg = \"JSON is not a valid collection:\\n{j}\".format(j=json_repr)\n            raise ValueError(msg)\n        if not collection_id:\n            raise ValueError(\"Collection id not provided (or invalid)\")\n        if not self.has_doc(collection_id):\n            msg = \"Unexpected collection id '{}' (expected an existing id!)\".format(collection_id)\n            raise ValueError(msg)\n        # pass the id and collection JSON to a proper git action\n        r = None\n        try:\n            # remove any 'url' field before saving; it will be restored when the doc is fetched (via API)\n            if 'url' in collection:\n                del collection['url']\n            # keep it simple (collection is already validated! no annotations needed!)\n            r = self.commit_and_try_merge2master(file_content=collection,\n                                                 doc_id=collection_id,\n                                                 auth_info=auth_info,\n                                                 parent_sha=parent_sha,\n                                                 commit_msg=commit_msg,\n                                                 merged_sha=merged_sha)\n            # identify shard for this id!?\n        except:\n            raise\n        return r"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _slugify_internal_collection_name(self, json_repr):\n        collection = self._coerce_json_to_collection(json_repr)\n        if collection is None:\n            return None\n        internal_name = collection['name']\n        return slugify(internal_name)", "response": "Parse the JSON and return a slug of its name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the JSON representation of a resource is valid.", "response": "def _is_valid_collection_json(self, json_repr):\n        \"\"\"Call the primary validator for a quick test\"\"\"\n        collection = self._coerce_json_to_collection(json_repr)\n        if collection is None:\n            # invalid JSON, definitely broken\n            return False\n        aa = validate_collection(collection)\n        errors = aa[0]\n        for e in errors:\n            _LOG.debug('> invalid JSON: {m}'.format(m=e.encode('utf-8')))\n        if len(errors) > 0:\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _coerce_json_to_collection(self, json_repr):\n        if isinstance(json_repr, dict):\n            collection = json_repr\n        else:\n            try:\n                collection = anyjson.loads(json_repr)\n            except:\n                _LOG.warn('> invalid JSON (failed anyjson parsing)')\n                return None\n        return collection", "response": "Use to ensure that a JSON string is parsed to the equivalent dict in python."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the address of a specific calendar.", "response": "def address(address=None, begin=None, end=None):\n    '''\n    HTTP REQUEST\n\n    GET https://api.nasa.gov/planetary/earth/temperature/address\n\n    QUERY PARAMETERS\n\n    Parameter\tType\tDefault\tDescription\n    text\tstring\tn/a\tAddress string\n    begin\tint\t1880\tbeginning year for date range, inclusive\n    end\tint\t2014\tend year for date range, inclusive\n    api_key\tstring\tDEMO_KEY\tapi.nasa.gov key for expanded usage\n    EXAMPLE QUERY\n\n    https://api.nasa.gov/planetary/earth/temperature/address?text=1800 F Street, NW, Washington DC&begin=1990\n    '''\n    base_url = \"https://api.nasa.gov/planetary/earth/temperature/address?\"\n\n    if not address:\n        raise ValueError(\n            \"address is missing, which is mandatory. example : 1800 F Street, NW, Washington DC\")\n    elif not isinstance(address, str):\n        try:\n            address = str(address)\n        except:\n            raise ValueError(\"address has to be type of string\")\n    else:\n        base_url += \"text=\" + address + \"&\"\n\n    if not begin:\n        raise ValueError(\n            \"Begin year is missing, which is mandatory. Format : YYYY\")\n    else:\n        try:\n            validate_year(begin)\n            base_url += \"begin=\" + begin + \"&\"\n        except:\n            raise ValueError(\"Incorrect begin year format, should be YYYY\")\n\n    if end:\n        try:\n            validate_year(end)\n            base_url += \"end=\" + end + \"&\"\n        except:\n            raise ValueError(\"Incorrect end year format, should be YYYY\")\n\n    req_url = base_url + \"api_key=\" + nasa_api_key()\n\n    return dispatch_http_get(req_url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsearches LAN for available Roku devices. Returns a Roku object.", "response": "def discover_roku():\n    \"\"\" Search LAN for available Roku devices. Returns a Roku object. \"\"\"\n\n    print(\"Searching for Roku devices within LAN ...\")\n    rokus = Roku.discover()\n    if not rokus:\n        print(\"Unable to discover Roku devices. \" +\n              \"Try again, or manually specify the IP address with \" +\n              \"\\'roku <ipaddr>\\' (e.g. roku 192.168.1.130)\")\n        return None\n\n    print(\"Found the following Roku devices:\")\n    for i, r in enumerate(rokus):\n        # dinfo = ' '.join(re.split(', |: ', str(r.device_info))[1:3])\n        dinfo = ''\n        print(\"[\" + str(i+1) + \"]   \" + str(r.host) + \":\" +\n              str(r.port) + ' (' + dinfo + ')')\n    print(\"\")\n\n    if len(rokus) == 1:\n        print(\"Selecting Roku 1 by default\")\n        return rokus[0]\n    else:\n        print(\"Multiple Rokus found. Select the index of the Roku to control:\")\n\n        while True:\n            try:\n                query = \"Select (1 to \" + str(len(rokus)) + \") > \"\n                sel = int(input(query)) - 1\n                if sel >= len(rokus):\n                    raise ValueError\n                else:\n                    break\n            except ValueError:\n                print(\"Invalid selection\")\n\n        return rokus[sel]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ot_tnrs_match_names(name_list,\n                        context_name=None,\n                        do_approximate_matching=True,\n                        include_dubious=False,\n                        include_deprecated=True,\n                        tnrs_wrapper=None):\n    \"\"\"Uses a peyotl wrapper around an Open Tree web service to get a list of OTT IDs matching\n    the `name_list`.\n    The tnrs_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.\n    All other arguments correspond to the arguments of the web-service call.\n    A ValueError will be raised if the `context_name` does not match one of the valid names for a\n        taxonomic context.\n    This uses the wrap_response option to create and return a TNRSRespose object around the response.\n    \"\"\"\n    if tnrs_wrapper is None:\n        from peyotl.sugar import tnrs\n        tnrs_wrapper = tnrs\n    match_obj = tnrs_wrapper.match_names(name_list,\n                                         context_name=context_name,\n                                         do_approximate_matching=do_approximate_matching,\n                                         include_deprecated=include_deprecated,\n                                         include_dubious=include_dubious,\n                                         wrap_response=True)\n    return match_obj", "response": "Uses a peyotl wrapper around an Open Tree web service to get a list of TNRS IDs matching the names in the name_list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_taxon_info(taxon, include_anc, output):\n    output.write('Taxon info for OTT ID (ot:ottId) = {}\\n'.format(taxon.ott_id))\n    output.write('    name (ot:ottTaxonName) = \"{}\"\\n'.format(taxon.name))\n    if taxon.synonyms:\n        output.write('    known synonyms: \"{}\"\\n'.format('\", \"'.join(taxon.synonyms)))\n    else:\n        output.write('    known synonyms: \\n')\n    output.write('    OTT flags for this taxon: {}\\n'.format(taxon.flags))\n    output.write('    The taxonomic rank associated with this name is: {}\\n'.format(taxon.rank))\n    output.write(\n        '    The (unstable) node ID in the current taxomachine instance is: {}\\n'.format(taxon.taxomachine_node_id))\n    if include_anc:\n        if taxon.parent is not None:\n            output.write('Taxon {c} is a child of {p}.\\n'.format(c=taxon.ott_id, p=taxon.parent.ott_id))\n            write_taxon_info(taxon.parent, True, output)\n        else:\n            output.write('Taxon {c} is the root of the taxonomy.'.format(c=taxon.ott_id))", "response": "Writes out information about a taxon object to the output stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match_and_print(name_list, context_name, do_approximate_matching, include_dubious, include_deprecated,\n                    include_subtree, output):\n    \"\"\"Demonstrates how to read the response from a match_names query when peyotl's wrap_response option is\n    used.\n\n    If the context_name is not recognized, the attempt to match_names will generate a ValueError exception.\n    Here this is caught, and we call the tnrs/contexts web service to get the list of valid context_names\n        to provide the user of the script with some hints.\n    \"\"\"\n    from peyotl.sugar import tnrs\n    try:\n        # Perform the match_names, and return the peyotl wrapper around the response.\n        result = ot_tnrs_match_names(name_list,\n                                     context_name=context_name,\n                                     do_approximate_matching=do_approximate_matching,\n                                     include_dubious=include_dubious,\n                                     include_deprecated=include_deprecated,\n                                     tnrs_wrapper=tnrs)\n    except Exception as x:\n        msg = str(x)\n        if 'is not a valid context name' in msg and context_name is not None:\n            # Here is a wrapper around the call to get the context names\n            valid_contexts = tnrs.contexts()\n            m = 'The valid context names are the strings in the values of the following \"tnrs/contexts\" dict:\\n'\n            sys.stderr.write(m)\n            epp = pprint.PrettyPrinter(indent=4, stream=sys.stderr)\n            epp.pprint(valid_contexts)\n        raise RuntimeError('ot-tnrs-match-names: exception raised. {}'.format(x))\n    # The code below demonstrates how to access the information from the response in the wrapper\n    #   that is created by using the wrap_response option in the call\n    output.write('A v2/tnrs/match_names query was performed using: {} \\n'.format(tnrs.endpoint))\n    output.write('The taxonomy being served by that server is:')\n    output.write(' {}'.format(result.taxonomy.source))\n    output.write(' by {}\\n'.format(result.taxonomy.author))\n    output.write('Information for the taxonomy can be found at {}\\n'.format(result.taxonomy.weburl))\n    output.write('{} out of {} queried name(s) were matched\\n'.format(len(result.matched_name_ids), len(name_list)))\n    output.write('{} out of {} queried name(s) were unambiguously matched\\n'.format(len(result.unambiguous_name_ids),\n                                                                                    len(name_list)))\n    output.write('The context_name for the matched names was \"{}\"'.format(result.context))\n    if result.context_inferred:\n        output.write(' (this context was inferred based on the matches).\\n')\n    else:\n        output.write(' (this context was supplied as an argument to speed up the name matching).\\n')\n    output.write('The name matching result(s) used approximate/fuzzy string matching? {}\\n'.format(\n        result.includes_approximate_matches))\n    output.write('The name matching result(s) included dubious names? {}\\n'.format(result.includes_dubious_names))\n    output.write('The name matching result(s) included deprecated taxa? {}\\n'.format(result.includes_deprecated_taxa))\n    for name in name_list:\n        match_tuple = result[name]\n        output.write('The query name \"{}\" produced {} result(s):\\n'.format(name, len(match_tuple)))\n        for match_ind, match in enumerate(match_tuple):\n            output.write('  Match #{}\\n'.format(match_ind))\n            output.write('    OTT ID (ot:ottId) = {}\\n'.format(match.ott_id))\n            output.write('    name (ot:ottTaxonName) = \"{}\"\\n'.format(match.name))\n            output.write('    query was matched using fuzzy/approximate string matching? {}\\n'.format(\n                match.is_approximate_match))\n            output.write('    match score = {}\\n'.format(match.score))\n            output.write('    query name is a junior synonym of this match? {}\\n'.format(match.is_synonym))\n            output.write('    is deprecated from OTT? {}\\n'.format(match.is_deprecated))\n            output.write('    is dubious taxon? {}\\n'.format(match.is_dubious))\n            if match.synonyms:\n                output.write('    known synonyms: \"{}\"\\n'.format('\", \"'.join(match.synonyms)))\n            else:\n                output.write('    known synonyms: \\n')\n            output.write('    OTT flags for this taxon: {}\\n'.format(match.flags))\n            output.write('    The taxonomic rank associated with this name is: {}\\n'.format(match.rank))\n            output.write('    The nomenclatural code for this name is: {}\\n'.format(match.nomenclature_code))\n            output.write('    The (unstable) node ID in the current taxomachine instance is: {}\\n'.format(\n                match.taxomachine_node_id))\n        if len(match_tuple) == 1:\n            sys.stderr.write('\\nOnly one match found, so we will request the info on the ancestors, too...\\n')\n            match = match_tuple[0]\n            ott_id = match.ott_id\n            fetch_and_write_taxon_info(id_list=[ott_id], include_anc=True, list_tips=False, output=output)\n            if include_subtree:\n                from peyotl.sugar import taxonomy\n                subtree = taxonomy.subtree(ott_id)['subtree']\n                output.write('The taxononmic subtree is:\\n')\n                output.write(subtree)\n                output.write('\\n')\n        else:\n            if include_subtree:\n                sys.stderr.write(\n                    '\\nMultiple matches found - ancestor info and subtreesuppressed.\\nSee ot-taxon-info.py and ot-taxon-subtree.py which can be called with an OTT ID\\n')\n            else:\n                sys.stderr.write(\n                    '\\nMultiple matches found - ancestor info suppressed.\\nSee ot-taxon-info.py which can be called with an OTT ID\\n')", "response": "This function is used to match the names of a taxonomy in a specific context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmakes value suitable for a binding object.", "response": "def _objectify(field, value, ns_info):\n    \"\"\"Make `value` suitable for a binding object.\n\n    If `value` is an Entity, call to_obj() on it. Otherwise, pass it\n    off to the TypedField for an appropriate value.\n    \"\"\"\n    if (getattr(field.type_, \"_treat_none_as_empty_list\", False) and\n            value is None):\n        return []\n\n    if value is None:\n        return None\n    elif field.type_:\n        return value.to_obj(ns_info=ns_info)\n    return field.binding_value(value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dictify(field, value):\n    if value is None:\n        return None\n    elif field.type_:\n        return value.to_dict()\n    return field.dict_value(value)", "response": "Make value suitable for a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef instance(cls, key, *args, **kwargs):\n        klass = cls.entity_class(key)\n        return klass(*args, **kwargs)", "response": "Create an instance of the class associated with the key and initialize it with the given arguments and keyword arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the dictionary representation of an Entity object and return an Entity instance.", "response": "def from_dict(cls, cls_dict, fallback_xsi_type=None):\n        \"\"\"Parse the dictionary and return an Entity instance.\n\n        This will attempt to extract type information from the input\n        dictionary and pass it to entity_class to resolve the correct class\n        for the type.\n\n        Args:\n            cls_dict: A dictionary representation of an Entity object.\n            fallback_xsi_type: An xsi_type to use for string input, which\n            doesn't have properties\n\n        Returns:\n            An Entity instance.\n        \"\"\"\n        if not cls_dict:\n            return None\n        \n        if isinstance(cls_dict, six.string_types):\n            if not getattr(cls, \"_convert_strings\", False):\n                return cls_dict\n\n        try:\n            typekey = cls.dictkey(cls_dict)\n        except TypeError:\n            typekey = fallback_xsi_type\n        klass   = cls.entity_class(typekey)\n        return klass.from_dict(cls_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_obj(cls, cls_obj):\n        if not cls_obj:\n            return None\n\n        typekey = cls.objkey(cls_obj)\n        klass   = cls.entity_class(typekey)\n        return klass.from_obj(cls_obj)", "response": "Parse the generateDS object and return an Entity instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of this entity s TypedFields.", "response": "def typed_fields(cls):\n        \"\"\"Return a tuple of this entity's TypedFields.\"\"\"\n\n        # Checking cls._typed_fields could return a superclass _typed_fields\n        # value. So we check our class __dict__ which does not include\n        # inherited attributes.\n        klassdict = cls.__dict__\n\n        try:\n            return klassdict[\"_typed_fields\"]\n        except KeyError:\n            fields = cls.typed_fields_with_attrnames()\n            cls._typed_fields = tuple(field for _, field in fields)\n        return cls._typed_fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef typed_fields_with_attrnames(cls):\n        # Checking cls._typed_fields could return a superclass _typed_fields\n        # value. So we check our class __dict__ which does not include\n        # inherited attributes.\n        klassdict = cls.__dict__\n\n        try:\n            return klassdict[\"_typed_fields_with_attrnames\"]\n        except KeyError:\n            # No typed_fields set on this Entity yet. Find them and store\n            # them in the _typed_fields class attribute.\n            from . import fields\n            typed_fields = tuple(fields.iterfields(cls))\n            cls._typed_fields_with_attrnames = typed_fields\n        return typed_fields", "response": "Return a list of ( TypedField attribute name TypedField object ) tuples for this Entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting this Entity to a GenerateDS binding object.", "response": "def to_obj(self, ns_info=None):\n        \"\"\"Convert to a GenerateDS binding object.\n\n        Subclasses can override this function.\n\n        Returns:\n            An instance of this Entity's ``_binding_class`` with properties\n            set from this Entity.\n        \"\"\"\n        if ns_info:\n            ns_info.collect(self)\n\n        # null behavior for classes that inherit from Entity but do not\n        # have _binding_class\n        if not hasattr(self, \"_binding_class\"):\n            return None\n\n        entity_obj = self._binding_class()\n\n        for field, val in six.iteritems(self._fields):\n            # EntityLists with no list items should be dropped\n            if isinstance(val, EntityList) and len(val) == 0:\n                val = None\n            elif field.multiple:\n                if val:\n                    val = [_objectify(field, x, ns_info) for x in val]\n                else:\n                    val = []\n            else:\n                val = _objectify(field, val, ns_info)\n\n            setattr(entity_obj, field.name, val)\n\n        self._finalize_obj(entity_obj)\n        return entity_obj"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting to a dict.", "response": "def to_dict(self):\n        \"\"\"Convert to a ``dict``\n\n        Subclasses can override this function.\n\n        Returns:\n            Python dict with keys set from this Entity.\n        \"\"\"\n        entity_dict = {}\n\n        for field, val in six.iteritems(self._fields):\n            if field.multiple:\n                if val:\n                    val = [_dictify(field, x) for x in val]\n                else:\n                    val = []\n            else:\n                val = _dictify(field, val)\n\n            # Only add non-None objects or non-empty lists\n            if val is not None and val != []:\n                entity_dict[field.key_name] = val\n\n        self._finalize_dict(entity_dict)\n\n        return entity_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nserializes a : class:`Entity` instance to an XML string.", "response": "def to_xml(self, include_namespaces=True, namespace_dict=None,\n               pretty=True, encoding=\"utf-8\"):\n        \"\"\"Serializes a :class:`Entity` instance to an XML string.\n\n        The default character encoding is ``utf-8`` and can be set via the\n        `encoding` parameter.\n\n        If `encoding` is ``None``, a unicode string is returned. That is, a\n        :class:`unicode` in Python 2 and a :class:`str` in Python 3.\n\n        Notes:\n            If you need to print a byte string in Python 3, you will need to\n            decode it using using the :class:`str`.\n\n        Args:\n            include_namespaces (bool): whether to include xmlns and\n                xsi:schemaLocation attributes on the root element. Set to true by\n                default.\n            namespace_dict (dict): mapping of additional XML namespaces to\n                prefixes\n            pretty (bool): whether to produce readable (``True``) or compact\n                (``False``) output. Defaults to ``True``.\n            encoding: The output character encoding. Default character encoding\n                is ``utf-8``.\n\n        Returns:\n            A byte string containing the XML representation for this :class:`Entity`\n            instance if an encoding was provided. If encoding is set to\n            ``None`` a unicode string containing the XML representation will be\n            returned.\n\n        \"\"\"\n        namespace_def = \"\"\n\n        ns_collector = NamespaceCollector()\n        gds_obj = self.to_obj(ns_info=ns_collector if include_namespaces else None)\n\n        if include_namespaces:\n            ns_collector.finalize(namespace_dict)\n            delim = \"\\n\\t\" if pretty else \" \"\n            namespace_def = ns_collector.get_xmlns_string(delim) + delim + \\\n                ns_collector.get_schema_location_string(delim)\n\n        with save_encoding(encoding):\n            sio = six.StringIO()\n            gds_obj.export(\n                sio.write,\n                0,\n                namespacedef_=namespace_def,\n                pretty_print=pretty\n            )\n\n        # Ensure that the StringIO buffer is unicode\n        s = six.text_type(sio.getvalue())\n\n        if encoding:\n            return s.encode(encoding)\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a JSON string and build an entity.", "response": "def from_json(cls, json_doc):\n        \"\"\"Parse a JSON string and build an entity.\"\"\"\n        try:\n            d = json.load(json_doc)\n        except AttributeError:  # catch the read() error\n            d = json.loads(json_doc)\n\n        return cls.from_dict(d)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _multiple_field(cls):\n        klassdict = cls.__dict__\n\n        try:\n            # Checking for cls.entitylist_multifield would return any inherited\n            # values, so we check the class __dict__ explicitly.\n            return klassdict[\"_entitylist_multifield\"][0]\n        except (KeyError, IndexError, TypeError):\n            from . import fields\n            multifield_tuple = tuple(fields.find(cls, multiple=True))\n            assert len(multifield_tuple) == 1\n\n            # Make sure that the multiple field actually has an Entity type.\n            multifield = multifield_tuple[0]\n            assert issubclass(multifield.type_, Entity)\n\n            # Store aside the multiple field. We wrap it in a tuple because\n            # just doing ``cls._entitylist_multifield = multifield`` would\n            # assign another TypedField descriptor to this class. We don't\n            # want that.\n            cls._entitylist_multifield =  multifield_tuple\n\n            # Return the multiple TypedField\n            return multifield_tuple[0]", "response": "Return the multiple TypedField associated with this EntityList."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattempt to resolve issues where our samples use 'http://example. com/' for our example namespace but python - stix uses 'http://example. com/' for our example namespace but python - stix uses 'http://example. com/' for our example namespace but python - stix uses 'http://example. com/' by removing the former.", "response": "def _fix_example_namespace(self):\n        \"\"\"Attempts to resolve issues where our samples use\n        'http://example.com/' for our example namespace but python-stix uses\n        'http://example.com' by removing the former.\n        \"\"\"\n        example_prefix = 'example'  # Example ns prefix\n        idgen_prefix = idgen.get_id_namespace_prefix()\n\n        # If the ID namespace alias doesn't match the example alias, return.\n        if idgen_prefix != example_prefix:\n            return\n\n        # If the example namespace prefix isn't in the parsed namespace\n        # prefixes, return.\n        if example_prefix not in self._input_namespaces:\n            return\n\n        self._input_namespaces[example_prefix] = idgen.EXAMPLE_NAMESPACE.name"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _finalize_namespaces(self, ns_dict=None):\n        if ns_dict:\n            # Add the user's entries to our set\n            for ns, alias in six.iteritems(ns_dict):\n                self._collected_namespaces.add_namespace_uri(ns, alias)\n\n        # Add the ID namespaces\n        self._collected_namespaces.add_namespace_uri(\n            ns_uri=idgen.get_id_namespace(),\n            prefix=idgen.get_id_namespace_alias()\n        )\n\n        # Remap the example namespace to the one expected by the APIs if the\n        # sample example namespace is found.\n        self._fix_example_namespace()\n\n        # Add _input_namespaces\n        for prefix, uri in six.iteritems(self._input_namespaces):\n            self._collected_namespaces.add_namespace_uri(uri, prefix)\n\n        # Add some default XML namespaces to make sure they're there.\n        self._collected_namespaces.import_from(namespaces.XML_NAMESPACES)\n\n        # python-stix's generateDS-generated binding classes can't handle\n        # default namespaces.  So make sure there are no preferred defaults in\n        # the set.  Get prefixes from the global namespace set if we have to.\n        for ns_uri in self._collected_namespaces.namespace_uris:\n            preferred_prefix = self._collected_namespaces.preferred_prefix_for_namespace(ns_uri)\n\n            if preferred_prefix:\n                continue\n\n            # No preferred prefix set for namespace. Try to assign one.\n            prefixes = self._collected_namespaces.get_prefixes(ns_uri)\n\n            if prefixes:\n                prefix = next(iter(prefixes))\n            else:\n                prefix = namespaces.lookup_name(ns_uri)\n\n            if prefix is None:\n                raise namespaces.NoPrefixesError(ns_uri)\n\n            self._collected_namespaces.set_preferred_prefix_for_namespace(\n                ns_uri=ns_uri,\n                prefix=prefix,\n                add_if_not_exist=True\n            )", "response": "Finalizes the set of namespaces that are needed by the user."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining whether the element conforms to etype", "response": "def element_conforms(element, etype) -> bool:\n    \"\"\" Determine whether element conforms to etype\"\"\"\n    from pyjsg.jsglib import Empty\n\n    if isinstance(element, etype):\n        return True\n    # This catches the Optional[] idiom\n    if (element is None or element is Empty) and issubclass(etype, type(None)):\n        return True\n    elif element is Empty:\n        return False\n    elif isinstance(etype, type(type)) and (issubclass(etype, type(None))):\n        return element is None\n    elif element is None:\n        return False\n    else:\n        return isinstance(element, etype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the given element is conforms to the given type and namespace.", "response": "def conforms(element, etype, namespace: Dict[str, Any]) -> bool:\n    \"\"\" Determine whether element conforms to etype\n\n    :param element: Element to test for conformance\n    :param etype: Type to test against\n    :param namespace: Namespace to use to resolve forward references\n    :return:\n    \"\"\"\n    etype = proc_forward(etype, namespace)\n    if is_union(etype):\n        return union_conforms(element, etype, namespace, conforms)\n    else:\n        return element_conforms(element, etype)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of all the communities.", "response": "def get(self, query, sort, page, size):\n        \"\"\"Get a list of all the communities.\n\n        .. http:get:: /communities/(string:id)\n            Returns a JSON list with all the communities.\n            **Request**:\n            .. sourcecode:: http\n                GET /communities HTTP/1.1\n                Accept: application/json\n                Content-Type: application/json\n                Host: localhost:5000\n            :reqheader Content-Type: application/json\n            **Response**:\n            .. sourcecode:: http\n                HTTP/1.0 200 OK\n                Content-Length: 334\n                Content-Type: application/json\n                [\n                    {\n                        \"id\": \"comm1\"\n                    },\n                    {\n                        \"id\": \"comm2\"\n                    }\n                ]\n            :resheader Content-Type: application/json\n            :statuscode 200: no error\n        \"\"\"\n        urlkwargs = {\n            'q': query,\n            'sort': sort,\n            'size': size,\n        }\n\n        communities = Community.filter_communities(query, sort)\n        page = communities.paginate(page, size)\n\n        links = default_links_pagination_factory(page, urlkwargs)\n\n        links_headers = map(lambda key: ('link', 'ref=\"{0}\" href=\"{1}\"'.format(\n            key, links[key])), links)\n\n        return self.make_response(\n            page,\n            headers=links_headers,\n            links_item_factory=default_links_item_factory,\n            page=page,\n            urlkwargs=urlkwargs,\n            links_pagination_factory=default_links_pagination_factory,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, community_id):\n        community = Community.get(community_id)\n        if not community:\n            abort(404)\n        etag = community.version_id\n        self.check_etag(etag)\n        response = self.make_response(\n            community, links_item_factory=default_links_item_factory)\n        response.set_etag(etag)\n        return response", "response": "Get the details of the specified community."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Phylesystem(repos_dict=None,\n                repos_par=None,\n                with_caching=True,\n                repo_nexml2json=None,\n                git_ssh=None,\n                pkey=None,\n                git_action_class=PhylesystemGitAction,\n                mirror_info=None,\n                new_study_prefix=None,\n                infrastructure_commit_author='OpenTree API <api@opentreeoflife.org>'):\n    \"\"\"Factory function for a _Phylesystem object.\n\n    A wrapper around the _Phylesystem class instantiation for\n    the most common use case: a singleton _Phylesystem.\n    If you need distinct _Phylesystem objects, you'll need to\n    call that class directly.\n    \"\"\"\n    if not repo_nexml2json:\n        repo_nexml2json = get_config_setting('phylesystem', 'repo_nexml2json')\n    global _THE_PHYLESYSTEM\n    if _THE_PHYLESYSTEM is None:\n        _THE_PHYLESYSTEM = _Phylesystem(repos_dict=repos_dict,\n                                        repos_par=repos_par,\n                                        with_caching=with_caching,\n                                        repo_nexml2json=repo_nexml2json,\n                                        git_ssh=git_ssh,\n                                        pkey=pkey,\n                                        git_action_class=git_action_class,\n                                        mirror_info=mirror_info,\n                                        new_study_prefix=new_study_prefix,\n                                        infrastructure_commit_author=infrastructure_commit_author)\n    return _THE_PHYLESYSTEM", "response": "Creates a new _Phylesystem object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_configuration(self, out, secret_attrs=False):\n        key_order = ['repo_nexml2json',\n                     'number_of_shards',\n                     'initialization', ]\n        cd = self.get_configuration_dict(secret_attrs=secret_attrs)\n        for k in key_order:\n            if k in cd:\n                out.write('  {} = {}'.format(k, cd[k]))\n        for n, shard in enumerate(self._shards):\n            out.write('Shard {}:\\n'.format(n))\n            shard.write_configuration(out)", "response": "Write the configuration for backward compatibility"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_configuration_dict(self, secret_attrs=False):\n        cd = {'repo_nexml2json': self.repo_nexml2json,\n              'number_of_shards': len(self._shards),\n              'initialization': self._filepath_args,\n              'shards': [],\n              }\n        for i in self._shards:\n            cd['shards'].append(i.get_configuration_dict(secret_attrs=secret_attrs))\n        return cd", "response": "Return a dictionary of type - specific configuration for backward compatibility"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef convert_html_entities(text_string):\n    '''\n    Converts HTML5 character references within text_string to their corresponding unicode characters\n    and returns converted string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return html.unescape(text_string).replace(\"&quot;\", \"'\")\n    else:\n        raise InputError(\"string not passed as argument for text_string\")", "response": "Converts HTML5 character references within text_string to their corresponding unicode characters\n    and returns converted string as type str."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert the Latin characters within text_string to their corresponding unicode characters and returns converted string as type str.", "response": "def convert_ligatures(text_string):\n    '''\n    Coverts Latin character references within text_string to their corresponding unicode characters\n    and returns converted string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a string or NoneType not be passed as an argument\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        for i in range(0, len(LIGATURES)):\n            text_string = text_string.replace(LIGATURES[str(i)][\"ligature\"], LIGATURES[str(i)][\"term\"])\n        return text_string\n    else:\n        raise InputError(\"none type or string not passed as an argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntake a string and converts words not found within a pre - built dictionary to their most likely actual word based on a relative probability dictionary. Returns a string that is edited by the user.", "response": "def correct_spelling(text_string):\n    '''\n    Splits string and converts words not found within a pre-built dictionary to their\n    most likely actual word based on a relative probability dictionary. Returns edited\n    string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a string or NoneType not be passed as an argument\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        word_list = text_string.split()\n        spellchecked_word_list = []\n        for word in word_list:\n            spellchecked_word_list.append(spellcheck.correct_word(word))\n        return \" \".join(spellchecked_word_list)\n    else:\n        raise InputError(\"none type or string not passed as an argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsplit text_string into a list of sentences based on NLTK s english. pickle tokenizer and returns said list of str.", "response": "def create_sentence_list(text_string):\n    '''\n    Splits text_string into a list of sentences based on NLTK's english.pickle tokenizer, and\n    returns said list as type list of str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return []\n    elif isinstance(text_string, str):\n        return SENTENCE_TOKENIZER.tokenize(text_string)\n    else:\n        raise InputError(\"non-string passed as argument for create_sentence_list\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting keywords from text_string using NLTK s list of English stopwords ignoring words of length smaller than 3 and returns the new string as type str.", "response": "def keyword_tokenize(text_string):\n    '''\n    Extracts keywords from text_string using NLTK's list of English stopwords, ignoring words of a\n    length smaller than 3, and returns the new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join([word for word in KEYWORD_TOKENIZER.tokenize(text_string) if word not in STOPWORDS and len(word) >= 3])\n    else:\n        raise InputError(\"string not passed as argument for text_string\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lemmatize(text_string):\n    '''\n        Returns base from of text_string using NLTK's WordNetLemmatizer as type str.\n\n        Keyword argument:\n\n        - text_string: string instance\n\n        Exceptions raised:\n\n        - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return LEMMATIZER.lemmatize(text_string)\n    else:\n        raise InputError(\"string not passed as primary argument\")", "response": "Returns base from of text_string using NLTK s WordNetLemmatizer as type str."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef lowercase(text_string):\n    '''\n    Converts text_string into lowercase and returns the converted string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return text_string.lower()\n    else:\n        raise InputError(\"string not passed as argument for text_string\")", "response": "Converts text_string into lowercase and returns the converted string as type str."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef preprocess_text(text_string, function_list):\n    '''\n    Given each function within function_list, applies the order of functions put forward onto\n    text_string, returning the processed string as type str.\n\n    Keyword argument:\n\n    - function_list: list of functions available in preprocessing.text\n    - text_string: string instance\n\n    Exceptions raised:\n    \n    - FunctionError: occurs should an invalid function be passed within the list of functions\n    - InputError: occurs should text_string be non-string, or function_list be non-list\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        if isinstance(function_list, list):\n            for func in function_list:\n                try:\n                    text_string = func(text_string)\n                except (NameError, TypeError):\n                    raise FunctionError(\"invalid function passed as element of function_list\")\n                except:\n                    raise\n            return text_string\n        else:\n            raise InputError(\"list of functions not passed as argument for function_list\")\n    else:\n        raise InputError(\"string not passed as argument for text_string\")", "response": "Given a string text_string applies each function in function_list put forward onto the text_string returning the processed string as type str."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving any escape characters within text_string and returns the new string as type str.", "response": "def remove_esc_chars(text_string):\n    '''\n    Removes any escape character within text_string and returns the new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join(re.sub(r'\\\\\\w', \"\", text_string).split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves any digit value discovered within text_string and returns the new string as type str.", "response": "def remove_numbers(text_string):\n    '''\n    Removes any digit value discovered within text_string and returns the new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join(re.sub(r'\\b[\\d.\\/,]+', \"\", text_string).split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove any integer represented as a word within text_string and returns the new string as type str.", "response": "def remove_number_words(text_string):\n    '''\n    Removes any integer represented as a word within text_string and returns the new string as\n    type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        for word in NUMBER_WORDS:\n            text_string = re.sub(r'[\\S]*\\b'+word+r'[\\S]*', \"\", text_string)\n        return \" \".join(text_string.split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves any time words associated to time within text_string and returns the resulting string as type str.", "response": "def remove_time_words(text_string):\n    '''\n    Removes any word associated to time (day, week, month, etc.) within text_string and returns the\n    new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        for word in TIME_WORDS:\n            text_string = re.sub(r'[\\S]*\\b'+word+r'[\\S]*', \"\", text_string)\n        return \" \".join(text_string.split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all punctuation unattached from a non - whitespace or attached to another punctuation and returns the new string as type str.", "response": "def remove_unbound_punct(text_string):\n    '''\n    Removes all punctuation unattached from a non-whitespace or attached to another punctuation\n    character unexpectedly (e.g. \".;';\") within text_string and returns the new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join(re.sub(r''.join([r'[', PUNCT, r'][', PUNCT, r']+|\\B[', PUNCT, r']+']), \"\",\n                               text_string).split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving all URLs within text_string and returns the new string as type str.", "response": "def remove_urls(text_string):\n    '''\n    Removes all URLs within text_string and returns the new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a non-string argument be passed\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join(re.sub(r'http\\S+', \"\", text_string).split())\n    else:\n        raise InputError(\"string not passed as argument\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_whitespace(text_string):\n    '''\n    Removes all whitespace found within text_string and returns new string as type str.\n\n    Keyword argument:\n\n    - text_string: string instance\n\n    Exceptions raised:\n\n    - InputError: occurs should a string or NoneType not be passed as an argument\n    '''\n    if text_string is None or text_string == \"\":\n        return \"\"\n    elif isinstance(text_string, str):\n        return \" \".join(text_string.split())\n    else:\n        raise InputError(\"none type or string not passed as an argument\")", "response": "Removes all whitespace found within text_string and returns new string as type str."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlogging a message at the specified level.", "response": "def log(self, level, message, *args, **kwargs):\n        \"\"\"\n        This is the primary method to override to ensure logging with extra\n        options gets correctly specified.\n        \"\"\"\n        extra = self.extras.copy()\n        extra.update(kwargs.pop('extra', {}))\n\n        kwargs['extra'] = extra\n        self.logger.log(level, message, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlog a warning message to the log file.", "response": "def warning(self, message, *args, **kwargs):\n        \"\"\"\n        Specialized warnings system. If a warning subclass is passed into\n        the keyword arguments and raise_warnings is True - the warnning will\n        be passed to the warnings module.\n        \"\"\"\n        warncls = kwargs.pop('warning', None)\n        if warncls and self.raise_warnings:\n            warnings.warn(message, warncls)\n\n        return self.log(logging.WARNING, message, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef log(self, level, message, *args, **kwargs):\n        extra = kwargs.pop('extra', {})\n        extra.update({\n            'user': self.user\n        })\n\n        kwargs['extra'] = extra\n        super(ServiceLogger, self).log(level, message, *args, **kwargs)", "response": "Log a message at the specified level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a ServiceLogger instance for the given .", "response": "def logger(self):\n        \"\"\"\n        Instantiates and returns a ServiceLogger instance\n        \"\"\"\n        if not hasattr(self, '_logger') or not self._logger:\n            self._logger = ServiceLogger()\n        return self._logger"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse a peyotl wrapper around an Open Tree web service to get a list of studies including values value for a given property.", "response": "def ot_find_studies(arg_dict, exact=True, verbose=False, oti_wrapper=None):\n    \"\"\"Uses a peyotl wrapper around an Open Tree web service to get a list of studies\n    including values `value` for a given property to be searched on `porperty`.\n\n    The oti_wrapper can be None (in which case the default wrapper from peyotl.sugar will be used.\n    All other arguments correspond to the arguments of the web-service call.\n    \"\"\"\n    if oti_wrapper is None:\n        from peyotl.sugar import oti\n        oti_wrapper = oti\n    return oti_wrapper.find_studies(arg_dict,\n                                    exact=exact,\n                                    verbose=verbose,\n                                    wrap_response=True)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef main(argv):\n    import argparse\n    description = 'Uses Open Tree of Life web services to try to find a tree with the value property pair specified. ' \\\n                  'setting --fuzzy will allow fuzzy matching'\n    parser = argparse.ArgumentParser(prog='ot-get-tree', description=description)\n    parser.add_argument('arg_dict', type=json.loads, help='name(s) for which we will try to find OTT IDs')\n    parser.add_argument('--property', default=None, type=str, required=False)\n    parser.add_argument('--fuzzy', action='store_true', default=False,\n                        required=False)  # exact matching and verbose not working atm...\n    parser.add_argument('--verbose', action='store_true', default=False, required=False)\n    try:\n        args = parser.parse_args(argv)\n        arg_dict = args.arg_dict\n        exact = not args.fuzzy\n        verbose = args.verbose\n    except:\n        arg_dict = {'ot:studyId': 'ot_308'}\n        sys.stderr.write('Running a demonstration query with {}\\n'.format(arg_dict))\n        exact = True\n        verbose = False\n    print_matching_studies(arg_dict, exact=exact, verbose=verbose)", "response": "This function is the main entry point for the command - line tool. It will parse the command - line arguments and then calls print_matching_studies to do all of the real work."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(argv):\n    import argparse\n    import codecs\n    # have to be ready to deal with utf-8 names\n    out = codecs.getwriter('utf-8')(sys.stdout)\n    description = '''Takes a series of at least 2 OTT ids and reports the OTT of their least inclusive taxonomic ancestor and that taxon's ancestors.'''\n    parser = argparse.ArgumentParser(prog='ot-taxo-mrca-to-root', description=description)\n    parser.add_argument('ids', nargs='+', type=int, help='OTT IDs')\n    args = parser.parse_args(argv)\n    id_list = args.ids\n    last_id = id_list.pop()\n    anc_list = get_taxonomic_ancestor_ids(last_id)\n    common_anc = set(anc_list)\n    for curr_id in id_list:\n        curr_anc_set = set(get_taxonomic_ancestor_ids(curr_id))\n        common_anc &= curr_anc_set\n        if not common_anc:\n            break\n    for anc_id in anc_list:\n        if anc_id in common_anc:\n            out.write('{}\\n'.format(anc_id))", "response": "This function is the main entry point for the ott - taxo - mrca - to - root command line interface."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_id2collection_info(path, tag):\n    d = {}\n    for triple in os.walk(path):\n        root, files = triple[0], triple[2]\n        for filename in files:\n            if filename.endswith('.json'):\n                # trim file extension and prepend owner_id (from path)\n                collection_id = \"{u}/{n}\".format(u=root.split('/')[-1], n=filename[:-5])\n                d[collection_id] = (tag, root, os.path.join(root, filename))\n    return d", "response": "Searchers for JSON files in this repo and returns\n    a map of collection id == > collection filepath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting the generic configuration of the current user.", "response": "def write_configuration(self, out, secret_attrs=False):\n        \"\"\"Generic configuration, may be overridden by type-specific version\"\"\"\n        key_order = ['name', 'path', 'git_dir', 'doc_dir', 'assumed_doc_version',\n                     'git_ssh', 'pkey', 'has_aliases', 'number of collections']\n        cd = self.get_configuration_dict(secret_attrs=secret_attrs)\n        for k in key_order:\n            if k in cd:\n                out.write('  {} = {}'.format(k, cd[k]))\n        out.write('  collections in alias groups:\\n')\n        for o in cd['collections']:\n            out.write('    {} ==> {}\\n'.format(o['keys'], o['relpath']))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the base class method and renames some properties", "response": "def get_configuration_dict(self, secret_attrs=False):\n        \"\"\"Overrides superclass method and renames some properties\"\"\"\n        cd = super(TreeCollectionsShard, self).get_configuration_dict(secret_attrs=secret_attrs)\n        # \"rename\" some keys in the dict provided\n        cd['number of collections'] = cd.pop('number of documents')\n        cd['collections'] = cd.pop('documents')\n        return cd"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _diagnose_prefixes(self):\n        from peyotl.collections_store import COLLECTION_ID_PATTERN\n        p = set()\n        for owner_dirname in os.listdir(self.doc_dir):\n            example_collection_name = \"{n}/xxxxx\".format(n=owner_dirname)\n            if COLLECTION_ID_PATTERN.match(example_collection_name):\n                p.add(owner_dirname)\n        return p", "response": "Returns a set of all of the prefixes seen in the main document dir"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a git action for a new collection.", "response": "def create_git_action_for_new_collection(self, new_collection_id=None):\n        \"\"\"Checks out master branch as a side effect\"\"\"\n        ga = self.create_git_action()\n        assert new_collection_id is not None\n        # id should have been sorted out by the caller\n        self.register_doc_id(ga, new_collection_id)\n        return ga, new_collection_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if a value is a sequence type.", "response": "def is_sequence(value):\n    \"\"\"Determine if a value is a sequence type.\n\n    Returns:\n      ``True`` if `value` is a sequence type (e.g., ``list``, or ``tuple``).\n      String types will return ``False``.\n\n    NOTE: On Python 3, strings have the __iter__ defined, so a simple hasattr\n    check is insufficient.\n    \"\"\"\n    return (hasattr(value, \"__iter__\") and not\n            isinstance(value, (six.string_types, six.binary_type)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef import_class(classpath):\n    modname, classname = classpath.rsplit(\".\", 1)\n    module = importlib.import_module(modname)\n    klass  = getattr(module, classname)\n    return klass", "response": "Imports the class referred to by the fully qualified class path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef resolve_class(classref):\n    if classref is None:\n        return None\n    elif isinstance(classref, six.class_types):\n        return classref\n    elif isinstance(classref, six.string_types):\n        return import_class(classref)\n    else:\n        raise ValueError(\"Unable to resolve class for '%s'\" % classref)", "response": "Attempt to return a Python class for the input class reference."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction decorator which checks that the decorated function is called with a set of required kwargs. Args: *argnames: String keyword argument names. Raises: ValueError: If a required kwarg is missing in the decorated function call.", "response": "def needkwargs(*argnames):\n    \"\"\"Function decorator which checks that the decorated function is called\n    with a set of required kwargs.\n\n    Args:\n        *argnames: String keyword argument names.\n\n    Raises:\n        ValueError: If a required kwarg is missing in the decorated function\n            call.\n    \"\"\"\n    required = set(argnames)\n\n    def decorator(func):\n        def inner(*args, **kwargs):\n            missing = required - set(kwargs)\n            if missing:\n                err = \"%s kwargs are missing.\" % list(missing)\n                raise ValueError(err)\n            return func(*args, **kwargs)\n        return inner\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(host=\"localhost\", port=3551, timeout=30):\n    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    sock.settimeout(timeout)\n    sock.connect((host, port))\n    sock.send(CMD_STATUS)\n    buffr = \"\"\n    while not buffr.endswith(EOF):\n        buffr += sock.recv(BUFFER_SIZE).decode()\n    sock.close()\n    return buffr", "response": "Get the APCUPSd NIS status."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse(raw_status, strip_units=False):\n    lines = split(raw_status)\n    if strip_units:\n        lines = strip_units_from_lines(lines)\n    # Split each line on the SEP character, strip extraneous whitespace and\n    # create an OrderedDict out of the keys/values.\n    return OrderedDict([[x.strip() for x in x.split(SEP, 1)] for x in lines])", "response": "Parse the raw_status output from get_status and return a OrderedDict."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef strip_units_from_lines(lines):\n    for line in lines:\n        for unit in ALL_UNITS:\n            if line.endswith(\" %s\" % unit):\n                line = line[:-1-len(unit)]\n        yield line", "response": "Yields all units from the end of the lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the status of the current apcaccess.", "response": "def print_status(raw_status, strip_units=False):\n    \"\"\"\n    Print the status to stdout in the same format as the original apcaccess.\n    \"\"\"\n    lines = split(raw_status)\n    if strip_units:\n        lines = strip_units_from_lines(lines)\n    for line in lines:\n        print(line)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking a name and optional contextName returns a list of matches. `wrap_response` can be True to return a TNRSResponse object, None to return the \"raw\" response dict, or a function/class that takes (response, query_data=dict) as its arguments. Each match is a dict with: 'higher' boolean DEF??? 'exact' boolean for exact match 'ottId' int 'name' name (or uniqname???) for the taxon in OTT 'nodeId' int ID of not in the taxomachine db. probably not of use to anyone...", "response": "def TNRS(self,\n             names,\n             context_name=None,\n             id_list=None,\n             fuzzy_matching=False,\n             include_deprecated=False,\n             include_dubious=False,\n             do_approximate_matching=None,\n             wrap_response=None):\n        \"\"\"Takes a name and optional contextName returns a list of matches.\n        `wrap_response` can be True to return a TNRSResponse object, None to return\n            the \"raw\" response dict, or a function/class that takes (response, query_data=dict)\n            as its arguments.\n\n        Each match is a dict with:\n           'higher' boolean DEF???\n           'exact' boolean for exact match\n           'ottId' int\n           'name'  name (or uniqname???) for the taxon in OTT\n           'nodeId' int ID of not in the taxomachine db. probably not of use to anyone...\n        \"\"\"\n        # if context_name is None:\n        #    context_name = 'All life'\n        if do_approximate_matching is not None:\n            fuzzy_matching = do_approximate_matching\n        if context_name and context_name not in self.valid_contexts:\n            raise ValueError('\"{}\" is not a valid context name'.format(context_name))\n        if not (isinstance(names, list) or isinstance(names, tuple)):\n            names = [names]\n        for name in names:\n            if len(name) < 2:\n                raise ValueError('Name \"{}\" found. Names must have at least 2 characters!'.format(name))\n        if id_list and len(id_list) != len(names):\n            raise ValueError('\"id_list must be the same size as \"names\"')\n        data = {'names': names}\n        if self.use_v1:\n            uri = '{p}/contextQueryForNames'.format(p=self.prefix)\n        else:\n            uri = '{p}/match_names'.format(p=self.prefix)\n        if context_name:\n            if self.use_v1:\n                data['contextName'] = context_name\n            else:\n                data['context_name'] = context_name\n        data['do_approximate_matching'] = bool(fuzzy_matching)\n        if id_list:\n            data['ids'] = list(id_list)\n        if include_deprecated:\n            data['include_deprecated'] = True\n        if include_dubious:\n            data['include_dubious'] = True\n        resp = self.json_http_post(uri, data=anyjson.dumps(data))\n        if wrap_response is None or wrap_response is False:\n            return resp\n        if wrap_response is True:\n            return TNRSResponse(self._wr, resp, query_data=data)\n        return wrap_response(resp, query_data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntake a name and optional context_name returns a list of matches.", "response": "def autocomplete(self, name, context_name=None, include_dubious=False):\n        \"\"\"Takes a name and optional context_name returns a list of matches.\n        Each match is a dict with:\n           'higher' boolean DEF???\n           'exact' boolean for exact match\n           'ottId' int\n           'name'  name (or uniqname???) for the taxon in OTT\n           'nodeId' int ID of not in the taxomachine db. probably not of use to anyone...\n        \"\"\"\n        if context_name and context_name not in self.valid_contexts:\n            raise ValueError('\"{}\" is not a valid context name'.format(context_name))\n        if self.use_v1:\n            uri = '{p}/autocompleteBoxQuery'.format(p=self.prefix)\n            data = {'queryString': name}\n            if context_name:\n                data['contextName'] = context_name\n        else:\n            uri = '{p}/autocomplete_name'.format(p=self.prefix)\n            data = {'name': name}\n            if context_name:\n                data['context_name'] = context_name\n            if include_dubious:\n                data['include_dubious'] = True\n        return self.json_http_post(uri, data=anyjson.dumps(data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef names_to_ott_ids_perfect(self, names, **kwargs):\n        results = self.TNRS(names, **kwargs)['results']\n        d = {}\n        for blob in results:\n            query_name = blob[\"id\"]\n            m = blob[\"matches\"]\n            perf_ind = None\n            for i, poss_m in enumerate(m):\n                if (not poss_m['is_approximate_match']) and (not poss_m['is_dubious']):\n                    if perf_ind is None:\n                        perf_ind = i\n                    else:\n                        raise ValueError('Multiple matches for \"{q}\"'.format(q=query_name))\n            if perf_ind is None:\n                raise ValueError('No matches for \"{q}\"'.format(q=query_name))\n            d[query_name] = m[perf_ind]['ot:ottId']\n        ret = []\n        for query_name in names:\n            ni = d.get(query_name)\n            if ni is None:\n                raise ValueError('No matches for \"{q}\"'.format(q=query_name))\n            ret.append(ni)\n        return ret", "response": "delegates a call to TNRS to get a list of OTT IDs that are perfectly matched in the given names."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a TaxonWrapper object for the parent of the given child_taxon.", "response": "def get_cached_parent_for_taxon(self, child_taxon):\n        \"\"\"If the taxa are being cached, this call will create a the lineage \"spike\" for taxon child_taxon\n\n        Expecting child_taxon to have a non-empty _taxonomic_lineage with response dicts that can create\n            an ancestral TaxonWrapper.\n        \"\"\"\n        if self._ott_id2taxon is None:\n            resp = child_taxon._taxonomic_lineage[0]\n            tl = child_taxon._taxonomic_lineage[1:]\n            assert 'taxonomic_lineage' not in resp\n            resp['taxonomic_lineage'] = tl\n            return TaxonWrapper(taxonomy=child_taxon.taxonomy,\n                                taxomachine_wrapper=self._wr,\n                                prop_dict=resp)  # TODO recursive (indirectly)\n        else:\n            anc = []\n            prev = None\n            for resp in reversed(child_taxon._taxonomic_lineage):\n                ott_id = resp['ot:ottId']\n                curr = self._ott_id2taxon.get(ott_id)\n                if curr is None:\n                    assert 'taxonomic_lineage' not in resp\n                    assert 'parent' not in resp\n                    resp['parent'] = prev\n                    resp['taxonomic_lineage'] = anc\n                    curr = TaxonWrapper(taxonomy=child_taxon.taxonomy,\n                                        taxomachine_wrapper=self._wr,\n                                        prop_dict=resp)\n                elif curr._parent is None and prev is not None:\n                    curr._parent = prev\n                prev = curr\n                anc.insert(0, curr)\n            return prev"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the credential definition identifier for input issuer DID and schema sequence number.", "response": "def cred_def_id(issuer_did: str, schema_seq_no: int) -> str:\n    \"\"\"\n    Return credential definition identifier for input issuer DID and schema sequence number.\n\n    :param issuer_did: DID of credential definition issuer\n    :param schema_seq_no: schema sequence number\n    :return: credential definition identifier\n    \"\"\"\n\n    return '{}:3:CL:{}:{}'.format(issuer_did, schema_seq_no, CD_ID_TAG)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a credential definition identifier and a tag return the corresponding revocation registry identifier.", "response": "def rev_reg_id(cd_id: str, tag: str) -> str:\n    \"\"\"\n    Given a credential definition identifier and a tag, return the corresponding\n    revocation registry identifier, repeating the issuer DID from the\n    input identifier.\n\n    :param cd_id: credential definition identifier\n    :param tag: tag to use\n    :return: revocation registry identifier\n    \"\"\"\n\n    return '{}:4:{}:CL_ACCUM:{}'.format(cd_id.split(':', 1)[0], cd_id, tag)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a revocation registry identifier return its corresponding credential definition identifier and tag.", "response": "def rev_reg_id2cred_def_id__tag(rr_id: str) -> (str, str):\n    \"\"\"\n    Given a revocation registry identifier, return its corresponding credential definition identifier and\n    (stringified int) tag.\n\n    :param rr_id: revocation registry identifier\n    :return: credential definition identifier and tag\n    \"\"\"\n\n    return (\n        ':'.join(rr_id.split(':')[2:-2]),  # rev reg id comprises (prefixes):<cred_def_id>:(suffixes)\n        str(rr_id.split(':')[-1])  # tag is last token\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef box_ids(creds: dict, cred_ids: list = None) -> dict:\n\n    rv = {}\n    for inner_creds in {**creds.get('attrs', {}), **creds.get('predicates', {})}.values():\n        for cred in inner_creds:  # cred is a dict in a list of dicts\n            cred_info = cred['cred_info']\n            cred_id = cred_info['referent']\n            if (cred_id not in rv) and (not cred_ids or cred_id in cred_ids):\n                rv[cred_id] = {\n                    'schema_id': cred_info['schema_id'],\n                    'cred_def_id': cred_info['cred_def_id'],\n                    'rev_reg_id': cred_info['rev_reg_id']\n                }\n\n    return rv", "response": "Given a credentials structure and a list of credential identifiers return a dict mapping each credential identifier to its corresponding box ids structure."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef creds_display(creds: dict, filt: dict = None, filt_dflt_incl: bool = False) -> dict:\n\n    rv = {}\n    if filt is None:\n        filt = {}\n    for cred_uuid in creds.get('attrs', {}):\n        for cred in creds['attrs'][cred_uuid]:  # creds['attrs'][cred_uuid] is a list of dict\n            cred_info = cred['cred_info']\n            if cred_info['referent'] in rv:\n                continue\n            cred_cd_id = cred_info['cred_def_id']\n            if (not filt) or (filt_dflt_incl and cred_cd_id not in filt):\n                rv[cred_info['referent']] = cred_info\n                continue\n            if filt and cred_cd_id in filt:\n                if ({k: str(filt[cred_cd_id][k]) for k in filt[cred_cd_id]}.items() <= cred_info['attrs'].items()):\n                    rv[cred_info['referent']] = cred_info\n\n    return rv", "response": "Display indy - sdk creds matching input filter from within input creds structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef revoc_info(creds: dict, filt: dict = None) -> dict:\n\n    rv = {}\n    for uuid2creds in (creds.get('attrs', {}), creds.get('predicates', {})):\n        for inner_creds in uuid2creds.values():\n            for cred in inner_creds:\n                cred_info = cred['cred_info']\n                (rr_id, cr_id) = (cred_info['rev_reg_id'], cred_info['cred_rev_id'])\n                if (rr_id, cr_id) in rv or rr_id is None or cr_id is None:\n                    continue\n                if not filt:\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n                    continue\n                if ({attr: str(filt[attr]) for attr in filt}.items() <= cred_info['attrs'].items()):\n                    rv[(rr_id, cr_id)] = cred_info['attrs']\n    return rv", "response": "Given a dict of revocation registry identifiers and credential revocation identifier return a dict mapping pairs\n    to decoded attributes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revealed_attrs(proof: dict) -> dict:\n\n    rv = {}\n    for sub_index in range(len(proof['identifiers'])):\n        cd_id = proof['identifiers'][sub_index]['cred_def_id']\n        rv[cd_id] = {\n            attr: decode(proof['proof']['proofs'][sub_index]['primary_proof']['eq_proof']['revealed_attrs'][attr])\n                for attr in proof['proof']['proofs'][sub_index]['primary_proof']['eq_proof']['revealed_attrs']\n        }\n\n    return rv", "response": "Fetch revealed attributes from input proof and return dict mapping cred - ids to dicts each mapping attribute names to values for processing in further creds downstream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the country code of the user that the user wants to recognize.", "response": "def get_country_from_request(request):\n    \"\"\"\n    Analyzes the request to find which country the user wants\n    the system to recognize. It checks the following sources\n    in the given order:\n    * session,\n    * cookie,\n    * HTTP_ACCEPT_LANGUAGE HTTP header, and\n    * IP address if USE_GEOIP is True.\n\n    It returns country code in ISO 3166-1 alpha-2 format.\n    \"\"\"\n    if hasattr(request, 'session'):\n        country_code = request.session.get(COUNTRY_SESSION_KEY)\n        if country_code:\n            return get_supported_country(country_code)\n\n    country_code = request.COOKIES.get(COUNTRY_COOKIE_NAME)\n    if country_code:\n        return get_supported_country(country_code)\n\n    if USE_GEOIP:\n        ip = _extract_ip_address(request.META)\n        country_code = _geo.country_code_by_addr(ip)\n        if country_code:\n            return get_supported_country(country_code)\n\n    if USE_LOCALE:\n        accept = request.META.get('HTTP_ACCEPT_LANGUAGE', '')\n        for accept_lang, _ in trans_real.parse_accept_lang_header(accept):\n            if LANG_COUNTRY_DELIM in accept_lang:\n                country_code = accept_lang.split(LANG_COUNTRY_DELIM)[-1]\n                if country_code:\n                    return get_supported_country(country_code)\n\n    return DEFAULT_COUNTRY_CODE"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_empty_fields(self, **kwargs):\n        if self._is_deprecated is None:\n            self._is_deprecated = kwargs.get('is_deprecated')\n        if self._is_dubious is None:\n            self._is_dubious = kwargs.get('is_dubious')\n        if self._is_synonym is None:\n            self._is_synonym = kwargs.get('is_synonym')\n        if self._synonyms is _EMPTY_TUPLE:\n            self._synonyms = kwargs.get('synonyms')\n            if self._synonyms is None:\n                self._synonyms = _EMPTY_TUPLE\n        if self.rank is None:\n            self._rank = kwargs.get('rank')\n        if self._nomenclature_code:\n            self._nomenclature_code = kwargs.get('nomenclature_code')\n        if not self._unique_name:\n            self._unique_name = kwargs.get('unique_name')\n        if self._taxonomic_lineage is None:\n            self._taxonomic_lineage = kwargs.get('taxonomic_lineage')\n        if self._parent is None:\n            self._parent = kwargs.get('parent')\n            if self._parent is None and self._taxomachine_wrapper is not None and self._taxonomic_lineage:\n                self._fill_parent_attr()", "response": "Updates the field of info about an OTU that might not be filled in by a match_names or taxon call."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_rev_dict(tree, ebt):\n    ebs = defaultdict(dict)\n    for edge in ebt.values():\n        source_id = edge['@source']\n        edge_id = edge['@id']\n        ebs[source_id][edge_id] = edge\n    assert ebs == tree['edgeBySourceId']", "response": "Verifyies that ebt is the inverse of the edgeBySourceId data member of tree"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a edge_by_target dict with the same edge objects as the edge_by_source.", "response": "def _create_edge_by_target(self):\n        \"\"\"creates a edge_by_target dict with the same edge objects as the edge_by_source.\n        Also adds an '@id' field to each edge.\"\"\"\n        ebt = {}\n        for edge_dict in self._edge_by_source.values():\n            for edge_id, edge in edge_dict.items():\n                target_id = edge['@target']\n                edge['@id'] = edge_id\n                assert target_id not in ebt\n                ebt[target_id] = edge\n        # _check_rev_dict(self._tree, ebt)\n        return ebt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prune_to_ingroup(self):\n        # Prune to just the ingroup\n        if not self._ingroup_node_id:\n            _LOG.debug('No ingroup node was specified.')\n            self._ingroup_node_id = self.root_node_id\n        elif self._ingroup_node_id != self.root_node_id:\n            self._do_prune_to_ingroup()\n            self.root_node_id = self._ingroup_node_id\n        else:\n            _LOG.debug('Ingroup node is root.')\n        return self.root_node_id", "response": "Remove nodes and edges from tree if they are not the ingroup or a descendant of it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prune_clade(self, node_id):\n        to_del_nodes = [node_id]\n        while bool(to_del_nodes):\n            node_id = to_del_nodes.pop(0)\n            self._flag_node_as_del_and_del_in_by_target(node_id)\n            ebsd = self._edge_by_source.get(node_id)\n            if ebsd is not None:\n                child_edges = list(ebsd.values())\n                to_del_nodes.extend([i['@target'] for i in child_edges])\n                del self._edge_by_source[\n                    node_id]", "response": "Prune node_id and the edges and nodes that are tipward of it. Caller must delete the edge to node_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nflags a node as deleted and removes it from the _edge_by_target and self. nodes_deleted.", "response": "def _flag_node_as_del_and_del_in_by_target(self, node_id):\n        \"\"\"Flags a node as deleted, and removes it from the _edge_by_target (and parent's edge_by_source), if it is still found there.\n        Does NOT remove the node's entries from self._edge_by_source.\"\"\"\n        self.nodes_deleted.append(node_id)\n        etp = self._edge_by_target.get(node_id)\n        if etp is not None:\n            del self._edge_by_target[node_id]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting the node from the tree. Returns the source_id of the node that was deleted.", "response": "def _del_tip(self, node_id):\n        \"\"\"Assumes that there is no entry in edge_by_source[node_id] to clean up.\"\"\"\n        self.nodes_deleted.append(node_id)\n        etp = self._edge_by_target.get(node_id)\n        assert etp is not None\n        source_id, target_id = self._del_edge(etp)\n        assert target_id == node_id\n        return source_id"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef group_and_sort_leaves_by_ott_id(self):\n        ott_id_to_sortable_list = defaultdict(list)\n        for node_id in self._edge_by_target.keys():\n            node_obj = self._node_by_id[node_id]\n            if node_id in self._edge_by_source:\n                continue\n            otu_id = node_obj['@otu']\n            otu_obj = self.otus[otu_id]\n            ott_id = otu_obj.get('^ot:ottId')\n            is_exemplar = node_obj.get('^ot:isTaxonExemplar', False)\n            int_is_exemplar = 0\n            if is_exemplar:\n                int_is_exemplar = -1  # to sort to the front of the list\n            sortable_el = (int_is_exemplar, node_id, node_obj, otu_obj)\n            ott_id_to_sortable_list[ott_id].append(sortable_el)\n        for v in ott_id_to_sortable_list.values():\n            v.sort()\n        return ott_id_to_sortable_list", "response": "returns a dict mapping OTT_id to list of elements referring to leaves mapped to that ott_id. The keys are ott_ids and values are None."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef suppress_deg_one_node(self, to_par_edge, nd_id, to_child_edge):\n        # circumvent the node with nd_id\n        to_child_edge_id = to_child_edge['@id']\n        par = to_par_edge['@source']\n        self._edge_by_source[par][to_child_edge_id] = to_child_edge\n        to_child_edge['@source'] = par\n        # make it a tip...\n        del self._edge_by_source[nd_id]\n        # delete it\n        self._del_tip(nd_id)", "response": "Removes to_par_edge and nd_id. To be used when nd_id is an out - degree = 1 node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prune_tree_for_supertree(self,\n                                 ott,\n                                 to_prune_fsi_set,\n                                 root_ott_id,\n                                 taxonomy_treefile=None,\n                                 id_to_other_prune_reason=None):\n        \"\"\"\n        `to_prune_fsi_set` is a set of flag indices to be pruned.\n        \"\"\"\n        if id_to_other_prune_reason is None:\n            id_to_other_prune_reason = {}\n        self.prune_to_ingroup()\n        self.prune_unmapped_leaves()\n        other_pruned = set()\n        if id_to_other_prune_reason:\n            id2p = set(id_to_other_prune_reason.keys()).intersection(set(self.by_ott_id.keys()))\n            for ott_id in id2p:\n                reason = id_to_other_prune_reason[ott_id]\n                self.prune_ott_problem_leaves_by_id(ott_id, reason)\n        # Check the stored OTT Ids against the current version of OTT\n        mapped, unrecog, forward2unrecog, pruned, above_root, old2new = ott.map_ott_ids(self.by_ott_id.keys(),\n                                                                                        to_prune_fsi_set, root_ott_id)\n        for ott_id in unrecog:\n            self.prune_ott_problem_leaves_by_id(ott_id, 'unrecognized_ott_id')\n        for ott_id in forward2unrecog:\n            self.prune_ott_problem_leaves_by_id(ott_id, 'forwarded_to_unrecognized_ott_id')\n        for ott_id in pruned:\n            self.prune_ott_problem_leaves_by_id(ott_id, 'flagged')\n        for ott_id in above_root:\n            self.prune_ott_problem_leaves_by_id(ott_id, 'above_root')\n        for old_id, new_id in old2new.items():\n            old_node_list = self.by_ott_id[old_id]\n            del self.by_ott_id[old_id]\n            if new_id in self.by_ott_id:\n                v = self.by_ott_id[new_id]\n                v.extend(old_node_list)\n                v.sort() # I think only the last step requires sorting (NEED to check that,\n                         # If so, we could move this sort to that point to avoid multiple sortings.\n            else:\n                self.by_ott_id[new_id] = old_node_list\n            for sortable_el in old_node_list:\n                otu = sortable_el[3]\n                assert otu['^ot:ottId'] == old_id\n                otu['^ot:ottId'] = new_id\n                assert '^ot:ottTaxonName' in otu\n                otu['^ot:ottTaxonName'] = ott.get_name(new_id)\n        lost_tips = set(unrecog)\n        lost_tips.update(forward2unrecog)\n        lost_tips.update(pruned)\n        lost_tips.update(other_pruned)\n        # Get the induced tree...\n        assert self.root_node_id\n        try:\n            ott_tree = ott.induced_tree(mapped, create_monotypic_nodes=True)\n        except SpikeTreeError:\n            error('SpikeTreeError from mapped ott_id list = {}'.format(', '.join([str(i) for i in mapped])))\n            raise EmptyTreeError()\n        if taxonomy_treefile is not None:\n            with codecs.open(taxonomy_treefile, 'w', encoding='utf-8') as tto:\n                ott_tree.write_newick(tto)\n        # ... so that we can look for leaves mapped to ancestors of other leaves\n        taxon_contains_other_ott_ids = []\n        to_retain = []\n        for ott_id in self.by_ott_id:\n            if ott_id in lost_tips:\n                continue\n            n = old2new.get(ott_id)\n            if n is None:\n                n = ott_id\n            nd = ott_tree.find_node(n)\n            assert nd is not None\n            if nd.children:\n                # nd must be an internal node.\n                #   given that the descendants of this node are mapped in a more specific\n                #   way, we will prune this ott_id from the tree\n                taxon_contains_other_ott_ids.append(ott_id)\n            else:\n                to_retain.append(ott_id)\n\n        for ott_id in taxon_contains_other_ott_ids:\n            self.prune_ott_problem_leaves_by_id(ott_id, 'mapped_to_taxon_containing_other_mapped_tips')\n        # finally, we walk through any ott_id's mapped to multiple nodes\n        for ott_id in to_retain:\n            nm = self.by_ott_id[ott_id]\n            if len(nm) > 1:\n                el = nm.pop(0)\n                reason = 'replaced_by_exemplar_node' if (el[0] == -1) else 'replaced_by_arbitrary_node'\n                self.prune_ott_problem_leaves_by_id(ott_id, reason)\n        return self", "response": "Prune the tree for the given set of supertree entries."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndescribes the method. :return: Description :rtype: dict[str, object]", "response": "def describe(self):\n        \"\"\"Describes the method.\n\n        :return: Description\n        :rtype: dict[str, object]\n        \"\"\"\n        return {\n            \"name\": self.name,\n            \"params\": self.params,\n            \"returns\": self.returns,\n            \"description\": self.description,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef params(self):\n        return [{\"name\": p_name, \"type\": p_type.__name__}\n                for (p_name, p_type) in self.signature.parameter_types]", "response": "The parameters for this method in a JSON - compatible format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef returns(self):\n        return_type = self.signature.return_type\n        none_type = type(None)\n        if return_type is not None and return_type is not none_type:\n            return return_type.__name__", "response": "The return type for this method in a JSON - compatible format."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a signature object ensuring order of parameter names and types.", "response": "def create(parameter_names, parameter_types, return_type):\n        \"\"\"Returns a signature object ensuring order of parameter names and types.\n\n        :param parameter_names: A list of ordered parameter names\n        :type parameter_names: list[str]\n        :param parameter_types: A dictionary of parameter names to types\n        :type parameter_types: dict[str, type]\n        :param return_type: The type the function returns\n        :type return_type: type\n        :rtype: MethodSignature\n        \"\"\"\n        ordered_pairs = [(name, parameter_types[name]) for name in parameter_names]\n        return MethodSignature(ordered_pairs, return_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _extract_text_and_child_element_list(minidom_node):\n    tl = []\n    ntl = []\n    for c in minidom_node.childNodes:\n        if c.nodeType == xml.dom.minidom.Node.TEXT_NODE:\n            tl.append(c)\n        else:\n            ntl.append(c)\n    try:\n        tl = [i.data.strip() for i in tl]\n        text_content = ''.join(tl)\n    except:\n        text_content = ''\n    return text_content, ntl", "response": "Extracts the text content and child element list of a minidom node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a dictionary from the DOM element x", "response": "def _gen_hbf_el(self, x):\n        \"\"\"\n        Builds a dictionary from the DOM element x\n        The function\n        Uses as hacky splitting of attribute or tag names using {}\n            to remove namespaces.\n        returns a pair of: the tag of `x` and the honeybadgerfish\n            representation of the subelements of x\n        Indirect recursion through _hbf_handle_child_elements\n        \"\"\"\n        obj = {}\n        # grab the tag of x\n        el_name = x.nodeName\n        assert el_name is not None\n        # add the attributes to the dictionary\n        att_container = x.attributes\n        ns_obj = {}\n        if att_container is not None:\n            for i in range(att_container.length):\n                attr = att_container.item(i)\n                n = attr.name\n                t = None\n                if n.startswith('xmlns'):\n                    if n == 'xmlns':\n                        t = '$'\n                    elif n.startswith('xmlns:'):\n                        t = n[6:]  # strip off the xmlns:\n                if t is None:\n                    obj['@' + n] = attr.value\n                else:\n                    ns_obj[t] = attr.value\n        if ns_obj:\n            obj['@xmlns'] = ns_obj\n\n        x.normalize()\n        # store the text content of the element under the key '$'\n        text_content, ntl = _extract_text_and_child_element_list(x)\n        if text_content:\n            obj['$'] = text_content\n        self._hbf_handle_child_elements(obj, ntl)\n        return el_name, obj"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _transform_meta_key_value(self, minidom_meta_element):\n        xt = minidom_meta_element.getAttribute('xsi:type')\n        if _LITERAL_META_PAT.match(xt):\n            return self._literal_transform_meta_key_value(minidom_meta_element)\n        elif _RESOURCE_META_PAT.match(xt):\n            return self._resource_transform_meta_key_value(minidom_meta_element)\n        else:\n            _LOG.debug('xsi:type attribute \"%s\" not LiteralMeta or ResourceMeta', xt)\n            return None, None", "response": "Checks if the minidom_meta_element can be represented as a\n            key value pair in a object. If not returns None None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning an etree. ETCompatXMLParser instance.", "response": "def get_xml_parser(encoding=None):\n    \"\"\"Returns an ``etree.ETCompatXMLParser`` instance.\"\"\"\n    parser = etree.ETCompatXMLParser(\n        huge_tree=True,\n        remove_comments=True,\n        strip_cdata=False,\n        remove_blank_text=True,\n        resolve_entities=False,\n        encoding=encoding\n    )\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an instance of lxml. etree. _Element for the given doc input.", "response": "def get_etree_root(doc, encoding=None):\n    \"\"\"Returns an instance of lxml.etree._Element for the given `doc` input.\n\n    Args:\n        doc: The input XML document. Can be an instance of\n            ``lxml.etree._Element``, ``lxml.etree._ElementTree``, a file-like\n            object, or a string filename.\n        encoding: The character encoding of `doc`. If ``None``, an attempt\n            will be made to determine the character encoding by the XML\n            parser.\n\n    Returns:\n        An ``lxml.etree._Element`` instance for `doc`.\n\n    Raises:\n        IOError: If `doc` cannot be found.\n        lxml.ParseError: If `doc` is a malformed XML document.\n\n    \"\"\"\n    tree = get_etree(doc, encoding)\n    root = tree.getroot()\n\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_schemaloc_pairs(node):\n    schemalocs = node.attrib[TAG_SCHEMALOCATION]\n    l = schemalocs.split()\n    return zip(l[::2], l[1::2])", "response": "Parses the xsi : schemaLocation attribute on node.\n    Returns a list of tuples for the ns schemaLocation attribute on node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef strip_cdata(text):\n    if not is_cdata(text):\n        return text\n\n    xml = \"<e>{0}</e>\".format(text)\n    node = etree.fromstring(xml)\n    return node.text", "response": "Removes all CDATA blocks from text if it contains them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping the input text in a CDATA block.", "response": "def cdata(text):\n    \"\"\"Wraps the input `text` in a ``<![CDATA[ ]]>`` block.\n\n    If the text contains CDATA sections already, they are stripped and replaced\n    by the application of an outer-most CDATA block.\n\n    Args:\n        text: A string to wrap in a CDATA block.\n\n    Returns:\n        The `text` value wrapped in ``<![CDATA[]]>``\n\n    \"\"\"\n    if not text:\n        return text\n\n    if is_cdata(text):\n        text = strip_cdata(text)\n\n    escaped = \"{0}{1}{2}\".format(CDATA_START, text, CDATA_END)\n    return escaped"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _is_valid(self, value):\n\n        # Entities have an istypeof method that can perform more sophisticated\n        # type checking.\n        if hasattr(self._type, \"istypeof\"):\n            return self._type.istypeof(value)\n        else:\n            return isinstance(value, self._type)", "response": "Return True if the input value is valid for insertion into the\n            inner list."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nattempts to coerce value into the correct type.", "response": "def _fix_value(self, value):\n        \"\"\"Attempt to coerce value into the correct type.\n\n        Subclasses can override this function.\n        \"\"\"\n        try:\n            return self._castfunc(value)\n        except:\n            error = \"Can't put '{0}' ({1}) into a {2}. Expected a {3} object.\"\n            error = error.format(\n                value,                  # Input value\n                type(value),            # Type of input value\n                type(self),             # Type of collection\n                self._type              # Expected type of input value\n            )\n            six.reraise(TypeError, TypeError(error), sys.exc_info()[-1])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef visitValueType(self, ctx: jsgParser.ValueTypeContext):\n        if ctx.idref():\n            self._typeid = as_token(ctx)\n        else:\n            self.visitChildren(ctx)", "response": "visitValueType - Gets the typeid of the value."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef text_entry(self):\n\n        allowed_sequences = set(['KEY_ENTER', 'KEY_ESCAPE', 'KEY_DELETE'])\n\n        sys.stdout.write('Enter text (<Esc> to abort) : ')\n        sys.stdout.flush()\n\n        # Track start column to ensure user doesn't backspace too far\n        start_column = self.term.get_location()[1]\n        cur_column = start_column\n\n        with self.term.cbreak():\n            val = ''\n            while val != 'KEY_ENTER' and val != 'KEY_ESCAPE':\n                val = self.term.inkey()\n                if not val:\n                    continue\n                elif val.is_sequence:\n                    val = val.name\n                    if val not in allowed_sequences:\n                        continue\n\n                if val == 'KEY_ENTER':\n                    self.roku.enter()\n                elif val == 'KEY_ESCAPE':\n                    pass\n                elif val == 'KEY_DELETE':\n                    self.roku.backspace()\n                    if cur_column > start_column:\n                        sys.stdout.write(u'\\b \\b')\n                        cur_column -= 1\n                else:\n                    self.roku.literal(val)\n                    sys.stdout.write(val)\n                    cur_column += 1\n                sys.stdout.flush()\n\n            # Clear to beginning of line\n            sys.stdout.write(self.term.clear_bol)\n            sys.stdout.write(self.term.move(self.term.height, 0))\n            sys.stdout.flush()", "response": "Relay literal text entry from user to Roku until the key presses Enter or Escape."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a NeXML tree to a dictionary of node and edge objects.", "response": "def convert_tree(self, tree):\n        \"\"\"Return (tree_id, tree) or None (if the tree has no edges).\n        \"\"\"\n        nodeById = {}\n        root_node = None\n        node_list = _index_list_of_values(tree, 'node')\n        for node in node_list:\n            nodeById[node['@id']] = node\n            r = node.get('@root')\n            # _LOG.debug(' node {} @root={}'.format(node['@id'], r))\n            if r in [True, 'true']:  # @TEMP accepting true or \"true\"\n                assert root_node is None\n                root_node = node\n        assert root_node is not None\n        edgeBySourceId = {}\n        edge_list = _get_index_list_of_values(tree, 'edge')\n        for edge in edge_list:\n            sourceId = edge['@source']\n            eid = edge['@id']\n            del edge['@id']\n            byso = edgeBySourceId.setdefault(sourceId, {})\n            byso[eid] = edge\n        # If all that succeeds, add the new object to the dict, creating a fat structure\n        tree['nodeById'] = nodeById\n        tree['edgeBySourceId'] = edgeBySourceId\n        tree['^ot:rootNodeId'] = root_node['@id']\n        # Make the struct leaner\n        tid = tree['@id']\n        if self.remove_old_structs:\n            del tree['@id']\n            del tree['node']\n            try:\n                del tree['edge']\n            except:\n                # Tree Tr75035 in http://treebase.org/treebase-web/search/study/summary.html?id=14763\n                #   is empty. in NeXML that shows up as a tree with a node but no edges.\n                #   See https://github.com/OpenTreeOfLife/opentree/issues/641\n                # TODO: returning None seems safest, but could cull trees with just metadata.\n                #       but creating a fake tree for metadata is ugly. So, I'm fine with not\n                #       supporting this.\n                _LOG.warn('Tree with ID \"{}\" is being dropped because it has no edges'.format(tid))\n                assert not edge_list\n                return None\n            for node in node_list:\n                if '^ot:isLeaf' in node:\n                    del node['^ot:isLeaf']\n                del node['@id']\n        return tid, tree"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntake a dict corresponding to the 1. 0. type and converts it to BY_ID_HONEY_BADGERFISH version. The object is modified in place .", "response": "def convert(self, obj):\n        \"\"\"Takes a dict corresponding to the honeybadgerfish JSON blob of the 1.0.* type and\n        converts it to BY_ID_HONEY_BADGERFISH version. The object is modified in place\n        and returned.\n        \"\"\"\n        if self.pristine_if_invalid:\n            raise NotImplementedError('pristine_if_invalid option is not supported yet')\n\n        nex = get_nexml_el(obj)\n        assert nex\n        # Create the new objects as locals. This section should not\n        #   mutate obj, so that if there is an exception the object\n        #   is unchanged on the error exit\n        otus = _index_list_of_values(nex, 'otus')\n        o_t = self.convert_otus(otus)\n        otusById, otusElementOrder = o_t\n        trees = _get_index_list_of_values(nex, 'trees')\n        treesById = dict((i['@id'], i) for i in trees)\n        treesElementOrder = [i['@id'] for i in trees]\n        if len(treesById) != len(treesElementOrder):\n            trees_id_set = set()\n            for tgid in treesElementOrder:\n                if tgid in trees_id_set:\n                    raise NexsonError('Repeated trees element id \"{}\"'.format(tgid))\n                trees_id_set.add(tgid)\n        tree_id_set = set()\n        treeContainingObjByTreesId = {}\n        for tree_group in trees:\n            # _LOG.debug('converting tree group {} to by_id'.format(tree_group['@id']))\n            treeById = {}\n            treeElementOrder = []\n            tree_array = _get_index_list_of_values(tree_group, 'tree')\n            for tree in tree_array:\n                # _LOG.debug('# pre-convert keys = {}'.format(tree.keys()))\n                t_t = self.convert_tree(tree)\n                if t_t is None:\n                    continue\n                tid, tree_alias = t_t  # pylint: disable=W0633\n                if tid in tree_id_set:\n                    raise NexsonError('Repeated tree element id \"{}\"'.format(tid))\n                tree_id_set.add(tid)\n\n                # _LOG.debug('converting tree {} to by_id'.format(tid))\n                # _LOG.debug('# post-convert keys = {}'.format(tree.keys()))\n                assert tree_alias is tree\n                treeById[tid] = tree\n                treeElementOrder.append(tid)\n            treeContainingObjByTreesId[tree_group['@id']] = treeById\n            tree_group['^ot:treeElementOrder'] = treeElementOrder\n\n        # If all that succeeds, add the new object to the dict, creating a fat structure\n        nex['otusById'] = otusById\n        nex['^ot:otusElementOrder'] = otusElementOrder\n        nex['treesById'] = treesById\n        nex['^ot:treesElementOrder'] = treesElementOrder\n        for k, v in treeContainingObjByTreesId.items():\n            treesById[k]['treeById'] = v\n        nex['@nexml2json'] = str(BY_ID_HONEY_BADGERFISH)\n        # Make the struct leaner\n        if self.remove_old_structs:\n            del nex['otus']\n            del nex['trees']\n            for k, v in treesById.items():\n                if 'tree' in v:\n                    del v['tree']\n                del v['@id']\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef members_entries(self, all_are_optional: Optional[bool] = False) -> List[Tuple[str, str]]:\n        if self._type_reference:\n            rval: List[Tuple[str, str]] = []\n            for n, t in self._context.reference(self._type_reference).members_entries(all_are_optional):\n                rval.append((n, self._ebnf.signature_cardinality(t, all_are_optional).format(name=n)))\n            return rval\n        else:\n            sig = self._ebnf.signature_cardinality(self._typ.reference_type(), all_are_optional)\n            return [(name, sig.format(name=name)) for name in self._names]", "response": "Generates a list quoted raw name signature type entries for this pairdef recursively traversing the types\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef signatures(self, all_are_optional: Optional[bool] = False) -> List[str]:\n        if self._type_reference:\n            # This assumes that references are to things that have signatures\n            ref = self._context.reference(self._type_reference)\n            if not getattr(ref, 'signatures', None):\n                raise NotImplementedError(\"Reference to \" + self._type_reference + \" is not valid\")\n            return self._context.reference(self._type_reference).signatures(all_are_optional)\n        else:\n            return [f\"{self._names[rn]}: {self.python_type()} = \" \n                    f\"{self._ebnf.mt_value(self._typ)}\" for rn, cn in self._names.items() if is_valid_python(cn)]", "response": "Return the list of signatures for this object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an initializer entry for the entry in the object.", "response": "def _initializer_for(self, raw_name: str, cooked_name: str, prefix: Optional[str]) -> List[str]:\n        \"\"\"Create an initializer entry for the entry\n\n        :param raw_name: name unadjusted for python compatibility.\n        :param cooked_name: name that may or may not be python compatible\n\n        :param prefix: owner of the element - used when objects passed as arguments\n\n        :return: Initialization statements\n        \"\"\"\n        mt_val = self._ebnf.mt_value(self._typ)\n        rval = []\n\n        if is_valid_python(raw_name):\n            if prefix:\n                # If a prefix exists, the input has already been processed - no if clause is necessary\n                rval.append(f\"self.{raw_name} = {prefix}.{raw_name}\")\n            else:\n                cons = raw_name\n                rval.append(f\"self.{raw_name} = {cons}\")\n        elif is_valid_python(cooked_name):\n            if prefix:\n                rval.append(f\"setattr(self, '{raw_name}', getattr({prefix}, '{raw_name}')\")\n            else:\n                cons = f\"{cooked_name} if {cooked_name} is not {mt_val} else _kwargs.get('{raw_name}', {mt_val})\"\n                rval.append(f\"setattr(self, '{raw_name}', {cons})\")\n        else:\n            getter = f\"_kwargs.get('{raw_name}', {mt_val})\"\n            if prefix:\n                rval.append(f\"setattr(self, '{raw_name}', getattr({prefix}, '{getter}')\")\n            else:\n                rval.append(f\"setattr(self, '{raw_name}', {getter})\")\n\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef initializers(self, prefix: Optional[str] = None) -> List[str]:\n        if self._type_reference:\n            # This assumes that references are to things that have initializers\n            # TODO: Remove this check once we are certian things are good\n            ref = self._context.reference(self._type_reference)\n            if not getattr(ref, 'signatures', None):\n                raise NotImplementedError(\"Reference to \" + self._type_reference + \" is not valid\")\n            return self._context.reference(self._type_reference).initializers(prefix)\n        else:\n            return flatten([self._initializer_for(rn, cn, prefix) for rn, cn in self._names.items()])", "response": "Return the list of initializers for this class."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset self. _type_reference if not set self. _type_reference = as_token ( ctx ) else self. _type_reference = as_token ( ctx )", "response": "def visitPairDef(self, ctx: jsgParser.PairDefContext):\n        \"\"\" pairDef: name COLON valueType ebnfSuffix? \n                     | idref ebnfSuffix?\n                     | OPREN name (BAR? name)+ CPREN COLON valueType ebnfSuffix?\n        \"\"\"\n        if ctx.name():          # Options 1 or 3\n            self.visitChildren(ctx)\n        else:\n            self._type_reference = as_token(ctx)\n            if ctx.ebnfSuffix():\n                self.visit(ctx.ebnfSuffix())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nnames is a name of the current term.", "response": "def visitName(self, ctx: jsgParser.NameContext):\n        \"\"\" name: ID | STRING \"\"\"\n        rtkn = get_terminal(ctx)\n        tkn = esc_kw(rtkn)\n        self._names[rtkn] = tkn"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nraise AbsentLinkSecret if link secret is not set.", "response": "def _assert_link_secret(self, action: str):\n        \"\"\"\n        Raise AbsentLinkSecret if link secret is not set.\n\n        :param action: action requiring link secret\n        \"\"\"\n\n        if self._link_secret is None:\n            LOGGER.debug('HolderProver._assert_link_secret: action %s requires link secret but it is not set', action)\n            raise AbsentLinkSecret('Action {} requires link secret but it is not set'.format(action))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cfg(self, value: dict) -> None:\n\n        self._cfg = value or {}\n        validate_config('holder-prover', self._cfg)", "response": "Set the configuration dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _sync_revoc(self, rr_id: str) -> None:\n\n        LOGGER.debug('HolderProver._sync_revoc >>> rr_id: %s', rr_id)\n\n        (cd_id, tag) = rev_reg_id2cred_def_id__tag(rr_id)\n\n        try:\n            json.loads(await self.get_cred_def(cd_id))\n        except AbsentCredDef:\n            LOGGER.debug(\n                'HolderProver._sync_revoc: <!< corrupt tails tree %s may be for another ledger', self._dir_tails)\n            raise AbsentCredDef('Corrupt tails tree {} may be for another ledger'.format(self._dir_tails))\n        except ClosedPool:\n            pass  # carry on, may be OK from cache only\n\n        with REVO_CACHE.lock:\n            revo_cache_entry = REVO_CACHE.get(rr_id, None)\n            tails = revo_cache_entry.tails if revo_cache_entry else None\n            if tails is None:  #  it's not yet set in cache\n                try:\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()\n                except AbsentTails:  # get hash from ledger and check for tails file\n                    rrdef = json.loads(await self._get_rev_reg_def(rr_id))\n                    tails_hash = rrdef['value']['tailsHash']\n                    path_tails = join(Tails.dir(self._dir_tails, rr_id), tails_hash)\n                    if not isfile(path_tails):\n                        LOGGER.debug('HolderProver._sync_revoc: <!< No tails file present at %s', path_tails)\n                        raise AbsentTails('No tails file present at {}'.format(path_tails))\n                    Tails.associate(self._dir_tails, rr_id, tails_hash)\n                    tails = await Tails(self._dir_tails, cd_id, tag).open()  # OK now since tails file present\n\n                if revo_cache_entry is None:\n                    REVO_CACHE[rr_id] = RevoCacheEntry(None, tails)\n                else:\n                    REVO_CACHE[rr_id].tails = tails\n\n        LOGGER.debug('HolderProver._sync_revoc <<<')", "response": "Synchronize tails file reader handles for given revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild a json delta from a revocation registry and its timestamp on the distributed ledger.", "response": "async def _build_rr_delta_json(self, rr_id: str, to: int, fro: int = None, fro_delta: dict = None) -> (str, int):\n        \"\"\"\n        Build rev reg delta json, potentially starting from existing (earlier) delta.\n\n        Return delta json and its timestamp on the distributed ledger.\n\n        Raise AbsentRevReg for no such revocation registry, or BadRevStateTime for a requested delta to\n        a time preceding revocation registry creation.\n\n        :param rr_id: rev reg id\n        :param to: time (epoch seconds) of interest; upper-bounds returned timestamp\n        :param fro: optional prior time of known delta json\n        :param fro_delta: optional known delta as of time fro\n        :return: rev reg delta json and ledger timestamp (epoch seconds)\n        \"\"\"\n\n        LOGGER.debug(\n            '_HolderProver._build_rr_delta_json >>> rr_id: %s, to: %s, fro: %s, fro_delta: %s',\n            rr_id,\n            to,\n            fro,\n            fro_delta)\n\n        rr_delta_json = None\n        ledger_timestamp = None\n\n        get_rr_delta_req_json = await ledger.build_get_revoc_reg_delta_request(self.did, rr_id, fro, to)\n        resp_json = await self._submit(get_rr_delta_req_json)\n        resp = json.loads(resp_json)\n        if resp.get('result', {}).get('data', None) and resp['result']['data'].get('value', None):\n            # delta is to some time at or beyond rev reg creation, carry on\n            try:\n                (_, rr_delta_json, ledger_timestamp) = await ledger.parse_get_revoc_reg_delta_response(resp_json)\n            except IndyError:  # ledger replied, but there is no such rev reg\n                LOGGER.debug('_HolderProver._build_rr_delta_json: <!< no rev reg exists on %s', rr_id)\n                raise AbsentRevReg('No rev reg exists on {}'.format(rr_id))\n        else:\n            LOGGER.debug(\n                '_HolderProver._build_rr_delta_json: <!< Rev reg %s created after asked-for time %s',\n                rr_id,\n                to)\n            raise BadRevStateTime('Rev reg {} created after asked-for time {}'.format(rr_id, to))\n\n        if fro and fro_delta:\n            rr_delta_json = await anoncreds.issuer_merge_revocation_registry_deltas(\n                json.dumps(fro_delta),\n                rr_delta_json)\n\n        rv = (rr_delta_json, ledger_timestamp)\n        LOGGER.debug('_HolderProver._build_rr_delta_json <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds and returns indy - sdk proof request for input attributes and timestamps by cred def id.", "response": "async def build_proof_req_json(self, cd_id2spec: dict, cache_only: bool = False) -> str:\n        \"\"\"\n        Build and return indy-sdk proof request for input attributes and timestamps by cred def id.\n\n        Raise AbsentInterval if caller specifies cache_only and default non-revocation intervals, but\n        revocation cache does not have delta frames for any revocation registries on a specified cred def.\n\n        :param cd_id2spec: dict mapping cred def ids to:\n            - (optionally) 'attrs': lists of names of attributes of interest (omit for all, empty list or None for none)\n            - (optionally) 'minima': (pred) integer lower-bounds of interest (omit, empty list, or None for none)\n            - (optionally), 'interval': (2-tuple) pair of epoch second counts marking 'from' and 'to' timestamps,\n                or single epoch second count to set 'from' and 'to' the same: default (now, now) if cache_only\n                is clear, or latest values from cache if cache_only is set.\n            e.g.,\n\n        ::\n\n            {\n                'Vx4E82R17q...:3:CL:16:0': {\n                    'attrs': [  # request attrs 'name' and 'favouriteDrink' from this cred def's schema\n                        'name',\n                        'favouriteDrink'\n                    ],\n                    'minima': {  # request predicate score>=80 from this cred def\n                        'score': 80\n                    }\n                    'interval': 1528116008  # same instant for all attrs and preds of corresponding schema\n                },\n                'R17v42T4pk...:3:CL:19:0': None,  # request all attrs, no preds, default intervals on all attrs\n                'e3vc5K168n...:3:CL:23:0': {},  # request all attrs, no preds, default intervals on all attrs\n                'Z9ccax812j...:3:CL:27:0': {  # request all attrs, no preds, this interval on all attrs\n                    'interval': (1528112408, 1528116008)\n                },\n                '9cHbp54C8n...:3:CL:37:0': {  # request no attrs, one pred, specify interval on pred\n                    'attrs': [],  # or equivalently, 'attrs': None\n                    'minima': {\n                        'employees': '50'  # nicety: implementation converts to int for caller\n                    },\n                    'interval': (1528029608, 1528116008)\n                },\n                '6caBcmLi33...:3:CL:41:0': {  # all attrs, one pred, default intervals to now on attrs & pred\n                    'minima': {\n                        'regEpoch': 1514782800\n                    }\n                }\n                ...\n            }\n\n        :param cache_only: (True) take default intervals (per cred def id) from latest cached deltas, or\n            (default False) use current time\n        :return: indy-sdk proof request json\n        \"\"\"\n\n        LOGGER.debug('HolderProver.build_proof_req_json >>> cd_id2spec: %s, cache_only: %s', cd_id2spec, cache_only)\n\n        cd_id2schema = {}\n        now = int(time())\n        proof_req = {\n            'nonce': str(int(time())),\n            'name': 'proof_req',\n            'version': '0.0',\n            'requested_attributes': {},\n            'requested_predicates': {}\n        }\n\n        for cd_id in cd_id2spec:\n            interval = None\n            cred_def = json.loads(await self.get_cred_def(cd_id))\n            seq_no = cred_def_id2seq_no(cd_id)\n            cd_id2schema[cd_id] = json.loads(await self.get_schema(seq_no))\n\n            if 'revocation' in cred_def['value']:\n                if cache_only and not (cd_id2spec.get(cd_id, {}) or {}).get('interval', None):\n                    with REVO_CACHE.lock:\n                        (fro, to) = REVO_CACHE.dflt_interval(cd_id)\n                        if not (fro and to):\n                            LOGGER.debug(\n                                'HolderProver.build_proof_req_json: <!< no cached delta for non-revoc interval on %s',\n                                cd_id)\n                            raise AbsentInterval('No cached delta for non-revoc interval on {}'.format(cd_id))\n                        interval = {\n                            'from': fro,\n                            'to': to\n                        }\n                else:\n                    fro_to = cd_id2spec[cd_id].get('interval', (now, now)) if cd_id2spec[cd_id] else (now, now)\n                    interval = {\n                        'from': fro_to if isinstance(fro_to, int) else min(fro_to),\n                        'to': fro_to if isinstance(fro_to, int) else max(fro_to)\n                    }\n\n            for attr in (cd_id2spec[cd_id].get('attrs', cd_id2schema[cd_id]['attrNames']) or []\n                    if cd_id2spec[cd_id] else cd_id2schema[cd_id]['attrNames']):\n                attr_uuid = '{}_{}_uuid'.format(seq_no, attr)\n                proof_req['requested_attributes'][attr_uuid] = {\n                    'name': attr,\n                    'restrictions': [{\n                        'cred_def_id': cd_id\n                    }]\n                }\n                if interval:\n                    proof_req['requested_attributes'][attr_uuid]['non_revoked'] = interval\n\n            for attr in (cd_id2spec[cd_id].get('minima', {}) or {} if cd_id2spec[cd_id] else {}):\n                pred_uuid = '{}_{}_uuid'.format(seq_no, attr)\n                try:\n                    proof_req['requested_predicates'][pred_uuid] = {\n                        'name': attr,\n                        'p_type': '>=',\n                        'p_value': int(cd_id2spec[cd_id]['minima'][attr]),\n                        'restrictions': [{\n                            'cred_def_id': cd_id\n                        }]\n                    }\n                except ValueError:\n                    LOGGER.info(\n                        'cannot build predicate on non-int minimum %s for %s',\n                        cd_id2spec[cd_id]['minima'][attr],\n                        attr)\n                    continue  # int conversion failed - reject candidate\n                if interval:\n                    proof_req['requested_predicates'][pred_uuid]['non_revoked'] = interval\n\n        rv_json = json.dumps(proof_req)\n        LOGGER.debug('HolderProver.build_proof_req_json <<< %s', rv_json)\n        return rv_json"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def build_req_creds_json(self, creds: dict, filt: dict = None, filt_dflt_incl: bool = False) -> str:\n\n        LOGGER.debug('HolderProver.build_req_creds_json >>> creds: %s, filt: %s', creds, filt)\n\n        req_creds = {\n            'self_attested_attributes': {},\n            'requested_attributes': {},\n            'requested_predicates': {}\n        }\n\n        def _add_cred(cred, uuid, key):\n            nonlocal req_creds\n            req_creds[key][uuid] = {\n                'cred_id': cred['cred_info']['referent'],\n                'revealed': True\n            }\n            if cred.get('interval', None):\n                req_creds[key][uuid]['timestamp'] = cred['interval']['to']\n            if key == 'requested_attributes':\n                req_creds[key][uuid]['revealed'] = True\n\n        if filt:\n            for cd_id in filt:\n                try:\n                    json.loads(await self.get_cred_def(cd_id))\n                except AbsentCredDef:\n                    LOGGER.warning(\n                        'HolderProver.build_req_creds_json: ignoring filter criterion, no cred def on %s', cd_id)\n                    filt.pop(cd_id)\n\n        for attr_uuid in creds.get('attrs', {}):\n            for cred in creds['attrs'][attr_uuid]:\n                if attr_uuid in req_creds['requested_attributes']:\n                    continue\n                cred_info = cred['cred_info']\n                cred_cd_id = cred_info['cred_def_id']\n\n                if filt:\n                    if cred_cd_id not in filt:\n                        if filt_dflt_incl:\n                            _add_cred(cred, attr_uuid, 'requested_attributes')\n                        continue\n                    if cred_cd_id in filt and 'attr-match' in (filt[cred_cd_id] or {}):  # maybe filt[cred_cd_id]: None\n                        if not {k: str(filt[cred_cd_id].get('attr-match', {})[k])\n                                for k in filt[cred_cd_id].get('attr-match', {})}.items() <= cred_info['attrs'].items():\n                            continue\n                    _add_cred(cred, attr_uuid, 'requested_attributes')\n                else:\n                    _add_cred(cred, attr_uuid, 'requested_attributes')\n\n        for pred_uuid in creds.get('predicates', {}):\n            for cred in creds['predicates'][pred_uuid]:\n                if pred_uuid in req_creds['requested_predicates']:\n                    continue\n                cred_info = cred['cred_info']\n                cred_cd_id = cred_info['cred_def_id']\n\n                if filt:\n                    if cred_cd_id not in filt:\n                        if filt_dflt_incl:\n                            _add_cred(cred, pred_uuid, 'requested_predicates')\n                        continue\n                    if cred_cd_id in filt and 'minima' in (filt[cred_cd_id] or {}):  # maybe filt[cred_cd_id]: None\n                        minima = filt[cred_cd_id].get('minima', {})\n                        try:\n                            if any((attr not in cred_info['attrs'])\n                                or (int(cred_info['attrs'][attr]) < int(minima[attr]))\n                                    for attr in minima):\n                                continue\n                        except ValueError:\n                            continue  # int conversion failed - reject candidate\n                    _add_cred(cred, pred_uuid, 'requested_predicates')\n                else:\n                    _add_cred(cred, pred_uuid, 'requested_predicates')\n\n        rv_json = json.dumps(req_creds)\n        LOGGER.debug('HolderProver.build_req_creds_json <<< %s', rv_json)\n        return rv_json", "response": "Builds and returns indy - sdk requested credentials json from input indy - sdk creds structure and filter."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dir_tails(self, rr_id: str) -> str:\n\n        return Tails.dir(self._dir_tails, rr_id)", "response": "Return the path to the correct directory for the tails file on input revocation registry identifier."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncloses the HolderProver instance.", "response": "async def close(self) -> None:\n        \"\"\"\n        Explicit exit. If so configured, populate cache to prove all creds in\n        wallet offline if need be, archive cache, and purge prior cache archives.\n\n        :return: current object\n        \"\"\"\n\n        LOGGER.debug('HolderProver.close >>>')\n\n        if self.cfg.get('archive-cache-on-close', False):\n            await self.load_cache(True)\n            Caches.purge_archives(self.dir_cache, True)\n\n        await super().close()\n        for path_rr_id in Tails.links(self._dir_tails):\n            rr_id = basename(path_rr_id)\n            try:\n                await self._sync_revoc(rr_id)\n            except ClosedPool:\n                LOGGER.warning('HolderProver sync-revoc on close required ledger for %s but pool was closed', rr_id)\n\n        LOGGER.debug('HolderProver.close <<<')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning list of revocation registry identifiers for which HolderProver has tails files.", "response": "def rev_regs(self) -> list:\n        \"\"\"\n        Return list of revocation registry identifiers for which HolderProver has tails files.\n\n        :return: list of revocation registry identifiers for which HolderProver has tails files\n        \"\"\"\n\n        LOGGER.debug('HolderProver.rev_regs >>>')\n\n        rv = [basename(f) for f in Tails.links(self._dir_tails)]\n        LOGGER.debug('HolderProver.rev_regs <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_link_secret(self, link_secret: str) -> None:\n\n        LOGGER.debug('HolderProver.create_link_secret >>> link_secret: %s', link_secret)\n\n        try:\n            await anoncreds.prover_create_master_secret(self.wallet.handle, link_secret)\n        except IndyError as x_indy:\n            if x_indy.error_code == ErrorCode.AnoncredsMasterSecretDuplicateNameError:\n                LOGGER.info('HolderProver did not create link secret - it already exists')\n            else:\n                LOGGER.debug(\n                    'HolderProver.create_link_secret: <!< cannot create link secret %s, indy error code %s',\n                    self.wallet.name,\n                    x_indy.error_code)\n                raise\n\n        self._link_secret = link_secret\n        LOGGER.debug('HolderProver.create_link_secret <<<')", "response": "Create link secret in wallet."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate credential request as HolderProver and store in wallet.", "response": "async def create_cred_req(self, cred_offer_json: str, cd_id: str) -> (str, str):\n        \"\"\"\n        Create credential request as HolderProver and store in wallet; return credential json and metadata json.\n\n        Raise AbsentLinkSecret if link secret not set.\n\n        :param cred_offer_json: credential offer json\n        :param cd_id: credential definition identifier\n        :return: cred request json and corresponding metadata json as created and stored in wallet\n        \"\"\"\n\n        LOGGER.debug('HolderProver.create_cred_req >>> cred_offer_json: %s, cd_id: %s', cred_offer_json, cd_id)\n\n        self._assert_link_secret('create_cred_req')\n\n        # Check that ledger has schema on ledger where cred def expects - in case of pool reset with extant wallet\n        cred_def_json = await self.get_cred_def(cd_id)\n        schema_seq_no = int(json.loads(cred_def_json)['schemaId'])\n        schema_json = await self.get_schema(schema_seq_no)\n        schema = json.loads(schema_json)\n        if not schema:\n            LOGGER.debug(\n                'HolderProver.create_cred_req: <!< absent schema@#%s, cred req may be for another ledger',\n                schema_seq_no)\n            raise AbsentSchema('Absent schema@#{}, cred req may be for another ledger'.format(schema_seq_no))\n        (cred_req_json, cred_req_metadata_json) = await anoncreds.prover_create_credential_req(\n            self.wallet.handle,\n            self.did,\n            cred_offer_json,\n            cred_def_json,\n            self._link_secret)\n        rv = (cred_req_json, cred_req_metadata_json)\n\n        LOGGER.debug('HolderProver.create_cred_req <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstores credential in wallet as HolderProver.", "response": "async def store_cred(self, cred_json: str, cred_req_metadata_json) -> str:\n        \"\"\"\n        Store cred in wallet as HolderProver, return its credential identifier as created in wallet.\n\n        Raise AbsentTails if tails file not available for revocation registry for input credential.\n\n        :param cred_json: credential json as HolderProver created\n        :param cred_req_metadata_json: credential request metadata as HolderProver created via create_cred_req()\n        :return: credential identifier within wallet\n        \"\"\"\n\n        LOGGER.debug(\n            'HolderProver.store_cred >>> cred_json: %s, cred_req_metadata_json: %s',\n            cred_json,\n            cred_req_metadata_json)\n\n        cred = json.loads(cred_json)\n        cred_def_json = await self.get_cred_def(cred['cred_def_id'])\n        rr_id = cred['rev_reg_id']\n        rrdef_json = None\n        if rr_id:\n            await self._sync_revoc(rr_id)\n            rrdef_json = await self._get_rev_reg_def(rr_id)\n\n        rv = await anoncreds.prover_store_credential(\n            self.wallet.handle,\n            None,  # cred_id, let indy-sdk generate random uuid\n            cred_req_metadata_json,\n            cred_json,\n            cred_def_json,\n            rrdef_json)\n\n        LOGGER.debug('HolderProver.store_cred <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload all caches and archive enough to go offline and can generate proof for all credentials in wallet.", "response": "async def load_cache(self, archive: bool = False) -> int:\n        \"\"\"\n        Load caches and archive enough to go offline and be able to generate proof\n        on all credentials in wallet.\n\n        Return timestamp (epoch seconds) of cache load event, also used as subdirectory\n        for cache archives.\n\n        :return: cache load event timestamp (epoch seconds)\n        \"\"\"\n\n        LOGGER.debug('HolderProver.load_cache >>> archive: %s', archive)\n\n        rv = int(time())\n        box_ids = json.loads(await self.get_box_ids_json())\n        for s_id in box_ids['schema_id']:\n            with SCHEMA_CACHE.lock:\n                await self.get_schema(s_id)\n        for cd_id in box_ids['cred_def_id']:\n            with CRED_DEF_CACHE.lock:\n                await self.get_cred_def(cd_id)\n        for rr_id in box_ids['rev_reg_id']:\n            await self._get_rev_reg_def(rr_id)\n            with REVO_CACHE.lock:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                if revo_cache_entry:\n                    try:\n                        await revo_cache_entry.get_delta_json(self._build_rr_delta_json, rv, rv)\n                    except ClosedPool:\n                        LOGGER.warning(\n                            'Holder-Prover %s is offline from pool %s, cannot update revo cache reg delta for %s to %s',\n                            self.wallet.name,\n                            self.pool.name,\n                            rr_id,\n                            rv)\n\n        if archive:\n            Caches.archive(self.dir_cache)\n        LOGGER.debug('HolderProver.load_cache <<< %s', rv)\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def get_box_ids_json(self) -> str:\n\n        LOGGER.debug('HolderProver.get_box_ids_json >>>')\n\n        s_ids = set()\n        cd_ids = set()\n        rr_ids = set()\n        for cred in json.loads(await self.get_creds_display_coarse()):\n            s_ids.add(cred['schema_id'])\n            cd_ids.add(cred['cred_def_id'])\n            if cred['rev_reg_id']:\n                rr_ids.add(cred['rev_reg_id'])\n\n        rv = json.dumps({\n            'schema_id': list(s_ids),\n            'cred_def_id': list(cd_ids),\n            'rev_reg_id': list(rr_ids)\n        })\n        LOGGER.debug('HolderProver.get_box_ids_json <<< %s', rv)\n        return rv", "response": "Return json object on lists of all unique box identifiers for credentials in wallet"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def get_creds_display_coarse(self, filt: dict = None) -> str:\n\n        LOGGER.debug('HolderProver.get_creds_display_coarse >>> filt: %s', filt)\n\n        rv_json = await anoncreds.prover_get_credentials(self.wallet.handle, json.dumps(filt or {}))\n        LOGGER.debug('HolderProver.get_creds_display_coarse <<< %s', rv_json)\n        return rv_json", "response": "Return human - readable credentials from wallet by input filter for\n        schema identifier and credential definition identifier components ; return all credentials for no filter."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def get_creds(self, proof_req_json: str, filt: dict = None, filt_dflt_incl: bool = False) -> (Set[str], str):\n\n        LOGGER.debug('HolderProver.get_creds >>> proof_req_json: %s, filt: %s', proof_req_json, filt)\n\n        if filt is None:\n            filt = {}\n        rv = None\n        creds_json = await anoncreds.prover_get_credentials_for_proof_req(self.wallet.handle, proof_req_json)\n        creds = json.loads(creds_json)\n        cred_ids = set()\n\n        if filt:\n            for cd_id in filt:\n                try:\n                    json.loads(await self.get_cred_def(cd_id))\n                except AbsentCredDef:\n                    LOGGER.warning('HolderProver.get_creds: ignoring filter criterion, no cred def on %s', cd_id)\n                    filt.pop(cd_id)\n\n        for inner_creds in {**creds['attrs'], **creds['predicates']}.values():\n            for cred in inner_creds:  # cred is a dict in a list of dicts\n                cred_info = cred['cred_info']\n                if filt:\n                    cred_cd_id = cred_info['cred_def_id']\n                    if cred_cd_id not in filt:\n                        if filt_dflt_incl:\n                            cred_ids.add(cred_info['referent'])\n                        continue\n                    if 'attr-match' in (filt[cred_cd_id] or {}):  # maybe filt[cred_cd_id]: None\n                        if not {k: str(filt[cred_cd_id].get('attr-match', {})[k])\n                                for k in filt[cred_cd_id].get('attr-match', {})}.items() <= cred_info['attrs'].items():\n                            continue\n                    if 'minima' in (filt[cred_cd_id] or {}):  # maybe filt[cred_cd_id]: None\n                        minima = filt[cred_cd_id].get('minima', {})\n                        try:\n                            if any((attr not in cred_info['attrs'])\n                                or (int(cred_info['attrs'][attr]) < int(minima[attr]))\n                                    for attr in minima):\n                                continue\n                        except ValueError:\n                            continue  # int conversion failed - reject candidate\n                    cred_ids.add(cred_info['referent'])\n                else:\n                    cred_ids.add(cred_info['referent'])\n\n        if filt:\n            creds = json.loads(prune_creds_json(creds, cred_ids))\n\n        rv = (cred_ids, json.dumps(creds))\n        LOGGER.debug('HolderProver.get_creds <<< %s', rv)\n        return rv", "response": "Get credentials from HolderProver wallet corresponding to proof request and filter criteria ; return credentials from wallet and empty production for no such credentials."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def get_creds_by_id(self, proof_req_json: str, cred_ids: set) -> str:\n\n        LOGGER.debug('HolderProver.get_creds_by_id >>> proof_req_json: %s, cred_ids: %s', proof_req_json, cred_ids)\n\n        creds_json = await anoncreds.prover_get_credentials_for_proof_req(self.wallet.handle, proof_req_json)\n\n        # retain only creds of interest: find corresponding referents\n        rv_json = prune_creds_json(json.loads(creds_json), cred_ids)\n        LOGGER.debug('HolderProver.get_cred_by_referent <<< %s', rv_json)\n        return rv_json", "response": "Get credentials structure for input credential identifiers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_proof(self, proof_req: dict, creds: dict, requested_creds: dict) -> str:\n\n        LOGGER.debug(\n            'HolderProver.create_proof >>> proof_req: %s, creds: %s, requested_creds: %s',\n            proof_req,\n            creds,\n            requested_creds)\n\n        self._assert_link_secret('create_proof')\n\n        x_uuids = [attr_uuid for attr_uuid in creds['attrs'] if len(creds['attrs'][attr_uuid]) != 1]\n        if x_uuids:\n            LOGGER.debug('HolderProver.create_proof: <!< creds specification out of focus (non-uniqueness)')\n            raise CredentialFocus('Proof request requires unique cred per attribute; violators: {}'.format(x_uuids))\n\n        s_id2schema = {}  # schema identifier to schema\n        cd_id2cred_def = {}  # credential definition identifier to credential definition\n        rr_id2timestamp = {}  # revocation registry of interest to timestamp of interest (or None)\n        rr_id2cr_id = {}  # revocation registry of interest to credential revocation identifier\n        for referents in {**creds['attrs'], **creds['predicates']}.values():\n            interval = referents[0].get('interval', None)\n            cred_info = referents[0]['cred_info']\n            s_id = cred_info['schema_id']\n            if s_id not in s_id2schema:\n                schema = json.loads(await self.get_schema(s_id))  # add to cache en passant\n                if not schema:\n                    LOGGER.debug(\n                        'HolderProver.create_proof: <!< absent schema %s, proof req may be for another ledger',\n                        s_id)\n                    raise AbsentSchema(\n                        'Absent schema {}, proof req may be for another ledger'.format(s_id))\n                s_id2schema[s_id] = schema\n\n            cd_id = cred_info['cred_def_id']\n            if cd_id not in cd_id2cred_def:\n                cred_def = json.loads(await self.get_cred_def(cd_id))  # add to cache en passant\n                cd_id2cred_def[cd_id] = cred_def\n\n            rr_id = cred_info['rev_reg_id']\n            if rr_id:\n                await self._sync_revoc(rr_id)  # link tails file to its rr_id if it's new\n                if interval:\n                    if rr_id not in rr_id2timestamp:\n                        if interval['to'] > int(time()):\n                            LOGGER.debug(\n                                'HolderProver.create_proof: <!< interval to %s for rev reg %s is in the future',\n                                interval['to'],\n                                rr_id)\n                            raise BadRevStateTime('Revocation registry {} timestamp {} is in the future'.format(\n                                rr_id,\n                                interval['to']))\n                        rr_id2timestamp[rr_id] = interval['to']\n                elif 'revocation' in cd_id2cred_def[cd_id]['value']:\n                    LOGGER.debug(\n                        'HolderProver.create_proof: <!< creds on cred def id %s missing non-revocation interval',\n                        cd_id)\n                    raise AbsentInterval('Creds on cred def id {} missing non-revocation interval'.format(cd_id))\n                if rr_id in rr_id2cr_id:\n                    continue\n                rr_id2cr_id[rr_id] = cred_info['cred_rev_id']\n\n        rr_id2rev_state = {}  # revocation registry identifier to its state\n        with REVO_CACHE.lock:\n            for rr_id in rr_id2timestamp:\n                revo_cache_entry = REVO_CACHE.get(rr_id, None)\n                tails = revo_cache_entry.tails if revo_cache_entry else None\n                if tails is None:  # missing tails file\n                    LOGGER.debug('HolderProver.create_proof: <!< missing tails file for rev reg id %s', rr_id)\n                    raise AbsentTails('Missing tails file for rev reg id {}'.format(rr_id))\n                rr_def_json = await self._get_rev_reg_def(rr_id)\n                (rr_delta_json, ledger_timestamp) = await revo_cache_entry.get_delta_json(\n                    self._build_rr_delta_json,\n                    rr_id2timestamp[rr_id],\n                    rr_id2timestamp[rr_id])\n                rr_state_json = await anoncreds.create_revocation_state(\n                    tails.reader_handle,\n                    rr_def_json,\n                    rr_delta_json,\n                    ledger_timestamp,\n                    rr_id2cr_id[rr_id])\n                rr_id2rev_state[rr_id] = {\n                    rr_id2timestamp[rr_id]: json.loads(rr_state_json)\n                }\n\n        rv = await anoncreds.prover_create_proof(\n            self.wallet.handle,\n            json.dumps(proof_req),\n            json.dumps(requested_creds),\n            self._link_secret,\n            json.dumps(s_id2schema),\n            json.dumps(cd_id2cred_def),\n            json.dumps(rr_id2rev_state))\n        LOGGER.debug('HolderProver.create_proof <<< %s', rv)\n        return rv", "response": "Create a new HolderProver proof."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def reset_wallet(self) -> str:\n\n        LOGGER.debug('HolderProver.reset_wallet >>>')\n\n        self._assert_link_secret('reset_wallet')\n\n        seed = self.wallet._seed\n        wallet_name = self.wallet.name\n        wallet_cfg = self.wallet.cfg\n        wallet_xtype = self.wallet.xtype\n        wallet_access_creds = self.wallet.access_creds\n\n        await self.wallet.close()\n        await self.wallet.remove()\n        self.wallet = await Wallet(\n            seed,\n            wallet_name,\n            wallet_xtype,\n            wallet_cfg,\n            wallet_access_creds).create()\n        await self.wallet.open()\n\n        await self.create_link_secret(self._link_secret)  # carry over link secret to new wallet\n\n        rv = self.wallet.name\n        LOGGER.debug('HolderProver.reset_wallet <<< %s', rv)\n        return rv", "response": "Reset HolderProver wallet and return new name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate and adds an xml element for the given parent tag and attrib.", "response": "def _create_sub_el(doc, parent, tag, attrib, data=None):\n    \"\"\"Creates and xml element for the `doc` with the given `parent`\n    and `tag` as the tagName.\n    `attrib` should be a dictionary of string keys to primitives or dicts\n        if the value is a dict, then the keys of the dict are joined with\n        the `attrib` key using a colon. This deals with the badgerfish\n        convention of nesting xmlns: attributes in a @xmnls object\n    If `data` is not None, then it will be written as data. If it is a boolean,\n        the xml true false will be writtten. Otherwise it will be\n        converted to python unicode string, stripped and written.\n    Returns the element created\n    \"\"\"\n    el = doc.createElement(tag)\n    if attrib:\n        if ('id' in attrib) and ('about' not in attrib):\n            about_val = '#' + attrib['id']\n            el.setAttribute('about', about_val)\n        for att_key, att_value in attrib.items():\n            if isinstance(att_value, dict):\n                for inner_key, inner_val in att_value.items():\n                    rk = ':'.join([att_key, inner_key])\n                    el.setAttribute(rk, inner_val)\n            else:\n                el.setAttribute(att_key, att_value)\n    if parent:\n        parent.appendChild(el)\n    if data is not None:\n        if data is True:\n            el.appendChild(doc.createTextNode('true'))\n        elif data is False:\n            el.appendChild(doc.createTextNode('false'))\n        else:\n            u = UNICODE(data).strip()\n            if u:\n                el.appendChild(doc.createTextNode(u))\n    return el"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _partition_keys_for_xml(self, o):\n        ak = {}\n        tk = None\n        ck = {}\n        mc = {}\n        # _LOG.debug('o = {o}'.format(o=o))\n        for k, v in o.items():\n            if k.startswith('@'):\n                if k == '@xmlns':\n                    if '$' in v:\n                        ak['xmlns'] = v['$']\n                    for nsk, nsv in v.items():\n                        if nsk != '$':\n                            ak['xmlns:' + nsk] = nsv\n                else:\n                    s = k[1:]\n                    if isinstance(v, bool):\n                        v = u'true' if v else u'false'\n                    ak[s] = UNICODE(v)\n            elif k == '$':\n                tk = v\n            elif k.startswith('^') and (not self._migrating_from_bf):\n                s = k[1:]\n                val = _convert_hbf_meta_val_for_xml(s, v)\n                _add_value_to_dict_bf(mc, s, val)\n            elif (k == u'meta') and self._migrating_from_bf:\n                s, val = _convert_bf_meta_val_for_xml(v)\n                _add_value_to_dict_bf(mc, s, val)\n            else:\n                ck[k] = v\n        return ak, tk, ck, mc", "response": "Breaks the keys of o into four content type by key syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_meta_dict_to_xml(self, doc, parent, meta_dict):\n        if not meta_dict:\n            return\n        key_list = list(meta_dict.keys())\n        key_list.sort()\n        for key in key_list:\n            el_list = _index_list_of_values(meta_dict, key)\n            for el in el_list:\n                self._add_meta_value_to_xml_doc(doc, parent, el)", "response": "Add the meta element dict to the xml doc."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_meta_value_to_xml_doc(self, doc, parent, obj):\n        return self._add_subtree_to_xml_doc(doc,\n                                            parent,\n                                            subtree=obj,\n                                            key='meta',\n                                            key_order=None)", "response": "Add the value in the meta element dict to the xml doc."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a histogram of your data.", "response": "def histogram(data):\n    \"\"\"Returns a histogram of your data.\n\n    :param data: The data to histogram\n    :type data: list[object]\n    :return: The histogram\n    :rtype: dict[object, int]\n    \"\"\"\n    ret = {}\n    for datum in data:\n        if datum in ret:\n            ret[datum] += 1\n        else:\n            ret[datum] = 1\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprint the object key - value pairs in a custom format", "response": "def print_data(data):\n    \"\"\"Prints object key-value pairs in a custom format\n\n    :param data: The dict to print\n    :type data: dict\n    :rtype: None\n    \"\"\"\n    print(\", \".join([\"{}=>{}\".format(key, value) for key, value in data]))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the subdirectories of a package", "response": "def find_subdirectories(package):\n    \"\"\"\n    Get the subdirectories within a package\n    This will include resources (non-submodules) and submodules\n    \"\"\"\n    try:\n        subdirectories = os.walk(package_to_path(package)).next()[1]\n    except StopIteration:\n        subdirectories = []\n    return subdirectories"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef subdir_findall(dir, subdir):\n    strip_n = len(dir.split('/'))\n    path = '/'.join((dir, subdir))\n    return ['/'.join(s.split('/')[strip_n:]) for s in setuptools.findall(path)]", "response": "Find all files in a subdirectory and return paths relative to dir\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_package_data(packages):\n    package_data = {}\n    for package in packages:\n        package_data[package] = []\n        for subdir in find_subdirectories(package):\n            if '.'.join((package, subdir)) in packages: # skip submodules\n                logging.debug(\"skipping submodule %s/%s\" % (package, subdir))\n                continue\n            if skip_tests and (subdir == 'tests'): # skip tests\n                logging.debug(\"skipping tests %s/%s\" % (package, subdir))\n                continue\n            package_data[package] += subdir_findall(package_to_path(package), subdir)\n    return package_data", "response": "This function scans the directory tree for a list of packages and finds all the data for the package in that list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the requirements from a file in the order they appear.", "response": "def parse_requirements(file_name):\n    \"\"\"\n    from:\n        http://cburgmer.posterous.com/pip-requirementstxt-and-setuppy\n    \"\"\"\n    requirements = []\n    with open(file_name, 'r') as f:\n        for line in f:\n            if re.match(r'(\\s*#)|(\\s*$)', line): continue\n            if re.match(r'\\s*-e\\s+', line):\n                requirements.append(re.sub(r'\\s*-e\\s+.*#egg=(.*)$',\\\n                        r'\\1', line).strip())\n            elif re.match(r'\\s*-f\\s+', line):\n                pass\n            else:\n                requirements.append(line.strip())\n    return requirements"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_dependency_links(file_name):\n    dependency_links = []\n    with open(file_name) as f:\n        for line in f:\n            if re.match(r'\\s*-[ef]\\s+', line):\n                dependency_links.append(re.sub(r'\\s*-[ef]\\s+',\\\n                        '', line))\n    return dependency_links", "response": "Parse the dependency links from a CBURGMER - POM file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the full path of the jupyter notebook.", "response": "def get_notebook_name():\n    \"\"\"\n    Return the full path of the jupyter notebook.\n    \"\"\"\n    kernel_id = re.search('kernel-(.*).json',\n                          ipykernel.connect.get_connection_file()).group(1)\n    servers = list_running_servers()\n    for ss in servers:\n        response = requests.get(urljoin(ss['url'], 'api/sessions'),\n                                params={'token': ss.get('token', '')})\n        for nn in json.loads(response.text):\n            if nn['kernel']['id'] == kernel_id:\n                full_path = nn['notebook']['path']\n                return os.path.basename(full_path)\n    \n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef json_encoder_to_serializer(encoder_cls: Union[Type[JSONEncoder], Callable[[], Type[JSONEncoder]]]) \\\n        -> Type[Serializer]:\n    \"\"\"\n    Converts a `JSONEncoder` class into an equivalent `Serializer` class.\n    :param encoder_cls: the encoder class type or a function that returns the type\n    :return: the equivalent `Serializer` class\n    \"\"\"\n    name = encoder_cls.__name__ if isinstance(encoder_cls, type) else \"%sLambdaTypeReturn\" % id(encoder_cls)\n    return type(\n        \"%sAsSerializer\" % name,\n        (_JSONEncoderAsSerializer,),\n        {\n            \"encoder_type\": property(lambda self: encoder_cls if isinstance(encoder_cls, type) else encoder_cls())\n        }\n    )", "response": "Converts a JSONEncoder class into an equivalent Serializer class."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef json_decoder_to_deserializer(decoder_cls: Union[Type[JSONDecoder], Callable[[], Type[JSONDecoder]]]) \\\n        -> Type[Deserializer]:\n    \"\"\"\n    Converts a `JSONDecoder` class into an equivalent `Deserializer` class.\n    :param decoder_cls: the decoder class type or a function that returns the type\n    :return: the equivalent `Deserializer` class\n    \"\"\"\n    name = decoder_cls.__name__ if isinstance(decoder_cls, type) else \"%sLambdaTypeReturn\" % id(decoder_cls)\n    return type(\n        \"%sAsDeserializer\" % name,\n        (_JSONDecoderAsDeserializer,),\n        {\n            \"decoder_type\": property(lambda self: decoder_cls if isinstance(decoder_cls, type) else decoder_cls())\n        }\n    )", "response": "Converts a JSONDecoder class into an equivalent Deserializer class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute output in CSV format.", "response": "def format(file_metrics, build_metrics):\n    \"\"\"Compute output in CSV format (only file_metrics).\"\"\"\n    # TODO maybe we need different output for build_metrics in csv format, too?\n    # filter out positions metric\n    def report_header(file_metrics):\n        values = list(file_metrics.values())[0]\n        print(values)\n        values.pop('block_positions', None)\n        return 'filename,' + ','.join(values) + '\\n'\n\n    def report_metrics(file_metrics):\n        report = ''\n        for key, values in file_metrics.items():\n            report += key + ','\n            report += ','.join([str(v) for k, v in values.items() if k not in ['block_positions']])\n            report += '\\n'\n        return report\n\n    report = report_header(file_metrics)\n    report += report_metrics(file_metrics)\n    return report"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef glob_files(root_dir, includes=None, excludes=None, gitignore=None):\n    # docu here: https://docs.python.org/3/library/pathlib.html\n    if not includes:\n        includes = ['**']\n    else:\n        # we need to iterate multiple times (iterator safeguard)\n        includes = list(includes)\n\n    if excludes:\n        # we need to iterate multiple times (iterator safeguard)\n        excludes = list(excludes)\n\n    if gitignore:\n        spec = pathspec.PathSpec.from_lines('gitwildmatch', gitignore)\n        log.debug('gitignore patterns: %s', gitignore)\n\n    while includes:\n        pattern = includes.pop(0)\n        # for compatibility with std. python Lib/glop.py:\n        # >>>If recursive is true, the pattern '**' will match any files and\n        #    zero or more directories and subdirectories.<<<\n        if pattern.endswith('**'):\n            pattern += '/*'\n        matches = list(Path(root_dir).glob(pattern))\n\n        for m in matches:\n            if m.is_dir():\n                continue\n\n            # some discussion on how to convert a pattern into regex:\n            # http://stackoverflow.com/questions/27726545/python-glob-but-against-a-list-of-strings-rather-than-the-filesystem\n            pp = PurePath(m)\n\n            # check if m is contained in remaining include patterns\n            # (last one wins)\n            if includes and any(map(lambda p: pp.match(p), includes)):\n                continue\n\n            # check if m is contained in exclude pattern\n            if excludes and any(map(lambda p: pp.match(p), excludes)):\n                continue\n\n            # check if m is contained in finkignore\n            if gitignore and spec.match_file(str(m)):\n                log.debug('Skipped file \\'%s\\' due to gitignore pattern',\n                          str(m.relative_to(root_dir)))\n                continue\n\n            yield (str(m), str(m.relative_to(root_dir)))", "response": "Powerful and flexible utility to search and tag files using patterns."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_build_metrics(context, build_processors):\n    build_metrics = OrderedDict()\n\n    # reset all processors\n    for p in build_processors:\n        p.reset()\n\n    # collect metrics from all processors\n    for p in build_processors:\n        build_metrics.update(p.build_metrics)\n\n    return build_metrics", "response": "use processors to collect build metrics."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summary(processors, metrics, context):\n    # display aggregated metric values on language level\n    def display_header(processors, before='', after=''):\n        \"\"\"Display the header for the summary results.\"\"\"\n        print(before, end=' ')\n        for processor in processors:\n            processor.display_header()\n        print(after)\n\n    def display_separator(processors, before='', after=''):\n        \"\"\"Display the header for the summary results.\"\"\"\n        print(before, end=' ')\n        for processor in processors:\n            processor.display_separator()\n        print(after)\n\n    def display_metrics(processors, before='', after='', metrics=[]):\n        \"\"\"Display the header for the summary results.\"\"\"\n        print(before, end=' ')\n        for processor in processors:\n            processor.display_metrics(metrics)\n        print(after)\n\n    summary = {}\n    for m in metrics:\n        lang = metrics[m]['language']\n        has_key = lang in summary\n        if not has_key:\n            summary[lang] = {'file_count': 0, 'language': lang}\n        summary[lang]['file_count'] += 1\n        for i in metrics[m]:\n            if i not in ['sloc', 'comments', 'mccabe']:  # include metrics to be used\n                continue\n            if not has_key:\n                summary[lang][i] = 0\n            summary[lang][i] += metrics[m][i]\n\n    total = {'language': 'Total'}\n    for m in summary:\n        for i in summary[m]:\n            if i == 'language':\n                continue\n            if i not in total:\n                total[i] = 0\n            total[i] += summary[m][i]\n\n    print('Metrics Summary:')\n\n    display_header(processors, 'Files', '')\n    display_separator(processors, '-'*5, '')\n    for k in sorted(summary.keys(), key=str.lower):\n        display_metrics(processors, '%5d' %\n                        summary[k]['file_count'], '', summary[k])\n    display_separator(processors, '-'*5, '')\n    display_metrics(processors, '%5d' % total['file_count'],\n                    '', total)", "response": "Print the summary of the available language level and the metrics."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming an action on the world that changes the internal state of the current node.", "response": "def performAction(self, action):\n        \"\"\" perform an action on the world that changes it's internal state\n        (maybe stochastically).\n\n        All you have to do is pull one of the arm and receive a payout. Each\n        arm has a distribution of different payouts that are delivered with\n        different probabilities.\n        \"\"\"\n        distrib = self.distrib[action[0], :]\n        payoutIdx = eventGenerator(distrib)\n#        payoutIdx = drawIndex(distrib)\n\n        self.lastPayout = self.payouts[action[0], payoutIdx]\n\n        print \"Payout [Arm: %d]: %.1f\" % (action[0], self.lastPayout)\n        print action"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a tuple of portfolios with U12 and U20 generators removed and generators of the same type at the same bus aggregated.", "response": "def get_portfolios3():\n    \"\"\" Returns portfolios with U12 and U20 generators removed and generators\n    of the same type at the same bus aggregated.\n    \"\"\"\n    g1 = [0]\n    g2 = [1]\n    g7 = [2]\n    g13 = [3]\n    g14 = [4] # sync cond\n    g15 = [5]\n    g16 = [6]\n    g18 = [7]\n    g21 = [8]\n    g22 = [9]\n    g23 = [10, 11]\n\n    portfolios = [g1 + g15 + g18,\n                  g2 + g16 + g21,\n                  g13 + g22,\n                  g7 + g23]\n\n    passive = g14 # sync_cond\n\n    return portfolios, passive"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an experiment that uses the Roth - Erev learning method.", "response": "def get_re_experiment(case, minor=1):\n    \"\"\" Returns an experiment that uses the Roth-Erev learning method.\n    \"\"\"\n    locAdj = \"ac\"\n    experimentation = 0.55\n    recency = 0.3\n    tau = 100.0\n    decay = 0.999\n    nStates = 3 # stateless RE?\n\n    Pd0 = get_pd_max(case, profile)\n    Pd_min = get_pd_min(case, profile)\n\n    market = pyreto.SmartMarket(case, priceCap=cap, decommit=decommit,\n                                auctionType=auctionType,\n                                locationalAdjustment=locAdj)\n\n    experiment = pyreto.continuous.MarketExperiment([], [], market)\n\n    portfolios, sync_cond = get_portfolios3()\n\n    for gidx in portfolios:\n        g = [case.generators[i] for i in gidx]\n\n        learner = VariantRothErev(experimentation, recency)\n        learner.explorer = BoltzmannExplorer(tau, decay)\n\n        task, agent = get_discrete_task_agent(g, market, nStates, nOffer,\n            markups, withholds, maxSteps, learner, Pd0, Pd_min)\n\n        print \"ALL ACTIONS:\", len(task.env._allActions) * nStates\n\n        experiment.tasks.append(task)\n        experiment.agents.append(agent)\n\n\n    passive = [case.generators[i] for i in sync_cond]\n    passive[0].p_min = 0.001 # Avoid invalid offer withholding.\n    passive[0].p_max = 0.002\n    task, agent = get_zero_task_agent(passive, market, 1, maxSteps)\n    experiment.tasks.append(task)\n    experiment.agents.append(agent)\n\n    return experiment"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call(self, tag_name: str, *args, **kwargs):\n        if hasattr(self, tag_name):\n            getattr(self, tag_name)(*args, **kwargs)", "response": "Convenience method for calling methods with walker."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef der(self, x: Sym):\n        name = 'der({:s})'.format(x.name())\n        if name not in self.scope['dvar'].keys():\n            self.scope['dvar'][name] = self.sym.sym(name, *x.shape)\n            self.scope['states'].append(x.name())\n        return self.scope['dvar'][name]", "response": "Get the derivative of the variable. Create it if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a gaussian noise variable", "response": "def noise_gaussian(self, mean, std):\n        \"\"\"Create a gaussian noise variable\"\"\"\n        assert std > 0\n        ng = self.sym.sym('ng_{:d}'.format(len(self.scope['ng'])))\n        self.scope['ng'].append(ng)\n        return mean + std*ng"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef noise_uniform(self, lower_bound, upper_bound):\n        assert upper_bound > lower_bound\n        nu = self.sym.sym('nu_{:d}'.format(len(self.scope['nu'])))\n        self.scope['nu'].append(nu)\n        return lower_bound + nu*(upper_bound - lower_bound)", "response": "Create a uniform noise variable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef log(self, *args, **kwargs):\n        if self.verbose:\n            print('   ' * self.depth, *args, **kwargs)", "response": "Convenience function for printing debug output."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_case6ww():\n    path = os.path.dirname(pylon.__file__)\n    path = os.path.join(path, \"test\", \"data\")\n    path = os.path.join(path, \"case6ww\", \"case6ww.pkl\")\n\n    case = pylon.Case.load(path)\n    case.generators[0].p_cost = (0.0, 4.0, 200.0)\n    case.generators[1].p_cost = (0.0, 3.0, 200.0)\n\n#    case.generators[0].p_cost = (0.0, 5.1, 200.0) # 10%\n#    case.generators[1].p_cost = (0.0, 4.5, 200.0) # 30%\n\n    case.generators[2].p_cost = (0.0, 6.0, 200.0) # passive\n\n#    case.generators[0].c_shutdown = 100.0\n#    case.generators[1].c_shutdown = 100.0\n#    case.generators[2].c_shutdown = 100.0\n\n    case.generators[0].p_min = 0.0 # TODO: Unit-decommitment.\n    case.generators[1].p_min = 0.0\n    case.generators[2].p_min = 0.0\n\n    case.generators[0].p_max = 110.0\n    case.generators[1].p_max = 110.0\n    case.generators[2].p_max = 220.0 # passive\n\n    # FIXME: Correct generator naming order.\n    for g in case.generators:\n        g.name\n\n    #pyreto.util.plotGenCost(case.generators)\n\n    return case", "response": "Returns the 6 bus case from Wood & Wollenberg PG&C.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the 24 bus IEEE Reliability Test System.", "response": "def get_case24_ieee_rts():\n    \"\"\" Returns the 24 bus IEEE Reliability Test System.\n    \"\"\"\n    path = os.path.dirname(pylon.__file__)\n    path = os.path.join(path, \"test\", \"data\")\n    path = os.path.join(path, \"case24_ieee_rts\", \"case24_ieee_rts.pkl\")\n\n    case = pylon.Case.load(path)\n\n    # FIXME: Correct generator naming order.\n    for g in case.generators:\n        g.name\n\n    return case"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_discrete_task_agent(generators, market, nStates, nOffer, markups,\n        withholds, maxSteps, learner, Pd0=None, Pd_min=0.0):\n    \"\"\" Returns a tuple of task and agent for the given learner.\n    \"\"\"\n    env = pyreto.discrete.MarketEnvironment(generators, market,\n                                            numStates=nStates,\n                                            numOffbids=nOffer,\n                                            markups=markups,\n                                            withholds=withholds,\n                                            Pd0=Pd0,\n                                            Pd_min=Pd_min)\n    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)\n\n    nActions = len(env._allActions)\n    module = ActionValueTable(numStates=nStates, numActions=nActions)\n\n    agent = LearningAgent(module, learner)\n\n    return task, agent", "response": "Returns a tuple of task and agent for the given learner."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_zero_task_agent(generators, market, nOffer, maxSteps):\n    env = pyreto.discrete.MarketEnvironment(generators, market, nOffer)\n    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)\n    agent = pyreto.util.ZeroAgent(env.outdim, env.indim)\n    return task, agent", "response": "Returns a task - agent tuple whose action is always zero."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_neg_one_task_agent(generators, market, nOffer, maxSteps):\n    env = pyreto.discrete.MarketEnvironment(generators, market, nOffer)\n    task = pyreto.discrete.ProfitTask(env, maxSteps=maxSteps)\n    agent = pyreto.util.NegOneAgent(env.outdim, env.indim)\n    return task, agent", "response": "Returns a task - agent tuple whose action is always minus one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_experiment(experiment, roleouts, episodes, in_cloud=False,\n                   dynProfile=None):\n    \"\"\" Runs the given experiment and returns the results.\n    \"\"\"\n    def run():\n        if dynProfile is None:\n            maxsteps = len(experiment.profile) # episode length\n        else:\n            maxsteps = dynProfile.shape[1]\n        na = len(experiment.agents)\n        ni = roleouts * episodes * maxsteps\n\n        all_action = zeros((na, 0))\n        all_reward = zeros((na, 0))\n        epsilon = zeros((na, ni)) # exploration rate\n\n        # Converts to action vector in percentage markup values.\n        vmarkup = vectorize(get_markup)\n\n        for roleout in range(roleouts):\n            if dynProfile is not None:\n                # Apply new load profile before each roleout (week).\n                i = roleout * episodes # index of first profile value\n                experiment.profile = dynProfile[i:i + episodes, :]\n\n#            print \"PROFILE:\", experiment.profile, episodes\n\n            experiment.doEpisodes(episodes) # number of samples per learning step\n\n            nei = episodes * maxsteps # num interactions per role\n            epi_action = zeros((0, nei))\n            epi_reward = zeros((0, nei))\n\n            for i, (task, agent) in \\\n            enumerate(zip(experiment.tasks, experiment.agents)):\n                action = copy(agent.history[\"action\"])\n                reward = copy(agent.history[\"reward\"])\n\n                for j in range(nei):\n                    if isinstance(agent.learner, DirectSearchLearner):\n                        action[j, :] = task.denormalize(action[j, :])\n                        k = nei * roleout\n                        epsilon[i, k:k + nei] = agent.learner.explorer.sigma[0]\n                    elif isinstance(agent.learner, ValueBasedLearner):\n                        action[j, :] = vmarkup(action[j, :], task)\n                        k = nei * roleout\n                        epsilon[i, k:k + nei] = agent.learner.explorer.epsilon\n                    else:\n                        action = vmarkup(action, task)\n\n                # FIXME: Only stores action[0] for all interactions.\n                epi_action = c_[epi_action.T, action[:, 0].flatten()].T\n                epi_reward = c_[epi_reward.T, reward.flatten()].T\n\n                if hasattr(agent, \"module\"):\n                    print \"PARAMS:\", agent.module.params\n\n                agent.learn()\n                agent.reset()\n\n            all_action = c_[all_action, epi_action]\n            all_reward = c_[all_reward, epi_reward]\n\n        return all_action, all_reward, epsilon\n\n    if in_cloud:\n        import cloud\n        job_id = cloud.call(run, _high_cpu=False)\n        result = cloud.result(job_id)\n        all_action, all_reward, epsilon = result\n    else:\n        all_action, all_reward, epsilon = run()\n\n    return all_action, all_reward, epsilon", "response": "Runs the given experiment and returns the results."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns percentages of peak load for all hours of the year.", "response": "def get_full_year():\n    \"\"\" Returns percentages of peak load for all hours of the year.\n\n    @return:\n        Numpy array of doubles with length 8736.\n    \"\"\"\n    weekly = get_weekly()\n    daily = get_daily()\n    hourly_winter_wkdy, hourly_winter_wknd = get_winter_hourly()\n    hourly_summer_wkdy, hourly_summer_wknd = get_summer_hourly()\n    hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd = \\\n        get_spring_autumn_hourly()\n\n    fullyear = zeros(364 * 24)\n    c = 0\n    l = [(0, 7, hourly_winter_wkdy, hourly_winter_wknd),\n         (8, 16, hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd),\n         (17, 29, hourly_summer_wkdy, hourly_summer_wknd),\n         (30, 42, hourly_spring_autumn_wkdy, hourly_spring_autumn_wknd),\n         (43, 51, hourly_winter_wkdy, hourly_winter_wknd)]\n\n    for start, end, wkdy, wknd in l:\n        for w in weekly[start:end + 1]:\n            for d in daily[:5]:\n                for h in wkdy:\n                    fullyear[c] = w * (d / 100.0) * (h / 100.0)\n                    c += 1\n            for d in daily[5:]:\n                for h in wknd:\n                    fullyear[c] = w * (d / 100.0) * (h / 100.0)\n                    c += 1\n    return fullyear"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning percentages of peak load for all days of the year.", "response": "def get_all_days():\n    \"\"\" Returns percentages of peak load for all days of the year.\n    Data from the IEEE RTS.\n    \"\"\"\n    weekly = get_weekly()\n    daily = get_daily()\n\n    return [w * (d / 100.0) for w in weekly for d in daily]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns an experiment that uses the Roth - Erev learning method.", "response": "def get_re_experiment(case, minor=1):\n    \"\"\" Returns an experiment that uses the Roth-Erev learning method.\n    \"\"\"\n    gen = case.generators\n\n    profile = array([1.0])\n    maxSteps = len(profile)\n    experimentation = 0.55\n    recency = 0.3\n    tau = 100.0\n    decay = 0.99#9995\n\n    market = pyreto.SmartMarket(case, priceCap=cap, decommit=decommit,\n                                auctionType=auctionType)\n\n    experiment = pyreto.continuous.MarketExperiment([], [], market, profile)\n\n    for g in gen[0:2]:\n        #learner = RothErev(experimentation, recency)\n        learner = VariantRothErev(experimentation, recency)\n        learner.explorer = BoltzmannExplorer(tau, decay)\n\n        task, agent = get_discrete_task_agent([g], market, nStates, nOffer,\n            markups, withholds, maxSteps, learner)\n\n        experiment.tasks.append(task)\n        experiment.agents.append(agent)\n\n    task1, agent1 = get_zero_task_agent(gen[2:3], market, nOffer, maxSteps)\n    experiment.tasks.append(task1)\n    experiment.agents.append(agent1)\n\n    return experiment"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_q_experiment(case, minor=1):\n    gen = case.generators\n\n    profile = array([1.0])\n    maxSteps = len(profile)\n\n    if minor == 1:\n        alpha = 0.3 # Learning rate.\n        gamma = 0.99 # Discount factor\n        # The closer epsilon gets to 0, the more greedy and less explorative.\n        epsilon = 0.9\n        decay = 0.97\n\n        tau = 150.0 # Boltzmann temperature.\n        qlambda = 0.9\n    elif minor == 2:\n        alpha = 0.1 # Learning rate.\n        gamma = 0.99 # Discount factor\n        # The closer epsilon gets to 0, the more greedy and less explorative.\n        epsilon = 0.9\n        decay = 0.99\n\n        tau = 150.0 # Boltzmann temperature.\n        qlambda = 0.9\n    else:\n        raise ValueError\n\n    market = pyreto.SmartMarket(case, priceCap=cap, decommit=decommit,\n                                auctionType=auctionType)\n\n    experiment = pyreto.continuous.MarketExperiment([], [], market, profile)\n\n    for g in gen[0:2]:\n        learner = Q(alpha, gamma)\n    #    learner = QLambda(alpha, gamma, qlambda)\n    #    learner = SARSA(alpha, gamma)\n\n        learner.explorer.epsilon = epsilon\n        learner.explorer.decay = decay\n#        learner.explorer = BoltzmannExplorer(tau, decay)\n\n        task, agent = get_discrete_task_agent([g], market, nStates, nOffer,\n            markups, withholds, maxSteps, learner)\n\n        experiment.tasks.append(task)\n        experiment.agents.append(agent)\n\n    # Passive agent.\n    task, agent = get_zero_task_agent(gen[2:3], market, nOffer, maxSteps)\n    experiment.tasks.append(task)\n    experiment.agents.append(agent)\n\n    return experiment", "response": "Returns an experiment that uses Q - learning."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef q_limited(self):\n        if (self.q >= self.q_max) or (self.q <= self.q_min):\n            return True\n        else:\n            return False", "response": "Is the machine at its limit of reactive power?"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef total_cost(self, p=None, p_cost=None, pcost_model=None):\n        p = self.p if p is None else p\n        p_cost = self.p_cost if p_cost is None else p_cost\n        pcost_model = self.pcost_model if pcost_model is None else pcost_model\n\n        p = 0.0 if not self.online else p\n\n        if pcost_model == PW_LINEAR:\n            n_segments = len(p_cost) - 1\n            # Iterate over the piece-wise linear segments.\n            for i in range(n_segments):\n                x1, y1 = p_cost[i]\n                x2, y2 = p_cost[i + 1]\n                m = (y2 - y1) / (x2 - x1)\n                c = y1 - m * x1\n                if x1 <= p <= x2:\n                    result = m*p + c\n                    break\n            else:\n#                print \"TOTC:\", self.name, p, self.p_max, p_cost\n\n#                raise ValueError, \"Value [%f] outwith pwl cost curve.\" % p\n\n                # Use the last segment for values outwith the cost curve.\n                logger.error(\"Value [%f] outside pwl cost curve [%s].\" %\n                             (p, p_cost[-1][0]))\n                result = m*p + c\n        elif pcost_model == POLYNOMIAL:\n#            result = p_cost[-1]\n#            for i in range(1, len(p_cost)):\n#                result += p_cost[-(i + 1)] * p**i\n            result = polyval(p_cost, p)\n        else:\n            raise ValueError\n\n        if self.is_load:\n            return -result\n        else:\n            return result", "response": "Computes the total cost for the generator at the given output level."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting the first segment of the pwl cost to linear quadratic.", "response": "def pwl_to_poly(self):\n        \"\"\" Converts the first segment of the pwl cost to linear quadratic.\n        FIXME: Curve-fit for all segments.\n        \"\"\"\n        if self.pcost_model == PW_LINEAR:\n            x0 = self.p_cost[0][0]\n            y0 = self.p_cost[0][1]\n            x1 = self.p_cost[1][0]\n            y1 = self.p_cost[1][1]\n            m = (y1 - y0) / (x1 - x0)\n            c = y0 - m * x0\n\n            self.pcost_model = POLYNOMIAL\n            self.p_cost = (m, c)\n        else:\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef poly_to_pwl(self, n_points=4):\n        assert self.pcost_model == POLYNOMIAL\n        p_min = self.p_min\n        p_max = self.p_max\n        p_cost = []\n\n        if p_min > 0.0:\n            # Make the first segment go from the origin to p_min.\n            step = (p_max - p_min) / (n_points - 2)\n\n            y0 = self.total_cost(0.0)\n            p_cost.append((0.0, y0))\n\n            x = p_min\n            n_points -= 1\n        else:\n            step = (p_max - p_min) / (n_points - 1)\n            x = 0.0\n\n        for _ in range(n_points):\n            y = self.total_cost(x)\n            p_cost.append((x, y))\n            x += step\n\n        # Change the cost model and set the new cost.\n        self.pcost_model = PW_LINEAR\n        self.p_cost = p_cost", "response": "Sets the piece - wise linear cost attribute by evaluating the polynomial cost variable by evaluating at zero and then at evenly spaced points between p_min and p_max."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning quantity and price offers created from the cost function.", "response": "def get_offers(self, n_points=6):\n        \"\"\" Returns quantity and price offers created from the cost function.\n        \"\"\"\n        from pyreto.smart_market import Offer\n\n        qtyprc = self._get_qtyprc(n_points)\n        return [Offer(self, qty, prc) for qty, prc in qtyprc]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn quantity and price bids created from the cost function.", "response": "def get_bids(self, n_points=6):\n        \"\"\" Returns quantity and price bids created from the cost function.\n        \"\"\"\n        from pyreto.smart_market import Bid\n\n        qtyprc = self._get_qtyprc(n_points)\n        return [Bid(self, qty, prc) for qty, prc in qtyprc]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of tuples of the form qty prc", "response": "def _get_qtyprc(self, n_points=6):\n        \"\"\" Returns a list of tuples of the form (qty, prc) created from the\n        cost function.  If the cost function is polynomial it will be converted\n        to piece-wise linear using poly_to_pwl(n_points).\n        \"\"\"\n        if self.pcost_model == POLYNOMIAL:\n            # Convert polynomial cost function to piece-wise linear.\n            self.poly_to_pwl(n_points)\n\n        n_segments = len(self.p_cost) - 1\n\n        qtyprc = []\n\n        for i in range(n_segments):\n            x1, y1 = self.p_cost[i]\n            x2, y2 = self.p_cost[(i + 1)]\n\n            quantity = x2 - x1\n            price = (y2 - y1) / quantity\n\n            qtyprc.append((quantity, price))\n\n        return qtyprc"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef offers_to_pwl(self, offers):\n        assert not self.is_load\n        # Only apply offers associated with this generator.\n        g_offers = [offer for offer in offers if offer.generator == self]\n        # Fliter out zero quantity offers.\n        gt_zero = [offr for offr in g_offers if round(offr.quantity, 4) > 0.0]\n        # Ignore withheld offers.\n        valid = [offer for offer in gt_zero if not offer.withheld]\n\n        p_offers = [v for v in valid if not v.reactive]\n        q_offers = [v for v in valid if v.reactive]\n\n        if p_offers:\n            self.p_cost = self._offbids_to_points(p_offers)\n            self.pcost_model = PW_LINEAR\n            self.online = True\n        else:\n            self.p_cost = [(0.0, 0.0), (self.p_max, 0.0)]\n            self.pcost_model = PW_LINEAR\n            if q_offers:\n                # Dispatch at zero real power without shutting down\n                # if capacity offered for reactive power.\n                self.p_min = 0.0\n                self.p_max = 0.0\n                self.online = True\n            else:\n                self.online = False\n\n        if q_offers:\n            self.q_cost = self._offbids_to_points(q_offers)\n            self.qcost_model = PW_LINEAR\n        else:\n            self.q_cost = None#[(0.0, 0.0), (self.q_max, 0.0)]\n            self.qcost_model = PW_LINEAR\n\n        if not len(p_offers) and not len(q_offers):\n            logger.info(\"No valid offers for generator [%s], shutting down.\" %\n                        self.name)\n            self.online = False\n\n        self._adjust_limits()", "response": "Updates the piece - wise linear total cost function using the given offers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bids_to_pwl(self, bids):\n        assert self.is_load\n        # Apply only those bids associated with this dispatchable load.\n        vl_bids = [bid for bid in bids if bid.vLoad == self]\n        # Filter out zero quantity bids.\n        gt_zero = [bid for bid in vl_bids if round(bid.quantity, 4) > 0.0]\n        # Ignore withheld offers.\n        valid_bids = [bid for bid in gt_zero if not bid.withheld]\n\n        p_bids = [v for v in valid_bids if not v.reactive]\n        q_bids = [v for v in valid_bids if v.reactive]\n\n        if p_bids:\n            self.p_cost = self._offbids_to_points(p_bids, True)\n            self.pcost_model = PW_LINEAR\n            self.online = True\n        else:\n            self.p_cost = [(0.0, 0.0), (self.p_max, 0.0)]\n            self.pcost_model = PW_LINEAR\n            logger.info(\"No valid active power bids for dispatchable load \"\n                        \"[%s], shutting down.\" % self.name)\n            self.online = False\n\n        if q_bids:\n            self.q_cost = self._offbids_to_points(q_bids, True)\n            self.qcost_model = PW_LINEAR\n            self.online = True\n        else:\n            self.q_cost = [(self.q_min, 0.0), (0.0, 0.0), (self.q_max, 0.0)]\n            self.qcost_model = PW_LINEAR\n#            logger.info(\"No valid bids for dispatchable load, shutting down.\")\n#            self.online = False\n\n        self._adjust_limits()", "response": "Updates the piece - wise linear total cost function using the given bids."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _offbids_to_points(self, offbids, arebids=False):\n        # Sort offers/bids by price in ascending order.\n        offbids.sort(key=lambda x: x.price, reverse=arebids)\n\n        points = [(0.0, offbids[0].noLoadCost)]\n        # Form piece-wise linear total cost function.\n        for i, offbid in enumerate(offbids):\n            x1, y1 = points[i]\n            x2 = x1 + offbid.quantity # MW.\n            m = offbid.price # $/MWh\n            y2 = m * (x2 - x1) + y1\n            points.append((x2, y2))\n\n        if arebids:\n            points = [(-x, -y) for x, y in points]\n            points.reverse()\n\n#        n_segs = len(points) - 1\n#        logger.info(\"%d segment pwl cost function: %s\" % (n_segs, points))\n\n        return points", "response": "Returns a list of points for a piece - wise linear function from the\n        given offer and bid blocks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _adjust_limits(self):\n        if not self.is_load:\n#            self.p_min = min([point[0] for point in self.p_cost])\n            self.p_max = max([point[0] for point in self.p_cost])\n        else:\n            p_min = min([point[0] for point in self.p_cost])\n            self.p_max = 0.0\n            self.q_min = self.q_min * p_min / self.p_min\n            self.q_max = self.q_max * p_min / self.p_min\n            self.p_min = p_min", "response": "Adjusts the active power limits p_max and p_min according to the pwl cost function points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the currently visible state of the world as a numpy array of doubles.", "response": "def getSensors(self):\n        \"\"\" Returns the currently visible state of the world as a numpy array\n            of doubles.\n        \"\"\"\n        sensors = array([])\n        sensors = r_[sensors, self._getTotalDemandSensor()]\n#        sensors = r_[sensors, self._getDemandSensor()]\n#        sensors = r_[sensors, self._getPriceSensor()]\n\n#        sensors = r_[sensors, self._getBusVoltageSensor()]\n\n#        sensors = r_[sensors, self._getBusVoltageMagnitudeSensor()]\n#        sensors = r_[sensors, self._getBusVoltageLambdaSensor()]\n#        sensors = r_[sensors, self._getBranchFlowSensor()]\n\n#        logger.info(\"State: %s\" % sensors)\n\n        return sensors"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms an action on the world that changes the internal state of the environment.", "response": "def performAction(self, action):\n        \"\"\" Performs an action on the world that changes it's internal state.\n            @param action: an action that should be executed in the Environment\n            @type action: array: [ g1_prc, g2_prc, g1_qty, g2_qty, ... ]\n        \"\"\"\n        self._lastAction = []\n\n        n = self.numOffbids * len(self.generators)\n\n        if self.maxWithhold is not None:\n            markups = action[:n]\n            withholds = action[n:]\n        else:\n            markups = action\n            withholds = [0.0] * n\n\n        self._offbid(markups, withholds)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef indim(self):\n        indim = self.numOffbids * len(self.generators)\n\n        if self.maxWithhold is not None:\n            return indim * 2\n        else:\n            return indim", "response": "The number of action values that the environment accepts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _getBusVoltageLambdaSensor(self):\n        muVmin = array([b.mu_vmin for b in self.market.case.connected_buses])\n        muVmax = array([b.mu_vmax for b in self.market.case.connected_buses])\n        muVmin = -1.0 * muVmin\n        diff = muVmin + muVmax\n        return diff", "response": "Returns the difference between the Lagrangian multipliers on the upper and the lower voltage limits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef flatten_component_refs(\n        container: ast.Class,\n        expression: ast.Union[ast.ConnectClause, ast.AssignmentStatement, ast.ForStatement, ast.Symbol],\n        instance_prefix: str) -> ast.Union[ast.ConnectClause, ast.AssignmentStatement, ast.ForStatement, ast.Symbol]:\n    \"\"\"\n    Flattens component refs in a tree\n    :param container: class\n    :param expression: original expression\n    :param instance_prefix: prefix for instance\n    :return: flattened expression\n    \"\"\"\n\n    expression_copy = copy.deepcopy(expression)\n\n    w = TreeWalker()\n    w.walk(ComponentRefFlattener(container, instance_prefix), expression_copy)\n\n    return expression_copy", "response": "Flattens component refs in a treenode."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fully_scope_function_calls(node: ast.Tree, expression: ast.Expression, function_set: OrderedDict) -> ast.Expression:\n    expression_copy = copy.deepcopy(expression)\n\n    w = TreeWalker()\n    w.walk(FunctionExpander(node, function_set), expression_copy)\n    return expression_copy", "response": "Turn the function references in this expression into fully scoped function calls."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef modify_symbol(sym: ast.Symbol, scope: ast.InstanceClass) -> None:\n\n    # We assume that we do not screw up the order of applying modifications\n    # when \"moving up\" with the scope.\n    apply_args = [x for x in sym.class_modification.arguments\n                  if x.scope is None or x.scope.full_reference().to_tuple() == scope.full_reference().to_tuple()]\n    skip_args = [x for x in sym.class_modification.arguments\n                 if x.scope is not None and x.scope.full_reference().to_tuple() != scope.full_reference().to_tuple()]\n\n    for class_mod_argument in apply_args:\n        argument = class_mod_argument.value\n\n        assert isinstance(argument, ast.ElementModification), \\\n            \"Found redeclaration modification which should already have been handled.\"\n\n        # TODO: Strip all non-symbol stuff.\n        if argument.component.name not in ast.Symbol.ATTRIBUTES:\n            raise Exception(\"Trying to set unknown symbol property {}\".format(argument.component.name))\n\n        setattr(sym, argument.component.name, argument.modifications[0])\n\n    sym.class_modification.arguments = skip_args", "response": "Modify a symbol in a new scope."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds all derivative expressions and annotate all differentiated symbols as states by adding state the prefix list", "response": "def annotate_states(node: ast.Node) -> None:\n    \"\"\"\n    Finds all derivative expressions and annotates all differentiated\n    symbols as states by adding state the prefix list\n    :param node: node of tree to walk\n    :return:\n    \"\"\"\n    w = TreeWalker()\n    w.walk(StateAnnotator(node), node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flatten(root: ast.Tree, class_name: ast.ComponentRef) -> ast.Class:\n    orig_class = root.find_class(class_name, copy=False)\n\n    flat_class = flatten_class(orig_class)\n\n    # expand connectors\n    expand_connectors(flat_class)\n\n    # add equations for state symbol values\n    add_state_value_equations(flat_class)\n    for func in flat_class.functions.values():\n        add_variable_value_statements(func)\n\n    # annotate states\n    annotate_states(flat_class)\n\n    # Put class in root\n    root = ast.Tree()\n    root.classes[orig_class.name] = flat_class\n\n    # pull functions to the top level,\n    # putting them prior to the model class so that they are visited\n    # first by the tree walker.\n    functions_and_classes = flat_class.functions\n    flat_class.functions = OrderedDict()\n    functions_and_classes.update(root.classes)\n    root.classes = functions_and_classes\n\n    return root", "response": "This function takes a Tree and flattens it so that all subclasses of class_name are replaced by their functions and symbols with names mangling\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef skip_child(self, tree: ast.Node, child_name: str) -> bool:\n        if isinstance(tree, ast.Class) and child_name == 'parent' or \\\n                isinstance(tree, ast.ClassModificationArgument) and child_name in ('scope', '__deepcopy__'):\n            return True\n        return False", "response": "Skip certain childs in the tree walking."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walk(self, listener: TreeListener, tree: ast.Node) -> None:\n        name = tree.__class__.__name__\n        if hasattr(listener, 'enterEvery'):\n            getattr(listener, 'enterEvery')(tree)\n        if hasattr(listener, 'enter' + name):\n            getattr(listener, 'enter' + name)(tree)\n        for child_name in self.order_keys(tree.__dict__.keys()):\n            if self.skip_child(tree, child_name):\n                continue\n            self.handle_walk(listener, tree.__dict__[child_name])\n        if hasattr(listener, 'exitEvery'):\n            getattr(listener, 'exitEvery')(tree)\n        if hasattr(listener, 'exit' + name):\n            getattr(listener, 'exit' + name)(tree)", "response": "Walks an AST tree recursively and calls the appropriate handlers."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_walk(self, listener: TreeListener, tree: Union[ast.Node, dict, list]) -> None:\n        if isinstance(tree, ast.Node):\n            self.walk(listener, tree)\n        elif isinstance(tree, dict):\n            for k in tree.keys():\n                self.handle_walk(listener, tree[k])\n        elif isinstance(tree, list):\n            for i in range(len(tree)):\n                self.handle_walk(listener, tree[i])\n        else:\n            pass", "response": "Handles tree walking, has to account for dictionaries and lists\n        :param listener: listener that reacts to walked events\n        :param tree: the tree to walk\n        :return: None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef DoxyfileParse(file_contents):\n   data = {}\n\n   import shlex\n   lex = shlex.shlex(instream = file_contents, posix = True)\n   lex.wordchars += \"*+./-:\"\n   lex.whitespace = lex.whitespace.replace(\"\\n\", \"\")\n   lex.escape = \"\"\n\n   lineno = lex.lineno\n   token = lex.get_token()\n   key = token   # the first token should be a key\n   last_token = \"\"\n   key_token = False\n   next_key = False\n   new_data = True\n\n   def append_data(data, key, new_data, token):\n      if new_data or len(data[key]) == 0:\n         data[key].append(token)\n      else:\n         data[key][-1] += token\n\n   while token:\n      if token in ['\\n']:\n         if last_token not in ['\\\\']:\n            key_token = True\n      elif token in ['\\\\']:\n         pass\n      elif key_token:\n         key = token\n         key_token = False\n      else:\n         if token == \"+=\":\n            if not data.has_key(key):\n               data[key] = list()\n         elif token == \"=\":\n            if key == \"TAGFILES\" and data.has_key(key):\n               append_data( data, key, False, \"=\" )\n               new_data=False\n            else:\n               data[key] = list()\n         else:\n            append_data( data, key, new_data, token )\n            new_data = True\n\n      last_token = token\n      token = lex.get_token()\n\n      if last_token == '\\\\' and token != '\\n':\n         new_data = False\n         append_data( data, key, new_data, '\\\\' )\n\n   # compress lists of len 1 into single strings\n   for (k, v) in data.items():\n      if len(v) == 0:\n         data.pop(k)\n\n      # items in the following list will be kept as lists and not converted to strings\n      if k in [\"INPUT\", \"FILE_PATTERNS\", \"EXCLUDE_PATTERNS\", \"TAGFILES\"]:\n         continue\n\n      if len(v) == 1:\n         data[k] = v[0]\n\n   return data", "response": "Parse a Doxygen source file and return a dictionary of all the values."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nscan the Doxygen file and add any files that match the pattern and exclude patterns.", "response": "def DoxySourceScan(node, env, path):\n   \"\"\"\n   Doxygen Doxyfile source scanner.  This should scan the Doxygen file and add\n   any files used to generate docs to the list of source files.\n   \"\"\"\n   default_file_patterns = [\n      '*.c', '*.cc', '*.cxx', '*.cpp', '*.c++', '*.java', '*.ii', '*.ixx',\n      '*.ipp', '*.i++', '*.inl', '*.h', '*.hh ', '*.hxx', '*.hpp', '*.h++',\n      '*.idl', '*.odl', '*.cs', '*.php', '*.php3', '*.inc', '*.m', '*.mm',\n      '*.py',\n   ]\n\n   default_exclude_patterns = [\n      '*~',\n   ]\n\n   sources = []\n\n   data = DoxyfileParse(node.get_contents())\n\n   if data.get(\"RECURSIVE\", \"NO\") == \"YES\":\n      recursive = True\n   else:\n      recursive = False\n\n   file_patterns = data.get(\"FILE_PATTERNS\", default_file_patterns)\n   exclude_patterns = data.get(\"EXCLUDE_PATTERNS\", default_exclude_patterns)\n\n   # We're running in the top-level directory, but the doxygen\n   # configuration file is in the same directory as node; this means\n   # that relative pathnames in node must be adjusted before they can\n   # go onto the sources list\n   conf_dir = os.path.dirname(str(node))\n   \n   for node in data.get(\"INPUT\", []):\n      if not os.path.isabs(node):\n         node = os.path.join(conf_dir, node)\n      if os.path.isfile(node):\n         sources.append(node)\n      elif os.path.isdir(node):\n         if recursive:\n            for root, dirs, files in os.walk(node):\n               for f in files:\n                  filename = os.path.join(root, f)\n\n                  pattern_check = reduce(lambda x, y: x or bool(fnmatch(filename, y)), file_patterns, False)\n                  exclude_check = reduce(lambda x, y: x and fnmatch(filename, y), exclude_patterns, True)\n\n                  if pattern_check and not exclude_check:\n                     sources.append(filename)\n         else:\n            for pattern in file_patterns:\n               sources.extend(glob.glob(\"/\".join([node, pattern])))\n\n   # Add tagfiles to the list of source files:\n   for node in data.get(\"TAGFILES\", []):\n      file = node.split(\"=\")[0]\n      if not os.path.isabs(file):\n         file = os.path.join(conf_dir, file)\n      sources.append(file)\n   \n   # Add additional files to the list of source files:\n   def append_additional_source(option):\n      file = data.get(option, \"\")\n      if file != \"\":\n         if not os.path.isabs(file):\n            file = os.path.join(conf_dir, file)\n         if os.path.isfile(file):\n            sources.append(file)\n\n   append_additional_source(\"HTML_STYLESHEET\")\n   append_additional_source(\"HTML_HEADER\")\n   append_additional_source(\"HTML_FOOTER\")\n\n   sources = map( lambda path: env.File(path), sources )\n   return sources"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate(env):\n   doxyfile_scanner = env.Scanner(\n      DoxySourceScan,\n      \"DoxySourceScan\",\n      scan_check = DoxySourceScanCheck,\n   )\n\n   import SCons.Builder\n   doxyfile_builder = SCons.Builder.Builder(\n      action = \"cd ${SOURCE.dir}  &&  ${DOXYGEN} ${SOURCE.file}\",\n      emitter = DoxyEmitter,\n      target_factory = env.fs.Entry,\n      single_source = True,\n      source_scanner =  doxyfile_scanner,\n   )\n\n   env.Append(BUILDERS = {\n      'Doxygen': doxyfile_builder,\n   })\n\n   env.AppendUnique(\n      DOXYGEN = 'doxygen',\n   )", "response": "Add builders and construction variables for the the\n   Doxygen tool."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreset the metric counter.", "response": "def reset(self):\n        \"\"\"Reset metric counter.\"\"\"\n        self._positions = []\n        self._line = 1\n        self._curr = None  # current scope we are analyzing\n        self._scope = 0\n        self.language = None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_scope(self, scope_type, scope_name, scope_start, is_method=False):\n        if self._curr is not None:\n            self._curr['end'] = scope_start - 1  # close last scope\n        self._curr = {\n            'type': scope_type, 'name': scope_name,\n            'start': scope_start, 'end': scope_start\n        }\n\n        if is_method and self._positions:\n            last = self._positions[-1]\n            if not 'methods' in last:\n                last['methods'] = []\n            last['methods'].append(self._curr)\n        else:\n            self._positions.append(self._curr)", "response": "we identified a scope and add it to positions."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses a token and update internal state", "response": "def process_token(self, tok):\n        \"\"\"count lines and track position of classes and functions\"\"\"\n        if tok[0] == Token.Text:\n            count = tok[1].count('\\n')\n            if count:\n                self._line += count  # adjust linecount\n\n        if self._detector.process(tok):\n            pass  # works been completed in the detector\n        elif tok[0] == Token.Punctuation:\n            if tok[0] == Token.Punctuation and tok[1] == '{':\n                self._scope += 1\n            if tok[0] == Token.Punctuation and tok[1] == '}':\n                self._scope += -1\n                if self._scope == 0 and self._curr is not None:\n                    self._curr['end'] = self._line  # close last scope\n                    self._curr = None\n        elif tok[0] == Token.Name.Class and self._scope == 0:\n            self.add_scope('Class', tok[1], self._line)\n        elif tok[0] == Token.Name.Function and self._scope in [0, 1]:\n            self.add_scope('Function', tok[1], self._line, self._scope == 1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nunpack the OPF model into a tuple of tuples.", "response": "def _unpack_model(self, om):\n        \"\"\" Returns data from the OPF model.\n        \"\"\"\n        buses = om.case.connected_buses\n        branches = om.case.online_branches\n        gens = om.case.online_generators\n\n        cp = om.get_cost_params()\n\n#        Bf = om._Bf\n#        Pfinj = om._Pfinj\n\n        return buses, branches, gens, cp"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the problem dimensions.", "response": "def _dimension_data(self, buses, branches, generators):\n        \"\"\" Returns the problem dimensions.\n        \"\"\"\n        ipol = [i for i, g in enumerate(generators)\n                if g.pcost_model == POLYNOMIAL]\n        ipwl = [i for i, g in enumerate(generators)\n                if g.pcost_model == PW_LINEAR]\n        nb = len(buses)\n        nl = len(branches)\n        # Number of general cost vars, w.\n        nw = self.om.cost_N\n        # Number of piece-wise linear costs.\n        if \"y\" in [v.name for v in self.om.vars]:\n            ny = self.om.get_var_N(\"y\")\n        else:\n            ny = 0\n        # Total number of control variables of all types.\n        nxyz = self.om.var_N\n\n        return ipol, ipwl, nb, nl, nw, ny, nxyz"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _linear_constraints(self, om):\n        A, l, u = om.linear_constraints() # l <= A*x <= u\n\n        # Indexes for equality, greater than (unbounded above), less than\n        # (unbounded below) and doubly-bounded box constraints.\n#        ieq = flatnonzero( abs(u - l) <= EPS )\n#        igt = flatnonzero( (u >=  1e10) & (l > -1e10) )\n#        ilt = flatnonzero( (l <= -1e10) & (u <  1e10) )\n#        ibx = flatnonzero( (abs(u - l) > EPS) & (u < 1e10) & (l > -1e10) )\n\n        # Zero-sized sparse matrices not supported.  Assume equality\n        # constraints exist.\n##        AA = A[ieq, :]\n##        if len(ilt) > 0:\n##            AA = vstack([AA, A[ilt, :]], \"csr\")\n##        if len(igt) > 0:\n##            AA = vstack([AA, -A[igt, :]], \"csr\")\n##        if len(ibx) > 0:\n##            AA = vstack([AA, A[ibx, :], -A[ibx, :]], \"csr\")\n#\n#        if len(ieq) or len(igt) or len(ilt) or len(ibx):\n#            sig_idx = [(1, ieq), (1, ilt), (-1, igt), (1, ibx), (-1, ibx)]\n#            AA = vstack([sig * A[idx, :] for sig, idx in sig_idx if len(idx)])\n#        else:\n#            AA = None\n#\n#        bb = r_[u[ieq, :], u[ilt], -l[igt], u[ibx], -l[ibx]]\n#\n#        self._nieq = ieq.shape[0]\n#\n#        return AA, bb\n\n        return A, l, u", "response": "Returns the linear problem constraints."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _var_bounds(self):\n        x0 = array([])\n        xmin = array([])\n        xmax = array([])\n\n        for var in self.om.vars:\n            x0 = r_[x0, var.v0]\n            xmin = r_[xmin, var.vl]\n            xmax = r_[xmax, var.vu]\n\n        return x0, xmin, xmax", "response": "Returns the bounds on the optimisation variables."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting an initial interior point for the solver.", "response": "def _initial_interior_point(self, buses, generators, xmin, xmax, ny):\n        \"\"\" Selects an interior initial point for interior point solver.\n        \"\"\"\n        Va = self.om.get_var(\"Va\")\n        va_refs = [b.v_angle * pi / 180.0 for b in buses\n                   if b.type == REFERENCE]\n        x0 = (xmin + xmax) / 2.0\n\n        x0[Va.i1:Va.iN + 1] = va_refs[0] # Angles set to first reference angle.\n\n        if ny > 0:\n            yvar = self.om.get_var(\"y\")\n\n            # Largest y-value in CCV data\n            c = []\n            for g in generators:\n                if g.pcost_model == PW_LINEAR:\n                    for _, y in g.p_cost:\n                        c.append(y)\n\n\n            x0[yvar.i1:yvar.iN + 1] = max(c) * 1.1\n\n        return x0"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef solve(self):\n        base_mva = self.om.case.base_mva\n        Bf = self.om._Bf\n        Pfinj = self.om._Pfinj\n        # Unpack the OPF model.\n        bs, ln, gn, cp = self._unpack_model(self.om)\n        # Compute problem dimensions.\n        ipol, ipwl, nb, nl, nw, ny, nxyz = self._dimension_data(bs, ln, gn)\n        # Split the constraints in equality and inequality.\n        AA, ll, uu = self._linear_constraints(self.om)\n        # Piece-wise linear components of the objective function.\n        Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl = self._pwl_costs(ny, nxyz, ipwl)\n        # Quadratic components of the objective function.\n        Npol, Hpol, Cpol, fparm_pol, polycf, npol = \\\n            self._quadratic_costs(gn, ipol, nxyz, base_mva)\n        # Combine pwl, poly and user costs.\n        NN, HHw, CCw, ffparm = \\\n            self._combine_costs(Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl,\n                                Npol, Hpol, Cpol, fparm_pol, npol, nw)\n        # Transform quadratic coefficients for w into coefficients for X.\n        HH, CC, C0 = self._transform_coefficients(NN, HHw, CCw, ffparm, polycf,\n                                                  any_pwl, npol, nw)\n        # Bounds on the optimisation variables.\n        _, xmin, xmax = self._var_bounds()\n\n        # Select an interior initial point for interior point solver.\n        x0 = self._initial_interior_point(bs, gn, xmin, xmax, ny)\n\n        # Call the quadratic/linear solver.\n        s = self._run_opf(HH, CC, AA, ll, uu, xmin, xmax, x0, self.opt)\n\n        # Compute the objective function value.\n        Va, Pg = self._update_solution_data(s, HH, CC, C0)\n\n        # Set case result attributes.\n        self._update_case(bs, ln, gn, base_mva, Bf, Pfinj, Va, Pg, s[\"lmbda\"])\n\n        return s", "response": "Solves the optimal power flow and returns a dictionary of results."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _pwl_costs(self, ny, nxyz, ipwl):\n        any_pwl = int(ny > 0)\n        if any_pwl:\n            y = self.om.get_var(\"y\")\n            # Sum of y vars.\n            Npwl = csr_matrix((ones(ny), (zeros(ny), array(ipwl) + y.i1)))\n            Hpwl = csr_matrix((1, 1))\n            Cpwl = array([1])\n            fparm_pwl = array([[1., 0., 0., 1.]])\n        else:\n            Npwl = None#zeros((0, nxyz))\n            Hpwl = None#array([])\n            Cpwl = array([])\n            fparm_pwl = zeros((0, 4))\n\n        return Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl", "response": "Returns the piece - wise linear components of the objective function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the quadratic cost components of the objective function.", "response": "def _quadratic_costs(self, generators, ipol, nxyz, base_mva):\n        \"\"\" Returns the quadratic cost components of the objective function.\n        \"\"\"\n        npol = len(ipol)\n        rnpol = range(npol)\n        gpol = [g for g in generators if g.pcost_model == POLYNOMIAL]\n\n        if [g for g in gpol if len(g.p_cost) > 3]:\n            logger.error(\"Order of polynomial cost greater than quadratic.\")\n\n        iqdr = [i for i, g in enumerate(generators)\n                if g.pcost_model == POLYNOMIAL and len(g.p_cost) == 3]\n        ilin = [i for i, g in enumerate(generators)\n                if g.pcost_model == POLYNOMIAL and len(g.p_cost) == 2]\n\n        polycf = zeros((npol, 3))\n        if npol > 0:\n            if len(iqdr) > 0:\n                polycf[iqdr, :] = array([list(g.p_cost)\n                                         for g in generators])#[iqdr, :].T\n            if len(ilin) > 0:\n                polycf[ilin, 1:] = array([list(g.p_cost[:2])\n                                          for g in generators])#[ilin, :].T\n            # Convert to per-unit.\n            polycf = polycf * array([base_mva**2, base_mva, 1])\n            Pg = self.om.get_var(\"Pg\")\n            Npol = csr_matrix((ones(npol), (rnpol, Pg.i1 + array(ipol))),\n                              (npol, nxyz))\n            Hpol = csr_matrix((2 * polycf[:, 0], (rnpol, rnpol)), (npol, npol))\n            Cpol = polycf[:, 1]\n            fparm_pol = (ones(npol) * array([[1], [0], [0], [1]])).T\n        else:\n            Npol = Hpol = None\n            Cpol = array([])\n            fparm_pol = zeros((0, 4))\n\n        return Npol, Hpol, Cpol, fparm_pol, polycf, npol"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _combine_costs(self, Npwl, Hpwl, Cpwl, fparm_pwl, any_pwl,\n                       Npol, Hpol, Cpol, fparm_pol, npol, nw):\n        \"\"\" Combines pwl, polynomial and user-defined costs.\n        \"\"\"\n        NN = vstack([n for n in [Npwl, Npol] if n is not None], \"csr\")\n\n        if (Hpwl is not None) and (Hpol is not None):\n            Hpwl = hstack([Hpwl, csr_matrix((any_pwl, npol))])\n            Hpol = hstack([csr_matrix((npol, any_pwl)), Hpol])\n#        if H is not None:\n#            H = hstack([csr_matrix((nw, any_pwl+npol)), H])\n\n        HHw = vstack([h for h in [Hpwl, Hpol] if h is not None], \"csr\")\n\n        CCw = r_[Cpwl, Cpol]\n\n        ffparm = r_[fparm_pwl, fparm_pol]\n\n        return NN, HHw, CCw, ffparm", "response": "Combines pwl polynomial and user - defined costs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _transform_coefficients(self, NN, HHw, CCw, ffparm, polycf,\n                               any_pwl, npol, nw):\n        \"\"\" Transforms quadratic coefficients for w into coefficients for x.\n        \"\"\"\n        nnw = any_pwl + npol + nw\n        M = csr_matrix((ffparm[:, 3], (range(nnw), range(nnw))))\n        MR = M * ffparm[:, 2] # FIXME: Possibly column 1.\n        HMR = HHw * MR\n        MN = M * NN\n        HH = MN.T * HHw * MN\n        CC = MN.T * (CCw - HMR)\n        # Constant term of cost.\n        C0 = 1./2. * MR.T * HMR + sum(polycf[:, 2])\n\n        return HH, CC, C0[0]", "response": "Transforms quadratic coefficients for w into coefficients for x."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_opf(self, HH, CC, AA, ll, uu, xmin, xmax, x0, opt):\n        N = self._nieq\n\n        if HH.nnz > 0:\n            solution = qps_pips(HH, CC, AA, ll, uu, xmin, xmax, x0, opt)\n        else:\n            solution = qps_pips(None, CC, AA, ll, uu, xmin, xmax, x0, opt)\n\n        return solution", "response": "Solves the either quadratic or linear program."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the solution data with the voltage angle and generator set - point vectors.", "response": "def _update_solution_data(self, s, HH, CC, C0):\n        \"\"\" Returns the voltage angle and generator set-point vectors.\n        \"\"\"\n        x = s[\"x\"]\n        Va_v = self.om.get_var(\"Va\")\n        Pg_v = self.om.get_var(\"Pg\")\n\n        Va = x[Va_v.i1:Va_v.iN + 1]\n        Pg = x[Pg_v.i1:Pg_v.iN + 1]\n#        f = 0.5 * dot(x.T * HH, x) + dot(CC.T, x)\n\n        s[\"f\"] = s[\"f\"] + C0\n\n        # Put the objective function value in the solution.\n#        solution[\"f\"] = f\n\n        return Va, Pg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the result attribute values for the current case.", "response": "def _update_case(self, bs, ln, gn, base_mva, Bf, Pfinj, Va, Pg, lmbda):\n        \"\"\" Calculates the result attribute values.\n        \"\"\"\n        Pmis = self.om.get_lin_constraint(\"Pmis\")\n        Pf = self.om.get_lin_constraint(\"Pf\")\n        Pt = self.om.get_lin_constraint(\"Pt\")\n        Pg_v = self.om.get_var(\"Pg\")\n\n        mu_l = lmbda[\"mu_l\"]\n        mu_u = lmbda[\"mu_u\"]\n        lower = lmbda[\"lower\"]\n        upper = lmbda[\"upper\"]\n\n        for i, bus in enumerate(bs):\n            bus.v_angle = Va[i] * 180.0 / pi\n\n            bus.p_lmbda = (mu_u[Pmis.i1:Pmis.iN + 1][i] -\n                           mu_l[Pmis.i1:Pmis.iN + 1][i]) / base_mva\n\n        for l, branch in enumerate(ln):\n            branch.p_from = (Bf * Va + Pfinj)[l] * base_mva\n            branch.p_to = -branch.p_from\n\n            branch.mu_s_from = mu_u[Pf.i1:Pf.iN + 1][l] / base_mva\n            branch.mu_s_to = mu_u[Pt.i1:Pt.iN + 1][l] / base_mva\n\n        for k, generator in enumerate(gn):\n            generator.p = Pg[k] * base_mva\n\n            generator.mu_pmin = lower[Pg_v.i1:Pg_v.iN + 1][k] / base_mva\n            generator.mu_pmax = upper[Pg_v.i1:Pg_v.iN + 1][k] / base_mva"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ref_bus_angle_constraint(self, buses, Va, xmin, xmax):\n        refs = [bus._i for bus in buses if bus.type == REFERENCE]\n        Varefs = array([b.v_angle for b in buses if b.type == REFERENCE])\n\n        xmin[Va.i1 - 1 + refs] = Varefs\n        xmax[Va.iN - 1 + refs] = Varefs\n\n        return xmin, xmax", "response": "Adds a constraint on the reference bus angles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving the AC optimal power flow.", "response": "def solve(self):\n        \"\"\" Solves AC optimal power flow.\n        \"\"\"\n        case = self.om.case\n        self._base_mva = case.base_mva\n        # TODO: Find an explanation for this value.\n        self.opt[\"cost_mult\"] = 1e-4\n\n        # Unpack the OPF model.\n        self._bs, self._ln, self._gn, _ = self._unpack_model(self.om)\n        # Compute problem dimensions.\n        self._ipol, _, self._nb, self._nl, _, self._ny, self._nxyz = \\\n            self._dimension_data(self._bs, self._ln, self._gn)\n\n        # Compute problem dimensions.\n        self._ng = len(self._gn)\n#        gpol = [g for g in gn if g.pcost_model == POLYNOMIAL]\n\n        # Linear constraints (l <= A*x <= u).\n        A, l, u = self.om.linear_constraints()\n#        AA, bb = self._linear_constraints(self.om)\n\n        _, xmin, xmax = self._var_bounds()\n\n        # Select an interior initial point for interior point solver.\n        x0 = self._initial_interior_point(self._bs, self._gn, xmin, xmax, self._ny)\n\n        # Build admittance matrices.\n        self._Ybus, self._Yf, self._Yt = case.Y\n\n        # Optimisation variables.\n\n        self._Pg = self.om.get_var(\"Pg\")\n        self._Qg = self.om.get_var(\"Qg\")\n        self._Va = self.om.get_var(\"Va\")\n        self._Vm = self.om.get_var(\"Vm\")\n\n        # Adds a constraint on the reference bus angles.\n#        xmin, xmax = self._ref_bus_angle_constraint(bs, Va, xmin, xmax)\n\n        # Solve using Python Interior Point Solver (PIPS).\n        s = self._solve(x0, A, l, u, xmin, xmax)\n\n        Vang, Vmag, Pgen, Qgen = self._update_solution_data(s)\n\n        self._update_case(self._bs, self._ln, self._gn, self._base_mva,\n            self._Yf, self._Yt, Vang, Vmag, Pgen, Qgen, s[\"lmbda\"])\n\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _solve(self, x0, A, l, u, xmin, xmax):\n        s = pips(self._costfcn, x0, A, l, u, xmin, xmax,\n                 self._consfcn, self._hessfcn, self.opt)\n        return s", "response": "Solves using Python Interior Point Solver."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _f(self, x, user_data=None):\n        p_gen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.\n        q_gen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.\n\n        # Polynomial cost of P and Q.\n        xx = r_[p_gen, q_gen] * self._base_mva\n        if len(self._ipol) > 0:\n            f = sum([g.total_cost(xx[i]) for i,g in enumerate(self._gn)])\n        else:\n            f = 0\n\n        # Piecewise linear cost of P and Q.\n        if self._ny:\n            y = self.om.get_var(\"y\")\n            self._ccost = csr_matrix((ones(self._ny),\n                (range(y.i1, y.iN + 1), zeros(self._ny))),\n                shape=(self._nxyz, 1)).T\n            f = f + self._ccost * x\n        else:\n            self._ccost = zeros((1, self._nxyz))\n        # TODO: Generalised cost term.\n\n        return f", "response": "Evaluates the objective function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nevaluate the cost gradient.", "response": "def _df(self, x, user_data=None):\n        \"\"\" Evaluates the cost gradient.\n        \"\"\"\n        p_gen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.\n        q_gen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.\n\n        # Polynomial cost of P and Q.\n        xx = r_[p_gen, q_gen] * self._base_mva\n\n        iPg = range(self._Pg.i1, self._Pg.iN + 1)\n        iQg = range(self._Qg.i1, self._Qg.iN + 1)\n\n        # Polynomial cost of P and Q.\n        df_dPgQg = zeros((2 * self._ng, 1))        # w.r.t p.u. Pg and Qg\n#            df_dPgQg[ipol] = matrix([g.poly_cost(xx[i], 1) for g in gpol])\n#            for i, g in enumerate(gn):\n#                der = polyder(list(g.p_cost))\n#                df_dPgQg[i] = polyval(der, xx[i]) * base_mva\n        for i in self._ipol:\n            p_cost = list(self._gn[i].p_cost)\n            df_dPgQg[i] = \\\n                self._base_mva * polyval(polyder(p_cost), xx[i])\n\n        df = zeros((self._nxyz, 1))\n        df[iPg] = df_dPgQg[:self._ng]\n        df[iQg] = df_dPgQg[self._ng:self._ng + self._ng]\n\n        # Piecewise linear cost of P and Q.\n        df = df + self._ccost.T\n        # TODO: Generalised cost term.\n\n        return asarray(df).flatten()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _d2f(self, x):\n        d2f_dPg2 = lil_matrix((self._ng, 1)) # w.r.t p.u. Pg\n        d2f_dQg2 = lil_matrix((self._ng, 1)) # w.r.t p.u. Qg]\n\n        for i in self._ipol:\n            p_cost = list(self._gn[i].p_cost)\n            d2f_dPg2[i, 0] = polyval(polyder(p_cost, 2),\n                self._Pg.v0[i] * self._base_mva) * self._base_mva**2\n#            for i in ipol:\n#                d2f_dQg2[i] = polyval(polyder(list(gn[i].p_cost), 2),\n#                                      Qg.v0[i] * base_mva) * base_mva**2\n\n        i = r_[range(self._Pg.i1, self._Pg.iN + 1),\n               range(self._Qg.i1, self._Qg.iN + 1)]\n\n        d2f = csr_matrix((vstack([d2f_dPg2, d2f_dQg2]).toarray().flatten(),\n                          (i, i)), shape=(self._nxyz, self._nxyz))\n        return d2f", "response": "Evaluates the cost Hessian."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _gh(self, x):\n        Pgen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.\n        Qgen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.\n\n        for i, gen in enumerate(self._gn):\n            gen.p = Pgen[i] * self._base_mva # active generation in MW\n            gen.q = Qgen[i] * self._base_mva # reactive generation in MVAr\n\n        # Rebuild the net complex bus power injection vector in p.u.\n        Sbus = self.om.case.getSbus(self._bs)\n\n        Vang = x[self._Va.i1:self._Va.iN + 1]\n        Vmag = x[self._Vm.i1:self._Vm.iN + 1]\n        V = Vmag * exp(1j * Vang)\n\n        # Evaluate the power flow equations.\n        mis = V * conj(self._Ybus * V) - Sbus\n\n        # Equality constraints (power flow).\n        g = r_[mis.real,  # active power mismatch for all buses\n               mis.imag]  # reactive power mismatch for all buses\n\n        # Inequality constraints (branch flow limits).\n        # (line constraint is actually on square of limit)\n        flow_max = array([(l.rate_a / self._base_mva)**2 for l in self._ln])\n        # FIXME: There must be a more elegant method for this.\n        for i, v in enumerate(flow_max):\n            if v == 0.0:\n                flow_max[i] = Inf\n\n        if self.flow_lim == IFLOW:\n            If = self._Yf * V\n            It = self._Yt * V\n            # Branch current limits.\n            h = r_[(If * conj(If)) - flow_max,\n                   (It * conj(It)) - flow_max]\n        else:\n            i_fbus = [e.from_bus._i for e in self._ln]\n            i_tbus = [e.to_bus._i for e in self._ln]\n            # Complex power injected at \"from\" bus (p.u.).\n            Sf = V[i_fbus] * conj(self._Yf * V)\n            # Complex power injected at \"to\" bus (p.u.).\n            St = V[i_tbus] * conj(self._Yt * V)\n            if self.flow_lim == PFLOW: # active power limit, P (Pan Wei)\n                # Branch real power limits.\n                h = r_[Sf.real()**2 - flow_max,\n                       St.real()**2 - flow_max]\n            elif self.flow_lim == SFLOW: # apparent power limit, |S|\n                # Branch apparent power limits.\n                h = r_[(Sf * conj(Sf)) - flow_max,\n                       (St * conj(St)) - flow_max].real\n            else:\n                raise ValueError\n\n        return h, g", "response": "Evaluate the constraint function values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating the objective function gradient and Hessian for OPF.", "response": "def _costfcn(self, x):\n        \"\"\" Evaluates the objective function, gradient and Hessian for OPF.\n        \"\"\"\n        f = self._f(x)\n        df = self._df(x)\n        d2f = self._d2f(x)\n\n        return f, df, d2f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nevaluating nonlinear constraints and their Jacobian for OPF.", "response": "def _consfcn(self, x):\n        \"\"\" Evaluates nonlinear constraints and their Jacobian for OPF.\n        \"\"\"\n        h, g = self._gh(x)\n        dh, dg = self._dgh(x)\n\n        return h, g, dh, dg"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nevaluate the Hessian of the AC OPF.", "response": "def _hessfcn(self, x, lmbda):\n        \"\"\" Evaluates Hessian of Lagrangian for AC OPF.\n        \"\"\"\n        Pgen = x[self._Pg.i1:self._Pg.iN + 1] # Active generation in p.u.\n        Qgen = x[self._Qg.i1:self._Qg.iN + 1] # Reactive generation in p.u.\n\n        for i, g in enumerate(self._gn):\n            g.p = Pgen[i] * self._base_mva # active generation in MW\n            g.q = Qgen[i] * self._base_mva # reactive generation in MVAr\n\n        Vang = x[self._Va.i1:self._Va.iN + 1]\n        Vmag = x[self._Vm.i1:self._Vm.iN + 1]\n        V = Vmag * exp(1j * Vang)\n        nxtra = self._nxyz - 2 * self._nb\n\n        #------------------------------------------------------------------\n        #  Evaluate d2f.\n        #------------------------------------------------------------------\n\n        d2f = self._d2f(x) * self.opt[\"cost_mult\"]\n        # TODO: Generalised cost model.\n\n        #------------------------------------------------------------------\n        #  Evaluate Hessian of power balance constraints.\n        #------------------------------------------------------------------\n\n        nlam = len(lmbda[\"eqnonlin\"]) / 2\n        lamP = lmbda[\"eqnonlin\"][:nlam]\n        lamQ = lmbda[\"eqnonlin\"][nlam:nlam + nlam]\n        Gpaa, Gpav, Gpva, Gpvv = self.om.case.d2Sbus_dV2(self._Ybus, V, lamP)\n        Gqaa, Gqav, Gqva, Gqvv = self.om.case.d2Sbus_dV2(self._Ybus, V, lamQ)\n\n        d2G = vstack([\n            hstack([\n                vstack([hstack([Gpaa, Gpav]),\n                        hstack([Gpva, Gpvv])]).real +\n                vstack([hstack([Gqaa, Gqav]),\n                        hstack([Gqva, Gqvv])]).imag,\n                csr_matrix((2 * self._nb, nxtra))]),\n            hstack([\n                csr_matrix((nxtra, 2 * self._nb)),\n                csr_matrix((nxtra, nxtra))\n            ])\n        ], \"csr\")\n\n        #------------------------------------------------------------------\n        #  Evaluate Hessian of flow constraints.\n        #------------------------------------------------------------------\n\n        nmu = len(lmbda[\"ineqnonlin\"]) / 2\n        muF = lmbda[\"ineqnonlin\"][:nmu]\n        muT = lmbda[\"ineqnonlin\"][nmu:nmu + nmu]\n        if self.flow_lim == \"I\":\n            dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It = \\\n                self.om.case.dIbr_dV(self._Yf, self._Yt, V)\n            Hfaa, Hfav, Hfva, Hfvv = \\\n                self.om.case.d2AIbr_dV2(dIf_dVa, dIf_dVm, If, self._Yf, V, muF)\n            Htaa, Htav, Htva, Htvv = \\\n                self.om.case.d2AIbr_dV2(dIt_dVa, dIt_dVm, It, self._Yt, V, muT)\n        else:\n            f = [e.from_bus._i for e in self._ln]\n            t = [e.to_bus._i for e in self._ln]\n            # Line-bus connection matrices.\n            Cf = csr_matrix((ones(self._nl), (range(self._nl), f)), (self._nl, self._nb))\n            Ct = csr_matrix((ones(self._nl), (range(self._nl), t)), (self._nl, self._nb))\n            dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St = \\\n                self.om.case.dSbr_dV(self._Yf, self._Yt, V)\n            if self.flow_lim == PFLOW:\n                Hfaa, Hfav, Hfva, Hfvv = \\\n                    self.om.case.d2ASbr_dV2(dSf_dVa.real(), dSf_dVm.real(),\n                                            Sf.real(), Cf, self._Yf, V, muF)\n                Htaa, Htav, Htva, Htvv = \\\n                    self.om.case.d2ASbr_dV2(dSt_dVa.real(), dSt_dVm.real(),\n                                            St.real(), Ct, self._Yt, V, muT)\n            elif self.flow_lim == SFLOW:\n                Hfaa, Hfav, Hfva, Hfvv = \\\n                    self.om.case.d2ASbr_dV2(\n                        dSf_dVa, dSf_dVm, Sf, Cf, self._Yf, V, muF)\n                Htaa, Htav, Htva, Htvv = \\\n                    self.om.case.d2ASbr_dV2(\n                        dSt_dVa, dSt_dVm, St, Ct, self._Yt, V, muT)\n            else:\n                raise ValueError\n\n        d2H = vstack([\n            hstack([\n                vstack([hstack([Hfaa, Hfav]),\n                        hstack([Hfva, Hfvv])]) +\n                vstack([hstack([Htaa, Htav]),\n                        hstack([Htva, Htvv])]),\n                csr_matrix((2 * self._nb, nxtra))\n            ]),\n            hstack([\n                csr_matrix((nxtra, 2 * self._nb)),\n                csr_matrix((nxtra, nxtra))\n            ])\n        ], \"csr\")\n\n        return d2f + d2G + d2H"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_solution_data(self, s):\n        x = s[\"x\"]\n#        Va_var = self.om.get_var(\"Va\")\n#        Vm_var = self.om.get_var(\"Vm\")\n#        Pg_var = self.om.get_var(\"Pg\")\n#        Qg_var = self.om.get_var(\"Qg\")\n\n        Va = x[self._Va.i1:self._Va.iN + 1]\n        Vm = x[self._Vm.i1:self._Vm.iN + 1]\n        Pg = x[self._Pg.i1:self._Pg.iN + 1]\n        Qg = x[self._Qg.i1:self._Qg.iN + 1]\n\n#        f = 0.5 * dot(x.T * HH, x) + dot(CC.T, x)\n\n#        s[\"f\"] = s[\"f\"] + C0\n\n        # Put the objective function value in the solution.\n#        solution[\"f\"] = f\n\n        return Va, Vm, Pg, Qg", "response": "Updates the solution data with the values from the voltage angle and generator set - point vectors."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the result attribute values for the current instance of the class.", "response": "def _update_case(self, bs, ln, gn, base_mva, Yf, Yt, Va, Vm, Pg, Qg,lmbda):\n        \"\"\" Calculates the result attribute values.\n        \"\"\"\n        V = Vm * exp(1j * Va)\n\n#        Va_var = self.om.get_var(\"Va\")\n        Vm_var = self._Vm\n        Pmis = self.om.get_nln_constraint(\"Pmis\")\n        Qmis = self.om.get_nln_constraint(\"Qmis\")\n        Pg_var = self._Pg\n        Qg_var = self._Qg\n\n#        mu_l = lmbda[\"mu_l\"]\n#        mu_u = lmbda[\"mu_u\"]\n        lower = lmbda[\"lower\"]\n        upper = lmbda[\"upper\"]\n\n        ineqnonlin = lmbda[\"ineqnonlin\"]\n        eqnonlin = lmbda[\"eqnonlin\"]\n\n        # Indexes of constrained lines.\n        nl2 = len([i for i,l in enumerate(ln) if 0.0 < l.rate_a < 1e10])\n\n        for i, bus in enumerate(bs):\n            bus.v_angle = Va[i] * 180.0 / pi\n            bus.v_magnitude = Vm[i]\n\n            bus.p_lmbda = eqnonlin[Pmis.i1:Pmis.iN + 1][i] / base_mva\n            bus.q_lmbda = eqnonlin[Qmis.i1:Qmis.iN + 1][i] / base_mva\n\n            bus.mu_vmax = upper[Vm_var.i1:Vm_var.iN + 1][i]\n            bus.mu_vmin = lower[Vm_var.i1:Vm_var.iN + 1][i]\n\n        for l, branch in enumerate(ln):\n            Sf = V[branch.from_bus._i] * conj(Yf[l, :] * V) * base_mva\n            St = V[branch.to_bus._i] * conj(Yt[l, :] * V) * base_mva\n\n            branch.p_from = Sf.real[0]\n            branch.q_from = Sf.imag[0]\n            branch.p_to = St.real[0]\n            branch.q_to = St.imag[0]\n\n            if 0.0 < branch.rate_a < 1e10:\n                branch.mu_s_from = \\\n                    2 * ineqnonlin[:nl2][l] * branch.rate_a / base_mva / base_mva\n                branch.mu_s_to = \\\n                    2 * ineqnonlin[nl2:2*nl2][l] * branch.rate_a / base_mva / base_mva\n\n        for k, generator in enumerate(gn):\n            generator.p = Pg[k] * base_mva\n            generator.q = Qg[k] * base_mva\n            generator.v_magnitude = generator.bus.v_magnitude\n\n            generator.mu_pmax = upper[Pg_var.i1:Pg_var.iN + 1][k] / base_mva\n            generator.mu_pmin = lower[Pg_var.i1:Pg_var.iN + 1][k] / base_mva\n\n            generator.mu_qmax = upper[Qg_var.i1:Qg_var.iN + 1][k] / base_mva\n            generator.mu_qmin = lower[Qg_var.i1:Qg_var.iN + 1][k] / base_mva"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self, serializable: Optional[Union[SerializableType, List[SerializableType]]]) \\\n            -> PrimitiveJsonType:\n        \"\"\"\n        Serializes the given serializable object or collection of serializable objects.\n        :param serializable: the object or objects to serialize\n        :return: a serialization of the given object\n        \"\"\"\n        if serializable is None:\n            # Implements #17\n            return None\n        elif isinstance(serializable, List):\n            return [self.serialize(item) for item in serializable]\n        else:\n            serialized = self._create_serialized_container()\n\n            for mapping in self._property_mappings:\n                if mapping.object_property_getter is not None and mapping.serialized_property_setter is not None:\n                    value = mapping.object_property_getter(serializable)\n                    if not (mapping.optional and value is None):\n                        if isinstance(value, type(mapping.collection_factory([]))):\n                            value = list(mapping.collection_iter(value))\n                        encoded_value = self._serialize_property_value(value, mapping.serializer_cls)\n                        mapping.serialized_property_setter(serialized, encoded_value)\n\n            return serialized", "response": "Serializes the given object or collection of serializable objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _serialize_property_value(self, to_serialize: Any, serializer_cls: Type) -> Any:\n        serializer = self._create_serializer_of_type_with_cache(serializer_cls)\n        assert serializer is not None\n        return serializer.serialize(to_serialize)", "response": "Serializes the given value using the given serializer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a serializer of the given type with a cache.", "response": "def _create_serializer_of_type_with_cache(self, serializer_type: Type) -> \"Serializer\":\n        \"\"\"\n        Creates a deserializer of the given type, exploiting a cache.\n        :param serializer_type: the type of deserializer to create\n        :return: the created serializer\n        \"\"\"\n        if serializer_type not in self._serializers_cache:\n            self._serializers_cache[serializer_type] = self._create_serializer_of_type(serializer_type)\n        return self._serializers_cache[serializer_type]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef deserialize(self, to_deserialize: PrimitiveJsonType) \\\n            -> Optional[Union[SerializableType, List[SerializableType]]]:\n        \"\"\"\n        Deserializes the given representation of the serialized object.\n        :param to_deserialize: the serialized object as a dictionary\n        :return: the deserialized object or collection of deserialized objects\n        \"\"\"\n        if to_deserialize is None:\n            # Implements #17\n            return None\n        elif isinstance(to_deserialize, List):\n            deserialized = []\n            for item in to_deserialize:\n                item_deserialized = self.deserialize(item)\n                deserialized.append(item_deserialized)\n            return deserialized\n        else:\n            mappings_not_set_in_constructor = []    # type: List[PropertyMapping]\n\n            init_kwargs = dict()    # type: Dict[str, Any]\n            for mapping in self._property_mappings:\n                if mapping.object_constructor_parameter_name is not None:\n                    value = mapping.serialized_property_getter(to_deserialize)\n                    if not (mapping.optional and value is None):\n                        decoded_value = self._deserialize_property_value(value, mapping.deserializer_cls)\n                        if isinstance(decoded_value, list):\n                            collection = mapping.collection_factory(decoded_value)\n                            decoded_value = collection\n\n                        argument = mapping.object_constructor_argument_modifier(decoded_value)\n                        init_kwargs[mapping.object_constructor_parameter_name] = argument\n                else:\n                    mappings_not_set_in_constructor.append(mapping)\n\n            decoded = self._deserializable_cls(**init_kwargs)\n            assert type(decoded) == self._deserializable_cls\n\n            for mapping in mappings_not_set_in_constructor:\n                assert mapping.object_constructor_parameter_name is None\n                if mapping.serialized_property_getter is not None and mapping.object_property_setter is not None:\n                    value = mapping.serialized_property_getter(to_deserialize)\n                    if not (mapping.optional and value is None):\n                        decoded_value = self._deserialize_property_value(value, mapping.deserializer_cls)\n                        if isinstance(decoded_value, list):\n                            collection = mapping.collection_factory(decoded_value)\n                            decoded_value = collection\n\n                        mapping.object_property_setter(decoded, decoded_value)\n\n            return decoded", "response": "Deserializes the given representation of the serialized object into a list of deserialized objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _deserialize_property_value(self, to_deserialize: PrimitiveJsonType, deserializer_cls: Type) -> Any:\n        deserializer = self._create_deserializer_of_type_with_cache(deserializer_cls)\n        assert deserializer is not None\n        return deserializer.deserialize(to_deserialize)", "response": "Deserializes the given value using the given deserializer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a deserializer of the given type with a cache.", "response": "def _create_deserializer_of_type_with_cache(self, deserializer_type: Type) -> \"Deserializer\":\n        \"\"\"\n        Creates a deserializer of the given type, exploiting a cache.\n        :param deserializer_type: the type of deserializer to create\n        :return: the deserializer\n        \"\"\"\n        if deserializer_type not in self._deserializers_cache:\n            self._deserializers_cache[deserializer_type] = self._create_deserializer_of_type(deserializer_type)\n        return self._deserializers_cache[deserializer_type]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sim(model: HybridOde, options: Dict = None,  # noqa: too-complex\n        user_callback=None) -> Dict[str, np.array]:\n    \"\"\"\n    Simulates a Dae model.\n\n    :model: The model to simulate\n    :options: See opt dict below\n    :user_callback: A routine to call after each integration step,\n      f(t, x, y, m, p, c) -> ret   (ret < 0 means abort)\n    \"\"\"\n    if model.f_x_rhs.shape[0] < 1:\n        raise ValueError(\"there are no ODE equations to simulate, \"\n                         \"check that the model is explicit\")\n\n    ic = {}\n    for f in ['x', 'y', 'm', 'p']:\n        ic[f] = []\n        for x in ca.vertsplit(getattr(model, f)):\n            start = model.prop[x.name()]['start']\n            value = model.prop[x.name()]['value']\n            if start is not None:\n                ic[f].append(ca.reshape(start, x.numel(), 1))\n            elif value is not None:\n                ic[f].append(ca.reshape(value, x.numel(), 1))\n            else:\n                ic[f].append(ca.DM.zeros(x.numel(), 1))\n                Warning(\"using default start value for\", x.name())\n\n        ic[f] = np.array([ic[f]], dtype=float).T\n\n    # set options\n    opt = {\n        'x0': ic['x'],\n        'p': ic['p'],\n        't0': 0,\n        'tf': 1,\n        'dt': None,\n        'integrator': 'dopri5',\n        'atol': 1e-6,\n        'rtol': 1e-6,\n        'max_step': None,\n        'record_event_times': True,\n        'verbose': False,\n    }\n    if options is not None:\n        for k in options.keys():\n            if k in opt.keys():\n                opt[k] = options[k]\n            else:\n                raise ValueError(\"unknown option {:s}\".format(k))\n    if opt['dt'] is None:\n        opt['dt'] = opt['tf']/100\n    if opt['max_step'] is None:\n        opt['max_step'] = opt['dt']/2\n\n    # create functions\n    f_y = model.create_function_f_y()\n    f_c = model.create_function_f_c()\n    f_m = model.create_function_f_m()\n    f_x_rhs = model.create_function_f_x_rhs()\n    f_J = model.create_function_f_J()\n    f_i = model.create_function_f_i()\n\n    # initialize sim loop\n    t0 = opt['t0']\n    tf = opt['tf']\n    x = opt['x0']\n    ng = np.zeros(model.ng.shape[0])\n    nu = np.zeros(model.nu.shape[0])\n    m = ic['m']\n    p = opt['p']\n    y0 = ic['y']\n    pre_c = np.array(f_c(t0, x, y0, m, p, ng, nu))\n    c = pre_c\n    y = f_y(t0, x, m, p, c, ng, nu)\n    dt = opt['dt']\n    t_vect = np.arange(t0, tf, dt)\n    n = len(t_vect)\n    data = {\n        't': [],\n        'x': [],\n        'm': [],\n        'y': [],\n        'c': [],\n    }\n\n    # create integrator\n    integrator = scipy.integrate.ode(f_x_rhs, f_J)\n    integrator.set_integrator(\n        opt['integrator'],\n        first_step=opt['max_step'],\n        atol=opt['atol'],\n        rtol=opt['rtol'],\n        max_step=opt['max_step'],\n    )\n\n    # try to catch events with sol out, (root finding)\n    def sol_out(t, x):\n        c = np.array(f_c(t, x, y, m, p, ng, nu))\n        if np.any(c != pre_c):\n            # print('event', t)\n            return -1\n        return 0\n    if opt['integrator'] in ['dopri5', 'dopri853']:\n        integrator.set_solout(sol_out)\n\n    # run sim loop\n    i = 0\n    dt_f_i = 0\n    dt_integrate = 0\n    integrator.set_initial_value(opt['x0'], t0)\n\n    while i < n:\n        t = integrator.t\n\n        # resample noise\n        ng = np.random.randn(model.ng.shape[0])\n        nu = np.random.randn(model.nu.shape[0])\n\n        # call reinit\n        start = time.time()\n        x = f_i(t, x, y, m, p, c, pre_c, ng, nu)\n        dt_f_i += (time.time() - start)\n\n        # setup next continuous integration step\n        integrator.set_initial_value(x, t)\n        integrator.set_f_params(y, m, p, c, ng, nu)\n        integrator.set_jac_params(y, m, p, c, ng, nu)\n\n        # integrate\n        t_goal = t0 + i*dt\n        start = time.time()\n        integrator.integrate(t_goal)\n        dt_integrate += (time.time() - start)\n        x = integrator.y\n\n        # compute new conditions\n        pre_c = c\n        c = np.array(f_c(t, x, y, m, p, ng, nu))\n\n        # compute output\n        y = f_y(t, x, m, p, c, ng, nu)\n\n        # compute discrete states\n        m = f_m(t, x, y, m, p, c, pre_c, ng, nu)\n\n        # store data\n        if opt['record_event_times'] or (integrator.t - t_goal == 0):\n            data['t'].append(t)\n            data['x'].append(ca.vertsplit(x))\n            data['y'].append(ca.vertsplit(y))\n            data['m'].append(ca.vertsplit(m))\n            data['c'].append(ca.vertsplit(c))\n            if user_callback is not None:\n                user_callback(t, x, y, m, p, c)\n\n        # increment time goal if reached\n        if integrator.t - t_goal == 0:\n            # update discrete states\n            # TODO: make this use sampling\n            i += 1\n\n    for k in data.keys():\n        data[k] = np.array(data[k])\n\n    data['labels'] = {}\n    for field in ['x', 'y', 'c', 'm']:\n        data['labels'][field] = [x.name() for x in ca.vertsplit(getattr(model, field))]\n\n    if opt['verbose']:\n        print('dt_f_i\\t\\t\\t:', dt_f_i)\n        print('dt_integrate\\t:', dt_integrate)\n    return data", "response": "Simulates a Dae model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, file_or_filename):\n        if isinstance(file_or_filename, basestring):\n            fname = os.path.basename(file_or_filename)\n            logger.info(\"Unpickling case file [%s].\" % fname)\n\n            file = None\n            try:\n                file = open(file_or_filename, \"rb\")\n            except:\n                logger.error(\"Error opening %s.\" % fname)\n                return None\n            finally:\n                if file is not None:\n                    case = pickle.load(file)\n                    file.close()\n        else:\n            file = file_or_filename\n            case = pickle.load(file)\n\n        return case", "response": "Reads a pickled case."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write(self, file_or_filename):\n        if isinstance(file_or_filename, basestring):\n            fname = os.path.basename(file_or_filename)\n            logger.info(\"Pickling case [%s].\" % fname)\n\n            file = None\n            try:\n                file = open(file_or_filename, \"wb\")\n            except:\n                logger.error(\"Error opening '%s'.\" % (fname))\n                return False\n            finally:\n                if file is not None:\n                    pickle.dump(self.case, file)\n                    file.close()\n        else:\n            file = file_or_filename\n            pickle.dump(file, self.case)\n\n        return True", "response": "Writes the case to file using pickle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_token(self, tok):\n        if(tok[0].__str__() in ('Token.Comment.Multiline', 'Token.Comment',\n                'Token.Literal.String.Doc')):\n            self.comments += tok[1].count('\\n')+1\n        elif(tok[0].__str__() in ('Token.Comment.Single')):\n            self.comments += 1\n        elif(self.contains_code and tok[0].__str__().startswith('Token.Text')\n                and tok[1].count(u'\\n')):\n            # start new line\n            self.contains_code = False\n            self.sloc += 1\n        # for c style includes\n        elif(tok[0].__str__() == 'Token.Comment.Preproc' and\n                tok[1].count(u'\\n')):\n            # start new line\n            self.contains_code = False\n            self.sloc += 1\n        elif(tok[0][0] in token_types):\n            self.contains_code = True", "response": "count comments and non - empty lines that contain code"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate ratio_comment_to_code and return with the other values", "response": "def get_metrics(self):\n        \"\"\"Calculate ratio_comment_to_code and return with the other values\"\"\"\n        if(self.sloc == 0):\n            if(self.comments == 0):\n                ratio_comment_to_code = 0.00\n            else:\n                ratio_comment_to_code = 1.00\n        else:\n            ratio_comment_to_code = float(self.comments) / self.sloc\n        metrics = OrderedDict([('sloc', self.sloc), ('comments', self.comments),\n                               ('ratio_comment_to_code', round(ratio_comment_to_code, 2))])\n        return metrics"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getSensors(self):\n        Pd = array([b.p_demand for b in self.case.buses if b.type == PQ])\n        logger.info(\"State: %s\" % list(Pd))\n        return Pd", "response": "Returns the currently visible state of the world as a numpy array of doubles."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef performAction(self, action):\n        gs = [g for g in self.case.online_generators if g.bus.type !=REFERENCE]\n\n        assert len(action) == len(gs)\n\n        logger.info(\"Action: %s\" % list(action))\n\n        # Set the output of each (non-reference) generator.\n        for i, g in enumerate(gs):\n            g.p = action[i]\n\n        # Compute power flows and slack generator set-point.\n        NewtonPF(self.case, verbose=False).solve()\n        #FastDecoupledPF(self.case, verbose=False).solve()\n\n        # Store all generator set-points (only used for plotting).\n        self._Pg[:, self._step] = [g.p for g in self.case.online_generators]\n\n        # Apply the next load profile value to the original demand at each bus.\n        if self._step != len(self.profile) - 1:\n            pq_buses = [b for b in self.case.buses if b.type == PQ]\n            for i, b in enumerate(pq_buses):\n                b.p_demand = self._Pd0[i] * self.profile[self._step + 1]\n\n        self._step += 1\n\n        logger.info(\"Entering step %d.\" % self._step)", "response": "Perform an action on the world that changes it s internal state."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the environment to its initial state.", "response": "def reset(self):\n        \"\"\" Re-initialises the environment.\n        \"\"\"\n        logger.info(\"Reseting environment.\")\n\n        self._step = 0\n\n        # Reset the set-point of each generator to its original value.\n        gs = [g for g in self.case.online_generators if g.bus.type !=REFERENCE]\n        for i, g in enumerate(gs):\n            g.p = self._Pg0[i]\n\n        # Apply load profile to the original demand at each bus.\n        for i, b in enumerate([b for b in self.case.buses if b.type == PQ]):\n            b.p_demand = self._Pd0[i] * self.profile[self._step]\n\n        # Initialise the record of generator set-points.\n        self._Pg = zeros((len(self.case.online_generators), len(self.profile)))\n\n        # Apply the first load profile value.\n#        self.step()\n\n        self.case.reset()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the reward corresponding to the last action performed.", "response": "def getReward(self):\n        \"\"\" Returns the reward corresponding to the last action performed.\n        \"\"\"\n        on = self.env.case.online_generators\n        generators = [g for g in on if g.bus.type != REFERENCE]\n\n        cost = sum([g.total_cost() for g in generators])\n\n\n        ref_penalty = 1000.0\n        refs = [g for g in on if g.bus.type == REFERENCE]\n        for g in refs:\n            # Do not receive payment for negative Pg at slack bus.\n            if g.p > 0.0:\n                cost += g.total_cost()\n            # Add a penalty if the output of the slack generator is infeasible.\n            if not (g.p_min <= g.p <= g.p_max):\n                cost += ref_penalty\n#                logger.info(\"Infeasible slack generator output: %.3f\" % g.p)\n\n#        logger.info(\"Cost: %.3f\" % cost)\n\n        return -cost"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nis the current episode over?", "response": "def isFinished(self):\n        \"\"\" Is the current episode over?\n        \"\"\"\n        finished = (self.env._step == len(self.env.profile))\n        if finished:\n            logger.info(\"Finished episode.\")\n        return finished"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of 2 - tuples one tuple per parameter giving min and max for that parameter.", "response": "def getSensorLimits(self):\n        \"\"\" Returns a list of 2-tuples, e.g. [(-3.14, 3.14), (-0.001, 0.001)],\n        one tuple per parameter, giving min and max for that parameter.\n        \"\"\"\n        limits = []\n        for i in range(len([b for b in self.env.case.buses if b.type == PQ])):\n            limits.append((0.0, self.env._Pd0[i]))\n\n        logger.info(\"Sensor limits: %s\" % limits)\n\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of 2 - tuples one tuple per parameter giving min and max for that parameter.", "response": "def getActorLimits(self):\n        \"\"\" Returns a list of 2-tuples, e.g. [(-3.14, 3.14), (-0.001, 0.001)],\n        one tuple per parameter, giving min and max for that parameter.\n        \"\"\"\n        generators = [g for g in self.env.case.online_generators\n                      if g.bus.type != REFERENCE]\n        limits = []\n        for g in generators:\n            limits.append((g.p_min, g.p_max))\n\n        logger.info(\"Actor limits: %s\" % limits)\n\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _oneInteraction(self):\n        if self.doOptimization:\n            raise Exception('When using a black-box learning algorithm, only full episodes can be done.')\n        else:\n            self.stepid += 1\n            self.agent.integrateObservation(self.task.getObservation())\n            self.task.performAction(self.agent.getAction())\n\n            # Save the cumulative sum of set-points for each period.\n            for i, g in enumerate(self.task.env.case.online_generators):\n                self.Pg[i, self.stepid - 1] = self.Pg[i, self.stepid - 1] + g.p\n\n            reward = self.task.getReward()\n            self.agent.giveReward(reward)\n            return reward", "response": "Does one interaction between the task and the agent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef doEpisodes(self, number=1):\n        env = self.task.env\n        self.Pg = zeros((len(env.case.online_generators), len(env.profile)))\n\n        rewards = super(OPFExperiment, self).doEpisodes(number)\n\n        # Average the set-points for each period.\n        self.Pg = self.Pg / number\n\n        return rewards", "response": "Does the given number of episodes."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the JSON encoders registered for the given type.", "response": "def get_json_encoders_for_type(self, type_to_encode: type) -> Optional[Iterable[JSONEncoder]]:\n        \"\"\"\n        Gets the registered JSON encoder for the given type.\n        :param type_to_encode: the type of object that is to be encoded\n        :return: the encoder for the given object else `None` if unknown\n        \"\"\"\n        if type_to_encode not in self._json_encoders:\n            return None\n        return self._json_encoders[type_to_encode]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nregister the given JSON encoder for use with the given object type.", "response": "def register_json_encoder(self, encoder_type: type, encoder: JSONEncoder):\n        \"\"\"\n        Register the given JSON encoder for use with the given object type.\n        :param encoder_type: the type of object to encode\n        :param encoder: the JSON encoder\n        :return: this builder\n        \"\"\"\n        self._json_encoders[encoder_type] = encoder\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self) -> RegisteredTypeJSONEncoderType:\n        class_name = \"%s_%s\" % (_RegisteredTypeJSONEncoder.__class__.__name__, id(self))\n        # Use encoders set at the point in time at which the encoder was built\n        builder_snapshot = copy.deepcopy(self)\n        return type(\n                class_name,\n                (_RegisteredTypeJSONEncoder, ),\n                {\n                    \"_get_json_encoders_for_type\": builder_snapshot.get_json_encoders_for_type\n                }\n        )", "response": "Builds a new JSON encoder that uses the encoders registered at the point in time when this method is called."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef getMethodByName(obj, name):\n\n    \"\"\"searches for an object with the name given inside the object given.\n       \"obj.child.meth\" will return the meth obj.\n    \"\"\"\n    try:#to get a method by asking the service\n        obj = obj._getMethodByName(name)\n    except:\n        #assumed a childObject is ment \n        #split the name from objName.childObjName... -> [objName, childObjName, ...]\n        #and get all objects up to the last in list with name checking from the service object\n        names = name.split(\".\")\n        for name in names:\n            if nameAllowed(name):\n                obj = getattr(obj, name)\n            else:\n                raise MethodNameNotAllowed()\n        \n    return obj", "response": "searches for an object with the name given inside the object given."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nblocking until the response arrived or timeout is reached.", "response": "def waitForResponse(self, timeOut=None):\n        \"\"\"blocks until the response arrived or timeout is reached.\"\"\"\n        self.__evt.wait(timeOut)\n        if self.waiting():\n            raise Timeout()\n        else:\n            if self.response[\"error\"]:\n                raise Exception(self.response[\"error\"])\n            else:\n                return self.response[\"result\"]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a request to the peer", "response": "def sendRequest(self, name, args):\n        \"\"\"sends a request to the peer\"\"\"\n        (respEvt, id) = self.newResponseEvent()\n        self.sendMessage({\"id\":id, \"method\":name, \"params\": args})\n        return respEvt"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a response to the peer", "response": "def sendResponse(self, id, result, error):\n        \"\"\"sends a response to the peer\"\"\"\n        self.sendMessage({\"result\":result, \"error\": error, \"id\":id})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new response event and adds it to the waiting list", "response": "def newResponseEvent(self):\n        \"\"\"creates a response event and adds it to a waiting list\n           When the reponse arrives it will be removed from the list. \n        \"\"\"\n        respEvt = ResponseEvent()\n        self.respLock.acquire()\n        eid = id(respEvt)\n        self.respEvents[eid] = respEvt\n        self.respLock.release()\n        return (respEvt,eid)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a response by fireing the response event for the response coming in", "response": "def handleResponse(self, resp):\n        \"\"\"handles a response by fireing the response event for the response coming in\"\"\"\n        id=resp[\"id\"]\n        evt = self.respEvents[id]\n        del(self.respEvents[id])\n        evt.handleResponse(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handleRequest(self, req):\n        name = req[\"method\"]\n        params = req[\"params\"]\n        id=req[\"id\"]\n        obj=None\n        try: #to get a callable obj \n            obj = getMethodByName(self.service, name)\n        except MethodNameNotAllowed,e:\n            self.sendResponse(id, None, e)\n        except:\n            self.sendResponse(id, None, MethodNotFound())\n        if obj:\n            try: #to call the object with parameters\n                rslt = obj(*params)\n                self.sendResponse(id, rslt, None)\n            except TypeError: # wrong arguments\n                #todo what if the TypeError was not thrown directly by the callable obj\n                s=getTracebackStr()\n                self.sendResponse(id, None, InvalidMethodParameters())\n            except: #error inside the callable object\n                s=getTracebackStr()\n                self.sendResponse(id, None, s)", "response": "handles a request by calling the appropriete method the service exposes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef handleNotification(self, req):\n        name = req[\"method\"]\n        params = req[\"params\"]\n        try: #to get a callable obj \n            obj = getMethodByName(self.service, name)\n            rslt = obj(*params)\n        except:\n            pass", "response": "handles a notification request by calling the appropriete method the service exposes"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a PSAT data file and returns a Case object.", "response": "def read(self, file_or_filename):\n        \"\"\" Parses a PSAT data file and returns a case object\n\n            file_or_filename: File object or path to PSAT data file\n            return: Case object\n        \"\"\"\n        self.file_or_filename = file_or_filename\n\n        logger.info(\"Parsing PSAT case file [%s].\" % file_or_filename)\n\n        t0 = time.time()\n\n        self.case = Case()\n\n        # Name the case\n        if isinstance(file_or_filename, basestring):\n            name, _ = splitext(basename(file_or_filename))\n        else:\n            name, _ = splitext(file_or_filename.name)\n\n        self.case.name = name\n\n        bus_array = self._get_bus_array_construct()\n        line_array = self._get_line_array_construct()\n        # TODO: Lines.con - Alternative line data format\n        slack_array = self._get_slack_array_construct()\n        pv_array = self._get_pv_array_construct()\n        pq_array = self._get_pq_array_construct()\n        demand_array = self._get_demand_array_construct()\n        supply_array = self._get_supply_array_construct()\n        # TODO: Varname.bus (Bus names)\n\n        # Pyparsing case:\n        case = \\\n            ZeroOrMore(matlab_comment) + bus_array + \\\n            ZeroOrMore(matlab_comment) + line_array + \\\n            ZeroOrMore(matlab_comment) + slack_array + \\\n            ZeroOrMore(matlab_comment) + pv_array + \\\n            ZeroOrMore(matlab_comment) + pq_array + \\\n            ZeroOrMore(matlab_comment) + demand_array + \\\n            ZeroOrMore(matlab_comment) + supply_array\n\n        case.parseFile(file_or_filename)\n\n        elapsed = time.time() - t0\n        logger.info(\"PSAT case file parsed in %.3fs.\" % elapsed)\n\n        return self.case"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_bus_array_construct(self):\n        bus_no = integer.setResultsName(\"bus_no\")\n        v_base = real.setResultsName(\"v_base\") # kV\n        v_magnitude = Optional(real).setResultsName(\"v_magnitude\")\n        v_angle = Optional(real).setResultsName(\"v_angle\") # radians\n        area = Optional(integer).setResultsName(\"area\") # not used yet\n        region = Optional(integer).setResultsName(\"region\") # not used yet\n\n        bus_data = bus_no + v_base + v_magnitude + v_angle + \\\n            area + region + scolon\n\n        bus_data.setParseAction(self.push_bus)\n\n        bus_array = Literal(\"Bus.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(bus_data + Optional(\"]\" + scolon))\n\n        # Sort buses according to their name (bus_no)\n        bus_array.setParseAction(self.sort_buses)\n\n        return bus_array", "response": "Returns a construct for an array of bus data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_line_array_construct(self):\n        from_bus = integer.setResultsName(\"fbus\")\n        to_bus = integer.setResultsName(\"tbus\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        v_rating = real.setResultsName(\"v_rating\") # kV\n        f_rating = real.setResultsName(\"f_rating\") # Hz\n        length = real.setResultsName(\"length\") # km (Line only)\n        v_ratio = real.setResultsName(\"v_ratio\") # kV/kV (Transformer only)\n        r = real.setResultsName(\"r\") # p.u. or Ohms/km\n        x = real.setResultsName(\"x\") # p.u. or Henrys/km\n        b = real.setResultsName(\"b\") # p.u. or Farads/km (Line only)\n        tap_ratio = real.setResultsName(\"tap\") # p.u./p.u. (Transformer only)\n        phase_shift = real.setResultsName(\"shift\") # degrees (Transformer only)\n        i_limit = Optional(real).setResultsName(\"i_limit\") # p.u.\n        p_limit = Optional(real).setResultsName(\"p_limit\") # p.u.\n        s_limit = Optional(real).setResultsName(\"s_limit\") # p.u.\n        status = Optional(boolean).setResultsName(\"status\")\n\n        line_data = from_bus + to_bus + s_rating + v_rating + \\\n            f_rating + length + v_ratio + r + x + b + tap_ratio + \\\n            phase_shift + i_limit + p_limit + s_limit + status + scolon\n\n        line_data.setParseAction(self.push_line)\n\n        line_array = Literal(\"Line.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(line_data + Optional(\"]\" + scolon))\n\n        return line_array", "response": "Returns a construct for an array of line data."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_slack_array_construct(self):\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        v_rating = real.setResultsName(\"v_rating\") # kV\n        v_magnitude = real.setResultsName(\"v_magnitude\") # p.u.\n        ref_angle = real.setResultsName(\"ref_angle\") # p.u.\n        q_max = Optional(real).setResultsName(\"q_max\") # p.u.\n        q_min = Optional(real).setResultsName(\"q_min\") # p.u.\n        v_max = Optional(real).setResultsName(\"v_max\") # p.u.\n        v_min = Optional(real).setResultsName(\"v_min\") # p.u.\n        p_guess = Optional(real).setResultsName(\"p_guess\") # p.u.\n        # Loss participation coefficient\n        lp_coeff = Optional(real).setResultsName(\"lp_coeff\")\n        ref_bus = Optional(boolean).setResultsName(\"ref_bus\")\n        status = Optional(boolean).setResultsName(\"status\")\n\n        slack_data = bus_no + s_rating + v_rating + v_magnitude + \\\n            ref_angle + q_max + q_min + v_max + v_min + p_guess + \\\n            lp_coeff + ref_bus + status + scolon\n\n        slack_data.setParseAction(self.push_slack)\n\n        slack_array = Literal(\"SW.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(slack_data + Optional(\"]\" + scolon))\n\n        return slack_array", "response": "Returns a construct for an array of slack bus data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a construct for an array of PV generator data.", "response": "def _get_pv_array_construct(self):\n        \"\"\" Returns a construct for an array of PV generator data.\n        \"\"\"\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        v_rating = real.setResultsName(\"v_rating\") # kV\n        p = real.setResultsName(\"p\") # p.u.\n        v = real.setResultsName(\"v\") # p.u.\n        q_max = Optional(real).setResultsName(\"q_max\") # p.u.\n        q_min = Optional(real).setResultsName(\"q_min\") # p.u.\n        v_max = Optional(real).setResultsName(\"v_max\") # p.u.\n        v_min = Optional(real).setResultsName(\"v_min\") # p.u.\n        # Loss participation coefficient\n        lp_coeff = Optional(real).setResultsName(\"lp_coeff\")\n        status = Optional(boolean).setResultsName(\"status\")\n\n        pv_data = bus_no + s_rating + v_rating + p + v + q_max + \\\n            q_min + v_max + v_min + lp_coeff + status + scolon\n\n        pv_data.setParseAction(self.push_pv)\n\n        pv_array = Literal(\"PV.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(pv_data + Optional(\"]\" + scolon))\n\n        return pv_array"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a construct for an array of PQ load data.", "response": "def _get_pq_array_construct(self):\n        \"\"\" Returns a construct for an array of PQ load data.\n        \"\"\"\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        v_rating = real.setResultsName(\"v_rating\") # kV\n        p = real.setResultsName(\"p\") # p.u.\n        q = real.setResultsName(\"q\") # p.u.\n        v_max = Optional(real).setResultsName(\"v_max\") # p.u.\n        v_min = Optional(real).setResultsName(\"v_min\") # p.u.\n        # Allow conversion to impedance\n        z_conv = Optional(boolean).setResultsName(\"z_conv\")\n        status = Optional(boolean).setResultsName(\"status\")\n\n        pq_data = bus_no + s_rating + v_rating + p + q + v_max + \\\n            v_min + z_conv + status + scolon\n\n        pq_data.setParseAction(self.push_pq)\n\n        pq_array = Literal(\"PQ.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(pq_data + Optional(\"]\" + scolon))\n\n        return pq_array"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_demand_array_construct(self):\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        p_direction = real.setResultsName(\"p_direction\") # p.u.\n        q_direction = real.setResultsName(\"q_direction\") # p.u.\n        p_bid_max = real.setResultsName(\"p_bid_max\") # p.u.\n        p_bid_min = real.setResultsName(\"p_bid_min\") # p.u.\n        p_optimal_bid = Optional(real).setResultsName(\"p_optimal_bid\")\n        p_fixed = real.setResultsName(\"p_fixed\") # $/hr\n        p_proportional = real.setResultsName(\"p_proportional\") # $/MWh\n        p_quadratic = real.setResultsName(\"p_quadratic\") # $/MW^2h\n        q_fixed = real.setResultsName(\"q_fixed\") # $/hr\n        q_proportional = real.setResultsName(\"q_proportional\") # $/MVArh\n        q_quadratic = real.setResultsName(\"q_quadratic\") # $/MVAr^2h\n        commitment = boolean.setResultsName(\"commitment\")\n        cost_tie_break = real.setResultsName(\"cost_tie_break\") # $/MWh\n        cost_cong_up = real.setResultsName(\"cost_cong_up\") # $/h\n        cost_cong_down = real.setResultsName(\"cost_cong_down\") # $/h\n        status = Optional(boolean).setResultsName(\"status\")\n\n        demand_data = bus_no + s_rating + p_direction + q_direction + \\\n            p_bid_max + p_bid_min + p_optimal_bid + p_fixed + \\\n            p_proportional + p_quadratic + q_fixed + q_proportional + \\\n            q_quadratic + commitment + cost_tie_break + cost_cong_up + \\\n            cost_cong_down + status + scolon\n\n        demand_data.setParseAction(self.push_demand)\n\n        demand_array = Literal(\"Demand.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(demand_data + Optional(\"]\" + scolon))\n\n        return demand_array", "response": "Returns a construct for an array of power demand data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_supply_array_construct(self):\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        p_direction = real.setResultsName(\"p_direction\") # CPF\n        p_bid_max = real.setResultsName(\"p_bid_max\") # p.u.\n        p_bid_min = real.setResultsName(\"p_bid_min\") # p.u.\n        p_bid_actual = real.setResultsName(\"p_bid_actual\") # p.u.\n        p_fixed = real.setResultsName(\"p_fixed\") # $/hr\n        p_proportional = real.setResultsName(\"p_proportional\") # $/MWh\n        p_quadratic = real.setResultsName(\"p_quadratic\") # $/MW^2h\n        q_fixed = real.setResultsName(\"q_fixed\") # $/hr\n        q_proportional = real.setResultsName(\"q_proportional\") # $/MVArh\n        q_quadratic = real.setResultsName(\"q_quadratic\") # $/MVAr^2h\n        commitment = boolean.setResultsName(\"commitment\")\n        cost_tie_break = real.setResultsName(\"cost_tie_break\") # $/MWh\n        lp_factor = real.setResultsName(\"lp_factor\")# Loss participation factor\n        q_max = real.setResultsName(\"q_max\") # p.u.\n        q_min = real.setResultsName(\"q_min\") # p.u.\n        cost_cong_up = real.setResultsName(\"cost_cong_up\") # $/h\n        cost_cong_down = real.setResultsName(\"cost_cong_down\") # $/h\n        status = Optional(boolean).setResultsName(\"status\")\n\n        supply_data = bus_no + s_rating + p_direction + p_bid_max + \\\n            p_bid_min + p_bid_actual + p_fixed + p_proportional + \\\n            p_quadratic + q_fixed + q_proportional + q_quadratic + \\\n            commitment + cost_tie_break + lp_factor + q_max + q_min + \\\n            cost_cong_up + cost_cong_down + status + scolon\n\n        supply_data.setParseAction(self.push_supply)\n\n        supply_array = Literal(\"Supply.con\") + \"=\" + \"[\" + \"...\" + \\\n            ZeroOrMore(supply_data + Optional(\"]\" + scolon))\n\n        return supply_array", "response": "Returns a construct for an array of power supply data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_generator_ramping_construct(self):\n        supply_no = integer.setResultsName(\"supply_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        up_rate = real.setResultsName(\"up_rate\") # p.u./h\n        down_rate = real.setResultsName(\"down_rate\") # p.u./h\n        min_period_up = real.setResultsName(\"min_period_up\") # h\n        min_period_down = real.setResultsName(\"min_period_down\") # h\n        initial_period_up = integer.setResultsName(\"initial_period_up\")\n        initial_period_down = integer.setResultsName(\"initial_period_down\")\n        c_startup = real.setResultsName(\"c_startup\") # $\n        status = boolean.setResultsName(\"status\")\n\n        g_ramp_data = supply_no + s_rating + up_rate + down_rate + \\\n            min_period_up + min_period_down + initial_period_up + \\\n            initial_period_down + c_startup + status + scolon\n\n        g_ramp_array = Literal(\"Rmpg.con\") + \"=\" + \"[\" + \\\n            ZeroOrMore(g_ramp_data + Optional(\"]\" + scolon))\n\n        return g_ramp_array", "response": "Returns a construct for an array of generator ramping data."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a construct for an array of load ramping data.", "response": "def _get_load_ramping_construct(self):\n        \"\"\" Returns a construct for an array of load ramping data.\n        \"\"\"\n        bus_no = integer.setResultsName(\"bus_no\")\n        s_rating = real.setResultsName(\"s_rating\") # MVA\n        up_rate = real.setResultsName(\"up_rate\") # p.u./h\n        down_rate = real.setResultsName(\"down_rate\") # p.u./h\n        min_up_time = real.setResultsName(\"min_up_time\") # min\n        min_down_time = real.setResultsName(\"min_down_time\") # min\n        n_period_up = integer.setResultsName(\"n_period_up\")\n        n_period_down = integer.setResultsName(\"n_period_down\")\n        status = boolean.setResultsName(\"status\")\n\n        l_ramp_data = bus_no + s_rating + up_rate + down_rate + \\\n            min_up_time + min_down_time + n_period_up + \\\n            n_period_down + status + scolon\n\n        l_ramp_array = Literal(\"Rmpl.con\") + \"=\" + \"[\" + \\\n            ZeroOrMore(l_ramp_data + Optional(\"]\" + scolon))\n\n        return l_ramp_array"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a Bus object to the case.", "response": "def push_bus(self, tokens):\n        \"\"\" Adds a Bus object to the case.\n        \"\"\"\n        logger.debug(\"Pushing bus data: %s\" % tokens)\n\n        bus = Bus()\n        bus.name = tokens[\"bus_no\"]\n        bus.v_magnitude = tokens[\"v_magnitude\"]\n        bus.v_angle = tokens[\"v_angle\"]\n        bus.v_magnitude = tokens[\"v_magnitude\"]\n        bus.v_angle = tokens[\"v_angle\"]\n\n        self.case.buses.append(bus)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sort_buses(self, tokens):\n        self.case.buses.sort(key=lambda obj: obj.name)", "response": "Sorts the bus list according to name."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a Branch object to the case.", "response": "def push_line(self, tokens):\n        \"\"\" Adds a Branch object to the case.\n        \"\"\"\n        logger.debug(\"Pushing line data: %s\" % tokens)\n\n        from_bus = self.case.buses[tokens[\"fbus\"]-1]\n        to_bus = self.case.buses[tokens[\"tbus\"]-1]\n\n        e = Branch(from_bus=from_bus, to_bus=to_bus)\n        e.r = tokens[\"r\"]\n        e.x = tokens[\"x\"]\n        e.b = tokens[\"b\"]\n        e.rate_a = tokens[\"s_limit\"]\n        e.rate_b = tokens[\"p_limit\"]\n        e.rate_c = tokens[\"i_limit\"]\n        # Optional parameter\n        if tokens[\"tap\"] == 0: #Transmission line\n            e.ratio = 1.0\n        else: # Transformer\n            e.ratio = tokens[\"tap\"]\n        e.phase_shift = tokens[\"shift\"]\n        # Optional parameter\n#        if \"status\" in tokens.keys:\n#        e.online = tokens[\"status\"]\n\n        self.case.branches.append(e)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a Generator to the list of generators that can be used to generate slack data.", "response": "def push_slack(self, tokens):\n        \"\"\" Finds the slack bus, adds a Generator with the appropriate data\n        and sets the bus type to slack.\n        \"\"\"\n        logger.debug(\"Pushing slack data: %s\" % tokens)\n\n        bus = self.case.buses[tokens[\"bus_no\"] - 1]\n\n        g = Generator(bus)\n        g.q_max = tokens[\"q_max\"]\n        g.q_min = tokens[\"q_min\"]\n        # Optional parameter\n#        if tokens.has_key(\"status\"):\n#        g.online = tokens[\"status\"]\n\n        self.case.generators.append(g)\n\n        bus.type = \"ref\""}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push_pv(self, tokens):\n        logger.debug(\"Pushing PV data: %s\" % tokens)\n\n        bus = self.case.buses[tokens[\"bus_no\"]-1]\n\n        g = Generator(bus)\n        g.p = tokens[\"p\"]\n        g.q_max = tokens[\"q_max\"]\n        g.q_min = tokens[\"q_min\"]\n        # Optional parameter\n#        if tokens.has_key(\"status\"):\n#        g.online = tokens[\"status\"]\n\n        self.case.generators.append(g)", "response": "Creates and adds a new Generator object to the list of generators that will be used to store the PV data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push_pq(self, tokens):\n        logger.debug(\"Pushing PQ data: %s\" % tokens)\n\n        bus = self.case.buses[tokens[\"bus_no\"] - 1]\n        bus.p_demand = tokens[\"p\"]\n        bus.q_demand = tokens[\"q\"]", "response": "Creates and loads object populates it with data finds its Bus and\n        adds it to the case."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef push_supply(self, tokens):\n        logger.debug(\"Pushing supply data: %s\" % tokens)\n\n        bus = self.case.buses[tokens[\"bus_no\"] - 1]\n        n_generators = len([g for g in self.case.generators if g.bus == bus])\n\n        if n_generators == 0:\n            logger.error(\"No generator at bus [%s] for matching supply\" % bus)\n            return\n        elif n_generators > 1:\n            g = [g for g in self.case.generators if g.bus == bus][0]\n            logger.warning(\n                \"More than one generator at bus [%s] for demand. Using the \"\n                \"first one [%s].\" % (bus, g)\n            )\n        else:\n            g = [g for g in self.case.generators if g.bus == bus][0]\n\n        g.pcost_model = \"poly\"\n        g.poly_coeffs = (\n            tokens[\"p_fixed\"],\n            tokens[\"p_proportional\"],\n            tokens[\"p_quadratic\"]\n        )", "response": "Adds OPF and CPF data to a Generator."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_file(self, file):\n        case = Case()\n        file.seek(0)\n\n        line = file.readline().split()\n        if line[0] != \"function\":\n            logger.error(\"Invalid data file header.\")\n            return case\n        if line[1] != \"mpc\":\n            self._is_struct = False\n            base = \"\"\n        else:\n            base = \"mpc.\"\n        case.name = line[-1]\n\n        for line in file:\n            if line.startswith(\"%sbaseMVA\" % base):\n                case_data = line.rstrip(\";\\n\").split()\n                case.base_mva = float(case_data[-1])\n            elif line.startswith(\"%sbus\" % base):\n                self._parse_buses(case, file)\n            elif line.startswith(\"%sgencost\" % base):\n                self._parse_gencost(case, file)\n            elif line.startswith(\"%sgen\" % base):\n                self._parse_generators(case, file)\n            elif line.startswith(\"%sbranch\" % base):\n                self._parse_branches(case, file)\n\n        return case", "response": "Parses the given file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write(self, file_or_filename):\n        if isinstance(file_or_filename, basestring):\n            self._fcn_name, _ = splitext(basename(file_or_filename))\n        else:\n            self._fcn_name = self.case.name\n\n        self._fcn_name = self._fcn_name.replace(\",\", \"\").replace(\" \", \"_\")\n\n        super(MATPOWERWriter, self).write(file_or_filename)", "response": "Writes case data to file in MATPOWER format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_case_data(self, file):\n        file.write(\"function mpc = %s\\n\" % self._fcn_name)\n        file.write('\\n%%%% MATPOWER Case Format : Version %d\\n' % 2)\n        file.write(\"mpc.version = '%d';\\n\" % 2)\n\n        file.write(\"\\n%%%%-----  Power Flow Data  -----%%%%\\n\")\n        file.write(\"%%%% system MVA base\\n\")\n        file.write(\"%sbaseMVA = %g;\\n\" % (self._prefix, self.case.base_mva))", "response": "Writes the case data in MATPOWER format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the bus data in MATPOWER format.", "response": "def write_bus_data(self, file):\n        \"\"\" Writes bus data in MATPOWER format.\n        \"\"\"\n#        labels = [\"bus_id\", \"type\", \"Pd\", \"Qd\", \"Gs\", \"Bs\", \"area\", \"Vm\", \"Va\",\n#            \"baseKV\", \"Vmax\", \"Vmin\"]\n\n        bus_attrs = [\"_i\", \"type\", \"p_demand\", \"q_demand\", \"g_shunt\",\"b_shunt\",\n            \"area\", \"v_magnitude\", \"v_angle\", \"v_base\", \"zone\",\n            \"v_max\", \"v_min\", \"p_lmbda\", \"q_lmbda\", \"mu_vmin\", \"mu_vmax\"]\n\n        file.write(\"\\n%%%% bus data\\n\")\n        file.write(\"%%\\tbus_i\\ttype\\tPd\\tQd\\tGs\\tBs\\tarea\\tVm\\tVa\\tbaseKV\"\n                   \"\\tzone\\tVmax\\tVmin\\tlam_P\\tlam_Q\\tmu_Vmax\\tmu_Vmin\")\n        file.write(\"\\n%sbus = [\\n\" % self._prefix)\n\n\n        for bus in self.case.buses:\n            vals = [getattr(bus, a) for a in bus_attrs]\n            d = {PQ: 1, PV: 2, REFERENCE: 3, ISOLATED: 4}\n            vals[1] = d[vals[1]]\n\n            assert len(vals) == 17\n\n            file.write(\"\\t%d\\t%d\\t%g\\t%g\\t%g\\t%g\\t%d\\t%.8g\\t%.8g\\t%g\\t%d\\t%g\"\n                       \"\\t%g\\t%.4f\\t%.4f\\t%.4f\\t%.4f;\\n\" % tuple(vals[:]))\n        file.write(\"];\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_generator_data(self, file):\n        gen_attr = [\"p\", \"q\", \"q_max\", \"q_min\", \"v_magnitude\",\n            \"base_mva\", \"online\", \"p_max\", \"p_min\", \"mu_pmax\", \"mu_pmin\",\n            \"mu_qmax\", \"mu_qmin\"]\n\n        file.write(\"\\n%%%% generator data\\n\")\n        file.write(\"%%\\tbus\\tPg\\tQg\\tQmax\\tQmin\\tVg\\tmBase\\tstatus\\tPmax\\tPmin\")\n        file.write(\"\\tmu_Pmax\\tmu_Pmin\\tmu_Qmax\\tmu_Qmin\")\n        file.write(\"\\n%sgen = [\\n\" % self._prefix)\n\n        for generator in self.case.generators:\n            vals = [getattr(generator, a) for a in gen_attr]\n            vals.insert(0, generator.bus._i)\n            assert len(vals) == 14\n            file.write(\"\\t%d\\t%g\\t%g\\t%g\\t%g\\t%.8g\\t%g\\t%d\\t%g\\t%g\\t%g\\t%g\"\n                       \"\\t%g\\t%g;\\n\" % tuple(vals))\n        file.write(\"];\\n\")", "response": "Writes generator data in MATPOWER format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting branch data to file.", "response": "def write_branch_data(self, file):\n        \"\"\" Writes branch data to file.\n        \"\"\"\n        branch_attr = [\"r\", \"x\", \"b\", \"rate_a\", \"rate_b\", \"rate_c\",\n            \"ratio\", \"phase_shift\", \"online\", \"ang_min\", \"ang_max\", \"p_from\",\n            \"q_from\", \"p_to\", \"q_to\", \"mu_s_from\", \"mu_s_to\", \"mu_angmin\",\n            \"mu_angmax\"]\n\n        file.write(\"\\n%%%% branch data\\n\")\n        file.write(\"%%\\tfbus\\ttbus\\tr\\tx\\tb\\trateA\\trateB\\trateC\\tratio\"\n                   \"\\tangle\\tstatus\")\n        file.write(\"\\tangmin\\tangmax\")\n        file.write(\"\\tPf\\tQf\\tPt\\tQt\")\n        file.write(\"\\tmu_Sf\\tmu_St\")\n        file.write(\"\\tmu_angmin\\tmu_angmax\")\n        file.write(\"\\n%sbranch = [\\n\" % self._prefix)\n\n        for branch in self.case.branches:\n            vals = [getattr(branch, a) for a in branch_attr]\n\n            vals.insert(0, branch.to_bus._i)\n            vals.insert(0, branch.from_bus._i)\n\n            file.write(\"\\t%d\\t%d\\t%g\\t%g\\t%g\\t%g\\t%g\\t%g\\t%g\\t%g\\t%d\\t%g\\t%g\"\n                       \"\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f\\t%.4f;\\n\" %\n                       tuple(vals))\n        file.write(\"];\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite generator cost data to file.", "response": "def write_generator_cost_data(self, file):\n        \"\"\" Writes generator cost data to file.\n        \"\"\"\n        file.write(\"\\n%%%% generator cost data\\n\")\n        file.write(\"%%\\t1\\tstartup\\tshutdown\\tn\\tx1\\ty1\\t...\\txn\\tyn\\n\")\n        file.write(\"%%\\t2\\tstartup\\tshutdown\\tn\\tc(n-1)\\t...\\tc0\\n\")\n        file.write(\"%sgencost = [\\n\" % self._prefix)\n\n        for generator in self.case.generators:\n            n = len(generator.p_cost)\n            template = '\\t%d\\t%g\\t%g\\t%d'\n            for _ in range(n):\n                template = '%s\\t%%g' % template\n            template = '%s;\\n' % template\n\n            if generator.pcost_model == PW_LINEAR:\n                t = 2\n#                cp = [p for p, q in generator.p_cost]\n#                cq = [q for p, q in generator.p_cost]\n#                c = zip(cp, cq)\n                c = [v for pc in generator.p_cost for v in pc]\n            elif generator.pcost_model == POLYNOMIAL:\n                t = 1\n                c = list(generator.p_cost)\n            else:\n                raise\n\n            vals = [t, generator.c_startup, generator.c_shutdown, n] + c\n\n            file.write(template % tuple(vals))\n        file.write(\"];\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_area_data(self, file):\n        file.write(\"%% area data\" + \"\\n\")\n        file.write(\"%\\tno.\\tprice_ref_bus\" + \"\\n\")\n        file.write(\"areas = [\" + \"\\n\")\n        # TODO: Implement areas\n        file.write(\"\\t1\\t1;\" + \"\\n\")\n\n        file.write(\"];\" + \"\\n\")", "response": "Writes area data to file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the file for each token in token_list.", "response": "def process_file(self, language, key, token_list):\n        \"\"\"\n        Initiate processing for each token.\n\n        Override this if you want tt control the processing of the tokens yourself.\n        \"\"\"\n        self.language = language\n        for tok in token_list:\n            self.process_token(tok)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the data used by the renderer.", "response": "def updateData(self, data):\n        \"\"\" Updates the data used by the renderer.\n        \"\"\"\n#        pylab.ion()\n        fig = pylab.figure(1)\n\n        n_agent = len(data)\n\n        idx = 1\n        for i, adata in enumerate(data):\n            saxis = fig.add_subplot(3, n_agent, i + 1)\n            saxis.plot(adata[0])\n            idx += 1\n\n            aaxis = fig.add_subplot(3, n_agent, i + 1 + n_agent)\n            aaxis.plot(adata[1])\n            idx += 1\n\n            raxis = fig.add_subplot(3, n_agent, i + 1 + (n_agent * 2))\n            raxis.plot(adata[2])\n            idx += 1\n\n        pylab.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the state action and reward data for the current locale.", "response": "def updateData(self, state_data, action_data, reward_data):\n        \"\"\" Updates the data used by the renderer.\n        \"\"\"\n#        self.dataLock.acquire()\n\n        self.state_data[:, self.updates] = state_data\n        self.action_data[:, self.updates] = action_data\n        self.reward_data[0, self.updates] = reward_data\n        self.updates += 1\n        self._render()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws the state action and reward plots of the environment.", "response": "def draw_plot(self):\n        \"\"\" Initialises plots of the environment.\n        \"\"\"\n        pylab.ion()\n        fig = pylab.figure(1)\n\n        # State plot.\n#        state_axis = fig.add_subplot(3, 1, 1) # numrows, numcols, fignum\n#        state_axis.title = 'State'\n#        state_axis.xlabel = 'Time (hours)'\n#        state_axis.grid = True\n#        for i in range(self.state_data.shape[0]):\n#            lines = state_axis.plot(self.state_data[i, 0], \"g+-\")\n#            self.state_lines.append(lines[0])\n\n        # Action plot.\n#        action_axis = fig.add_subplot(3, 1, 2)\n#        action_axis.title = 'Action'\n#        action_axis.xlabel = 'Time (hours)'\n#        action_axis.ylabel = 'Price ($/MWh)'\n#        action_axis.grid = True\n#        for i in range(self.action_data.shape[0]):\n#            lines = action_axis.plot(self.action_data[i, 0], \"ro-\")\n#            self.action_lines.append(lines[0])\n\n        # Reward plot.\n        reward_axis = fig.add_subplot(3, 1, 3)\n#        reward_axis.title = 'Reward'\n#        reward_axis.xlabel = 'Time (hours)'\n#        reward_axis.ylabel = 'Earnings ($)'\n#        reward_axis.grid(True)\n        reward_lines = reward_axis.plot(self.reward_data[0, 0], [0], \"mx-\")\n        self.reward_line = reward_lines[0]\n\n        pylab.draw()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate(ast_tree: ast.Tree, model_name: str):\n    component_ref = ast.ComponentRef.from_string(model_name)\n    ast_tree_new = copy.deepcopy(ast_tree)\n    ast_walker = TreeWalker()\n    flat_tree = flatten(ast_tree_new, component_ref)\n    gen = XmlGenerator()\n    ast_walker.walk(gen, flat_tree)\n    return etree.tostring(gen.xml[flat_tree], pretty_print=True).decode('utf-8')", "response": "Generate sympy source code for a given model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting simulation data. :data: A dictionary of arrays. :fields: A list of variables you want to plot (e.g. ['x', y', 'c'])", "response": "def plot(data: Dict[str, np.array], fields: List[str] = None, *args, **kwargs):\n    \"\"\"\n    Plot simulation data.\n    :data: A dictionary of arrays.\n    :fields: A list of variables you want to plot (e.g. ['x', y', 'c'])\n    \"\"\"\n    if plt is None:\n        return\n\n    if fields is None:\n        fields = ['x', 'y', 'm', 'c']\n    labels = []\n    lines = []\n    for field in fields:\n        if min(data[field].shape) > 0:\n            f_lines = plt.plot(data['t'], data[field], *args, **kwargs)\n            lines.extend(f_lines)\n            labels.extend(data['labels'][field])\n    plt.legend(lines, labels, ncol=2, loc='best')\n    plt.xlabel('t, sec')\n    plt.grid()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a version 30 PSS / E raw file.", "response": "def read(self, file_or_filename):\r\n        \"\"\" Returns a case from a version 30 PSS/E raw file.\r\n\r\n        @param file_or_filename: File name or file like object with PSS/E data\r\n        @return: Case object\r\n        \"\"\"\r\n        t0 = time.time()\r\n        self.init()\r\n        if isinstance(file_or_filename, basestring):\r\n            fname = os.path.basename(file_or_filename)\r\n            logger.info(\"Loading PSS/E Raw file [%s].\" % fname)\r\n\r\n            file = None\r\n            try:\r\n                file = open(file_or_filename, \"rb\")\r\n            except:\r\n                logger.error(\"Error opening %s.\" % fname)\r\n                return None\r\n            finally:\r\n                if file is not None:\r\n                    case = self._parse_file(file)\r\n                    file.close()\r\n        else:\r\n            file = file_or_filename\r\n            case = self._parse_file(file)\r\n\r\n        logger.info(\"PSS/E Raw file parsed in %.2fs.\" % (time.time() - t0))\r\n\r\n        return case"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the given file and returns a new instance of the class instance.", "response": "def _parse_file(self, file):\r\n        \"\"\" Parses the given file.\r\n        \"\"\"\r\n        case = Case()\r\n        file.seek(0)\r\n        case.base_mva = float(file.next().split(\",\")[1].split(\"/\")[0])\r\n        case.name = \"%s %s\" % (file.next().strip(), file.next().strip())\r\n\r\n        bustype_map = {1: \"PQ\", 2: \"PV\", 3: \"ref\", 4: \"isolated\"}\r\n\r\n        # I, 'NAME', BASKV, IDE, GL, BL, AREA, ZONE, VM, VA, OWNER\r\n        bus_data = file.next().split(\",\")\r\n        while bus_data[0].strip()[0] != \"0\":\r\n            bus = Bus()\r\n            i = int(bus_data[0].strip())\r\n            self.bus_map[i] = bus\r\n            bus._i = i\r\n            bus.name =  bus_data[1].strip(\"'\").strip()\r\n            bus.v_base = float(bus_data[2])\r\n            bus.type = bustype_map[int(bus_data[3])]\r\n            bus.g_shunt = float(bus_data[4])\r\n            bus.b_shunt = float(bus_data[5])\r\n            bus.v_magnitude = float(bus_data[8])\r\n            bus.v_angle = float(bus_data[9])\r\n            # bus.area = 1;  # hcui7 added\r\n            case.buses.append(bus)\r\n            bus_data = file.next().split(\",\")\r\n\r\n        # I, ID, STATUS, AREA, ZONE, PL, QL, IP, IQ, YP, YQ, OWNER\r\n        load_data = file.next().split(\",\")\r\n        while load_data[0].strip()[0] != \"0\":\r\n            bus = self.bus_map[int(load_data[0].strip())]\r\n            bus.p_demand += float(load_data[5])\r\n            bus.q_demand += float(load_data[6])\r\n            load_data = file.next().split(\",\")\r\n\r\n        #I,ID,PG,QG,QT,QB,VS,IREG,MBASE,ZR,ZX,RT,XT,GTAP,STAT,RMPCT,PT,PB,O1,F1\r\n        gen_data = file.next().split(\",\")\r\n        while gen_data[0].strip()[0] != \"0\":\r\n            bus = self.bus_map[int(gen_data[0].strip())]\r\n            g = Generator(bus)\r\n            g.p = float(gen_data[2])\r\n            g.q = float(gen_data[3])\r\n            g.q_max = float(gen_data[4])\r\n            g.q_min = float(gen_data[5])\r\n            g.v_magnitude = float(gen_data[6])\r\n            g.base_mva = float(gen_data[8])\r\n            g.online = bool(int(gen_data[14]))\r\n            g.p_max = float(gen_data[16])\r\n            g.p_min = float(gen_data[17])\r\n            case.generators.append(g)\r\n            gen_data = file.next().split(\",\")\r\n\r\n        # I,J,CKT,R,X,B,RATEA,RATEB,RATEC,GI,BI,GJ,BJ,ST,LEN,O1,F1,...,O4,F4\r\n        branch_data = file.next().split(\",\")\r\n        while branch_data[0].strip()[0] != \"0\":\r\n            from_bus = self.bus_map[abs(int(branch_data[0]))]\r\n            to_bus = self.bus_map[abs(int(branch_data[1]))]\r\n            l = Branch(from_bus, to_bus)\r\n            l.r = float(branch_data[3])\r\n            l.x = float(branch_data[4])\r\n            l.b = float(branch_data[5])\r\n            l.rate_a = float(branch_data[6])\r\n            l.rate_b = float(branch_data[7])\r\n            l.rate_c = float(branch_data[8])\r\n#            l.online = bool(int(branch_data[13]))\r\n            case.branches.append(l)\r\n            branch_data = file.next().split(\",\")\r\n\r\n        # I,J,K,CKT,CW,CZ,CM,MAG1,MAG2,NMETR,'NAME',STAT,O1,F1,...,O4,F4\r\n        # R1-2,X1-2,SBASE1-2\r\n        # WINDV1,NOMV1,ANG1,RATA1,RATB1,RATC1,COD1,CONT1,RMA1,RMI1,VMA1,VMI1,NTP1,TAB1,CR1,CX1\r\n        # WINDV2,NOMV2\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            trx_data2 = file.next().split(\",\")\r\n            trx_data3 = file.next().split(\",\")\r\n            trx_data4 = file.next().split(\",\") # second winding\r\n            if len(trx_data2) < 5:\r\n                from_bus = self.bus_map[abs(int(trx_data[0]))]\r\n                to_bus = self.bus_map[abs(int(trx_data[1]))]\r\n                l = Branch(from_bus, to_bus)\r\n                l.name = trx_data[10].strip(\"'\").strip()\r\n                l.online = bool(int(trx_data[11]))\r\n                l.b = float(trx_data[8])\r\n                l.r = float(trx_data2[0])\r\n                l.x = float(trx_data2[1])\r\n                l.ratio = float(trx_data3[0])\r\n                l.phase_shift = float(trx_data3[2])\r\n                rate_a = float(trx_data3[3])\r\n                if rate_a != 0.0:\r\n                    l.rate_a = rate_a\r\n                rate_b = float(trx_data3[4])\r\n                if rate_b != 0.0:\r\n                    l.rate_b = rate_b\r\n                rate_c = float(trx_data3[5])\r\n                if rate_c != 0.0:\r\n                    l.rate_c = rate_c\r\n                case.branches.append(l)\r\n                trx_data = file.next().split(\",\")\r\n            else:\r\n                # I,J,K,CKT,CW,CZ,CM,MAG1,MAG2,NMETR,'NAME',STAT,O1,F1,...,O4,F4\r\n                # R1-2,X1-2,SBASE1-2,R2-3,X2-3,SBASE2-3,R3-1,X3-1,SBASE3-1,VMSTAR,ANSTAR\r\n                # WINDV1,NOMV1,ANG1,RATA1,RATB1,RATC1,COD1,CONT1,RMA1,RMI1,VMA1,VMI1,NTP1,TAB1,CR1,CX1\r\n                # WINDV2,NOMV2,ANG2,RATA2,RATB2,RATC2,COD2,CONT2,RMA2,RMI2,VMA2,VMI2,NTP2,TAB2,CR2,CX2\r\n                # WINDV3,NOMV3,ANG3,RATA3,RATB3,RATC3,COD3,CONT3,RMA3,RMI3,VMA3,VMI3,NTP3,TAB3,CR3,CX3\r\n\r\n                trx_data5 = file.next().split(\",\") # third winding\r\n                # Three-winding transformers are modelled as a group of three\r\n                # two-winding transformers with a fictitious neutral bus.\r\n                tmp_bus = Bus()\r\n                tmp_bus.name = \"n\" + tmp_bus.name\r\n                tmp_bus._i = len(case.buses) + 1\r\n\r\n                bus1 = self.bus_map[abs(int(trx_data[0]))]\r\n                bus2 = self.bus_map[abs(int(trx_data[1]))]\r\n                bus3 = self.bus_map[abs(int(trx_data[2]))]\r\n                l1 = Branch(tmp_bus, bus1)\r\n                l2 = Branch(tmp_bus, bus2)\r\n                l3 = Branch(tmp_bus, bus3)\r\n\r\n                b = float(trx_data[8]) # MAG2\r\n                l1.b = b# / 3.0\r\n#                l2.b = b / 3.0\r\n#                l3.b = b / 3.0\r\n\r\n                on = bool(int(trx_data[11]))\r\n                l1.online = on\r\n                l2.online = on\r\n                l3.online = on\r\n\r\n                r12 = float(trx_data2[0])\r\n                x12 = float(trx_data2[1])\r\n                r23 = float(trx_data2[3])\r\n                x23 = float(trx_data2[4])\r\n                r31 = float(trx_data2[6])\r\n                x31 = float(trx_data2[7])\r\n\r\n                l1.r = 0.5 * (r12 + r31 - r23)\r\n                l1.x = 0.5 * (x12 + x31 - x23)\r\n                l2.r = 0.5 * (r12 + r23 - r31)\r\n                l2.x = 0.5 * (x12 + x23 - x31)\r\n                l3.r = 0.5 * (r23 + r31 - r12)\r\n                l3.x = 0.5 * (x23 + x31 - x12)\r\n\r\n                for l in [l1, l2, l3]:\r\n                    if abs(l.x) < 1e-5:\r\n                        logger.warning(\"Zero branch reactance [%s].\" % l.name)\r\n                        l.x = self.xtol\r\n                    if abs(complex(l.r, l.x)) < 0.00001:\r\n                        logger.warning(\"Zero branch impedance [%s].\" % l.name)\r\n\r\n                l1.ratio = float(trx_data3[0])\r\n                l1.phase_shift = float(trx_data3[2])\r\n                l2.ratio = float(trx_data4[0])\r\n                l2.phase_shift = float(trx_data4[2])\r\n                l3.ratio = float(trx_data5[0])\r\n                l3.phase_shift = float(trx_data5[2])\r\n\r\n                rate_a1 = float(trx_data3[3])\r\n                rate_b1 = float(trx_data3[4])\r\n                rate_c1 = float(trx_data3[5])\r\n                if rate_a1 > 0.0:\r\n                    l1.rate_a = rate_a1\r\n                if rate_b1 > 0.0:\r\n                    l1.rate_b = rate_b1\r\n                if rate_c1 > 0.0:\r\n                    l1.rate_c = rate_c1\r\n\r\n                rate_a2 = float(trx_data4[3])\r\n                rate_b2 = float(trx_data4[4])\r\n                rate_c2 = float(trx_data4[5])\r\n                if rate_a2 > 0.0:\r\n                    l2.rate_a = rate_a2\r\n                if rate_b2 > 0.0:\r\n                    l2.rate_b = rate_b2\r\n                if rate_c2 > 0.0:\r\n                    l2.rate_c = rate_c2\r\n\r\n                rate_a3 = float(trx_data5[3])\r\n                rate_b3 = float(trx_data5[4])\r\n                rate_c3 = float(trx_data5[5])\r\n                if rate_a3 > 0.0:\r\n                    l3.rate_a = rate_a3\r\n                if rate_b2 > 0.0:\r\n                    l3.rate_b = rate_b3\r\n                if rate_c2 > 0.0:\r\n                    l3.rate_c = rate_c3\r\n\r\n                case.buses.append(tmp_bus)\r\n                case.branches.append(l1)\r\n                case.branches.append(l2)\r\n                case.branches.append(l3)\r\n\r\n                trx_data = file.next().split(\",\")\r\n\r\n        # Area interchange data.\r\n        # I, ISW, PDES, PTOL, 'ARNAME'\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring area interchange data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Two-terminal DC line data.\r\n        # I,MDC,RDC,SETVL,VSCHD,VCMOD,RCOMP,DELTI,METER,DCVMIN,CCCITMX,CCCACC\r\n        # IPR,NBR,ALFMX,ALFMN,RCR,XCR,EBASR,TRR,TAPR,TMXR,TMNR,STPR,ICR,IFR,ITR,IDR,XCAPR\r\n        # IPI,NBI,GAMMX,GAMMN,RCI,XCI,EBASI,TRI,TAPI,TMXI,TMNI,STPI,ICI,IFI,ITI,IDI,XCAPI\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring two-terminal DC line data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # VSC DC line data.\r\n        # 'NAME', MDC, RDC, O1, F1, ... O4, F4\r\n        # IBUS,TYPE,MODE,DOCET,ACSET,ALOSS,BLOSS,MINOSS,SMAX,IMAX,PWF,MAXQ,MINQ,\r\n        # REMOT,RMPCT\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring VSC DC line data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Switched shunt data.\r\n        # I,MODSW,VSWHI,VSWLO,SWREM,RMPCT,'RMIDNT',BINIT,N1,B1,N2,B2,...N8,B8\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            bus = self.bus_map[abs(int(trx_data[0]))]\r\n            bus.b_shunt += float(trx_data[7])\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Transformer impedance correction table.\r\n        # I, T1, F1, T2, F2, T3, F3, ... T11, F11\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring transformer X correction table data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Multi-terminal dc line data.\r\n        # I, NCONV, NDCBS, NDCLN, MDC, VCONV, VCMOD, VCONVN\r\n        # IB,N,ANGMX,ANGMN,RC,XC,EBAS,TR,TAP,TPMX,TPMN,TSTP,SETVL,DCPF,MARG,CNVCOD\r\n        # IDC, IB, IA, ZONE, 'NAME', IDC2, RGRND, OWNER\r\n        # IDC, JDC, DCCKT, RDC, LDC\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring multi-terminal dc line data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Multisection line data.\r\n        # I,J,ID,DUM1,DUM2,...DUM9\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring multisection line data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Zone data.\r\n        # I,'ZONAME'\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring zone data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Interarea transfer data.\r\n        # ARFROM, ARTO, TRID, PTRAN\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring interarea transfer data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # Owner data.\r\n        # I,'OWNAME'\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring owner data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        # FACTS device data.\r\n        # N,I,J,MODE,PDES,QDES,VSET,SHMX,TRMX,VTMN,VTMX,VSMX,IMX,LINX,RMPCT,OWNER,SET1,SET2,VSREF\r\n        trx_data = file.next().split(\",\")\r\n        while trx_data[0].strip()[0] != \"0\":\r\n            logger.warning(\"Ignoring FACTS device data.\")\r\n            trx_data = file.next().split(\",\")\r\n\r\n        return case"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites the data of the current case to file.", "response": "def write_case_data(self, file):\r\n        \"\"\" Writes case data to file.\r\n        \"\"\"\r\n        change_code = 0\r\n        s_base = self.case.base_mva\r\n        timestr = time.strftime(\"%Y%m%d%H%M\", time.gmtime())\r\n        file.write(\"%d, %8.2f, 30 / PSS(tm)E-30 RAW created by Pylon (%s).\\n\" %\r\n                   (change_code, s_base, timestr))\r\n        file.write(\"Modified by Hantao Cui, CURENT, UTK\\n \")\r\n        file.write(\"%s, %d BUSES, %d BRANCHES\\n\" %\r\n                   (self.case.name, len(self.case.buses), len(self.case.branches)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the bus data in MATPOWER format.", "response": "def write_bus_data(self, file):\r\n        \"\"\" Writes bus data in MATPOWER format.\r\n        \"\"\"\r\n        # I, 'NAME', BASKV, IDE, GL, BL, AREA, ZONE, VM, VA, OWNER\r\n        bus_attrs = [\"_i\", \"name\", \"v_base\", \"type\", \"g_shunt\", \"b_shunt\",\r\n                     \"area\", \"zone\",\r\n                     \"v_magnitude\", \"v_angle\"]\r\n\r\n        for bus in self.case.buses:\r\n            vals = [getattr(bus, a) for a in bus_attrs]\r\n            if float(vals[6]) == 0.0:\r\n                vals[6] = 1  # default AREA: 1\r\n            if float(vals[7])==0.0:\r\n                vals[7] = 1  # default ZONE: 1\r\n            d = {PQ: 1, PV: 2, REFERENCE: 3, ISOLATED: 4}\r\n            vals[3] = d[vals[3]]\r\n            vals.append(1)\r\n\r\n#            print len(vals), vals\r\n\r\n            file.write(\"%6d,'%-10s',%10.4f,%d,%10.3f,%10.3f,%4d,%4d,%10.3f,\"\r\n                       \"%10.3f%4d\\n\" % tuple(vals))\r\n        file.write(\" 0 / END OF BUS DATA, BEGIN LOAD DATA\\n\")\r\n\r\n\r\n        # I, ID, STATUS, AREA, ZONE, PL, QL, IP, IQ, YP, YQ, OWNER\r\n        load_attrs = [\"_i\", \"area\", \"zone\", \"p_demand\", \"q_demand\"]\r\n\r\n        for bus in self.case.buses:\r\n            if bus.p_demand > 0.0 or bus.q_demand > 0.0:\r\n                vals = [getattr(bus, a) for a in load_attrs]\r\n                if float(vals[1])==0.0:\r\n                    vals[1] = 1  # default AREA: 1\r\n                if float(vals[2])==0.0:\r\n                    vals[2] = 1  # default ZONE: 1\r\n                vals.insert(1, 1) # STATUS\r\n                vals.insert(1, \"1 \") # ID\r\n                vals.extend([0., 0., 0., 0.])\r\n                vals.append(1) # OWNER\r\n\r\n                file.write(\"%6d,'%s',%2d,%2d,%2d,%10.3f,%10.3f,%10.3f,%10.3f,\"\r\n                           \"%10.3f,%10.3f,%4d\\n\" % tuple(vals))\r\n\r\n        file.write(\" 0 / END OF LOAD DATA, BEGIN GENERATOR DATA\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites generator data in MATPOWER format.", "response": "def write_generator_data(self, file):\r\n        \"\"\" Writes generator data in MATPOWER format.\r\n        \"\"\"\r\n        for generator in self.case.generators:\r\n            vals = []\r\n            vals.append(generator.bus._i) # I\r\n            vals.append(\"1 \") # ID\r\n            vals.append(generator.p)\r\n            vals.append(generator.q)\r\n            vals.append(generator.q_max)\r\n            vals.append(generator.q_min)\r\n            vals.append(generator.v_magnitude)\r\n            vals.append(0) # IREG\r\n            vals.append(generator.base_mva)\r\n            vals.extend([0., 1., 0., 0., 0.])\r\n            vals.append(generator.online)\r\n            vals.append(100.0) # RMPCT\r\n            vals.append(generator.p_max)\r\n            vals.append(generator.p_min)\r\n            vals.extend([1, 1.0]) # O1,F1\r\n\r\n            file.write(\"%6d,'%s',%10.3f,%10.3f,%10.3f,%10.3f,%10.5f,%6d,%10.3f,\"\r\n                       \"%10.5f,%10.5f,%10.5f,%10.5f,%7.5f,%d,%7.1f,%10.3f,\"\r\n                       \"%10.3f,%4d,%6.4f\\n\" % tuple(vals))\r\n        file.write(\" 0 / END OF GENERATOR DATA, BEGIN NON-TRANSFORMER BRANCH DATA\\n\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_branch_data(self, file):\r\n        # I,J,CKT,R,X,B,RATEA,RATEB,RATEC,GI,BI,GJ,BJ,ST,LEN,O1,F1,...,O4,F4\r\n        branch_attr = [\"r\", \"x\", \"b\", \"rate_a\", \"rate_b\", \"rate_c\"]\r\n\r\n        for branch in self.case.branches:\r\n            if feq(branch.ratio, 0.0):\r\n                vals = [getattr(branch, a) for a in branch_attr]\r\n                if float(vals[1])<0.001:\r\n                    vals[1] = 0.001  # small reactance, todo: increase decimal\r\n                vals.insert(0, \"1 \")\r\n                vals.insert(0, branch.to_bus._i)\r\n                vals.insert(0, branch.from_bus._i)\r\n                vals.extend([0., 0., 0., 0.])\r\n                vals.append(branch.online)\r\n                vals.extend([0.0, 1, 1.0,])\r\n\r\n                file.write(\"%6d,%6d,'%s',%10.3f,%10.3f,%10.3f,%10.3f,%10.3f,\"\r\n                    \"%10.3f,%10.3f,%10.3f,%10.3f,%10.3f,%d,%10.3f,%4d,%6.4f\\n\" %\r\n                    tuple(vals))\r\n        file.write(\" 0 / END OF NON-TRANSFORMER BRANCH DATA, BEGIN TRANSFORMER DATA\\n\")\r\n\r\n        # I,J,K,CKT,CW,CZ,CM,MAG1,MAG2,NMETR,'NAME',STAT,O1,F1,...,O4,F4\r\n        # R1-2,X1-2,SBASE1-2\r\n        # WINDV1,NOMV1,ANG1,RATA1,RATB1,RATC1,COD1,CONT1,RMA1,RMI1,VMA1,VMI1,NTP1,TAB1,CR1,CX1\r\n        # WINDV2,NOMV2\r\n        for branch in self.case.branches:\r\n            if not feq(branch.ratio, 0.0):\r\n                vals = []\r\n                vals.append(branch.from_bus._i)\r\n                vals.append(branch.to_bus._i)\r\n                # K,CKT,CW,CZ,CM,MAG1,MAG2,NMETR\r\n                vals.extend([0, \"1 \", 1, 1, 1, 0.0, 0.0, 2])\r\n                vals.append(branch.name)\r\n                vals.append(branch.online)\r\n                vals.extend([1, 1.0]) # O1,F1\r\n\r\n                file.write(\"%6d,%6d,%6d,'%2s',%d,%d,%d,%10.3f,%10.3f,%d,\"\r\n                           \"'%-12s',%d,%4d,%6.4f\\n\" % tuple(vals))\r\n                file.write(\"%8.3f,%8.3f,%10.2f\\n\" % (branch.r, branch.x,\r\n                                                   self.case.base_mva))\r\n\r\n                line3 = []\r\n                line3.append(branch.ratio) # Winding-1 RATIO\r\n                line3.append(0.0)\r\n                line3.append(branch.phase_shift)\r\n                line3.append(branch.rate_a)\r\n                line3.append(branch.rate_b)\r\n                line3.append(branch.rate_c)\r\n                # COD1,CONT1,RMA1,RMI1,VMA1,VMI1,NTP1,TAB1,CR1,CX1\r\n                line3.extend([0, 0, 1.1, 0.9, 1.1, 0.9, 33, 0, 0.0, 0.0])\r\n\r\n                file.write(\"%7.5f,%8.3f,%8.3f,%8.2f,%8.2f,%8.2f,%d,%7d,%8.5f,\"\r\n                    \"%8.5f,%8.5f,%8.5f,%4d,%2d,%8.5f,%8.5f\\n\" % tuple(line3))\r\n\r\n                file.write(\"%7.5f,%8.3f\\n\" % (1.0, 0.0))  # Winding-2 RATIO: 1\r\n\r\n        file.write(\"\"\" 0 / END OF TRANSFORMER DATA, BEGIN AREA INTERCHANGE DATA\r\n 0 / END OF AREA INTERCHANGE DATA, BEGIN TWO-TERMINAL DC DATA\r\n 0 / END OF TWO-TERMINAL DC DATA, BEGIN VSC DC LINE DATA\r\n 0 / END OF VSC DC LINE DATA, BEGIN SWITCHED SHUNT DATA\r\n 0 / END OF SWITCHED SHUNT DATA, BEGIN TRANS. IMP. CORR. TABLE DATA\r\n 0 / END OF TRANS. IMP. CORR. TABLE DATA, BEGIN MULTI-TERMINAL DC LINE DATA\r\n 0 / END OF MULTI-TERMINAL DC LINE DATA, BEGIN MULTI-SECTION LINE DATA\r\n 0 / END OF MULTI-SECTION LINE DATA, BEGIN ZONE DATA\r\n 0 / END OF ZONE DATA, BEGIN INTERAREA TRANSFER DATA\r\n 0 / END OF INTERAREA TRANSFER DATA, BEGIN OWNER DATA\r\n 0 / END OF OWNER DATA, BEGIN FACTS DEVICE DATA\r\n 0 / END OF FACTS DEVICE DATA, END OF CASE DATA\r\n\"\"\")", "response": "Writes branch data to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes weighted choices. Accepts a list of tuples with the item and probability as a pair like: >>> x = [('one', 0.25), ('two', 0.25), ('three', 0.5)] >>> y=windex(x)", "response": "def weighted_choice(lst):\n    \"\"\" Makes weighted choices.  Accepts a list of tuples with the item and\n    probability as a pair like:\n    >>> x = [('one', 0.25), ('two', 0.25), ('three', 0.5)]\n    >>> y=windex(x) \"\"\"\n    n = random.uniform(0, 1)\n    for item, weight in lst:\n        if n < weight:\n            break\n        n = n - weight\n    return item"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntakes n elements from the sequence and returns a list of lists of all the items that are in the order of the items.", "response": "def xselections(items, n):\n    \"\"\" Takes n elements (not necessarily distinct) from the sequence, order\n    matters.\n\n    @see: http://aspn.activestate.com/ASPN/Cookbook/Python/Recipe/190465\n    \"\"\"\n    if n==0:\n        yield []\n    else:\n        for i in xrange(len(items)):\n            for ss in xselections(items, n-1):\n                yield [items[i]]+ss"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plotGenCost(generators):\n    figure()\n    plots = []\n    for generator in generators:\n        if generator.pcost_model == PW_LINEAR:\n            x = [x for x, _ in generator.p_cost]\n            y = [y for _, y in generator.p_cost]\n        elif generator.pcost_model == POLYNOMIAL:\n            x = scipy.arange(generator.p_min, generator.p_max, 5)\n            y = scipy.polyval(scipy.array(generator.p_cost), x)\n        else:\n            raise\n        plots.append(plot(x, y))\n        xlabel(\"P (MW)\")\n        ylabel(\"Cost ($)\")\n    legend(plots, [g.name for g in generators])\n    show()", "response": "Plots the costs of the given generators."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sparklineData(data, filename):\n    fd = file(filename, \"w+b\")\n    for name in data.keys():\n        action, reward = data[name]\n\n        altName = name.lower().replace(\"_\", \"\")\n\n        fd.write(\"\\def\")\n        fd.write(\"\\REWARDDATA%s{\" % altName)\n        for i, r in enumerate(reward):\n            fd.write(\"(%.2f,%.3f)\" % (i / 10.0, r / 10.0)) # dimension too large\n        fd.write(\"}\\n\")\n\n        maxreward, maxindex = max(izip(reward, count()))\n        minreward, minindex = min(izip(reward, count()))\n        meanreward = scipy.mean(reward)\n        fd.write(\"\\def\\REWARDMAX%s{%.1f}\\n\" % (altName, maxreward))\n        fd.write(\"\\def\\REWARDMAXIDX%s{%d}\\n\" % (altName, maxindex))\n        fd.write(\"\\def\\REWARDMIN%s{%.1f}\\n\" % (altName, minreward))\n        fd.write(\"\\def\\REWARDMINIDX%s{%d}\\n\" % (altName, minindex))\n        fd.write(\"\\def\\REWARDMEAN%s{%.1f}\\n\" % (altName, meanreward))\n\n        fd.write(\"\\def\")\n        fd.write(\"\\ACTIONDATA%s{\" % altName)\n        for i, a in enumerate(action):\n            fd.write(\"(%.2f,%.3f)\" % (i / 10.0, a / 10.0))\n        fd.write(\"}\\n\")\n\n        maxaction, maxindex = max(izip(reward, count()))\n        minaction, minindex = min(izip(reward, count()))\n        meanaction = scipy.mean(reward)\n        fd.write(\"\\def\\ACTIONMAX%s{%.1f}\\n\" % (altName, maxaction))\n        fd.write(\"\\def\\ACTIONMAXIDX%s{%d}\\n\" % (altName, maxindex))\n        fd.write(\"\\def\\ACTIONMIN%s{%.1f}\\n\" % (altName, minaction))\n        fd.write(\"\\def\\ACTIONMINIDX%s{%d}\\n\" % (altName, minindex))\n        fd.write(\"\\def\\ACTIONMEAN%s{%.1f}\\n\" % (altName, meanaction))\n    fd.close()", "response": "Writes the data for plotting sparklines with PGF."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, file):\n        # Write environment state data.\n        file.write(\"State\\n\")\n        file.write( (\"-\" * 5) + \"\\n\")\n        self.writeDataTable(file, type=\"state\")\n\n        # Write action data.\n        file.write(\"Action\\n\")\n        file.write( (\"-\" * 6) + \"\\n\")\n        self.writeDataTable(file, type=\"action\")\n\n        # Write reward data.\n        file.write(\"Reward\\n\")\n        file.write( (\"-\" * 6) + \"\\n\")\n        self.writeDataTable(file, type=\"reward\")", "response": "Writes market experiment data to file in ReStructuredText format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef writeDataTable(self, file, type):\n        agents = self.experiment.agents\n        numAgents = len(self.experiment.agents)\n\n        colWidth = 8\n        idxColWidth = 3\n\n        sep = (\"=\" * idxColWidth) + \" \" + \\\n            (\"=\" * colWidth + \" \") * numAgents + \"\\n\"\n\n        file.write(sep)\n\n        # Table column headers.\n        file.write(\"..\".rjust(idxColWidth) + \" \")\n        for agent in agents:\n            # The end of the name is typically the unique part.\n            file.write(agent.name[-colWidth:].center(colWidth) + \" \")\n        file.write(\"\\n\")\n\n        file.write(sep)\n\n        # Table values.\n        if agents:\n            rows, _ = agents[0].history.getField( type ).shape\n        else:\n            rows, _ = (0, 0)\n\n        for sequence in range( min(rows, 999) ):\n            file.write( str(sequence + 1).rjust(idxColWidth) + \" \" )\n\n            for agent in agents:\n                field = agent.history.getField( type )\n                # FIXME: Handle multiple state values.\n                file.write(\"%8.3f \" % field[sequence, 0])\n\n            file.write(\"\\n\")\n\n        file.write(sep)", "response": "Writes agent data to a ReST table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _getActorLimits(self):\n        actorLimits = []\n\n        for _ in range(self.env.numOffbids):\n            for _ in self.env.generators:\n                actorLimits.append((0.0, self.env.maxMarkup))\n\n        for _ in range(self.env.numOffbids):\n            for _ in self.env.generators:\n                if self.env.maxWithhold is not None:\n                    actorLimits.append((0.0, self.env.maxWithhold))\n\n        logger.debug(\"Actor limits: %s\" % actorLimits)\n\n        return actorLimits", "response": "Returns a list of 2 - tuples one tuple per parameter giving min and max for that parameter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of 2 - tuples one tuple per parameter giving min and max for that parameter.", "response": "def _getSensorLimits(self):\n        \"\"\" Returns a list of 2-tuples, e.g. [(-3.14, 3.14), (-0.001, 0.001)],\n            one tuple per parameter, giving min and max for that parameter.\n        \"\"\"\n        limits = []\n        limits.extend(self._getTotalDemandLimits())\n#        limits.extend(self._getDemandLimits())\n#        limits.extend(self._getPriceLimits())\n\n#        limits.extend(self._getVoltageSensorLimits())\n\n#        limits.extend(self._getVoltageMagnitudeLimits())\n#        limits.extend(self._getVoltageAngleLimits())\n#        limits.extend(self._getVoltageLambdaLimits())\n#        limits.extend(self._getFlowLimits())\n\n        logger.debug(\"Sensor limits: %s\" % limits)\n        return limits"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsplitting equations into differential algebraic and algebraic only", "response": "def split_dae_alg(eqs: SYM, dx: SYM) -> Dict[str, SYM]:\n    \"\"\"Split equations into differential algebraic and algebraic only\"\"\"\n    dae = []\n    alg = []\n    for eq in ca.vertsplit(eqs):\n        if ca.depends_on(eq, dx):\n            dae.append(eq)\n        else:\n            alg.append(eq)\n    return {\n        'dae': ca.vertcat(*dae),\n        'alg': ca.vertcat(*alg)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef permute(x: SYM, perm: List[int]) -> SYM:\n    x_s = []\n    for i in perm:\n        x_s.append(x[i])\n    return ca.vertcat(*x_s)", "response": "Perumute a vector x."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort equations by dependence", "response": "def blt(f: List[SYM], x: List[SYM]) -> Dict[str, Any]:\n    \"\"\"\n    Sort equations by dependence\n    \"\"\"\n    J = ca.jacobian(f, x)\n    nblock, rowperm, colperm, rowblock, colblock, coarserow, coarsecol = J.sparsity().btf()\n    return {\n        'J': J,\n        'nblock': nblock,\n        'rowperm': rowperm,\n        'colperm': colperm,\n        'rowblock': rowblock,\n        'colblock': colblock,\n        'coarserow': coarserow,\n        'coarsecol': coarsecol\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef tangent_approx(f: SYM, x: SYM, a: SYM = None, assert_linear: bool = False) -> Dict[str, SYM]:\n    # find f(a)\n    if a is None:\n        a = ca.DM.zeros(x.numel(), 1)\n    f_a = ca.substitute(f, x, a)  # f(a)\n    J = ca.jacobian(f, x)\n    if assert_linear and ca.depends_on(J, x):\n        raise AssertionError('not linear')\n    # solve is smart enough to to convert to blt if necessary\n    return ca.solve(J, -f_a)", "response": "Create a tangent approximation of a non - linear function f about point x"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_function_f_i(self):\n        return ca.Function(\n            'f_i',\n            [self.t, self.x, self.y, self.m, self.p, self.c, self.pre_c, self.ng, self.nu],\n            [self.f_i],\n            ['t', 'x', 'y', 'm', 'p', 'c', 'pre_c', 'ng', 'nu'], ['x_n'], self.func_opt)", "response": "state reinitialization ( reset ) function"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_function_f_m(self):\n        return ca.Function(\n            'f_m',\n            [self.t, self.x, self.y, self.m, self.p, self.c, self.pre_c, self.ng, self.nu],\n            [self.f_m],\n            ['t', 'x', 'y', 'm', 'p', 'c', 'pre_c', 'ng', 'nu'], ['m'], self.func_opt)", "response": "Creates a Function object from the properties of the class instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_function_f_J(self):\n        return ca.Function(\n            'J',\n            [self.t, self.x, self.y, self.m, self.p, self.c, self.ng, self.nu],\n            [ca.jacobian(self.f_x_rhs, self.x)],\n            ['t', 'x', 'y', 'm', 'p', 'c', 'ng', 'nu'], ['J'], self.func_opt)", "response": "Create a function for state integration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_ode(self) -> HybridOde:\n        res_split = split_dae_alg(self.f_x, self.dx)\n        alg = res_split['alg']\n        dae = res_split['dae']\n\n        x_rhs = tangent_approx(dae, self.dx, assert_linear=True)\n        y_rhs = tangent_approx(alg, self.y, assert_linear=True)\n\n        return HybridOde(\n            c=self.c,\n            dx=self.dx,\n            f_c=self.f_c,\n            f_i=self.f_i,\n            f_m=self.f_m,\n            f_x_rhs=x_rhs,\n            y_rhs=y_rhs,\n            m=self.m,\n            ng=self.ng,\n            nu=self.nu,\n            p=self.p,\n            pre_m=self.pre_m,\n            pre_c=self.pre_c,\n            prop=self.prop,\n            sym=self.sym,\n            t=self.t,\n            x=self.x,\n            y=self.y,\n        )", "response": "Convert the current object to a HybridOde object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef performAction(self, action):\n        self._lastAction = []\n\n        # Markups chosen for each generator.\n        actions = self._allActions[action]\n\n        n = self.numOffbids * len(self.generators)\n        markups = actions[:n]\n        withholds = actions[n:]\n\n#        print \"ALL ACTIONS:\", self._allActions\n#        print \"ACTIONS:\", markups, withholds\n\n        self._offbid(markups, withholds)", "response": "Performs an action on the world that changes the internal state of the environment."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts arrays of percentage price markups and capacity withholds into offers and bids and submits them to the marketplace.", "response": "def _offbid(self, markups, withholds):\n        \"\"\" Converts arrays of percentage price markups and capacity withholds\n        into offers/bids and submits them to the marketplace.\n        \"\"\"\n        for i, g in enumerate(self.generators):\n            ratedPMin = self._g0[g][\"p_min\"]\n            ratedPMax = self._g0[g][\"p_max\"]\n            margPCost = self._g0[g][\"p_cost\"]\n            margPCostModel = self._g0[g][\"pcost_model\"]\n\n            # Index of the first markup in 'markups' for generator 'i'.\n            k = i * (len(markups) / len(self.generators))\n            # Index of the first withhold in 'withholds' for generator 'i'.\n            kk = i * (len(withholds) / len(self.generators))\n\n            # Determine the cost at zero output.\n            if margPCostModel == POLYNOMIAL:\n                costNoLoad = margPCost[-1]\n            else:\n                costNoLoad = 0.0\n\n            # Divide available capacity equally among the offers/bids.\n            if g.is_load:\n                qty = ratedPMin / self.numOffbids\n            else:\n                qty = ratedPMax / self.numOffbids\n\n            # Track the total quantity offered/bid for by the generator.\n            totQty = 0.0\n\n#            p0 = 0.0\n#            c0 = costNoLoad\n            for j in range(self.numOffbids):\n                wh = withholds[kk+j]\n                qty = qty * ((100.0 - wh) / 100.0)\n\n                totQty += qty\n\n                # The markups are cumulative to ensure cost function convexity.\n                mk = sum(markups[k:k + j + 1])\n\n                # Marginal cost (cost function gradient).\n                if margPCostModel == POLYNOMIAL:\n                    cmarg = polyval(polyder(margPCost), totQty)\n                elif margPCostModel == PW_LINEAR:\n                    n_segments = len(margPCost) - 1\n                    for i in range(n_segments):\n                        x1, y1 = margPCost[i]\n                        x2, y2 = margPCost[i + 1]\n                        if x1 <= totQty <= x2:\n                            cmarg = (y2 - y1) / (x2 - x1)\n                    else:\n                        raise ValueError, \"Invalid bid quantity [%f].\" % totQty\n                else:\n                    raise ValueError\n\n                # Markup the marginal cost of the generator.\n                if not g.is_load:\n                    prc = cmarg * ((100.0 + mk) / 100.0)\n                else:\n                    prc = cmarg * ((100.0 + mk) / 100.0)\n\n                if not g.is_load:\n                    offer = Offer(g, qty, prc, costNoLoad)\n                    self.market.offers.append(offer)\n\n                    self._lastAction.append(offer)\n\n                    logger.info(\n                        \"%.2fMW offered at %.2f$/MWh for %s (%.1f%%, %.1f%%).\"\n                        % (qty, prc, g.name, mk, wh))\n                else:\n                    bid = Bid(g, -qty, prc, costNoLoad)\n                    self.market.bids.append(bid)\n\n                    self._lastAction.append(bid)\n\n                    logger.info(\n                        \"%.2f$/MWh bid for %.2fMW for %s (%.1f%%, %.1f%%).\"\n                        % (prc, -qty, g.name, mk, wh))\n\n        return self._lastAction"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to infer a protocol from the file extension.", "response": "def format_from_extension(fname):\n    \"\"\" Tries to infer a protocol from the file extension.\"\"\"\n    _base, ext = os.path.splitext(fname)\n    if not ext:\n        return None\n    try:\n        format = known_extensions[ext.replace('.', '')]\n    except KeyError:\n        format = None\n    return format"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread the MATPOWER case files at the given paths and pickles the resulting Case objects to the same directory.", "response": "def pickle_matpower_cases(case_paths, case_format=2):\n    \"\"\" Parses the MATPOWER case files at the given paths and pickles the\n        resulting Case objects to the same directory.\n    \"\"\"\n    import pylon.io\n\n    if isinstance(case_paths, basestring):\n        case_paths = [case_paths]\n\n    for case_path in case_paths:\n        # Read the MATPOWER case file.\n        case = pylon.io.MATPOWERReader(case_format).read(case_path)\n\n        # Give the new file the same name, but with a different extension.\n        dir_path = os.path.dirname(case_path)\n        case_basename = os.path.basename(case_path)\n        root, _ = os.path.splitext(case_basename)\n        pickled_case_path = os.path.join(dir_path, root + '.pkl')\n\n        # Pickle the resulting Pylon Case object.\n        pylon.io.PickleWriter(case).write(pickled_case_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntake a single iterable as an argument and returns the same output as the built-in function max with two output parameters, except that where the maximum value occurs at more than one position in the vector, the index is chosen randomly from these positions as opposed to just choosing the first occurance.", "response": "def fair_max(x):\n    \"\"\" Takes a single iterable as an argument and returns the same output as\n    the built-in function max with two output parameters, except that where\n    the maximum value occurs at more than one position in the  vector, the\n    index is chosen randomly from these positions as opposed to just choosing\n    the first occurance.\n    \"\"\"\n    value = max(x)\n    # List indexes of max value.\n    i = [x.index(v) for v in x if v == value]\n    # Select index randomly among occurances.\n    idx = random.choice(i)\n\n    return idx, value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef factorial(n):\n    f = 1\n    while (n > 0):\n        f = f * n\n        n = n - 1\n    return f", "response": "Returns the factorial of n."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_name(self):\n        if self._name is None:\n            self._name = self._generate_name()\n        return self._name", "response": "Returns the name of the class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsaving the object to a given file like object in the given format.", "response": "def save_to_file_object(self, fd, format=None, **kwargs):\n        \"\"\" Save the object to a given file like object in the given format.\n        \"\"\"\n        format = 'pickle' if format is None else format\n        save = getattr(self, \"save_%s\" % format, None)\n        if save is None:\n            raise ValueError(\"Unknown format '%s'.\" % format)\n        save(fd, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload the object from a given file like object in the given format.", "response": "def load_from_file_object(cls, fd, format=None):\n        \"\"\" Load the object from a given file like object in the given format.\n        \"\"\"\n        format = 'pickle' if format is None else format\n        load = getattr(cls, \"load_%s\" % format, None)\n        if load is None:\n            raise ValueError(\"Unknown format '%s'.\" % format)\n        return load(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef save(self, filename, format=None, **kwargs):\n        if format is None:\n            # try to derive protocol from file extension\n            format = format_from_extension(filename)\n        with file(filename, 'wb') as fp:\n            self.save_to_file_object(fp, format, **kwargs)", "response": "Save the object to file given by filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load(cls, filename, format=None):\n        if format is None:\n            # try to derive protocol from file extension\n            format = format_from_extension(filename)\n        with file(filename, 'rbU') as fp:\n            obj = cls.load_from_file_object(fp, format)\n            obj.filename = filename\n            return obj", "response": "Load an instance of the class from the given file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef solve(self):\n        case = self.case\n        logger.info(\"Starting DC power flow [%s].\" % case.name)\n        t0 = time.time()\n        # Update bus indexes.\n        self.case.index_buses()\n\n        # Find the index of the refence bus.\n        ref_idx = self._get_reference_index(case)\n        if ref_idx < 0:\n            return False\n\n        # Build the susceptance matrices.\n        B, Bsrc, p_businj, p_srcinj = case.Bdc\n        # Get the vector of initial voltage angles.\n        v_angle_guess = self._get_v_angle_guess(case)\n        # Calculate the new voltage phase angles.\n        v_angle, p_ref = self._get_v_angle(case, B, v_angle_guess, p_businj,\n                                           ref_idx)\n        logger.debug(\"Bus voltage phase angles: \\n%s\" % v_angle)\n        self.v_angle = v_angle\n\n        # Push the results to the case.\n        self._update_model(case, B, Bsrc, v_angle, p_srcinj, p_ref, ref_idx)\n\n        logger.info(\"DC power flow completed in %.3fs.\" % (time.time() - t0))\n\n        return True", "response": "Solves a DC power flow."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_reference_index(self, case):\n        refs = [bus._i for bus in case.connected_buses if bus.type == REFERENCE]\n        if len(refs) == 1:\n            return refs [0]\n        else:\n            logger.error(\"Single swing bus required for DCPF.\")\n            return -1", "response": "Returns the index of the reference bus."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting vector of voltage phase guesses.", "response": "def _get_v_angle_guess(self, case):\n        \"\"\" Make the vector of voltage phase guesses.\n        \"\"\"\n        v_angle = array([bus.v_angle * (pi / 180.0)\n                         for bus in case.connected_buses])\n        return v_angle"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the voltage phase angles and the susceptance matrix for the current case.", "response": "def _get_v_angle(self, case, B, v_angle_guess, p_businj, iref):\n        \"\"\" Calculates the voltage phase angles.\n        \"\"\"\n        buses = case.connected_buses\n\n        pv_idxs = [bus._i for bus in buses if bus.type == PV]\n        pq_idxs = [bus._i for bus in buses if bus.type == PQ]\n        pvpq_idxs = pv_idxs + pq_idxs\n        pvpq_rows = [[i] for i in pvpq_idxs]\n\n        # Get the susceptance matrix with the column and row corresponding to\n        # the reference bus removed.\n        Bpvpq = B[pvpq_rows, pvpq_idxs]\n\n        Bref = B[pvpq_rows, [iref]]\n\n        # Bus active power injections (generation - load) adjusted for phase\n        # shifters and real shunts.\n        p_surplus = array([case.s_surplus(v).real for v in buses])\n        g_shunt = array([bus.g_shunt for bus in buses])\n        Pbus = (p_surplus - p_businj - g_shunt) / case.base_mva\n\n        Pbus.shape = len(Pbus), 1\n\n        A = Bpvpq\n        b = Pbus[pvpq_idxs] - Bref * v_angle_guess[iref]\n\n#        x, res, rank, s = linalg.lstsq(A.todense(), b)\n        x = spsolve(A, b)\n\n        # Insert the reference voltage angle of the slack bus.\n        v_angle = r_[x[:iref], v_angle_guess[iref], x[iref:]]\n\n        return v_angle, Pbus[iref]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _update_model(self, case, B, Bsrc, v_angle, p_srcinj, p_ref, ref_idx):\n        iref = ref_idx\n        base_mva = case.base_mva\n        buses = case.connected_buses\n        branches = case.online_branches\n\n        p_from = (Bsrc * v_angle + p_srcinj) * base_mva\n        p_to = -p_from\n\n        for i, branch in enumerate(branches):\n            branch.p_from = p_from[i]\n            branch.p_to = p_to[i]\n            branch.q_from = 0.0\n            branch.q_to = 0.0\n\n        for j, bus in enumerate(buses):\n            bus.v_angle = v_angle[j] * (180 / pi)\n            bus.v_magnitude = 1.0\n\n        # Update Pg for swing generator.\n        g_ref = [g for g in case.generators if g.bus == buses[iref]][0]\n        # Pg = Pinj + Pload + Gs\n        # newPg = oldPg + newPinj - oldPinj\n        p_inj = (B[iref, :] * v_angle - p_ref) * base_mva\n        g_ref.p += p_inj[0]", "response": "Updates the case with values computed from the voltage phase phase\n            angle solution."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self.p_lmbda = 0.0\n        self.q_lmbda = 0.0\n        self.mu_vmin = 0.0\n        self.mu_vmax = 0.0", "response": "Resets the readonly variables."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset(self):\n        self.p_from = 0.0\n        self.p_to = 0.0\n        self.q_from = 0.0\n        self.q_to = 0.0\n\n        self.mu_s_from = 0.0\n        self.mu_s_to = 0.0\n\n        self.mu_angmin = 0.0\n        self.mu_angmax = 0.0", "response": "Resets the readonly variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getSbus(self, buses=None):\n        bs = self.buses if buses is None else buses\n        s = array([self.s_surplus(v) / self.base_mva for v in bs])\n        return s", "response": "Returns the net complex bus power injection vector in p. u."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sort_generators(self):\n        self.generators.sort(key=lambda gn: gn.bus._i)", "response": "Reorders the list of generators according to bus index."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the indices of all connected buses.", "response": "def index_buses(self, buses=None, start=0):\n        \"\"\" Updates the indices of all buses.\n\n        @param start: Starting index, typically 0 or 1.\n        @type start: int\n        \"\"\"\n        bs = self.connected_buses if buses is None else buses\n        for i, b in enumerate(bs):\n            b._i = start + i"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates the indices of all online branches.", "response": "def index_branches(self, branches=None, start=0):\n        \"\"\" Updates the indices of all branches.\n\n        @param start: Starting index, typically 0 or 1.\n        @type start: int\n        \"\"\"\n        ln = self.online_branches if branches is None else branches\n        for i, l in enumerate(ln):\n            l._i = start + i"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef s_supply(self, bus):\n        Sg = array([complex(g.p, g.q) for g in self.generators if\n                   (g.bus == bus) and not g.is_load], dtype=complex64)\n\n        if len(Sg):\n            return sum(Sg)\n        else:\n            return 0 + 0j", "response": "Returns the total complex power generation capacity."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef s_demand(self, bus):\n        Svl = array([complex(g.p, g.q) for g in self.generators if\n                    (g.bus == bus) and g.is_load], dtype=complex64)\n\n        Sd = complex(bus.p_demand, bus.q_demand)\n\n        return -sum(Svl) + Sd", "response": "Returns the total complex power demand."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getYbus(self, buses=None, branches=None):\n        buses = self.buses if buses is None else buses\n        branches = self.branches if branches is None else branches\n\n        nb = len(buses)\n        nl = len(branches)\n        ib = array(range(nb), dtype=int32)\n        il = array(range(nl), dtype=int32)\n\n        online = array([e.online for e in branches])\n\n        # Series admittance.\n        r = array([e.r for e in branches])\n        x = array([e.x for e in branches])\n        Ys = online / (r + 1j * x)\n\n        # Line charging susceptance.\n        b = array([e.b for e in branches])\n        Bc = online * b\n\n        #  Transformer tap ratios.\n        tap = ones(nl) # Default tap ratio = 1.0.\n        # Indices of branches with non-zero tap ratio.\n        i_trx = array([i for i, e in enumerate(branches) if e.ratio != 0.0],\n                      dtype=int32)\n        # Transformer off nominal turns ratio ( = 0 for lines ) (taps at\n        # \"from\" bus, impedance at 'to' bus, i.e. ratio = Vf / Vt)\"\n        ratio = array([e.ratio for e in branches])\n\n        # Set non-zero tap ratios.\n        if len(i_trx) > 0:\n            tap[i_trx] = ratio[i_trx]\n\n        # Phase shifters.\n        shift = array([e.phase_shift * pi / 180.0 for e in branches])\n\n        tap = tap * exp(1j * shift)\n\n        # Branch admittance matrix elements.\n        Ytt = Ys + 1j * Bc / 2.0\n        Yff = Ytt / (tap * conj(tap))\n        Yft = -Ys / conj(tap)\n        Ytf = -Ys / tap\n\n        # Shunt admittance.\n        g_shunt = array([v.g_shunt for v in buses])\n        b_shunt = array([v.b_shunt for v in buses])\n        Ysh = (g_shunt + 1j * b_shunt) / self.base_mva\n\n        # Connection matrices.\n        f = [e.from_bus._i for e in branches]\n        t = [e.to_bus._i for e in branches]\n\n        Cf = csc_matrix((ones(nl), (il, f)), shape=(nl, nb))\n        Ct = csc_matrix((ones(nl), (il, t)), shape=(nl, nb))\n\n        # Build bus admittance matrix\n        i = r_[il, il]\n        j = r_[f, t]\n        Yf = csc_matrix((r_[Yff, Yft], (i, j)), (nl, nb))\n        Yt = csc_matrix((r_[Ytf, Ytt], (i, j)), (nl, nb))\n\n        # Branch admittances plus shunt admittances.\n        Ysh_diag = csc_matrix((Ysh, (ib, ib)), shape=(nb, nb))\n        Ybus = Cf.T * Yf + Ct.T * Yt + Ysh_diag\n        assert Ybus.shape == (nb, nb)\n\n        return Ybus, Yf, Yt", "response": "This function generates a triple containing the bus admittance matrix Yf and Yt for each line in the branch list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbase on makeB.m from MATPOWER by Ray Zimmerman, developed at PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more information. @param method: Specify \"XB\" or \"BX\" method. @type method: string @rtype: tuple @return: Two matrices, B prime and B double prime, used in the fast decoupled power flow solver.", "response": "def makeB(self, buses=None, branches=None, method=\"XB\"):\n        \"\"\" Based on makeB.m from MATPOWER by Ray Zimmerman, developed at\n        PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more\n        information.\n\n        @param method: Specify \"XB\" or \"BX\" method.\n        @type method: string\n\n        @rtype: tuple\n        @return: Two matrices, B prime and B double prime, used in the fast\n        decoupled power flow solver.\n        \"\"\"\n        buses = self.connected_buses if buses is None else buses\n        branches = self.online_branches if branches is None else branches\n\n        B_buses = copy.deepcopy(buses) # modify bus copies\n        Bp_branches = copy.deepcopy(branches) # modify branch copies\n        Bpp_branches = copy.deepcopy(branches)\n\n        for bus in B_buses:\n            bus.b_shunt = 0.0\n        for branch in Bp_branches:\n            branch.b = 0.0\n            branch.ratio = 1.0\n            if method == \"XB\":\n                branch.r = 0.0\n\n        Yp, _, _ = self.getYbus(B_buses, Bp_branches)\n\n        for branch in Bpp_branches:\n            branch.phase_shift = 0.0\n            if method == \"BX\":\n                branch.r = 0.0\n\n        Ypp, _, _ = self.getYbus(B_buses, Bpp_branches)\n\n        del B_buses\n        del Bp_branches\n\n        return -Yp.imag, -Ypp.imag"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef makeBdc(self, buses=None, branches=None):\n        buses = self.connected_buses if buses is None else buses\n        branches = self.online_branches if branches is None else branches\n\n        nb = len(buses)\n        nl = len(branches)\n\n        # Ones at in-service branches.\n        online = array([br.online for br in branches])\n        # Series susceptance.\n        b = online / array([br.x for br in branches])\n\n        # Default tap ratio = 1.0.\n        tap = ones(nl)\n        # Transformer off nominal turns ratio (equals 0 for lines) (taps at\n        # \"from\" bus, impedance at 'to' bus, i.e. ratio = Vsrc / Vtgt)\n        for i, branch in enumerate(branches):\n            if branch.ratio != 0.0:\n                tap[i] = branch.ratio\n        b = b / tap\n\n        f = [br.from_bus._i for br in branches]\n        t = [br.to_bus._i for br in branches]\n        i = r_[array(range(nl)), array(range(nl))]\n        one = ones(nl)\n        Cft = csc_matrix((r_[one, -one], (i, r_[f, t])), shape=(nl, nb))\n#        Cf = spmatrix(1.0, f, range(nl), (nb, nl))\n#        Ct = spmatrix(1.0, t, range(nl), (nb, nl))\n\n        # Build Bsrc such that Bsrc * Va is the vector of real branch powers\n        # injected at each branch's \"from\" bus.\n        Bf = csc_matrix((r_[b, -b], (i, r_[f, t])), (nl, nb))\n\n        Bbus = Cft.T * Bf\n\n        # Build phase shift injection vectors.\n        shift = array([br.phase_shift * pi / 180.0 for br in branches])\n        Pfinj = b * shift\n        #Ptinj = -Pfinj\n        # Pbusinj = Cf * Pfinj + Ct * Ptinj\n        Pbusinj = Cft.T * Pfinj\n\n        return Bbus, Bf, Pbusinj, Pfinj", "response": "This function creates a BDC matrix for the given set of buses and branches."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dSbus_dV(self, Y, V):\n        ib = range(len(V))\n\n        I = Y * V\n\n        diagV = csr_matrix((V, (ib, ib)))\n        diagIbus = csr_matrix((I, (ib, ib)))\n        # Element-wise division.\n        diagVnorm = csr_matrix((V / abs(V), (ib, ib)))\n\n        dS_dVm = diagV * conj(Y * diagVnorm) + conj(diagIbus) * diagVnorm\n        dS_dVa = 1j * diagV * conj(diagIbus - Y * diagV)\n\n        return dS_dVm, dS_dVa", "response": "Based on dSbus_dV. m from Ray Zimmerman."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the partial derivatives of branch currents w. r. t. voltage from the branch currents.", "response": "def dIbr_dV(self, Yf, Yt, V):\n        \"\"\" Based on dIbr_dV.m from MATPOWER by Ray Zimmerman, developed at\n        PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more\n        information.\n\n        @return: The partial derivatives of branch currents w.r.t. voltage\n                 magnitude and voltage angle.\n        @rtype: tuple\n        \"\"\"\n        i = range(len(V))\n\n        Vnorm = V / abs(V)\n        diagV = csr_matrix((V, (i, i)))\n        diagVnorm = csr_matrix((Vnorm, (i, i)))\n        dIf_dVa = Yf * 1j * diagV\n        dIf_dVm = Yf * diagVnorm\n        dIt_dVa = Yt * 1j * diagV\n        dIt_dVm = Yt * diagVnorm\n\n        # Compute currents.\n        If = Yf * V\n        It = Yt * V\n\n        return dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the partial derivative of S w. r. t voltage magnitude and voltage angle.", "response": "def dSbr_dV(self, Yf, Yt, V, buses=None, branches=None):\n        \"\"\" Based on dSbr_dV.m from MATPOWER by Ray Zimmerman, developed at\n        PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more\n        information.\n\n        @return: The branch power flow vectors and the partial derivatives of\n                 branch power flow w.r.t voltage magnitude and voltage angle.\n        @rtype: tuple\n        \"\"\"\n        buses = self.buses if buses is None else buses\n        branches = self.branches if branches is None else branches\n\n        nl = len(branches)\n        nb = len(V)\n        il = range(nl)\n        ib = range(nb)\n\n        f = [l.from_bus._i for l in branches]\n        t = [l.to_bus._i for l in branches]\n\n        # Compute currents.\n        If = Yf * V\n        It = Yt * V\n\n        Vnorm = V / abs(V)\n\n        diagVf = csr_matrix((V[f], (il, il)))\n        diagIf = csr_matrix((If, (il, il)))\n        diagVt = csr_matrix((V[t], (il, il)))\n        diagIt = csr_matrix((It, (il, il)))\n        diagV  = csr_matrix((V, (ib, ib)))\n        diagVnorm = csr_matrix((Vnorm, (ib, ib)))\n\n        shape = (nl, nb)\n        # Partial derivative of S w.r.t voltage phase angle.\n        dSf_dVa = 1j * (conj(diagIf) *\n            csr_matrix((V[f], (il, f)), shape) - diagVf * conj(Yf * diagV))\n\n        dSt_dVa = 1j * (conj(diagIt) *\n            csr_matrix((V[t], (il, t)), shape) - diagVt * conj(Yt * diagV))\n\n        # Partial derivative of S w.r.t. voltage amplitude.\n        dSf_dVm = diagVf * conj(Yf * diagVnorm) + conj(diagIf) * \\\n            csr_matrix((Vnorm[f], (il, f)), shape)\n\n        dSt_dVm = diagVt * conj(Yt * diagVnorm) + conj(diagIt) * \\\n            csr_matrix((Vnorm[t], (il, t)), shape)\n\n        # Compute power flow vectors.\n        Sf = V[f] * conj(If)\n        St = V[t] * conj(It)\n\n        return dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbases on dAbr_dV. m from Ray Zimmerman.", "response": "def dAbr_dV(self, dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St):\n        \"\"\" Based on dAbr_dV.m from MATPOWER by Ray Zimmerman, developed at\n        PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more\n        information.\n\n        @rtype: tuple\n        @return: The partial derivatives of the squared flow magnitudes w.r.t\n                 voltage magnitude and voltage angle given the flows and flow\n                 sensitivities. Flows could be complex current or complex or\n                 real power.\n        \"\"\"\n        il = range(len(Sf))\n\n        dAf_dPf = csr_matrix((2 * Sf.real, (il, il)))\n        dAf_dQf = csr_matrix((2 * Sf.imag, (il, il)))\n        dAt_dPt = csr_matrix((2 * St.real, (il, il)))\n        dAt_dQt = csr_matrix((2 * St.imag, (il, il)))\n\n        # Partial derivative of apparent power magnitude w.r.t voltage\n        # phase angle.\n        dAf_dVa = dAf_dPf * dSf_dVa.real + dAf_dQf * dSf_dVa.imag\n        dAt_dVa = dAt_dPt * dSt_dVa.real + dAt_dQt * dSt_dVa.imag\n        # Partial derivative of apparent power magnitude w.r.t. voltage\n        # amplitude.\n        dAf_dVm = dAf_dPf * dSf_dVm.real + dAf_dQf * dSf_dVm.imag\n        dAt_dVm = dAt_dPt * dSt_dVm.real + dAt_dQt * dSt_dVm.imag\n\n        return dAf_dVa, dAf_dVm, dAt_dVa, dAt_dVm"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbase on d2Sbus_dV2. m from Ray Zimmerman.", "response": "def d2Sbus_dV2(self, Ybus, V, lam):\n        \"\"\" Based on d2Sbus_dV2.m from MATPOWER by Ray Zimmerman, developed\n        at PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for\n        more information.\n\n        @rtype: tuple\n        @return: The 2nd derivatives of power injection w.r.t. voltage.\n        \"\"\"\n        nb = len(V)\n        ib = range(nb)\n        Ibus = Ybus * V\n        diaglam = csr_matrix((lam, (ib, ib)))\n        diagV = csr_matrix((V, (ib, ib)))\n\n        A = csr_matrix((lam * V, (ib, ib)))\n        B = Ybus * diagV\n        C = A * conj(B)\n        D = Ybus.H * diagV\n        E = diagV.conj() * (D * diaglam - csr_matrix((D * lam, (ib, ib))))\n        F = C - A * csr_matrix((conj(Ibus), (ib, ib)))\n        G = csr_matrix((ones(nb) / abs(V), (ib, ib)))\n\n        Gaa = E + F\n        Gva = 1j * G * (E - F)\n        Gav = Gva.T\n        Gvv = G * (C + C.T) * G\n\n        return Gaa, Gav, Gva, Gvv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef d2Ibr_dV2(self, Ybr, V, lam):\n        nb = len(V)\n        ib = range(nb)\n        diaginvVm = csr_matrix((ones(nb) / abs(V), (ib, ib)))\n\n        Haa = csr_matrix((-(Ybr.T * lam) / V, (ib, ib)))\n        Hva = -1j * Haa * diaginvVm\n        Hav = Hva\n        Hvv = csr_matrix((nb, nb))\n\n        return Haa, Hav, Hva, Hvv", "response": "Based on d2Ibr_dV2. m from Ray Zimmerman."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbases on d2Sbr_dV2. m from PSERC Cornell.", "response": "def d2Sbr_dV2(self, Cbr, Ybr, V, lam):\n        \"\"\" Based on d2Sbr_dV2.m from MATPOWER by Ray Zimmerman, developed\n        at PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for\n        more information.\n\n        @rtype: tuple\n        @return: The 2nd derivatives of complex power flow w.r.t. voltage.\n        \"\"\"\n        nb = len(V)\n        nl = len(lam)\n        ib = range(nb)\n        il = range(nl)\n\n        diaglam = csr_matrix((lam, (il, il)))\n        diagV = csr_matrix((V, (ib, ib)))\n\n        A = Ybr.H * diaglam * Cbr\n        B = conj(diagV) * A * diagV\n        D = csr_matrix( ((A * V) * conj(V), (ib, ib)) )\n        E = csr_matrix( ((A.T * conj(V) * V), (ib, ib)) )\n        F = B + B.T\n        G = csr_matrix((ones(nb) / abs(V), (ib, ib)))\n\n        Haa = F - D - E\n        Hva = 1j * G * (B - B.T - D + E)\n        Hav = Hva.T\n        Hvv = G * F * G\n\n        return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbase on d2ASbr_dV2. m from Ray Zimmerman.", "response": "def d2ASbr_dV2(self, dSbr_dVa, dSbr_dVm, Sbr, Cbr, Ybr, V, lam):\n        \"\"\" Based on d2ASbr_dV2.m from MATPOWER by Ray Zimmerman, developed\n        at PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for\n        more information.\n\n        @rtype: tuple\n        @return: The 2nd derivatives of |complex power flow|**2 w.r.t. V.\n        \"\"\"\n        il = range(len(lam))\n\n        diaglam = csr_matrix((lam, (il, il)))\n        diagSbr_conj = csr_matrix((Sbr.conj(), (il, il)))\n\n        Saa, Sav, Sva, Svv = self.d2Sbr_dV2(Cbr, Ybr, V, diagSbr_conj * lam)\n\n        Haa = 2 * ( Saa + dSbr_dVa.T * diaglam * dSbr_dVa.conj() ).real\n        Hva = 2 * ( Sva + dSbr_dVm.T * diaglam * dSbr_dVa.conj() ).real\n        Hav = 2 * ( Sav + dSbr_dVa.T * diaglam * dSbr_dVm.conj() ).real\n        Hvv = 2 * ( Svv + dSbr_dVm.T * diaglam * dSbr_dVm.conj() ).real\n\n        return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbasing on d2AIbr_dV2. m from Ray Zimmerman.", "response": "def d2AIbr_dV2(self, dIbr_dVa, dIbr_dVm, Ibr, Ybr, V, lam):\n        \"\"\" Based on d2AIbr_dV2.m from MATPOWER by Ray Zimmerman, developed\n        at PSERC Cornell. See U{http://www.pserc.cornell.edu/matpower/} for\n        more information.\n\n        @rtype: tuple\n        @return: The 2nd derivatives of |complex current|**2 w.r.t. V.\n        \"\"\"\n        il = range(len(lam))\n\n        diaglam = csr_matrix((lam, (il, il)))\n        diagIbr_conj = csr_matrix((conj(Ibr), (il, il)))\n\n        Iaa, Iav, Iva, Ivv = self.d2Ibr_dV2(Ybr, V, diagIbr_conj * lam)\n\n        Haa = 2 * ( Iaa + dIbr_dVa.T * diaglam * dIbr_dVa.conj() ).real\n        Hva = 2 * ( Iva + dIbr_dVm.T * diaglam * dIbr_dVa.conj() ).real\n        Hav = 2 * ( Iav + dIbr_dVa.T * diaglam * dIbr_dVm.conj() ).real\n        Hvv = 2 * ( Ivv + dIbr_dVm.T * diaglam * dIbr_dVm.conj() ).real\n\n        return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pf_solution(self, Ybus, Yf, Yt, V):\n        buses = self.connected_buses\n        branches = self.online_branches\n        generators = self.online_generators\n\n        self.reset()\n        self.index_buses()\n        self.index_branches()\n\n        Va = angle(V)\n        Vm = abs(V)\n        for i, b in enumerate(buses):\n            b.v_angle = Va[i] * 180.0 / pi\n            b.v_magnitude = Vm[i]\n\n        # Update Qg for all gens and Pg for swing bus.\n#        gbus = [g.bus._i for g in generators]\n        refgen = [i for i, g in enumerate(generators)\n                  if g.bus.type == REFERENCE]\n\n        # Compute total injected bus powers.\n#        Sg = V[gbus] * conj(Ybus[gbus, :] * V)\n        Sg = V * conj(Ybus * V)\n\n\n        # Update Qg for all generators.\n#        for i in gbus:\n#            g = generators[i]\n        for g in generators:\n            # inj Q + local Qd\n            g.q = Sg.imag[g.bus._i] * self.base_mva + g.bus.q_demand\n\n        # At this point any buses with more than one generator will have\n        # the total Q dispatch for the bus assigned to each generator. This\n        # must be split between them. We do it first equally, then in proportion\n        # to the reactive range of the generator.\n        if generators:\n            pass\n\n        # Update Pg for swing bus.\n        for i in refgen:\n            g = generators[i]\n            # inj P + local Pd\n            g.p = Sg.real[i] * self.base_mva + g.bus.p_demand\n\n        # More than one generator at the ref bus subtract off what is generated\n        # by other gens at this bus.\n        if len(refgen) > 1:\n            pass\n\n        br = [l._i for l in branches]\n        f_idx = [l.from_bus._i for l in branches]\n        t_idx = [l.to_bus._i for l in branches]\n\n        Sf = V[f_idx] * conj(Yf[br, :] * V) * self.base_mva\n        St = V[t_idx] * conj(Yt[br, :] * V) * self.base_mva\n\n        # Complex power at \"from\" bus.\n        for i, l in enumerate(branches):\n            l.p_from = Sf[i].real\n            l.q_from = Sf[i].imag\n            l.p_to = St[i].real\n            l.q_to = St[i].imag", "response": "This function calculates the power flow solution for a given set of bus assignments and power flow assignments."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef reset(self):\n        for bus in self.buses:\n            bus.reset()\n        for branch in self.branches:\n            branch.reset()\n        for generator in self.generators:\n            generator.reset()", "response": "Resets the readonly variables for all of the case components."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_matpower(self, fd):\n        from pylon.io import MATPOWERWriter\n        MATPOWERWriter(self).write(fd)", "response": "Serialize the case as a MATPOWER data file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize the case as a PSS or E data file.", "response": "def save_psse(self, fd):\n        \"\"\" Serialize the case as a PSS/E data file.\n        \"\"\"\n        from pylon.io import PSSEWriter\n        return PSSEWriter(self).write(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_psat(cls, fd):\n        from pylon.io.psat import PSATReader\n        return PSATReader().read(fd)", "response": "Loads a case object from the given PSAT data file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_rst(self, fd):\n        from pylon.io import ReSTWriter\n        ReSTWriter(self).write(fd)", "response": "Save a reStructuredText representation of the case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the case as a series of Comma -Separated Values.", "response": "def save_csv(self, fd):\n        \"\"\" Saves the case as a series of Comma-Separated Values.\n        \"\"\"\n        from pylon.io.excel import CSVWriter\n        CSVWriter(self).write(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_excel(self, fd):\n        from pylon.io.excel import ExcelWriter\n        ExcelWriter(self).write(fd)", "response": "Saves the case as an Excel spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_dot(self, fd):\n        from pylon.io import DotWriter\n        DotWriter(self).write(fd)", "response": "Saves the current case in the Graphviz DOT language."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves a precompiled CasADi model to disk.", "response": "def save_model(model_folder: str, model_name: str, model: Model,\n               compiler_options: Dict[str, str]) -> None:\n    \"\"\"\n    Saves a CasADi model to disk.\n\n    :param model_folder: Folder where the precompiled CasADi model will be stored.\n    :param model_name: Name of the model.\n    :param model: Model instance.\n    :param compiler_options: Dictionary of compiler options.\n    \"\"\"\n\n    objects = {'dae_residual': None, 'initial_residual': None, 'variable_metadata': None, 'delay_arguments': None}\n    for o in objects.keys():\n        f = getattr(model, o + '_function')\n\n        if compiler_options.get('codegen', False):\n            objects[o] = _codegen_model(model_folder, f, '{}_{}'.format(model_name, o))\n        else:\n            objects[o] = f\n\n    # Output metadata\n    db_file = os.path.join(model_folder, model_name + \".pymoca_cache\")\n    with open(db_file, 'wb') as f:\n        db = {}\n\n        # Store version\n        db['version'] = __version__\n\n        # Include references to the shared libraries (codegen) or pickled functions (cache)\n        db.update(objects)\n\n        db['library_os'] = os.name\n\n        db['options'] = compiler_options\n\n        # Describe variables per category\n        for key in ['states', 'der_states', 'alg_states', 'inputs', 'parameters', 'constants']:\n            db[key] = [e.to_dict() for e in getattr(model, key)]\n\n\n        # Caching using CasADi functions will lead to constants seemingly\n        # depending on MX variables. Figuring out that they do not is slow,\n        # especially when doing it on a lazy function call, as would be the\n        # case when reading from cache. So instead, we do the depency check\n        # once when saving the model.\n\n        # Metadata dependency checking\n        parameter_vector = ca.veccat(*[v.symbol for v in model.parameters])\n\n        for k, key in enumerate(['states', 'alg_states', 'inputs', 'parameters', 'constants']):\n            metadata_shape = (len(getattr(model, key)), len(CASADI_ATTRIBUTES))\n            m = db[key + \"__metadata_dependent\"] = np.zeros(metadata_shape, dtype=bool)\n            for i, v in enumerate(getattr(model, key)):\n                for j, tmp in enumerate(CASADI_ATTRIBUTES):\n                    attr = getattr(v, tmp)\n                    if (isinstance(attr, ca.MX) and not attr.is_constant()\n                        and ca.depends_on(attr, parameter_vector)):\n                        m[i, j] = True\n\n        # Delay dependency checking\n        if model.delay_states:\n\n            all_symbols = [model.time,\n                           *model._symbols(model.states),\n                           *model._symbols(model.der_states),\n                           *model._symbols(model.alg_states),\n                           *model._symbols(model.inputs),\n                           *model._symbols(model.constants),\n                           *model._symbols(model.parameters)]\n            symbol_to_index = {x: i for i, x in enumerate(all_symbols)}\n\n            expressions, durations = zip(*model.delay_arguments)\n\n            duration_dependencies = []\n            for dur in durations:\n                duration_dependencies.append(\n                    [symbol_to_index[var] for var in ca.symvar(dur) if ca.depends_on(dur, var)])\n            db['__delay_duration_dependent'] = duration_dependencies\n\n        db['outputs'] = model.outputs\n\n        db['delay_states'] = model.delay_states\n\n        db['alias_relation'] = model.alias_relation\n\n        pickle.dump(db, f, protocol=-1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a precompiled CasADi model into a CachedModel instance.", "response": "def load_model(model_folder: str, model_name: str, compiler_options: Dict[str, str]) -> CachedModel:\n    \"\"\"\n    Loads a precompiled CasADi model into a CachedModel instance.\n\n    :param model_folder: Folder where the precompiled CasADi model is located.\n    :param model_name: Name of the model.\n    :param compiler_options: Dictionary of compiler options.\n\n    :returns: CachedModel instance.\n    \"\"\"\n\n    db_file = os.path.join(model_folder, model_name + \".pymoca_cache\")\n\n    if compiler_options.get('mtime_check', True):\n        # Mtime check\n        cache_mtime = os.path.getmtime(db_file)\n        for folder in [model_folder] + compiler_options.get('library_folders', []):\n            for root, dir, files in os.walk(folder, followlinks=True):\n                for item in fnmatch.filter(files, \"*.mo\"):\n                    filename = os.path.join(root, item)\n                    if os.path.getmtime(filename) > cache_mtime:\n                        raise InvalidCacheError(\"Cache out of date\")\n\n    # Create empty model object\n    model = CachedModel()\n\n    # Load metadata\n    with open(db_file, 'rb') as f:\n        db = pickle.load(f)\n\n        if db['version'] != __version__:\n            raise InvalidCacheError('Cache generated for a different version of pymoca')\n\n        # Check compiler options. We ignore the library folders, as they have\n        # already been checked, and checking them will impede platform\n        # portability of the cache.\n        exclude_options = ['library_folders']\n        old_opts = {k: v for k, v in db['options'].items() if k not in exclude_options}\n        new_opts = {k: v for k, v in compiler_options.items() if k not in exclude_options}\n\n        if old_opts != new_opts:\n            raise InvalidCacheError('Cache generated for different compiler options')\n\n        # Pickles are platform independent, but dynamic libraries are not\n        if compiler_options.get('codegen', False):\n            if db['library_os'] != os.name:\n                raise InvalidCacheError('Cache generated for incompatible OS')\n\n        # Include references to the shared libraries\n        for o in ['dae_residual', 'initial_residual', 'variable_metadata', 'delay_arguments']:\n            if isinstance(db[o], str):\n                # Path to codegen'd library\n                f = ca.external(o, db[o])\n            else:\n                # Pickled CasADi Function; use as is\n                assert isinstance(db[o], ca.Function)\n                f = db[o]\n\n            setattr(model, '_' + o + '_function', f)\n\n        # Load variables per category\n        variables_with_metadata = ['states', 'alg_states', 'inputs', 'parameters', 'constants']\n        variable_dict = {}\n        for key in variables_with_metadata:\n            variables = getattr(model, key)\n            for i, d in enumerate(db[key]):\n                variable = Variable.from_dict(d)\n                variables.append(variable)\n                variable_dict[variable.symbol.name()] = variable\n\n        model.der_states = [Variable.from_dict(d) for d in db['der_states']]\n        model.outputs = db['outputs']\n        model.delay_states = db['delay_states']\n        model.alias_relation = db['alias_relation']\n\n        # Evaluate variable metadata:\n        parameter_vector = ca.veccat(*[v.symbol for v in model.parameters])\n        metadata = dict(zip(variables_with_metadata, model.variable_metadata_function(parameter_vector)))\n        independent_metadata = dict(zip(\n            variables_with_metadata,\n            (np.array(x) for x in model.variable_metadata_function(ca.veccat(*[np.nan for v in model.parameters])))))\n\n        for k, key in enumerate(variables_with_metadata):\n            m = db[key + \"__metadata_dependent\"]\n            for i, d in enumerate(db[key]):\n                variable = variable_dict[d['name']]\n                for j, tmp in enumerate(CASADI_ATTRIBUTES):\n                    if m[i, j]:\n                        setattr(variable, tmp, metadata[key][i, j])\n                    else:\n                        setattr(variable, tmp, independent_metadata[key][i, j])\n\n        # Evaluate delay arguments:\n        if model.delay_states:\n            args = [model.time,\n                    ca.veccat(*model._symbols(model.states)),\n                    ca.veccat(*model._symbols(model.der_states)),\n                    ca.veccat(*model._symbols(model.alg_states)),\n                    ca.veccat(*model._symbols(model.inputs)),\n                    ca.veccat(*model._symbols(model.constants)),\n                    ca.veccat(*model._symbols(model.parameters))]\n            delay_arguments_raw = model.delay_arguments_function(*args)\n\n            nan_args = [ca.repmat(np.nan, *arg.size()) for arg in args]\n            independent_delay_arguments_raw = model.delay_arguments_function(*nan_args)\n\n            delay_expressions_raw = delay_arguments_raw[::2]\n            delay_durations_raw = delay_arguments_raw[1::2]\n            independent_delay_durations_raw = independent_delay_arguments_raw[1::2]\n\n            assert 1 == len({len(delay_expressions_raw), len(delay_durations_raw),\n                len(independent_delay_durations_raw)})\n\n            all_symbols = [model.time,\n                           *model._symbols(model.states),\n                           *model._symbols(model.der_states),\n                           *model._symbols(model.alg_states),\n                           *model._symbols(model.inputs),\n                           *model._symbols(model.constants),\n                           *model._symbols(model.parameters)]\n\n            duration_dependencies = db['__delay_duration_dependent']\n\n            # Get rid of false dependency symbols not used in any delay\n            # durations. This significantly reduces the work the (slow)\n            # substitute() calls have to do later on.\n            actual_deps = sorted(set(np.array(duration_dependencies).ravel()))\n\n            actual_dep_symbols = [np.nan] * len(all_symbols)\n            for i in actual_deps:\n                actual_dep_symbols[i] = all_symbols[i]\n\n            delay_durations_simplified = ca.Function(\n                'replace_false_deps',\n                all_symbols,\n                delay_durations_raw).call(\n                    actual_dep_symbols)\n\n            # Get rid of remaining hidden dependencies in the delay durations\n            for i, expr in enumerate(delay_expressions_raw):\n                if duration_dependencies[i]:\n                    dur = delay_durations_simplified[i]\n\n                    if len(duration_dependencies[i]) < len(actual_deps):\n                        deps = set(ca.symvar(dur))\n                        actual_deps = {all_symbols[j] for j in duration_dependencies[i]}\n                        false_deps = deps - actual_deps\n\n                        if false_deps:\n                            [dur] = ca.substitute(\n                                [dur],\n                                list(false_deps),\n                                [np.nan] * len(false_deps))\n                    else:\n                        # Already removed all false dependencies\n                        pass\n                else:\n                    dur = independent_delay_durations_raw[i]\n\n                model.delay_arguments.append(DelayArgument(expr, dur))\n\n        # Try to coerce parameters into their Python types\n        for p in model.parameters:\n            for attr in CASADI_ATTRIBUTES:\n                v = getattr(p, attr)\n                v_mx = ca.MX(v)\n                if v_mx.is_constant() and v_mx.is_regular():\n                    setattr(p, attr, p.python_type(v))\n\n    # Done\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef solve(self):\n        case = self.om.case\n        base_mva = case.base_mva\n        # TODO: Explain this value.\n        self.opt[\"cost_mult\"] = 1e-4\n\n        # Unpack the OPF model.\n        bs, ln, gn, _ = self._unpack_model(self.om)\n        # Compute problem dimensions.\n        ipol, _, nb, nl, _, ny, nxyz = self._dimension_data(bs, ln, gn)\n\n        # Compute problem dimensions.\n        ng = len(gn)\n#        gpol = [g for g in gn if g.pcost_model == POLYNOMIAL]\n        # Indexes of constrained lines.\n        il = array([i for i,l in enumerate(ln) if 0.0 < l.rate_a < 1e10])\n        nl2 = len(il)\n\n        # Linear constraints (l <= A*x <= u).\n        A, l, u = self.om.linear_constraints()\n#        AA, bb = self._linear_constraints(self.om)\n\n        _, xmin, xmax = self._var_bounds()\n\n        # Select an interior initial point for interior point solver.\n        x0 = self._initial_interior_point(bs, gn, xmin, xmax, ny)\n\n        # Build admittance matrices.\n        Ybus, Yf, Yt = case.Y\n\n        # Optimisation variables.\n        Va = self.om.get_var(\"Va\")\n        Vm = self.om.get_var(\"Vm\")\n        Pg = self.om.get_var(\"Pg\")\n        Qg = self.om.get_var(\"Qg\")\n\n        # Adds a constraint on the reference bus angles.\n#        xmin, xmax = self._ref_bus_angle_constraint(bs, Va, xmin, xmax)\n\n        def f_fcn(x, user_data=None):\n            \"\"\" Evaluates the objective function.\n            \"\"\"\n            p_gen = x[Pg.i1:Pg.iN + 1] # Active generation in p.u.\n            q_gen = x[Qg.i1:Qg.iN + 1] # Reactive generation in p.u.\n\n            # Polynomial cost of P and Q.\n            xx = r_[p_gen, q_gen] * base_mva\n            if len(ipol) > 0:\n                f = sum([g.total_cost(xx[i]) for i,g in enumerate(gn)])\n            else:\n                f = 0\n\n            # Piecewise linear cost of P and Q.\n            if ny:\n                y = self.om.get_var(\"y\")\n                ccost = csr_matrix((ones(ny),\n                    (range(y.i1, y.iN + 1), zeros(ny))), shape=(nxyz, 1)).T\n                f = f + ccost * x\n            else:\n                ccost = zeros((1, nxyz))\n                # TODO: Generalised cost term.\n\n            return f\n\n\n        def df_fcn(x, usr_data=None):\n            \"\"\" Calculates gradient of the objective function.\n            \"\"\"\n            p_gen = x[Pg.i1:Pg.iN + 1] # Active generation in p.u.\n            q_gen = x[Qg.i1:Qg.iN + 1] # Reactive generation in p.u.\n\n            xx = r_[p_gen, q_gen] * base_mva\n\n            if ny > 0:\n                y = self.om.get_var(\"y\")\n                iy = range(y.i1, y.iN + 1)\n                ccost = \\\n                    csr_matrix((ones(ny), (iy, zeros(ny))), shape=(nxyz, 1)).T\n            else:\n                ccost = zeros((1, nxyz))\n                # TODO: Generalised cost term.\n\n            iPg = range(Pg.i1, Pg.iN + 1)\n            iQg = range(Qg.i1, Qg.iN + 1)\n\n            # Polynomial cost of P and Q.\n            df_dPgQg = zeros((2 * ng, 1))        # w.r.t p.u. Pg and Qg\n#            df_dPgQg[ipol] = matrix([g.poly_cost(xx[i], 1) for g in gpol])\n#            for i, g in enumerate(gn):\n#                der = polyder(list(g.p_cost))\n#                df_dPgQg[i] = polyval(der, xx[i]) * base_mva\n            for i in ipol:\n                df_dPgQg[i] = \\\n                    base_mva * polyval(polyder(list(gn[i].p_cost)), xx[i])\n\n            df = zeros((nxyz, 1))\n            df[iPg] = df_dPgQg[:ng]\n            df[iQg] = df_dPgQg[ng:ng + ng]\n\n            # Piecewise linear cost of P and Q.\n            df = df + ccost.T\n            # TODO: Generalised cost term.\n\n            return asarray(df).flatten()\n\n\n        def g_fcn(x, usr_data=None):\n            \"\"\" Evaluates the non-linear constraint values.\n            \"\"\"\n            Pgen = x[Pg.i1:Pg.iN + 1] # Active generation in p.u.\n            Qgen = x[Qg.i1:Qg.iN + 1] # Reactive generation in p.u.\n\n            for i, g in enumerate(gn):\n                g.p = Pgen[i] * base_mva # active generation in MW\n                g.q = Qgen[i] * base_mva # reactive generation in MVAr\n\n            # Rebuild the net complex bus power injection vector in p.u.\n            Sbus = case.getSbus(bs)\n\n            Vang = x[Va.i1:Va.iN + 1]\n            Vmag = x[Vm.i1:Vm.iN + 1]\n            V = Vmag * exp(1j * Vang)\n\n            # Evaluate the power flow equations.\n            mis = V * conj(Ybus * V) - Sbus\n\n            # Equality constraints (power flow).\n            g = r_[mis.real,  # active power mismatch for all buses\n                   mis.imag]  # reactive power mismatch for all buses\n\n            # Inequality constraints (branch flow limits).\n            # (line constraint is actually on square of limit)\n            flow_max = array([(l.rate_a / base_mva)**2 for l in ln])\n            # FIXME: There must be a more elegant method for this.\n            for i, v in enumerate(flow_max):\n                if v == 0.0:\n                    flow_max[i] = Inf\n\n            if self.flow_lim == IFLOW:\n                If = Yf * V\n                It = Yt * V\n                # Branch current limits.\n                h = r_[(If * conj(If)) - flow_max,\n                       (If * conj(It)) - flow_max]\n            else:\n                i_fbus = [e.from_bus._i for e in ln]\n                i_tbus = [e.to_bus._i for e in ln]\n                # Complex power injected at \"from\" bus (p.u.).\n                Sf = V[i_fbus] * conj(Yf * V)\n                # Complex power injected at \"to\" bus (p.u.).\n                St = V[i_tbus] * conj(Yt * V)\n                if self.flow_lim == PFLOW: # active power limit, P (Pan Wei)\n                    # Branch real power limits.\n                    h = r_[Sf.real()**2 - flow_max,\n                           St.real()**2 - flow_max]\n                elif self.flow_lim == SFLOW: # apparent power limit, |S|\n                    # Branch apparent power limits.\n                    h = r_[(Sf * conj(Sf)) - flow_max,\n                           (St * conj(St)) - flow_max].real\n                else:\n                    raise ValueError\n\n            return r_[g, h]\n\n\n        def dg_fcn(x, flag, usr_data=None):\n            \"\"\" Calculates the Jacobian matrix. It takes two arguments, the\n                first is the variable x and the second is a Boolean flag. If\n                the flag is true, the function returns a tuple of arrays\n                (row, col) to indicate the sparse structure of the Jacobian\n                matrix. If the flag is false the function returns the values\n                of the Jacobian matrix with length nnzj.\n            \"\"\"\n            iVa = range(Va.i1, Va.iN + 1)\n            iVm = range(Vm.i1, Vm.iN + 1)\n            iPg = range(Pg.i1, Pg.iN + 1)\n            iQg = range(Qg.i1, Qg.iN + 1)\n            iVaVmPgQg = r_[iVa, iVm, iPg, iQg].T\n\n            Vang = x[Va.i1:Va.iN + 1]\n            Vmag = x[Vm.i1:Vm.iN + 1]\n            V = Vmag * exp(1j * Vang)\n\n            # Compute partials of injected bus powers.\n            dSbus_dVm, dSbus_dVa = case.dSbus_dV(Ybus, V)\n\n            i_gbus = [gen.bus._i for gen in gn]\n            neg_Cg = csr_matrix((-ones(ng), (i_gbus, range(ng))), (nb, ng))\n\n            # Transposed Jacobian of the power balance equality constraints.\n            dg = lil_matrix((nxyz, 2 * nb))\n\n            blank = csr_matrix((nb, ng))\n            dg[iVaVmPgQg, :] = vstack([\n                hstack([dSbus_dVa.real, dSbus_dVm.real, neg_Cg, blank]),\n                hstack([dSbus_dVa.imag, dSbus_dVm.imag, blank, neg_Cg])\n            ], \"csr\").T\n\n            # Compute partials of flows w.r.t V.\n            if self.flow_lim == IFLOW:\n                dFf_dVa, dFf_dVm, dFt_dVa, dFt_dVm, Ff, Ft = \\\n                    case.dIbr_dV(Yf, Yt, V)\n            else:\n                dFf_dVa, dFf_dVm, dFt_dVa, dFt_dVm, Ff, Ft = \\\n                    case.dSbr_dV(Yf, Yt, V, bs, ln)\n            if self.flow_lim == PFLOW:\n                dFf_dVa = dFf_dVa.real\n                dFf_dVm = dFf_dVm.real\n                dFt_dVa = dFt_dVa.real\n                dFt_dVm = dFt_dVm.real\n                Ff = Ff.real\n                Ft = Ft.real\n\n            # Squared magnitude of flow (complex power, current or real power).\n            df_dVa, df_dVm, dt_dVa, dt_dVm = \\\n                case.dAbr_dV(dFf_dVa, dFf_dVm, dFt_dVa, dFt_dVm, Ff, Ft)\n\n            # Construct Jacobian of inequality constraints (branch limits) and\n            # transpose it.\n            dh = lil_matrix((nxyz, 2 * nl))\n            dh[r_[iVa, iVm].T, :] = vstack([hstack([df_dVa, df_dVm]),\n                                            hstack([dt_dVa, dt_dVm])], \"csr\").T\n\n            J = vstack([dg, dh, A]).tocoo()\n\n            if flag:\n                return (J.row, J.col)\n            else:\n                return J.data\n\n\n        def h_fcn(x, lagrange, obj_factor, flag, usr_data=None):\n            \"\"\" Evaluates the Hessian of the Lagrangian.\n            \"\"\"\n            neqnln = 2 * nb\n            niqnln = 2 * len(il) # no. of lines with constraints\n\n            Pgen = x[Pg.i1:Pg.iN + 1] # Active generation in p.u.\n            Qgen = x[Qg.i1:Qg.iN + 1] # Reactive generation in p.u.\n\n            for i, g in enumerate(gn):\n                g.p = Pgen[i] * base_mva # active generation in MW\n                g.q = Qgen[i] * base_mva # reactive generation in MVAr\n\n            Vang = x[Va.i1:Va.iN + 1]\n            Vmag = x[Vm.i1:Vm.iN + 1]\n            V = Vmag * exp(1j * Vang)\n            nxtra = nxyz - 2 * nb\n\n            #------------------------------------------------------------------\n            #  Evaluate d2f.\n            #------------------------------------------------------------------\n\n            d2f_dPg2 = lil_matrix((ng, 1)) # w.r.t p.u. Pg\n            d2f_dQg2 = lil_matrix((ng, 1)) # w.r.t p.u. Qg]\n\n            for i in ipol:\n                d2f_dPg2[i, 0] = polyval(polyder(list(gn[i].p_cost), 2),\n                                         Pg.v0[i] * base_mva) * base_mva**2\n#            for i in ipol:\n#                d2f_dQg2[i] = polyval(polyder(list(gn[i].p_cost), 2),\n#                                      Qg.v0[i] * base_mva) * base_mva**2\n\n            i = r_[range(Pg.i1, Pg.iN + 1), range(Qg.i1, Qg.iN + 1)]\n\n            d2f = csr_matrix((vstack([d2f_dPg2, d2f_dQg2]).toarray().flatten(),\n                              (i, i)), shape=(nxyz, nxyz))\n            # TODO: Generalised cost model.\n            d2f = d2f * self.opt[\"cost_mult\"]\n\n            #------------------------------------------------------------------\n            #  Evaluate Hessian of power balance constraints.\n            #------------------------------------------------------------------\n\n            eqnonlin = lagrange[:neqnln]\n#            nlam = len(lagrange[\"eqnonlin\"]) / 2\n            nlam = len(eqnonlin) / 2\n            lamP = eqnonlin[:nlam]\n            lamQ = eqnonlin[nlam:nlam + nlam]\n            Gpaa, Gpav, Gpva, Gpvv = case.d2Sbus_dV2(Ybus, V, lamP)\n            Gqaa, Gqav, Gqva, Gqvv = case.d2Sbus_dV2(Ybus, V, lamQ)\n\n            d2G = vstack([\n                hstack([\n                    vstack([hstack([Gpaa, Gpav]),\n                            hstack([Gpva, Gpvv])]).real +\n                    vstack([hstack([Gqaa, Gqav]),\n                            hstack([Gqva, Gqvv])]).imag,\n                    csr_matrix((2 * nb, nxtra))]),\n                hstack([\n                    csr_matrix((nxtra, 2 * nb)),\n                    csr_matrix((nxtra, nxtra))\n                ])\n            ], \"csr\")\n\n            #------------------------------------------------------------------\n            #  Evaluate Hessian of flow constraints.\n            #------------------------------------------------------------------\n\n            ineqnonlin = lagrange[neqnln:neqnln + niqnln]\n            nmu = len(ineqnonlin) / 2\n            muF = ineqnonlin[:nmu]\n            muT = ineqnonlin[nmu:nmu + nmu]\n            if self.flow_lim == \"I\":\n                dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It = \\\n                    case.dIbr_dV(Yf, Yt, V)\n                Hfaa, Hfav, Hfva, Hfvv = \\\n                    case.d2AIbr_dV2(dIf_dVa, dIf_dVm, If, Yf, V, muF)\n                Htaa, Htav, Htva, Htvv = \\\n                    case.d2AIbr_dV2(dIt_dVa, dIt_dVm, It, Yt, V, muT)\n            else:\n                f = [e.from_bus._i for e in ln]\n                t = [e.to_bus._i for e in ln]\n                # Line-bus connection matrices.\n                Cf = csr_matrix((ones(nl), (range(nl), f)), (nl, nb))\n                Ct = csr_matrix((ones(nl), (range(nl), t)), (nl, nb))\n                dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St = \\\n                    case.dSbr_dV(Yf, Yt, V)\n                if self.flow_lim == PFLOW:\n                    Hfaa, Hfav, Hfva, Hfvv = \\\n                        case.d2ASbr_dV2(dSf_dVa.real(), dSf_dVm.real(),\n                                        Sf.real(), Cf, Yf, V, muF)\n                    Htaa, Htav, Htva, Htvv = \\\n                        case.d2ASbr_dV2(dSt_dVa.real(), dSt_dVm.real(),\n                                        St.real(), Ct, Yt, V, muT)\n                elif self.flow_lim == SFLOW:\n                    Hfaa, Hfav, Hfva, Hfvv = \\\n                        case.d2ASbr_dV2(dSf_dVa, dSf_dVm, Sf, Cf, Yf, V, muF)\n                    Htaa, Htav, Htva, Htvv = \\\n                        case.d2ASbr_dV2(dSt_dVa, dSt_dVm, St, Ct, Yt, V, muT)\n                else:\n                    raise ValueError\n\n            d2H = vstack([\n                hstack([\n                    vstack([hstack([Hfaa, Hfav]),\n                            hstack([Hfva, Hfvv])]) +\n                    vstack([hstack([Htaa, Htav]),\n                            hstack([Htva, Htvv])]),\n                    csr_matrix((2 * nb, nxtra))\n                ]),\n                hstack([\n                    csr_matrix((nxtra, 2 * nb)),\n                    csr_matrix((nxtra, nxtra))\n                ])\n            ], \"csr\")\n\n            H = d2f + d2G + d2H\n\n            if flag:\n                return (H.row, H.col)\n            else:\n                return H.data\n\n        n = len(x0) # the number of variables\n        gl = r_[zeros(2 * nb), -Inf * ones(2 * nl2), l]\n        gu = r_[zeros(2 * nb),       zeros(2 * nl2), u]\n        m = len(gl) # the number of constraints\n        nnzj = 0 # the number of nonzeros in Jacobian matrix\n        nnzh = 0 # the number of non-zeros in Hessian matrix\n\n        nlp = pyipopt.create(n, xmin, xmax, m, gl, gu, nnzj, nnzh,\n                             f_fcn, df_fcn, g_fcn, dg_fcn, h_fcn)\n\n#        x, zl, zu, obj = nlp.solve(x0)\n        success = nlp.solve(x0)\n        nlp.close()\n\n        print \"Success:\", success\n        print \"Solution of the primal variables, x\"\n#        print x\n        print \"Solution of the bound multipliers, z_L and z_U\"\n#        print zl, zu\n        print \"Objective value\"", "response": "Solves the AC optimal power flow."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrun a power flow on a complex bus and returns a dictionary with the keys V f S f and S g.", "response": "def solve(self):\n        \"\"\" Runs a power flow\n\n        @rtype: dict\n        @return: Solution dictionary with the following keys:\n                   - C{V} - final complex voltages\n                   - C{converged} - boolean value indicating if the solver\n                     converged or not\n                   - C{iterations} - the number of iterations performed\n        \"\"\"\n        # Zero result attributes.\n        self.case.reset()\n\n        # Retrieve the contents of the case.\n        b, l, g, _, _, _, _ = self._unpack_case(self.case)\n\n        # Update bus indexes.\n        self.case.index_buses(b)\n\n        # Index buses accoding to type.\n#        try:\n#            _, pq, pv, pvpq = self._index_buses(b)\n#        except SlackBusError:\n#            logger.error(\"Swing bus required for DCPF.\")\n#            return {\"converged\": False}\n\n        refs, pq, pv, pvpq = self._index_buses(b)\n        if len(refs) != 1:\n            logger.error(\"Swing bus required for DCPF.\")\n            return {\"converged\": False}\n\n        # Start the clock.\n        t0 = time()\n\n        # Build the vector of initial complex bus voltages.\n        V0 = self._initial_voltage(b, g)\n\n        # Save index and angle of original reference bus.\n#        if self.qlimit:\n#            ref0 = ref\n#            Varef0 = b[ref0].Va\n#            # List of buses at Q limits.\n#            limits = []\n#            # Qg of generators at Q limits.\n#            fixedQg = matrix(0.0, (g.size[0], 1))\n\n        repeat = True\n        while repeat:\n            # Build admittance matrices.\n            Ybus, Yf, Yt = self.case.getYbus(b, l)\n\n            # Compute complex bus power injections (generation - load).\n            Sbus = self.case.getSbus(b)\n\n            # Run the power flow.\n            V, converged, i = self._run_power_flow(Ybus, Sbus, V0, pv, pq, pvpq)\n\n            # Update case with solution.\n            self.case.pf_solution(Ybus, Yf, Yt, V)\n\n            # Enforce generator Q limits.\n            if self.qlimit:\n                raise NotImplementedError\n            else:\n                repeat = False\n\n        elapsed = time() - t0\n\n        if converged and self.verbose:\n            logger.info(\"AC power flow converged in %.3fs\" % elapsed)\n\n        return {\"converged\": converged, \"elapsed\": elapsed, \"iterations\": i,\n                \"V\":V}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unpack_case(self, case):\n        base_mva = case.base_mva\n        b = case.connected_buses\n        l = case.online_branches\n        g = case.online_generators\n        nb = len(b)\n        nl = len(l)\n        ng = len(g)\n\n        return b, l, g, nb, nl, ng, base_mva", "response": "Unpacks the contents of the OPF.\n        into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _index_buses(self, buses):\n        refs = [bus._i for bus in buses if bus.type == REFERENCE]\n#        if len(refs) != 1:\n#            raise SlackBusError\n        pv = [bus._i for bus in buses if bus.type == PV]\n        pq = [bus._i for bus in buses if bus.type == PQ]\n        pvpq = pv + pq\n\n        return refs, pq, pv, pvpq", "response": "Index the given buses for updating v."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _initial_voltage(self, buses, generators):\n        Vm = array([bus.v_magnitude for bus in buses])\n\n        # Initial bus voltage angles in radians.\n        Va = array([bus.v_angle * (pi / 180.0) for bus in buses])\n\n        V = Vm * exp(1j * Va)\n\n        # Get generator set points.\n        for g in generators:\n            i = g.bus._i\n            V[i] = g.v_magnitude / abs(V[i]) * V[i]\n\n        return V", "response": "Returns the initial vector of complex bus voltages."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _run_power_flow(self, Ybus, Sbus, V, pv, pq, pvpq, **kw_args):\n        Va = angle(V)\n        Vm = abs(V)\n\n        # Initial evaluation of F(x0)...\n        F = self._evaluate_function(Ybus, V, Sbus, pv, pq)\n        # ...and convergency check.\n        converged = self._check_convergence(F)\n\n        # Perform Newton iterations.\n        i = 0\n        while (not converged) and (i < self.iter_max):\n            V, Vm, Va = self._one_iteration(F, Ybus, V, Vm, Va, pv, pq, pvpq)\n            F = self._evaluate_function(Ybus, V, Sbus, pv, pq)\n            converged = self._check_convergence(F)\n            i += 1\n\n        if converged:\n            if self.verbose:\n                logger.info(\"Newton's method power flow converged in %d \"\n                            \"iterations.\" % i)\n        else:\n            logger.error(\"Newton's method power flow did not converge in %d \"\n                         \"iterations.\" % i)\n\n        return V, converged, i", "response": "Runs the power flow using a full Newton s method."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nperform one Newton iteration.", "response": "def _one_iteration(self, F, Ybus, V, Vm, Va, pv, pq, pvpq):\n        \"\"\" Performs one Newton iteration.\n        \"\"\"\n        J = self._build_jacobian(Ybus, V, pv, pq, pvpq)\n\n        # Update step.\n        dx = -1 * spsolve(J, F)\n#        dx = -1 * linalg.lstsq(J.todense(), F)[0]\n\n        # Update voltage vector.\n        npv = len(pv)\n        npq = len(pq)\n        if npv > 0:\n            Va[pv] = Va[pv] + dx[range(npv)]\n        if npq > 0:\n            Va[pq] = Va[pq] + dx[range(npv, npv + npq)]\n            Vm[pq] = Vm[pq] + dx[range(npv + npq, npv + npq + npq)]\n\n        V = Vm * exp(1j * Va)\n        Vm = abs(V) # Avoid wrapped round negative Vm.\n        Va = angle(V)\n\n        return V, Vm, Va"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_convergence(self, F):\n        normF = linalg.norm(F, Inf)\n\n        if normF < self.tolerance:\n            converged = True\n        else:\n            converged = False\n            if self.verbose:\n                logger.info(\"Difference: %.3f\" % (normF - self.tolerance))\n\n        return converged", "response": "Checks if the solution has converged to within the specified tolerance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding the Jacobian matrix for the current species.", "response": "def _build_jacobian(self, Ybus, V, pv, pq, pvpq):\n        \"\"\" Returns the Jacobian matrix.\n        \"\"\"\n        pq_col = [[i] for i in pq]\n        pvpq_col = [[i] for i in pvpq]\n\n        dS_dVm, dS_dVa = self.case.dSbus_dV(Ybus, V)\n\n        J11 = dS_dVa[pvpq_col, pvpq].real\n\n        J12 = dS_dVm[pvpq_col, pq].real\n        J21 = dS_dVa[pq_col, pvpq].imag\n        J22 = dS_dVm[pq_col, pq].imag\n\n        J = vstack([\n            hstack([J11, J12]),\n            hstack([J21, J22])\n        ], format=\"csr\")\n\n        return J"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun power flow using Newton s method.", "response": "def _run_power_flow(self, Ybus, Sbus, V, pv, pq, pvpq):\n        \"\"\" Solves the power flow using a full Newton's method.\n        \"\"\"\n        i = 0\n        Va = angle(V)\n        Vm = abs(V)\n\n        # FIXME: Do not repeat build for each Q limit loop.\n        Bp, Bpp = self.case.makeB(method=self.method)\n\n        # Evaluate initial mismatch.\n        P, Q = self._evaluate_mismatch(Ybus, V, Sbus, pq, pvpq)\n\n        if self.verbose:\n            logger.info(\"iteration     max mismatch (p.u.)  \\n\")\n            logger.info(\"type   #        P            Q     \\n\")\n            logger.info(\"---- ----  -----------  -----------\\n\")\n\n        # Check tolerance.\n        converged = self._check_convergence(P, Q, i, \"P\")\n\n        if converged and self.verbose:\n            logger.info(\"Converged!\")\n\n        # Reduce B matrices.\n        pq_col = [[k] for k in pq]\n        pvpq_col = [[k] for k in pvpq]\n        Bp = Bp[pvpq_col, pvpq].tocsc() # splu requires a CSC matrix\n        Bpp = Bpp[pq_col, pq].tocsc()\n\n        # Factor B matrices.\n        Bp_solver = splu(Bp)\n        Bpp_solver = splu(Bpp)\n#        L = decomp.lu(Bp.todense())\n#        LU, P = decomp.lu_factor(Bp.todense())\n\n        # Perform Newton iterations.\n        while (not converged) and (i < self.iter_max):\n            i += 1\n            # Perform P iteration, update Va.\n            V, Vm, Va = self._p_iteration(P, Bp_solver, Vm, Va, pvpq)\n\n            # Evalute mismatch.\n            P, Q = self._evaluate_mismatch(Ybus, V, Sbus, pq, pvpq)\n            # Check tolerance.\n            converged = self._check_convergence(P, Q, i, \"P\")\n\n            if self.verbose and converged:\n                logger.info(\"Fast-decoupled power flow converged in %d \"\n                    \"P-iterations and %d Q-iterations.\" % (i, i - 1))\n                break\n\n            # Perform Q iteration, update Vm.\n            V, Vm, Va = self._q_iteration(Q, Bpp_solver, Vm, Va, pq)\n\n            # Evalute mismatch.\n            P, Q = self._evaluate_mismatch(Ybus, V, Sbus, pq, pvpq)\n            # Check tolerance.\n            converged = self._check_convergence(P, Q, i, \"Q\")\n\n            if self.verbose and converged:\n                logger.info(\"Fast-decoupled power flow converged in %d \"\n                    \"P-iterations and %d Q-iterations.\" % (i, i))\n                break\n\n        if self.verbose and not converged:\n            logger.error(\"FDPF did not converge in %d iterations.\" % i)\n\n        return V, converged, i"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_convergence(self, P, Q, i, type):\n        normP = linalg.norm(P, Inf)\n        normQ = linalg.norm(Q, Inf)\n\n        if self.verbose:\n            logger.info(\"  %s  %3d   %10.3e   %10.3e\" % (type,i, normP, normQ))\n\n        if (normP < self.tolerance) and (normQ < self.tolerance):\n            converged = True\n        else:\n            converged = False\n\n        return converged", "response": "Checks if the solution has converged to within the specified tolerance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _p_iteration(self, P, Bp_solver, Vm, Va, pvpq):\n        dVa = -Bp_solver.solve(P)\n\n        # Update voltage.\n        Va[pvpq] = Va[pvpq] + dVa\n        V = Vm * exp(1j * Va)\n\n        return V, Vm, Va", "response": "Performs a P iteration updates Va."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _q_iteration(self, Q, Bpp_solver, Vm, Va, pq):\n        dVm = -Bpp_solver.solve(Q)\n\n        # Update voltage.\n        Vm[pq] = Vm[pq] + dVm\n        V = Vm * exp(1j * Va)\n\n        return V, Vm, Va", "response": "Performs a Q iteration updates Vm."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfunction to generate a signal with sinusoidal frequency modulation.", "response": "def fmsin(N, fnormin=0.05, fnormax=0.45, period=None, t0=None, fnorm0=0.25, pm1=1):\n    \"\"\"\n    Signal with sinusoidal frequency modulation.\n\n    generates a frequency modulation with a sinusoidal frequency.\n    This sinusoidal modulation is designed such that the instantaneous\n    frequency at time T0 is equal to FNORM0, and the ambiguity between\n    increasing or decreasing frequency is solved by PM1.\n\n    N       : number of points.\n    FNORMIN : smallest normalized frequency          (default: 0.05)\n    FNORMAX : highest normalized frequency           (default: 0.45)\n    PERIOD  : period of the sinusoidal fm            (default: N   )\n    T0      : time reference for the phase           (default: N/2 )\n    FNORM0  : normalized frequency at time T0        (default: 0.25)\n    PM1     : frequency direction at T0 (-1 or +1)       (default: +1  )\n\n    Returns:\n    Y       : signal\n    IFLAW   : its instantaneous frequency law\n\n    Example:\n    z,i=fmsin(140,0.05,0.45,100,20,0.3,-1.0)\n\n    Original MATLAB code F. Auger, July 1995.\n    (note: Licensed under GPL; see main LICENSE file)\n    \"\"\"\n\n    if period==None:\n\tperiod = N\n    if t0==None:\n\tt0 = N/2\n    pm1 = nx.sign(pm1)\n\n    fnormid=0.5*(fnormax+fnormin);\n    delta  =0.5*(fnormax-fnormin);\n    phi    =-pm1*nx.arccos((fnorm0-fnormid)/delta);\n    time   =nx.arange(1,N)-t0;\n    phase  =2*nx.pi*fnormid*time+delta*period*(nx.sin(2*nx.pi*time/period+phi)-nx.sin(phi));\n    y      =nx.exp(1j*phase)\n    iflaw  =fnormid+delta*nx.cos(2*nx.pi*time/period+phi);\n\n    return y,iflaw"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert XY point from Spherical Mercator EPSG : 900913 to lat / lon in WGS84 Datum", "response": "def metres2latlon(mx, my, origin_shift= 2 * pi * 6378137 / 2.0):\n    \"\"\"Converts XY point from Spherical Mercator EPSG:900913 to lat/lon in\n    WGS84 Datum\"\"\"\n    lon = (mx / origin_shift) * 180.0\n    lat = (my / origin_shift) * 180.0\n\n    lat = 180 / pi * (2 * atan( exp( lat * pi / 180.0)) - pi / 2.0)\n    return lat, lon"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_rdf(self, file):\n        store = Graph()\n        store.parse(file)\n\n        print len(store)", "response": "Returns a case from the given file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload and installed metrics plugins.", "response": "def load_plugins(group='metrics.plugin.10'):\n    \"\"\"Load and installed metrics plugins.\n    \"\"\"\n    # on using entrypoints:\n    # http://stackoverflow.com/questions/774824/explain-python-entry-points\n    file_processors = []\n    build_processors = []\n    for ep in pkg_resources.iter_entry_points(group, name=None):\n        log.debug('loading \\'%s\\'', ep)\n        plugin = ep.load()  # load the plugin\n        if hasattr(plugin, 'get_file_processors'):\n            file_processors.extend(plugin.get_file_processors())\n        if hasattr(plugin, 'get_build_processors'):\n            build_processors.extend(plugin.get_build_processors())\n    return file_processors, build_processors"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump(obj, fp, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, **kw):\n    \"\"\"\n    Serialize ``obj`` as a JSON formatted stream to ``fp`` (a\n    ``.write()``-supporting file-like object).\n\n    If ``skipkeys`` is ``True`` then ``dict`` keys that are not basic types\n    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) \n    will be skipped instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is ``False``, then the some chunks written to ``fp``\n    may be ``unicode`` instances, subject to normal Python ``str`` to\n    ``unicode`` coercion rules.  Unless ``fp.write()`` explicitly\n    understands ``unicode`` (as in ``codecs.getwriter()``) this is likely\n    to cause an error.\n\n    If ``check_circular`` is ``False``, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is ``False``, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``)\n    in strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and object\n    members will be pretty-printed with that indent level.  An indent level\n    of 0 will only insert newlines.  ``None`` is the most compact representation.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg.\n    \"\"\"\n    if cls is None:\n        cls = JSONEncoder\n    iterable = cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n        check_circular=check_circular, allow_nan=allow_nan, indent=indent,\n        **kw).iterencode(obj)\n    # could accelerate with writelines in some versions of Python, at\n    # a debuggability cost\n    for chunk in iterable:\n        fp.write(chunk)", "response": "Serialize obj as a JSON formatted stream to fp."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef dumps(obj, skipkeys=False, ensure_ascii=True, check_circular=True,\n        allow_nan=True, cls=None, indent=None, **kw):\n    \"\"\"\n    Serialize ``obj`` to a JSON formatted ``str``.\n\n    If ``skipkeys`` is ``True`` then ``dict`` keys that are not basic types\n    (``str``, ``unicode``, ``int``, ``long``, ``float``, ``bool``, ``None``) \n    will be skipped instead of raising a ``TypeError``.\n\n    If ``ensure_ascii`` is ``False``, then the return value will be a\n    ``unicode`` instance subject to normal Python ``str`` to ``unicode``\n    coercion rules instead of being escaped to an ASCII ``str``.\n\n    If ``check_circular`` is ``False``, then the circular reference check\n    for container types will be skipped and a circular reference will\n    result in an ``OverflowError`` (or worse).\n\n    If ``allow_nan`` is ``False``, then it will be a ``ValueError`` to\n    serialize out of range ``float`` values (``nan``, ``inf``, ``-inf``) in\n    strict compliance of the JSON specification, instead of using the\n    JavaScript equivalents (``NaN``, ``Infinity``, ``-Infinity``).\n\n    If ``indent`` is a non-negative integer, then JSON array elements and object\n    members will be pretty-printed with that indent level.  An indent level\n    of 0 will only insert newlines.  ``None`` is the most compact representation.\n\n    To use a custom ``JSONEncoder`` subclass (e.g. one that overrides the\n    ``.default()`` method to serialize additional types), specify it with\n    the ``cls`` kwarg.\n    \"\"\"\n    if cls is None:\n        cls = JSONEncoder\n    return cls(skipkeys=skipkeys, ensure_ascii=ensure_ascii,\n        check_circular=check_circular, allow_nan=allow_nan, indent=indent, **kw).encode(obj)", "response": "Serialize obj to a JSON formatted string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load(fp, encoding=None, cls=None, object_hook=None, **kw):\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    return cls(encoding=encoding, **kw).decode(fp.read())", "response": "Deserialize a JSON object from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef loads(s, encoding=None, cls=None, object_hook=None, **kw):\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    return cls(encoding=encoding, **kw).decode(s)", "response": "Deserialize a JSON object from a string or unicode instance containing a JSON\n    document."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads a single case from the given input file object.", "response": "def read_case(input, format=None):\n    \"\"\" Returns a case object from the given input file object. The data\n    format may be optionally specified.\n    \"\"\"\n    # Map of data file types to readers.\n    format_map = {\"matpower\": MATPOWERReader,\n        \"psse\": PSSEReader, \"pickle\": PickleReader}\n\n    # Read case data.\n    if format_map.has_key(format):\n        reader_klass = format_map[format]\n        reader = reader_klass()\n        case = reader.read(input)\n    else:\n        # Try each of the readers at random.\n        for reader_klass in format_map.values():\n            reader = reader_klass()\n            try:\n                case = reader.read(input)\n                if case is not None:\n                    break\n            except:\n                pass\n        else:\n            case = None\n\n    return case"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef detect_data_file(input, file_name=\"\"):\n    _, ext = os.path.splitext(file_name)\n\n    if ext == \".m\":\n        line = input.readline() # first line\n        if line.startswith(\"function\"):\n            type = \"matpower\"\n            logger.info(\"Recognised MATPOWER data file.\")\n        elif line.startswith(\"Bus.con\" or line.startswith(\"%\")):\n            type = \"psat\"\n            logger.info(\"Recognised PSAT data file.\")\n        else:\n            type = \"unrecognised\"\n        input.seek(0) # reset buffer for parsing\n\n    elif (ext == \".raw\") or (ext == \".psse\"):\n        type = \"psse\"\n        logger.info(\"Recognised PSS/E data file.\")\n\n    elif (ext == \".pkl\") or (ext == \".pickle\"):\n        type = \"pickle\"\n        logger.info(\"Recognised pickled case.\")\n\n    else:\n        type = None\n\n    return type", "response": "Detects the format of a network data file according to the file extension and the header."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef main():\n    parser = optparse.OptionParser(usage=\"usage: pylon [options] input_file\",\n                                   version=\"%prog 0.4.4\")\n\n    parser.add_option(\"-o\", \"--output\", dest=\"output\", metavar=\"FILE\",\n        help=\"Write the solution report to FILE.\")\n\n#    parser.add_option(\"-q\", \"--quiet\", action=\"store_true\", dest=\"quiet\",\n#        default=False, help=\"Print less information.\")\n\n    parser.add_option(\"-v\", \"--verbose\", action=\"store_true\", dest=\"verbose\",\n        default=False, help=\"Print more information.\")\n\n#    parser.add_option(\"-g\", \"--gui\", action=\"store_true\", dest=\"gui\",\n#        default=False, help=\"Use the portable graphical interface to Pylon.\")\n\n#    parser.add_option(\"-n\", \"--no-report\", action=\"store_true\",\n#        dest=\"no_report\", default=False, help=\"Suppress report output.\")\n\n    parser.add_option(\"-d\", \"--debug\", action=\"store_true\", dest=\"debug\",\n        default=False, help=\"Print debug information.\")\n\n    parser.add_option(\"-t\", \"--input-type\", dest=\"type\", metavar=\"TYPE\",\n        default=\"any\", help=\"The argument following the -t is used to \"\n        \"indicate the format type of the input data file. The types which are \"\n        \"currently supported include: matpower, psse [default: %default]\"\n        \" If not specified Pylon will try to determine the type according to \"\n        \"the file name extension and the file header.\")\n\n    parser.add_option(\"-s\", \"--solver\", dest=\"solver\", metavar=\"SOLVER\",\n        default=\"acpf\", help=\"The argument following the -s is used to\"\n        \"indicate the type of routine to use in solving. The types which are \"\n        \"currently supported are: 'dcpf', 'acpf', 'dcopf', 'acopf', 'udopf' \"\n        \"and 'none' [default: %default].\")\n\n    parser.add_option(\"-a\", \"--algorithm\", action=\"store_true\",\n        metavar=\"ALGORITHM\", dest=\"algorithm\", default=\"newton\",\n        help=\"Indicates the algorithm type to be used for AC power flow. The \"\n        \"types which are currently supported are: 'newton' and 'fdpf' \"\n        \"[default: %default].\")\n\n    parser.add_option(\"-T\", \"--output-type\", dest=\"output_type\",\n        metavar=\"OUTPUT_TYPE\", default=\"rst\", help=\"Indicates the output \"\n        \"format type.  The type swhich are currently supported include: rst, \"\n        \"matpower, csv, excel and none [default: %default].\")\n\n    (options, args) = parser.parse_args()\n\n    if options.verbose:\n        logger.setLevel(logging.INFO)\n    elif options.debug:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.ERROR)\n\n    # Output.\n    outext = {'psse': '.raw', 'matpower': '.m'}\n    if options.output:\n        if options.output == \"-\":\n            outfile = sys.stdout\n            logger.setLevel(logging.CRITICAL) # must stay quiet\n#            options.output_type = \"none\"\n        else:\n            outfile = open(options.output, \"wb\")\n    elif options.output_type is not None:\n        if options.output_type in outext.keys():\n            inname, ext = os.path.splitext(args[0])\n            outfile = inname + outext[options.output_type]\n        else:\n            outfile = sys.stdout\n    else:\n        outfile = sys.stdout\n#        if not options.no_report:\n#            logger.setLevel(logging.CRITICAL) # must stay quiet\n\n    # Input.\n    if len(args) > 1:\n        parser.print_help()\n        sys.exit(1)\n    elif (len(args) == 0) or (args[0] == \"-\"):\n        filename = \"\"\n        if sys.stdin.isatty():\n            # True if the file is connected to a tty device, and False\n            # otherwise (pipeline or file redirection).\n            parser.print_help()\n            sys.exit(1)\n        else:\n            # Handle piped input ($ cat ehv3.raw | pylon | rst2pdf -o ans.pdf).\n            infile = sys.stdin\n    else:\n        filename = args[0]\n        infile = open(filename, \"rb\")\n\n    if options.type == \"any\":\n        type = detect_data_file(infile, filename)\n    else:\n        type = options.type\n\n    # Get the case from the input file-like object.\n    case = read_case(infile, type)\n\n    if case is not None:\n        # Routine (and algorithm) selection.\n        if options.solver == \"dcpf\":\n            solver = DCPF(case)\n        elif options.solver == \"acpf\":\n            if options.algorithm == \"newton\":\n                solver = NewtonPF(case)\n            elif options.algorithm == \"fdpf\":\n                solver = FastDecoupledPF(case)\n            else:\n                logger.critical(\"Invalid algorithm [%s].\" % options.algorithm)\n                sys.exit(1)\n        elif options.solver == \"dcopf\":\n            solver = OPF(case, True)\n        elif options.solver == \"acopf\":\n            solver = OPF(case, False)\n        elif options.solver == \"udopf\":\n            solver = UDOPF(case)\n        elif options.solver == \"none\":\n            solver = None\n        else:\n            logger.critical(\"Invalid solver [%s].\" % options.solver)\n#            sys.exit(1)\n            solver = None\n\n        # Output writer selection.\n        if options.output_type == \"matpower\":\n            writer = MATPOWERWriter(case)\n        elif options.output_type == \"psse\":\n            writer = PSSEWriter(case)\n        elif options.output_type == \"rst\":\n            writer = ReSTWriter(case)\n        elif options.output_type == \"csv\":\n            from pylon.io.excel import CSVWriter\n            writer = CSVWriter(case)\n        elif options.output_type == \"excel\":\n            from pylon.io.excel import ExcelWriter\n            writer = ExcelWriter(case)\n        elif options.output_type == \"pickle\":\n            writer = PickleWriter(case)\n        else:\n            logger.critical(\"Invalid output type [%s].\" % options.output_type)\n            sys.exit(1)\n\n        if solver is not None:\n            solver.solve()\n        if options.output_type != \"none\":\n            writer.write(outfile)\n            print('Output file {0} written'.format(outfile))\n    else:\n        logger.critical(\"Unable to read case data.\")\n\n    # Don't close stdin or stdout.\n    if len(args) == 1:\n        infile.close()\n    if options.output and not (options.output == \"-\"):\n        outfile.close()", "response": "Parse the command line and call Pylon with the correct data."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, file_or_filename, prog=None, format='xdot'):\n        if prog is None:\n            file = super(DotWriter, self).write(file_or_filename)\n        else:\n            buf = StringIO.StringIO()\n            super(DotWriter, self).write(buf)\n            buf.seek(0)\n            data = self.create(buf.getvalue(), prog, format)\n\n            if isinstance(file_or_filename, basestring):\n                file = None\n                try:\n                    file = open(file_or_filename, \"wb\")\n                except:\n                    logger.error(\"Error opening %s.\" % file_or_filename)\n                finally:\n                    if file is not None:\n                        file.write(data)\n                        file.close()\n            else:\n                file = file_or_filename\n                file.write(data)\n\n        return file", "response": "Writes the case data in Graphviz DOT language."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_bus_data(self, file, padding=\"    \"):\n        for bus in self.case.buses:\n            attrs = ['%s=\"%s\"' % (k, v) for k, v in self.bus_attr.iteritems()]\n#            attrs.insert(0, 'label=\"%s\"' % bus.name)\n            attr_str = \", \".join(attrs)\n\n            file.write(\"%s%s [%s];\\n\" % (padding, bus.name, attr_str))", "response": "Writes the bus data to file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting branch data in Graphviz DOT language.", "response": "def write_branch_data(self, file, padding=\"    \"):\n        \"\"\" Writes branch data in Graphviz DOT language.\n        \"\"\"\n        attrs = ['%s=\"%s\"' % (k,v) for k,v in self.branch_attr.iteritems()]\n        attr_str = \", \".join(attrs)\n\n        for br in self.case.branches:\n            file.write(\"%s%s -> %s [%s];\\n\" % \\\n                (padding, br.from_bus.name, br.to_bus.name, attr_str))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite generator data in Graphviz DOT language.", "response": "def write_generator_data(self, file, padding=\"    \"):\n        \"\"\" Write generator data in Graphviz DOT language.\n        \"\"\"\n        attrs = ['%s=\"%s\"' % (k, v) for k, v in self.gen_attr.iteritems()]\n        attr_str = \", \".join(attrs)\n\n        edge_attrs = ['%s=\"%s\"' % (k,v) for k,v in {}.iteritems()]\n        edge_attr_str = \", \".join(edge_attrs)\n\n        for g in self.case.generators:\n            # Generator node.\n            file.write(\"%s%s [%s];\\n\" % (padding, g.name, attr_str))\n\n            # Edge connecting generator and bus.\n            file.write(\"%s%s -> %s [%s];\\n\" % \\\n                       (padding, g.name, g.bus.name, edge_attr_str))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, dotdata, prog=\"dot\", format=\"xdot\"):\n        import os, tempfile\n        from dot2tex.dotparsing import find_graphviz\n\n        # Map Graphviz executable names to their paths.\n        progs = find_graphviz()\n        if progs is None:\n            logger.warning(\"GraphViz executables not found.\")\n            return None\n        if not progs.has_key(prog):\n            logger.warning('Invalid program [%s]. Available programs are: %s' % \\\n                           (prog, progs.keys()))\n            return None\n\n        # Make a temporary file ...\n        tmp_fd, tmp_name = tempfile.mkstemp()\n        os.close(tmp_fd)\n        # ... and save the graph to it.\n        dot_fd = file(tmp_name, \"w+b\")\n        dot_fd.write(dotdata) # DOT language.\n        dot_fd.close()\n\n        # Get the temporary file directory name.\n        tmp_dir = os.path.dirname(tmp_name)\n\n        # Process the file using the layout program, specifying the format.\n        p = subprocess.Popen((progs[prog], '-T'+format, tmp_name),\n            cwd=tmp_dir, stderr=subprocess.PIPE, stdout=subprocess.PIPE)\n\n        stderr = p.stderr\n        stdout = p.stdout\n\n        # Make sense of the standard output form the process.\n        stdout_output = list()\n        while True:\n            data = stdout.read()\n            if not data:\n                break\n            stdout_output.append(data)\n        stdout.close()\n\n        if stdout_output:\n            stdout_output = ''.join(stdout_output)\n\n        # Similarly so for any standard error.\n        if not stderr.closed:\n            stderr_output = list()\n            while True:\n                data = stderr.read()\n                if not data:\n                    break\n                stderr_output.append(data)\n            stderr.close()\n\n            if stderr_output:\n                stderr_output = ''.join(stderr_output)\n\n        status = p.wait()\n\n        if status != 0 :\n            logger.error(\"Program [%s] terminated with status: %d. stderr \" \\\n                \"follows: %s\" % ( prog, status, stderr_output ) )\n        elif stderr_output:\n            logger.error( \"%s\", stderr_output )\n\n        # Remove the temporary file.\n        os.unlink(tmp_name)\n\n        return stdout_output", "response": "Creates and returns a representation of the graph using the Graphviz layout program given by prog."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format(file_metrics, build_metrics):\n    def indent(elem, level=0):\n        i = \"\\n\" + level*\"  \"\n        if len(elem):\n            if not elem.text or not elem.text.strip():\n                elem.text = i + \"  \"\n            if not elem.tail or not elem.tail.strip():\n                elem.tail = i\n            for elem in elem:\n                indent(elem, level+1)\n            if not elem.tail or not elem.tail.strip():\n                elem.tail = i\n        else:\n            if level and (not elem.tail or not elem.tail.strip()):\n                elem.tail = i\n\n    root = ET.Element('metrics')\n\n    # file_metrics\n    files = ET.Element('files')\n    root.append(files)\n\n    for key in file_metrics.keys():\n        tmp_file = ET.SubElement(files, \"file\",\n                                 {'name': key, 'language': file_metrics[key]['language']})\n        for name in file_metrics[key].keys():\n            if name == 'language':\n                continue\n            tmp_metric = ET.SubElement(tmp_file, \"metric\",\n                                       {'name': name, 'value': str(file_metrics[key][name])})\n\n    # build_metrics\n    if build_metrics:\n        build = ET.Element('build')\n        root.append(build)\n        # TODO\n\n    indent(root)\n    if PY3:\n        body = ET.tostring(root, encoding='unicode')\n    else:\n        body = ET.tostring(root)\n    return body", "response": "compute output in XML format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pips(f_fcn, x0, A=None, l=None, u=None, xmin=None, xmax=None,\n         gh_fcn=None, hess_fcn=None, opt=None):\n    \"\"\"Primal-dual interior point method for NLP (non-linear programming).\n    Minimize a function F(X) beginning from a starting point M{x0}, subject to\n    optional linear and non-linear constraints and variable bounds::\n\n            min f(x)\n             x\n\n    subject to::\n\n            g(x) = 0            (non-linear equalities)\n            h(x) <= 0           (non-linear inequalities)\n            l <= A*x <= u       (linear constraints)\n            xmin <= x <= xmax   (variable bounds)\n\n    Note: The calling syntax is almost identical to that of FMINCON from\n    MathWorks' Optimization Toolbox. The main difference is that the linear\n    constraints are specified with C{A}, C{L}, C{U} instead of C{A}, C{B},\n    C{Aeq}, C{Beq}. The functions for evaluating the objective function,\n    constraints and Hessian are identical.\n\n    Example from U{http://en.wikipedia.org/wiki/Nonlinear_programming}:\n        >>> from numpy import array, r_, float64, dot\n        >>> from scipy.sparse import csr_matrix\n        >>> def f2(x):\n        ...     f = -x[0] * x[1] - x[1] * x[2]\n        ...     df = -r_[x[1], x[0] + x[2], x[1]]\n        ...     # actually not used since 'hess_fcn' is provided\n        ...     d2f = -array([[0, 1, 0], [1, 0, 1], [0, 1, 0]], float64)\n        ...     return f, df, d2f\n        >>> def gh2(x):\n        ...     h = dot(array([[1, -1, 1],\n        ...                    [1,  1, 1]]), x**2) + array([-2.0, -10.0])\n        ...     dh = 2 * csr_matrix(array([[ x[0], x[0]],\n        ...                                [-x[1], x[1]],\n        ...                                [ x[2], x[2]]]))\n        ...     g = array([])\n        ...     dg = None\n        ...     return h, g, dh, dg\n        >>> def hess2(x, lam):\n        ...     mu = lam[\"ineqnonlin\"]\n        ...     a = r_[dot(2 * array([1, 1]), mu), -1, 0]\n        ...     b = r_[-1, dot(2 * array([-1, 1]),mu),-1]\n        ...     c = r_[0, -1, dot(2 * array([1, 1]),mu)]\n        ...     Lxx = csr_matrix(array([a, b, c]))\n        ...     return Lxx\n        >>> x0 = array([1, 1, 0], float64)\n        >>> solution = pips(f2, x0, gh_fcn=gh2, hess_fcn=hess2)\n        >>> round(solution[\"f\"], 11) == -7.07106725919\n        True\n        >>> solution[\"output\"][\"iterations\"]\n        8\n\n    Ported by Richard Lincoln from the MATLAB Interior Point Solver (MIPS)\n    (v1.9) by Ray Zimmerman.  MIPS is distributed as part of the MATPOWER\n    project, developed at the Power System Engineering Research Center (PSERC),\n    Cornell. See U{http://www.pserc.cornell.edu/matpower/} for more info.\n    MIPS was ported by Ray Zimmerman from C code written by H. Wang for his\n    PhD dissertation:\n      - \"On the Computation and Application of Multi-period\n        Security-Constrained Optimal Power Flow for Real-time\n        Electricity Market Operations\", Cornell University, May 2007.\n\n    See also:\n      - H. Wang, C. E. Murillo-Sanchez, R. D. Zimmerman, R. J. Thomas,\n        \"On Computational Issues of Market-Based Optimal Power Flow\",\n        IEEE Transactions on Power Systems, Vol. 22, No. 3, Aug. 2007,\n        pp. 1185-1193.\n\n    All parameters are optional except C{f_fcn} and C{x0}.\n    @param f_fcn: Function that evaluates the objective function, its gradients\n                  and Hessian for a given value of M{x}. If there are\n                  non-linear constraints, the Hessian information is provided\n                  by the 'hess_fcn' argument and is not required here.\n    @type f_fcn: callable\n    @param x0: Starting value of optimization vector M{x}.\n    @type x0: array\n    @param A: Optional linear constraints.\n    @type A: csr_matrix\n    @param l: Optional linear constraints. Default values are M{-Inf}.\n    @type l: array\n    @param u: Optional linear constraints. Default values are M{Inf}.\n    @type u: array\n    @param xmin: Optional lower bounds on the M{x} variables, defaults are\n                 M{-Inf}.\n    @type xmin: array\n    @param xmax: Optional upper bounds on the M{x} variables, defaults are\n                 M{Inf}.\n    @type xmax: array\n    @param gh_fcn: Function that evaluates the optional non-linear constraints\n                   and their gradients for a given value of M{x}.\n    @type gh_fcn: callable\n    @param hess_fcn: Handle to function that computes the Hessian of the\n                     Lagrangian for given values of M{x}, M{lambda} and M{mu},\n                     where M{lambda} and M{mu} are the multipliers on the\n                     equality and inequality constraints, M{g} and M{h},\n                     respectively.\n    @type hess_fcn: callable\n    @param opt: optional options dictionary with the following keys, all of\n                which are also optional (default values shown in parentheses)\n                  - C{verbose} (False) - Controls level of progress output\n                    displayed\n                  - C{feastol} (1e-6) - termination tolerance for feasibility\n                    condition\n                  - C{gradtol} (1e-6) - termination tolerance for gradient\n                    condition\n                  - C{comptol} (1e-6) - termination tolerance for\n                    complementarity condition\n                  - C{costtol} (1e-6) - termination tolerance for cost\n                    condition\n                  - C{max_it} (150) - maximum number of iterations\n                  - C{step_control} (False) - set to True to enable step-size\n                    control\n                  - C{max_red} (20) - maximum number of step-size reductions if\n                    step-control is on\n                  - C{cost_mult} (1.0) - cost multiplier used to scale the\n                    objective function for improved conditioning. Note: The\n                    same value must also be passed to the Hessian evaluation\n                    function so that it can appropriately scale the objective\n                    function term in the Hessian of the Lagrangian.\n    @type opt: dict\n\n    @rtype: dict\n    @return: The solution dictionary has the following keys:\n               - C{x} - solution vector\n               - C{f} - final objective function value\n               - C{converged} - exit status\n                   - True = first order optimality conditions satisfied\n                   - False = maximum number of iterations reached\n                   - None = numerically failed\n               - C{output} - output dictionary with keys:\n                   - C{iterations} - number of iterations performed\n                   - C{hist} - dictionary of arrays with trajectories of the\n                     following: feascond, gradcond, compcond, costcond, gamma,\n                     stepsize, obj, alphap, alphad\n                   - C{message} - exit message\n               - C{lmbda} - dictionary containing the Langrange and Kuhn-Tucker\n                 multipliers on the constraints, with keys:\n                   - C{eqnonlin} - non-linear equality constraints\n                   - C{ineqnonlin} - non-linear inequality constraints\n                   - C{mu_l} - lower (left-hand) limit on linear constraints\n                   - C{mu_u} - upper (right-hand) limit on linear constraints\n                   - C{lower} - lower bound on optimization variables\n                   - C{upper} - upper bound on optimization variables\n\n    @license: Apache License version 2.0\n    \"\"\"\n    nx = x0.shape[0]                        # number of variables\n    nA = A.shape[0] if A is not None else 0 # number of original linear constr\n\n    # default argument values\n#    l = array([]) if A is None else l\n#    u = array([]) if A is None else u\n    l = -Inf * ones(nA) if l is None else l\n    u =  Inf * ones(nA) if u is None else u\n    xmin = -Inf * ones(x0.shape[0]) if xmin is None else xmin\n    xmax =  Inf * ones(x0.shape[0]) if xmax is None else xmax\n    if gh_fcn is None:\n        nonlinear = False\n        gn = array([])\n        hn = array([])\n    else:\n        nonlinear = True\n\n    opt = {} if opt is None else opt\n    # options\n    if not opt.has_key(\"feastol\"):\n        opt[\"feastol\"] = 1e-06\n    if not opt.has_key(\"gradtol\"):\n        opt[\"gradtol\"] = 1e-06\n    if not opt.has_key(\"comptol\"):\n        opt[\"comptol\"] = 1e-06\n    if not opt.has_key(\"costtol\"):\n        opt[\"costtol\"] = 1e-06\n    if not opt.has_key(\"max_it\"):\n        opt[\"max_it\"] = 150\n    if not opt.has_key(\"max_red\"):\n        opt[\"max_red\"] = 20\n    if not opt.has_key(\"step_control\"):\n        opt[\"step_control\"] = False\n    if not opt.has_key(\"cost_mult\"):\n        opt[\"cost_mult\"] = 1\n    if not opt.has_key(\"verbose\"):\n        opt[\"verbose\"] = False\n\n    # initialize history\n    hist = {}\n\n    # constants\n    xi = 0.99995\n    sigma = 0.1\n    z0 = 1\n    alpha_min = 1e-8\n#    rho_min = 0.95\n#    rho_max = 1.05\n    mu_threshold = 1e-5\n\n    # initialize\n    i = 0                       # iteration counter\n    converged = False           # flag\n    eflag = False               # exit flag\n\n    # add var limits to linear constraints\n    eyex = eye(nx, nx, format=\"csr\")\n    AA = eyex if A is None else vstack([eyex, A], \"csr\")\n    ll = r_[xmin, l]\n    uu = r_[xmax, u]\n\n    # split up linear constraints\n    ieq = flatnonzero( absolute(uu - ll) <= EPS )\n    igt = flatnonzero( (uu >=  1e10) & (ll > -1e10) )\n    ilt = flatnonzero( (ll <= -1e10) & (uu <  1e10) )\n    ibx = flatnonzero( (absolute(uu - ll) > EPS) & (uu < 1e10) & (ll > -1e10) )\n    # zero-sized sparse matrices unsupported\n    Ae = AA[ieq, :] if len(ieq) else None\n    if len(ilt) or len(igt) or len(ibx):\n        idxs = [(1, ilt), (-1, igt), (1, ibx), (-1, ibx)]\n        Ai = vstack([sig * AA[idx, :] for sig, idx in idxs if len(idx)])\n    else:\n        Ai = None\n    be = uu[ieq, :]\n    bi = r_[uu[ilt], -ll[igt], uu[ibx], -ll[ibx]]\n\n    # evaluate cost f(x0) and constraints g(x0), h(x0)\n    x = x0\n    f, df, _ = f_fcn(x)                 # cost\n    f = f * opt[\"cost_mult\"]\n    df = df * opt[\"cost_mult\"]\n    if nonlinear:\n        hn, gn, dhn, dgn = gh_fcn(x)        # non-linear constraints\n        h = hn if Ai is None else r_[hn, Ai * x - bi] # inequality constraints\n        g = gn if Ae is None else r_[gn, Ae * x - be] # equality constraints\n\n        if (dhn is None) and (Ai is None):\n            dh = None\n        elif dhn is None:\n            dh = Ai.T\n        elif Ae is None:\n            dh = dhn\n        else:\n            dh = hstack([dhn, Ai.T])\n\n        if (dgn is None) and (Ae is None):\n            dg = None\n        elif dgn is None:\n            dg = Ae.T\n        elif Ae is None:\n            dg = dgn\n        else:\n            dg = hstack([dgn, Ae.T])\n    else:\n        h = -bi if Ai is None else Ai * x - bi        # inequality constraints\n        g = -be if Ae is None else Ae * x - be        # equality constraints\n        dh = None if Ai is None else Ai.T     # 1st derivative of inequalities\n        dg = None if Ae is None else Ae.T     # 1st derivative of equalities\n\n    # some dimensions\n    neq = g.shape[0]           # number of equality constraints\n    niq = h.shape[0]           # number of inequality constraints\n    neqnln = gn.shape[0]       # number of non-linear equality constraints\n    niqnln = hn.shape[0]       # number of non-linear inequality constraints\n    nlt = len(ilt)             # number of upper bounded linear inequalities\n    ngt = len(igt)             # number of lower bounded linear inequalities\n    nbx = len(ibx)             # number of doubly bounded linear inequalities\n\n    # initialize gamma, lam, mu, z, e\n    gamma = 1                  # barrier coefficient\n    lam = zeros(neq)\n    z = z0 * ones(niq)\n    mu = z0 * ones(niq)\n    k = flatnonzero(h < -z0)\n    z[k] = -h[k]\n    k = flatnonzero((gamma / z) > z0)\n    mu[k] = gamma / z[k]\n    e = ones(niq)\n\n    # check tolerance\n    f0 = f\n#    if opt[\"step_control\"]:\n#        L = f + lam.T * g + mu.T * (h + z) - gamma * sum(log(z))\n\n    Lx = df\n    Lx = Lx + dg * lam if dg is not None else Lx\n    Lx = Lx + dh * mu  if dh is not None else Lx\n\n    gnorm = norm(g, Inf) if len(g) else 0.0\n    lam_norm = norm(lam, Inf) if len(lam) else 0.0\n    mu_norm = norm(mu, Inf) if len(mu) else 0.0\n    feascond = \\\n        max([gnorm, max(h)]) / (1 + max([norm(x, Inf), norm(z, Inf)]))\n    gradcond = \\\n        norm(Lx, Inf) / (1 + max([lam_norm, mu_norm]))\n    compcond = dot(z, mu) / (1 + norm(x, Inf))\n    costcond = absolute(f - f0) / (1 + absolute(f0))\n\n    # save history\n    hist[i] = {'feascond': feascond, 'gradcond': gradcond,\n        'compcond': compcond, 'costcond': costcond, 'gamma': gamma,\n        'stepsize': 0, 'obj': f / opt[\"cost_mult\"], 'alphap': 0, 'alphad': 0}\n\n    if opt[\"verbose\"]:\n#        s = '-sc' if opt[\"step_control\"] else ''\n#        version, date = '1.0b2', '24-Mar-2010'\n#        print 'Python Interior Point Solver - PIPS%s, Version %s, %s' % \\\n#                    (s, version, date)\n        print \" it    objective   step size   feascond     gradcond     \" \\\n              \"compcond     costcond  \"\n        print \"----  ------------ --------- ------------ ------------ \" \\\n              \"------------ ------------\"\n        print \"%3d  %12.8g %10s %12g %12g %12g %12g\" % \\\n            (i, (f / opt[\"cost_mult\"]), \"\",\n             feascond, gradcond, compcond, costcond)\n\n    if feascond < opt[\"feastol\"] and gradcond < opt[\"gradtol\"] and \\\n        compcond < opt[\"comptol\"] and costcond < opt[\"costtol\"]:\n        converged = True\n        if opt[\"verbose\"]:\n            print \"Converged!\"\n\n    # do Newton iterations\n    while (not converged and i < opt[\"max_it\"]):\n        # update iteration counter\n        i += 1\n\n        # compute update step\n        lmbda = {\"eqnonlin\": lam[range(neqnln)],\n                 \"ineqnonlin\": mu[range(niqnln)]}\n        if nonlinear:\n            if hess_fcn is None:\n                print \"pips: Hessian evaluation via finite differences \" \\\n                      \"not yet implemented.\\nPlease provide \" \\\n                      \"your own hessian evaluation function.\"\n            Lxx = hess_fcn(x, lmbda)\n        else:\n            _, _, d2f = f_fcn(x)      # cost\n            Lxx = d2f * opt[\"cost_mult\"]\n        rz = range(len(z))\n        zinvdiag = csr_matrix((1.0 / z, (rz, rz))) if len(z) else None\n        rmu = range(len(mu))\n        mudiag = csr_matrix((mu, (rmu, rmu))) if len(mu) else None\n        dh_zinv = None if dh is None else dh * zinvdiag\n        M = Lxx if dh is None else Lxx + dh_zinv * mudiag * dh.T\n        N = Lx if dh is None else Lx + dh_zinv * (mudiag * h + gamma * e)\n\n        Ab = M if dg is None else vstack([\n            hstack([M, dg]),\n            hstack([dg.T, csr_matrix((neq, neq))])\n        ])\n        bb = r_[-N, -g]\n\n        dxdlam = spsolve(Ab.tocsr(), bb)\n\n        dx = dxdlam[:nx]\n        dlam = dxdlam[nx:nx + neq]\n        dz = -h - z if dh is None else -h - z - dh.T * dx\n        dmu = -mu if dh is None else -mu + zinvdiag * (gamma * e - mudiag * dz)\n\n        # optional step-size control\n#        sc = False\n        if opt[\"step_control\"]:\n            raise NotImplementedError\n#            x1 = x + dx\n#\n#            # evaluate cost, constraints, derivatives at x1\n#            f1, df1 = ipm_f(x1)          # cost\n#            f1 = f1 * opt[\"cost_mult\"]\n#            df1 = df1 * opt[\"cost_mult\"]\n#            gn1, hn1, dgn1, dhn1 = ipm_gh(x1) # non-linear constraints\n#            g1 = gn1 if Ai is None else r_[gn1, Ai * x1 - bi] # ieq constraints\n#            h1 = hn1 if Ae is None else r_[hn1, Ae * x1 - be] # eq constraints\n#            dg1 = dgn1 if Ai is None else r_[dgn1, Ai.T]      # 1st der of ieq\n#            dh1 = dhn1 if Ae is None else r_[dhn1, Ae.T]      # 1st der of eqs\n#\n#            # check tolerance\n#            Lx1 = df1 + dh1 * lam + dg1 * mu\n#            feascond1 = max([ norm(h1, Inf), max(g1) ]) / \\\n#                (1 + max([ norm(x1, Inf), norm(z, Inf) ]))\n#            gradcond1 = norm(Lx1, Inf) / \\\n#                (1 + max([ norm(lam, Inf), norm(mu, Inf) ]))\n#\n#            if feascond1 > feascond and gradcond1 > gradcond:\n#                sc = True\n#        if sc:\n#            alpha = 1.0\n#            for j in range(opt[\"max_red\"]):\n#                dx1 = alpha * dx\n#                x1 = x + dx1\n#                f1 = ipm_f(x1)             # cost\n#                f1 = f1 * opt[\"cost_mult\"]\n#                gn1, hn1 = ipm_gh(x1)              # non-linear constraints\n#                g1 = r_[gn1, Ai * x1 - bi]         # inequality constraints\n#                h1 = r_[hn1, Ae * x1 - be]         # equality constraints\n#                L1 = f1 + lam.H * h1 + mu.H * (g1 + z) - gamma * sum(log(z))\n#                if opt[\"verbose\"]:\n#                    logger.info(\"\\n   %3d            %10.f\" % (-j, norm(dx1)))\n#                rho = (L1 - L) / (Lx.H * dx1 + 0.5 * dx1.H * Lxx * dx1)\n#                if rho > rho_min and rho < rho_max:\n#                    break\n#                else:\n#                    alpha = alpha / 2.0\n#            dx = alpha * dx\n#            dz = alpha * dz\n#            dlam = alpha * dlam\n#            dmu = alpha * dmu\n\n        # do the update\n        k = flatnonzero(dz < 0.0)\n        alphap = min([xi * min(z[k] / -dz[k]), 1]) if len(k) else 1.0\n        k = flatnonzero(dmu < 0.0)\n        alphad = min([xi * min(mu[k] / -dmu[k]), 1]) if len(k) else 1.0\n        x = x + alphap * dx\n        z = z + alphap * dz\n        lam = lam + alphad * dlam\n        mu = mu + alphad * dmu\n        if niq > 0:\n            gamma = sigma * dot(z, mu) / niq\n\n        # evaluate cost, constraints, derivatives\n        f, df, _ = f_fcn(x)             # cost\n        f = f * opt[\"cost_mult\"]\n        df = df * opt[\"cost_mult\"]\n        if nonlinear:\n            hn, gn, dhn, dgn = gh_fcn(x)                   # nln constraints\n#            g = gn if Ai is None else r_[gn, Ai * x - bi] # ieq constraints\n#            h = hn if Ae is None else r_[hn, Ae * x - be] # eq constraints\n            h = hn if Ai is None else r_[hn, Ai * x - bi] # ieq constr\n            g = gn if Ae is None else r_[gn, Ae * x - be]  # eq constr\n\n            if (dhn is None) and (Ai is None):\n                dh = None\n            elif dhn is None:\n                dh = Ai.T\n            elif Ae is None:\n                dh = dhn\n            else:\n                dh = hstack([dhn, Ai.T])\n\n            if (dgn is None) and (Ae is None):\n                dg = None\n            elif dgn is None:\n                dg = Ae.T\n            elif Ae is None:\n                dg = dgn\n            else:\n                dg = hstack([dgn, Ae.T])\n        else:\n            h = -bi if Ai is None else Ai * x - bi    # inequality constraints\n            g = -be if Ae is None else Ae * x - be    # equality constraints\n            # 1st derivatives are constant, still dh = Ai.T, dg = Ae.T\n\n        Lx = df\n        Lx = Lx + dg * lam if dg is not None else Lx\n        Lx = Lx + dh * mu  if dh is not None else Lx\n\n        gnorm = norm(g, Inf) if len(g) else 0.0\n        lam_norm = norm(lam, Inf) if len(lam) else 0.0\n        mu_norm = norm(mu, Inf) if len(mu) else 0.0\n        feascond = \\\n            max([gnorm, max(h)]) / (1+max([norm(x, Inf), norm(z, Inf)]))\n        gradcond = \\\n            norm(Lx, Inf) / (1 + max([lam_norm, mu_norm]))\n        compcond = dot(z, mu) / (1 + norm(x, Inf))\n        costcond = float(absolute(f - f0) / (1 + absolute(f0)))\n\n        hist[i] = {'feascond': feascond, 'gradcond': gradcond,\n            'compcond': compcond, 'costcond': costcond, 'gamma': gamma,\n            'stepsize': norm(dx), 'obj': f / opt[\"cost_mult\"],\n            'alphap': alphap, 'alphad': alphad}\n\n        if opt[\"verbose\"]:\n            print \"%3d  %12.8g %10.5g %12g %12g %12g %12g\" % \\\n                (i, (f / opt[\"cost_mult\"]), norm(dx), feascond, gradcond,\n                 compcond, costcond)\n\n        if feascond < opt[\"feastol\"] and gradcond < opt[\"gradtol\"] and \\\n            compcond < opt[\"comptol\"] and costcond < opt[\"costtol\"]:\n            converged = True\n            if opt[\"verbose\"]:\n                print \"Converged!\"\n        else:\n            if any(isnan(x)) or (alphap < alpha_min) or \\\n                (alphad < alpha_min) or (gamma < EPS) or (gamma > 1.0 / EPS):\n                if opt[\"verbose\"]:\n                    print \"Numerically failed.\"\n                eflag = -1\n                break\n            f0 = f\n\n#            if opt[\"step_control\"]:\n#                L = f + dot(lam, g) + dot(mu * (h + z)) - gamma * sum(log(z))\n\n    if opt[\"verbose\"]:\n        if not converged:\n            print \"Did not converge in %d iterations.\" % i\n\n    # package results\n    if eflag != -1:\n        eflag = converged\n\n    if eflag == 0:\n        message = 'Did not converge'\n    elif eflag == 1:\n        message = 'Converged'\n    elif eflag == -1:\n        message = 'Numerically failed'\n    else:\n        raise\n\n    output = {\"iterations\": i, \"history\": hist, \"message\": message}\n\n    # zero out multipliers on non-binding constraints\n    mu[flatnonzero( (h < -opt[\"feastol\"]) & (mu < mu_threshold) )] = 0.0\n\n    # un-scale cost and prices\n    f = f / opt[\"cost_mult\"]\n    lam = lam / opt[\"cost_mult\"]\n    mu = mu / opt[\"cost_mult\"]\n\n    # re-package multipliers into struct\n    lam_lin = lam[neqnln:neq]           # lambda for linear constraints\n    mu_lin = mu[niqnln:niq]             # mu for linear constraints\n    kl = flatnonzero(lam_lin < 0.0)     # lower bound binding\n    ku = flatnonzero(lam_lin > 0.0)     # upper bound binding\n\n    mu_l = zeros(nx + nA)\n    mu_l[ieq[kl]] = -lam_lin[kl]\n    mu_l[igt] = mu_lin[nlt:nlt + ngt]\n    mu_l[ibx] = mu_lin[nlt + ngt + nbx:nlt + ngt + nbx + nbx]\n\n    mu_u = zeros(nx + nA)\n    mu_u[ieq[ku]] = lam_lin[ku]\n    mu_u[ilt] = mu_lin[:nlt]\n    mu_u[ibx] = mu_lin[nlt + ngt:nlt + ngt + nbx]\n\n    lmbda = {'mu_l': mu_l[nx:], 'mu_u': mu_u[nx:],\n             'lower': mu_l[:nx], 'upper': mu_u[:nx]}\n\n    if niqnln > 0:\n        lmbda['ineqnonlin'] = mu[:niqnln]\n    if neqnln > 0:\n        lmbda['eqnonlin'] = lam[:neqnln]\n\n#    lmbda = {\"eqnonlin\": lam[:neqnln], 'ineqnonlin': mu[:niqnln],\n#             \"mu_l\": mu_l[nx:], \"mu_u\": mu_u[nx:],\n#             \"lower\": mu_l[:nx], \"upper\": mu_u[:nx]}\n\n    solution =  {\"x\": x, \"f\": f, \"converged\": converged,\n                 \"lmbda\": lmbda, \"output\": output}\n\n    return solution", "response": "Primal - dual interior point method for NLP."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef qps_pips(H, c, A, l, u, xmin=None, xmax=None, x0=None, opt=None):\n    if H is None or H.nnz == 0:\n        if A is None or A.nnz == 0 and \\\n           xmin is None or len(xmin) == 0 and \\\n           xmax is None or len(xmax) == 0:\n            print 'qps_pips: LP problem must include constraints or variable bounds'\n            return\n        else:\n            if A is not None and A.nnz >= 0:\n                nx = A.shape[1]\n            elif xmin is not None and len(xmin) > 0:\n                nx = xmin.shape[0]\n            elif xmax is not None and len(xmax) > 0:\n                nx = xmax.shape[0]\n        H = csr_matrix((nx, nx))\n    else:\n        nx = H.shape[0]\n\n    xmin = -Inf * ones(nx) if xmin is None else xmin\n    xmax =  Inf * ones(nx) if xmax is None else xmax\n\n    c = zeros(nx) if c is None else c\n\n#    if x0 is None:\n#        x0 = zeros(nx)\n#        k = flatnonzero( (VUB < 1e10) & (VLB > -1e10) )\n#        x0[k] = ((VUB[k] + VLB[k]) / 2)\n#        k = flatnonzero( (VUB < 1e10) & (VLB <= -1e10) )\n#        x0[k] = VUB[k] - 1\n#        k = flatnonzero( (VUB >= 1e10) & (VLB > -1e10) )\n#        x0[k] = VLB[k] + 1\n\n    x0 = zeros(nx) if x0 is None else x0\n\n    opt = {} if opt is None else opt\n    if not opt.has_key(\"cost_mult\"):\n        opt[\"cost_mult\"] = 1\n\n    def qp_f(x):\n        f = 0.5 * dot(x.T * H, x) + dot(c.T, x)\n        df = H * x + c\n        d2f = H\n        return f, df, d2f\n\n#    def qp_gh(x):\n#        g = array([])\n#        h = array([])\n#        dg = None\n#        dh = None\n#        return g, h, dg, dh\n#\n#    def qp_hessian(x, lmbda):\n#        Lxx = H * opt[\"cost_mult\"]\n#        return Lxx\n\n#    l = -Inf * ones(b.shape[0])\n#    l[:N] = b[:N]\n\n    return pips(qp_f, x0, A, l, u, xmin, xmax, opt=opt)", "response": "Uses the Python Interior Point Solver to solve the following algorithm"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nask the user his opinion.", "response": "def ask(message='Are you sure? [y/N]'):\n    \"\"\"Asks the user his opinion.\"\"\"\n    agree = False\n    answer = raw_input(message).lower()\n    if answer.startswith('y'):\n        agree = True\n    return agree"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_configuration(self, file_path, test_program, custom_args):\n        # checking filepath\n        if not os.path.isdir(file_path):\n            raise InvalidFilePath(\"INVALID CONFIGURATION: file path %s is not a directory\" %\n                os.path.abspath(file_path)\n            )\n\n        if not test_program in IMPLEMENTED_TEST_PROGRAMS:\n            raise InvalidTestProgram('The `%s` is unknown, or not yet implemented. Please chose another one.' % test_program)\n\n        if custom_args:\n            if not self.quiet and not ask(\"WARNING!!!\\nYou are about to run the following command\\n\\n   $ %s\\n\\nAre you sure you still want to proceed [y/N]? \" % self.get_cmd()):\n                raise CancelDueToUserRequest('Test cancelled...')", "response": "Checks if configuration is ok."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the test program is available in the python environnement", "response": "def check_dependencies(self):\n        \"Checks if the test program is available in the python environnement\"\n        if self.test_program == 'nose':\n            try:\n                import nose\n            except ImportError:\n                sys.exit('Nosetests is not available on your system. Please install it and try to run it again')\n        if self.test_program == 'py':\n            try:\n                import py\n            except:\n                sys.exit('py.test is not available on your system. Please install it and try to run it again')\n        if self.test_program == 'django':\n            try:\n                import django\n            except:\n                sys.exit('django is not available on your system. Please install it and try to run it again')\n        if self.test_program == 'phpunit':\n            try:\n                process = subprocess.check_call(['phpunit','--version']) \n            except:\n                sys.exit('phpunit is not available on your system. Please install it and try to run it again')\n        if self.test_program == 'tox':\n            try:\n                import tox\n            except ImportError:\n                sys.exit('tox is not available on your system. Please install it and try to run it again')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the full command to be executed at runtime", "response": "def get_cmd(self):\n        \"\"\"Returns the full command to be executed at runtime\"\"\"\n\n        cmd = None\n        if self.test_program in ('nose', 'nosetests'):\n            cmd = \"nosetests %s\" % self.file_path\n        elif self.test_program == 'django':\n            executable = \"%s/manage.py\" % self.file_path\n            if os.path.exists(executable):\n                cmd = \"python %s/manage.py test\" % self.file_path\n            else:\n                cmd = \"django-admin.py test\"\n        elif self.test_program == 'py':\n            cmd = 'py.test %s' % self.file_path\n        elif self.test_program == 'symfony':\n            cmd = 'symfony test-all'\n        elif self.test_program == 'jelix':\n            # as seen on http://jelix.org/articles/fr/manuel-1.1/tests_unitaires\n            cmd = 'php tests.php'\n        elif self.test_program == 'phpunit':\n            cmd = 'phpunit'\n        elif self.test_program == 'sphinx':\n            cmd = 'make html'\n        elif self.test_program == 'tox':\n            cmd = 'tox'\n\n        if not cmd:\n            raise InvalidTestProgram(\"The test program %s is unknown. Valid options are: `nose`, `django` and `py`\" % self.test_program)\n\n        # adding custom args\n        if self.custom_args:\n            cmd = '%s %s' % (cmd, self.custom_args)\n        return cmd"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the file is not ignored", "response": "def include(self, path):\n        \"\"\"Returns `True` if the file is not ignored\"\"\"\n        for extension in IGNORE_EXTENSIONS:\n            if path.endswith(extension):\n                return False\n        parts = path.split(os.path.sep)\n        for part in parts:\n            if part in self.ignore_dirs:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef walk(self, top, file_list={}):\n        for root, dirs, files in os.walk(top, topdown=False):\n            if os.path.basename(root) in self.ignore_dirs:\n                # Do not dig in ignored dirs\n                continue\n\n            for name in files:\n                full_path = os.path.join(root, name)\n                if self.include(full_path):\n                    if os.path.isfile(full_path):\n                        # preventing fail if the file vanishes\n                        content = open(full_path).read()\n                        hashcode = hashlib.sha224(content).hexdigest()\n                        file_list[full_path] = hashcode\n            for name in dirs:\n                if name not in self.ignore_dirs:\n                    self.walk(os.path.join(root, name), file_list)\n        return file_list", "response": "Walks the walk. nah seriously reads the file and stores a hashkey\nEffectiveFile corresponding to its content."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn total filesize in MB", "response": "def file_sizes(self):\n        \"\"\"Returns total filesize (in MB)\"\"\"\n        size = sum(map(os.path.getsize, self.file_list))\n        return size / 1024 / 1024"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract differences between lists. For debug purposes", "response": "def diff_list(self, list1, list2):\n        \"\"\"Extracts differences between lists. For debug purposes\"\"\"\n        for key in list1:\n            if key in list2 and list2[key] != list1[key]:\n                print key\n            elif key not in list2:\n                print key"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, cmd):\n        print datetime.datetime.now()\n        output = subprocess.Popen(cmd, shell=True)\n        output = output.communicate()[0]\n        print output", "response": "Runs the appropriate command"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format(file_metrics, build_metrics):\n    metrics = {'files': file_metrics}\n    if build_metrics:\n        metrics['build'] = build_metrics\n    body = json.dumps(metrics, sort_keys=True, indent=4) + '\\n'\n    return body", "response": "compute output in JSON format."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_linear_constraints(A, l, u):\n    ieq = []\n    igt = []\n    ilt = []\n    ibx = []\n    for i in range(len(l)):\n        if abs(u[i] - l[i]) <= EPS:\n            ieq.append(i)\n        elif (u[i] > 1e10) and (l[i] > -1e10):\n            igt.append(i)\n        elif (l[i] <= -1e10) and (u[i] < 1e10):\n            ilt.append(i)\n        elif (abs(u[i] - l[i]) > EPS) and (u[i] < 1e10) and (l[i] > -1e10):\n            ibx.append(i)\n        else:\n            raise ValueError\n\n    Ae = A[ieq, :]\n    Ai = sparse([A[ilt, :], -A[igt, :], A[ibx, :], -A[ibx, :]])\n    be = u[ieq, :]\n    bi = matrix([u[ilt], -l[igt], u[ibx], -l[ibx]])\n\n    return Ae, be, Ai, bi", "response": "Splits the linear constraints A and l into two lists of Ae Ai and bi."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the partial derivative of power injection w. r. t. voltage.", "response": "def dSbus_dV(Y, V):\n    \"\"\" Computes the partial derivative of power injection w.r.t. voltage.\n\n        References:\n            Ray Zimmerman, \"dSbus_dV.m\", MATPOWER, version 3.2,\n            PSERC (Cornell), http://www.pserc.cornell.edu/matpower/\n    \"\"\"\n    I = Y * V\n\n    diagV = spdiag(V)\n    diagIbus = spdiag(I)\n    diagVnorm = spdiag(div(V, abs(V))) # Element-wise division.\n\n    dS_dVm = diagV * conj(Y * diagVnorm) + conj(diagIbus) * diagVnorm\n    dS_dVa = 1j * diagV * conj(diagIbus - Y * diagV)\n\n    return dS_dVm, dS_dVa"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dIbr_dV(Yf, Yt, V):\n#        nb = len(V)\n\n    Vnorm = div(V, abs(V))\n    diagV = spdiag(V)\n    diagVnorm = spdiag(Vnorm)\n    dIf_dVa = Yf * 1j * diagV\n    dIf_dVm = Yf * diagVnorm\n    dIt_dVa = Yt * 1j * diagV\n    dIt_dVm = Yt * diagVnorm\n\n    # Compute currents.\n    If = Yf * V\n    It = Yt * V\n\n    return dIf_dVa, dIf_dVm, dIt_dVa, dIt_dVm, If, It", "response": "Computes partial derivatives of branch currents w. r. t. voltage."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the partial derivative of S w. r. t voltage phase angle and power flow vector.", "response": "def dSbr_dV(Yf, Yt, V, buses, branches):\n    \"\"\" Computes the branch power flow vector and the partial derivative of\n        branch power flow w.r.t voltage.\n    \"\"\"\n    nl = len(branches)\n    nb = len(V)\n\n    f = matrix([l.from_bus._i for l in branches])\n    t = matrix([l.to_bus._i for l in branches])\n\n    # Compute currents.\n    If = Yf * V\n    It = Yt * V\n\n    Vnorm = div(V, abs(V))\n\n    diagVf = spdiag(V[f])\n    diagIf = spdiag(If)\n    diagVt = spdiag(V[t])\n    diagIt = spdiag(It)\n    diagV = spdiag(V)\n    diagVnorm = spdiag(Vnorm)\n\n    ibr = range(nl)\n    size = (nl, nb)\n    # Partial derivative of S w.r.t voltage phase angle.\n    dSf_dVa = 1j * (conj(diagIf) *\n        spmatrix(V[f], ibr, f, size) - diagVf * conj(Yf * diagV))\n\n    dSt_dVa = 1j * (conj(diagIt) *\n        spmatrix(V[t], ibr, t, size) - diagVt * conj(Yt * diagV))\n\n    # Partial derivative of S w.r.t. voltage amplitude.\n    dSf_dVm = diagVf * conj(Yf * diagVnorm) + conj(diagIf) * \\\n        spmatrix(Vnorm[f], ibr, f, size)\n\n    dSt_dVm = diagVt * conj(Yt * diagVnorm) + conj(diagIt) * \\\n        spmatrix(Vnorm[t], ibr, t, size)\n\n    # Compute power flow vectors.\n    Sf = mul(V[f], conj(If))\n    St = mul(V[t], conj(It))\n\n    return dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing the derivative of the apparent power flow in the system.", "response": "def dAbr_dV(dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, Sf, St):\n    \"\"\" Partial derivatives of squared flow magnitudes w.r.t voltage.\n\n        Computes partial derivatives of apparent power w.r.t active and\n        reactive power flows.  Partial derivative must equal 1 for lines\n        with zero flow to avoid division by zero errors (1 comes from\n        L'Hopital).\n    \"\"\"\n    dAf_dPf = spdiag(2 * Sf.real())\n    dAf_dQf = spdiag(2 * Sf.imag())\n    dAt_dPt = spdiag(2 * St.real())\n    dAt_dQt = spdiag(2 * St.imag())\n\n    # Partial derivative of apparent power magnitude w.r.t voltage\n    # phase angle.\n    dAf_dVa = dAf_dPf * dSf_dVa.real() + dAf_dQf * dSf_dVa.imag()\n    dAt_dVa = dAt_dPt * dSt_dVa.real() + dAt_dQt * dSt_dVa.imag()\n    # Partial derivative of apparent power magnitude w.r.t. voltage\n    # amplitude.\n    dAf_dVm = dAf_dPf * dSf_dVm.real() + dAf_dQf * dSf_dVm.imag()\n    dAt_dVm = dAt_dPt * dSt_dVm.real() + dAt_dQt * dSt_dVm.imag()\n\n    return dAf_dVa, dAf_dVm, dAt_dVa, dAt_dVm"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef d2Sbus_dV2(Ybus, V, lam):\n    n = len(V)\n    Ibus = Ybus * V\n    diaglam = spdiag(lam)\n    diagV = spdiag(V)\n\n    A = spmatrix(mul(lam, V), range(n), range(n))\n    B = Ybus * diagV\n    C = A * conj(B)\n    D = Ybus.H * diagV\n    E = conj(diagV) * (D * diaglam - spmatrix(D*lam, range(n), range(n)))\n    F = C - A * spmatrix(conj(Ibus), range(n), range(n))\n    G = spmatrix(div(matrix(1.0, (n, 1)), abs(V)), range(n), range(n))\n\n    Gaa = E + F\n    Gva = 1j * G * (E - F)\n    Gav = Gva.T\n    Gvv = G * (C + C.T) * G\n\n    return Gaa, Gav, Gva, Gvv", "response": "Computes 2nd derivatives of power injection w. r. t. voltage."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing 2nd derivatives of complex branch current w. r. t. voltage.", "response": "def d2Ibr_dV2(Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of complex branch current w.r.t. voltage.\n    \"\"\"\n    nb = len(V)\n    diaginvVm = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))\n\n    Haa = spdiag(mul(-(Ybr.T * lam), V))\n    Hva = -1j * Haa * diaginvVm\n    Hav = Hva\n    Hvv = spmatrix([], [], [], (nb, nb))\n\n    return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes 2nd derivatives of complex power flow w. r. t. voltage.", "response": "def d2Sbr_dV2(Cbr, Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of complex power flow w.r.t. voltage.\n    \"\"\"\n    nb = len(V)\n\n    diaglam = spdiag(lam)\n    diagV = spdiag(V)\n\n    A = Ybr.H * diaglam * Cbr\n    B = conj(diagV) * A * diagV\n    D = spdiag(mul((A*V), conj(V)))\n    E = spdiag(mul((A.T * conj(V)), V))\n    F = B + B.T\n    G = spdiag(div(matrix(1.0, (nb, 1)), abs(V)))\n\n    Haa = F - D - E\n    Hva = 1j * G * (B - B.T - D + E)\n    Hav = Hva.T\n    Hvv = G * F * G\n\n    return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncomputing 2nd derivatives of |complex power flow| ** 2 w. r. t. V.", "response": "def d2ASbr_dV2(dSbr_dVa, dSbr_dVm, Sbr, Cbr, Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of |complex power flow|**2 w.r.t. V.\n    \"\"\"\n    diaglam = spdiag(lam)\n    diagSbr_conj = spdiag(conj(Sbr))\n\n    Saa, Sav, Sva, Svv = d2Sbr_dV2(Cbr, Ybr, V, diagSbr_conj * lam)\n\n    Haa = 2 * ( Saa + dSbr_dVa.T * diaglam * conj(dSbr_dVa) ).real()\n    Hva = 2 * ( Sva + dSbr_dVm.T * diaglam * conj(dSbr_dVa) ).real()\n    Hav = 2 * ( Sav + dSbr_dVa.T * diaglam * conj(dSbr_dVm) ).real()\n    Hvv = 2 * ( Svv + dSbr_dVm.T * diaglam * conj(dSbr_dVm) ).real()\n\n    return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes 2nd derivatives of |complex current| ** 2 w. r. t. V.", "response": "def d2AIbr_dV2(dIbr_dVa, dIbr_dVm, Ibr, Ybr, V, lam):\n    \"\"\" Computes 2nd derivatives of |complex current|**2 w.r.t. V.\n    \"\"\"\n    diaglam = spdiag(lam)\n    diagIbr_conj = spdiag(conj(Ibr))\n\n    Iaa, Iav, Iva, Ivv = d2Ibr_dV2(Ybr, V, diagIbr_conj * lam)\n\n    Haa = 2 * ( Iaa + dIbr_dVa.T * diaglam * conj(dIbr_dVa) ).real()\n    Hva = 2 * ( Iva + dIbr_dVm.T * diaglam * conj(dIbr_dVa) ).real()\n    Hav = 2 * ( Iav + dIbr_dVa.T * diaglam * conj(dIbr_dVm) ).real()\n    Hvv = 2 * ( Ivv + dIbr_dVm.T * diaglam * conj(dIbr_dVm) ).real()\n\n    return Haa, Hav, Hva, Hvv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a sparse SciPy matrix into a sparse CVXOPT matrix.", "response": "def tocvx(B):\n    \"\"\" Converts a sparse SciPy matrix into a sparse CVXOPT matrix.\n    \"\"\"\n    Bcoo = B.tocoo()\n    return spmatrix(Bcoo.data, Bcoo.row.tolist(), Bcoo.col.tolist())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _run_opf(self, P, q, AA, ll, uu, xmin, xmax, x0, opt):\n        AAcvx = tocvx(AA)\n        nx = x0.shape[0] # number of variables\n        # add var limits to linear constraints\n        eyex = spmatrix(1.0, range(nx), range(nx))\n        A = sparse([eyex, AAcvx])\n        l = matrix([-xmin, ll])\n        u = matrix([ xmax, uu])\n\n        Ae, be, Ai, bi = split_linear_constraints(A, l, u)\n\n\n        if len(P.V) > 0:\n            cvx_sol = solvers.qp(P, q, Ai, bi, Ae, be, self.solver, {\"x\": x0})\n        else:\n            cvx_sol = solvers.lp(q, Ai, bi, Ae, be, self.solver, {\"x\": x0})\n\n        return cvx_sol", "response": "Solves the opf problem."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the result attribute values.", "response": "def _update_case(self, bs, ln, gn, base_mva, Bf, Pfinj, Va, Pg, lmbda):\n        \"\"\" Calculates the result attribute values.\n        \"\"\"\n        for i, bus in enumerate(bs):\n            bus.v_angle = Va[i] * 180.0 / pi"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef doInteractions(self, number=1):\n        t0 = time.time()\n\n        for _ in range(number):\n            self._oneInteraction()\n\n        elapsed = time.time() - t0\n        logger.info(\"%d interactions executed in %.3fs.\" % (number, elapsed))\n\n        return self.stepid", "response": "Directly maps the agents and the tasks."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _oneInteraction(self):\n        self.stepid += 1\n\n        logger.info(\"\\nEntering simulation period %d.\" % self.stepid)\n\n        # Initialise the market.\n        self.market.reset()\n\n        # Get an action from each agent and perform it.\n        for task, agent in zip(self.tasks, self.agents):\n            observation = task.getObservation()\n            agent.integrateObservation(observation)\n\n            action = agent.getAction()\n            task.performAction(action)\n\n        # Clear the market.\n        self.market.run()\n\n        # Reward each agent appropriately.\n        for task, agent in zip(self.tasks, self.agents):\n            reward = task.getReward()\n            agent.giveReward(reward)", "response": "Perform one interaction between each agent and its environment."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbasing on AugYbus.m from MatDyn by Stijn Cole, developed at Katholieke Universiteit Leuven. See U{http://www.esat.kuleuven.be/ electa/teaching/matdyn/} for more information. @rtype: csr_matrix @return: The augmented bus admittance matrix.", "response": "def getAugYbus(self, U0, gbus):\n        \"\"\" Based on AugYbus.m from MatDyn by Stijn Cole, developed at\n        Katholieke Universiteit Leuven. See U{http://www.esat.kuleuven.be/\n        electa/teaching/matdyn/} for more information.\n\n        @rtype: csr_matrix\n        @return: The augmented bus admittance matrix.\n        \"\"\"\n        j = 0 + 1j\n        buses = self.case.connected_buses\n        nb = len(buses)\n\n        Ybus, _, _ = self.case.getYbus()\n\n        # Steady-state bus voltages.\n\n        # Calculate equivalent load admittance\n        Sd = array([self.case.s_demand(bus) for bus in buses])\n        Yd = conj(Sd) / abs(U0)**2\n\n        xd_tr = array([g.xd_tr for g in self.dyn_generators])\n\n        # Calculate equivalent generator admittance.\n        Yg = zeros(nb)\n        Yg[gbus] = 1 / (j * xd_tr)\n\n        # Add equivalent load and generator admittance to Ybus matrix\n        for i in range(nb):\n            Ybus[i, i] = Ybus[i, i] + Yg[i] + Yd[i]\n\n        return Ybus"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef generatorInit(self, U0):\n        j = 0 + 1j\n        generators = self.dyn_generators\n\n        Efd0 = zeros(len(generators))\n        Xgen0 = zeros((len(generators), 4))\n\n        typ1 = [g._i for g in generators if g.model == CLASSICAL]\n        typ2 = [g._i for g in generators if g.model == FOURTH_ORDER]\n\n        # Generator type 1: classical model\n        x_tr = array([g.x_tr for g in generators])\n\n        omega0 = ones(len(typ1)) * 2 * pi * self.freq\n\n        # Initial machine armature currents.\n        Sg = array([g.p + j * g.q for g in generators])\n        Ia0 = conj(Sg[typ1]) / conj(U0) / self.base_mva\n\n        # Initial Steady-state internal EMF.\n        Eq_tr0 = U0[typ1] + j * x_tr * Ia0\n        delta0 = angle(Eq_tr0)\n        Eq_tr0 = abs(Eq_tr0)\n\n        Xgen0[typ1, :] = c_[delta0, omega0, Eq_tr0]\n\n        # Generator type 2: 4th order model\n        xd = array([g.xd for g in generators])\n        xq = array([g.xq for g in generators])\n        xd_tr = array([g.xd_tr for g in generators])\n        xq_tr = array([g.xq_tr for g in generators])\n\n        omega0 = ones(len(typ2)) * 2 * pi * self.freq\n\n        # Initial machine armature currents.\n        Ia0 = conj(Sg[typ2]) / conj(U0[typ2]) / self.base_mva\n        phi0 = angle(Ia0)\n\n        # Initial Steady-state internal EMF.\n        Eq0 = U0[typ2] + j * xq * Ia0\n        delta0 = angle(Eq0)\n\n        # Machine currents in dq frame.\n        Id0 = -abs(Ia0) * sin(delta0 - phi0)\n        Iq0 =  abs(Ia0) * cos(delta0 - phi0)\n\n        # Field voltage.\n        Efd0[typ2] = abs(Eq0) - (xd - xq) * Id0\n\n        # Initial Transient internal EMF.\n        Eq_tr0 = Efd0[typ2] + (xd - xd_tr) * Id0\n        Ed_tr0 = -(xq - xq_tr) * Iq0\n\n        Xgen0[typ2, :] = c_[delta0, omega0, Eq_tr0, Ed_tr0]\n\n        # Generator type 3:\n\n        # Generator type 4:\n\n        return Efd0, Xgen0", "response": "Based on GeneratorInit. m from MatDyn by Katholieke Universiteit Leuven. See U { http://www. esat. kuleuven. be / electa / teaching / matdyn."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exciterInit(self, Xexc, Vexc):\n        exciters = self.exciters\n\n        Xexc0 = zeros(Xexc.shape)\n        Pexc0 = zeros(len(exciters))\n\n        typ1 = [e.generator._i for e in exciters if e.model ==CONST_EXCITATION]\n        typ2 = [e.generator._i for e in exciters if e.model == IEEE_DC1A]\n\n        # Exciter type 1: constant excitation\n        Efd0 = Xexc[typ1, 0]\n        Xexc0[typ1, 0] = Efd0\n\n        # Exciter type 2: IEEE DC1A\n        Efd0 = Xexc[typ2, 0]\n        Ka = array([e.ka for e in exciters])\n        Ta = array([e.ta for e in exciters])\n        Ke = array([e.ke for e in exciters])\n        Te = array([e.te for e in exciters])\n        Kf = array([e.kf for e in exciters])\n        Tf = array([e.tf for e in exciters])\n        Aex = array([e.aex for e in exciters])\n        Bex = array([e.bex for e in exciters])\n        Ur_min = array([e.ur_min for e in exciters])\n        Ur_max = array([e.ur_max for e in exciters])\n\n        U = Vexc[typ2, 0]\n\n        Uf = zeros(len(typ2))\n        Ux = Aex * exp(Bex * Efd0)\n        Ur = Ux + Ke * Efd0\n        Uref2 = U + (Ux + Ke * Efd0) / Ka - U\n        Uref = U\n\n        Xexc0[typ2, :] = c_[Efd0, Uf, Ur]\n        Pexc0[typ2, :] = c_[Ka, Ta, Ke, Te, Kf, Tf, Aex, Bex,\n                            Ur_min, Ur_max, Uref, Uref2]\n\n        # Exciter type 3:\n\n        # Exciter type 4:\n\n        return Xexc0, Pexc0", "response": "Based on ExciterInit. m from MatDyn by Stijn Cole Universiteit Leuven. See U { http://www. esat. kuleuven. be / exciter_init. m for more information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbase on GovernorInit. m from MatDyn by Katholieke Universiteit Leuven. See U { http://www. esat. kuleuven. be / governors.", "response": "def governorInit(self, Xgov, Vgov):\n        \"\"\" Based on GovernorInit.m from MatDyn by Stijn Cole, developed at\n        Katholieke Universiteit Leuven. See U{http://www.esat.kuleuven.be/\n        electa/teaching/matdyn/} for more information.\n\n        @rtype: tuple\n        @return: Initial governor conditions.\n        \"\"\"\n        governors = self.governors\n\n        Xgov0 = zeros(Xgov.shape)\n        Pgov0 = zeros(len(governors))\n\n        typ1 = [g.generator._i for g in governors if g.model == CONST_POWER]\n        typ2 = [g.generator._i for g in governors if g.model == GENERAL_IEEE]\n\n        # Governor type 1: constant power\n        Pm0 = Xgov[typ1, 0]\n        Xgov0[typ1, 0] = Pm0\n\n        # Governor type 2: IEEE general speed-governing system\n        Pm0 = Xgov[typ2, 0]\n\n        K = array([g.k for g in governors])\n        T1 = array([g.t1 for g in governors])\n        T2 = array([g.t2 for g in governors])\n        T3 = array([g.t3 for g in governors])\n        Pup = array([g.p_up for g in governors])\n        Pdown = array([g.p_down for g in governors])\n        Pmax = array([g.p_max for g in governors])\n        Pmin = array([g.p_min for g in governors])\n\n        omega0 = Vgov[typ2, 0]\n\n        zz0 = Pm0\n        PP0 = Pm0\n\n        P0 = K * (2 * pi * self.freq - omega0)\n        xx0 = T1 * (1 - T2 / T1) * (2 * pi * self.freq - omega0)\n\n        Xgov0[typ2, :] = c_[Pm0, P0, xx0, zz0]\n        Pgov0[typ2, :] = c_[K, T1, T2, T3, Pup, Pdown, Pmax, Pmin, PP0]\n\n        # Governor type 3:\n\n        # Governor type 4:\n\n        return Xgov0, Pgov0"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef machineCurrents(self, Xg, U):\n        generators = self.dyn_generators\n\n        # Initialise.\n        ng = len(generators)\n        Id = zeros(ng)\n        Iq = zeros(ng)\n        Pe = zeros(ng)\n\n        typ1 = [g._i for g in generators if g.model == CLASSICAL]\n        typ2 = [g._i for g in generators if g.model == FOURTH_ORDER]\n\n        # Generator type 1: classical model\n        delta = Xg[typ1, 0]\n        Eq_tr = Xg[typ1, 2]\n\n        xd = array([g.xd for g in generators])\n\n        Pe[typ1] = \\\n            1 / xd * abs(U[typ1]) * abs(Eq_tr) * sin(delta - angle(U[typ1]))\n\n        # Generator type 2: 4th order model\n        delta = Xg[typ1, 0]\n        Eq_tr = Xg[typ1, 2]\n        Ed_tr = Xg[typ1, 3]\n\n        xd_tr = array([g.xd_tr for g in generators])\n        xq_tr = array([g.xq_tr for g in generators])\n\n        theta = angle(U)\n\n        # Transform U to rotor frame of reference.\n        vd = -abs(U[typ2]) * sin(delta - theta[typ2])\n        vq =  abs(U[typ2]) * cos(delta - theta[typ2])\n\n        Id[typ2] =  (vq - Eq_tr) / xd_tr\n        Iq[typ2] = -(vd - Ed_tr) / xq_tr\n\n        Pe[typ2] = \\\n            Eq_tr * Iq[typ2] + Ed_tr * Id[typ2] + \\\n            (xd_tr - xq_tr) * Id[typ2] * Iq[typ2]\n\n        return Id, Iq, Pe", "response": "Based on MachineCurrents. m from MatDyn by Katholieke Universiteit Leuven. See U { http://www. esat. kuleuven. be / atical_generators_nik_ke."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsolves the network for the given set of generators and Y - buses.", "response": "def solveNetwork(self, Xgen, augYbus_solver, gbus):\n        \"\"\" Based on SolveNetwork.m from MatDyn by Stijn Cole, developed at\n        Katholieke Universiteit Leuven. See U{http://www.esat.kuleuven.be/\n        electa/teaching/matdyn/} for more information.\n\n        @rtype: array\n        @return: Bus voltages.\n        \"\"\"\n        generators = self.dyn_generators\n        j = 0 + 1j\n\n        ng = len(gbus)\n        Igen = zeros(ng)\n\n        s = len(augYbus_solver)\n        Ig = zeros(s)\n\n        # Define generator types.\n        typ1 = [g._i for g in generators if g.model == CLASSICAL]\n        typ2 = [g._i for g in generators if g.model == FOURTH_ORDER]\n\n        # Generator type 1: classical model\n        delta = Xgen[typ1, 0]\n        Eq_tr = Xgen[typ1, 2]\n\n        xd_tr = array([g.xd_tr for g in generators])[typ1]\n\n        # Calculate generator currents\n        Igen[typ1] = (Eq_tr * exp(j * delta)) / (j * xd_tr)\n\n        # Generator type 2: 4th order model\n        delta = Xgen[typ2, 0]\n        Eq_tr = Xgen[typ2, 2]\n        Ed_tr = Xgen[typ2, 3]\n\n        xd_tr = array([g.xd_tr for g in generators])[typ2] # Pgen(type2,8)\n\n        # Calculate generator currents. (Padiyar, p.417.)\n        Igen[typ2] = (Eq_tr + j * Ed_tr) * exp(j * delta) / (j * xd_tr)\n\n        # Calculations --------------------------------------------------------\n\n        # Generator currents\n        Ig[gbus] = Igen\n\n        # Calculate network voltages: U = Y/Ig\n        U = augYbus_solver.solve(Ig)\n\n        return U"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exciter(self, Xexc, Pexc, Vexc):\n        exciters = self.exciters\n\n        F = zeros(Xexc.shape)\n\n        typ1 = [e.generator._i for e in exciters if e.model ==CONST_EXCITATION]\n        typ2 = [e.generator._i for e in exciters if e.model == IEEE_DC1A]\n\n        # Exciter type 1: constant excitation\n        F[typ1, :] = 0.0\n\n        # Exciter type 2: IEEE DC1A\n        Efd = Xexc[typ2, 0]\n        Uf = Xexc[typ2, 1]\n        Ur = Xexc[typ2, 2]\n\n        Ka = Pexc[typ2, 0]\n        Ta = Pexc[typ2, 1]\n        Ke = Pexc[typ2, 2]\n        Te = Pexc[typ2, 3]\n        Kf = Pexc[typ2, 4]\n        Tf = Pexc[typ2, 5]\n        Aex = Pexc[typ2, 6]\n        Bex = Pexc[typ2, 7]\n        Ur_min = Pexc[typ2, 8]\n        Ur_max = Pexc[typ2, 9]\n        Uref = Pexc[typ2, 10]\n        Uref2 = Pexc[typ2, 11]\n\n        U = Vexc[typ2, 1]\n\n        Ux = Aex * exp(Bex * Efd)\n        dUr = 1 / Ta * (Ka * (Uref - U + Uref2 - Uf) - Ur)\n        dUf = 1 / Tf * (Kf / Te * (Ur - Ux - Ke * Efd) - Uf)\n\n        if sum(flatnonzero(Ur > Ur_max)) >= 1:\n            Ur2 = Ur_max\n        elif sum(flatnonzero(Ur < Ur_max)) >= 1:\n            Ur2 = Ur_min\n        else:\n            Ur2 = Ur\n\n        dEfd = 1 / Te * (Ur2 - Ux - Ke * Efd)\n        F[typ2, :] = c_[dEfd, dUf, dUr]\n\n        # Exciter type 3:\n\n        # Exciter type 4:\n\n        return F", "response": "Return the current state of the exciter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef governor(self, Xgov, Pgov, Vgov):\n        governors = self.governors\n        omegas = 2 * pi * self.freq\n\n        F = zeros(Xgov.shape)\n\n        typ1 = [g.generator._i for g in governors if g.model == CONST_POWER]\n        typ2 = [g.generator._i for g in governors if g.model == GENERAL_IEEE]\n\n        # Governor type 1: constant power\n        F[typ1, 0] = 0\n\n        # Governor type 2: IEEE general speed-governing system\n        Pm = Xgov[typ2, 0]\n        P = Xgov[typ2, 1]\n        x = Xgov[typ2, 2]\n        z = Xgov[typ2, 3]\n\n        K = Pgov[typ2, 0]\n        T1 = Pgov[typ2, 1]\n        T2 = Pgov[typ2, 2]\n        T3 = Pgov[typ2, 3]\n        Pup = Pgov[typ2, 4]\n        Pdown = Pgov[typ2, 5]\n        Pmax = Pgov[typ2, 6]\n        Pmin = Pgov[typ2, 7]\n        P0 = Pgov[typ2, 8]\n\n        omega = Vgov[typ2, 0]\n\n        dx = K * (-1 / T1 * x + (1 - T2 / T1) * (omega - omegas))\n        dP = 1 / T1 * x + T2 / T1 * (omega - omegas)\n\n        y = 1 / T3 * (P0 - P - Pm)\n\n        y2 = y\n\n        if sum(flatnonzero(y > Pup)) >= 1:\n            y2 = (1 - flatnonzero(y > Pup)) * y2 + flatnonzero(y > Pup) * Pup\n        if sum(flatnonzero(y < Pdown)) >= 1:\n            y2 = (1 - flatnonzero(y<Pdown)) * y2 + flatnonzero(y<Pdown) * Pdown\n\n        dz = y2\n\n        dPm = y2\n\n        if sum(flatnonzero(z > Pmax)) >= 1:\n            dPm = (1 - flatnonzero(z > Pmax)) * dPm + flatnonzero(z > Pmax) * 0\n        if sum(flatnonzero(z < Pmin)) >= 1:\n            dPm = (1 - flatnonzero(z < Pmin)) * dPm + flatnonzero(z < Pmin) * 0\n\n        F[typ2, :] = c_[dPm, dP, dx, dz]\n\n        # Governor type 3:\n\n        # Governor type 4:\n\n        return F", "response": "Governor model.\n\n        Based on Governor.m from MatDyn by Stijn Cole, developed at Katholieke\n        Universiteit Leuven. See U{http://www.esat.kuleuven.be/electa/teaching/\n        matdyn/} for more information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a new entry for each generator in the system.", "response": "def generator(self, Xgen, Xexc, Xgov, Vgen):\n        \"\"\" Generator model.\n\n        Based on Generator.m from MatDyn by Stijn Cole, developed at Katholieke\n        Universiteit Leuven. See U{http://www.esat.kuleuven.be/electa/teaching/\n        matdyn/} for more information.\n        \"\"\"\n        generators = self.dyn_generators\n        omegas = 2 * pi * self.freq\n\n        F = zeros(Xgen.shape)\n\n        typ1 = [g._i for g in generators if g.model == CLASSICAL]\n        typ2 = [g._i for g in generators if g.model == FOURTH_ORDER]\n\n        # Generator type 1: classical model\n        omega = Xgen[typ1, 1]\n        Pm0 = Xgov[typ1, 0]\n\n        H = array([g.h for g in generators])[typ1]\n        D = array([g.d for g in generators])[typ1]\n\n        Pe = Vgen[typ1, 2]\n\n        ddelta = omega = omegas\n        domega = pi * self.freq / H * (-D * (omega - omegas) + Pm0 - Pe)\n        dEq = zeros(len(typ1))\n\n        F[typ1, :] = c_[ddelta, domega, dEq]\n\n        # Generator type 2: 4th order model\n        omega = Xgen[typ2, 1]\n        Eq_tr = Xgen[typ2, 2]\n        Ed_tr = Xgen[typ2, 3]\n\n        H = array([g.h for g in generators])\n        D = array([g.d for g in generators])\n        xd = array([g.xd for g in generators])\n        xq = array([g.xq for g in generators])\n        xd_tr = array([g.xd_tr for g in generators])\n        xq_tr = array([g.xq_tr for g in generators])\n        Td0_tr = array([g.td for g in generators])\n        Tq0_tr = array([g.tq for g in generators])\n\n        Id = Vgen[typ2, 0]\n        Iq = Vgen[typ2, 1]\n        Pe = Vgen[typ2, 2]\n\n        Efd = Xexc[typ2, 0]\n        Pm = Xgov[typ2, 0]\n\n        ddelta = omega - omegas\n        domega = pi * self.freq / H * (-D * (omega - omegas) + Pm - Pe)\n        dEq = 1 / Td0_tr * (Efd - Eq_tr + (xd - xd_tr) * Id)\n        dEd = 1 / Tq0_tr * (-Ed_tr - (xq - xq_tr) * Iq)\n\n        F[typ2, :] = c_[ddelta, domega, dEq, dEd]\n\n        # Generator type 3:\n\n        # Generator type 4:\n\n        return F"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nruns dynamic simulation. @rtype: dict @return: Solution dictionary with the following keys: - C{angles} - generator angles - C{speeds} - generator speeds - C{eq_tr} - q component of transient voltage behind reactance - C{ed_tr} - d component of transient voltage behind reactance - C{efd} - Excitation voltage - C{pm} - mechanical power - C{voltages} - bus voltages - C{stepsize} - step size integration method - C{errest} - estimation of integration error - C{failed} - failed steps - C{time} - time points", "response": "def solve(self):\n        \"\"\" Runs dynamic simulation.\n\n        @rtype: dict\n        @return: Solution dictionary with the following keys:\n                   - C{angles} - generator angles\n                   - C{speeds} - generator speeds\n                   - C{eq_tr} - q component of transient voltage behind\n                     reactance\n                   - C{ed_tr} - d component of transient voltage behind\n                     reactance\n                   - C{efd} - Excitation voltage\n                   - C{pm} - mechanical power\n                   - C{voltages} - bus voltages\n                   - C{stepsize} - step size integration method\n                   - C{errest} - estimation of integration error\n                   - C{failed} - failed steps\n                   - C{time} - time points\n        \"\"\"\n        t0 = time()\n        buses = self.dyn_case.buses\n\n        solution = NewtonPF(self.case).solve()\n\n        if not solution[\"converged\"]:\n            logger.error(\"Power flow did not converge. Exiting...\")\n            return {}\n        elif self.verbose:\n            logger.info(\"Power flow converged.\")\n\n        # Construct augmented Ybus.\n        if self.verbose:\n            logger.info(\"Constructing augmented admittance matrix...\")\n\n        gbus = [g.bus._i for g in self.dyn_generators]\n        ng = len(gbus)\n\n        Um = array([bus.v_magnitude for bus in buses])\n        Ua = array([bus.v_angle * (pi / 180.0) for bus in buses])\n        U0 = Um * exp(1j * Ua)\n        U00 = U0\n\n        augYbus = self.dyn_case.getAugYbus(U0, gbus)\n        augYbus_solver = splu(augYbus)\n\n        # Calculate initial machine state.\n        if self.verbose:\n            logger.info(\"Calculating initial state...\")\n\n        Efd0, Xgen0 = self.dyn_case.generatorInit(U0)\n        omega0 = Xgen0[:, 1]\n\n        Id0, Iq0, Pe0 = self.dyn_case.machineCurrents(Xgen0, U0)\n        Vgen0 = r_[Id0, Iq0, Pe0]\n\n        # Exciter initial conditions.\n        Vexc0 = abs(U0[gbus])\n        Xexc0, Pexc0 = self.dyn_case.exciterInit(Efd0, Vexc0)\n\n        # Governor initial conditions.\n        Pm0 = Pe0\n        Xgov0, Pgov0 = self.dyn_case.governorInit(Pm0, omega0)\n        Vgov0 = omega0\n\n        # Check steady-state.\n        Fexc0 = self.dyn_case.exciter(Xexc0, Pexc0, Vexc0)\n        Fgov0 = self.dyn_case.governor(Xgov0, Pgov0, Vgov0)\n        Fgen0 = self.dyn_case.generator(Xgen0, Xexc0, Xgov0, Vgen0)\n\n        # Check Generator Steady-state\n        if sum(abs(Fgen0)) > 1e-06:\n            logger.error(\"Generator not in steady-state. Exiting...\")\n            return {}\n        # Check Exciter Steady-state\n        if sum(abs(Fexc0)) > 1e-06:\n            logger.error(\"Exciter not in steady-state. Exiting...\")\n            return {}\n        # Check Governor Steady-state\n        if sum(abs(Fgov0)) > 1e-06:\n            logger.error(\"Governor not in steady-state. Exiting...\")\n            return {}\n\n        if self.verbose:\n            logger.info(\"System in steady-state.\")\n\n        # Initialization of main stability loop.\n        t = -0.02 # simulate 0.02s without applying events\n        erst = False\n        failed = False\n        eulerfailed = False\n\n        stoptime = self.dyn_case.stoptime\n\n        if (isinstance(self.method, RungeKuttaFehlberg) or\n            isinstance(self.method, RungeKuttaHighamHall)):\n            stepsize = self.minstep\n        else:\n            stepsize = self.dyn_case.stepsize\n\n        ev = 0\n        eventhappened = False\n        i = 0\n\n        # Allocate memory for variables.\n        if self.verbose:\n            logger.info(\"Allocating memory...\")\n\n        chunk = 5000\n        time = zeros(chunk)\n        time[0, :] = t\n        errest = zeros(chunk)\n        errest[0, :] = erst\n        stepsizes = zeros(chunk)\n        stepsizes[0, :] = stepsize\n\n        # System variables\n        voltages = zeros(chunk)\n        voltages[0, :] = U0.H\n\n        # Generator\n        angles = zeros((chunk, ng))\n        angles[0, :] = Xgen0[:, 0] * 180.0 / pi\n        speeds = zeros((chunk, ng))\n        speeds[0, :] = Xgen0[:, 0] / 2 * pi * self.dyn_case.freq\n        Eq_tr = zeros((chunk, ng))\n        Eq_tr[0, :] = Xgen0[:, 2]\n        Ed_tr = zeros((chunk, ng))\n        Ed_tr[0, :] = Xgen0[:, 3]\n\n        # Exciter and governor\n        Efd = zeros((chunk, ng))\n        Efd[0, :] = Efd0[:, 0]\n        PM = zeros((chunk, ng))\n        PM[0, :] = Pm0[:, 0]\n\n        # Main stability loop.\n        while t < stoptime + stepsize:\n            i += 1\n            if i % 45 == 0 and self.verbose:\n                logger.info(\"%6.2f%% completed.\" % t / stoptime * 100)\n\n            # Numerical Method.\n            Xgen0, self.Pgen0, Vgen0, Xexc0, Pexc0, Vexc0, Xgov0, Pgov0, \\\n                Vgov0, U0, t, newstepsize = self.method.solve(t, Xgen0,\n                    self.Pgen0, Vgen0, Xexc0, Pexc0, Vexc0, Xgov0, Pgov0,\n                    Vgov0, augYbus_solver, gbus, stepsize)\n\n#            if self.method == MODIFIED_EULER:\n#                solver = ModifiedEuler(t, Xgen0, self.Pgen0, Vgen0, Xexc0,\n#                                       Pexc0, Vexc0, Xgov0, Pgov0, Vgov0,\n#                                       augYbus_solver, gbus, stepsize)\n#\n#                Xgen0, self.Pgen0, Vgen0, Xexc0, Pexc0, Vexc0, Xgov0, Pgov0,\n#                Vgov0, U0, t, newstepsize = solver.solve()\n#            elif self.method == RUNGE_KUTTA:\n#                pass\n#            elif self.method == RUNGE_KUTTA_FEHLBERG:\n#                pass\n#            elif self.method == HIGHAM_HALL:\n#                pass\n#            elif self.method == MODIFIED_EULER2:\n#                pass\n#            else:\n#                raise ValueError\n\n            if eulerfailed:\n                logger.info(\"No solution found. Exiting... \")\n                return {}\n\n            if failed:\n                t = t - stepsize\n\n            # End exactly at stop time.\n            if t + newstepsize > stoptime:\n                newstepsize = stoptime - t\n            elif stepsize < self.minstep:\n                logger.info(\"No solution found with minimum step size. Exiting... \")\n                return {}\n\n            # Allocate new memory chunk if matrices are full.\n            if i > time.shape[0]:\n                time = zeros(chunk)\n                errest = zeros(chunk)\n                stepsize = zeros(chunk)\n                voltages = zeros(chunk)\n                angles = zeros((chunk, ng))\n                speeds = zeros((chunk, ng))\n                Eq_tr = zeros((chunk, ng))\n                Ed_tr = zeros((chunk, ng))\n                Efd = zeros((chunk, ng))\n                PM = zeros((chunk, ng))\n\n            # Save values.\n            stepsizes[i, :] = stepsize\n            errest[i, :] = erst\n            time[i, :] = t\n\n            voltages[i, :] = U0\n\n            # Exciters\n            Efd[i, :] = Xexc0[:, 0]\n            # TODO: Set Efd to zero when using classical generator model.\n\n            # Governors\n            PM[i, :] = Xgov0[:, 0]\n\n            # Generators\n            angles[i, :] = Xgen0[:, 0] * 180.0 / pi\n            speeds[i, :] = Xgen0[:, 1] * (2 * pi * self.dyn_case.freq)\n            Eq_tr[i, :] = Xgen0[:, 2]\n            Ed_tr[i, :] = Xgen0[:, 3]\n\n            # Adapt step size if event will occur in next step.\n            if (len(self.events) > 0 and ev <= len(self.events) and\n                isinstance(self.method, RungeKuttaFehlberg) and\n                isinstance(self.method, RungeKutta)):\n\n                if t + newstepsize >= self.events[ev].t:\n                    if self.events[ev] - t < newstepsize:\n                        newstepsize = self.events[ev].t - t\n\n            # Check for events.\n            if len(self.events) > 0 and ev <= len(self.events):\n                for event in self.events:\n                    if (abs(t - self.events[ev].t) > 10 * EPS or\n                        ev > len(self.events)):\n                        break\n                    else:\n                        eventhappened = True\n\n                    event.obj.set_attr(event.param, event.newval)\n\n                    ev += 1\n\n                if eventhappened:\n                    # Refactorise.\n                    self.dyn_case.getAugYbus(U00, gbus)\n                    U0 = self.dyn_case.solveNetwork(Xgen0, self.Pgen0,\n                                                    augYbus_solver, gbus)\n\n                    Id0, Iq0, Pe0 = self.dyn_case.machineCurrents(Xgen0,\n                                                                  self.Pgen0,\n                                                                  U0[gbus])\n                    Vgen0 = r_[Id0, Iq0, Pe0]\n                    Vexc0 = abs(U0[gbus])\n\n                    # Decrease stepsize after event occured.\n                    if (isinstance(self.method, RungeKuttaFehlberg) or\n                        isinstance(self.method, RungeKuttaHighamHall)):\n\n                        newstepsize = self.minstepsize\n\n                    # If event occurs, save values at t- and t+.\n                    i += 1\n\n                    # Save values\n                    stepsize[i, :] = stepsize.T\n                    errest[i, :] = erst.T\n                    time[i, :] = t\n\n                    voltages[i, :] = U0.T\n\n                    # Exciters.\n                    # Set Efd to zero when using classical generator model.\n#                    Efd[i, :] = Xexc0[:, 1] * (flatnonzero(genmodel > 1))\n\n                    # Governors.\n                    PM[i, :] = Xgov0[:, 1]\n\n                    # Generators.\n                    angles[i, :] = Xgen0[:, 0] * 180.0 / pi\n                    speeds[i, :] = Xgen0[:, 1] / (2.0 * pi * self.freq)\n                    Eq_tr[i, :] = Xgen0[:, 2]\n                    Ed_tr[i, :] = Xgen0[:, 3]\n\n                    eventhappened = False\n\n            # Advance time\n            stepsize = newstepsize\n            t += stepsize\n\n        # End of main stability loop ------------------------------------------\n\n        # Output --------------------------------------------------------------\n\n        if self.verbose:\n            logger.info(\"100%% completed\")\n            elapsed = time() - t0\n            logger.info(\"Simulation completed in %5.2f seconds.\" % elapsed)\n\n        # Save only the first i elements.\n        angles = angles[0:i, :]\n        speeds = speeds[0:i, :]\n        Eq_tr = Eq_tr[0:i, :]\n        Ed_tr = Ed_tr[0:i, :]\n\n        Efd = Efd[0:i, :]\n        PM = PM[0:i, :]\n\n        voltages = voltages[0:i, :]\n\n        stepsize = stepsize[0:i, :]\n        errest = errest[0:i, :]\n        time = time[0:i, :]\n\n        if self.plot:\n            raise NotImplementedError\n\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a TeX table from two arrays a1 and a2.", "response": "def tex_table(a1, a2, mup):\n    \"\"\" NB: Insert \\newcolumntype{.}[1]{D{.}{.}{#1}} in header.\n    \"\"\"\n    assert a1.shape == a2.shape\n    m, n = a1.shape\n    s = \"\"\n    s += \"\\\\begin{table}\\n\"\n    s += \"\\\\begin{center}\\n\"\n    cols = \"c.{2.2}\" + (\"|.{2.1}.{3.1}\" * (n)) + \"|\"\n    s += \"\\\\begin{tabular}{%s}\\n\" % cols\n    s += \"\\cline{3-10}\\n\"\n    s += \" & &\\multicolumn{8}{c|}{$G_1$} \\\\\\\\\\n\"\n    s += \"\\cline{3-10}\\n\"\n    s += \" &\"\n    for i in range(n):\n#        align = \"c\" if i == 0 else \"c|\"\n        s += \" &\\multicolumn{2}{c|}{%s\\\\%%}\" % mup[i]\n    s += \" \\\\\\\\\\n\"\n    s += \" &\"\n    for i in range(n):\n        s += \" &r_1 &r_2\"\n    s += \" \\\\\\\\\\n\"\n    s += \"\\hline\\n\"\n    s += \"\\multicolumn{1}{|c|}{\\multirow{4}{*}{$G_2$}}\"\n    for i in range(m):\n        if i != 0:\n            s += \"\\multicolumn{1}{|c|}{}\"\n        s += \" &%.1f\\\\%%\" % mup[i]\n        for j in range(n):\n            s += \" &%.1f\" % abs(a1[i, j])\n            s += \" &%.1f\" % abs(a2[i, j])\n        s += \" \\\\\\\\\\n\"\n    s += \"\\hline\\n\"\n    s += \"\\end{tabular}\\n\"\n    s += \"\\caption{Agent reward for Nash equilibrium analysis}\\n\"\n    s += \"\\label{tbl:nash}\\n\"\n    s += \"\\end{center}\\n\"\n    s += \"\\end{table}\"\n\n    return s"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _write_data(self, file):\n        self.write_case_data(file)\n\n        file.write(\"Bus Data\\n\")\n        file.write(\"-\" * 8 + \"\\n\")\n        self.write_bus_data(file)\n        file.write(\"\\n\")\n\n        file.write(\"Branch Data\\n\")\n        file.write(\"-\" * 11 + \"\\n\")\n        self.write_branch_data(file)\n        file.write(\"\\n\")\n\n        file.write(\"Generator Data\\n\")\n        file.write(\"-\" * 14 + \"\\n\")\n        self.write_generator_data(file)\n        file.write(\"\\n\")", "response": "Writes the data for the current locale to the file in ReStructuredText format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_case_data(self, file):\n#        title = \"Power Flow Solution\"\n#        file.write(\"=\" * len(title))\n#        file.write(\"\\n\")\n#        file.write(\"\\n%s\\n\" % title)\n#        file.write(\"=\" * len(title))\n#        file.write(\"\\n\")\n\n        # Document subtitle.\n        subtitle = self.case.name\n        file.write(\"-\" * len(subtitle))\n        file.write(\"\\n%s\\n\" % subtitle)\n        file.write(\"-\" * len(subtitle))\n        file.write(\"\\n\")\n\n        file.write(\"System Summary\\n\")\n        file.write(\"-\" * 14)\n        file.write(\"\\n\")\n\n        self.write_how_many(file)\n        self.write_how_much(file)\n        self.write_min_max(file)", "response": "Writes the case data to file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite bus data to a ReST table.", "response": "def write_bus_data(self, file):\n        \"\"\" Writes bus data to a ReST table.\n        \"\"\"\n        report = CaseReport(self.case)\n        buses = self.case.buses\n\n        col_width = 8\n        col_width_2 = col_width * 2 + 1\n        col1_width = 6\n\n        sep = \"=\" * 6 + \" \" + (\"=\" * col_width + \" \") * 6 + \"\\n\"\n\n        file.write(sep)\n        # Line one of column headers\n        file.write(\"Name\".center(col1_width) + \" \")\n        file.write(\"Voltage (pu)\".center(col_width_2) + \" \")\n        file.write(\"Generation\".center(col_width_2) + \" \")\n        file.write(\"Load\".center(col_width_2) + \" \")\n        file.write(\"\\n\")\n\n        file.write(\"-\" * col1_width +\" \"+ (\"-\" * col_width_2 + \" \") * 3 + \"\\n\")\n\n        # Line two of column header\n        file.write(\"..\".ljust(col1_width) + \" \")\n        file.write(\"Amp\".center(col_width) + \" \")\n        file.write(\"Phase\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"\\n\")\n\n        file.write(sep)\n\n        # Bus rows\n        for bus in buses:\n            file.write(bus.name[:col1_width].ljust(col1_width))\n            file.write(\" %8.3f\" % bus.v_magnitude)\n            file.write(\" %8.3f\" % bus.v_angle)\n            file.write(\" %8.2f\" % self.case.s_supply(bus).real)\n            file.write(\" %8.2f\" % self.case.s_supply(bus).imag)\n            file.write(\" %8.2f\" % self.case.s_demand(bus).real)\n            file.write(\" %8.2f\" % self.case.s_demand(bus).imag)\n            file.write(\"\\n\")\n\n        # Totals\n#        file.write(\"..\".ljust(col1_width) + \" \")\n#        file.write((\"..\".ljust(col_width) + \" \")*2)\n#        file.write((\"_\"*col_width + \" \")*4 + \"\\n\")\n        file.write(\"..\".ljust(col1_width) + \" \" + \"..\".ljust(col_width) + \" \")\n        file.write(\"*Total:*\".rjust(col_width) + \" \")\n        ptot = report.actual_pgen\n        qtot = report.actual_qgen\n        file.write(\"%8.2f \" % ptot)\n        file.write(\"%8.2f \" % qtot)\n        file.write(\"%8.2f \" % report.p_demand)\n        file.write(\"%8.2f \" % report.q_demand)\n        file.write(\"\\n\")\n        file.write(sep)\n        del report"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_branch_data(self, file):\n        report = CaseReport(self.case)\n        branches = self.case.branches\n\n        col_width   = 8\n        col_width_2 = col_width*2+1\n        col1_width  = 7\n\n        sep = (\"=\" * 7 + \" \") * 3 + (\"=\" * col_width + \" \") * 6 + \"\\n\"\n\n        file.write(sep)\n        # Line one of column headers\n        file.write(\"Name\".center(col1_width) + \" \")\n        file.write(\"From\".center(col1_width) + \" \")\n        file.write(\"To\".center(col1_width) + \" \")\n        file.write(\"From Bus Inj\".center(col_width_2) + \" \")\n        file.write(\"To Bus Inj\".center(col_width_2) + \" \")\n        file.write(\"Loss (I^2 * Z)\".center(col_width_2) + \" \")\n        file.write(\"\\n\")\n\n        file.write((\"-\"*col1_width +\" \")*3)\n        file.write((\"-\"*col_width_2 +\" \")*3 + \"\\n\")\n\n        # Line two of column header\n        file.write(\"..\".ljust(col1_width) + \" \")\n        file.write(\"Bus\".center(col1_width) + \" \")\n        file.write(\"Bus\".center(col1_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"P (MW)\".center(col_width) + \" \")\n        file.write(\"Q (MVAr)\".center(col_width) + \" \")\n        file.write(\"\\n\")\n\n        file.write(sep)\n        # Branch rows\n        loss = report._loss()\n        for each in branches:\n            file.write(each.name[:col1_width].ljust(col1_width) + \" \")\n            file.write(each.from_bus.name[:col1_width].ljust(col1_width)+\" \")\n            file.write(each.to_bus.name[:col1_width].ljust(col1_width)+\" \")\n            file.write(\"%8.2f \" % each.p_from)\n            file.write(\"%8.2f \" % each.q_from)\n            file.write(\"%8.2f \" % each.p_to)\n            file.write(\"%8.2f \" % each.q_to)\n            file.write(\"%8.2f \" % loss.real[each._i])\n            file.write(\"%8.2f \" % loss.imag[each._i])\n            file.write(\"\\n\")\n\n        # Totals\n#        file.write(\"..\".ljust(col1_width) + \" \")\n#        file.write((\"..\".ljust(col_width) + \" \")*2)\n#        file.write((\"_\"*col_width + \" \")*4 + \"\\n\")\n        file.write((\"..\".ljust(col1_width) + \" \")*3)\n        file.write((\"..\".ljust(col_width) + \" \")*3)\n        file.write(\"*Total:*\".rjust(col_width) + \" \")\n        pl, ql = report.losses\n        file.write(\"%8.2f \" % pl)\n        file.write(\"%8.2f \" % ql)\n        file.write(\"\\n\")\n\n        file.write(sep)\n\n        del report", "response": "Writes branch data to a ReST table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_generator_data(self, file):\n        report = CaseReport(self.case)\n        generators = self.case.generators\n\n        col_width = 8\n        col_width_2 = col_width*2+1\n        col1_width = 6\n        col_width_bool = 3\n        col_width_poly = 4\n        col_width_3 = col_width_poly*3+2\n\n        sep = (\"=\" * col1_width + \" \") * 2 + \\\n            (\"=\" * col_width_bool + \" \") + \\\n            (\"=\" * col_width + \" \") * 5 + \\\n            (\"=\" * col_width_poly + \" \") * 3 + \"\\n\"\n\n        file.write(sep)\n        # Line one of column headers.\n        file.write(\"Name\".center(col1_width) + \" \")\n        file.write(\"Bus\".center(col1_width) + \" \")\n        file.write(\"On\".center(col_width_bool) + \" \")\n        file.write(\"Voltage\".center(col_width) + \" \")\n        file.write(\"Pg\".center(col_width) + \" \")\n        file.write(\"Qg\".center(col_width) + \" \")\n#        file.write(\"Lambda ($/MVA-hr)\".center(col_width_2) + \" \")\n        file.write(\"Active Power\".center(col_width_2) + \" \")\n        file.write(\"Polynomial\".center(col_width_3) + \" \")\n        file.write(\"\\n\")\n\n        file.write((\"-\" * col1_width + \" \") * 2)\n        file.write((\"-\" * col_width_bool + \" \"))\n        file.write((\"-\" * col_width + \" \") * 3)\n        file.write((\"-\" * col_width_2 + \" \"))\n        file.write((\"-\" * col_width_3 + \" \") + \"\\n\")\n\n        # Line two of column header\n        file.write(\"..\".ljust(col1_width) + \" \")\n        file.write(\"..\".ljust(col1_width) + \" \")\n        file.write(\"..\".ljust(col_width_bool) + \" \")\n        file.write(\"..\".ljust(col_width) + \" \")\n        file.write(\"(MW)\".center(col_width) + \" \")\n        file.write(\"(MVAr)\".center(col_width) + \" \")\n        file.write(\"Pmax\".center(col_width) + \" \")\n        file.write(\"Pmin\".center(col_width) + \" \")\n        file.write(\"c2\".center(col_width_poly) + \" \")\n        file.write(\"c1\".center(col_width_poly) + \" \")\n        file.write(\"c0\".center(col_width_poly) + \" \")\n        file.write(\"\\n\")\n        file.write(sep)\n\n        # Branch rows.\n        for each in generators:\n            file.write(each.name[:col1_width].ljust(col1_width) + \" \")\n            file.write(\"..\".ljust(col1_width) + \" \")\n            if each.online:\n                file.write(\"1\".center(col_width_bool) + \" \")\n            else:\n                file.write(\"0\".center(col_width_bool) + \" \")\n            file.write(\"%8.2f\" % each.v_magnitude + \" \")\n            file.write(\"%8.2f\" % each.p + \" \")\n            file.write(\"%8.2f\" % each.q + \" \")\n#            file.write(\"..\".ljust(col_width) + \" \")\n#            file.write(\"..\".ljust(col_width) + \" \")\n            file.write(\"%8.2f\" % each.p_max + \" \")\n            file.write(\"%8.2f\" % each.p_min + \" \")\n            if each.pcost_model == POLYNOMIAL:\n                if len(each.p_cost) == 3:\n                    file.write(\"%4.2f %4.1f %4.0f\" % each.p_cost)\n                elif len(each.p_cost) == 2:\n                    file.write(\"0.0 %4.1f %4.0f\" % each.p_cost)\n                elif len(each.p_cost) == 1:\n                    file.write(\"0.0  0.0 %4.0f\" % each.p_cost)\n            file.write(\"\\n\")\n\n        # Totals.\n        file.write((\"..\".ljust(col1_width) +  \" \") * 2)\n        file.write((\"..\".ljust(col_width_bool) +  \" \"))\n        file.write(\"*Total:*\".rjust(col1_width) + \" \")\n        ptot = getattr(report, \"actual_pgen\")\n        qtot = getattr(report, \"actual_qgen\")\n        file.write(\"%8.2f \" % ptot)\n        file.write(\"%8.2f \" % qtot)\n        file.write((\"..\".ljust(col_width) + \" \") * 2)\n        file.write((\"..\".ljust(col_width_poly) + \" \") * 3)\n        file.write(\"\\n\")\n        file.write(sep)\n        del report", "response": "Writes generator data to a ReST table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting how many components are in a table.", "response": "def write_how_many(self, file):\n        \"\"\" Writes component numbers to a table.\n        \"\"\"\n        report = CaseReport(self.case)\n\n        # Map component labels to attribute names\n        components = [(\"Bus\", \"n_buses\"), (\"Generator\", \"n_generators\"),\n            (\"Committed Generator\", \"n_online_generators\"),\n            (\"Load\", \"n_loads\"), (\"Fixed Load\", \"n_fixed_loads\"),\n            (\"Despatchable Load\", \"n_online_vloads\"), (\"Shunt\", \"n_shunts\"),\n            (\"Branch\", \"n_branches\"), (\"Transformer\", \"n_transformers\"),\n            (\"Inter-tie\", \"n_interties\"), (\"Area\", \"n_areas\")\n        ]\n\n        # Column 1 width\n        longest = max([len(c[0]) for c in components])\n\n        col1_header = \"Object\"\n        col1_width = longest\n        col2_header = \"Quantity\"\n        col2_width = len(col2_header)\n\n        # Row separator\n        sep = \"=\"*col1_width + \" \" + \"=\"*col2_width + \"\\n\"\n\n        # Row headers\n        file.write(sep)\n\n        file.write(col1_header.center(col1_width))\n        file.write(\" \")\n        file.write(\"%s\\n\" % col2_header.center(col2_width))\n\n        file.write(sep)\n\n        # Rows\n        for label, attr in components:\n            col2_value = str(getattr(report, attr))\n            file.write(\"%s %s\\n\" %\n                (label.ljust(col1_width), col2_value.rjust(col2_width)))\n        else:\n            file.write(sep)\n            file.write(\"\\n\")\n\n        del report"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwrite how many component quantities are in a table.", "response": "def write_how_much(self, file):\n        \"\"\" Write component quantities to a table.\n        \"\"\"\n        report = CaseReport(self.case)\n\n        col1_header = \"Attribute\"\n        col1_width  = 24\n        col2_header = \"P (MW)\"\n        col3_header = \"Q (MVAr)\"\n        col_width   = 8\n\n        sep = \"=\"*col1_width +\" \"+ \"=\"*col_width +\" \"+ \"=\"*col_width + \"\\n\"\n\n        # Row headers\n        file.write(sep)\n\n        file.write(\"%s\" % col1_header.center(col1_width))\n        file.write(\" \")\n        file.write(\"%s\" % col2_header.center(col_width))\n        file.write(\" \")\n        file.write(\"%s\" % col3_header.center(col_width))\n        file.write(\"\\n\")\n\n        file.write(sep)\n\n        # Rows\n        pgen = getattr(report, \"total_pgen_capacity\")\n        qmin, qmax = getattr(report, \"total_qgen_capacity\")\n        file.write(\"%s %8.1f %4.1f to %4.1f\\n\" %\n            (\"Total Gen Capacity\".ljust(col1_width), pgen, qmin, qmax))\n\n        pgen = getattr(report, \"online_pgen_capacity\")\n        qmin, qmax = getattr(report, \"online_qgen_capacity\")\n        file.write(\"%s %8.1f %4.1f to %4.1f\\n\" %\n            (\"On-line Capacity\".ljust(col1_width), pgen, qmin, qmax))\n\n        pgen = getattr(report, \"actual_pgen\")\n        qgen = getattr(report, \"actual_qgen\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"Generation (actual)\".ljust(col1_width), pgen, qgen))\n\n        pd = getattr(report, \"p_demand\")\n        qd = getattr(report, \"q_demand\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"Load\".ljust(col1_width), pd, qd))\n\n        pd = getattr(report, \"fixed_p_demand\")\n        qd = getattr(report, \"fixed_q_demand\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"  Fixed\".ljust(col1_width), pd, qd))\n\n        pd, pmin = getattr(report, \"vload_p_demand\")\n        qd = getattr(report, \"vload_q_demand\")\n        file.write(\"%s %4.1f of %4.1f %8.1f\\n\" %\n            (\"  Despatchable\".ljust(col1_width), pd, pmin, qd))\n\n        pinj = getattr(report, \"shunt_pinj\")\n        qinj = getattr(report, \"shunt_qinj\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"Shunt (inj)\".ljust(col1_width), pinj, qinj))\n\n        pl, ql = getattr(report, \"losses\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"Losses (I^2 * Z)\".ljust(col1_width), pl, ql))\n\n        qinj = getattr(report, \"branch_qinj\")\n        file.write(\"%s %8s %8.1f\\n\" %\n            (\"Branch Charging (inj)\".ljust(col1_width), \"-\", qinj))\n\n        pval = getattr(report, \"total_tie_pflow\")\n        qval = getattr(report, \"total_tie_qflow\")\n        file.write(\"%s %8.1f %8.1f\\n\" %\n            (\"Total Inter-tie Flow\".ljust(col1_width), pval, qval))\n\n        file.write(sep)\n        file.write(\"\\n\")\n\n        del report"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_min_max(self, file):\n        report = CaseReport(self.case)\n\n        col1_header = \"Attribute\"\n        col1_width  = 19\n        col2_header = \"Minimum\"\n        col3_header = \"Maximum\"\n        col_width   = 22\n\n        sep = \"=\"*col1_width +\" \"+ \"=\"*col_width +\" \"+ \"=\"*col_width + \"\\n\"\n\n        # Row headers\n        file.write(sep)\n\n        file.write(\"%s\" % col1_header.center(col1_width))\n        file.write(\" \")\n        file.write(\"%s\" % col2_header.center(col_width))\n        file.write(\" \")\n        file.write(\"%s\" % col3_header.center(col_width))\n        file.write(\"\\n\")\n\n        file.write(sep)\n\n        # Rows\n        min_val, min_i = getattr(report, \"min_v_magnitude\")\n        max_val, max_i = getattr(report, \"max_v_magnitude\")\n        file.write(\"%s %7.3f p.u. @ bus %2d %7.3f p.u. @ bus %2d\\n\" %\n            (\"Voltage Amplitude\".ljust(col1_width),\n             min_val, min_i, max_val, max_i))\n\n        min_val, min_i = getattr(report, \"min_v_angle\")\n        max_val, max_i = getattr(report, \"max_v_angle\")\n        file.write(\"%s %16.3f %16.3f\\n\" %\n            (\"Voltage Phase Angle\".ljust(col1_width), min_val, max_val))\n\n        file.write(sep)\n        file.write(\"\\n\")\n\n        del report", "response": "Writes minimum and maximum values to a table."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a unique name based on the specified base name and a sequence of existing names.", "response": "def make_unique_name(base, existing=[], format=\"%s_%s\"):\n    \"\"\" Return a name, unique within a context, based on the specified name.\n\n    @param base: the desired base name of the generated unique name.\n    @param existing: a sequence of the existing names to avoid returning.\n    @param format: a formatting specification for how the name is made unique.\n    \"\"\"\n    count = 2\n    name = base\n    while name in existing:\n        name = format % (base, count)\n        count += 1\n\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls antlr4 on grammar file", "response": "def call_antlr4(arg):\n    \"calls antlr4 on grammar file\"\n    # pylint: disable=unused-argument, unused-variable\n    antlr_path = os.path.join(ROOT_DIR, \"java\", \"antlr-4.7-complete.jar\")\n    classpath = os.pathsep.join([\".\", \"{:s}\".format(antlr_path), \"$CLASSPATH\"])\n    generated = os.path.join(ROOT_DIR, 'src', 'pymoca', 'generated')\n    cmd = \"java -Xmx500M -cp \\\"{classpath:s}\\\" org.antlr.v4.Tool {arg:s}\" \\\n          \" -o {generated:s} -visitor -Dlanguage=Python3\".format(**locals())\n    print(cmd)\n    proc = subprocess.Popen(cmd.split(), cwd=os.path.join(ROOT_DIR, 'src', 'pymoca'))\n    proc.communicate()\n    with open(os.path.join(generated, '__init__.py'), 'w') as fid:\n        fid.write('')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the dialog body. Returns the widget that should have initial focus.", "response": "def body(self, frame):\n        \"\"\" Creates the dialog body. Returns the widget that should have\n            initial focus.\n        \"\"\"\n        master = Frame(self)\n        master.pack(padx=5, pady=0, expand=1, fill=BOTH)\n\n        title = Label(master, text=\"Buses\")\n        title.pack(side=TOP)\n\n        bus_lb = self.bus_lb = Listbox(master, selectmode=SINGLE, width=10)\n        bus_lb.pack(side=LEFT)\n\n        for bus in self.case.buses:\n            bus_lb.insert(END, bus.name)\n\n        bus_lb.bind(\"<<ListboxSelect>>\", self.on_bus)\n\n        self.bus_params = BusProperties(master)\n\n        return bus_lb"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\napply the data to the current bus.", "response": "def apply(self):\n        ''' Process the data. This method is called automatically to process\n            the data, *after* the dialog is destroyed.\n        '''\n        bus = self.case.buses[int(self.bus_lb.curselection()[0])]\n\n        for attr in [a for a in BUS_ATTRS if a not in self.excluded+['mode']]:\n            value = getattr(self.bus_params, attr).get()\n            setattr(bus, attr, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsolves an optimal power flow and returns a results dictionary.", "response": "def solve(self, solver_klass=None):\n        \"\"\" Solves an optimal power flow and returns a results dictionary.\n        \"\"\"\n        # Start the clock.\n        t0 = time()\n\n        # Build an OPF model with variables and constraints.\n        om = self._construct_opf_model(self.case)\n        if om is None:\n            return {\"converged\": False, \"output\": {\"message\": \"No Ref Bus.\"}}\n\n        # Call the specific solver.\n#        if self.opt[\"verbose\"]:\n#            print '\\nPYLON Version %s, %s', \"0.4.2\", \"April 2010\"\n        if solver_klass is not None:\n            result = solver_klass(om, opt=self.opt).solve()\n        elif self.dc:\n#            if self.opt[\"verbose\"]:\n#                print ' -- DC Optimal Power Flow\\n'\n            result = DCOPFSolver(om, opt=self.opt).solve()\n        else:\n#            if self.opt[\"verbose\"]:\n#                print ' -- AC Optimal Power Flow\\n'\n            result = PIPSSolver(om, opt=self.opt).solve()\n\n        result[\"elapsed\"] = time() - t0\n\n        if self.opt.has_key(\"verbose\"):\n            if self.opt[\"verbose\"]:\n                logger.info(\"OPF completed in %.3fs.\" % result[\"elapsed\"])\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _construct_opf_model(self, case):\n        # Zero the case result attributes.\n        self.case.reset()\n\n        base_mva = case.base_mva\n\n        # Check for one reference bus.\n        oneref, refs = self._ref_check(case)\n        if not oneref: #return {\"status\": \"error\"}\n            None\n\n        # Remove isolated components.\n        bs, ln, gn = self._remove_isolated(case)\n\n        # Update bus indexes.\n        self.case.index_buses(bs)\n\n        # Convert single-block piecewise-linear costs into linear polynomial.\n        gn = self._pwl1_to_poly(gn)\n\n        # Set-up initial problem variables.\n        Va = self._get_voltage_angle_var(refs, bs)\n        Pg = self._get_pgen_var(gn, base_mva)\n\n        if self.dc: # DC model.\n            # Get the susceptance matrices and phase shift injection vectors.\n            B, Bf, Pbusinj, Pfinj = self.case.makeBdc(bs, ln)\n\n            # Power mismatch constraints (B*Va + Pg = Pd).\n            Pmis = self._power_mismatch_dc(bs, gn, B, Pbusinj, base_mva)\n\n            # Branch flow limit constraints.\n            Pf, Pt = self._branch_flow_dc(ln, Bf, Pfinj, base_mva)\n        else:\n            # Set-up additional AC-OPF problem variables.\n            Vm = self._get_voltage_magnitude_var(bs, gn)\n            Qg = self._get_qgen_var(gn, base_mva)\n\n            Pmis, Qmis, Sf, St = self._nln_constraints(len(bs), len(ln))\n\n            vl = self._const_pf_constraints(gn, base_mva)\n\n            # TODO: Generator PQ capability curve constraints.\n#            PQh, PQl = self._pq_capability_curve_constraints(gn)\n\n        # Branch voltage angle difference limits.\n        ang = self._voltage_angle_diff_limit(bs, ln)\n\n        if self.dc:\n            vars = [Va, Pg]\n            constraints = [Pmis, Pf, Pt, ang]\n        else:\n            vars = [Va, Vm, Pg, Qg]\n            constraints = [Pmis, Qmis, Sf, St, #PQh, PQL,\n                           vl, ang]\n\n        # Piece-wise linear generator cost constraints.\n        y, ycon = self._pwl_gen_costs(gn, base_mva)\n\n        if ycon is not None:\n            vars.append(y)\n            constraints.append(ycon)\n\n        # Add variables and constraints to the OPF model object.\n        opf = OPFModel(case)\n        opf.add_vars(vars)\n        opf.add_constraints(constraints)\n\n        if self.dc: # user data\n            opf._Bf = Bf\n            opf._Pfinj = Pfinj\n\n        return opf", "response": "Constructs an OPF model for a single - block case."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck that there is only one reference bus.", "response": "def _ref_check(self, case):\n        \"\"\" Checks that there is only one reference bus.\n        \"\"\"\n        refs = [bus._i for bus in case.buses if bus.type == REFERENCE]\n\n        if len(refs) == 1:\n            return True, refs\n        else:\n            logger.error(\"OPF requires a single reference bus.\")\n            return False, refs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _remove_isolated(self, case):\n#        case.deactivate_isolated()\n        buses = case.connected_buses\n        branches = case.online_branches\n        gens = case.online_generators\n\n        return buses, branches, gens", "response": "Removes isolated case components."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _pwl1_to_poly(self, generators):\n        for g in generators:\n            if (g.pcost_model == PW_LINEAR) and (len(g.p_cost) == 2):\n                g.pwl_to_poly()\n\n        return generators", "response": "Converts single - block piecewise - linear costs into linear\n        polynomial."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_voltage_angle_var(self, refs, buses):\n        Va = array([b.v_angle * (pi / 180.0) for b in buses])\n\n        Vau = Inf * ones(len(buses))\n        Val = -Vau\n        Vau[refs] = Va[refs]\n        Val[refs] = Va[refs]\n\n        return Variable(\"Va\", len(buses), Va, Val, Vau)", "response": "Returns the voltage angle variable set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_voltage_magnitude_var(self, buses, generators):\n        Vm = array([b.v_magnitude for b in buses])\n\n        # For buses with generators initialise Vm from gen data.\n        for g in generators:\n            Vm[g.bus._i] = g.v_magnitude\n\n        Vmin = array([b.v_min for b in buses])\n        Vmax = array([b.v_max for b in buses])\n\n        return Variable(\"Vm\", len(buses), Vm, Vmin, Vmax)", "response": "Returns the voltage magnitude variable set."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the active power set - point variable.", "response": "def _get_pgen_var(self, generators, base_mva):\n        \"\"\" Returns the generator active power set-point variable.\n        \"\"\"\n        Pg = array([g.p / base_mva for g in generators])\n\n        Pmin = array([g.p_min / base_mva for g in generators])\n        Pmax = array([g.p_max / base_mva for g in generators])\n\n        return Variable(\"Pg\", len(generators), Pg, Pmin, Pmax)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the generator reactive power variable set.", "response": "def _get_qgen_var(self, generators, base_mva):\n        \"\"\" Returns the generator reactive power variable set.\n        \"\"\"\n        Qg = array([g.q / base_mva for g in generators])\n\n        Qmin = array([g.q_min / base_mva for g in generators])\n        Qmax = array([g.q_max / base_mva for g in generators])\n\n        return Variable(\"Qg\", len(generators), Qg, Qmin, Qmax)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _nln_constraints(self, nb, nl):\n        Pmis = NonLinearConstraint(\"Pmis\", nb)\n        Qmis = NonLinearConstraint(\"Qmis\", nb)\n        Sf = NonLinearConstraint(\"Sf\", nl)\n        St = NonLinearConstraint(\"St\", nl)\n\n        return Pmis, Qmis, Sf, St", "response": "Returns non - linear constraints for OPF.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _power_mismatch_dc(self, buses, generators, B, Pbusinj, base_mva):\n        nb, ng = len(buses), len(generators)\n        # Negative bus-generator incidence matrix.\n        gen_bus = array([g.bus._i for g in generators])\n        neg_Cg = csr_matrix((-ones(ng), (gen_bus, range(ng))), (nb, ng))\n\n        Amis = hstack([B, neg_Cg], format=\"csr\")\n\n        Pd = array([bus.p_demand for bus in buses])\n        Gs = array([bus.g_shunt for bus in buses])\n\n        bmis = -(Pd - Gs) / base_mva - Pbusinj\n\n        return LinearConstraint(\"Pmis\", Amis, bmis, bmis, [\"Va\", \"Pg\"])", "response": "Returns the power mismatch constraint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _branch_flow_dc(self, branches, Bf, Pfinj, base_mva):\n        # Indexes of constrained lines.\n        il = array([i for i,l in enumerate(branches) if 0.0 < l.rate_a < 1e10])\n        lpf = -Inf * ones(len(il))\n        rate_a = array([l.rate_a / base_mva for l in branches])\n        upf = rate_a[il] - Pfinj[il]\n        upt = rate_a[il] + Pfinj[il]\n\n        Pf = LinearConstraint(\"Pf\",  Bf[il, :], lpf, upf, [\"Va\"])\n        Pt = LinearConstraint(\"Pt\", -Bf[il, :], lpf, upt, [\"Va\"])\n\n        return Pf, Pt", "response": "Returns the branch flow limit constraint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _const_pf_constraints(self, gn, base_mva):\n        ivl = array([i for i, g in enumerate(gn)\n                     if g.is_load and (g.q_min != 0.0 or g.q_max != 0.0)])\n        vl = [gn[i] for i in ivl]\n        nvl = len(vl)\n\n        ng = len(gn)\n        Pg = array([g.p for g in vl]) / base_mva\n        Qg = array([g.q for g in vl]) / base_mva\n        Pmin = array([g.p_min for g in vl]) / base_mva\n        Qmin = array([g.q_min for g in vl]) / base_mva\n        Qmax = array([g.q_max for g in vl]) / base_mva\n\n        # At least one of the Q limits must be zero (corresponding to Pmax==0).\n        for g in vl:\n            if g.qmin != 0.0 and g.q_max != 0.0:\n                logger.error(\"Either Qmin or Qmax must be equal to zero for \"\n                \"each dispatchable load.\")\n\n        # Initial values of PG and QG must be consistent with specified power\n        # factor. This is to prevent a user from unknowingly using a case file\n        # which would have defined a different power factor constraint under a\n        # previous version which used PG and QG to define the power factor.\n        Qlim = (Qmin == 0.0) * Qmax + (Qmax == 0.0) * Qmin\n        if any( abs(Qg - Pg * Qlim / Pmin) > 1e-6 ):\n            logger.error(\"For a dispatchable load, PG and QG must be \"\n                         \"consistent with the power factor defined by \"\n                         \"PMIN and the Q limits.\")\n\n        # Make Avl, lvl, uvl, for lvl <= Avl * r_[Pg, Qg] <= uvl\n        if nvl > 0:\n            xx = Pmin\n            yy = Qlim\n            pftheta = arctan2(yy, xx)\n            pc = sin(pftheta)\n            qc = -cos(pftheta)\n            ii = array([range(nvl), range(nvl)])\n            jj = r_[ivl, ivl + ng]\n            Avl = csr_matrix(r_[pc, qc], (ii, jj), (nvl, 2 * ng))\n            lvl = zeros(nvl)\n            uvl = lvl\n        else:\n            Avl = zeros((0, 2 * ng))\n            lvl = array([])\n            uvl = array([])\n\n        return LinearConstraint(\"vl\", Avl, lvl, uvl, [\"Pg\", \"Qg\"])", "response": "Returns a linear constraint enforcing constant power factor for all dispatchable loads."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _voltage_angle_diff_limit(self, buses, branches):\n        nb = len(buses)\n\n        if not self.ignore_ang_lim:\n            iang = [i for i, b in enumerate(branches)\n                    if (b.ang_min and (b.ang_min > -360.0))\n                    or (b.ang_max and (b.ang_max < 360.0))]\n            iangl = array([i for i, b in enumerate(branches)\n                     if b.ang_min is not None])[iang]\n            iangh = array([i for i, b in enumerate(branches)\n                           if b.ang_max is not None])[iang]\n            nang = len(iang)\n\n            if nang > 0:\n                ii = range(nang) + range(nang)\n                jjf = array([b.from_bus._i for b in branches])[iang]\n                jjt = array([b.to_bus._i for b in branches])[iang]\n                jj = r_[jjf, jjt]\n                Aang = csr_matrix(r_[ones(nang), -ones(nang)], (ii, jj))\n                uang = Inf * ones(nang)\n                lang = -uang\n                lang[iangl] = array([b.ang_min * (pi / 180.0)\n                                    for b in branches])[iangl]\n                uang[iangh] = array([b.ang_max * (pi / 180.0)\n                                    for b in branches])[iangh]\n            else:\n#                Aang = csr_matrix((0, nb), dtype=float64)\n#                lang = array([], dtype=float64)\n#                uang = array([], dtype=float64)\n                Aang = zeros((0, nb))\n                lang = array([])\n                uang = array([])\n        else:\n#            Aang = csr_matrix((0, nb), dtype=float64)\n#            lang = array([], dtype=float64)\n#            uang = array([], dtype=float64)\n#            iang = array([], dtype=float64)\n            Aang = zeros((0, nb))\n            lang = array([])\n            uang = array([])\n\n        return LinearConstraint(\"ang\", Aang, lang, uang, [\"Va\"])", "response": "Returns the constraint on the voltage angle differences."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _pwl_gen_costs(self, generators, base_mva):\n        ng = len(generators)\n        gpwl = [g for g in generators if g.pcost_model == PW_LINEAR]\n#        nq = len([g for g in gpwl if g.qcost_model is not None])\n\n        if self.dc:\n            pgbas = 0        # starting index within x for active sources\n            nq = 0           # number of Qg vars\n#            qgbas = None     # index of 1st Qg column in Ay\n            ybas = ng        # starting index within x for y variables\n        else:\n            pgbas = 0\n            nq = ng\n#            qgbas = ng + 1 # index of 1st Qg column in Ay\n            ybas = ng + nq\n\n        # Number of extra y variables.\n        ny = len(gpwl)\n\n        if ny == 0:\n            return None, None\n\n        # Total number of cost points.\n        nc = len([co for gn in gpwl for co in gn.p_cost])\n#        Ay = lil_matrix((nc - ny, ybas + ny))\n        # Fill rows and then transpose.\n        Ay = lil_matrix((ybas + ny, nc - ny))\n        by = array([])\n\n        j = 0\n        k = 0\n        for i, g in enumerate(gpwl):\n            # Number of cost points: segments = ns-1\n            ns = len(g.p_cost)\n\n            p = array([x / base_mva for x, c in g.p_cost])\n            c = array([c for x, c in g.p_cost])\n            m = diff(c) / diff(p)        # Slopes for Pg (or Qg).\n\n            if 0.0 in diff(p):\n                raise ValueError, \"Bad Pcost data: %s (%s)\" % (p, g.name)\n                logger.error(\"Bad Pcost data: %s\" % p)\n\n            b = m * p[:ns-1] - c[:ns-1] # rhs\n            by = r_[by, b.T]\n\n#            if i > ng:\n#                sidx = qgbas + (i-ng) - 1       # this was for a q cost\n#            else:\n#                sidx = pgbas + i - 1            # this was for a p cost\n\n            Ay[pgbas + i, k:k + ns - 1] = m\n\n            # FIXME: Repeat for Q costs.\n\n            # Now fill the y rows with -1's\n            Ay[ybas + j, k:k + ns - 1] = -ones(ns-1)\n\n            k += (ns - 1)\n            j += 1\n\n        y = Variable(\"y\", ny)\n\n        # Transpose Ay since lil_matrix stores in rows.\n        if self.dc:\n            ycon = LinearConstraint(\"ycon\", Ay.T, None, by, [\"Pg\", \"y\"])\n        else:\n            ycon = LinearConstraint(\"ycon\", Ay.T, None, by, [\"Pg\", \"Qg\",\"y\"])\n\n        return y, ycon", "response": "Returns the basin constraints for piece - wise linear gen cost for the given set of generators and base MVA."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsolve the combined unit decommitment problem.", "response": "def solve(self, solver_klass=None):\n        \"\"\" Solves the combined unit decommitment / optimal power flow problem.\n        \"\"\"\n        case = self.case\n        generators = case.online_generators\n\n        logger.info(\"Solving OPF with unit de-commitment [%s].\" % case.name)\n\n        t0 = time()\n\n        # 1. Begin at stage zero (N = 0), assuming all generators are on-line\n        # with all limits in place. At most one generator shutdown per stage.\n        i_stage = 0\n\n        # Check for sum(p_min) > total load, decommit as necessary.\n        online = [g for g in generators if not g.is_load]\n        online_vload = [g for g in generators if g.is_load]\n\n        # Total dispatchable load capacity.\n        vload_capacity = sum([g.p_min for g in online_vload])\n        # Total load capacity.\n        load_capacity = sum([b.p_demand for b in case.buses]) - vload_capacity\n\n        # Minimum total online generation capacity.\n        p_min_tot = sum([g.p_min for g in online])\n\n        # Shutdown the most expensive units until the minimum generation\n        # capacity is less than the total load capacity.\n        while p_min_tot > load_capacity:\n            i_stage += 1\n            logger.debug(\"De-commitment stage %d.\" % i_stage)\n\n            # Find generator with the maximum average cost at Pmin.\n            avg_pmin_cost = [g.total_cost(g.p_min) / g.p_min for g in online]\n            # Select at random from maximal generators with equal cost.\n            g_idx, _ = fair_max(avg_pmin_cost)\n            generator = online[g_idx]\n\n            logger.info(\"Shutting down generator [%s] to satisfy all \"\n                        \"p_min limits.\" % generator.name)\n\n            # Shut down most expensive unit.\n            generator.online = False\n\n            # Update minimum generation capacity for while loop.\n            online = [g for g in case.online_generators if not g.is_load]\n            p_min_tot = sum([g.p_min for g in online])\n\n        # 2. Solve a normal OPF and save the solution as the current best.\n        solution = super(UDOPF, self).solve(solver_klass)\n\n        logger.debug(\"Initial system cost: $%.3f\" % solution[\"f\"])\n\n        if not solution[\"converged\"] == True:\n            logger.error(\"Non-convergent UDOPF [%s].\" %\n                         solution[\"output\"][\"message\"])\n            return solution\n\n        # 3. Go to the next stage, N = N + 1. Using the best solution from the\n        # previous stage as the base case for this stage, ...\n\n        # Best case so far. A list of the on-line status of all generators.\n        overall_online = [g.online for g in case.generators]\n        # The objective function value is the total system cost.\n        overall_cost = solution[\"f\"]\n\n        # Best case for this stage.\n        stage_online = overall_online\n        stage_cost = overall_cost\n\n        # Shutdown at most one generator per stage.\n        while True:\n            # 4. Form a candidate list of generators with minimum\n            # generation limits binding.\n\n            # Activate generators according to the stage best.\n            for i, generator in enumerate(case.generators):\n                generator.online = stage_online[i]\n\n            # Get candidates for shutdown. Lagrangian multipliers are often\n            # very small so we round to four decimal places.\n            candidates = [g for g in case.online_generators if \\\n                          (round(g.mu_pmin, 4) > 0.0) and (g.p_min > 0.0)]\n\n            if len(candidates) == 0:\n                break\n\n            # Assume no improvement during this stage.\n            done = True\n\n            i_stage += 1\n            logger.debug(\"De-commitment stage %d.\" % i_stage)\n\n            for candidate in candidates:\n                # 5. For each generator on the candidate list, solve an OPF to\n                # find the total system cost with the generator shut down.\n\n                # Activate generators according to the stage best.\n                for i, generator in enumerate(case.generators):\n                    generator.online = stage_online[i]\n\n                # Shutdown candidate generator.\n                candidate.online = False\n\n                logger.debug(\"Solving OPF with generator '%s' shutdown.\" %\n                    candidate.name)\n\n                # Run OPF.\n                solution = super(UDOPF, self).solve(solver_klass)\n\n                # Compare total system costs for improvement.\n                if solution[\"converged\"] == True \\\n                    and (solution[\"f\"] < overall_cost):\n                    logger.debug(\"System cost improvement: $%.3f ($%.3f)\" %\n                                 (stage_cost - solution[\"f\"], solution[\"f\"]))\n                    # 6. Replace the current best solution with this one if\n                    # it has a lower cost.\n                    overall_online = [g.online for g in case.generators]\n                    overall_cost = solution[\"f\"]\n                    best_candidate = candidate\n                    # Check for further decommitment.\n                    done = False\n                else:\n                    logger.debug(\"Candidate OPF failed [%s].\" %\n                                 solution[\"output\"][\"message\"])\n\n                # Reactivate the candidate before deactivating the next.\n#                candidate.online = True\n\n            if done:\n                # Decommits at this stage did not help.\n                break\n            else:\n                # 7. If any of the candidate solutions produced an improvement,\n                # return to step 3.\n\n                # Shutting something else down helps, so let's keep going.\n                logger.info(\"Shutting down generator '%s'.\",\n                            best_candidate.name)\n\n                stage_online = overall_online\n                stage_cost = overall_cost\n\n        # 8. Use the best overall solution as the final solution.\n        for i, generator in enumerate(case.generators):\n            generator.online = overall_online[i]\n\n        # One final solve using the best case to ensure all results are\n        # up-to-date.\n        solution = super(UDOPF, self).solve(solver_klass)\n\n        logger.debug(\"UDOPF system cost: $%.3f\" % solution[\"f\"])\n\n        # Compute elapsed time and log it.\n        elapsed = time() - t0\n\n        plural = \"\" if i_stage == 1 else \"s\"\n        logger.info(\"Unit decommitment OPF solved in %.3fs (%d decommitment \"\n                    \"stage%s).\" % (elapsed, i_stage, plural))\n\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_var(self, var):\n        if var.name in [v.name for v in self.vars]:\n            logger.error(\"Variable set named '%s' already exists.\" % var.name)\n            return\n\n        var.i1 = self.var_N\n        var.iN = self.var_N + var.N - 1\n        self.vars.append(var)", "response": "Adds a variable to the model."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the variable set with the given name.", "response": "def get_var(self, name):\n        \"\"\" Returns the variable set with the given name.\n        \"\"\"\n        for var in self.vars:\n            if var.name == name:\n                return var\n        else:\n            raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef linear_constraints(self):\n        if self.lin_N == 0:\n            return None, array([]), array([])\n\n        A = lil_matrix((self.lin_N, self.var_N), dtype=float64)\n        l = -Inf * ones(self.lin_N)\n        u = -l\n\n        for lin in self.lin_constraints:\n            if lin.N:                   # non-zero number of rows to add\n                Ak = lin.A              # A for kth linear constrain set\n                i1 = lin.i1             # starting row index\n                iN = lin.iN             # ending row index\n                vsl = lin.vs            # var set list\n                kN = -1                 # initialize last col of Ak used\n                Ai = lil_matrix((lin.N, self.var_N), dtype=float64)\n                for v in vsl:\n                    var = self.get_var(v)\n                    j1 = var.i1         # starting column in A\n                    jN = var.iN         # ending column in A\n                    k1 = kN + 1         # starting column in Ak\n                    kN = kN + var.N     # ending column in Ak\n\n                    if j1 == jN:\n                        # FIXME: Single column slicing broken in lil.\n                        for i in range(Ai.shape[0]):\n                            Ai[i, j1] = Ak[i, k1]\n                    else:\n                        Ai[:, j1:jN + 1] = Ak[:, k1:kN + 1]\n\n                A[i1:iN + 1, :] = Ai\n                l[i1:iN + 1] = lin.l\n                u[i1:iN + 1] = lin.u\n\n        return A.tocsr(), l, u", "response": "Returns the linear constraints."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_constraint(self, con):\n        if isinstance(con, LinearConstraint):\n            N, M = con.A.shape\n            if con.name in [c.name for c in self.lin_constraints]:\n                logger.error(\"Constraint set named '%s' already exists.\"\n                             % con.name)\n                return False\n            else:\n                con.i1 = self.lin_N# + 1\n                con.iN = self.lin_N + N - 1\n\n                nv = 0\n                for vs in con.vs:\n                    nv = nv + self.get_var_N(vs)\n                if M != nv:\n                    logger.error(\"Number of columns of A does not match number\"\n                        \" of variables, A is %d x %d, nv = %d\", N, M, nv)\n                self.lin_constraints.append(con)\n        elif isinstance(con, NonLinearConstraint):\n            N = con.N\n            if con.name in [c.name for c in self.nln_constraints]:\n                logger.error(\"Constraint set named '%s' already exists.\"\n                             % con.name)\n                return False\n            else:\n                con.i1 = self.nln_N# + 1\n                con.iN = self.nln_N + N\n                self.nln_constraints.append(con)\n        else:\n            raise ValueError\n\n        return True", "response": "Adds a constraint to the model."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_lin_constraint(self, name):\n        for c in self.lin_constraints:\n            if c.name == name:\n                return c\n        else:\n            raise ValueError", "response": "Returns the constraint set with the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the constraint set with the given name.", "response": "def get_nln_constraint(self, name):\n        \"\"\" Returns the constraint set with the given name.\n        \"\"\"\n        for c in self.nln_constraints:\n            if c.name == name:\n                return c\n        else:\n            raise ValueError"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef linearize(self, x0: np.array=None, u0: np.array=None) -> List[np.array]:\n        ss = self.linearize_symbolic()\n        ss_eval = []\n        ss_subs = {}\n        if x0 is None:\n            # noinspection PyUnusedLocal\n            x0 = self.x.subs(self.x0)[:]\n        if u0 is None:\n            # noinspection PyUnusedLocal\n            u0 = self.u.subs(self.u0)[:]\n        # note, we don't substitute y here since\n        # all equations should be in terms of x, u\n        # if you substitute y, it will resubstitute\n        # over x and cause issues\n        ss_subs.update({self.x[i]: x0[i] for i in range(len(self.x))})\n        ss_subs.update({self.u[i]: u0[i] for i in range(len(self.u))})\n        ss_subs.update(self.p0)\n        ss_subs.update(self.c0)\n        for i in range(len(ss)):\n            ss_eval += [np.matrix(ss[i].subs(ss_subs)).astype(float)]\n        return ss_eval", "response": "Linearization of the Jacobians."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_all_property_mappings(encoder: MappingJSONEncoder, property_mappings: Iterable[JsonPropertyMapping],\n                               superclasses: Tuple[PropertyMapper]) -> List[JsonPropertyMapping]:\n    \"\"\"\n    Gets all of the property mappings from the given property mapper, considering the property mappings for self and the\n    property mappings defined by the superclass.\n    :param encoder: `self` when binded as class method\n    :param property_mappings: mappings defined for the given encoder, excluding mappings defined by superclasses\n    :param superclasses: superclasses of the given encoder. Property mappers in later superclasses may override the\n    effects of property mappers defined by superclasses closer to the start of the list\n    :return: all of the property mappings for the given encoder\n    \"\"\"\n    mappings = []\n    for superclass in superclasses:\n        super_mappings = superclass._get_property_mappings(superclass)\n        mappings.extend(super_mappings)\n\n    # Add property mappings of own to end of the mappings list\n    mappings.extend(property_mappings)\n\n    # Note: It is very difficult to cull all property mappers that target the same properties, leaving only the ones\n    # from the lowest class in the hierarchy. This is because such mappers may be encoded as functions. Given that such\n    # overloading is unlikely to be used much and the cost of doing a mapping and then mapping again over the top of it\n    # will likely be small, there will be no attempt of such a cull.\n    return mappings", "response": "Returns all of the property mappings defined by the given encoder and all superclasses."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a new class of MappingJSONEncoder.", "response": "def build(self) -> type:\n        \"\"\"\n        Build a subclass of `MappingJSONEncoder`.\n        :return: the built subclass\n        \"\"\"\n        def _get_property_mappings(encoder: MappingJSONEncoder) -> List[JsonPropertyMapping]:\n            return _get_all_property_mappings(encoder, self.mappings, self.superclasses)\n\n        def get_serializable_cls(encoder: MappingJSONEncoder) -> type:\n            return self.target_cls\n\n        def default(encoder: MappingJSONEncoder, serializable):\n            if serializable is None:\n                # Fix for #18\n                return None\n            elif isinstance(serializable, List):\n                # Fix for #8\n                return [encoder.default(item) for item in serializable]\n            else:\n                # Sort subclasses so subclass' default method is called last\n                superclasses_as_list = list(self.superclasses)\n                superclasses_as_list.sort(key=lambda superclass: 1 if superclass == MappingJSONEncoder else -1)\n\n                encoded_combined = {}\n                for superclass in superclasses_as_list:\n                    encoded = superclass.default(encoder, serializable)\n                    encoded_combined.update(encoded)\n\n                return encoded_combined\n\n        return type(\n            \"%sDynamicMappingJSONEncoder\" % self.target_cls.__name__,\n            self.superclasses,\n            {\n                \"_get_property_mappings\": _get_property_mappings,\n                \"_get_serializable_cls\": get_serializable_cls,\n                \"default\": default\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self) -> type:\n        def _get_property_mappings(encoder: MappingJSONEncoder) -> List[JsonPropertyMapping]:\n            return _get_all_property_mappings(encoder, self.mappings, self.superclasses)\n\n        def get_deserializable_cls(decoder: MappingJSONDecoder) -> type:\n            return self.target_cls\n\n        return type(\n            \"%sDynamicMappingJSONDecoder\" % self.target_cls.__name__,\n            self.superclasses,\n            {\n                \"_get_property_mappings\": _get_property_mappings,\n                \"_get_deserializable_cls\": get_deserializable_cls\n            }\n        )", "response": "Build a subclass of MappingJSONDecoder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsolves using the Interior Point OPTimizer.", "response": "def _solve(self, x0, A, l, u, xmin, xmax):\n        \"\"\" Solves using the Interior Point OPTimizer.\n        \"\"\"\n        # Indexes of constrained lines.\n        il = [i for i,ln in enumerate(self._ln) if 0.0 < ln.rate_a < 1e10]\n        nl2 = len(il)\n\n        neqnln = 2 * self._nb # no. of non-linear equality constraints\n        niqnln = 2 * len(il)  # no. of lines with constraints\n\n        user_data = {\"A\": A, \"neqnln\": neqnln, \"niqnln\": niqnln}\n\n        self._f(x0)\n        Jdata = self._dg(x0, False, user_data)\n#        Hdata = self._h(x0, ones(neqnln + niqnln), None, False, user_data)\n\n        lmbda = {\"eqnonlin\": ones(neqnln),\n                 \"ineqnonlin\": ones(niqnln)}\n        H = tril(self._hessfcn(x0, lmbda), format=\"coo\")\n        self._Hrow, self._Hcol = H.row, H.col\n\n        n = len(x0) # the number of variables\n        xl = xmin\n        xu = xmax\n        gl = r_[zeros(2 * self._nb), -Inf * ones(2 * nl2), l]\n        gu = r_[zeros(2 * self._nb),       zeros(2 * nl2), u]\n        m = len(gl) # the number of constraints\n        nnzj = len(Jdata) # the number of nonzeros in Jacobian matrix\n        nnzh = 0#len(H.data) # the number of non-zeros in Hessian matrix\n\n        f_fcn, df_fcn, g_fcn, dg_fcn, h_fcn = \\\n            self._f, self._df, self._g, self._dg, self._h\n\n        nlp = pyipopt.create(n, xl, xu, m, gl, gu, nnzj, nnzh,\n                             f_fcn, df_fcn, g_fcn, dg_fcn)#, h_fcn)\n\n#        print dir(nlp)\n#        nlp.str_option(\"print_options_documentation\", \"yes\")\n#        nlp.int_option(\"max_iter\", 10)\n\n#        x, zl, zu, obj = nlp.solve(x0)\n        success = nlp.solve(x0, user_data)\n        nlp.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reset_case(self):\n        for bus in self.market.case.buses:\n            bus.p_demand = self.pdemand[bus]\n        for task in self.tasks:\n            for g in task.env.generators:\n                g.p = task.env._g0[g][\"p\"]\n                g.p_max = task.env._g0[g][\"p_max\"]\n                g.p_min = task.env._g0[g][\"p_min\"]\n                g.q = task.env._g0[g][\"q\"]\n                g.q_max = task.env._g0[g][\"q_max\"]\n                g.q_min = task.env._g0[g][\"q_min\"]\n                g.p_cost = task.env._g0[g][\"p_cost\"]\n                g.pcost_model = task.env._g0[g][\"pcost_model\"]\n                g.q_cost = task.env._g0[g][\"q_cost\"]\n                g.qcost_model = task.env._g0[g][\"qcost_model\"]\n                g.c_startup = task.env._g0[g][\"startup\"]\n                g.c_shutdown = task.env._g0[g][\"shutdown\"]", "response": "Resets the case to its original state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef doEpisodes(self, number=1):\n        for episode in range(number):\n            print \"Starting episode %d.\" % episode\n\n            # Initialise the profile cycle.\n            if len(self.profile.shape) == 1: # 1D array\n                self._pcycle = cycle(self.profile)\n            else:\n                assert self.profile.shape[0] >= number\n                self._pcycle = cycle(self.profile[episode, :])\n\n            # Scale the initial load.\n            c = self._pcycle.next()\n            for bus in self.market.case.buses:\n                bus.p_demand = self.pdemand[bus] * c\n\n            # Initialise agents and their tasks.\n            for task, agent in zip(self.tasks, self.agents):\n                agent.newEpisode()\n                task.reset()\n\n            while False in [task.isFinished() for task in self.tasks]:\n                if True in [task.isFinished() for task in self.tasks]:\n                    raise ValueError\n                self._oneInteraction()\n\n        self.reset_case()", "response": "Do the given numer of episodes and return the rewards of each episode in the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nperform one interaction between each agent and the environment.", "response": "def _oneInteraction(self):\n        \"\"\" Coordinates one interaction between each agent and its environment.\n        \"\"\"\n        self.stepid += 1\n\n        logger.info(\"Entering simulation period %d.\" % self.stepid)\n\n        # Apply branches outages.\n        if self.branchOutages is not None:\n            self.doOutages()\n\n        # Initialise the market.\n        self.market.reset()\n\n        # Get an action from each agent and perform it.\n        for task, agent in zip(self.tasks, self.agents):\n#            if self.do_optimisation[agent]:\n#                raise Exception(\"When using a black-box learning algorithm, \"\n#                                \"only full episodes can be done.\")\n\n#            if not task.isFinished():\n            observation = task.getObservation()\n            agent.integrateObservation(observation)\n\n            action = agent.getAction()\n            task.performAction(action)\n\n        # Clear the market.\n        self.market.run()\n\n        # Reward each agent appropriately.\n        for task, agent in zip(self.tasks, self.agents):\n#            if not task.isFinished():\n            reward = task.getReward()\n            agent.giveReward(reward)\n\n        # Scale loads.\n        c = self._pcycle.next()\n        for bus in self.market.case.buses:\n            bus.p_demand = self.pdemand[bus] * c\n\n        logger.info(\"\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self.stepid = 0\n\n        for task, agent in zip(self.tasks, self.agents):\n            task.reset()\n\n            agent.module.reset()\n            agent.history.reset()", "response": "Resets the internal state of the internal state of the experiment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlearning on the current dataset.", "response": "def learn(self):\n        \"\"\" Learn on the current dataset, either for many timesteps and even\n        episodes (batchMode = True) or for a single timestep\n        (batchMode = False). Batch mode is possible, because Q-Learning is an\n        off-policy method.\n\n        In batchMode, the algorithm goes through all the samples in the history\n        and performs an update on each of them. if batchMode is False, only the\n        last data sample is considered. The user himself has to make sure to\n        keep the dataset consistent with the agent's history.\n        \"\"\"\n        if self.batchMode:\n            samples = self.dataset\n        else:\n            samples = [[self.dataset.getSample()]]\n\n        for seq in samples:\n            for lastState, lastAction, reward in seq:\n                self._updatePropensities(int(lastState), int(lastAction), reward)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the propensities for all actions.", "response": "def _updatePropensities(self, lastState, lastAction, reward):\n        \"\"\" Update the propensities for all actions. The propensity for last\n        action chosen will be updated using the feedback value that resulted\n        from performing the action.\n\n        If j is the index of the last action chosen, r_j is the reward received\n        for performing j, i is the current action being updated, q_i is the\n        propensity for i, and phi is the recency parameter, then this update\n        function can be expressed as::\n\n                q_i = (1-phi) * q_i + E(i, r_j)\n        \"\"\"\n        phi = self.recency\n\n        for action in range(self.module.numActions):\n            carryOver = (1 - phi) * self.module.getValue(lastState, action)\n            experience = self._experience(lastState, action, lastAction,reward)\n\n            self.module.updateValue(lastState, action, carryOver + experience)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _experience(self, previousState, action, previousAction, reward):\n        e = self.experimentation\n\n        if action == previousAction:\n            experience = reward * (1 - e)\n        else:\n            propensity = self.module.getValue(previousState, action)\n            experience = propensity * (e / (self.module.numActions - 1))\n\n        return experience", "response": "This function returns the experience of the current in - memory action."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforwards implementation of the method.", "response": "def _forwardImplementation(self, inbuf, outbuf):\n        \"\"\" Proportional probability method.\n        \"\"\"\n        assert self.module\n\n        propensities = self.module.getActionValues(0)\n\n        summedProps = sum(propensities)\n        probabilities = propensities / summedProps\n\n        action = eventGenerator(probabilities)\n#        action = drawIndex(probabilities)\n\n        outbuf[:] = scipy.array([action])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite case data to file in Excel format.", "response": "def write(self, file_or_filename):\n        \"\"\" Writes case data to file in Excel format.\n        \"\"\"\n        self.book = Workbook()\n        self._write_data(None)\n        self.book.save(file_or_filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the case data to file.", "response": "def write_case_data(self, file):\n        \"\"\" Writes the header to file.\n        \"\"\"\n        case_sheet = self.book.add_sheet(\"Case\")\n        case_sheet.write(0, 0, \"Name\")\n        case_sheet.write(0, 1, self.case.name)\n        case_sheet.write(1, 0, \"base_mva\")\n        case_sheet.write(1, 1, self.case.base_mva)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwrites bus data to an Excel spreadsheet.", "response": "def write_bus_data(self, file):\n        \"\"\" Writes bus data to an Excel spreadsheet.\n        \"\"\"\n        bus_sheet = self.book.add_sheet(\"Buses\")\n\n        for i, bus in enumerate(self.case.buses):\n            for j, attr in enumerate(BUS_ATTRS):\n                bus_sheet.write(i, j, getattr(bus, attr))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_branch_data(self, file):\n        branch_sheet = self.book.add_sheet(\"Branches\")\n\n        for i, branch in enumerate(self.case.branches):\n            for j, attr in enumerate(BRANCH_ATTRS):\n                branch_sheet.write(i, j, getattr(branch, attr))", "response": "Writes branch data to an Excel spreadsheet."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites generator data to file.", "response": "def write_generator_data(self, file):\n        \"\"\" Write generator data to file.\n        \"\"\"\n        generator_sheet = self.book.add_sheet(\"Generators\")\n\n        for j, generator in enumerate(self.case.generators):\n            i = generator.bus._i\n            for k, attr in enumerate(GENERATOR_ATTRS):\n                generator_sheet.write(j, 0, i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write(self, file_or_filename):\n        if isinstance(file_or_filename, basestring):\n            file = open(file_or_filename, \"wb\")\n        else:\n            file = file_or_filename\n\n        self.writer = csv.writer(file)\n\n        super(CSVWriter, self).write(file)", "response": "Writes case data as CSV."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef write_case_data(self, file):\n        writer = self._get_writer(file)\n        writer.writerow([\"Name\", \"base_mva\"])\n        writer.writerow([self.case.name, self.case.base_mva])", "response": "Writes the case data as CSV."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite bus data as CSV.", "response": "def write_bus_data(self, file):\n        \"\"\" Writes bus data as CSV.\n        \"\"\"\n        writer = self._get_writer(file)\n        writer.writerow(BUS_ATTRS)\n        for bus in self.case.buses:\n            writer.writerow([getattr(bus, attr) for attr in BUS_ATTRS])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite branch data as CSV.", "response": "def write_branch_data(self, file):\n        \"\"\" Writes branch data as CSV.\n        \"\"\"\n        writer = self._get_writer(file)\n        writer.writerow(BRANCH_ATTRS)\n        for branch in self.case.branches:\n            writer.writerow([getattr(branch, a) for a in BRANCH_ATTRS])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_generator_data(self, file):\n        writer = self._get_writer(file)\n        writer.writerow([\"bus\"] + GENERATOR_ATTRS)\n\n        for g in self.case.generators:\n            i = g.bus._i\n            writer.writerow([i] + [getattr(g,a) for a in GENERATOR_ATTRS])", "response": "Write generator data as CSV."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getOffbids(self, g):\n        if not g.is_load:\n            offbids = [x for x in self.offers if x.generator == g]\n        else:\n            offbids = [x for x in self.bids if x.vLoad == g]\n\n        return offbids", "response": "Returns the offers and bids for the given generator."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef run(self):\n        # Start the clock.\n        t0 = time.time()\n\n        # Manage reactive power offers/bids.\n        haveQ = self._isReactiveMarket()\n\n        # Withhold offers/bids outwith optional price limits.\n        self._withholdOffbids()\n\n        # Convert offers/bids to pwl functions and update limits.\n        self._offbidToCase()\n\n        # Compute dispatch points and LMPs using OPF.\n        success = self._runOPF()\n\n        if success:\n            # Get nodal marginal prices from OPF.\n            gteeOfferPrice, gteeBidPrice = self._nodalPrices(haveQ)\n            # Determine quantity and price for each offer/bid.\n            self._runAuction(gteeOfferPrice, gteeBidPrice, haveQ)\n\n            logger.info(\"SmartMarket cleared in %.3fs\" % (time.time() - t0))\n        else:\n            for offbid in self.offers + self.bids:\n                offbid.clearedQuantity = 0.0\n                offbid.clearedPrice = 0.0\n                offbid.accepted = False\n\n                offbid.generator.p = 0.0\n\n            logger.error(\"Non-convergent market OPF. Blackout!\")\n\n        return self.offers, self.bids", "response": "Computes cleared offers and bids and returns the set of cleared offers and bids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _isReactiveMarket(self):\n        vLoads = [g for g in self.case.generators if g.is_load]\n\n        if [offbid for offbid in self.offers + self.bids if offbid.reactive]:\n            haveQ = True\n            logger.warning(\"Combined active/reactive power \" \\\n                \"market not yet implemented.\")\n            raise NotImplementedError\n        else:\n            haveQ = False\n\n        combinedTypes = [DISCRIMINATIVE, FIRST_PRICE]#, LAO]\n\n        if haveQ and vLoads and self.auctionType not in combinedTypes:\n            logger.error(\"Combined active/reactive power markets with \"\n                \"constant power factor dispatchable loads are only \"\n                \"implemented for 'discriminative', 'lao' and 'first price' \"\n                \"auction types.\")\n\n        return haveQ", "response": "Returns a flag indicating if the current active or reactive power is a reactive market."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _withholdOffbids(self):\n        limits = self.limits\n\n        # Eliminate offers (but not bids) above 'price_cap'.\n        if not limits.has_key('max_offer'):\n            limits['max_offer'] = self.priceCap\n\n        # Limit cleared offer prices after locational adjustments.\n        if not self.limits.has_key('maxClearedOffer'):\n            self.limits['maxClearedOffer'] = self.priceCap\n\n        # Withhold invalid offers/bids.\n        for offer in self.offers:\n            if round(offer.quantity, 4) <= 0.0:\n                logger.info(\"Withholding non-posistive quantity [%.2f] \"\n                            \"offer.\" % offer.quantity)\n                offer.withheld = True\n\n        for bid in self.bids:\n            if round(bid.quantity, 4) <= 0.0:\n                logger.info(\"Withholding non-posistive quantity [%.2f] \"\n                            \"bid.\" % bid.quantity)\n                bid.withheld = True\n\n        # Optionally, withhold offers/bids beyond price limits.\n        if limits.has_key(\"maxOffer\"):\n            for offer in self.offers:\n                if offer.price > limits[\"maxOffer\"]:\n                    logger.info(\"Offer price [%.2f] above limit [%.3f], \"\n                        \"withholding.\" % (offer.price, limits[\"maxOffer\"]))\n                    offer.withheld = True\n\n        if limits.has_key(\"minBid\"):\n            for bid in self.bids:\n                if bid.price < limits[\"minBid\"]:\n                    logger.info(\"Bid price [%.2f] below limit [%.2f], \"\n                        \"withholding.\" % (bid.price, limits[\"minBid\"]))\n                    bid.withheld = True", "response": "Withholds offers and bids with invalid quantities or prices or price_cap."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _offbidToCase(self):\n        generators = [g for g in self.case.generators if not g.is_load]\n        vLoads = [g for g in self.case.generators if g.is_load]\n\n        # Convert offers into piecewise linear segments and update limits.\n        for g in generators:\n#            print \"G: \", g.p_min, g.p_max, g.p_cost, g.pcost_model\n\n            g.offers_to_pwl(self.offers)\n\n#            print \"GG:\", g.p_min, g.p_max, g.p_cost, g.pcost_model\n\n        for vl in vLoads:\n#            print \"L: \", vl.p_min, vl.p_max, vl.p_cost\n\n            vl.bids_to_pwl(self.bids)\n\n#            print \"VL:\", vl.p_min, vl.p_max, g.q_min, g.q_max, vl.p_cost\n\n        # Move p_min and p_max limits out slightly to avoid problems with\n        # lambdas caused by rounding errors when corner point of cost function\n        # lies at exactly p_min or p_max.\n        for g in generators: # Skip dispatchable loads.\n            g.p_min -= 100 * self.violation\n            g.p_max += 100 * self.violation", "response": "Convert offers and bids to case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _runOPF(self):\n        if self.decommit:\n            solver = UDOPF(self.case, dc=(self.locationalAdjustment == \"dc\"))\n        elif self.locationalAdjustment == \"dc\":\n            solver = OPF(self.case, dc=True)\n        else:\n            solver = OPF(self.case, dc=False, opt={\"verbose\": True})\n\n        self._solution = solver.solve()\n\n#        for ob in self.offers + self.bids:\n#            ob.f = solution[\"f\"]\n\n        return self._solution[\"converged\"]", "response": "Computes dispatch points and LMPs using OPF."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the nodal prices associated with each offer and bid.", "response": "def _nodalPrices(self, haveQ):\n        \"\"\" Sets the nodal prices associated with each offer/bid.\n        \"\"\"\n        # Guarantee that cleared offer prices are >= offered prices.\n        gteeOfferPrice = True\n        gteeBidPrice = True\n\n        for offer in self.offers:\n            if not offer.reactive:\n                # Get nodal marginal price from OPF results.\n                offer.lmbda = offer.generator.bus.p_lmbda\n                offer.totalQuantity = offer.generator.p *offer.generator.online\n            else:\n                offer.lmbda = offer.generator.bus.q_lmbda\n                offer.totalQuantity = \\\n                    abs(offer.generator.q) * offer.generator.online\n        for bid in self.bids:\n            bus = bid.vLoad.bus\n\n            if not bid.reactive:\n                # Fudge factor to include price of bundled reactive power.\n                if bid.vLoad.q_max == 0.0:\n                    pf = bid.vLoad.q_min / bid.vLoad.p_min\n                elif bid.vLoad.q_min == 0.0:\n                    pf = bid.vLoad.q_max / bid.vLoad.p_min\n                else:\n                    pf = 0.0\n\n                # Use bundled lambdas. For loads Q = pf * P.\n                bid.lmbda = bus.p_lmbda + pf * bus.q_lmbda\n\n                bid.totalQuantity = -bid.vLoad.p * bid.vLoad.online\n                # Guarantee that cleared bids are <= bids.\n                gteeBidPrice = True\n            else:\n                # Use unbundled lambdas.\n                bid.lmbda = bus.q_lmbda\n\n                bid.totalQuantity = abs(bid.vLoad.q) * bid.vLoad.online\n                # Allow cleared bids to be above bid price.\n                gteeBidPrice = False\n\n        return gteeOfferPrice, gteeBidPrice"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _runAuction(self, gteeOfferPrice, gteeBidPrice, haveQ):\n        pOffers = [offer for offer in self.offers if not offer.reactive]\n        pBids = [bid for bid in self.bids if not bid.reactive]\n\n        # Clear offer/bid quantities and prices.\n        auction = Auction(self.case, pOffers, pBids, self.auctionType,\n                          gteeOfferPrice, gteeBidPrice, self.limits)\n        auction.run()\n\n        # Separate auction for reactive power.\n        if haveQ:\n            qOffers = [offer for offer in self.offers if offer.reactive]\n            qBids = [bid for bid in self.bids if bid.reactive]\n\n            # Too complicated to scale with mixed bids/offers (only\n            # auction_types LAO and FIRST_PRICE allowed)\n            qAuction = Auction(self.case, qOffers, qBids, self.auctionType,\n                                gteeOfferPrice, gteeBidPrice, self.limits)\n            qAuction.run()", "response": "Runs the auction for each available auction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a JSON string representation of a Python data structure.", "response": "def encode(self, o):\n        \"\"\"\n        Return a JSON string representation of a Python data structure.\n\n        >>> JSONEncoder().encode({\"foo\": [\"bar\", \"baz\"]})\n        '{\"foo\":[\"bar\", \"baz\"]}'\n        \"\"\"\n        # This doesn't pass the iterator directly to ''.join() because it\n        # sucks at reporting exceptions.  It's going to do this internally\n        # anyway because it uses PySequence_Fast or similar.\n        chunks = list(self.iterencode(o))\n        return ''.join(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a CASADI model from an AST.", "response": "def generate(ast_tree: ast.Tree, model_name: str, options: Dict[str, bool]=None) -> Model:\n    \"\"\"\n    :param ast_tree: AST to generate from\n    :param model_name: class to generate\n    :param options: dictionary of generator options\n    :return: casadi model\n    \"\"\"\n    if options is None:\n        options = {}\n    component_ref = ast.ComponentRef.from_string(model_name)\n    ast_walker = GeneratorWalker()\n    flat_tree = flatten(ast_tree, component_ref)\n    component_ref_tuple = component_ref.to_tuple()\n    casadi_gen = Generator(flat_tree, component_ref_tuple[-1], options)\n    ast_walker.walk(casadi_gen, flat_tree)\n    return casadi_gen.model"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the MX object for a given tree.", "response": "def get_mx(self, tree: Union[ast.Symbol, ast.ComponentRef, ast.Expression]) -> ca.MX:\n        \"\"\"\n        We pull components and symbols from the AST on demand.\n        This is to ensure that parametrized vector dimensions can be resolved.  Vector\n        dimensions need to be known at CasADi MX creation time.\n        :param tree:\n        :return:\n        \"\"\"\n        if tree not in self.src:\n            if isinstance(tree, ast.Symbol):\n                s = self.get_symbol(tree)\n            elif isinstance(tree, ast.ComponentRef):\n                s = self.get_component(tree)\n            else:\n                raise Exception('Tried to look up expression before it was reached by the tree walker')\n            self.src[tree] = s\n        return self.src[tree]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compute_file_metrics(processors, language, key, token_list):\n    # multiply iterator\n    tli = itertools.tee(token_list, len(processors))\n    metrics = OrderedDict()\n\n    # reset all processors\n    for p in processors:\n        p.reset()\n\n    # process all tokens\n    for p, tl in zip(processors, tli):\n        p.process_file(language, key, tl)\n\n    # collect metrics from all processors\n    for p in processors:\n        metrics.update(p.metrics)\n\n    return metrics", "response": "use processors to compute file metrics."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_lemmas(self, word, pos=None, ignore_case=False):\n        entries = self.get_entries(word, pos, ignore_case)\n        lemmas = list(set([entry[\"Lemma\"] for entry in entries]))\n        return sorted(lemmas)", "response": "Returns all lemmas for a given word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of lemmas for the given word.", "response": "def lemmatize(self, word, pos_universal_google):\n        \"\"\"\n        Python port of the lemmatize method, see https://github.com/Liebeck/IWNLP.Lemmatizer/blob/master/IWNLP.Lemmatizer.Predictor/IWNLPSentenceProcessor.cs\n\n        \"\"\"\n        if pos_universal_google == \"NOUN\":\n            if self.contains_entry(word, \"Noun\"):\n                return self.get_lemmas(word, \"Noun\")\n            elif self.contains_entry(word, \"X\"):\n                return self.get_lemmas(word, \"X\")\n            elif self.contains_entry(word, \"AdjectivalDeclension\"):\n                return self.get_lemmas(word, \"AdjectivalDeclension\")\n            elif self.contains_entry(word, [\"Noun\", \"X\"], ignore_case=True):\n                return self.get_lemmas(word, [\"Noun\", \"X\"], ignore_case=True)\n            else:\n                return None\n        elif pos_universal_google == \"ADJ\":\n            if self.contains_entry(word, \"Adjective\"):\n                return self.get_lemmas(word, \"Adjective\")\n            elif self.contains_entry(word, \"Adjective\", ignore_case=True):\n                return self.get_lemmas(word, \"Adjective\", ignore_case=True)\n            # Account for possible errors in the POS tagger. This order was fine-tuned in terms of accuracy\n            elif self.contains_entry(word, \"Noun\", ignore_case=True):\n                return self.get_lemmas(word, \"Noun\", ignore_case=True)\n            elif self.contains_entry(word, \"X\", ignore_case=True):\n                return self.get_lemmas(word, \"X\", ignore_case=True)\n            elif self.contains_entry(word, \"Verb\", ignore_case=True):\n                return self.get_lemmas(word, \"Verb\", ignore_case=True)\n            else:\n                return None\n        elif pos_universal_google in [\"VERB\", \"AUX\"]:\n            if self.contains_entry(word, \"Verb\", ignore_case=True):\n                return self.get_lemmas(word, \"Verb\", ignore_case=True)\n            else:\n                return None\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the case data to file.", "response": "def write(self, file_or_filename):\n        \"\"\" Writes the case data to file.\n        \"\"\"\n        if isinstance(file_or_filename, basestring):\n            file = None\n            try:\n                file = open(file_or_filename, \"wb\")\n            except Exception, detail:\n                logger.error(\"Error opening %s.\" % detail)\n            finally:\n                if file is not None:\n                    self._write_data(file)\n                    file.close()\n        else:\n            file = file_or_filename\n            self._write_data(file)\n\n        return file"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates serializer that is used by this encoder.", "response": "def _create_serializer(self) -> JsonObjectSerializer:\n        \"\"\"\n        Create serializer that is to be used by this encoder\n        :return: the serializer\n        \"\"\"\n        if self._serializer_cache is None:\n            serializer_cls = type(\n                \"%sInternalSerializer\" % type(self),\n                (JsonObjectSerializer,),\n                {\n                    \"_JSON_ENCODER_ARGS\": self._args,\n                    \"_JSON_ENCODER_KWARGS\": self._kwargs\n                }\n            )\n            self._serializer_cache = serializer_cls(self._get_property_mappings())\n        return self._serializer_cache"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a deserializer that is used by this decoder.", "response": "def _create_deserializer(self) -> JsonObjectDeserializer:\n        \"\"\"\n        Creates a deserializer that is to be used by this decoder.\n        :return: the deserializer\n        \"\"\"\n        if self._deserializer_cache is None:\n            deserializer_cls = type(\n                \"%sInternalDeserializer\" % type(self),\n                (JsonObjectDeserializer,),\n                {\n                    \"_JSON_ENCODER_ARGS\": self._args,\n                    \"_JSON_ENCODER_KWARGS\": self._kwargs\n                }\n            )\n            self._deserializer_cache = deserializer_cls(self._get_property_mappings(), self._get_deserializable_cls())\n        return self._deserializer_cache"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_tuple(self) -> tuple:\n\n        if self.child:\n            return (self.name, ) + self.child[0].to_tuple()\n        else:\n            return self.name,", "response": "Convert the nested component reference to flat tuple of names which is hashable and can therefore be used as dictionary key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the tuple pointing to a component to a component reference.", "response": "def from_tuple(cls, components: tuple) -> 'ComponentRef':\n        \"\"\"\n        Convert the tuple pointing to a component to\n        a component reference.\n        :param components: tuple of components name\n        :return: ComponentRef\n        \"\"\"\n\n        component_ref = ComponentRef(name=components[0], child=[])\n        c = component_ref\n        for component in components[1:]:\n            c.child.append(ComponentRef(name=component, child=[]))\n            c = c.child[0]\n        return component_ref"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_string(cls, s: str) -> 'ComponentRef':\n\n        components = s.split('.')\n        return cls.from_tuple(components)", "response": "Convert the string pointing to a component using dot notation to\n        a component reference."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef concatenate(cls, *args: List['ComponentRef']) -> 'ComponentRef':\n\n        a = copy.deepcopy(args[0])\n        n = a\n        for b in args[1:]:\n            while n.child:\n                n = n.child[0]\n            b = copy.deepcopy(b)  # Not strictly necessary\n            n.child = [b]\n        return a", "response": "Helper function to concatenate two component references to eachother."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_class(self, c: 'Class') -> None:\n        self.classes[c.name] = c\n        c.parent = self", "response": "Add a ( sub ) class to this class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate(ast_tree: ast.Tree, model_name: str):\n    component_ref = ast.ComponentRef.from_string(model_name)\n    ast_tree_new = copy.deepcopy(ast_tree)\n    ast_walker = TreeWalker()\n    flat_tree = flatten(ast_tree_new, component_ref)\n    sympy_gen = SympyGenerator()\n    ast_walker.walk(sympy_gen, flat_tree)\n    return sympy_gen.src[flat_tree]", "response": "Generate a sympy source code for a given model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getReward(self):\n        t = self.env.market.period\n\n        # Compute revenue minus costs.\n        totalEarnings = 0.0\n        for g in self.env.generators:\n            # Compute costs in $ (not $/hr).\n            costs = g.total_cost(round(g.p, 4),\n                                 self.env._g0[g][\"p_cost\"],\n                                 self.env._g0[g][\"pcost_model\"])\n\n            offbids = [ob for ob in self.env._lastAction if ob.generator == g]\n\n            revenue = t * sum([ob.revenue for ob in offbids])\n            if offbids:\n                revenue += offbids[0].noLoadCost\n\n            if g.is_load:\n                earnings = costs - revenue\n            else:\n                earnings = revenue - costs#(fixedCost + variableCost)\n\n            logger.debug(\"Generator [%s] earnings: %.2f (%.2f, %.2f)\" %\n                         (g.name, earnings, revenue, costs))\n\n            totalEarnings += earnings\n\n        # Startup/shutdown costs.\n        onlineCosts = 0.0\n        for i, g in enumerate(self.env.generators):\n            if self._gOnline[i] and not g.online:\n                onlineCosts += g.c_shutdown\n            elif not self._gOnline[i] and g.online:\n                onlineCosts += g.c_startup\n        self._gOnline = [g.online for g in self.env.generators]\n\n        reward = totalEarnings - onlineCosts\n        self.addReward(reward)\n\n        logger.debug(\"Task reward: %.2f (%.2f - %.2f)\" %\n                     (reward, totalEarnings, onlineCosts))\n\n        return reward", "response": "Returns the reward corresponding to the last action performed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef addReward(self, r=None):\n        r = self.getReward() if r is None else r\n\n        # by default, the cumulative reward is just the sum over the episode\n        if self.discount:\n            self.cumulativeReward += power(self.discount, self.samples) * r\n        else:\n            self.cumulativeReward += r", "response": "Adds a new reward to the cumulative reward."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsolving a state estimation problem.", "response": "def run(self):\n        \"\"\" Solves a state estimation problem.\n        \"\"\"\n        case = self.case\n        baseMVA = case.base_mva\n        buses = self.case.connected_buses\n        branches = case.online_branches\n        generators = case.online_generators\n        meas = self.measurements\n        # Update indices.\n        self.case.index_buses()\n        self.case.index_branches()\n\n        # Index buses.\n#        ref = [b._i for b in buses if b.type == REFERENCE]\n        pv  = [b._i for b in buses if b.type == PV]\n        pq  = [b._i for b in buses if b.type == PQ]\n\n        # Build admittance matrices.\n        Ybus, Yf, Yt = case.Y\n\n        # Prepare initial guess.\n        V0 = self.getV0(self.v_mag_guess, buses, generators)\n\n        # Start the clock.\n        t0 = time()\n\n        # Initialise SE.\n        converged = False\n        i = 0\n        V = V0\n        Va = angle(V0)\n        Vm = abs(V0)\n\n        nb = Ybus.shape[0]\n        f = [b.from_bus._i for b in branches]\n        t = [b.to_bus._i for b in branches]\n        nonref = pv + pq\n\n        # Form measurement vector.\n        z = array([m.value for m in meas])\n\n        # Form measurement index vectors.\n        idx_zPf = [m.b_or_l._i for m in meas if m.type == PF]\n        idx_zPt = [m.b_or_l._i for m in meas if m.type == PT]\n        idx_zQf = [m.b_or_l._i for m in meas if m.type == QF]\n        idx_zQt = [m.b_or_l._i for m in meas if m.type == QT]\n        idx_zPg = [m.b_or_l._i for m in meas if m.type == PG]\n        idx_zQg = [m.b_or_l._i for m in meas if m.type == QG]\n        idx_zVm = [m.b_or_l._i for m in meas if m.type == VM]\n        idx_zVa = [m.b_or_l._i for m in meas if m.type == VA]\n\n        def col(seq):\n            return [[k] for k in seq]\n\n        # Create inverse of covariance matrix with all measurements.\n#        full_scale = 30\n#        sigma = [\n#            0.02 * abs(Sf)      + 0.0052 * full_scale * ones(nbr,1),\n#            0.02 * abs(St)      + 0.0052 * full_scale * ones(nbr,1),\n#            0.02 * abs(Sbus)    + 0.0052 * full_scale * ones(nb,1),\n#            0.2 * pi/180 * 3*ones(nb,1),\n#            0.02 * abs(Sf)      + 0.0052 * full_scale * ones(nbr,1),\n#            0.02 * abs(St)      + 0.0052 * full_scale * ones(nbr,1),\n#            0.02 * abs(Sbus)    + 0.0052 * full_scale * ones(nb,1),\n#            0.02 * abs(V0)      + 0.0052 * 1.1 * ones(nb,1),\n#        ] ./ 3\n\n        # Get R inverse matrix.\n        sigma_vector = r_[\n            self.sigma[0] * ones(len(idx_zPf)),\n            self.sigma[1] * ones(len(idx_zPt)),\n            self.sigma[2] * ones(len(idx_zQf)),\n            self.sigma[3] * ones(len(idx_zQt)),\n            self.sigma[4] * ones(len(idx_zPg)),\n            self.sigma[5] * ones(len(idx_zQg)),\n            self.sigma[6] * ones(len(idx_zVm)),\n            self.sigma[7] * ones(len(idx_zVa))\n        ]\n        sigma_squared = sigma_vector**2\n\n        rsig = range(len(sigma_squared))\n        Rinv = csr_matrix((1.0 / sigma_squared, (rsig, rsig)))\n\n        # Do Newton iterations.\n        while (not converged) and (i < self.max_iter):\n            i += 1\n\n            # Compute estimated measurement.\n            Sfe = V[f] * conj(Yf * V)\n            Ste = V[t] * conj(Yt * V)\n            # Compute net injection at generator buses.\n            gbus = [g.bus._i for g in generators]\n            Sgbus = V[gbus] * conj(Ybus[gbus, :] * V)\n            # inj S + local Sd\n            Sd = array([complex(b.p_demand, b.q_demand) for b in buses])\n            Sgen = (Sgbus * baseMVA + Sd) / baseMVA\n\n            z_est = r_[\n                Sfe[idx_zPf].real,\n                Ste[idx_zPt].real,\n                Sfe[idx_zQf].imag,\n                Ste[idx_zQt].imag,\n                Sgen[idx_zPg].real,\n                Sgen[idx_zQg].imag,\n                abs(V[idx_zVm]),\n                angle(V[idx_zVa])\n            ]\n\n            # Get H matrix.\n            dSbus_dVm, dSbus_dVa = case.dSbus_dV(Ybus, V)\n            dSf_dVa, dSf_dVm, dSt_dVa, dSt_dVm, _, _ = case.dSbr_dV(Yf, Yt,V)\n\n            # Get sub-matrix of H relating to line flow.\n            dPF_dVa = dSf_dVa.real # from end\n            dQF_dVa = dSf_dVa.imag\n            dPF_dVm = dSf_dVm.real\n            dQF_dVm = dSf_dVm.imag\n            dPT_dVa = dSt_dVa.real # to end\n            dQT_dVa = dSt_dVa.imag\n            dPT_dVm = dSt_dVm.real\n            dQT_dVm = dSt_dVm.imag\n            # Get sub-matrix of H relating to generator output.\n            dPG_dVa = dSbus_dVa[gbus, :].real\n            dQG_dVa = dSbus_dVa[gbus, :].imag\n            dPG_dVm = dSbus_dVm[gbus, :].real\n            dQG_dVm = dSbus_dVm[gbus, :].imag\n            # Get sub-matrix of H relating to voltage angle.\n            dVa_dVa = csr_matrix((ones(nb), (range(nb), range(nb))))\n            dVa_dVm = csr_matrix((nb, nb))\n            # Get sub-matrix of H relating to voltage magnitude.\n            dVm_dVa = csr_matrix((nb, nb))\n            dVm_dVm = csr_matrix((ones(nb), (range(nb), range(nb))))\n\n            h = [(col(idx_zPf), dPF_dVa, dPF_dVm),\n                 (col(idx_zQf), dQF_dVa, dQF_dVm),\n                 (col(idx_zPt), dPT_dVa, dPT_dVm),\n                 (col(idx_zQt), dQT_dVa, dQT_dVm),\n                 (col(idx_zPg), dPG_dVa, dPG_dVm),\n                 (col(idx_zQg), dQG_dVa, dQG_dVm),\n                 (col(idx_zVm), dVm_dVa, dVm_dVm),\n                 (col(idx_zVa), dVa_dVa, dVa_dVm)]\n\n            H = vstack([hstack([dVa[idx, nonref], dVm[idx, nonref]])\n                        for idx, dVa, dVm in h if len(idx) > 0 ])\n\n            # Compute update step.\n            J = H.T * Rinv * H\n            F = H.T * Rinv * (z - z_est) # evalute F(x)\n            dx = spsolve(J, F)\n\n            # Check for convergence.\n            normF = linalg.norm(F, Inf)\n\n            if self.verbose:\n                logger.info(\"Iteration [%d]: Norm of mismatch: %.3f\" %\n                            (i, normF))\n            if normF < self.tolerance:\n                converged = True\n\n            # Update voltage.\n            npvpq = len(nonref)\n\n            Va[nonref] = Va[nonref] + dx[:npvpq]\n            Vm[nonref] = Vm[nonref] + dx[npvpq:2 * npvpq]\n\n            V = Vm * exp(1j * Va)\n            Va = angle(V)\n            Vm = abs(V)\n\n        # Weighted sum squares of error.\n        error_sqrsum = sum((z - z_est)**2 / sigma_squared)\n\n        # Update case with solution.\n        case.pf_solution(Ybus, Yf, Yt, V)\n\n        # Stop the clock.\n        elapsed = time() - t0\n\n        if self.verbose and converged:\n            print \"State estimation converged in: %.3fs (%d iterations)\" % \\\n            (elapsed, i)\n#            self.output_solution(sys.stdout, z, z_est)\n\n        solution = {\"V\": V, \"converged\": converged, \"iterations\": i,\n                    \"z\": z, \"z_est\": z_est, \"error_sqrsum\": error_sqrsum,\n                    \"elapsed\": elapsed}\n\n        return solution"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef getV0(self, v_mag_guess, buses, generators, type=CASE_GUESS):\n        if type == CASE_GUESS:\n            Va = array([b.v_angle * (pi / 180.0) for b in buses])\n            Vm = array([b.v_magnitude for b in buses])\n            V0 = Vm * exp(1j * Va)\n        elif type == FLAT_START:\n            V0 = ones(len(buses))\n        elif type == FROM_INPUT:\n            V0 = v_mag_guess\n        else:\n            raise ValueError\n\n        # Set the voltages of PV buses and the reference bus in the guess.\n#        online = [g for g in self.case.generators if g.online]\n        gbus = [g.bus._i for g in generators]\n        Vg = array([g.v_magnitude for g in generators])\n\n        V0[gbus] = Vg * abs(V0[gbus]) / V0[gbus]\n\n        return V0", "response": "Returns the initial voltage profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprints comparison of measurements and their estimations.", "response": "def output_solution(self, fd, z, z_est, error_sqrsum):\n        \"\"\" Prints comparison of measurements and their estimations.\n        \"\"\"\n        col_width = 11\n        sep = (\"=\" * col_width + \" \") * 4 + \"\\n\"\n\n        fd.write(\"State Estimation\\n\")\n        fd.write(\"-\" * 16 + \"\\n\")\n        fd.write(sep)\n        fd.write(\"Type\".center(col_width) + \" \")\n        fd.write(\"Name\".center(col_width) + \" \")\n        fd.write(\"Measurement\".center(col_width) + \" \")\n        fd.write(\"Estimation\".center(col_width) + \" \")\n        fd.write(\"\\n\")\n        fd.write(sep)\n\n        c = 0\n        for t in [PF, PT, QF, QT, PG, QG, VM, VA]:\n            for meas in self.measurements:\n                if meas.type == t:\n                    n = meas.b_or_l.name[:col_width].ljust(col_width)\n                    fd.write(t.ljust(col_width) + \" \")\n                    fd.write(n + \" \")\n                    fd.write(\"%11.5f \" % z[c])\n                    fd.write(\"%11.5f\\n\" % z_est[c])\n#                    fd.write(\"%s\\t%s\\t%.3f\\t%.3f\\n\" % (t, n, z[c], z_est[c]))\n                    c += 1\n\n        fd.write(\"\\nWeighted sum of error squares = %.4f\\n\" % error_sqrsum)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclearing a set of bids and offers.", "response": "def run(self):\n        \"\"\" Clears a set of bids and offers.\n        \"\"\"\n        # Compute cleared offer/bid quantities from total dispatched quantity.\n        self._clearQuantities()\n\n        # Compute shift values to add to lam to get desired pricing.\n#        lao, fro, lab, frb = self._first_rejected_last_accepted()\n\n        # Clear offer/bid prices according to auction type.\n        self._clearPrices()\n#        self._clear_prices(lao, fro, lab, frb)\n\n        # Clip cleared prices according to guarantees and limits.\n        self._clipPrices()\n\n        self._logClearances()\n\n        return self.offers, self.bids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute the cleared quantities for each offer and bid according to the dispatched output from the OPF solution.", "response": "def _clearQuantities(self):\n        \"\"\" Computes the cleared quantities for each offer/bid according\n        to the dispatched output from the OPF solution.\n        \"\"\"\n        generators = [g for g in self.case.generators if not g.is_load]\n        vLoads = [g for g in self.case.generators if g.is_load]\n\n        for g in generators:\n            self._clearQuantity(self.offers, g)\n\n        for vl in vLoads:\n            self._clearQuantity(self.bids, vl)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _clearQuantity(self, offbids, gen):\n        # Filter out offers/bids not applicable to the generator in question.\n        gOffbids = [offer for offer in offbids if offer.generator == gen]\n\n        # Offers/bids within valid price limits (not withheld).\n        valid = [ob for ob in gOffbids if not ob.withheld]\n\n        # Sort offers by price in ascending order and bids in decending order.\n        valid.sort(key=lambda ob: ob.price, reverse=[False, True][gen.is_load])\n\n        acceptedQty = 0.0\n        for ob in valid:\n            # Compute the fraction of the block accepted.\n            accepted = (ob.totalQuantity - acceptedQty) / ob.quantity\n\n            # Clip to the range 0-1.\n            if accepted > 1.0:\n                accepted = 1.0\n            elif accepted < 1.0e-05:\n                accepted = 0.0\n\n            ob.clearedQuantity = accepted * ob.quantity\n\n            ob.accepted = (accepted > 0.0)\n\n            # Log the event.\n#            if ob.accepted:\n#                logger.info(\"%s [%s, %.3f, %.3f] accepted at %.2f MW.\" %\n#                    (ob.__class__.__name__, ob.generator.name, ob.quantity,\n#                     ob.price, ob.clearedQuantity))\n#            else:\n#                logger.info(\"%s [%s, %.3f, %.3f] rejected.\" %\n#                    (ob.__class__.__name__, ob.generator.name, ob.quantity,\n#                     ob.price))\n\n            # Increment the accepted quantity.\n            acceptedQty += ob.quantity", "response": "Computes the cleared bid quantity from the total dispatched quantity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _clearPrices(self):\n        for offbid in self.offers + self.bids:\n            if self.auctionType == DISCRIMINATIVE:\n                offbid.clearedPrice = offbid.price\n            elif self.auctionType == FIRST_PRICE:\n                offbid.clearedPrice = offbid.lmbda\n            else:\n                raise ValueError", "response": "Clears prices according to auction type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _clipPrices(self):\n        # Guarantee that cleared offer prices are >= offers.\n        if self.guaranteeOfferPrice:\n            for offer in self.offers:\n                if offer.accepted and offer.clearedPrice < offer.price:\n                    offer.clearedPrice = offer.price\n\n        # Guarantee that cleared bid prices are <= bids.\n        if self.guaranteeBidPrice:\n            for bid in self.bids:\n                if bid.accepted and bid.clearedPrice > bid.price:\n                    bid.clearedPrice = bid.price\n\n        # Clip cleared offer prices.\n        if self.limits.has_key(\"maxClearedOffer\"):\n            maxClearedOffer = self.limits[\"maxClearedOffer\"]\n\n            for offer in self.offers:\n                if offer.clearedPrice > maxClearedOffer:\n                    offer.clearedPrice = maxClearedOffer\n\n        # Clip cleared bid prices.\n        if self.limits.has_key(\"minClearedBid\"):\n            minClearedBid = self.limits[\"minClearedBid\"]\n\n            for bid in self.bids:\n                if bid.clearedPrice < minClearedBid:\n                    bid.clearedPrice = minClearedBid\n\n        # Make prices uniform across all offers/bids for each generator after\n        # clipping (except for discrim auction) since clipping may only affect\n        # a single block of a multi-block generator.\n        if self.auctionType != DISCRIMINATIVE:\n            for g in self.case.generators:\n                gOffers = [of for of in self.offers if of.generator == g]\n                if gOffers:\n                    uniformPrice = max([of.clearedPrice for of in gOffers])\n                    for of in gOffers:\n                        of.clearedPrice = uniformPrice\n\n                gBids = [bid for bid in self.bids if bid.vLoad == g]\n                if gBids:\n                    uniformPrice = min([bid.cleared_price for bid in gBids])\n                    for bid in gBids:\n                        bid.clearedPrice = uniformPrice", "response": "Clip cleared prices according to guarantees and limits."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _logClearances(self):\n        for offer in self.offers:\n            logger.info(\"%.2fMW offer cleared at %.2f$/MWh for %s (%.2f).\" %\n                        (offer.clearedQuantity, offer.clearedPrice,\n                         offer.generator.name, offer.revenue))\n        for bid in self.bids:\n            logger.info(\"%.2fMW bid cleared at %.2f$/MWh for %s (%.2f).\" %\n                        (bid.clearedQuantity, bid.clearedPrice,\n                         bid.vLoad.name, bid.revenue))", "response": "Logs the cleared values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait for a response from a given path and returns the one that matches the expected status code.", "response": "def wait_for_response(client, timeout, path='/', expected_status_code=None):\n    \"\"\"\n    Try make a GET request with an HTTP client against a certain path and\n    return once any response has been received, ignoring any errors.\n\n    :param ContainerHttpClient client:\n        The HTTP client to use to connect to the container.\n    :param timeout:\n        Timeout value in seconds.\n    :param path:\n        HTTP path to request.\n    :param int expected_status_code:\n        If set, wait until a response with this status code is received. If not\n        set, the status code will not be checked.\n    :raises TimeoutError:\n        If a request fails to be made within the timeout period.\n    \"\"\"\n    # We want time.monotonic on Pythons that have it, otherwise time.time will\n    # have to do.\n    get_time = getattr(time, 'monotonic', time.time)\n\n    deadline = get_time() + timeout\n    while True:\n        try:\n            # Don't care what the response is, as long as we get one\n            time_left = deadline - get_time()\n            response = client.get(\n                path, timeout=max(time_left, 0.001), allow_redirects=False)\n\n            if (expected_status_code is None\n                    or response.status_code == expected_status_code):\n                return\n        except requests.exceptions.Timeout:\n            # Requests timed out, our time must be up\n            break\n        except Exception:\n            # Ignore other exceptions\n            pass\n\n        if get_time() >= deadline:\n            break\n        time.sleep(0.1)\n\n    raise TimeoutError('Timeout waiting for HTTP response.')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new ContainerClient object configured to make requests to the specified container.", "response": "def for_container(cls, container, container_port=None):\n        \"\"\"\n        :param container:\n            The container to make requests against.\n        :param container_port:\n            The container port to make requests against. If ``None``, the first\n            container port is used.\n        :returns:\n            A ContainerClient object configured to make requests to the\n            container.\n        \"\"\"\n        if container_port is not None:\n            host, port = container.get_host_port(container_port)\n        else:\n            host, port = container.get_first_host_port()\n\n        return cls(host, port)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nmakes a request against a container.", "response": "def request(self, method, path=None, url_kwargs=None, **kwargs):\n        \"\"\"\n        Make a request against a container.\n\n        :param method:\n            The HTTP method to use.\n        :param list path:\n            The HTTP path (either absolute or relative).\n        :param dict url_kwargs:\n            Parameters to override in the generated URL. See `~hyperlink.URL`.\n        :param kwargs:\n            Any other parameters to pass to Requests.\n        \"\"\"\n        return self._session.request(\n            method, self._url(path, url_kwargs), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, path=None, url_kwargs=None, **kwargs):\n        return self._session.get(self._url(path, url_kwargs), **kwargs)", "response": "Sends a GET request to the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef options(self, path=None, url_kwargs=None, **kwargs):\n        return self._session.options(self._url(path, url_kwargs), **kwargs)", "response": "Sends an OPTIONS request to the specified URL."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a HEAD request to the specified URL.", "response": "def head(self, path=None, url_kwargs=None, **kwargs):\n        \"\"\"\n        Sends a HEAD request.\n\n        :param path:\n            The HTTP path (either absolute or relative).\n        :param url_kwargs:\n            Parameters to override in the generated URL. See `~hyperlink.URL`.\n        :param **kwargs:\n            Optional arguments that ``request`` takes.\n        :return: response object\n        \"\"\"\n        return self._session.head(self._url(path, url_kwargs), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a POST request to the specified URL.", "response": "def post(self, path=None, url_kwargs=None, **kwargs):\n        \"\"\"\n        Sends a POST request.\n\n        :param path:\n            The HTTP path (either absolute or relative).\n        :param url_kwargs:\n            Parameters to override in the generated URL. See `~hyperlink.URL`.\n        :param **kwargs:\n            Optional arguments that ``request`` takes.\n        :return: response object\n        \"\"\"\n        return self._session.post(self._url(path, url_kwargs), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a PUT request to the specified resource.", "response": "def put(self, path=None, url_kwargs=None, **kwargs):\n        \"\"\"\n        Sends a PUT request.\n\n        :param path:\n            The HTTP path (either absolute or relative).\n        :param url_kwargs:\n            Parameters to override in the generated URL. See `~hyperlink.URL`.\n        :param **kwargs:\n            Optional arguments that ``request`` takes.\n        :return: response object\n        \"\"\"\n        return self._session.put(self._url(path, url_kwargs), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch(self, path=None, url_kwargs=None, **kwargs):\n        return self._session.patch(self._url(path, url_kwargs), **kwargs)", "response": "Sends a PUT request to the specified resource."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a PUT request to the specified resource.", "response": "def delete(self, path=None, url_kwargs=None, **kwargs):\n        \"\"\"\n        Sends a PUT request.\n\n        :param path:\n            The HTTP path (either absolute or relative).\n        :param url_kwargs:\n            Parameters to override in the generated URL. See `~hyperlink.URL`.\n        :param **kwargs:\n            Optional arguments that ``request`` takes.\n        :return: response object\n        \"\"\"\n        return self._session.delete(self._url(path, url_kwargs), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ser_iuwt_decomposition(in1, scale_count, scale_adjust, store_smoothed):\n\n    wavelet_filter = (1./16)*np.array([1,4,6,4,1])      # Filter-bank for use in the a trous algorithm.\n\n    # Initialises an empty array to store the coefficients.\n\n    detail_coeffs = np.empty([scale_count-scale_adjust, in1.shape[0], in1.shape[1]])\n\n    C0 = in1    # Sets the initial value to be the input array.\n\n    # The following loop, which iterates up to scale_adjust, applies the a trous algorithm to the scales which are\n    # considered insignificant. This is important as each set of wavelet coefficients depends on the last smoothed\n    # version of the input.\n\n    if scale_adjust>0:\n        for i in range(0, scale_adjust):\n            C0 = ser_a_trous(C0, wavelet_filter, i)\n\n    # The meat of the algorithm - two sequential applications fo the a trous followed by determination and storing of\n    # the detail coefficients. C0 is reassigned the value of C on each loop - C0 is always the smoothest version of the\n    # input image.\n\n    for i in range(scale_adjust,scale_count):\n        C = ser_a_trous(C0, wavelet_filter, i)                                  # Approximation coefficients.\n        C1 = ser_a_trous(C, wavelet_filter, i)                                  # Approximation coefficients.\n        detail_coeffs[i-scale_adjust,:,:] = C0 - C1                             # Detail coefficients.\n        C0 = C\n\n    if store_smoothed:\n        return detail_coeffs, C0\n    else:\n        return detail_coeffs", "response": "This function calls the a trous algorithm code to decompose the input into its wavelet coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mp_iuwt_recomposition(in1, scale_adjust, core_count, smoothed_array):\n\n    wavelet_filter = (1./16)*np.array([1,4,6,4,1])      # Filter-bank for use in the a trous algorithm.\n\n    # Determines scale with adjustment and creates a zero array to store the output, unless smoothed_array is given.\n\n    max_scale = in1.shape[0] + scale_adjust\n\n    if smoothed_array is None:\n        recomposition = np.zeros([in1.shape[1], in1.shape[2]])\n    else:\n        recomposition = smoothed_array\n\n    # The following loops call the a trous algorithm code to recompose the input. The first loop assumes that there are\n    # non-zero wavelet coefficients at scales above scale_adjust, while the second loop completes the recomposition\n    # on the scales less than scale_adjust.\n\n    for i in range(max_scale-1, scale_adjust-1, -1):\n        recomposition = mp_a_trous(recomposition, wavelet_filter, i, core_count) + in1[i-scale_adjust,:,:]\n\n    if scale_adjust>0:\n        for i in range(scale_adjust-1, -1, -1):\n            recomposition = mp_a_trous(recomposition, wavelet_filter, i, core_count)\n\n    return recomposition", "response": "This function recompose the input array into a single array."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef mp_a_trous(C0, wavelet_filter, scale, core_count):\n\n    # Creates an array which may be accessed by multiple processes.\n\n    shared_array_base = mp.Array(ctypes.c_float, C0.shape[0]**2, lock=False)\n    shared_array = np.frombuffer(shared_array_base, dtype=ctypes.c_float)\n    shared_array = shared_array.reshape(C0.shape)\n    shared_array[:,:] = C0\n\n    # Division of the problem and allocation of processes to cores.\n\n    processes = []\n\n    for i in range(core_count):\n        process = mp.Process(target = mp_a_trous_kernel, args = (shared_array, wavelet_filter, scale, i,\n                                                     C0.shape[0]//core_count, 'row',))\n        process.start()\n        processes.append(process)\n\n    for i in processes:\n        i.join()\n\n    processes = []\n\n    for i in range(core_count):\n        process = mp.Process(target = mp_a_trous_kernel, args = (shared_array, wavelet_filter, scale, i,\n                                                     C0.shape[1]//core_count, 'col',))\n        process.start()\n        processes.append(process)\n\n    for i in processes:\n        i.join()\n\n    return shared_array", "response": "This function is a reimplementation of the a trous filter which makes use of multiprocessing."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mp_a_trous_kernel(C0, wavelet_filter, scale, slice_ind, slice_width, r_or_c=\"row\"):\n\n    lower_bound = slice_ind*slice_width\n    upper_bound = (slice_ind+1)*slice_width\n\n    if r_or_c == \"row\":\n        row_conv = wavelet_filter[2]*C0[:,lower_bound:upper_bound]\n\n        row_conv[(2**(scale+1)):,:] += wavelet_filter[0]*C0[:-(2**(scale+1)),lower_bound:upper_bound]\n        row_conv[:(2**(scale+1)),:] += wavelet_filter[0]*C0[(2**(scale+1))-1::-1,lower_bound:upper_bound]\n\n        row_conv[(2**scale):,:] += wavelet_filter[1]*C0[:-(2**scale),lower_bound:upper_bound]\n        row_conv[:(2**scale),:] += wavelet_filter[1]*C0[(2**scale)-1::-1,lower_bound:upper_bound]\n\n        row_conv[:-(2**scale),:] += wavelet_filter[3]*C0[(2**scale):,lower_bound:upper_bound]\n        row_conv[-(2**scale):,:] += wavelet_filter[3]*C0[:-(2**scale)-1:-1,lower_bound:upper_bound]\n\n        row_conv[:-(2**(scale+1)),:] += wavelet_filter[4]*C0[(2**(scale+1)):,lower_bound:upper_bound]\n        row_conv[-(2**(scale+1)):,:] += wavelet_filter[4]*C0[:-(2**(scale+1))-1:-1,lower_bound:upper_bound]\n\n        C0[:,lower_bound:upper_bound] = row_conv\n\n    elif r_or_c == \"col\":\n        col_conv = wavelet_filter[2]*C0[lower_bound:upper_bound,:]\n\n        col_conv[:,(2**(scale+1)):] += wavelet_filter[0]*C0[lower_bound:upper_bound,:-(2**(scale+1))]\n        col_conv[:,:(2**(scale+1))] += wavelet_filter[0]*C0[lower_bound:upper_bound,(2**(scale+1))-1::-1]\n\n        col_conv[:,(2**scale):] += wavelet_filter[1]*C0[lower_bound:upper_bound,:-(2**scale)]\n        col_conv[:,:(2**scale)] += wavelet_filter[1]*C0[lower_bound:upper_bound,(2**scale)-1::-1]\n\n        col_conv[:,:-(2**scale)] += wavelet_filter[3]*C0[lower_bound:upper_bound,(2**scale):]\n        col_conv[:,-(2**scale):] += wavelet_filter[3]*C0[lower_bound:upper_bound,:-(2**scale)-1:-1]\n\n        col_conv[:,:-(2**(scale+1))] += wavelet_filter[4]*C0[lower_bound:upper_bound,(2**(scale+1)):]\n        col_conv[:,-(2**(scale+1)):] += wavelet_filter[4]*C0[lower_bound:upper_bound,:-(2**(scale+1))-1:-1]\n\n        C0[lower_bound:upper_bound,:] = col_conv", "response": "This function is used to perform a trous convolution step of the a trous algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unauth(request):\n    if check_key(request):\n        api = get_api(request)\n        request.session.clear()\n        logout(request)\n    return HttpResponseRedirect(reverse('main'))", "response": "Logout and remove all session data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef info(request):\n    if check_key(request):\n        api = get_api(request)\n        user = api.users(id='self')\n        print dir(user)\n        return render_to_response('djfoursquare/info.html', {'user': user})\n    else:\n        return HttpResponseRedirect(reverse('main'))", "response": "Display some user info to show we have authenticated successfully"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks to see if we already have an access_key stored in the request session.", "response": "def check_key(request):\n    \"\"\"\n    Check to see if we already have an access_key stored,\n    if we do then we have already gone through\n    OAuth. If not then we haven't and we probably need to.\n    \"\"\"\n    try:\n        access_key = request.session.get('oauth_token', None)\n        if not access_key:\n            return False\n    except KeyError:\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields items from a given Docker client within a given timeout.", "response": "def stream_timeout(stream, timeout, timeout_msg=None):\n    \"\"\"\n    Iterate over items in a streaming response from the Docker client within\n    a timeout.\n\n    :param ~docker.types.daemon.CancellableStream stream:\n        Stream from the Docker client to consume items from.\n    :param timeout:\n        Timeout value in seconds.\n    :param timeout_msg:\n        Message to raise in the exception when a timeout occurs.\n    \"\"\"\n    timed_out = threading.Event()\n\n    def timeout_func():\n        timed_out.set()\n        stream.close()\n\n    timer = threading.Timer(timeout, timeout_func)\n    try:\n        timer.start()\n        for item in stream:\n            yield item\n\n        # A timeout looks the same as the loop ending. So we need to check a\n        # flag to determine whether a timeout occurred or not.\n        if timed_out.is_set():\n            raise TimeoutError(timeout_msg)\n    finally:\n        timer.cancel()\n        # Close the stream's underlying response object (if it has one) to\n        # avoid potential socket leaks.\n        # This method seems to have more success at preventing ResourceWarnings\n        # than just stream.close() (should this be improved upstream?)\n        # FIXME: Potential race condition if Timer thread closes the stream at\n        # the same time we do here, but hopefully not with serious side effects\n        if hasattr(stream, '_response'):\n            stream._response.close()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_state(self, caller):\n\n        if caller in self.state:\n            return self.state[caller]\n        else:\n            rv = self.state[caller] = DictObject()\n            return rv", "response": "Get per - program state."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_eval(self, value, caller, return_value=True, **kwargs):\n        value = self.name_to_system_object(value)\n        if return_value and isinstance(value, AbstractStatusObject):\n            return value.status\n        if hasattr(value, 'call'):\n            return self.call_eval(value.call(caller, **kwargs), caller, return_value, **kwargs)\n        else:\n            return value", "response": "Evaluate the value of a resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef name_to_system_object(self, value):\n        if not self.system:\n            raise SystemNotReady\n\n        if isinstance(value, (str, Object)):\n            rv = self.system.name_to_system_object(value)\n            return rv if rv else value\n        else:\n            return value", "response": "Converts given name to a system object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel(self, caller):\n        for o in {i for i in self.children if isinstance(i, AbstractCallable)}:\n            o.cancel(caller)", "response": "Cancel all threaded background processes of this Callable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving string representation of the callable.", "response": "def give_str(self):\n        \"\"\"\n            Give string representation of the callable.\n        \"\"\"\n        args = self._args[:]\n        kwargs = self._kwargs\n        return self._give_str(args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef give_str_indented(self, tags=False):\n        args = self._args[:]\n        kwargs = self._kwargs\n        rv = self._give_str_indented(args, kwargs, tags)\n        if not tags:\n            rv = self.strip_color_tags(rv)\n        return rv", "response": "Gives indented string representation of the callable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_request(self, endpoint, params):\n        # params = {\n        #     **self._base_params,  # Mind order to allow params to overwrite base params\n        #     **params\n        # }\n        full_params = self._base_params.copy()\n        full_params.update(params)\n        try:\n            r = requests.get(endpoint, full_params)\n            data = r.json()\n            if r.status_code == 401 and not endpoint.endswith('lookup'):\n                raise exceptions.UnauthorizedKeyError\n            elif r.status_code == 400 and not endpoint.endswith('shorten'):\n                raise exceptions.BadApiRequest\n            elif r.status_code == 500:\n                raise exceptions.ServerOrConnectionError\n            return data, r\n        except ValueError as e:\n            raise exceptions.BadApiResponse(e)\n        except requests.RequestException:\n            raise exceptions.ServerOrConnectionError", "response": "Makes a request to the given endpoint and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef shorten(self, long_url, custom_ending=None, is_secret=False):\n        params = {\n            'url': long_url,\n            'is_secret': 'true' if is_secret else 'false',\n            'custom_ending': custom_ending\n        }\n        data, r = self._make_request(self.api_shorten_endpoint, params)\n        if r.status_code == 400:\n            if custom_ending is not None:\n                raise exceptions.CustomEndingUnavailable(custom_ending)\n            raise exceptions.BadApiRequest\n        elif r.status_code == 403:\n            raise exceptions.QuotaExceededError\n        action = data.get('action')\n        short_url = data.get('result')\n        if action == 'shorten' and short_url is not None:\n            return short_url\n        raise exceptions.DebugTempWarning", "response": "Creates a short url if valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the short url ending from a short url or short url ending.", "response": "def _get_ending(self, lookup_url):\n        \"\"\"\n        Returns the short url ending from a short url or an short url ending.\n\n        Example:\n         - Given `<your Polr server>/5N3f8`, return `5N3f8`.\n         - Given `5N3f8`, return `5N3f8`.\n\n        :param lookup_url: A short url or short url ending\n        :type lookup_url: str\n        :return: The url ending\n        :rtype: str\n        \"\"\"\n        if lookup_url.startswith(self.api_server):\n            return lookup_url[len(self.api_server) + 1:]\n        return lookup_url"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlooking up the url_ending to obtain information about the short url. If it exists, the API will return a dictionary with information, including the long_url that is the destination of the given short url URL. The lookup object looks like something like this: .. code-block:: python { 'clicks': 42, 'created_at': { 'date': '2017-12-03 00:40:45.000000', 'timezone': 'UTC', 'timezone_type': 3 }, 'long_url': 'https://stackoverflow.com/questions/tagged/python', 'updated_at': { 'date': '2017-12-24 13:37:00.000000', 'timezone': 'UTC', 'timezone_type': 3 } } :param str lookup_url: An url ending or full short url address :param url_key: optional URL ending key for lookups against secret URLs :type url_key: str or None :return: Lookup dictionary containing, among others things, the long url; or None if not existing :rtype: dict or None", "response": "def lookup(self, lookup_url, url_key=None):\n        \"\"\"\n        Looks up the url_ending to obtain information about the short url.\n\n        If it exists, the API will return a dictionary with information, including\n        the long_url that is the destination of the given short url URL.\n\n\n        The lookup object looks like something like this:\n\n        .. code-block:: python\n\n            {\n                'clicks': 42,\n                'created_at':\n                    {\n                        'date': '2017-12-03 00:40:45.000000',\n                        'timezone': 'UTC',\n                        'timezone_type': 3\n                    },\n                'long_url': 'https://stackoverflow.com/questions/tagged/python',\n                'updated_at':\n                    {\n                        'date': '2017-12-24 13:37:00.000000',\n                        'timezone': 'UTC',\n                        'timezone_type': 3\n                    }\n            }\n\n        :param str lookup_url: An url ending or full short url address\n        :param url_key: optional URL ending key for lookups against secret URLs\n        :type url_key: str or None\n        :return: Lookup dictionary containing, among others things, the long url; or None if not existing\n        :rtype: dict or None\n        \"\"\"\n        url_ending = self._get_ending(lookup_url)\n        params = {\n            'url_ending': url_ending,\n            'url_key': url_key\n        }\n        data, r = self._make_request(self.api_lookup_endpoint, params)\n        if r.status_code == 401:\n            if url_key is not None:\n                raise exceptions.UnauthorizedKeyError('given url_key is not valid for secret lookup.')\n            raise exceptions.UnauthorizedKeyError\n        elif r.status_code == 404:\n            return False  # no url found in lookup\n        action = data.get('action')\n        full_url = data.get('result')\n        if action == 'lookup' and full_url is not None:\n            return full_url\n        raise exceptions.DebugTempWarning"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef beam_fit(psf, cdelt1, cdelt2):\n\n    if psf.shape>512:\n        psf_slice = tuple([slice(psf.shape[0]/2-256, psf.shape[0]/2+256),slice(psf.shape[1]/2-256, psf.shape[1]/2+256)])\n    else:\n        psf_slice = tuple([slice(0, psf.shape[0]),slice(0, psf.shape[1])])\n\n    psf_centre = psf[psf_slice]/np.max(psf[psf_slice])\n\n    max_location = np.unravel_index(np.argmax(psf_centre), psf_centre.shape)\n\n    threshold_psf = np.where(psf_centre>0.5 , psf_centre, 0)\n\n    labelled_psf, labels = ndimage.label(threshold_psf)\n\n    extracted_primary_beam = np.where(labelled_psf==labelled_psf[max_location], psf_centre, 0)\n\n    # Following creates row and column values of interest for the central PSF lobe and then selects those values\n    # from the PSF using np.where. Additionally, the inputs for the fitting are created by reshaping the x,y,\n    # and z data into columns.\n\n    x = np.arange(-max_location[1],-max_location[1]+psf_centre.shape[1],1)\n    y = np.arange(-max_location[0],-max_location[0]+psf_centre.shape[0],1)\n    z = extracted_primary_beam\n\n    gridx, gridy = np.meshgrid(x,-y)\n\n    xyz = np.column_stack((gridx.reshape(-1,1),gridy.reshape(-1,1),z.reshape(-1,1,order=\"C\")))\n\n    # Elliptical gaussian which can be fit to the central lobe of the PSF. xy must be an Nx2 array consisting of\n    # pairs of row and column values for the region of interest.\n\n    def ellipgauss(xy,A,xsigma,ysigma,theta):\n        return A*np.exp(-1*(((xy[:,0]*np.cos(theta)-xy[:,1]*np.sin(theta))**2)/(2*(xsigma**2)) +\n                            ((xy[:,0]*np.sin(theta)+xy[:,1]*np.cos(theta))**2)/(2*(ysigma**2))))\n\n    # This command from scipy performs the fitting of the 2D gaussian, and returns the optimal coefficients in opt.\n\n    opt = curve_fit(ellipgauss, xyz[:,0:2],xyz[:,2],(1,1,1,0))[0]\n\n    # Following create the data for the new images. The cleanbeam has to be reshaped to reclaim it in 2D.\n\n    clean_beam = np.zeros_like(psf)\n    clean_beam[psf_slice] = ellipgauss(xyz[:,0:2],opt[0],opt[1],opt[2],opt[3]).reshape(psf_centre.shape,order=\"C\")\n\n    # Experimental - forces the beam to be normalised. This should be redundant, but helps when the PSF is bad.\n\n    clean_beam = clean_beam/np.max(clean_beam)\n\n    bmaj = 2*np.sqrt(2*np.log(2))*max(opt[1],opt[2])*cdelt1\n    bmin = 2*np.sqrt(2*np.log(2))*min(opt[1],opt[2])*cdelt2\n    bpa = np.degrees(opt[3])%360 - 90\n\n    beam_params = [abs(bmaj), abs(bmin), bpa]\n\n    return clean_beam, beam_params", "response": "Fits a beam to the PSF."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_argparser():\n    parser = argparse.ArgumentParser(prog='mypolr',\n                                     description=\"Interacts with the Polr Project's API.\\n\\n\"\n                                                 \"User Guide and documentation: https://mypolr.readthedocs.io\",\n                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter,\n                                     epilog=\"NOTE: if configurations are saved, they are stored as plain text on disk, \"\n                                            \"and can be read by anyone with access to the file.\")\n    parser.add_argument(\"-v\", \"--version\", action=\"store_true\", help=\"Print version and exit.\")\n\n    parser.add_argument(\"url\", nargs='?', default=None, help=\"The url to process.\")\n\n    api_group = parser.add_argument_group('API server arguments',\n                                          'Use these for configure the API. Can be stored locally with --save.')\n\n    api_group.add_argument(\"-s\", \"--server\", default=None, help=\"Server hosting the API.\")\n    api_group.add_argument(\"-k\", \"--key\", default=None, help=\"API_KEY to authenticate against server.\")\n    api_group.add_argument(\"--api-root\", default=DEFAULT_API_ROOT,\n                           help=\"API endpoint root.\")\n\n    option_group = parser.add_argument_group('Action options',\n                                             'Configure the API action to use.')\n\n    option_group.add_argument(\"-c\", \"--custom\", default=None,\n                              help=\"Custom short url ending.\")\n    option_group.add_argument(\"--secret\", action=\"store_true\",\n                              help=\"Set option if using secret url.\")\n    option_group.add_argument(\"-l\", \"--lookup\", action=\"store_true\",\n                              help=\"Perform lookup action instead of shorten action.\")\n\n    manage_group = parser.add_argument_group('Manage credentials',\n                                             'Use these to save, delete or update SERVER, KEY and/or '\n                                             'API_ROOT locally in ~/.mypolr/config.ini.')\n\n    manage_group.add_argument(\"--save\", action=\"store_true\",\n                              help=\"Save configuration (including credentials) in plaintext(!).\")\n    manage_group.add_argument(\"--clear\", action=\"store_true\",\n                              help=\"Clear configuration.\")\n    return parser", "response": "Setup argparse arguments.\n\n    :return: The parser which :class:`MypolrCli` expects parsed arguments from.\n    :rtype: argparse.ArgumentParser"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef snr_ratio(in1, in2):\n\n    out1 = 20*(np.log10(np.linalg.norm(in1)/np.linalg.norm(in1-in2)))\n\n    return out1", "response": "Calculates the signal to noise ratio between two signals."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwait for the RabbitMQ process to come up.", "response": "def wait_for_start(self):\n        \"\"\"\n        Wait for the RabbitMQ process to be come up.\n        \"\"\"\n        er = self.exec_rabbitmqctl(\n            'wait', ['--pid', '1', '--timeout', str(int(self.wait_timeout))])\n        output_lines(er, error_exc=TimeoutError)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef exec_rabbitmqctl(self, command, args=[], rabbitmqctl_opts=['-q']):\n        cmd = ['rabbitmqctl'] + rabbitmqctl_opts + [command] + args\n        return self.inner().exec_run(cmd)", "response": "Execute a rabbitmqctl command inside a running container."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a rabbitmqctl command to list the given resources.", "response": "def exec_rabbitmqctl_list(self, resources, args=[],\n                              rabbitmq_opts=['-q', '--no-table-headers']):\n        \"\"\"\n        Execute a ``rabbitmqctl`` command to list the given resources.\n\n        :param resources: the resources to list, e.g. ``'vhosts'``\n        :param args: a list of args for the command\n        :param rabbitmqctl_opts:\n            a list of extra options to pass to ``rabbitmqctl``\n        :returns: a tuple of the command exit code and output\n        \"\"\"\n        command = 'list_{}'.format(resources)\n        return self.exec_rabbitmqctl(command, args, rabbitmq_opts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the queues in the default vhost.", "response": "def list_queues(self):\n        \"\"\"\n        Run the ``list_queues`` command (for the default vhost) and return a\n        list of tuples describing the queues.\n\n        :return:\n            A list of 2-element tuples. The first element is the queue name,\n            the second is the current queue size.\n        \"\"\"\n        lines = output_lines(\n            self.exec_rabbitmqctl_list('queues', ['-p', self.vhost]))\n        return [tuple(line.split(None, 1)) for line in lines]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_users(self):\n        lines = output_lines(self.exec_rabbitmqctl_list('users'))\n        return [_parse_rabbitmq_user(line) for line in lines]", "response": "Run the list_users command and return a list of tuples describing the users."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a broker URL for use with Celery.", "response": "def broker_url(self):\n        \"\"\" Returns a \"broker URL\" for use with Celery. \"\"\"\n        return 'amqp://{}:{}@{}/{}'.format(\n            self.user, self.password, self.name, self.vhost)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef exec_pg_success(self, cmd):\n        result = self.inner().exec_run(cmd, user='postgres')\n        assert result.exit_code == 0, result.output.decode('utf-8')\n        return result", "response": "Execute a command inside a running container as the postgres user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove all data by dropping and recreating the configured database.", "response": "def clean(self):\n        \"\"\"\n        Remove all data by dropping and recreating the configured database.\n\n        .. note::\n\n            Only the configured database is removed. Any other databases\n            remain untouched.\n        \"\"\"\n        self.exec_pg_success(['dropdb', '-U', self.user, self.database])\n        self.exec_pg_success(['createdb', '-U', self.user, self.database])"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a psql command inside a running container.", "response": "def exec_psql(self, command, psql_opts=['-qtA']):\n        \"\"\"\n        Execute a ``psql`` command inside a running container. By default the\n        container's database is connected to.\n\n        :param command: the command to run (passed to ``-c``)\n        :param psql_opts: a list of extra options to pass to ``psql``\n        :returns: a tuple of the command exit code and output\n        \"\"\"\n        cmd = ['psql'] + psql_opts + [\n            '--dbname', self.database,\n            '-U', self.user,\n            '-c', command,\n        ]\n        return self.inner().exec_run(cmd, user='postgres')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_databases(self):\n        lines = output_lines(self.exec_psql('\\\\list'))\n        return [line.split('|') for line in lines]", "response": "Runs the \\ list command and returns a list of column values of all databases."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef list_tables(self):\n        lines = output_lines(self.exec_psql('\\\\dt'))\n        return [line.split('|') for line in lines]", "response": "Runs the \\ dt command and returns a list of column values about all tables in the database."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_users(self):\n        lines = output_lines(self.exec_psql('\\\\du'))\n        return [line.split('|') for line in lines]", "response": "Runs the \\\\ du command and returns a list of column values that are available for all user roles."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef database_url(self):\n        return 'postgres://{}:{}@{}/{}'.format(\n            self.user, self.password, self.name, self.database)", "response": "Returns a database URL for use with DJ - Database - URL and similar\n            libraries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a matrix from a configuration dictionary.", "response": "def from_config(config):\n    \"\"\"\n    Generate a matrix from a configuration dictionary.\n    \"\"\"\n    matrix = {}\n    variables = config.keys()\n    for entries in product(*config.values()):\n        combination = dict(zip(variables, entries))\n        include = True\n        for value in combination.values():\n            for reducer in value.reducers:\n                if reducer.pattern == '-':\n                    match = not combination[reducer.variable].value\n                else:\n                    match = fnmatch(combination[reducer.variable].value, reducer.pattern)\n                if match if reducer.is_exclude else not match:\n                    include = False\n        if include:\n            key = '-'.join(entry.alias for entry in entries if entry.alias)\n            data = dict(\n                zip(variables, (entry.value for entry in entries))\n            )\n            if key in matrix and data != matrix[key]:\n                raise DuplicateEnvironment(key, data, matrix[key])\n            matrix[key] = data\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a matrix from a. ini file.", "response": "def from_file(filename, section='matrix'):\n    \"\"\"\n    Generate a matrix from a .ini file. Configuration is expected to be in a ``[matrix]`` section.\n    \"\"\"\n    config = parse_config(open(filename), section=section)\n    return from_config(config)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_string(string, section='matrix'):\n    config = parse_config(StringIO(string), section=section)\n    return from_config(config)", "response": "Generate a matrix from a. ini file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef no_raise(f):\n    @wraps(f)\n    def new_f(*args, **kwargs):\n        try:\n            return f(*args, **kwargs)\n        except MypolrError:\n            pass\n        return None\n    return new_f", "response": "Decorator to force return None instead of raising module exceptions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nflush the queue to the local cache", "response": "def flush(self):\n        \"\"\"\n        This only needs to be called manually\n        from unit tests\n        \"\"\"\n        self.logger.debug('Flush joining')\n        self.queue.join()\n        self.logger.debug('Flush joining ready')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef output_lines(output, encoding='utf-8', error_exc=None):\n    if isinstance(output, ExecResult):\n        exit_code, output = output\n        if exit_code != 0 and error_exc is not None:\n            raise error_exc(output.decode(encoding))\n\n    return output.decode(encoding).splitlines()", "response": "Convert bytestring container output or an exec result instance into a list of unicode lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting file from WeedFS.", "response": "def get_file(self, fid):\n        \"\"\"Get file from WeedFS.\n\n        Returns file content. May be problematic for large files as content is\n        stored in memory.\n\n        Args:\n            **fid**: File identifier <volume_id>,<file_name_hash>\n\n        Returns:\n            Content of the file with provided fid or None if file doesn't\n            exist on the server\n\n        .. versionadded:: 0.3.1\n        \"\"\"\n        url = self.get_file_url(fid)\n        return self.conn.get_raw_data(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_file_url(self, fid, public=None):\n        try:\n            volume_id, rest = fid.strip().split(\",\")\n        except ValueError:\n            raise BadFidFormat(\n                \"fid must be in format: <volume_id>,<file_name_hash>\")\n        file_location = self.get_file_location(volume_id)\n        if public is None:\n            public = self.use_public_url\n        volume_url = file_location.public_url if public else file_location.url\n        url = \"http://{volume_url}/{fid}\".format(\n            volume_url=volume_url, fid=fid)\n        return url", "response": "Get url for the file with the given fid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_file_location(self, volume_id):\n        url = (\"http://{master_addr}:{master_port}/\"\n               \"dir/lookup?volumeId={volume_id}\").format(\n            master_addr=self.master_addr,\n            master_port=self.master_port,\n            volume_id=volume_id)\n        data = json.loads(self.conn.get_data(url))\n        _file_location = random.choice(data['locations'])\n        FileLocation = namedtuple('FileLocation', \"public_url url\")\n        return FileLocation(_file_location['publicUrl'], _file_location['url'])", "response": "Get location for the file weedFS volume is choosed randomly"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_file_size(self, fid):\n        url = self.get_file_url(fid)\n        res = self.conn.head(url)\n        if res is not None:\n            size = res.headers.get(\"content-length\", None)\n            if size is not None:\n                return int(size)\n        return None", "response": "Gets size of uploaded file in a specific resource."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef file_exists(self, fid):\n        res = self.get_file_size(fid)\n        if res is not None:\n            return True\n        return False", "response": "Checks if a file with the given fid exists in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete file from WeedFS", "response": "def delete_file(self, fid):\n        \"\"\"\n        Delete file from WeedFS\n\n        :param string fid: File ID\n        \"\"\"\n        url = self.get_file_url(fid)\n        return self.conn.delete_data(url)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload file to WeedFS server.", "response": "def upload_file(self, path=None, stream=None, name=None, **kwargs):\n        \"\"\"\n        Uploads file to WeedFS\n\n        I takes either path or stream and name and upload it\n        to WeedFS server.\n\n        Returns fid of the uploaded file.\n\n        :param string path:\n        :param string stream:\n        :param string name:\n        :rtype: string or None\n\n        \"\"\"\n        params = \"&\".join([\"%s=%s\" % (k, v) for k, v in kwargs.items()])\n        url = \"http://{master_addr}:{master_port}/dir/assign{params}\".format(\n            master_addr=self.master_addr,\n            master_port=self.master_port,\n            params=\"?\" + params if params else ''\n        )\n        data = json.loads(self.conn.get_data(url))\n        if data.get(\"error\") is not None:\n            return None\n        post_url = \"http://{url}/{fid}\".format(\n            url=data['publicUrl' if self.use_public_url else 'url'],\n            fid=data['fid']\n        )\n\n        if path is not None:\n            filename = os.path.basename(path)\n            with open(path, \"rb\") as file_stream:\n                res = self.conn.post_file(post_url, filename, file_stream)\n        # we have file like object and filename\n        elif stream is not None and name is not None:\n            res = self.conn.post_file(post_url, name, stream)\n        else:\n            raise ValueError(\n                \"If `path` is None then *both* `stream` and `name` must not\"\n                \" be None \")\n        response_data = json.loads(res)\n        if \"size\" in response_data:\n            return data.get('fid')\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nforcing garbage collection of the available resource names.", "response": "def vacuum(self, threshold=0.3):\n        '''\n        Force garbage collection\n\n        :param float threshold (optional): The threshold is optional, and\n        will not change the default threshold.\n        :rtype: boolean\n\n        '''\n        url = (\"http://{master_addr}:{master_port}/\"\n               \"vol/vacuum?garbageThreshold={threshold}\").format(\n            master_addr=self.master_addr,\n            master_port=self.master_port,\n            threshold=threshold)\n        res = self.conn.get_data(url)\n        if res is not None:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn Weed - FS master version", "response": "def version(self):\n        '''\n        Returns Weed-FS master version\n\n        :rtype: string\n        '''\n        url = \"http://{master_addr}:{master_port}/dir/status\".format(\n            master_addr=self.master_addr,\n            master_port=self.master_port)\n        data = self.conn.get_data(url)\n        response_data = json.loads(data)\n        return response_data.get(\"Version\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef gpu_r2c_fft(in1, is_gpuarray=False, store_on_gpu=False):\n\n    if is_gpuarray:\n        gpu_in1 = in1\n    else:\n        gpu_in1 = gpuarray.to_gpu_async(in1.astype(np.float32))\n\n    output_size = np.array(in1.shape)\n    output_size[1] = 0.5*output_size[1] + 1\n\n    gpu_out1 = gpuarray.empty([output_size[0], output_size[1]], np.complex64)\n    gpu_plan = Plan(gpu_in1.shape, np.float32, np.complex64)\n    fft(gpu_in1, gpu_out1, gpu_plan)\n\n    if store_on_gpu:\n        return gpu_out1\n    else:\n        return gpu_out1.get()", "response": "This function takes the real to complex FFT for GPUs."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef gpu_c2r_ifft(in1, is_gpuarray=False, store_on_gpu=False):\n\n    if is_gpuarray:\n        gpu_in1 = in1\n    else:\n        gpu_in1 = gpuarray.to_gpu_async(in1.astype(np.complex64))\n\n    output_size = np.array(in1.shape)\n    output_size[1] = 2*(output_size[1]-1)\n\n    gpu_out1 = gpuarray.empty([output_size[0],output_size[1]], np.float32)\n    gpu_plan = Plan(output_size, np.complex64, np.float32)\n    ifft(gpu_in1, gpu_out1, gpu_plan)\n    scale_fft(gpu_out1)\n\n    if store_on_gpu:\n        return gpu_out1\n    else:\n        return gpu_out1.get()", "response": "This function takes the complex to real IFFT and returns the result of the GPU array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pad_array(in1):\n\n    padded_size = 2*np.array(in1.shape)\n\n    out1 = np.zeros([padded_size[0],padded_size[1]])\n    out1[padded_size[0]/4:3*padded_size[0]/4,padded_size[1]/4:3*padded_size[1]/4] = in1\n\n    return out1", "response": "Simple convenience function to pad arrays for linear convolution."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the specified host is a dragon.", "response": "def is_dragon(host, timeout=1):\n        \"\"\"\n        Check if host is a dragon.\n\n        Check if the specified host is a dragon based on simple heuristic.\n        The code simply checks if particular strings are in the index page.\n        It should work for DragonMint or Innosilicon branded miners.\n        \"\"\"\n        try:\n            r = requests.get('http://{}/'.format(host), timeout=timeout)\n            if r.status_code == 200:\n                if '<title>DragonMint</title>' in r.text or \\\n                        '<title>AsicMiner</title>' in r.text:\n                    return True\n        except requests.exceptions.RequestException:\n            pass\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef auth(self):\n        response = requests.post(\n            parse.urljoin(self.base_url, '/api/auth'),\n            timeout=self.timeout,\n            data={'username': self.username, 'password': self.password})\n        response.raise_for_status()\n        json = response.json()\n        if 'jwt' not in json:\n            raise ValueError(\"Not authorized: didn't receive token, check username or password.\")\n        self.jwt = json['jwt']\n        return json", "response": "Authenticate with the miner and obtain a JSON web token."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchanges the pools of the miner. This call will restart cgminer.", "response": "def updatePools(self,\n                    pool1,\n                    username1,\n                    password1,\n                    pool2=None,\n                    username2=None,\n                    password2=None,\n                    pool3=None,\n                    username3=None,\n                    password3=None):\n        \"\"\"Change the pools of the miner. This call will restart cgminer.\"\"\"\n        return self.__post('/api/updatePools',\n                           data={\n                               'Pool1': pool1,\n                               'UserName1': username1,\n                               'Password1': password1,\n                               'Pool2': pool2,\n                               'UserName2': username2,\n                               'Password2': password2,\n                               'Pool3': pool3,\n                               'UserName3': username3,\n                               'Password3': password3,\n                           })"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef updatePassword(self,\n                       user,\n                       currentPassword,\n                       newPassword):\n        \"\"\"Change the password of a user.\"\"\"\n        return self.__post('/api/updatePassword',\n                           data={\n                               'user': user,\n                               'currentPassword': currentPassword,\n                               'newPassword': newPassword\n                           })", "response": "Change the password of a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef updateNetwork(self,\n                      dhcp='dhcp',\n                      ipaddress=None,\n                      netmask=None,\n                      gateway=None,\n                      dns=None):\n        \"\"\"Change the current network settings.\"\"\"\n        return self.__post('/api/updateNetwork',\n                           data={\n                               'dhcp': dhcp,\n                               'ipaddress': ipaddress,\n                               'netmask': netmask,\n                               'gateway': gateway,\n                               'dns': json.dumps(dns)\n                           })", "response": "Change the current network settings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupgrade the firmware of the miner.", "response": "def upgradeUpload(self, file):\n        \"\"\"Upgrade the firmware of the miner.\"\"\"\n        files = {'upfile': open(file, 'rb')}\n        return self.__post_files('/upgrade/upload',\n                                 files=files)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_program(self):\n        from automate.callables import Empty\n        return not (isinstance(self.on_activate, Empty)\n                    and isinstance(self.on_deactivate, Empty)\n                    and isinstance(self.on_update, Empty))", "response": "Returns True if the status object is a program status object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_status_display(self, **kwargs):\n        if 'value' in kwargs:\n            value = kwargs['value']\n        else:\n            value = self.status\n\n        if self.show_stdev_seconds:\n            stdev = self.stdev(self.show_stdev_seconds)\n            return f'{value}\u00b1{stdev:2.2}'\n        else:\n            return str(value)", "response": "Get the status of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets data of this object as a data dictionary. Used by websocket service.", "response": "def get_as_datadict(self):\n        \"\"\"\n            Get data of this object as a data dictionary. Used by websocket service.\n        \"\"\"\n        d = super().get_as_datadict()\n        d.update(dict(status=self.status, data_type=self.data_type, editable=self.editable))\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _do_change_status(self, status, force=False):\n        self.system.worker_thread.put(DummyStatusWorkerTask(self._request_status_change_in_queue, status, force=force))", "response": "This function is called by _update_program_stack and _update_status_change_in_queue."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_status(self, status, origin=None, force=False):\n\n        if status != self.default:\n            self._setup_reset_delay()\n\n        if self.status_filter:\n            status = self.status_filter(status)\n\n        return self._do_change_status(status, force)", "response": "Sets the status of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_status(self, status, origin=None, force=False):\n\n        if not self.slave and origin not in self.program_stack:\n            raise ValueError('Status cannot be changed directly')\n\n        with self._actuator_status_lock:\n            self.logger.debug(\"set_status got through, program: %s\", origin)\n            self.logger.debug(\"Set_status %s %s %s\", self.name, origin, status)\n\n            if self.slave:\n                return self._do_change_status(status, force)\n\n            self.logger.debug(\"Sets status %s for %s\", status, origin.name)\n\n            with self._program_lock:\n                self.program_status[origin] = status\n\n                if self.program == origin:\n                    return self._do_change_status(status, force)", "response": "Set the status of the active actuator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling by program which desires to manipulate this actuator, when it is activated.", "response": "def activate_program(self, program):\n        \"\"\"\n            Called by program which desires to manipulate this actuator, when it is activated.\n        \"\"\"\n        self.logger.debug(\"activate_program %s\", program)\n        if program in self.program_stack:\n            return\n\n        with self._program_lock:\n            self.logger.debug(\"activate_program got through %s\", program)\n            self.program_stack.append(program)\n            self._update_program_stack()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef deactivate_program(self, program):\n        self.logger.debug(\"deactivate_program %s\", program)\n\n        with self._program_lock:\n            self.logger.debug(\"deactivate_program got through %s\", program)\n            if program not in self.program_stack:\n                import ipdb\n                ipdb.set_trace()\n            self.program_stack.remove(program)\n            if program in self.program_status:\n                del self.program_status[program]\n            self._update_program_stack()", "response": "Called by the program when it is deactivated."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstreaming logs from a Docker container within a timeout.", "response": "def stream_logs(container, timeout=10.0, **logs_kwargs):\n    \"\"\"\n    Stream logs from a Docker container within a timeout.\n\n    :param ~docker.models.containers.Container container:\n        Container who's log lines to stream.\n    :param timeout:\n        Timeout value in seconds.\n    :param logs_kwargs:\n        Additional keyword arguments to pass to ``container.logs()``. For\n        example, the ``stdout`` and ``stderr`` boolean arguments can be used to\n        determine whether to stream stdout or stderr or both (the default).\n\n    :raises TimeoutError:\n        When the timeout value is reached before the logs have completed.\n    \"\"\"\n    stream = container.logs(stream=True, **logs_kwargs)\n    return stream_timeout(\n        stream, timeout, 'Timeout waiting for container logs.')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_for_logs_matching(container, matcher, timeout=10, encoding='utf-8',\n                           **logs_kwargs):\n    \"\"\"\n    Wait for matching log line(s) from the given container by streaming the\n    container's stdout and/or stderr outputs.\n\n    Each log line is decoded and any trailing whitespace is stripped before the\n    line is matched.\n\n    :param ~docker.models.containers.Container container:\n        Container who's log lines to wait for.\n    :param matcher:\n        Callable that returns True once it has matched a decoded log line(s).\n    :param timeout:\n        Timeout value in seconds.\n    :param encoding:\n        Encoding to use when decoding container output to strings.\n    :param logs_kwargs:\n        Additional keyword arguments to pass to ``container.logs()``. For\n        example, the ``stdout`` and ``stderr`` boolean arguments can be used to\n        determine whether to stream stdout or stderr or both (the default).\n\n    :returns:\n        The final matching log line.\n    :raises TimeoutError:\n        When the timeout value is reached before matching log lines have been\n        found.\n    :raises RuntimeError:\n        When all log lines have been consumed but matching log lines have not\n        been found (the container must have stopped for its stream to have\n        ended without error).\n    \"\"\"\n    try:\n        for line in stream_logs(container, timeout=timeout, **logs_kwargs):\n            # Drop the trailing newline\n            line = line.decode(encoding).rstrip()\n            if matcher(line):\n                return line\n    except TimeoutError:\n        raise TimeoutError('\\n'.join([\n            ('Timeout ({}s) waiting for logs matching {}.'.format(\n                timeout, matcher)),\n            'Last few log lines:',\n            _last_few_log_lines(container),\n        ]))\n\n    raise RuntimeError('\\n'.join([\n        'Logs matching {} not found.'.format(matcher),\n        'Last few log lines:',\n        _last_few_log_lines(container),\n    ]))", "response": "Wait for log lines from a given container to match a given matcher."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fetch_image(client, name):\n    try:\n        image = client.images.get(name)\n    except docker.errors.ImageNotFound:\n        name, tag = _parse_image_tag(name)\n        tag = 'latest' if tag is None else tag\n\n        log.info(\"Pulling tag '{}' for image '{}'...\".format(tag, name))\n        image = client.images.pull(name, tag=tag)\n\n    log.debug(\"Found image '{}' for tag '{}'\".format(image.id, name))\n    return image", "response": "Fetch an image if it isn t already present."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_id_and_model(self, id_or_model):\n        if isinstance(id_or_model, self.collection.model):\n            model = id_or_model\n        elif isinstance(id_or_model, str):\n            # Assume we have an ID string\n            model = self.collection.get(id_or_model)\n        else:\n            raise TypeError('Unexpected type {}, expected {} or {}'.format(\n                type(id_or_model), str, self.collection.model))\n\n        return model.id, model", "response": "Get both the model and ID of an object that could be an ID or a model object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of this resource type.", "response": "def create(self, name, *args, **kwargs):\n        \"\"\"\n        Create an instance of this resource type.\n        \"\"\"\n        resource_name = self._resource_name(name)\n        log.info(\n            \"Creating {} '{}'...\".format(self._model_name, resource_name))\n        resource = self.collection.create(*args, name=resource_name, **kwargs)\n        self._ids.add(resource.id)\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an instance of this resource type.", "response": "def remove(self, resource, **kwargs):\n        \"\"\"\n        Remove an instance of this resource type.\n        \"\"\"\n        log.info(\n            \"Removing {} '{}'...\".format(self._model_name, resource.name))\n        resource.remove(**kwargs)\n        self._ids.remove(resource.id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new container.", "response": "def create(self, name, image, fetch_image=False, network=None, volumes={},\n               **kwargs):\n        \"\"\"\n        Create a new container.\n\n        :param name:\n            The name for the container. This will be prefixed with the\n            namespace.\n        :param image:\n            The image tag or image object to create the container from.\n        :param network:\n            The network to connect the container to. The container will be\n            given an alias with the ``name`` parameter. Note that, unlike the\n            Docker Python client, this parameter can be a ``Network`` model\n            object, and not just a network ID or name.\n        :param volumes:\n            A mapping of volumes to bind parameters. The keys to this mapping\n            can be any of three types of objects:\n\n            - A ``Volume`` model object\n            - The name of a volume (str)\n            - A path on the host to bind mount into the container (str)\n\n            The bind parameters, i.e. the values in the mapping, can be of\n            two types:\n\n            - A full bind specifier (dict), for example\n              ``{'bind': '/mnt', 'mode': 'rw'}``\n            - A \"short-form\" bind specifier (str), for example ``/mnt:rw``\n        :param fetch_image:\n            Whether to attempt to pull the image if it is not found locally.\n        :param kwargs:\n            Other parameters to create the container with.\n        \"\"\"\n        create_kwargs = {\n            'detach': True,\n        }\n\n        # Convert network & volume models to IDs\n        network = self._network_for_container(network, kwargs)\n        if network is not None:\n            network_id, network = (\n                self._network_helper._get_id_and_model(network))\n            create_kwargs['network'] = network_id\n\n        if volumes:\n            create_kwargs['volumes'] = self._volumes_for_container(volumes)\n\n        create_kwargs.update(kwargs)\n\n        if fetch_image:\n            self._image_helper.fetch(image)\n\n        container = super().create(name, image, **create_kwargs)\n\n        if network is not None:\n            self._connect_container_network(container, network, aliases=[name])\n\n        return container"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove(self, container, force=True, volumes=True):\n        super().remove(container, force=force, v=volumes)", "response": "Remove a container from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_default(self, create=True):\n        if self._default_network is None and create:\n            log.debug(\"Creating default network...\")\n            self._default_network = self.create('default', driver='bridge')\n\n        return self._default_network", "response": "Get the default bridge network that containers are connected to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new network with the specified name.", "response": "def create(self, name, check_duplicate=True, **kwargs):\n        \"\"\"\n        Create a new network.\n\n        :param name:\n            The name for the network. This will be prefixed with the namespace.\n        :param check_duplicate:\n            Whether or not to check for networks with the same name. Docker\n            allows the creation of multiple networks with the same name (unlike\n            containers). This seems to cause problems sometimes for some reason\n            (?). The Docker Python client _claims_ (as of 2.5.1) that\n            ``check_duplicate`` defaults to True but it actually doesn't. We\n            default it to True ourselves here.\n        :param kwargs:\n            Other parameters to create the network with.\n        \"\"\"\n        return super().create(name, check_duplicate=check_duplicate, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _helper_for_model(self, model_type):\n        if model_type is models.containers.Container:\n            return self.containers\n        if model_type is models.images.Image:\n            return self.images\n        if model_type is models.networks.Network:\n            return self.networks\n        if model_type is models.volumes.Volume:\n            return self.volumes\n\n        raise ValueError('Unknown model type {}'.format(model_type))", "response": "Returns the helper function for a given model type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef teardown(self):\n        self.containers._teardown()\n        self.networks._teardown()\n        self.volumes._teardown()\n\n        # We need to close the underlying APIClient explicitly to avoid\n        # ResourceWarnings from unclosed HTTP connections.\n        self._client.api.close()", "response": "Clean up all resources when we re done with them."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef exec_redis_cli(self, command, args=[], db=0, redis_cli_opts=[]):\n        cli_opts = ['-n', str(db)] + redis_cli_opts\n        cmd = ['redis-cli'] + cli_opts + [command] + [str(a) for a in args]\n        return self.inner().exec_run(cmd)", "response": "Execute a redis - cli command inside a running container."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_keys(self, pattern='*', db=0):\n        lines = output_lines(self.exec_redis_cli('KEYS', [pattern], db=db))\n        return [] if lines == [''] else lines", "response": "Run the KEYS command and return the list of matching keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nusing thread_init as a decorator-style", "response": "def threaded(system, func, *args, **kwargs):\n    \"\"\" uses thread_init as a decorator-style \"\"\"\n    @wraps(func)\n    def wrapper(*args, **kwargs):\n        try:\n            return func(*args, **kwargs)\n        except Exception as e:\n            if system.raven_client:\n                system.raven_client.captureException()\n            logger.exception('Exception occurred in thread: %s', e)\n            return False\n\n    return lambda: wrapper(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_container_processes(container):\n    cmd = ['ps', 'ax', '-o', ','.join(PsRow.columns())]\n    ps_lines = output_lines(container.exec_run(cmd))\n\n    header = ps_lines.pop(0)\n    # We can't trust the header alignment because different ps implementations\n    # use different alignments, some of which depend on the alignment of the\n    # columns. Instead, we assume that all columns are whitespace-separated and\n    # that only the last column may contain spaces.\n    maxsplit = len(header.strip().split()) - 1\n    ps_entries = [line.strip().split(None, maxsplit) for line in ps_lines]\n\n    # Convert to PsRows\n    ps_rows = [PsRow(*entry) for entry in ps_entries]\n\n    # Filter out the row for ps itself\n    cmd_string = ' '.join(cmd)\n    ps_rows = [row for row in ps_rows if row.args != cmd_string]\n\n    return ps_rows", "response": "List the processes running inside a container."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nbuilding a tree structure from a list of PsRow objects.", "response": "def build_process_tree(ps_rows):\n    \"\"\"\n    Build a tree structure from a list of PsRow objects.\n    :param ps_rows: a list of PsRow objects\n    :return: a PsTree object\n    \"\"\"\n    ps_tree = None\n    for row in ps_rows:\n        if row.ppid == 0:\n            if ps_tree is not None:\n                raise PsException(\"Too many process tree roots (ppid=0) found\")\n            ps_tree = PsTree(row)\n    if ps_tree is None:\n        raise PsException(\"No process tree root (ppid=0) found\")\n    _build_process_subtree(ps_rows, ps_tree, set([ps_tree.row.pid]))\n    if ps_tree.count() < len(ps_rows):\n        raise PsException(\"Unreachable processes detected\")\n    assert ps_tree.count() == len(ps_rows)\n    return ps_tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef cmd_balance():\n    r = rocket.operations.cool_feed.get(params={\"per_page\": 1})\n    r = handle_error(r)\n    j = r.json()\n\n    template = \"\".join([\n        click.style(\"{rur} {code}, \", fg=\"green\", bold=True),\n        \"{miles} \u0440\u043e\u043a\u0435\u0442\u0440\u0443\u0431\u043b\u0435\u0439\"])\n    click.echo(template.format(\n        rur=j[\"balance\"][\"amount\"],\n        code=j[\"balance\"][\"currency_code\"],\n        miles=int(j[\"miles\"])))", "response": "Display the balance of the current user."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match(self, item):\n        if self._position == len(self._matchers):\n            raise RuntimeError('Matcher exhausted, no more matchers to use')\n\n        matcher = self._matchers[self._position]\n        if matcher(item):\n            self._position += 1\n\n        if self._position == len(self._matchers):\n            # All patterns have been matched\n            return True\n\n        return False", "response": "Returns True if the expected matchers match the item."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an args string for the repr.", "response": "def args_str(self):\n        \"\"\"\n        Return an args string for the repr.\n        \"\"\"\n        matched = [str(m) for m in self._matchers[:self._position]]\n        unmatched = [str(m) for m in self._matchers[self._position:]]\n        return 'matched=[{}], unmatched=[{}]'.format(\n            ', '.join(matched), ', '.join(unmatched))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef match(self, item):\n        if not self._unused_matchers:\n            raise RuntimeError('Matcher exhausted, no more matchers to use')\n\n        for matcher in self._unused_matchers:\n            if matcher(item):\n                self._used_matchers.append(matcher)\n                break\n\n        if not self._unused_matchers:\n            # All patterns have been matched\n            return True\n\n        return False", "response": "Returns True if the expected matchers are matched in any order otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an args string for the repr.", "response": "def args_str(self):\n        \"\"\"\n        Return an args string for the repr.\n        \"\"\"\n        matched = [str(m) for m in self._used_matchers]\n        unmatched = [str(m) for m in self._unused_matchers]\n        return 'matched=[{}], unmatched=[{}]'.format(\n            ', '.join(matched), ', '.join(unmatched))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef moresane_by_scale(self, start_scale=1, stop_scale=20, subregion=None, sigma_level=4, loop_gain=0.1,\n                          tolerance=0.75, accuracy=1e-6, major_loop_miter=100, minor_loop_miter=30, all_on_gpu=False,\n                          decom_mode=\"ser\", core_count=1, conv_device='cpu', conv_mode='linear', extraction_mode='cpu',\n                          enforce_positivity=False, edge_suppression=False,\n                          edge_offset=0, flux_threshold=0, neg_comp=False, edge_excl=0, int_excl=0):\n        \"\"\"\n        Extension of the MORESANE algorithm. This takes a scale-by-scale approach, attempting to remove all sources\n        at the lower scales before moving onto the higher ones. At each step the algorithm may return to previous\n        scales to remove the sources uncovered by the deconvolution.\n\n        INPUTS:\n        start_scale         (default=1)         The first scale which is to be considered.\n        stop_scale          (default=20)        The maximum scale which is to be considered. Optional.\n        subregion           (default=None):     Size, in pixels, of the central region to be analyzed and deconvolved.\n        sigma_level         (default=4)         Number of sigma at which thresholding is to be performed.\n        loop_gain           (default=0.1):      Loop gain for the deconvolution.\n        tolerance           (default=0.75):     Tolerance level for object extraction. Significant objects contain\n                                                wavelet coefficients greater than the tolerance multiplied by the\n                                                maximum wavelet coefficient in the scale under consideration.\n        accuracy            (default=1e-6):     Threshold on the standard deviation of the residual noise. Exit main\n                                                loop when this threshold is reached.\n        major_loop_miter    (default=100):      Maximum number of iterations allowed in the major loop. Exit\n                                                condition.\n        minor_loop_miter    (default=30):       Maximum number of iterations allowed in the minor loop. Serves as an\n                                                exit condition when the SNR does not reach a maximum.\n        all_on_gpu          (default=False):    Boolean specifier to toggle all gpu modes on.\n        decom_mode          (default='ser'):    Specifier for decomposition mode - serial, multiprocessing, or gpu.\n        core_count          (default=1):        In the event that multiprocessing, specifies the number of cores.\n        conv_device         (default='cpu'):    Specifier for device to be used - cpu or gpu.\n        conv_mode           (default='linear'): Specifier for convolution mode - linear or circular.\n        extraction_mode     (default='cpu'):    Specifier for mode to be used - cpu or gpu.\n        enforce_positivity  (default=False):    Boolean specifier for whether or not a model must be strictly positive.\n        edge_suppression    (default=False):    Boolean specifier for whether or not the edges are to be suprressed.\n        edge_offset         (default=0):        Numeric value for an additional user-specified number of edge pixels\n                                                to be ignored. This is added to the minimum suppression.\n\n        OUTPUTS:\n        self.model          (no default):       Model extracted by the algorithm.\n        self.residual       (no default):       Residual signal after deconvolution.\n        \"\"\"\n\n        # The following preserves the dirty image as it will be changed on every iteration.\n\n        dirty_data = self.dirty_data\n\n        scale_count = start_scale\n\n\n        while not (self.complete):\n\n            logger.info(\"MORESANE at scale {}\".format(scale_count))\n\n            self.moresane(subregion=subregion, scale_count=scale_count, sigma_level=sigma_level, loop_gain=loop_gain,\n                          tolerance=tolerance, accuracy=accuracy, major_loop_miter=major_loop_miter,\n                          minor_loop_miter=minor_loop_miter, all_on_gpu=all_on_gpu, decom_mode=decom_mode,\n                          core_count=core_count, conv_device=conv_device, conv_mode=conv_mode,\n                          extraction_mode=extraction_mode, enforce_positivity=enforce_positivity,\n                          edge_suppression=edge_suppression, edge_offset=edge_offset,\n                          flux_threshold=flux_threshold, neg_comp=neg_comp,\n                          edge_excl=edge_excl, int_excl=int_excl)\n\n            self.dirty_data = self.residual\n\n            scale_count += 1\n\n            if (scale_count>(np.log2(self.dirty_data.shape[0]))-1):\n                logger.info(\"Maximum scale reached - finished.\")\n                break\n\n            if (scale_count>stop_scale):\n                logger.info(\"Maximum scale reached - finished.\")\n                break\n\n        # Restores the original dirty image.\n\n        self.dirty_data = dirty_data\n        self.complete = False", "response": "This method is used to remove sources uncovered by the Moresane algorithm. This method is used to remove sources uncovered by the Moresane algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_input(self, input_hdr):\n\n        input_slice = input_hdr['NAXIS']*[0]\n\n        for i in range(input_hdr['NAXIS']):\n            if input_hdr['CTYPE%d'%(i+1)].startswith(\"RA\"):\n                input_slice[-1] = slice(None)\n            if input_hdr['CTYPE%d'%(i+1)].startswith(\"DEC\"):\n                input_slice[-2] = slice(None)\n\n        return input_slice", "response": "This method handles the input data and returns the input data slice."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlisting actuators programs or sensors", "response": "def ls(self, what):\n        \"\"\"List actuators, programs or sensors (what is string)\"\"\"\n        for i in getattr(self.system, what):\n            self.logger.info('%s: %s: %s', i.__class__.__name__, i, i.status)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef help(self, *args, **kwargs):\n        if len(args) > 0 or len(kwargs) > 0:\n            import pydoc\n\n            pydoc.help(*args, **kwargs)\n        else:\n            hstr = helpstr\n            for i in hstr.split(\"\\n\"):\n                self.logger.info(i)\n        return True", "response": "Print Automate help if no parameter is given act as pydoc. help"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef text_ui(self):\n        self.logger.info(\"Starting command line interface\")\n        self.help()\n        try:\n            self.ipython_ui()\n        except ImportError:\n            self.fallback_ui()\n        self.system.cleanup()", "response": "Start the text UI main loop"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _prepare_headers(self, additional_headers=None, **kwargs):\n        user_agent = \"pyseaweed/{version}\".format(version=__version__)\n        headers = {\"User-Agent\": user_agent}\n        if additional_headers is not None:\n            headers.update(additional_headers)\n        return headers", "response": "Prepare headers for http communication."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns response to http HEAD on provided url", "response": "def head(self, url, *args, **kwargs):\n        \"\"\"Returns response to http HEAD\n        on provided url\n        \"\"\"\n        res = self._conn.head(url, headers=self._prepare_headers(**kwargs))\n        if res.status_code == 200:\n            return res\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget data under the provided url as text Returns None if the url is not found", "response": "def get_data(self, url, *args, **kwargs):\n        \"\"\"Gets data from url as text\n\n        Returns content under the provided url as text\n\n        Args:\n            **url**: address of the wanted data\n\n            .. versionadded:: 0.3.2\n                **additional_headers**: (optional) Additional headers\n                to be used with request\n\n        Returns:\n            string\n\n        \"\"\"\n        res = self._conn.get(url, headers=self._prepare_headers(**kwargs))\n        if res.status_code == 200:\n            return res.text\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets data from url as bytes ie. for binary data Returns None if the url is not found", "response": "def get_raw_data(self, url, *args, **kwargs):\n        \"\"\"Gets data from url as bytes\n\n        Returns content under the provided url as bytes\n        ie. for binary data\n\n        Args:\n            **url**: address of the wanted data\n\n            .. versionadded:: 0.3.2\n                **additional_headers**: (optional) Additional headers\n                to be used with request\n\n        Returns:\n            bytes\n\n        \"\"\"\n        res = self._conn.get(url, headers=self._prepare_headers(**kwargs))\n        if res.status_code == 200:\n            return res.content\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef post_file(self, url, filename, file_stream, *args, **kwargs):\n        res = self._conn.post(url, files={filename: file_stream},\n                              headers=self._prepare_headers(**kwargs))\n        if res.status_code == 200 or res.status_code == 201:\n            return res.text\n        else:\n            return None", "response": "Uploads file to provided url. Returns contents as text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes data under provided url Returns status as boolean.", "response": "def delete_data(self, url, *args, **kwargs):\n        \"\"\"Deletes data under provided url\n\n        Returns status as boolean.\n\n        Args:\n            **url**: address of file to be deleted\n\n            .. versionadded:: 0.3.2\n                **additional_headers**: (optional) Additional headers\n                to be used with request\n\n        Returns:\n            Boolean. True if request was successful. False if not.\n        \"\"\"\n        res = self._conn.delete(url, headers=self._prepare_headers(**kwargs))\n        if res.status_code == 200 or res.status_code == 202:\n            return True\n        else:\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cmd_feed():\n    r = rocket.operations.cool_feed.get()\n    r = handle_error(r)\n    j = r.json()\n\n    lines = []\n\n    for date, operations in sorted(list(j[\"dates\"].items())):\n        lines += [[\n            click.style(\"===== %s =====\" % date, fg=\"blue\", bold=True),\n            None, None, None\n        ]] + [[\n            op[\"merchant\"][\"name\"],\n            \"\",\n            op[\"display_money\"][\"amount\"],\n            op[\"display_money\"][\"currency_code\"],\n        ] for op in reversed(operations)] + [[\n            None, None, None, None\n        ]]\n\n    click.echo(tabulate(lines, tablefmt=\"plain\"))", "response": "Display the current feed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransferring a single item from another recipient to another.", "response": "def cmd_transfer(recipient, amount):\n    \"\"\"\n    \u041f\u0435\u0440\u0435\u0432\u0435\u0441\u0442\u0438 \u0434\u0435\u043d\u044c\u0433\u0438 \u043d\u0430 \u043d\u043e\u043c\u0435\u0440 \u043a\u0430\u0440\u0442\u044b.\n    \"\"\"\n    r = rocket.card2card.transfer.post(params={\n        \"source_card\": recipient,\n        \"amount\": amount\n    })\n    r = handle_error(r)\n    j = r.json()\n\n    if j[\"status\"] == \"approved\":\n        template = \"\".join([\n            click.style(\"\u041f\u043b\u0430\u0442\u0451\u0436 \u043f\u0440\u0438\u043d\u044f\u0442! \", fg=\"green\", bold=True),\n            \"\u041e\u0441\u0442\u0430\u0442\u043e\u043a: {rur} \u0440\u0443\u0431\u043b\u0435\u0439\"])\n        click.echo(template.format(rur=j[\"balance\"]))\n    else:\n        click.secho(j[\"errors\"], fg=\"red\", bold=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef extract_diacritic(*diacritics):\n    def _(ch):\n        decomposed_form = unicodedata.normalize(\"NFD\", ch)\n        for diacritic in diacritics:\n            if diacritic in decomposed_form:\n                return diacritic\n    return _", "response": "Returns a function that takes a Unicode character and returns the member of the collection that has or None if the character is not in the collection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_breathing(ch, breathing):\n    decomposed = unicodedata.normalize(\"NFD\", ch)\n    if len(decomposed) > 1 and decomposed[1] == LONG:\n        return unicodedata.normalize(\n            \"NFC\", decomposed[0:2] + breathing + decomposed[2:])\n    else:\n        return unicodedata.normalize(\n            \"NFC\", decomposed[0] + breathing + decomposed[1:])", "response": "Add the given breathing to the given character."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a function that takes a string and returns the string without those diacritics.", "response": "def remove_diacritic(*diacritics):\n    \"\"\"\n    Given a collection of Unicode diacritics, return a function that takes a\n    string and returns the string without those diacritics.\n    \"\"\"\n    def _(text):\n        return unicodedata.normalize(\"NFC\", \"\".join(\n            ch\n            for ch in unicodedata.normalize(\"NFD\", text)\n            if ch not in diacritics)\n        )\n    return _"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef change_digital(self, pin_nr, value):\n        if not self._board:\n            return\n        with self._lock:\n            self._act_digital[pin_nr].pin.write(value)", "response": "Change the value of a digital pin."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create(self, **kwargs):\n        if self.created:\n            raise RuntimeError(\n                '{} already created.'.format(self.__model_type__.__name__))\n\n        kwargs = self.merge_kwargs(self._create_kwargs, kwargs)\n\n        self._inner = self.helper.create(\n            self.name, *self._create_args, **kwargs)", "response": "Create an instance of this resource definition."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove(self, **kwargs):\n        self.helper.remove(self.inner(), **kwargs)\n        self._inner = None", "response": "Remove an instance of this resource definition."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setup(self, helper=None, **create_kwargs):\n        if self.created:\n            return\n\n        self.set_helper(helper)\n        self.create(**create_kwargs)\n        return self", "response": "Setup this resource so that it is ready to be used in a test."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the helper for this object.", "response": "def set_helper(self, helper):\n        \"\"\"\n        .. todo::\n\n            Document this.\n        \"\"\"\n        # We don't want to \"unset\" in this method.\n        if helper is None:\n            return\n\n        # Get the right kind of helper if given a DockerHelper\n        if isinstance(helper, DockerHelper):\n            helper = helper._helper_for_model(self.__model_type__)\n\n        # We already have this one.\n        if helper is self._helper:\n            return\n        if self._helper is None:\n            self._helper = helper\n        else:\n            raise RuntimeError('Cannot replace existing helper.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef as_fixture(self, name=None):\n        if name is None:\n            name = self.name\n\n        def deco(f):\n            @functools.wraps(f)\n            def wrapper(*args, **kw):\n                with self:\n                    kw[name] = self\n                    return f(*args, **kw)\n            return wrapper\n        return deco", "response": "A decorator to inject this container into a function as a test fixture."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef inner(self):\n        if not self.created:\n            raise RuntimeError(\n                '{} not created yet.'.format(self.__model_type__.__name__))\n        return self._inner", "response": "Returns the underlying Docker model object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef setup(self, helper=None, **run_kwargs):\n        if self.created:\n            return\n\n        self.set_helper(helper)\n        self.run(**run_kwargs)\n        self.wait_for_start()\n        return self", "response": "Creates the container starts it and waits for it to completely start."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstopping and remove the container if it exists.", "response": "def teardown(self):\n        \"\"\"\n        Stop and remove the container if it exists.\n        \"\"\"\n        while self._http_clients:\n            self._http_clients.pop().close()\n        if self.created:\n            self.halt()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the current status of the container from Docker.", "response": "def status(self):\n        \"\"\"\n        Get the container's current status from Docker.\n\n        If the container does not exist (before creation and after removal),\n        the status is ``None``.\n        \"\"\"\n        if not self.created:\n            return None\n        self.inner().reload()\n        return self.inner().status"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping the container. The container must have been created. :param timeout: Timeout in seconds to wait for the container to stop before sending a ``SIGKILL``. Default: 5 (half the Docker default)", "response": "def stop(self, timeout=5):\n        \"\"\"\n        Stop the container. The container must have been created.\n\n        :param timeout:\n            Timeout in seconds to wait for the container to stop before sending\n            a ``SIGKILL``. Default: 5 (half the Docker default)\n        \"\"\"\n        self.inner().stop(timeout=timeout)\n        self.inner().reload()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates the container and start it. Similar to docker run.", "response": "def run(self, fetch_image=True, **kwargs):\n        \"\"\"\n        Create the container and start it. Similar to ``docker run``.\n\n        :param fetch_image:\n            Whether to try pull the image if it's not found. The behaviour here\n            is similar to ``docker run`` and this parameter defaults to\n            ``True``.\n        :param **kwargs: Keyword arguments passed to :meth:`.create`.\n        \"\"\"\n        self.create(fetch_image=fetch_image, **kwargs)\n        self.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for_start(self):\n        if self.wait_matchers:\n            matcher = UnorderedMatcher(*self.wait_matchers)\n            self.wait_for_logs_matching(matcher, timeout=self.wait_timeout)", "response": "Wait for the container to start."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_host_port(self, container_port, proto='tcp', index=0):\n        port_spec = '{}/{}'.format(container_port, proto)\n        return self._host_port(port_spec, index)", "response": "Returns the IP and port of the container."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the first host port that has a mapping. Useful when a container publishes only one port.", "response": "def get_first_host_port(self):\n        \"\"\"\n        Get the first mapping of the first (lowest) container port that has a\n        mapping. Useful when a container publishes only one port.\n\n        Note that unlike the Docker API, which sorts ports lexicographically\n        (e.g. ``90/tcp`` > ``8000/tcp``), we sort ports numerically so that the\n        lowest port is always chosen.\n        \"\"\"\n        mapped_ports = {p: m for p, m in self.ports.items() if m is not None}\n        if not mapped_ports:\n            raise RuntimeError('Container has no published ports')\n\n        def sort_key(port_string):\n            port, proto = port_string.split('/', 1)\n            return int(port), proto\n        firt_port_spec = sorted(mapped_ports.keys(), key=sort_key)[0]\n\n        return self._host_port(firt_port_spec, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget container logs. This method does not support streaming, use :meth:`stream_logs` for that.", "response": "def get_logs(self, stdout=True, stderr=True, timestamps=False, tail='all',\n                 since=None):\n        \"\"\"\n        Get container logs.\n\n        This method does not support streaming, use :meth:`stream_logs` for\n        that.\n        \"\"\"\n        return self.inner().logs(\n            stdout=stdout, stderr=stderr, timestamps=timestamps, tail=tail,\n            since=since)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stream_logs(self, stdout=True, stderr=True, tail='all', timeout=10.0):\n        return stream_logs(\n            self.inner(), stdout=stdout, stderr=stderr, tail=tail,\n            timeout=timeout)", "response": "Stream logs from the current container."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wait_for_logs_matching(self, matcher, timeout=10, encoding='utf-8',\n                               **logs_kwargs):\n        \"\"\"\n        Wait for logs matching the given matcher.\n        \"\"\"\n        wait_for_logs_matching(\n            self.inner(), matcher, timeout=timeout, encoding=encoding,\n            **logs_kwargs)", "response": "Wait for logs matching the given matcher."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef http_client(self, port=None):\n        # Local import to avoid potential circularity.\n        from seaworthy.client import ContainerHttpClient\n        client = ContainerHttpClient.for_container(self, container_port=port)\n        self._http_clients.append(client)\n        return client", "response": "Construct an HTTP client for this container."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cmd_register(phone):\n    if phone is None:\n        phone = click.prompt(\"\u041d\u043e\u043c\u0435\u0440 \u0442\u0435\u043b\u0435\u0444\u043e\u043d\u0430\")\n\n    r = rocket.devices.register.post(data={\"phone\": phone})\n    r = handle_error(r)\n\n    id = r.json()[\"sms_verification\"][\"id\"]\n    code = click.prompt(\"\u0412\u0432\u0435\u0434\u0438\u0442\u0435 \u043a\u043e\u0434 \u0438\u0437 SMS\", type=int)\n\n    r = rocket.sms_verifications[id][\"verify\"].patch(data={\"code\": code})\n    r = handle_error(r)\n    j = r.json()\n\n    click.secho(\"\u0414\u043e\u0431\u0440\u043e \u043f\u043e\u0436\u0430\u043b\u043e\u0432\u0430\u0442\u044c, {}!\".format(j[\"user\"][\"first_name\"]), fg=\"green\")\n\n    config.email = j[\"user\"][\"email\"]\n    config.write()", "response": "\u0412\u043e\u0439\u0442\u0438 \u0432 \u043f\u0440\u0438\u043b\u043e\u0436\u0435\u043d\u0438\u0435 \u0441 \u043f\u043e\u043c\u043e\u0449\u044c\u044e SMS."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_autosummary_file(modules, opts):\n    # type: (List[unicode], Any, unicode) -> None\n    \"\"\"Create the module's index.\"\"\"\n    lines = [\n        'API Reference',\n        '=============',\n        '',\n        '.. autosummary::',\n        '   :template: api_module.rst',\n        '   :toctree: {}'.format(opts.destdir),\n        '',\n    ]\n\n    modules.sort()\n    for module in modules:\n        lines.append('   {}'.format(module))\n    lines.append('')\n\n    fname = path.join(opts.srcdir, '{}.rst'.format(opts.docname))\n    logger.info('[apigen] creating API docs file: {}'.format(fname))\n    with FileAvoidWrite(fname) as f:\n        f.write('\\n'.join(lines))", "response": "Create the autosummary file for the given modules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlooking for every file in the directory tree and create the corresponding ReST files.", "response": "def recurse_tree(rootpath, excludes, opts):\n    # type: (unicode, List[unicode], Any) -> List[unicode]\n    \"\"\"\n    Look for every file in the directory tree and create the corresponding\n    ReST files.\n    \"\"\"\n    if INITPY in os.listdir(rootpath):\n        path_prefix = path.sep.join(rootpath.split(path.sep)[:-1])\n    else:\n        path_prefix = rootpath\n\n    toplevels = []\n    followlinks = getattr(opts, 'followlinks', False)\n    includeprivate = getattr(opts, 'includeprivate', False)\n    implicit_namespaces = getattr(opts, 'implicit_namespaces', False)\n    for root, subs, files in walk(rootpath, followlinks=followlinks):\n        # document only Python module files (that aren't excluded)\n        py_files = sorted(f for f in files\n                          if path.splitext(f)[1] in PY_SUFFIXES and\n                          not is_excluded(path.join(root, f), excludes))\n        is_pkg = INITPY in py_files\n        if is_pkg:\n            py_files.remove(INITPY)\n            py_files.insert(0, INITPY)\n        elif root != rootpath:\n            # only accept non-package at toplevel unless using implicit\n            # namespaces\n            if not implicit_namespaces:\n                del subs[:]\n                continue\n        # remove hidden ('.') and private ('_') directories, as well as\n        # excluded dirs\n        if includeprivate:\n            exclude_prefixes = ('.',)  # type: Tuple[unicode, ...]\n        else:\n            exclude_prefixes = ('.', '_')\n        subs[:] = sorted(sub for sub in subs\n                         if not sub.startswith(exclude_prefixes) and\n                         not is_excluded(path.join(root, sub), excludes))\n\n        pkg = root[len(path_prefix):].lstrip(path.sep).replace(path.sep, '.')\n        for py_file in py_files:\n            if not shall_skip(path.join(root, py_file), opts):\n                if py_file == INITPY:\n                    module = ''\n                else:\n                    module = path.splitext(py_file)[0]\n                toplevels.append(makename(pkg, module))\n\n    return toplevels"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(argv=sys.argv):\n    # type: (List[str]) -> int\n    \"\"\"Parse and check the command line arguments.\"\"\"\n    parser = optparse.OptionParser(\n        usage=\"\"\"\\\nusage: %prog [options] -o <output_path> <module_path> [exclude_pattern, ...]\n\nLook recursively in <module_path> for Python modules and packages and create\none reST file with automodule directives per package in the <output_path>.\n\nThe <exclude_pattern>s can be file and/or directory patterns that will be\nexcluded from generation.\n\nNote: By default this script will not overwrite already created files.\"\"\")\n\n    parser.add_option('-o', '--output-dir', action='store', dest='destdir',\n                      help='Directory to place all output', default='api')\n    parser.add_option('-s', '--source-dir', action='store', dest='srcdir',\n                      help='Documentation source directory', default=BASEDIR)\n    parser.add_option('-n', '--docname', action='store', dest='docname',\n                      help='Index document name', default='api')\n    parser.add_option('-l', '--follow-links', action='store_true',\n                      dest='followlinks', default=False,\n                      help='Follow symbolic links. Powerful when combined '\n                      'with collective.recipe.omelette.')\n    parser.add_option('-P', '--private', action='store_true',\n                      dest='includeprivate',\n                      help='Include \"_private\" modules')\n    parser.add_option('--implicit-namespaces', action='store_true',\n                      dest='implicit_namespaces',\n                      help='Interpret module paths according to PEP-0420 '\n                           'implicit namespaces specification')\n    parser.add_option('--version', action='store_true', dest='show_version',\n                      help='Show version information and exit')\n    parser.add_option('--clean', action='store_true', dest='cleanup',\n                      help='Clean up generated files and exit')\n    group = parser.add_option_group('Extension options')\n    for ext in EXTENSIONS:\n        group.add_option('--ext-' + ext, action='store_true',\n                         dest='ext_' + ext, default=False,\n                         help='enable %s extension' % ext)\n\n    (opts, args) = parser.parse_args(argv[1:])\n\n    # Make this more explicitly the current directory.\n    if not opts.srcdir:\n        opts.srcdir = '.'\n\n    if opts.show_version:\n        print('Sphinx (sphinx-apidoc) %s' % __display_version__)\n        return 0\n\n    if opts.cleanup:\n        print(\"Removing generated API docs from '{}'...\".format(opts.srcdir))\n        return cleanup_api_docs(opts)\n\n    if not args:\n        parser.error('A package path is required.')\n\n    opts.rootpath, opts.excludes = args[0], args[1:]\n    return generate_api_docs(opts)", "response": "Parse and check the command line arguments and return the index file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cmd_repl():\n    import code\n    import rlcompleter  # noqa\n    import readline\n    import sys\n\n    readline.parse_and_bind(\"tab: complete\")\n    shell = code.InteractiveConsole({\n        \"rocket\": rocket,\n    })\n    shell.interact(banner=\"%s %s, Python %s on %s\" %\n                          (APP_NAME, __version__, sys.version, sys.platform))", "response": "Command line tool for repl"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_authorization_url(self):\n        url = self._get_oauth_url('authenticate')\n        query = {\n            'client_id': self._client_id,\n            'response_type': 'code',\n            'redirect_uri': self.callback\n        }\n        query_str = self.urlencode(query)\n\n        return url + '?' + query_str", "response": "Get the authorization URL to redirect the user to"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_access_token(self, verifier=None):\n        try:\n            url = self._get_oauth_url('access_token')\n\n            #build request\n            query = {\n                'client_id': self._client_id,\n                'client_secret': self._client_secret,\n                'grant_type': 'authorization_code',\n                'redirect_uri': self.callback,\n                'code': str(verifier)\n            }\n\n            query_str = self.urlencode(query)\n            request = url + '?' + query_str\n\n            #send request\n            resp = urllib.urlopen(request)\n            json = simplejson.loads(resp.read())\n\n            self.access_token = json['access_token']\n\n            return self.access_token\n\n        except Exception, e:\n            raise FoursquareError(e)", "response": "Get the access token from the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dispatch_change_event(self, object, trait_name, old, new, handler):\n\n    # Extract the arguments needed from the handler.\n    args = self.argument_transform(object, trait_name, old, new)\n\n    # Send a description of the event to the change event tracer.\n    if tnotifier._pre_change_event_tracer is not None:\n        tnotifier._pre_change_event_tracer(object, trait_name, old, new, handler)\n\n    # Dispatch the event to the listener.\n    from automate.common import SystemNotReady\n    try:\n        self.dispatch(handler, *args)\n    except SystemNotReady:\n        pass\n    except Exception as e:\n        if tnotifier._post_change_event_tracer is not None:\n            tnotifier._post_change_event_tracer(object, trait_name, old, new,\n                                                handler, exception=e)\n        # This call needs to be made inside the `except` block in case\n        # the handler wants to re-raise the exception.\n        tnotifier.handle_exception(object, trait_name, old, new)\n    else:\n        if tnotifier._post_change_event_tracer is not None:\n            tnotifier._post_change_event_tracer(object, trait_name, old, new,\n                                                handler, exception=None)", "response": "Dispatches a trait change event to a listener."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting value into value and exponent - of - 10.", "response": "def split(value, precision=1):\n    '''\n    Split `value` into value and \"exponent-of-10\", where \"exponent-of-10\" is a\n    multiple of 3.  This corresponds to SI prefixes.\n\n    Returns tuple, where the second value is the \"exponent-of-10\" and the first\n    value is `value` divided by the \"exponent-of-10\".\n\n    Args\n    ----\n    value : int, float\n        Input value.\n    precision : int\n        Number of digits after decimal place to include.\n\n    Returns\n    -------\n    tuple\n        The second value is the \"exponent-of-10\" and the first value is `value`\n        divided by the \"exponent-of-10\".\n\n    Examples\n    --------\n\n    .. code-block:: python\n\n        si_prefix.split(0.04781)   ->  (47.8, -3)\n        si_prefix.split(4781.123)  ->  (4.8, 3)\n\n    See :func:`si_format` for more examples.\n    '''\n    negative = False\n    digits = precision + 1\n\n    if value < 0.:\n        value = -value\n        negative = True\n    elif value == 0.:\n        return 0., 0\n\n    expof10 = int(math.log10(value))\n    if expof10 > 0:\n        expof10 = (expof10 // 3) * 3\n    else:\n        expof10 = (-expof10 + 3) // 3 * (-3)\n\n    value *= 10 ** (-expof10)\n\n    if value >= 1000.:\n        value /= 1000.0\n        expof10 += 3\n    elif value >= 100.0:\n        digits -= 2\n    elif value >= 10.0:\n        digits -= 1\n\n    if negative:\n        value *= -1\n\n    return value, int(expof10)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prefix(expof10):\n    '''\n    Args:\n\n        expof10 : Exponent of a power of 10 associated with a SI unit\n            character.\n\n    Returns:\n\n        str : One of the characters in \"yzafpnum kMGTPEZY\".\n    '''\n    prefix_levels = (len(SI_PREFIX_UNITS) - 1) // 2\n    si_level = expof10 // 3\n\n    if abs(si_level) > prefix_levels:\n        raise ValueError(\"Exponent out range of available prefixes.\")\n    return SI_PREFIX_UNITS[si_level + prefix_levels]", "response": "Returns the prefix of a given SI unit\n            character."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef si_format(value, precision=1, format_str=u'{value} {prefix}',\n              exp_format_str=u'{value}e{expof10}'):\n    '''\n    Format value to string with SI prefix, using the specified precision.\n\n    Parameters\n    ----------\n    value : int, float\n        Input value.\n    precision : int\n        Number of digits after decimal place to include.\n    format_str : str or unicode\n        Format string where ``{prefix}`` and ``{value}`` represent the SI\n        prefix and the value (scaled according to the prefix), respectively.\n        The default format matches the `SI prefix style`_ format.\n    exp_str : str or unicode\n        Format string where ``{expof10}`` and ``{value}`` represent the\n        exponent of 10 and the value (scaled according to the exponent of 10),\n        respectively.  This format is used if the absolute exponent of 10 value\n        is greater than 24.\n\n    Returns\n    -------\n    unicode\n        :data:`value` formatted according to the `SI prefix style`_.\n\n    Examples\n    --------\n\n    For example, with `precision=2`:\n\n    .. code-block:: python\n\n        1e-27 --> 1.00e-27\n        1.764e-24 --> 1.76 y\n        7.4088e-23 --> 74.09 y\n        3.1117e-21 --> 3.11 z\n        1.30691e-19 --> 130.69 z\n        5.48903e-18 --> 5.49 a\n        2.30539e-16 --> 230.54 a\n        9.68265e-15 --> 9.68 f\n        4.06671e-13 --> 406.67 f\n        1.70802e-11 --> 17.08 p\n        7.17368e-10 --> 717.37 p\n        3.01295e-08 --> 30.13 n\n        1.26544e-06 --> 1.27 u\n        5.31484e-05 --> 53.15 u\n        0.00223223 --> 2.23 m\n        0.0937537 --> 93.75 m\n        3.93766 --> 3.94\n        165.382 --> 165.38\n        6946.03 --> 6.95 k\n        291733 --> 291.73 k\n        1.22528e+07 --> 12.25 M\n        5.14617e+08 --> 514.62 M\n        2.16139e+10 --> 21.61 G\n        9.07785e+11 --> 907.78 G\n        3.8127e+13 --> 38.13 T\n        1.60133e+15 --> 1.60 P\n        6.7256e+16 --> 67.26 P\n        2.82475e+18 --> 2.82 E\n        1.1864e+20 --> 118.64 E\n        4.98286e+21 --> 4.98 Z\n        2.0928e+23 --> 209.28 Z\n        8.78977e+24 --> 8.79 Y\n        3.6917e+26 --> 369.17 Y\n        1.55051e+28 --> 15.51e+27\n        6.51216e+29 --> 651.22e+27\n\n    .. versionchanged:: 1.0\n        Use unicode string for :data:`format_str` and SI value format string to\n        support micro (i.e., \u00b5) characte, and change return type to unicode\n        string.\n\n        .. seealso::\n\n            `Issue #4`_.\n\n    .. _`Issue #4`: https://github.com/cfobel/si-prefix/issues/4\n    .. _SI prefix style:\n        http://physics.nist.gov/cuu/Units/checklist.html\n    '''\n    svalue, expof10 = split(value, precision)\n    value_format = u'%%.%df' % precision\n    value_str = value_format % svalue\n    try:\n        return format_str.format(value=value_str,\n                                 prefix=prefix(expof10).strip())\n    except ValueError:\n        sign = ''\n        if expof10 > 0:\n            sign = \"+\"\n        return exp_format_str.format(value=value_str,\n                                     expof10=''.join([sign, str(expof10)]))", "response": "Format value to string with SI prefix and exponent."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef si_parse(value):\n    '''\n    Parse a value expressed using SI prefix units to a floating point number.\n\n    Parameters\n    ----------\n    value : str or unicode\n        Value expressed using SI prefix units (as returned by :func:`si_format`\n        function).\n\n\n    .. versionchanged:: 1.0\n        Use unicode string for SI unit to support micro (i.e., \u00b5) character.\n\n        .. seealso::\n\n            `Issue #4`_.\n\n    .. _`Issue #4`: https://github.com/cfobel/si-prefix/issues/4\n    '''\n    CRE_10E_NUMBER = re.compile(r'^\\s*(?P<integer>[\\+\\-]?\\d+)?'\n                                r'(?P<fraction>.\\d+)?\\s*([eE]\\s*'\n                                r'(?P<expof10>[\\+\\-]?\\d+))?$')\n    CRE_SI_NUMBER = re.compile(r'^\\s*(?P<number>(?P<integer>[\\+\\-]?\\d+)?'\n                               r'(?P<fraction>.\\d+)?)\\s*'\n                               u'(?P<si_unit>[%s])?\\s*$' % SI_PREFIX_UNITS)\n    match = CRE_10E_NUMBER.match(value)\n    if match:\n        # Can be parse using `float`.\n        assert(match.group('integer') is not None or\n               match.group('fraction') is not None)\n        return float(value)\n    match = CRE_SI_NUMBER.match(value)\n    assert(match.group('integer') is not None or\n           match.group('fraction') is not None)\n    d = match.groupdict()\n    si_unit = d['si_unit'] if d['si_unit'] else ' '\n    prefix_levels = (len(SI_PREFIX_UNITS) - 1) // 2\n    scale = 10 ** (3 * (SI_PREFIX_UNITS.index(si_unit) - prefix_levels))\n    return float(d['number']) * scale", "response": "Parses a value expressed using SI prefix units to a floating point number."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the status of a specific sensor.", "response": "def set_status(self, name, status):\n        \"\"\"\n            Set sensor ``name`` status to ``status``.\n        \"\"\"\n        getattr(self.system, name).status = status\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets statuses of the objects in the system from a dictionary of format name = > status", "response": "def set_object_status(self, statusdict):\n        \"\"\"\n            Set statuses from a dictionary of format ``{name: status}``\n        \"\"\"\n        for name, value in statusdict.items():\n            getattr(self.system, name).status = value\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntoggles boolean - valued sensor status between True and False.", "response": "def toggle_object_status(self, objname):\n        \"\"\"\n            Toggle boolean-valued sensor status between ``True`` and ``False``.\n        \"\"\"\n        o = getattr(self.system, objname)\n        o.status = not o.status\n        self.system.flush()\n        return o.status"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets sensors as a dictionary of format name = > status", "response": "def get_sensors(self):\n        \"\"\"\n            Get sensors as a dictionary of format ``{name: status}``\n        \"\"\"\n        return {i.name: i.status for i in self.system.sensors}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_websensors(self):\n        return {i.name: i.status for i in self.system.sensors if self.tag & i.tags}", "response": "Get sensors with defined tag as a dictionary of format name = > status"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_actuators(self):\n        return {i.name: i.status for i in self.system.actuators}", "response": "Get the list of actuators in the system as a dictionary of format name = > status"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the last log entries as a string.", "response": "def log(self):\n        \"\"\"\n            Return recent log entries as a string.\n        \"\"\"\n        logserv = self.system.request_service('LogStoreService')\n        return logserv.lastlog(html=False)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all the available tariffs", "response": "def cmd_tariffs():\n    \"\"\"\n    \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0435\u0442\u044c \u0441\u043f\u0438\u0441\u043e\u043a \u0442\u0430\u0440\u0438\u0444\u043e\u0432.\n    \"\"\"\n    r = rocket.tariffs.get()\n    r = handle_error(r)\n\n    for tariff in r.json():\n        click.echo(\"- {name} <{url}>\".format(\n            name=click.style(tariff[\"name\"], fg=\"green\", bold=True),\n            url=tariff[\"url\"]))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_or_create(cls, filename=None, no_input=False, create_new=False, **kwargs):\n        parser = argparse.ArgumentParser()\n        parser.add_argument('--no_input', action='store_true')\n        parser.add_argument('--create_new', action='store_true')\n        args = parser.parse_args()\n\n        if args.no_input:\n            print('Parameter --no_input was given')\n            no_input = True\n        if args.create_new:\n            print('Parameter --create_new was given')\n            create_new = True\n            no_input = True\n\n        def savefile_more_recent():\n            time_savefile = os.path.getmtime(filename)\n            time_program = os.path.getmtime(sys.argv[0])\n            return time_savefile > time_program\n\n        def load_pickle():\n            with open(filename, 'rb') as of:\n                statefile_version, data = pickle.load(of)\n\n            if statefile_version != STATEFILE_VERSION:\n                raise RuntimeError(f'Wrong statefile version, please remove state file {filename}')\n            return data\n\n        def load():\n            print('Loading %s' % filename)\n            obj_list, config = load_pickle()\n            system = System(load_state=obj_list, filename=filename, **kwargs)\n\n            return system\n\n        def create():\n            print('Creating new system')\n            config = None\n            if filename:\n                try:\n                    obj_list, config = load_pickle()\n                except FileNotFoundError:\n                    config = None\n            return cls(filename=filename, load_config=config, **kwargs)\n\n        if filename and os.path.isfile(filename):\n            if savefile_more_recent() and not create_new:\n                return load()\n            else:\n                if no_input:\n                    print('Program file more recent. Loading that instead.')\n                    return create()\n                while True:\n                    answer = input('Program file more recent. Do you want to load it? (y/n) ')\n                    if answer == 'y':\n                        return create()\n                    elif answer == 'n':\n                        return load()\n        else:\n            return create()", "response": "Load a new system from a dump file or create a new one if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves state of the system to a dump file.", "response": "def save_state(self):\n        \"\"\"\n            Save state of the system to a dump file :attr:`System.filename`\n        \"\"\"\n        if not self.filename:\n            self.logger.error('Filename not specified. Could not save state')\n            return\n        self.logger.debug('Saving system state to %s', self.filename)\n        for i in reversed(range(self.num_state_backups)):\n            fname = self.filename if i == 0 else '%s.%d' % (self.filename, i)\n            new_fname = '%s.%d' % (self.filename, i+1)\n            try:\n                os.rename(fname, new_fname)\n            except FileNotFoundError:\n                pass\n\n        with open(self.filename, 'wb') as file, self.worker_thread.queue.mutex:\n            obj_list = list(self.objects)\n            config = {obj.name: obj.status for obj in obj_list\n                      if getattr(obj, 'user_editable', False)}\n            data = obj_list, config\n            pickle.dump((STATEFILE_VERSION, data), file, pickle.HIGHEST_PROTOCOL)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cmd_namespace(self):\n        import automate\n        ns = dict(list(automate.__dict__.items()) + list(self.namespace.items()))\n        return ns", "response": "A read - only property that gives the namespace of the system for evaluating commands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_unique_name(self, obj, name='', name_from_system=''):\n        ns = self.namespace\n        newname = name\n        if not newname:\n            newname = name_from_system\n\n        if not newname:\n            newname = u\"Nameless_\" + obj.__class__.__name__\n\n        if not newname in ns:\n            return newname\n\n        counter = 0\n        while True:\n            newname1 = u\"%s_%.2d\" % (newname, counter)\n            if not newname1 in ns:\n                return newname1\n            counter += 1", "response": "Get unique name for an object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef name_to_system_object(self, name):\n        if isinstance(name, str):\n            if self.allow_name_referencing:\n                name = name\n            else:\n                raise NameError('System.allow_name_referencing is set to False, cannot convert string to name')\n        elif isinstance(name, Object):\n            name = str(name)\n        return self.namespace.get(name, None)", "response": "Converts a string name to a SystemObject instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nevaluating in system namespace", "response": "def eval_in_system_namespace(self, exec_str):\n        \"\"\"\n            Get Callable for specified string (for GUI-based editing)\n        \"\"\"\n        ns = self.cmd_namespace\n        try:\n            return eval(exec_str, ns)\n        except Exception as e:\n            self.logger.warning('Could not execute %s, gave error %s', exec_str, e)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef register_service_functions(self, *funcs):\n        for func in funcs:\n            self.namespace[func.__name__] = func", "response": "Register functions in the system namespace. Called by Services."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a new service into the system. Called by Services. add_service", "response": "def register_service(self, service):\n        \"\"\"\n            Register service into the system. Called by Services.\n        \"\"\"\n        if service not in self.services:\n            self.services.append(service)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef request_service(self, type, id=0):\n        srvs = self.services_by_name.get(type)\n        if not srvs:\n            return\n\n        ser = srvs[id]\n\n        if not ser.system:\n            ser.setup_system(self, id=id)\n        return ser", "response": "Request a specific service from the system."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cleanup(self):\n\n        self.pre_exit_trigger = True\n\n        self.logger.info(\"Shutting down %s, please wait a moment.\", self.name)\n        for t in threading.enumerate():\n            if isinstance(t, TimerClass):\n                t.cancel()\n        self.logger.debug('Timers cancelled')\n\n        for i in self.objects:\n            i.cleanup()\n\n        self.logger.debug('Sensors etc cleanups done')\n\n        for ser in (i for i in self.services if isinstance(i, AbstractUserService)):\n            ser.cleanup_system()\n        self.logger.debug('User services cleaned up')\n        if self.worker_thread.is_alive():\n            self.worker_thread.stop()\n        self.logger.debug('Worker thread really stopped')\n\n        for ser in (i for i in self.services if isinstance(i, AbstractSystemService)):\n            ser.cleanup_system()\n        self.logger.debug('System services cleaned up')\n        threads = list(t.name for t in threading.enumerate() if t.is_alive() and not t.daemon)\n        if threads:\n            self.logger.info('After cleanup, we have still the following threads '\n                             'running: %s', ', '.join(threads))", "response": "Clean up after quitting the object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute commands in automate namespace", "response": "def cmd_exec(self, cmd):\n        \"\"\"\n            Execute commands in automate namespace\n        \"\"\"\n\n        if not cmd:\n            return\n        ns = self.cmd_namespace\n        import copy\n        rval = True\n        nscopy = copy.copy(ns)\n        try:\n            r = eval(cmd, ns)\n            if isinstance(r, SystemObject) and not r.system:\n                r.setup_system(self)\n            if callable(r):\n                r = r()\n                cmd += \"()\"\n            self.logger.info(\"Eval: %s\", cmd)\n            self.logger.info(\"Result: %s\", r)\n        except SyntaxError:\n            r = {}\n            try:\n                exec (cmd, ns)\n                self.logger.info(\"Exec: %s\", cmd)\n            except ExitException:\n                raise\n            except Exception as e:\n                self.logger.info(\"Failed to exec cmd %s: %s.\", cmd, e)\n                rval = False\n            for key, value in list(ns.items()):\n                if key not in nscopy or not value is nscopy[key]:\n                    if key in self.namespace:\n                        del self.namespace[key]\n                    self.namespace[key] = value\n                    r[key] = value\n            self.logger.info(\"Set items in namespace: %s\", r)\n        except ExitException:\n            raise\n        except Exception as e:\n            self.logger.info(\"Failed to eval cmd %s: %s\", cmd, e)\n            return False\n\n        return rval"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_parser():\n    parser = argparse.ArgumentParser(description=\"Runs the pymoresane deconvolution algorithm with the specified \"\n                                                 \"arguments. In the event that non-critical parameters are missing, \"\n                                                 \"the defaults will be used.\",\n                                                  formatter_class=argparse.ArgumentDefaultsHelpFormatter)\n\n    parser.add_argument(\"dirty\", help=\"File name and location of the input dirty map .fits file.\")\n\n    parser.add_argument(\"psf\", help=\"File name and location input psf .fits file.\")\n\n    parser.add_argument(\"outputname\", help=\"File name and location of the output model and residual .fits files.\",\n                        nargs='?', default=None)\n\n    parser.add_argument(\"-sr\", \"--singlerun\", help=\"Specify whether pymoresane is to be run in scale-by-scale mode or \"\n                                                   \"in single-run mode. Scale-by-scale is usually the better choice.\"\n                                                   , action=\"store_true\")\n\n    parser.add_argument(\"-sbr\", \"--subregion\", help=\"Specify pixel width of the central region of the dirty .fits \"\n                                                   \"which is to be deconvolved.\", default=None, type=int)\n\n    parser.add_argument(\"-sc\", \"--scalecount\", help=\"Specify the maximum number of wavelet scales which the algorithm \"\n                                                    \"is to consider. Only applies in single-run mode. For \"\n                                                    \"scale-by-scale mode, use --startscale and --stopscale\"\n                                                    , default=None,type=int)\n\n    parser.add_argument(\"-sts\", \"--startscale\", help=\"Specify first scale to consider in the scale-by-scale case. \"\n                                                     \"Should be 1 in almost all cases.\", default=1, type=int)\n\n    parser.add_argument(\"-sps\", \"--stopscale\", help=\"Specify last scale to consider in the scale-by-scale case. \"\n                                                    \"Can be safely omitted if all scales are to be considered.\"\n                                                    , default=20, type=int)\n\n    parser.add_argument(\"-sl\", \"--sigmalevel\", help=\"Specify the sigma level at which the thresholding of the wavelet \"\n                                                    \"coefficients is to be performed. May be used to reduce false \"\n                                                    \"detections.\", default=4, type=float)\n\n    parser.add_argument(\"-lg\", \"--loopgain\", help=\"Specify the loop gain for the deconvolution step.\"\n                                                  , default=0.2, type=float)\n\n    parser.add_argument(\"-tol\", \"--tolerance\", help=\"Specify the percentage of the maximum wavelet coefficient which \"\n                                                    \"is to be used as a threshold for significance at each iteration.\"\n                                                    , default=0.75, type=float)\n\n    parser.add_argument(\"-ac\", \"--accuracy\", help=\"Specify the relative change in the STD of the residual that is \"\n                                                  \"considered sufficient as a stopping criteria.\"\n                                                  , default=1e-6, type=float)\n\n    parser.add_argument(\"-malm\", \"--majorloopmiter\", help=\"Specify the maximum number of major loop iterations.\"\n                                                          , default=100, type=int)\n\n    parser.add_argument(\"-milm\", \"--minorloopmiter\", help=\"Specify the maximum number of minor loop iterations.\"\n                                                          , default=50, type=int)\n\n    parser.add_argument(\"-aog\", \"--allongpu\", help=\"Specify whether as much code as possible is to be executed on the \"\n                                                   \"gpu. Overrides the behaviour of all other gpu options\"\n                                                   , action='store_true')\n\n    parser.add_argument(\"-dm\", \"--decommode\", help=\"Specify whether wavelet decompositions are to performed using a \"\n                                                   \"single CPU core, multiple CPU cores or the GPU.\"\n                                                   , default=\"ser\", choices=[\"ser\",\"mp\",\"gpu\"])\n\n    parser.add_argument(\"-cc\", \"--corecount\", help=\"Specify the number of CPU cores to be used in the event that \"\n                                                   \"multiprocessing is enabled. This might not improve performance.\"\n                                                   , default=1, type=int)\n\n    parser.add_argument(\"-cd\", \"--convdevice\", help=\"Specify whether convolutions are to performed using the CPU or \"\n                                                    \"the GPU.\", default=\"cpu\", choices=[\"cpu\",\"gpu\"])\n\n    parser.add_argument(\"-cm\", \"--convmode\", help=\"Specify convolution is to be circular or linear.\"\n                                                  , default=\"circular\", choices=[\"circular\",\"linear\"])\n\n    parser.add_argument(\"-em\", \"--extractionmode\", help=\"Specify whether source extraction is to be performed \"\n                                                        \"using the CPU or the GPU.\"\n                                                        , default=\"cpu\", choices=[\"cpu\",\"gpu\"])\n\n    parser.add_argument(\"-ep\", \"--enforcepositivity\", help=\"Specify whether or not the model is constrained to be \"\n                                                           \"strictly positive. Will massively improve the model in \"\n                                                           \"simulated cases, but may cause problems with real data.\"\n                                                           , action=\"store_true\")\n\n    parser.add_argument(\"-es\", \"--edgesuppression\", help=\"Specify whether or not the edge-corrupted wavelets are to \"\n                                                         \"be suppressed. Will decrease sensitivity along data edges.\"\n                                                         , action=\"store_true\")\n\n    parser.add_argument(\"-eo\", \"--edgeoffset\", help=\"Specify an additional offset along the edges. May reduce false \"\n                                                    \"detections caused by convolution artifacts.\"\n                                                    , default=0, type=int)\n\n    parser.add_argument(\"-ll\", \"--loglevel\", help=\"Specify logging level.\", default=\"INFO\"\n                                                  , choices=[\"DEBUG\",\"INFO\", \"WARNING\", \"ERROR\",\"CRITICAL\"])\n\n    parser.add_argument(\"-m\", \"--mask\", help=\"File name and location of the input .fits mask.\", default=None)\n\n    parser.add_argument(\"-ft\", \"--fluxthreshold\", help=\"Flux threshold level for shallow deconvolution.\", default=0,\n                        type=float)\n\n    parser.add_argument(\"-rn\", \"--residualname\", help=\"Specific residual image name.\", default=None)\n\n    parser.add_argument(\"-mn\", \"--modelname\", help=\"Specific model image name.\", default=None)\n\n    parser.add_argument(\"-rsn\", \"--restoredname\", help=\"Specific restored image name.\", default=None)\n\n    parser.add_argument(\"-nc\", \"--negcomp\", help=\"Specify whether \"\n                                                 \"anticorrelations are to be \"\n                                                 \"used in the source \"\n                                                 \"extraction step.\"\n                                                 , action='store_true')\n\n    parser.add_argument(\"-ee\", \"--edgeexcl\", help=\"Number of pixels to \"\n                                                  \"exclude from the edges \"\n                                                  \"when estimating the noise\"\n                                                  \".\", type=int, default=0)\n\n    parser.add_argument(\"-ie\", \"--intexcl\", help=\"Number of pixels to \"\n                                                  \"exclude from the center \"\n                                                  \"when estimating the noise\"\n                                                  \".\", type=int, default=0)\n\n    return parser.parse_args()", "response": "This function handles the command line arguments and returns a parser object."}
